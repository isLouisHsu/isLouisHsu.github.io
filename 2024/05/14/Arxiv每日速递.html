<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2024-05-07) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新380篇论文，其中：  69篇自然语言处理（cs.CL） 62篇计算机视觉（cs.CV） 117篇机器学习（cs.LG） 99篇人工智能（cs.AI）  自然语言处理    1. 标题：Vibe-Eval: A hard evaluation suite f">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2024-05-07)">
<meta property="og:url" content="http://louishsu.xyz/2024/05/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。 统计 今日共更新380篇论文，其中：  69篇自然语言处理（cs.CL） 62篇计算机视觉（cs.CV） 117篇机器学习（cs.LG） 99篇人工智能（cs.AI）  自然语言处理    1. 标题：Vibe-Eval: A hard evaluation suite f">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2024-05-14T07:22:26.423Z">
<meta property="article:modified_time" content="2024-05-14T07:24:10.775Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2024/05/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-05-14 15:24:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2024-05-07)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-14T07:22:26.423Z" title="发表于 2024-05-14 15:22:26">2024-05-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-14T07:24:10.775Z" title="更新于 2024-05-14 15:24:10">2024-05-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">32.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>196分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/05/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、计算机视觉、机器学习、人工智能等大方向进行划分。</p>
<h1>统计</h1>
<p>今日共更新380篇论文，其中：</p>
<ul>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">69篇自然语言处理（cs.CL）</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">62篇计算机视觉（cs.CV）</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">117篇机器学习（cs.LG）</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">99篇人工智能（cs.AI）</a></li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>标题：Vibe-Eval: A hard evaluation suite for measuring progress of multimodal  language models</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02287">https://arxiv.org/abs/2405.02287</a></p>
  <p><b>作者</b>：Piotr Padlewski,  Max Bain,  Matthew Henderson,  Zhongkai Zhu,  Nishant Relan,  Hai Pham,  Donovan Ong,  Kaloyan Aleksiev,  Aitor Ormazabal,  Samuel Phua,  Ethan Yeo,  Eugenie Lamprecht,  Qi Liu,  Yuqi Wang,  Eric Chen,  Deyu Fu,  Lei Li,  Che Zheng,  Cyprien de Masson d'Autume,  Dani Yogatama,  Mikel Artetxe,  Yi Tay</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal chat models, open benchmark, benchmark and framework, multimodal chat, evaluating multimodal chat</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce Vibe-Eval: a new open benchmark and framework for evaluating multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts, including 100 of hard difficulty, complete with gold-standard responses authored by experts. Vibe-Eval is open-ended and challenging with dual objectives: (i) vibe checking multimodal chat models for day-to-day tasks and (ii) rigorously testing and probing the capabilities of present frontier models. Notably, our hard set contains >50% questions that all frontier models answer incorrectly. We explore the nuances of designing, evaluating, and ranking models on ultra challenging prompts. We also discuss trade-offs between human and automatic evaluation, and show that automatic model evaluation using Reka Core roughly correlates to human judgment. We offer free API access for the purpose of lightweight evaluation and plan to conduct formal human evaluations for public models that perform well on the Vibe-Eval's automatic scores. We release the evaluation code and data, see this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Structural Pruning of Pre-trained Language Models via Neural  Architecture Search</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02267">https://arxiv.org/abs/2405.02267</a></p>
  <p><b>作者</b>：Aaron Klein,  Jacek Golebiowski,  Xingchen Ma,  Valerio Perrone,  Cedric Archambeau</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language understanding, language understanding task, Pre-trained language models, BERT or RoBERTa, Pre-trained language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state-of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real-world applications, due to significant GPU memory requirements and high inference latency. This paper explores neural architecture search (NAS) for structural pruning to find sub-parts of the fine-tuned network that optimally trade-off efficiency, for example in terms of model size or latency, and generalization performance. We also show how we can utilize more recently developed two-stage weight-sharing NAS approaches in this setting to accelerate the search process. Unlike traditional pruning methods with fixed thresholds, we propose to adopt a multi-objective approach that identifies the Pareto optimal set of sub-networks, allowing for a more flexible and automated compression process.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific  Sentences using Public and Proprietary LLMs</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02228">https://arxiv.org/abs/2405.02228</a></p>
  <p><b>作者</b>：Deepa Tilwani,  Yash Saxena,  Ali Mohammadi,  Edward Raff,  Amit Sheth,  Srinivasan Parthasarathy,  Manas Gaur</p>
  <p><b>备注</b>：Submitted to ACL ARR April 2024</p>
  <p><b>关键词</b>：intelligence analysts, education personnel, document or report, report is paramount, paramount for intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic citation generation for sentences in a document or report is paramount for intelligence analysts, cybersecurity, news agencies, and education personnel. In this research, we investigate whether large language models (LLMs) are capable of generating references based on two forms of sentence queries: (a) Direct Queries, LLMs are asked to provide author names of the given research article, and (b) Indirect Queries, LLMs are asked to provide the title of a mentioned article when given a sentence from a different article. To demonstrate where LLM stands in this task, we introduce a large dataset called REASONS comprising abstracts of the 12 most popular domains of scientific research on arXiv. From around 20K research articles, we make the following deductions on public and proprietary LLMs: (a) State-of-the-art, often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass percentage (PP) to minimize the hallucination rate (HR). When tested with this http URL (7B), they unexpectedly made more errors; (b) Augmenting relevant metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented generation (RAG) using Mistral demonstrates consistent and robust citation support on indirect queries and matched performance to GPT-3.5 and GPT-4. The HR across all domains and models decreased by an average of 41.93% and the PP was reduced to 0% in most cases. In terms of generation quality, the average F1 Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with adversarial samples showed that LLMs, including the Advance RAG Mistral, struggle to understand context, but the extent of this issue was small in Mistral and GPT-4-Preview. Our study con tributes valuable insights into the reliability of RAG for automated citation generation tasks.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Impact of emoji exclusion on the performance of Arabic sarcasm detection  models</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02195">https://arxiv.org/abs/2405.02195</a></p>
  <p><b>作者</b>：Ghalyah H. Aleryani,  Wael Deabes,  Khaled Albishre,  Alaa E. Abdel-Hakim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sarcasm detection, sarcasm, complex challenge, challenge of detecting, nature of sarcastic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The complex challenge of detecting sarcasm in Arabic speech on social media is increased by the language diversity and the nature of sarcastic expressions. There is a significant gap in the capability of existing models to effectively interpret sarcasm in Arabic, which mandates the necessity for more sophisticated and precise detection methods. In this paper, we investigate the impact of a fundamental preprocessing component on sarcasm speech detection. While emojis play a crucial role in mitigating the absence effect of body language and facial expressions in modern communication, their impact on automated text analysis, particularly in sarcasm detection, remains underexplored. We investigate the impact of emoji exclusion from datasets on the performance of sarcasm detection models in social media content for Arabic as a vocabulary-super rich language. This investigation includes the adaptation and enhancement of AraBERT pre-training models, specifically by excluding emojis, to improve sarcasm detection capabilities. We use AraBERT pre-training to refine the specified models, demonstrating that the removal of emojis can significantly boost the accuracy of sarcasm detection. This approach facilitates a more refined interpretation of language, eliminating the potential confusion introduced by non-textual elements. The evaluated AraBERT models, through the focused strategy of emoji removal, adeptly navigate the complexities of Arabic sarcasm. This study establishes new benchmarks in Arabic natural language processing and presents valuable insights for social media platforms.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Assessing and Verifying Task Utility in LLM-Powered Applications</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02178">https://arxiv.org/abs/2405.02178</a></p>
  <p><b>作者</b>：Negar Arabzadeh,  Siging Huo,  Nikhil Mehta,  Qinqyun Wu,  Chi Wang,  Ahmed Awadallah,  Charles L. A. Clarke,  Julia Kiseleva</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2402.09015</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, development of Large, multiple agents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at this https URL .</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02175">https://arxiv.org/abs/2405.02175</a></p>
  <p><b>作者</b>：Hsuvas Borkakoty,  Luis Espinosa-Anke</p>
  <p><b>备注</b>：Short paper</p>
  <p><b>关键词</b>：disinformation created deliberately, reference knowledge resources, created deliberately, recognised form, form of disinformation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hoaxes are a recognised form of disinformation created deliberately, with potential serious implications in the credibility of reference knowledge resources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that they often are written according to the official style guidelines. In this work, we first provide a systematic analysis of the similarities and discrepancies between legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a collection of 311 Hoax articles (from existing literature as well as official Wikipedia lists) alongside semantically similar real articles. We report results of binary classification experiments in the task of predicting whether a Wikipedia article is real or hoax, and analyze several settings as well as a range of language models. Our results suggest that detecting deceitful content in Wikipedia based on content alone, despite not having been explored much in the past, is a promising direction.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and  Multi-View Transformer</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02165">https://arxiv.org/abs/2405.02165</a></p>
  <p><b>作者</b>：Hanwen Liu,  Daniel Hajialigol,  Benny Antony,  Aiguo Han,  Xuan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deciphering the intricacies, curiosity for centuries, captivated curiosity, EEG, human brain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deciphering the intricacies of the human brain has captivated curiosity for centuries. Recent strides in Brain-Computer Interface (BCI) technology, particularly using motor imagery, have restored motor functions such as reaching, grasping, and walking in paralyzed individuals. However, unraveling natural language from brain signals remains a formidable challenge. Electroencephalography (EEG) is a non-invasive technique used to record electrical activity in the brain by placing electrodes on the scalp. Previous studies of EEG-to-text decoding have achieved high accuracy on small closed vocabularies, but still fall short of high accuracy when dealing with large open vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy of open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG pre-training to enhance the learning of semantics from EEG signals and proposes a multi-view transformer to model the EEG signal processing by different spatial regions of the brain. Experiments show that EEG2TEXT has superior performance, outperforming the state-of-the-art baseline methods by a large margin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great potential for a high-performance open-vocabulary brain-to-text system to facilitate communication.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：MedReadMe: A Systematic Study for Fine-grained Sentence Readability in  Medical Domain</b></summary>
  <p><b>编号</b>：[55]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02144">https://arxiv.org/abs/2405.02144</a></p>
  <p><b>作者</b>：Chao Jiang,  Wei Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：challenging to read, texts are notoriously, notoriously challenging, Medical texts, fine-grained complex span</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. In this paper, we present a systematic study on fine-grained readability measurements in the medical domain at both sentence-level and span-level. We introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel "Google-Easy" and "Google-Hard" categories. It supports our quantitative analysis, which covers 650 linguistic features and automatic complex word and jargon identification. Enabled by our high-quality annotation, we benchmark and improve several state-of-the-art sentence-level readability metrics for the medical domain specifically, which include unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments. We will publicly release the dataset and code.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Optimising Calls to Large Language Models with Uncertainty-Based  Two-Tier Selection</b></summary>
  <p><b>编号</b>：[59]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02134">https://arxiv.org/abs/2405.02134</a></p>
  <p><b>作者</b>：Guillem Ramírez,  Alexandra Birch,  Ivan Titov</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：cost-performance trade-off dilemma, limited budget face, Researchers and practitioners, trade-off dilemma, practitioners operating</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Researchers and practitioners operating on a limited budget face the cost-performance trade-off dilemma. The challenging decision often centers on whether to use a large LLM with better performance or a smaller one with reduced costs. This has motivated recent research in the optimisation of LLM calls. Either a cascading strategy is used, where a smaller LLM or both are called sequentially, or a routing strategy is used, where only one model is ever called. Both scenarios are dependent on a decision criterion which is typically implemented by an extra neural model. In this work, we propose a simpler solution; we use only the uncertainty of the generations of the small LLM as the decision criterion. We compare our approach with both cascading and routing strategies using three different pairs of pre-trained small and large LLMs, on nine different tasks and against approaches that require an additional neural model. Our experiments reveal this simple solution optimally balances cost and performance, outperforming existing methods on 25 out of 27 experimental setups.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Unveiling the Potential of LLM-Based ASR on Chinese Open-Source Datasets</b></summary>
  <p><b>编号</b>：[61]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02132">https://arxiv.org/abs/2405.02132</a></p>
  <p><b>作者</b>：Xuelong Geng,  Tianyi Xu,  Kun Wei,  Bingsheng Mu,  Hongfei Xue,  He Wang,  Yangze Li,  Pengcheng Guo,  Yuhang Dai,  Longhao Li,  Mingchen Shao,  Lei Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：demonstrated unparalleled effectiveness, Large Language Models, NLP tasks, automatic speech recognition, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models have demonstrated unparalleled effectiveness in various NLP tasks, and integrating LLMs with automatic speech recognition is becoming a mainstream paradigm. Building upon this momentum, our research delves into an indepth examination of this paradigm on a large opensource Chinese dataset. Specifically, our research aims to evaluate the impact of various configurations of speech encoders, LLMs, and projector modules in the context of the speech foundation encoderLLM ASR paradigm. Furthermore, we introduce a threestage training approach, expressly developed to enhance the model's ability to align auditory and textual information. The implementation of this approach, alongside the strategic integration of ASR components, enabled us to achieve the SOTA performance on the AISHELL1, TestNet, and TestMeeting test sets. Our analysis presents an empirical foundation for future research in LLMbased ASR systems and offers insights into optimizing performance using Chinese datasets. We will publicly release all scripts used for data preparation, training, inference, and scoring, as well as pretrained models and training logs to promote reproducible research.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Single and Multi-Hop Question-Answering Datasets for Reticular Chemistry  with GPT-4-Turbo</b></summary>
  <p><b>编号</b>：[62]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02128">https://arxiv.org/abs/2405.02128</a></p>
  <p><b>作者</b>：Nakul Rampal,  Kaiyu Wang,  Matthew Burigana,  Lingxiang Hou,  Juri Al-Johani,  Anna Sackmann,  Hanan S. Murayshid,  Walaa Abdullah Al-Sumari,  Arwa M. Al-Abdulkarim,  Nahla Eid Al-Hazmi,  Majed O. Al-Awad,  Christian Borgs,  Jennifer T. Chayes,  Omar M. Yaghi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large-scale datasets aimed, natural language processing, rapid advancement, advancement in artificial, artificial intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid advancement in artificial intelligence and natural language processing has led to the development of large-scale datasets aimed at benchmarking the performance of machine learning models. Herein, we introduce 'RetChemQA,' a comprehensive benchmark dataset designed to evaluate the capabilities of such models in the domain of reticular chemistry. This dataset includes both single-hop and multi-hop question-answer pairs, encompassing approximately 45,000 Q&As for each type. The questions have been extracted from an extensive corpus of literature containing about 2,530 research papers from publishers including NAS, ACS, RSC, Elsevier, and Nature Publishing Group, among others. The dataset has been generated using OpenAI's GPT-4 Turbo, a cutting-edge model known for its exceptional language understanding and generation capabilities. In addition to the Q&A dataset, we also release a dataset of synthesis conditions extracted from the corpus of literature used in this study. The aim of RetChemQA is to provide a robust platform for the development and evaluation of advanced machine learning algorithms, particularly for the reticular chemistry community. The dataset is structured to reflect the complexities and nuances of real-world scientific discourse, thereby enabling nuanced performance assessments across a variety of tasks. The dataset is available at the following link: this https URL</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Evaluating Large Language Models for Structured Science Summarization in  the Open Research Knowledge Graph</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02105">https://arxiv.org/abs/2405.02105</a></p>
  <p><b>作者</b>：Vladyslav Nechakhin,  Jennifer D'Souza,  Steffen Eger</p>
  <p><b>备注</b>：22 pages, 11 figures. In review at this https URL</p>
  <p><b>关键词</b>：traditional keywords enhances, Research Knowledge Graph, Open Research Knowledge, Large Language Models, enhances science findability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability. Current methods, such as those used by the Open Research Knowledge Graph (ORKG), involve manually curating properties to describe research papers' contributions in a structured manner, but this is labor-intensive and inconsistent between the domain expert human curators. We propose using Large Language Models (LLMs) to automatically suggest these properties. However, it's essential to assess the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before application. Our study performs a comprehensive comparative analysis between ORKG's manually curated properties and those generated by the aforementioned state-of-the-art LLMs. We evaluate LLM performance through four unique perspectives: semantic alignment and deviation with ORKG properties, fine-grained properties mapping accuracy, SciNCL embeddings-based cosine similarity, and expert surveys comparing manual annotations with LLM outputs. These evaluations occur within a multidisciplinary science setting. Overall, LLMs show potential as recommendation systems for structuring science, but further finetuning is recommended to improve their alignment with scientific tasks and mimicry of human expertise.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Argumentative Large Language Models for Explainable and Contestable  Decision-Making</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02079">https://arxiv.org/abs/2405.02079</a></p>
  <p><b>作者</b>：Gabriel Freedman,  Adam Dejl,  Deniz Gorur,  Xiang Yin,  Antonio Rago,  Francesca Toni</p>
  <p><b>备注</b>：19 pages, 17 figures</p>
  <p><b>关键词</b>：large language models, language models, knowledge encoded, knowledge zero-shot, encoded in large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited by their inability to reliably provide outputs which are explainable and contestable. In this paper, we attempt to reconcile these strengths and weaknesses by introducing a method for supplementing LLMs with argumentative reasoning. Concretely, we introduce argumentative LLMs, a method utilising LLMs to construct argumentation frameworks, which then serve as the basis for formal reasoning in decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by the supplemented LLM may be naturally explained to, and contested by, humans. We demonstrate the effectiveness of argumentative LLMs experimentally in the decision-making task of claim verification. We obtain results that are competitive with, and in some cases surpass, comparable state-of-the-art techniques.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Large Multimodal Model based Standardisation of Pathology Reports with  Confidence and their Prognostic Significance</b></summary>
  <p><b>编号</b>：[98]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02040">https://arxiv.org/abs/2405.02040</a></p>
  <p><b>作者</b>：Ethar Alzaid,  Gabriele Pergola,  Harriet Evans,  David Snead,  Fayyaz Minhas</p>
  <p><b>备注</b>：19 pages, 6 figures</p>
  <p><b>关键词</b>：Pathology reports, free-text format, large multimodal models, rich in clinical, clinical and pathological</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pathology reports are rich in clinical and pathological details but are often presented in free-text format. The unstructured nature of these reports presents a significant challenge limiting the accessibility of their content. In this work, we present a practical approach based on the use of large multimodal models (LMMs) for automatically extracting information from scanned images of pathology reports with the goal of generating a standardised report specifying the value of different fields along with estimated confidence about the accuracy of the extracted fields. The proposed approach overcomes limitations of existing methods which do not assign confidence scores to extracted fields limiting their practical use. The proposed framework uses two stages of prompting a Large Multimodal Model (LMM) for information extraction and validation. The framework generalises to textual reports from multiple medical centres as well as scanned images of legacy pathology reports. We show that the estimated confidence is an effective indicator of the accuracy of the extracted information that can be used to select only accurately extracted fields. We also show the prognostic significance of structured and unstructured data from pathology reports and show that the automatically extracted field values significant prognostic value for patient stratification. The framework is available for evaluation via the URL: this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Analyzing Narrative Processing in Large Language Models (LLMs): Using  GPT4 to test BERT</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02024">https://arxiv.org/abs/2405.02024</a></p>
  <p><b>作者</b>：Patrick Krauss,  Jannik Hösch,  Claus Metzner,  Andreas Maier,  Peter Uhrig,  Achim Schilling</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：versatile social interactions, receive complex information, basis of traditions, culture and versatile, social interactions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to transmit and receive complex information via language is unique to humans and is the basis of traditions, culture and versatile social interactions. Through the disruptive introduction of transformer based large language models (LLMs) humans are not the only entity to "understand" and produce language any more. In the present study, we have performed the first steps to use LLMs as a model to understand fundamental mechanisms of language processing in neural networks, in order to make predictions and generate hypotheses on how the human brain does language processing. Thus, we have used ChatGPT to generate seven different stylistic variations of ten different narratives (Aesop's fables). We used these stories as input for the open source LLM BERT and have analyzed the activation patterns of the hidden units of BERT using multi-dimensional scaling and cluster analysis. We found that the activation vectors of the hidden units cluster according to stylistic variations in earlier layers of BERT (1) than narrative content (4-5). Despite the fact that BERT consists of 12 identical building blocks that are stacked and trained on large text corpora, the different layers perform different tasks. This is a very useful model of the human brain, where self-similar structures, i.e. different areas of the cerebral cortex, can have different functions and are therefore well suited to processing language in a very efficient way. The proposed approach has the potential to open the black box of LLMs on the one hand, and might be a further step to unravel the neural processes underlying human language processing and cognition in general.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：The Trade-off between Performance, Efficiency, and Fairness in Adapter  Modules for Text Classification</b></summary>
  <p><b>编号</b>：[108]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02010">https://arxiv.org/abs/2405.02010</a></p>
  <p><b>作者</b>：Minh Duc Bui,  Katharina von der Wense</p>
  <p><b>备注</b>：Accepted to the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP) at NAACL 2024</p>
  <p><b>关键词</b>：achieving trustworthy NLP, natural language processing, Current natural language, trustworthy NLP, NLP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Current natural language processing (NLP) research tends to focus on only one or, less frequently, two dimensions - e.g., performance, privacy, fairness, or efficiency - at a time, which may lead to suboptimal conclusions and often overlooking the broader goal of achieving trustworthy NLP. Work on adapter modules (Houlsby et al., 2019; Hu et al., 2021) focuses on improving performance and efficiency, with no investigation of unintended consequences on other aspects such as fairness. To address this gap, we conduct experiments on three text classification datasets by either (1) finetuning all parameters or (2) using adapter modules. Regarding performance and efficiency, we confirm prior findings that the accuracy of adapter-enhanced models is roughly on par with that of fully finetuned models, while training time is substantially reduced. Regarding fairness, we show that adapter modules result in mixed fairness across sensitive groups. Further investigation reveals that, when the standard fine-tuned model exhibits limited biases, adapter modules typically do not introduce extra bias. On the other hand, when the finetuned model exhibits increased bias, the impact of adapter modules on bias becomes more unpredictable, introducing the risk of significantly magnifying these biases for certain groups. Our findings highlight the need for a case-by-case evaluation rather than a one-size-fits-all judgment.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Exploring Combinatorial Problem Solving with Large Language Models: A  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01997">https://arxiv.org/abs/2405.01997</a></p>
  <p><b>作者</b>：Mahmoud Masoud,  Ahmed Abdelhay,  Mohammed Elhenawy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, generate text based, Large Language, Language Models, textual input</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) are deep learning models designed to generate text based on textual input. Although researchers have been developing these models for more complex tasks such as code generation and general reasoning, few efforts have explored how LLMs can be applied to combinatorial problems. In this research, we investigate the potential of LLMs to solve the Travelling Salesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments employing various approaches, including zero-shot in-context learning, few-shot in-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned GPT-3.5 Turbo to solve a specific problem size and tested it using a set of various instance sizes. The fine-tuned models demonstrated promising performance on problems identical in size to the training instances and generalized well to larger problems. Furthermore, to improve the performance of the fine-tuned model without incurring additional training costs, we adopted a self-ensemble approach to improve the quality of the solutions.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Joint sentiment analysis of lyrics and audio in music</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01988">https://arxiv.org/abs/2405.01988</a></p>
  <p><b>作者</b>：Lea Schaab,  Anna Kruspe</p>
  <p><b>备注</b>：published at DAGA 2024</p>
  <p><b>关键词</b>：levels in music, audio, lyrics, mood can express, Sentiment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sentiment or mood can express themselves on various levels in music. In automatic analysis, the actual audio data is usually analyzed, but the lyrics can also play a crucial role in the perception of moods. We first evaluate various models for sentiment analysis based on lyrics and audio separately. The corresponding approaches already show satisfactory results, but they also exhibit weaknesses, the causes of which we examine in more detail. Furthermore, different approaches to combining the audio and lyrics results are proposed and evaluated. Considering both modalities generally leads to improved performance. We investigate misclassifications and (also intentional) contradictions between audio and lyrics sentiment more closely, and identify possible causes. Finally, we address fundamental problems in this research area, such as high subjectivity, lack of data, and inconsistency in emotion taxonomies.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Conformal Prediction for Natural Language Processing: A Survey</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01976">https://arxiv.org/abs/2405.01976</a></p>
  <p><b>作者</b>：Margarida M. Campos,  António Farinhas,  Chrysoula Zerva,  Mário A.T. Figueiredo,  André F.T. Martins</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, large language models, enhance decision-making reliability, language processing, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：A quantitative and typological study of Early Slavic participle clauses  and their competition</b></summary>
  <p><b>编号</b>：[125]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01972">https://arxiv.org/abs/2405.01972</a></p>
  <p><b>作者</b>：Nilo Pedrazzini</p>
  <p><b>备注</b>：259 pages, 138 figures. DPhil Thesis in Linguistics submitted and defended at the University of Oxford (December 2023). This manuscript is a version formatted for improved readability and broader dissemination</p>
  <p><b>关键词</b>：Early Slavic participle, Slavic participle constructions, Early Slavic, Early Slavic corpora, participle constructions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This thesis is a corpus-based, quantitative, and typological analysis of the functions of Early Slavic participle constructions and their finite competitors ($jegda$-'when'-clauses). The first part leverages detailed linguistic annotation on Early Slavic corpora at the morphosyntactic, dependency, information-structural, and lexical levels to obtain indirect evidence for different potential functions of participle clauses and their main finite competitor and understand the roles of compositionality and default discourse reasoning as explanations for the distribution of participle constructions and $jegda$-clauses in the corpus. The second part uses massively parallel data to analyze typological variation in how languages express the semantic space of English $when$, whose scope encompasses that of Early Slavic participle constructions and $jegda$-clauses. Probabilistic semantic maps are generated and statistical methods (including Kriging, Gaussian Mixture Modelling, precision and recall analysis) are used to induce cross-linguistically salient dimensions from the parallel corpus and to study conceptual variation within the semantic space of the hypothetical concept WHEN.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large  Language Models</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01943">https://arxiv.org/abs/2405.01943</a></p>
  <p><b>作者</b>：Zhiyu Guo,  Hidetaka Kamigaito,  Taro Wanatnabe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, advancement in Large, language understanding, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent SwiGLU-based LLMs pruning. Our approach incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on Mistral and LLaMA2 model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：CRCL at SemEval-2024 Task 2: Simple prompt optimizations</b></summary>
  <p><b>编号</b>：[130]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01942">https://arxiv.org/abs/2405.01942</a></p>
  <p><b>作者</b>：Clément Brutti-Mairesse,  Loïc Verlingue</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：clinical trial report, trial report sections, sections and statements, LLM Instruct models, present a baseline</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a baseline for the SemEval 2024 task 2 challenge, whose objective is to ascertain the inference relationship between pairs of clinical trial report sections and statements. We apply prompt optimization techniques with LLM Instruct models provided as a Language Model-as-a-Service (LMaaS). We observed, in line with recent findings, that synthetic CoT prompts significantly enhance manually crafted ones.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：OARelatedWork: A Large-Scale Dataset of Related Work Sections with  Full-texts from Open Access Sources</b></summary>
  <p><b>编号</b>：[136]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01930">https://arxiv.org/abs/2405.01930</a></p>
  <p><b>作者</b>：Martin Docekal,  Martin Fajcik,  Pavel Smrz</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：related work sections, paper introduces OARelatedWork, related work, work sections, related work generation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces OARelatedWork, the first large-scale multi-document summarization dataset for related work generation containing whole related work sections and full-texts of cited papers. The dataset includes 94 450 papers and 5 824 689 unique referenced papers. It was designed for the task of automatically generating related work to shift the field toward generating entire related work sections from all available content instead of generating parts of related work sections from abstracts only, which is the current mainstream in this field for abstractive approaches. We show that the estimated upper bound for extractive summarization increases by 217% in the ROUGE-2 score, when using full content instead of abstracts. Furthermore, we show the benefits of full content data on naive, oracle, traditional, and transformer-based baselines. Long outputs, such as related work sections, pose challenges for automatic evaluation metrics like BERTScore due to their limited input length. We tackle this issue by proposing and evaluating a meta-metric using BERTScore. Despite operating on smaller blocks, we show this meta-metric correlates with human judgment, comparably to the original BERTScore.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Semi-Parametric Retrieval via Binary Token Index</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01924">https://arxiv.org/abs/2405.01924</a></p>
  <p><b>作者</b>：Jiawei Zhou,  Li Dong,  Furu Wei,  Lei Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Semi-parametric Vocabulary Disentangled, Vocabulary Disentangled Retrieval, advanced applications, indexing efficiency, remain less explored</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The landscape of information retrieval has broadened from search services to a critical component in various advanced applications, where indexing efficiency, cost-effectiveness, and freshness are increasingly important yet remain less explored. To address these demands, we introduce Semi-parametric Vocabulary Disentangled Retrieval (SVDR). SVDR is a novel semi-parametric retrieval framework that supports two types of indexes: an embedding-based index for high effectiveness, akin to existing neural retrieval methods; and a binary token index that allows for quick and cost-effective setup, resembling traditional term-based retrieval. In our evaluation on three open-domain question answering benchmarks with the entire Wikipedia as the retrieval corpus, SVDR consistently demonstrates superiority. It achieves a 3% higher top-1 retrieval accuracy compared to the dense retriever DPR when using an embedding-based index and an 9% higher top-1 accuracy compared to BM25 when using a binary token index. Specifically, the adoption of a binary token index reduces index preparation time from 30 GPU hours to just 2 CPU hours and storage size from 31 GB to 2 GB, achieving a 90% reduction compared to an embedding-based index.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Aloe: A Family of Fine-tuned Open Healthcare LLMs</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01886">https://arxiv.org/abs/2405.01886</a></p>
  <p><b>作者</b>：Ashwin Kumar Gururajan,  Enrique Lopez-Cuena,  Jordi Bayarri-Planas,  Adrian Tormos,  Daniel Hinjos,  Pablo Bernabeu-Perez,  Anna Arias-Duart,  Pablo Agustin Martin-Torres,  Lucia Urcelay-Ganzabal,  Marta Gonzalez-Mallo,  Sergio Alvarez-Napagao,  Eduard Ayguadé-Parra,  Ulises Cortés Dario Garcia-Gasulla</p>
  <p><b>备注</b>：Five appendix</p>
  <p><b>关键词</b>：Large Language Models, Large Language, capabilities of Large, safeguard public interest, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the capabilities of Large Language Models (LLMs) in healthcare and medicine continue to advance, there is a growing need for competitive open-source models that can safeguard public interest. With the increasing availability of highly competitive open base models, the impact of continued pre-training is increasingly uncertain. In this work, we explore the role of instruct tuning, model merging, alignment, red teaming and advanced inference schemes, as means to improve current open models. To that end, we introduce the Aloe family, a set of open medical LLMs highly competitive within its scale range. Aloe models are trained on the current best base models (Mistral, LLaMA 3), using a new custom dataset which combines public data sources improved with synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase, becoming one of the first few policy-aligned open healthcare LLM using Direct Preference Optimization, setting a new standard for ethical performance in healthcare LLMs. Model evaluation expands to include various bias and toxicity datasets, a dedicated red teaming effort, and a much-needed risk assessment for healthcare LLMs. Finally, to explore the limits of current LLMs in inference, we study several advanced prompt engineering strategies to boost performance across benchmarks, yielding state-of-the-art results for open healthcare 7B LLMs, unprecedented at this scale.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Beyond Single-Event Extraction: Towards Efficient Document-Level  Multi-Event Argument Extraction</b></summary>
  <p><b>编号</b>：[157]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01884">https://arxiv.org/abs/2405.01884</a></p>
  <p><b>作者</b>：Wanlong Liu,  Li Zhou,  Dingyi Zeng,  Yichen Xiao,  Shaohuan Cheng,  Chen Zhang,  Grandee Lee,  Malu Zhang,  Wenyu Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Recent mainstream event, Event-specific Information Aggregation, mainstream event argument, extraction methods process, extraction model DEEIA</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent mainstream event argument extraction methods process each event in isolation, resulting in inefficient inference and ignoring the correlations among multiple events. To address these limitations, here we propose a multiple-event argument extraction model DEEIA (Dependency-guided Encoding and Event-specific Information Aggregation), capable of extracting arguments from all events within a document simultaneouslyThe proposed DEEIA model employs a multi-event prompt mechanism, comprising DE and EIA modules. The DE module is designed to improve the correlation between prompts and their corresponding event contexts, whereas the EIA module provides event-specific information to improve contextual understanding. Extensive experiments show that our method achieves new state-of-the-art performance on four public datasets (RAMS, WikiEvents, MLEE, and ACE05), while significantly saving the inference time compared to the baselines. Further analyses demonstrate the effectiveness of the proposed modules.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：DALLMi: Domain Adaption for LLM-based Multi-label Classifier</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01883">https://arxiv.org/abs/2405.01883</a></p>
  <p><b>作者</b>：Miruna Beţianu,  Abele Mălan,  Marco Aldinucci,  Robert Birke,  Lydia Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：domain adaptation, Large language models, Adaptation Large Language, increasingly serve, backbone for classifying</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) increasingly serve as the backbone for classifying text associated with distinct domains and simultaneously several labels (classes). When encountering domain shifts, e.g., classifier of movie reviews from IMDb to Rotten Tomatoes, adapting such an LLM-based multi-label classifier is challenging due to incomplete label sets at the target domain and daunting training overhead. The existing domain adaptation methods address either image multi-label classifiers or text binary classifiers. In this paper, we design DALLMi, Domain Adaptation Large Language Model interpolator, a first-of-its-kind semi-supervised domain adaptation method for text data models based on LLMs, specifically BERT. The core of DALLMi is the novel variation loss and MixUp regularization, which jointly leverage the limited positively labeled and large quantity of unlabeled text and, importantly, their interpolation from the BERT word embeddings. DALLMi also introduces a label-balanced sampling strategy to overcome the imbalance between labeled and unlabeled data. We evaluate DALLMi against the partial-supervised and unsupervised approach on three datasets under different scenarios of label availability for the target domain. Our results show that DALLMi achieves higher mAP than unsupervised and partially-supervised approaches by 19.9% and 52.2%, respectively.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Enhancing Bangla Language Next Word Prediction and Sentence Completion  through Extended RNN with Bi-LSTM Model On N-gram Language</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01873">https://arxiv.org/abs/2405.01873</a></p>
  <p><b>作者</b>：Md Robiul Islam,  Al Amin,  Aniqua Nusrat Zereen</p>
  <p><b>备注</b>：This paper contains 6 pages, 8 figures</p>
  <p><b>关键词</b>：Texting stands, communication worldwide, prominent form, form of communication, Bangla</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Texting stands out as the most prominent form of communication worldwide. Individual spend significant amount of time writing whole texts to send emails or write something on social media, which is time consuming in this modern era. Word prediction and sentence completion will be suitable and appropriate in the Bangla language to make textual information easier and more convenient. This paper expands the scope of Bangla language processing by introducing a Bi-LSTM model that effectively handles Bangla next-word prediction and Bangla sentence generation, demonstrating its versatility and potential impact. We proposed a new Bi-LSTM model to predict a following word and complete a sentence. We constructed a corpus dataset from various news portals, including bdnews24, BBC News Bangla, and Prothom Alo. The proposed approach achieved superior results in word prediction, reaching 99\% accuracy for both 4-gram and 5-gram word predictions. Moreover, it demonstrated significant improvement over existing methods, achieving 35\%, 75\%, and 95\% accuracy for uni-gram, bi-gram, and tri-gram word prediction, respectively</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Incorporating External Knowledge and Goal Guidance for LLM-based  Conversational Recommender Systems</b></summary>
  <p><b>编号</b>：[164]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01868">https://arxiv.org/abs/2405.01868</a></p>
  <p><b>作者</b>：Chuang Li,  Yang Deng,  Hengchang Hu,  Min-Yen Kan,  Haizhou Li</p>
  <p><b>备注</b>：Main paper 8 pages; References and Appendix 9 pages; 7 figures and 14 tables</p>
  <p><b>关键词</b>：conversational recommender system, efficiently enable large, large language models, enable large language, recommender system</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper aims to efficiently enable large language models (LLMs) to use external knowledge and goal guidance in conversational recommender system (CRS) tasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks for 1) generating grounded responses with recommendation-oriented knowledge, or 2) proactively leading the conversations through different dialogue goals. In this work, we first analyze those limitations through a comprehensive evaluation, showing the necessity of external knowledge and goal guidance which contribute significantly to the recommendation accuracy and language quality. In light of this finding, we propose a novel ChatCRS framework to decompose the complex CRS task into several sub-tasks through the implementation of 1) a knowledge retrieval agent using a tool-augmented approach to reason over external Knowledge Bases and 2) a goal-planning agent for dialogue goal prediction. Experimental results on two multi-goal CRS datasets reveal that ChatCRS sets new state-of-the-art benchmarks, improving language quality of informativeness by 17% and proactivity by 27%, and achieving a tenfold enhancement in recommendation accuracy.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：SUKHSANDESH: An Avatar Therapeutic Question Answering Platform for  Sexual Education in Rural India</b></summary>
  <p><b>编号</b>：[167]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01858">https://arxiv.org/abs/2405.01858</a></p>
  <p><b>作者</b>：Salam Michael Singh,  Shubhmoy Kumar Garg,  Amitesh Misra,  Aaditeshwar Seth,  Tanmoy Chakraborty</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Sexual education, terms of emotional, mental and social, social well-being, lifestyle in terms</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sexual education aims to foster a healthy lifestyle in terms of emotional, mental and social well-being. In countries like India, where adolescents form the largest demographic group, they face significant vulnerabilities concerning sexual health. Unfortunately, sexual education is often stigmatized, creating barriers to providing essential counseling and information to this at-risk population. Consequently, issues such as early pregnancy, unsafe abortions, sexually transmitted infections, and sexual violence become prevalent. Our current proposal aims to provide a safe and trustworthy platform for sexual education to the vulnerable rural Indian population, thereby fostering the healthy and overall growth of the nation. In this regard, we strive towards designing SUKHSANDESH, a multi-staged AI-based Question Answering platform for sexual education tailored to rural India, adhering to safety guardrails and regional language support. By utilizing information retrieval techniques and large language models, SUKHSANDESH will deliver effective responses to user queries. We also propose to anonymise the dataset to mitigate safety measures and set AI guardrails against any harmful or unwanted response generation. Moreover, an innovative feature of our proposal involves integrating ``avatar therapy'' with SUKHSANDESH. This feature will convert AI-generated responses into real-time audio delivered by an animated avatar speaking regional Indian languages. This approach aims to foster empathy and connection, which is particularly beneficial for individuals with limited literacy skills. Partnering with Gram Vaani, an industry leader, we will deploy SUKHSANDESH to address sexual education needs in rural India.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：SGHateCheck: Functional Tests for Detecting Hate Speech in Low-Resource  Languages of Singapore</b></summary>
  <p><b>编号</b>：[177]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01842">https://arxiv.org/abs/2405.01842</a></p>
  <p><b>作者</b>：Ri Chi Ng,  Nirmalendu Prakash,  Ming Shan Hee,  Kenny Tsu Wei Choo,  Roy Ka-Wei Lee</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Southeast Asia contexts, Southeast Asia, current hate speech, hate speech detection, Singapore and Southeast</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To address the limitations of current hate speech detection models, we introduce \textsf{SGHateCheck}, a novel framework designed for the linguistic and cultural context of Singapore and Southeast Asia. It extends the functional testing approach of HateCheck and MHC, employing large language models for translation and paraphrasing into Singapore's main languages, and refining these with native annotators. \textsf{SGHateCheck} reveals critical flaws in state-of-the-art models, highlighting their inadequacy in sensitive content moderation. This work aims to foster the development of more effective hate speech detection tools for diverse linguistic environments, particularly for Singapore and Southeast Asia contexts.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：SoftMCL: Soft Momentum Contrastive Learning for Fine-grained  Sentiment-aware Pre-training</b></summary>
  <p><b>编号</b>：[182]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01827">https://arxiv.org/abs/2405.01827</a></p>
  <p><b>作者</b>：Jin Wang,  Liang-Chih Yu,  Xuejie Zhang</p>
  <p><b>备注</b>：Accepted by LREC-COLING 2024</p>
  <p><b>关键词</b>：captures general language, general language understanding, language models captures, models captures general, general language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The pre-training for language models captures general language understanding but fails to distinguish the affective impact of a particular context to a specific word. Recent works have sought to introduce contrastive learning (CL) for sentiment-aware pre-training in acquiring affective information. Nevertheless, these methods present two significant limitations. First, the compatibility of the GPU memory often limits the number of negative samples, hindering the opportunities to learn good representations. In addition, using only a few sentiment polarities as hard labels, e.g., positive, neutral, and negative, to supervise CL will force all representations to converge to a few points, leading to the issue of latent space collapse. This study proposes a soft momentum contrastive learning (SoftMCL) for fine-grained sentiment-aware pre-training. Instead of hard labels, we introduce valence ratings as soft-label supervision for CL to fine-grained measure the sentiment similarities between samples. The proposed SoftMCL is conducted on both the word- and sentence-level to enhance the model's ability to learn affective information. A momentum queue was introduced to expand the contrastive samples, allowing storing and involving more negatives to overcome the limitations of hardware platforms. Extensive experiments were conducted on four different sentiment-related tasks, which demonstrates the effectiveness of the proposed SoftMCL method. The code and data of the proposed SoftMCL is available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders  and Identifying Distinct Features</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01799">https://arxiv.org/abs/2405.01799</a></p>
  <p><b>作者</b>：Chuanbo Hu,  Wenqi Li,  Mindi Ruan,  Xiangxu Yu,  Lynn K. Paul,  Shuo Wang,  Xin Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional assessment methods, nuanced challenge, complex and nuanced, subjective nature, nature and variability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diagnosing language disorders associated with autism is a complex and nuanced challenge, often hindered by the subjective nature and variability of traditional assessment methods. Traditional diagnostic methods not only require intensive human effort but also often result in delayed interventions due to their lack of speed and specificity. In this study, we explored the application of ChatGPT, a state of the art large language model, to overcome these obstacles by enhancing diagnostic accuracy and profiling specific linguistic features indicative of autism. Leveraging ChatGPT advanced natural language processing capabilities, this research aims to streamline and refine the diagnostic process. Specifically, we compared ChatGPT's performance with that of conventional supervised learning models, including BERT, a model acclaimed for its effectiveness in various natural language processing tasks. We showed that ChatGPT substantially outperformed these models, achieving over 13% improvement in both accuracy and F1 score in a zero shot learning configuration. This marked enhancement highlights the model potential as a superior tool for neurological diagnostics. Additionally, we identified ten distinct features of autism associated language disorders that vary significantly across different experimental scenarios. These features, which included echolalia, pronoun reversal, and atypical language usage, were crucial for accurately diagnosing ASD and customizing treatment plans. Together, our findings advocate for adopting sophisticated AI tools like ChatGPT in clinical settings to assess and diagnose developmental disorders. Our approach not only promises greater diagnostic precision but also aligns with the goals of personalized medicine, potentially transforming the evaluation landscape for autism and similar neurological conditions.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：TOPICAL: TOPIC Pages AutomagicaLly</b></summary>
  <p><b>编号</b>：[200]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01796">https://arxiv.org/abs/2405.01796</a></p>
  <p><b>作者</b>：John Giorgi,  Amanpreet Singh,  Doug Downey,  Sergey Feldman,  Lucy Lu Wang</p>
  <p><b>备注</b>：10 pages, 7 figures, 2 tables, NAACL System Demonstrations 2024</p>
  <p><b>关键词</b>：Topic pages, Topic pages aggregate, accessible article, single succinct, succinct and accessible</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Topic pages aggregate useful information about an entity or concept into a single succinct and accessible article. Automated creation of topic pages would enable their rapid curation as information resources, providing an alternative to traditional web search. While most prior work has focused on generating topic pages about biographical entities, in this work, we develop a completely automated process to generate high-quality topic pages for scientific entities, with a focus on biomedical concepts. We release TOPICAL, a web app and associated open-source code, comprising a model pipeline combining retrieval, clustering, and prompting, that makes it easy for anyone to generate topic pages for a wide variety of biomedical entities on demand. In a human evaluation of 150 diverse topic pages generated using TOPICAL, we find that the vast majority were considered relevant, accurate, and coherent, with correct supporting citations. We make all code publicly available and host a free-to-use web app at: this https URL</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Understanding Position Bias Effects on Fairness in Social Multi-Document  Summarization</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01790">https://arxiv.org/abs/2405.01790</a></p>
  <p><b>作者</b>：Olubusayo Olabisi,  Ameeta Agrawal</p>
  <p><b>备注</b>：Accepted at VarDial 2024</p>
  <p><b>关键词</b>：Text summarization models, summarization models, social multi-document summarization, typically focused, focused on optimizing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text summarization models have typically focused on optimizing aspects of quality such as fluency, relevance, and coherence, particularly in the context of news articles. However, summarization models are increasingly being used to summarize diverse sources of text, such as social media data, that encompass a wide demographic user base. It is thus crucial to assess not only the quality of the generated summaries, but also the extent to which they can fairly represent the opinions of diverse social groups. Position bias, a long-known issue in news summarization, has received limited attention in the context of social multi-document summarization. We deeply investigate this phenomenon by analyzing the effect of group ordering in input documents when summarizing tweets from three distinct linguistic communities: African-American English, Hispanic-aligned Language, and White-aligned Language. Our empirical analysis shows that although the textual quality of the summaries remains consistent regardless of the input document order, in terms of fairness, the results vary significantly depending on how the dialect groups are presented in the input data. Our results suggest that position bias manifests differently in social multi-document summarization, severely impacting the fairness of summarization models.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Layers of technology in pluriversal design. Decolonising language  technology with the LiveLanguage initiative</b></summary>
  <p><b>编号</b>：[208]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01783">https://arxiv.org/abs/2405.01783</a></p>
  <p><b>作者</b>：Gertraud Koch,  Gábor Bella,  Paula Helm,  Fausto Giunchiglia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：facilitate intercultural communication, Language technology, meaningful translations, potential to facilitate, facilitate intercultural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Language technology has the potential to facilitate intercultural communication through meaningful translations. However, the current state of language technology is deeply entangled with colonial knowledge due to path dependencies and neo-colonial tendencies in the global governance of artificial intelligence (AI). Language technology is a complex and emerging field that presents challenges for co-design interventions due to enfolding in assemblages of global scale and diverse sites and its knowledge intensity. This paper uses LiveLanguage, a lexical database, a set of services with particular emphasis on modelling language diversity and integrating small and minority languages, as an example to discuss and close the gap from pluriversal design theory to practice. By diversifying the concept of emerging technology, we can better approach language technology in global contexts. The paper presents a model comprising of five layers of technological activity. Each layer consists of specific practices and stakeholders, thus provides distinctive spaces for co-design interventions as mode of inquiry for de-linking, re-thinking and re-building language technology towards pluriversality. In that way, the paper contributes to reflecting the position of co-design in decolonising emergent technologies, and to integrating complex theoretical knowledge towards decoloniality into language technology design.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：A Survey on Large Language Models for Critical Societal Domains:  Finance, Healthcare, and Law</b></summary>
  <p><b>编号</b>：[215]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01769">https://arxiv.org/abs/2405.01769</a></p>
  <p><b>作者</b>：Zhiyu Zoey Chen,  Jing Ma,  Xinlu Zhang,  Nan Hao,  An Yan,  Armineh Nourbakhsh,  Xianjun Yang,  Julian McAuley,  Linda Petzold,  William Yang Wang</p>
  <p><b>备注</b>：35 pages, 6 figures</p>
  <p><b>关键词</b>：large language models, challenging data acquisition, stringent regulatory compliance, artificial intelligence, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the fast-evolving domain of artificial intelligence, large language models (LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance, healthcare, and law: domains characterized by their reliance on professional expertise, challenging data acquisition, high-stakes, and stringent regulatory compliance. This survey offers a detailed exploration of the methodologies, applications, challenges, and forward-looking opportunities of LLMs within these high-stakes sectors. We highlight the instrumental role of LLMs in enhancing diagnostic and treatment methodologies in healthcare, innovating financial analytics, and refining legal interpretation and compliance strategies. Moreover, we critically examine the ethics for LLM applications in these fields, pointing out the existing ethical concerns and the need for transparent, fair, and robust AI systems that respect regulatory norms. By presenting a thorough review of current literature and practical applications, we showcase the transformative impact of LLMs, and outline the imperative for interdisciplinary cooperation, methodological advancements, and ethical vigilance. Through this lens, we aim to spark dialogue and inspire future research dedicated to maximizing the benefits of LLMs while mitigating their risks in these precision-dependent sectors. To facilitate future research on LLMs in these critical societal domains, we also initiate a reading list that tracks the latest advancements under this topic, which will be continually updated: \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：CoS: Enhancing Personalization and Mitigating Bias with Context Steering</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01768">https://arxiv.org/abs/2405.01768</a></p>
  <p><b>作者</b>：Jerry Zhi-Yang He,  Sashrika Pandey,  Mariah L. Schrum,  Anca Dragan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language model, cultural information specific, querying a large, large language, cultural information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When querying a large language model (LLM), the context, i.e. personal, demographic, and cultural information specific to an end-user, can significantly shape the response of the LLM. For example, asking the model to explain Newton's second law with the context "I am a toddler" yields a different answer compared to the context "I am a physics professor." Proper usage of the context enables the LLM to generate personalized responses, whereas inappropriate contextual influence can lead to stereotypical and potentially harmful generations (e.g. associating "female" with "housekeeper"). In practice, striking the right balance when leveraging context is a nuanced and challenging problem that is often situation-dependent. One common approach to address this challenge is to fine-tune LLMs on contextually appropriate responses. However, this approach is expensive, time-consuming, and not controllable for end-users in different situations. In this work, we propose Context Steering (CoS) - a simple training-free method that can be easily applied to autoregressive LLMs at inference time. By measuring the contextual influence in terms of token prediction likelihood and modulating it, our method enables practitioners to determine the appropriate level of contextual influence based on their specific use case and end-user base. We showcase a variety of applications of CoS including amplifying the contextual influence to achieve better personalization and mitigating unwanted influence for reducing model bias. In addition, we show that we can combine CoS with Bayesian Inference to quantify the extent of hate speech on the internet. We demonstrate the effectiveness of CoS on state-of-the-art LLMs and benchmarks.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：ALCM: Autonomous LLM-Augmented Causal Discovery Framework</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01744">https://arxiv.org/abs/2405.01744</a></p>
  <p><b>作者</b>：Elahe Khatibi,  Mahyar Abbasian,  Zhongqi Yang,  Iman Azimi,  Amir M. Rahmani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：perform effective causal, causal, Large Language Models, causal discovery, effective causal inference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP-hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：The Psychosocial Impacts of Generative AI Harms</b></summary>
  <p><b>编号</b>：[228]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01740">https://arxiv.org/abs/2405.01740</a></p>
  <p><b>作者</b>：Faye-Marie Vassel,  Evan Shieh,  Cassidy R. Sugimoto,  Thema Monroe-White</p>
  <p><b>备注</b>：Presented in Impact of GenAI on Social and Individual Well-being at AAAI 2024 Spring Symposium Series (2024)</p>
  <p><b>关键词</b>：generative Language Models, Language Models, generative Language, rapid emergence, led to growing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid emergence of generative Language Models (LMs) has led to growing concern about the impacts that their unexamined adoption may have on the social well-being of diverse user groups. Meanwhile, LMs are increasingly being adopted in K-20 schools and one-on-one student settings with minimal investigation of potential harms associated with their deployment. Motivated in part by real-world/everyday use cases (e.g., an AI writing assistant) this paper explores the potential psychosocial harms of stories generated by five leading LMs in response to open-ended prompting. We extend findings of stereotyping harms analyzing a total of 150K 100-word stories related to student classroom interactions. Examining patterns in LM-generated character demographics and representational harms (i.e., erasure, subordination, and stereotyping) we highlight particularly egregious vignettes, illustrating the ways LM-generated outputs may influence the experiences of users with marginalized and minoritized identities, and emphasizing the need for a critical understanding of the psychosocial impacts of generative AI tools when deployed and utilized in diverse social contexts.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Question Suggestion for Conversational Shopping Assistants Using Product  Metadata</b></summary>
  <p><b>编号</b>：[230]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01738">https://arxiv.org/abs/2405.01738</a></p>
  <p><b>作者</b>：Nikhita Vedula,  Oleg Rokhlenko,  Shervin Malmasi</p>
  <p><b>备注</b>：5 pages, 1 figure</p>
  <p><b>关键词</b>：Generative Artificial Intelligence, Natural Language Processing, Information Retrieval, Artificial Intelligence, Generative Artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Digital assistants have become ubiquitous in e-commerce applications, following the recent advancements in Information Retrieval (IR), Natural Language Processing (NLP) and Generative Artificial Intelligence (AI). However, customers are often unsure or unaware of how to effectively converse with these assistants to meet their shopping needs. In this work, we emphasize the importance of providing customers a fast, easy to use, and natural way to interact with conversational shopping assistants. We propose a framework that employs Large Language Models (LLMs) to automatically generate contextual, useful, answerable, fluent and diverse questions about products, via in-context learning and supervised fine-tuning. Recommending these questions to customers as helpful suggestions or hints to both start and continue a conversation can result in a smoother and faster shopping experience with reduced conversation overhead and friction. We perform extensive offline evaluations, and discuss in detail about potential customer impact, and the type, length and latency of our generated product questions if incorporated into a real-world shopping assistant.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Large Language Models are Inconsistent and Biased Evaluators</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01724">https://arxiv.org/abs/2405.01724</a></p>
  <p><b>作者</b>：Rickard Stureborg,  Dimitris Alikaniotis,  Yoshi Suhara</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, enabled highly flexible, tools in NLP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low "inter-sample" agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Automatically Extracting Numerical Results from Randomized Controlled  Trials with Large Language Models</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01686">https://arxiv.org/abs/2405.01686</a></p>
  <p><b>作者</b>：Hye Sun Yun,  David Pogrebitskiy,  Iain J. Marshall,  Byron C. Wallace</p>
  <p><b>备注</b>：24 pages, 7 figures, 6 tables</p>
  <p><b>关键词</b>：assess treatment effectiveness, Meta-analyses statistically aggregate, randomized controlled trials, treatment effectiveness, statistically aggregate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Meta-analyses statistically aggregate the findings of different randomized controlled trials (RCTs) to assess treatment effectiveness. Because this yields robust estimates of treatment effectiveness, results from meta-analyses are considered the strongest form of evidence. However, rigorous evidence syntheses are time-consuming and labor-intensive, requiring manual extraction of data from individual trials to be synthesized. Ideally, language technologies would permit fully automatic meta-analysis, on demand. This requires accurately extracting numerical results from individual trials, which has been beyond the capabilities of natural language processing (NLP) models to date. In this work, we evaluate whether modern large language models (LLMs) can reliably perform this task. We annotate (and release) a modest but granular evaluation dataset of clinical trial reports with numerical findings attached to interventions, comparators, and outcomes. Using this dataset, we evaluate the performance of seven LLMs applied zero-shot for the task of conditionally extracting numerical findings from trial reports. We find that massive LLMs that can accommodate lengthy inputs are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality). However, LLMs -- including ones trained on biomedical texts -- perform poorly when the outcome measures are complex and tallying the results requires inference. This work charts a path toward fully automatic meta-analysis of RCTs via LLMs, while also highlighting the limitations of existing models for this aim.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Leveraging Prompt-Learning for Structured Information Extraction from  Crohn's Disease Radiology Reports in a Low-Resource Language</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01682">https://arxiv.org/abs/2405.01682</a></p>
  <p><b>作者</b>：Liam Hazan,  Gili Focht,  Naama Gavrielov,  Roi Reichart,  Talar Hagopian,  Mary-Louise C. Greer,  Ruth Cytter Kuint,  Dan Turner,  Moti Freiman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Processing, Language Processing, Natural Language, Automatic conversion, techniques is crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic conversion of free-text radiology reports into structured data using Natural Language Processing (NLP) techniques is crucial for analyzing diseases on a large scale. While effective for tasks in widely spoken languages like English, generative large language models (LLMs) typically underperform with less common languages and can pose potential risks to patient privacy. Fine-tuning local NLP models is hindered by the skewed nature of real-world medical datasets, where rare findings represent a significant data imbalance. We introduce SMP-BERT, a novel prompt learning method that leverages the structured nature of reports to overcome these challenges. In our studies involving a substantial collection of Crohn's disease radiology reports in Hebrew (over 8,000 patients and 10,000 reports), SMP-BERT greatly surpassed traditional fine-tuning methods in performance, notably in detecting infrequent conditions (AUC: 0.99 vs 0.94, F1: 0.84 vs 0.34). SMP-BERT empowers more accurate AI diagnostics available for low-resource languages.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：1-Diffractor: Efficient and Utility-Preserving Text Obfuscation  Leveraging Word-Level Metric Differential Privacy</b></summary>
  <p><b>编号</b>：[266]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01678">https://arxiv.org/abs/2405.01678</a></p>
  <p><b>作者</b>：Stephen Meisenbacher,  Maulik Chevli,  Florian Matthes</p>
  <p><b>备注</b>：12 pages, 7 figures, 7 tables, 10th ACM International Workshop on Security and Privacy Analytics (IWSPA 2024)</p>
  <p><b>关键词</b>：Natural Language Processing, privacy-preserving Natural Language, Language Processing, Natural Language, gained rising attention</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The study of privacy-preserving Natural Language Processing (NLP) has gained rising attention in recent years. One promising avenue studies the integration of Differential Privacy in NLP, which has brought about innovative methods in a variety of application settings. Of particular note are $\textit{word-level Metric Local Differential Privacy (MLDP)}$ mechanisms, which work to obfuscate potentially sensitive input text by performing word-by-word $\textit{perturbations}$. Although these methods have shown promising results in empirical tests, there are two major drawbacks: (1) the inevitable loss of utility due to addition of noise, and (2) the computational expensiveness of running these mechanisms on high-dimensional word embeddings. In this work, we aim to address these challenges by proposing $\texttt{1-Diffractor}$, a new mechanism that boasts high speedups in comparison to previous mechanisms, while still demonstrating strong utility- and privacy-preserving capabilities. We evaluate $\texttt{1-Diffractor}$ for utility on several NLP tasks, for theoretical and task-based privacy, and for efficiency in terms of speed and memory. $\texttt{1-Diffractor}$ shows significant improvements in efficiency, while still maintaining competitive utility and privacy scores across all conducted comparative tests against previous MLDP mechanisms. Our code is made available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Investigating Wit, Creativity, and Detectability of Large Language  Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01660">https://arxiv.org/abs/2405.01660</a></p>
  <p><b>作者</b>：Tolga Buz,  Benjamin Frost,  Nikola Genchev,  Moritz Schneider,  Lucie-Aimée Kaffee,  Gerard de Melo</p>
  <p><b>备注</b>：Accepted to *SEM 2024 (StarSEM) conference</p>
  <p><b>关键词</b>：Large Language Models, Recent Large Language, Language Models, Large Language, Recent Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent Large Language Models (LLMs) have shown the ability to generate content that is difficult or impossible to distinguish from human writing. We investigate the ability of differently-sized LLMs to replicate human writing style in short, creative texts in the domain of Showerthoughts, thoughts that may occur during mundane activities. We compare GPT-2 and GPT-Neo fine-tuned on Reddit data as well as GPT-3.5 invoked in a zero-shot manner, against human-authored texts. We measure human preference on the texts across the specific dimensions that account for the quality of creative, witty texts. Additionally, we compare the ability of humans versus fine-tuned RoBERTa classifiers to detect AI-generated texts. We conclude that human evaluators rate the generated texts slightly worse on average regarding their creative quality, but they are unable to reliably distinguish between human-written and AI-generated texts. We further provide a dataset for creative, witty text generation based on Reddit Showerthoughts posts.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Improving Complex Reasoning over Knowledge Graph with Logic-Aware  Curriculum Tuning</b></summary>
  <p><b>编号</b>：[279]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01649">https://arxiv.org/abs/2405.01649</a></p>
  <p><b>作者</b>：Tianle Xia,  Liang Ding,  Guojia Wan,  Yibing Zhan,  Bo Du,  Dacheng Tao</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2305.01157, arXiv:2212.09567 by other authors</p>
  <p><b>关键词</b>：Answering complex logical, incomplete knowledge graphs, Answering complex, logical, incomplete knowledge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Answering complex logical queries over incomplete knowledge graphs (KGs) is challenging. Most previous works have focused on learning entity/relation embeddings and simulating first-order logic operators with various neural networks. However, they are bottlenecked by the inability to share world knowledge to improve logical reasoning, thus resulting in suboptimal performance. In this paper, we propose a complex logical reasoning schema over knowledge graphs upon large language models (LLMs), containing a curriculum-based logical-aware instruction tuning framework, named LACT. Specifically, we augment the arbitrary first-order logical queries via binary tree decomposition, to stimulate the reasoning capability of LLMs. To address the difficulty gap among different types of complex queries, we design a simple and flexible logic-aware curriculum learning framework. Experiments across widely used datasets demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR score) over advanced methods, achieving the new state-of-the-art. Our code and model will be released at GitHub and huggingface soon.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Automating the Analysis of Public Saliency and Attitudes towards  Biodiversity from Digital Media</b></summary>
  <p><b>编号</b>：[289]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01610">https://arxiv.org/abs/2405.01610</a></p>
  <p><b>作者</b>：Noah Giebink,  Amrita Gupta,  Diogo Verìssimo,  Charlotte H. Chang,  Tony Chang,  Angela Brennan,  Brett Dickson,  Alex Bowmer,  Jonathan Baillie</p>
  <p><b>备注</b>：v0.1, 21 pages with 10 figures</p>
  <p><b>关键词</b>：Biodiversity Framework targets, Global Biodiversity Framework, Framework targets, Measuring public attitudes, Biodiversity Framework</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Measuring public attitudes toward wildlife provides crucial insights into our relationship with nature and helps monitor progress toward Global Biodiversity Framework targets. Yet, conducting such assessments at a global scale is challenging. Manually curating search terms for querying news and social media is tedious, costly, and can lead to biased results. Raw news and social media data returned from queries are often cluttered with irrelevant content and syndicated articles. We aim to overcome these challenges by leveraging modern Natural Language Processing (NLP) tools. We introduce a folk taxonomy approach for improved search term generation and employ cosine similarity on Term Frequency-Inverse Document Frequency vectors to filter syndicated articles. We also introduce an extensible relevance filtering pipeline which uses unsupervised learning to reveal common topics, followed by an open-source zero-shot Large Language Model (LLM) to assign topics to news article titles, which are then used to assign relevance. Finally, we conduct sentiment, topic, and volume analyses on resulting data. We illustrate our methodology with a case study of news and X (formerly Twitter) data before and during the COVID-19 pandemic for various mammal taxa, including bats, pangolins, elephants, and gorillas. During the data collection period, up to 62% of articles including keywords pertaining to bats were deemed irrelevant to biodiversity, underscoring the importance of relevance filtering. At the pandemic's onset, we observed increased volume and a significant sentiment shift toward horseshoe bats, which were implicated in the pandemic, but not for other focal taxa. The proposed methods open the door to conservation practitioners applying modern and emerging NLP tools, including LLMs "out of the box," to analyze public perceptions of biodiversity during current events or campaigns.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Efficient Sample-Specific Encoder Perturbations</b></summary>
  <p><b>编号</b>：[294]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01601">https://arxiv.org/abs/2405.01601</a></p>
  <p><b>作者</b>：Yassir Fathullah,  Mark J. F. Gales</p>
  <p><b>备注</b>：To appear in NAACL 2024</p>
  <p><b>关键词</b>：autoregressive sequence tasks, sequence tasks, range of autoregressive, autoregressive sequence, attribute of interest</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Encoder-decoder foundation models have displayed state-of-the-art performance on a range of autoregressive sequence tasks. This paper proposes a simple and lightweight modification to such systems to control the behaviour according to a specific attribute of interest. This paper proposes a novel inference-efficient approach to modifying the behaviour of an encoder-decoder system according to a specific attribute of interest. Specifically, we show that a small proxy network can be used to find a sample-by-sample perturbation of the encoder output of a frozen foundation model to trigger the decoder to generate improved decodings. This work explores a specific realization of this framework focused on improving the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper foundation models on Speech Recognition. Results display consistent improvements in performance evaluated through COMET and WER respectively. Furthermore, experiments also show that the proxies are robust to the exact nature of the data used to train them and can extend to other domains.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Improving Disease Detection from Social Media Text via Self-Augmentation  and Contrastive Learning</b></summary>
  <p><b>编号</b>：[296]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01597">https://arxiv.org/abs/2405.01597</a></p>
  <p><b>作者</b>：Pervaiz Iqbal Khan,  Andreas Dengel,  Sheraz Ahmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：public health monitoring, disease spread detection, diverse applications, spread detection, public health</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting diseases from social media has diverse applications, such as public health monitoring and disease spread detection. While language models (LMs) have shown promising performance in this domain, there remains ongoing research aimed at refining their discriminating representations. In this paper, we propose a novel method that integrates Contrastive Learning (CL) with language modeling to address this challenge. Our approach introduces a self-augmentation method, wherein hidden representations of the model are augmented with their own representations. This method comprises two branches: the first branch, a traditional LM, learns features specific to the given data, while the second branch incorporates augmented representations from the first branch to encourage generalization. CL further refines these representations by pulling pairs of original and augmented versions closer while pushing other samples away. We evaluate our method on three NLP datasets encompassing binary, multi-label, and multi-class classification tasks involving social media posts related to various diseases. Our approach demonstrates notable improvements over traditional fine-tuning methods, achieving up to a 2.48% increase in F1-score compared to baseline approaches and a 2.1% enhancement over state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Large Language Model Agent for Fake News Detection</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01593">https://arxiv.org/abs/2405.01593</a></p>
  <p><b>作者</b>：Xinyi Li,  Yongfeng Zhang,  Edward C. Malthouse</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：current digital era, influencing critical decision, online platforms presents, platforms presents significant, critical decision making</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Text and Audio Simplification: Human vs. ChatGPT</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01592">https://arxiv.org/abs/2405.01592</a></p>
  <p><b>作者</b>：Gondy Leroy,  David Kauchak,  Philip Harber,  Ankit Pal,  Akash Shukla</p>
  <p><b>备注</b>：AMIA Summit, Boston, 2024</p>
  <p><b>关键词</b>：increase information comprehension, important in healthcare, increase information, information comprehension, comprehension are important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text and audio simplification to increase information comprehension are important in healthcare. With the introduction of ChatGPT, an evaluation of its simplification performance is needed. We provide a systematic comparison of human and ChatGPT simplified texts using fourteen metrics indicative of text difficulty. We briefly introduce our online editor where these simplification tools, including ChatGPT, are available. We scored twelve corpora using our metrics: six text, one audio, and five ChatGPT simplified corpora. We then compare these corpora with texts simplified and verified in a prior user study. Finally, a medical domain expert evaluated these texts and five, new ChatGPT simplified versions. We found that simple corpora show higher similarity with the human simplified texts. ChatGPT simplification moves metrics in the right direction. The medical domain expert evaluation showed a preference for the ChatGPT style, but the text itself was rated lower for content retention.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in  Radiology with General-Domain Large Language Model</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01591">https://arxiv.org/abs/2405.01591</a></p>
  <p><b>作者</b>：Seonhee Cho,  Choonghan Kim,  Jiho Lee,  Chetan Chilkunda,  Sujin Choi,  Joo Heung Yoon</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：Large Language Model, Large Multimodal Models, Recent advancements, attracted interest, generalization capability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data pose unique challenges for model training and application. However, the dependency on high-quality data for effective in-context learning raises questions about the feasibility of these models when encountering with the inevitable variations and errors inherent in real-world medical data. In this paper, we introduce MID-M, a novel framework that leverages the in-context learning capabilities of a general-domain Large Language Model (LLM) to process multimodal data via image descriptions. MID-M achieves a comparable or superior performance to task-specific fine-tuned LMMs and other general-domain ones, without the extensive domain-specific training or pre-training on multimodal data, with significantly fewer parameters. This highlights the potential of leveraging general-domain LLMs for domain-specific tasks and offers a sustainable and cost-effective alternative to traditional LMM developments. Moreover, the robustness of MID-M against data quality issues demonstrates its practical utility in real-world medical domain applications.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：101 Billion Arabic Words Dataset</b></summary>
  <p><b>编号</b>：[300]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01590">https://arxiv.org/abs/2405.01590</a></p>
  <p><b>作者</b>：Manel Aloui,  Hasna Chouikhi,  Ghaith Chaabane,  Haithem Kchaou,  Chehir Dhaouadi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Arabic Language Models, impressive rise predominantly, natural language processing, Arabic language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, Large Language Models have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic LLMs capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic LLMs, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue -the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale data mining project, extracting a substantial volume of text from the Common Crawl WET files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic LLMs. This study not only highlights the potential for creating linguistically and culturally accurate Arabic LLMs but also sets a precedent for future research in enhancing the authenticity of Arabic language models.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：GPT-4 passes most of the 297 written Polish Board Certification  Examinations</b></summary>
  <p><b>编号</b>：[301]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01589">https://arxiv.org/abs/2405.01589</a></p>
  <p><b>作者</b>：Jakub Pokrywka,  Jeremi Kaczmarek,  Edward Gorzelańczyk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, Polish Board Certification, Państwowy Egzamin Specjalizacyjny</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Introduction: Recently, the effectiveness of Large Language Models (LLMs) has increased rapidly, allowing them to be used in a great number of applications. However, the risks posed by the generation of false information through LLMs significantly limit their applications in sensitive areas such as healthcare, highlighting the necessity for rigorous validations to determine their utility and reliability. To date, no study has extensively compared the performance of LLMs on Polish medical examinations across a broad spectrum of specialties on a very large dataset. Objectives: This study evaluated the performance of three Generative Pretrained Transformer (GPT) models on the Polish Board Certification Exam (Państwowy Egzamin Specjalizacyjny, PES) dataset, which consists of 297 tests. Methods: We developed a software program to download and process PES exams and tested the performance of GPT models using OpenAI Application Programming Interface. Results: Our findings reveal that GPT-3.5 did not pass any of the analyzed exams. In contrast, the GPT-4 models demonstrated the capability to pass the majority of the exams evaluated, with the most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The performance of the GPT models varied significantly, displaying excellence in exams related to certain specialties while completely failing others. Conclusions: The significant progress and impressive performance of LLM models hold great promise for the increased application of AI in the field of medicine in Poland. For instance, this advancement could lead to the development of AI-based medical assistants for healthcare professionals, enhancing the efficiency and accuracy of medical services.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Towards Unbiased Evaluation of Detecting Unanswerable Questions in  EHRSQL</b></summary>
  <p><b>编号</b>：[302]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01588">https://arxiv.org/abs/2405.01588</a></p>
  <p><b>作者</b>：Yongjin Yang,  Sihyeon Kim,  SangMook Kim,  Gyubok Lee,  Se-Young Yun,  Edward Choi</p>
  <p><b>备注</b>：DPFM Workshop, ICLR 2024</p>
  <p><b>关键词</b>：providing non-existent responses, Incorporating unanswerable questions, unanswerable questions, crucial for testing, testing the trustworthiness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Incorporating unanswerable questions into EHR QA systems is crucial for testing the trustworthiness of a system, as providing non-existent responses can mislead doctors in their diagnoses. The EHRSQL dataset stands out as a promising benchmark because it is the only dataset that incorporates unanswerable questions in the EHR QA system alongside practical questions. However, in this work, we identify a data bias in these unanswerable questions; they can often be discerned simply by filtering with specific N-gram patterns. Such biases jeopardize the authenticity and reliability of QA system evaluations. To tackle this problem, we propose a simple debiasing method of adjusting the split between the validation and test sets to neutralize the undue influence of N-gram filtering. By experimenting on the MIMIC-III dataset, we demonstrate both the existing data bias in EHRSQL and the effectiveness of our data split strategy in mitigating this bias.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Improve Academic Query Resolution through BERT-based Question Extraction  from Images</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01587">https://arxiv.org/abs/2405.01587</a></p>
  <p><b>作者</b>：Nidhi Kamal,  Saurabh Yadav,  Jorawar Singh,  Aditi Avasthi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Providing fast, essential solution provided, fast and accurate, Edtech organizations, essential solution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：Transfer Learning and Transformer Architecture for Financial Sentiment  Analysis</b></summary>
  <p><b>编号</b>：[304]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01586">https://arxiv.org/abs/2405.01586</a></p>
  <p><b>作者</b>：Tohida Rehman,  Raghubir Bose,  Samiran Chattopadhyay,  Debarshi Kumar Sanyal</p>
  <p><b>备注</b>：12 pages, 9 figures</p>
  <p><b>关键词</b>：Banks and Insurance, Insurance Companies, institutions like Banks, sentiment analysis, manage the credit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Financial sentiment analysis allows financial institutions like Banks and Insurance Companies to better manage the credit scoring of their customers in a better way. Financial domain uses specialized mechanisms which makes sentiment analysis difficult. In this paper, we propose a pre-trained language model which can help to solve this problem with fewer labelled data. We extend on the principles of Transfer learning and Transformation architecture principles and also take into consideration recent outbreak of pandemics like COVID. We apply the sentiment analysis to two different sets of data. We also take smaller training set and fine tune the same as part of the model.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular  RAG Applications</b></summary>
  <p><b>编号</b>：[305]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01585">https://arxiv.org/abs/2405.01585</a></p>
  <p><b>作者</b>：Sujit Khanna,  Shishir Subedi</p>
  <p><b>备注</b>：11 pages, 5 figures</p>
  <p><b>关键词</b>：times Large Language, Large Language Models, exhibited tremendous capabilities, recent times Large, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent times Large Language Models have exhibited tremendous capabilities, especially in the areas of mathematics, code generation and general-purpose reasoning. However for specialized domains especially in applications that require parsing and analyzing large chunks of numeric or tabular data even state-of-the-art (SOTA) models struggle. In this paper, we introduce a new approach to solving domain-specific tabular data analysis tasks by presenting a unique RAG workflow that mitigates the scalability issues of existing tabular LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel approach to fine-tune embedding models for tabular Retrieval-Augmentation Generation (RAG) applications. Embedding models form a crucial component in the RAG workflow and even current SOTA embedding models struggle as they are predominantly trained on textual datasets and thus underperform in scenarios involving complex tabular data. The evaluation results showcase that our approach not only outperforms current SOTA embedding models in this domain but also does so with a notably smaller and more efficient model structure.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Lightweight Conceptual Dictionary Learning for Text Classification Using  Information Compression</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01584">https://arxiv.org/abs/2405.01584</a></p>
  <p><b>作者</b>：Li Wan,  Tansu Alpcan,  Margreta Kuijper,  Emanuele Viterbo</p>
  <p><b>备注</b>：12 pages, TKDE format</p>
  <p><b>关键词</b>：lightweight supervised dictionary, supervised dictionary learning, dictionary learning framework, lightweight supervised, text classification based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel, lightweight supervised dictionary learning framework for text classification based on data compression and representation. This two-phase algorithm initially employs the Lempel-Ziv-Welch (LZW) algorithm to construct a dictionary from text datasets, focusing on the conceptual significance of dictionary elements. Subsequently, dictionaries are refined considering label data, optimizing dictionary atoms to enhance discriminative power based on mutual information and class distribution. This process generates discriminative numerical representations, facilitating the training of simple classifiers such as SVMs and neural networks. We evaluate our algorithm's information-theoretic performance using information bottleneck principles and introduce the information plane area rank (IPAR) as a novel metric to quantify the information-theoretic performance. Tested on six benchmark text datasets, our algorithm competes closely with top models, especially in limited-vocabulary contexts, using significantly fewer parameters. \review{Our algorithm closely matches top-performing models, deviating by only ~2\% on limited-vocabulary datasets, using just 10\% of their parameters. However, it falls short on diverse-vocabulary datasets, likely due to the LZW algorithm's constraints with low-repetition data. This contrast highlights its efficiency and limitations across different dataset types.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology  with Multimodal Learning</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01583">https://arxiv.org/abs/2405.01583</a></p>
  <p><b>作者</b>：Nadia Saeed</p>
  <p><b>备注</b>：7 pages, 3 figures, Clinical NLP 2024 workshop proceedings in Shared Task</p>
  <p><b>关键词</b>：wai Yim, challenge necessitates, necessitates novel solutions, Multimodal Medical Answer, Yim</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual & Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a). This paper addresses the limitations of traditional methods by proposing a weakly supervised learning approach for open-ended medical question-answering (QA). Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations. Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion. This approach tackles complex, open-ended questions even without predefined answer choices. We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images. This work advances medical QA research, paving the way for clinical decision support systems and ultimately improving healthcare delivery.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Text Quality-Based Pruning for Efficient Training of Language Models</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01582">https://arxiv.org/abs/2405.01582</a></p>
  <p><b>作者</b>：Vasu Sharma,  Karthik Padthe,  Newsha Ardalani,  Kushal Tirumala,  Russell Howes,  Hu Xu,  Po-Yao Huang,  Shang-Wen Li,  Armen Aghajanyan,  Gargi Ghosh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：process extremely laborious, times training Language, training Language Models, training process extremely, recent times training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text quality in large unlabelled NLP datasets in a model agnostic manner to assign the text instances a "quality score".
By proposing the text quality metric, the paper establishes a framework to identify and eliminate low-quality text instances, leading to improved training efficiency for LM models. Experimental results over multiple models and datasets demonstrate the efficacy of this approach, showcasing substantial gains in training effectiveness and highlighting the potential for resource-efficient LM training.
For example, we observe an absolute accuracy improvement of 0.9% averaged over 14 downstream evaluation tasks for multiple LM models while using 40% lesser data and training 42% faster when training on the OpenWebText dataset and 0.8% average absolute accuracy improvement while using 20% lesser data and training 21% faster on the Wikipedia dataset.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：The Mercurial Top-Level Ontology of Large Language Models</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01581">https://arxiv.org/abs/2405.01581</a></p>
  <p><b>作者</b>：Nele Köhler,  Fabian Neuhaus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, analyze implicit ontological, language models, case study, ontological commitments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In our work, we systematize and analyze implicit ontological commitments in the responses generated by large language models (LLMs), focusing on ChatGPT 3.5 as a case study. We investigate how LLMs, despite having no explicit ontology, exhibit implicit ontological categorizations that are reflected in the texts they generate. The paper proposes an approach to understanding the ontological commitments of LLMs by defining ontology as a theory that provides a systematic account of the ontological commitments of some text. We investigate the ontological assumptions of ChatGPT and present a systematized account, i.e., GPT's top-level ontology. This includes a taxonomy, which is available as an OWL file, as well as a discussion about ontological assumptions (e.g., about its mereology or presentism). We show that in some aspects GPT's top-level ontology is quite similar to existing top-level ontologies. However, there are significant challenges arising from the flexible nature of LLM-generated texts, including ontological overload, ambiguity, and inconsistency.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：HateTinyLLM : Hate Speech Detection Using Tiny Large Language Models</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01577">https://arxiv.org/abs/2405.01577</a></p>
  <p><b>作者</b>：Tanmay Sen,  Ansuman Das,  Mrinmay Sen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：speech encompasses verbal, Hate speech encompasses, encompasses verbal, sensitive characteristics, Hate speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hate speech encompasses verbal, written, or behavioral communication that targets derogatory or discriminatory language against individuals or groups based on sensitive characteristics. Automated hate speech detection plays a crucial role in curbing its propagation, especially across social media platforms. Various methods, including recent advancements in deep learning, have been devised to address this challenge. In this study, we introduce HateTinyLLM, a novel framework based on fine-tuned decoder-only tiny large language models (tinyLLMs) for efficient hate speech detection. Our experimental findings demonstrate that the fine-tuned HateTinyLLM outperforms the pretrained mixtral-7b model by a significant margin. We explored various tiny LLMs, including PY007/TinyLlama-1.1B-step-50K-105b, Microsoft/phi-2, and facebook/opt-1.3b, and fine-tuned them using LoRA and adapter methods. Our observations indicate that all LoRA-based fine-tuned models achieved over 80\% accuracy.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Uncovering Deceptive Tendencies in Language Models: A Simulated Company  AI Assistant</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01576">https://arxiv.org/abs/2405.01576</a></p>
  <p><b>作者</b>：Olli Järviniemi,  Evan Hubinger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：realistic simulation setting, study the tendency, systems to deceive, deceive by constructing, simulation setting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant. The simulated company employees provide tasks for the assistant to complete, these tasks spanning writing assistance, information retrieval and programming. We then introduce situations where the model might be inclined to behave deceptively, while taking care to not instruct or otherwise pressure the model to do so. Across different scenarios, we find that Claude 3 Opus
1) complies with a task of mass-generating comments to influence public perception of the company, later deceiving humans about it having done so,
2) lies to auditors when asked questions, and
3) strategically pretends to be less capable than it is during capability evaluations.
Our work demonstrates that even models trained to be helpful, harmless and honest sometimes behave deceptively in realistic scenarios, without notable external pressure to do so.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Software Mention Recognition with a Three-Stage Framework Based on  BERTology Models at SOMD 2024</b></summary>
  <p><b>编号</b>：[315]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01575">https://arxiv.org/abs/2405.01575</a></p>
  <p><b>作者</b>：Thuy Nguyen Thi,  Anh Nguyen Viet,  Thin Dang Van,  Ngan Nguyen Luu Thuy</p>
  <p><b>备注</b>：Software mention recognition, Named entity recognition, Transformer, Three-stage framework</p>
  <p><b>关键词</b>：Scholarly Publications shared-task, Detection in Scholarly, Scholarly Publications, Software Mention Detection, Publications shared-task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper describes our systems for the sub-task I in the Software Mention Detection in Scholarly Publications shared-task. We propose three approaches leveraging different pre-trained language models (BERT, SciBERT, and XLM-R) to tackle this challenge. Our bestperforming system addresses the named entity recognition (NER) problem through a three-stage framework. (1) Entity Sentence Classification - classifies sentences containing potential software mentions; (2) Entity Extraction - detects mentions within classified sentences; (3) Entity Type Classification - categorizes detected mentions into specific software types. Experiments on the official dataset demonstrate that our three-stage framework achieves competitive performance, surpassing both other participating teams and our alternative approaches. As a result, our framework based on the XLM-R-based model achieves a weighted F1-score of 67.80%, delivering our team the 3rd rank in Sub-task I for the Software Mention Recognition task.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Mitigating LLM Hallucinations via Conformal Abstention</b></summary>
  <p><b>编号</b>：[325]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01563">https://arxiv.org/abs/2405.01563</a></p>
  <p><b>作者</b>：Yasin Abbasi Yadkori,  Ilja Kuzborskij,  David Stutz,  András György,  Adam Fisch,  Arnaud Doucet,  Iuliya Beloshapka,  Wei-Hung Weng,  Yao-Yuan Yang,  Csaba Szepesvári,  Ali Taylan Cemgil,  Nenad Tomasev</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language model, abstain from responding, general domain, resorting to possibly, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying "I don't know") in a general domain, instead of resorting to possibly "hallucinating" a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Semantically Aligned Question and Code Generation for Automated Insight  Generation</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01556">https://arxiv.org/abs/2405.01556</a></p>
  <p><b>作者</b>：Ananya Singha,  Bhavya Chopra,  Anirudh Khatry,  Sumit Gulwani,  Austin Z. Henley,  Vu Le,  Chris Parnin,  Mukul Singh,  Gust Verbruggen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automated insight generation, helping knowledge workers, automated insights produced, common tactic, tactic for helping</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated insight generation is a common tactic for helping knowledge workers, such as data scientists, to quickly understand the potential value of new and unfamiliar data. Unfortunately, automated insights produced by large-language models can generate code that does not correctly correspond (or align) to the insight. In this paper, we leverage the semantic knowledge of large language models to generate targeted and insightful questions about data and the corresponding code to answer those questions. Then through an empirical study on data from Open-WikiTable, we show that embeddings can be effectively used for filtering out semantically unaligned pairs of question and code. Additionally, we found that generating questions and code together yields more diverse questions.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on  Self-Supervised Learning and Knowledge Transfer</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02124">https://arxiv.org/abs/2405.02124</a></p>
  <p><b>作者</b>：Noé Tits,  Prernna Bhatnagar,  Thierry Dutoit</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Connectionist Temporal Classification, Montreal Forced Aligner, text independent, alignment based, knowledge transfer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a novel approach for text independent phone-to-audio alignment based on phoneme recognition, representation learning and knowledge transfer. Our method leverages a self-supervised model (wav2vec2) fine-tuned for phoneme recognition using a Connectionist Temporal Classification (CTC) loss, a dimension reduction model and a frame-level phoneme classifier trained thanks to forced-alignment labels (using Montreal Forced Aligner) to produce multi-lingual phonetic representations, thus requiring minimal additional training. We evaluate our model using synthetic native data from the TIMIT dataset and the SCRIBE dataset for American and British English, respectively. Our proposed model outperforms the state-of-the-art (charsiu) in statistical metrics and has applications in language learning and speech processing systems. We leave experiments on other languages for future work but the design of the system makes it easily adaptable to other languages.</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>标题：Vibe-Eval: A hard evaluation suite for measuring progress of multimodal  language models</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02287">https://arxiv.org/abs/2405.02287</a></p>
  <p><b>作者</b>：Piotr Padlewski,  Max Bain,  Matthew Henderson,  Zhongkai Zhu,  Nishant Relan,  Hai Pham,  Donovan Ong,  Kaloyan Aleksiev,  Aitor Ormazabal,  Samuel Phua,  Ethan Yeo,  Eugenie Lamprecht,  Qi Liu,  Yuqi Wang,  Eric Chen,  Deyu Fu,  Lei Li,  Che Zheng,  Cyprien de Masson d'Autume,  Dani Yogatama,  Mikel Artetxe,  Yi Tay</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal chat models, open benchmark, benchmark and framework, multimodal chat, evaluating multimodal chat</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce Vibe-Eval: a new open benchmark and framework for evaluating multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts, including 100 of hard difficulty, complete with gold-standard responses authored by experts. Vibe-Eval is open-ended and challenging with dual objectives: (i) vibe checking multimodal chat models for day-to-day tasks and (ii) rigorously testing and probing the capabilities of present frontier models. Notably, our hard set contains >50% questions that all frontier models answer incorrectly. We explore the nuances of designing, evaluating, and ranking models on ultra challenging prompts. We also discuss trade-offs between human and automatic evaluation, and show that automatic model evaluation using Reka Core roughly correlates to human judgment. We offer free API access for the purpose of lightweight evaluation and plan to conduct formal human evaluations for public models that perform well on the Vibe-Eval's automatic scores. We release the evaluation code and data, see this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：DreamScene4D: Dynamic Multi-Object Scene Generation from Monocular  Videos</b></summary>
  <p><b>编号</b>：[3]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02280">https://arxiv.org/abs/2405.02280</a></p>
  <p><b>作者</b>：Wen-Hsuan Chu,  Lei Ke,  Katerina Fragkiadaki</p>
  <p><b>备注</b>：Project page: this https URL</p>
  <p><b>关键词</b>：powerful visual priors, provide powerful visual, Existing VLMs, highly under-constrained, powerful visual</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Existing VLMs can track in-the-wild 2D video objects while current generative models provide powerful visual priors for synthesizing novel views for the highly under-constrained 2D-to-3D object lifting. Building upon this exciting progress, we present DreamScene4D, the first approach that can generate three-dimensional dynamic scenes of multiple objects from monocular in-the-wild videos with large object motion across occlusions and novel viewpoints. Our key insight is to design a "decompose-then-recompose" scheme to factorize both the whole video scene and each object's 3D motion. We first decompose the video scene by using open-vocabulary mask trackers and an adapted image diffusion model to segment, track, and amodally complete the objects and background in the video. Each object track is mapped to a set of 3D Gaussians that deform and move in space and time. We also factorize the observed motion into multiple components to handle fast motion. The camera motion can be inferred by re-rendering the background to match the video frames. For the object motion, we first model the object-centric deformation of the objects by leveraging rendering losses and multi-view generative priors in an object-centric frame, then optimize object-centric to world-frame transformations by comparing the rendered outputs against the perceived pixel and optical flow. Finally, we recompose the background and objects and optimize for relative object scales using monocular depth prediction guidance. We show extensive results on the challenging DAVIS, Kubric, and self-captured videos, detail some limitations, and provide future directions. Besides 4D scene generation, our results show that DreamScene4D enables accurate 2D point motion tracking by projecting the inferred 3D trajectories to 2D, while never explicitly trained to do so.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：On the test-time zero-shot generalization of vision-language models: Do  we really need prompt learning?</b></summary>
  <p><b>编号</b>：[5]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02266">https://arxiv.org/abs/2405.02266</a></p>
  <p><b>作者</b>：Maxime Zanella,  Ismail Ben Ayed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：notably CLIP, soft prompt tuning, effective adaptation techniques, large vision-language models, development of large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of large vision-language models, notably CLIP, has catalyzed research into effective adaptation techniques, with a particular focus on soft prompt tuning. Conjointly, test-time augmentation, which utilizes multiple augmented views of a single image to enhance zero-shot generalization, is emerging as a significant area of interest. This has predominantly directed research efforts toward test-time prompt tuning. In contrast, we introduce a robust MeanShift for Test-time Augmentation (MTA), which surpasses prompt-based methods without requiring this intensive training procedure. This positions MTA as an ideal solution for both standalone and API-based applications. Additionally, our method does not rely on ad hoc rules (e.g., confidence threshold) used in some previous test-time augmentation techniques to filter the augmented views. Instead, MTA incorporates a quality assessment variable for each view directly into its optimization process, termed as the inlierness score. This score is jointly optimized with a density mode seeking process, leading to an efficient training- and hyperparameter-free approach. We extensively benchmark our method on 15 datasets and demonstrate MTA's superiority and computational efficiency. Deployed easily as plug-and-play module on top of zero-shot models and state-of-the-art few-shot methods, MTA shows systematic and consistent improvements.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：What matters when building vision-language models?</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02246">https://arxiv.org/abs/2405.02246</a></p>
  <p><b>作者</b>：Hugo Laurençon,  Léo Tronchon,  Matthieu Cord,  Victor Sanh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, vision transformers, growing interest, interest in vision-language, driven by improvements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Designed Dithering Sign Activation for Binary Neural Networks</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02220">https://arxiv.org/abs/2405.02220</a></p>
  <p><b>作者</b>：Brayan Monroy,  Juan Estupiñan,  Tatiana Gelvez-Barrera,  Jorge Bacca,  Henry Arguello</p>
  <p><b>备注</b>：7 pages</p>
  <p><b>关键词</b>：Sign activation function, computer vision tasks, Sign activation, Neural Networks emerged, Binary Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Binary Neural Networks emerged as a cost-effective and energy-efficient solution for computer vision tasks by binarizing either network weights or activations. However, common binary activations, such as the Sign activation function, abruptly binarize the values with a single threshold, losing fine-grained details in the feature outputs. This work proposes an activation that applies multiple thresholds following dithering principles, shifting the Sign activation function for each pixel according to a spatially periodic threshold kernel. Unlike literature methods, the shifting is defined jointly for a set of adjacent pixels, taking advantage of spatial correlations. Experiments over the classification task demonstrate the effectiveness of the designed dithering Sign activation function as an alternative activation for binary neural networks, without increasing the computational cost. Further, DeSign balances the preservation of details with the efficiency of binary operations.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Multispectral Fine-Grained Classification of Blackgrass in Wheat and  Barley Crops</b></summary>
  <p><b>编号</b>：[23]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02218">https://arxiv.org/abs/2405.02218</a></p>
  <p><b>作者</b>：Madeleine Darbyshire,  Shaun Coutts,  Eleanor Hammond,  Fazilet Gokbudak,  Cengiz Oztireli,  Petra Bosilj,  Junfeng Gao,  Elizabeth Sklar,  Simon Parsons</p>
  <p><b>备注</b>：19 pages, 6 figures</p>
  <p><b>关键词</b>：managing weed populations, herbicide resistance grows, populations are needed, environmental repercussions, repercussions of excessive</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the burden of herbicide resistance grows and the environmental repercussions of excessive herbicide use become clear, new ways of managing weed populations are needed. This is particularly true for cereal crops, like wheat and barley, that are staple food crops and occupy a globally significant portion of agricultural land. Even small improvements in weed management practices across these major food crops worldwide would yield considerable benefits for both the environment and global food security. Blackgrass is a major grass weed which causes particular problems in cereal crops in north-west Europe, a major cereal production area, because it has high levels of of herbicide resistance and is well adapted to agronomic practice in this region. With the use of machine vision and multispectral imaging, we investigate the effectiveness of state-of-the-art methods to identify blackgrass in wheat and barley crops. As part of this work, we provide a large dataset with which we evaluate several key aspects of blackgrass weed recognition. Firstly, we determine the performance of different CNN and transformer-based architectures on images from unseen fields. Secondly, we demonstrate the role that different spectral bands have on the performance of weed classification. Lastly, we evaluate the role of dataset size in classification performance for each of the models trialled. We find that even with a fairly modest quantity of training data an accuracy of almost 90% can be achieved on images from unseen fields.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Non-Destructive Peat Analysis using Hyperspectral Imaging and Machine  Learning</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02191">https://arxiv.org/abs/2405.02191</a></p>
  <p><b>作者</b>：Yijun Yan,  Jinchang Ren,  Barry Harrison,  Oliver Lewis,  Yinhe Li,  Ping Ma</p>
  <p><b>备注</b>：4 pages,4 figures</p>
  <p><b>关键词</b>：imparts distinctive, final product, crucial component, distinctive and irreplaceable, irreplaceable flavours</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Peat, a crucial component in whisky production, imparts distinctive and irreplaceable flavours to the final product. However, the extraction of peat disrupts ancient ecosystems and releases significant amounts of carbon, contributing to climate change. This paper aims to address this issue by conducting a feasibility study on enhancing peat use efficiency in whisky manufacturing through non-destructive analysis using hyperspectral imaging. Results show that shot-wave infrared (SWIR) data is more effective for analyzing peat samples and predicting total phenol levels, with accuracies up to 99.81%.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Training-Free Deepfake Voice Recognition by Leveraging Large-Scale  Pre-Trained Models</b></summary>
  <p><b>编号</b>：[37]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02179">https://arxiv.org/abs/2405.02179</a></p>
  <p><b>作者</b>：Alessandro Pianese,  Davide Cozzolino,  Giovanni Poggi,  Luisa Verdoliva</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：provide reliable results, current audio deepfake, generalization ability, main issue, issue for current</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Generalization is a main issue for current audio deepfake detectors, which struggle to provide reliable results on out-of-distribution data. Given the speed at which more and more accurate synthesis methods are developed, it is very important to design techniques that work well also on data they were not trained this http URL this paper we study the potential of large-scale pre-trained models for audio deepfake detection, with special focus on generalization ability. To this end, the detection problem is reformulated in a speaker verification framework and fake audios are exposed by the mismatch between the voice sample under test and the voice of the claimed identity. With this paradigm, no fake speech sample is necessary in training, cutting off any link with the generation method at the root, and ensuring full generalization ability. Features are extracted by general-purpose large pre-trained models, with no need for training or fine-tuning on specific fake detection or speaker verification datasets. At detection time only a limited set of voice fragments of the identity under test is required. Experiments on several datasets widespread in the community show that detectors based on pre-trained models achieve excellent performance and show strong generalization ability, rivaling supervised methods on in-distribution data and largely overcoming them on out-of-distribution data.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Self-Supervised Learning for Real-World Super-Resolution from Dual and  Multiple Zoomed Observations</b></summary>
  <p><b>编号</b>：[42]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02171">https://arxiv.org/abs/2405.02171</a></p>
  <p><b>作者</b>：Zhilu Zhang,  Ruohao Wang,  Hongzhi Zhang,  Wangmeng Zuo</p>
  <p><b>备注</b>：Accpted by IEEE TPAMI in 2024. Extended version of ECCV 2022 paper "Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations" (arXiv:2203.01325)</p>
  <p><b>关键词</b>：proper reference image, image, challenging issues, issues in reference-based, choose a proper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic  Labeling using Foundation Models</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02162">https://arxiv.org/abs/2405.02162</a></p>
  <p><b>作者</b>：Mohamad Al Mdfaa,  Raghad Salameh,  Sergey Zagoruyko,  Gonzalo Ferrer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：significant challenge due, accurate semantic mapping, semantic mapping remains, computer vision, efficient and accurate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the field of robotics and computer vision, efficient and accurate semantic mapping remains a significant challenge due to the growing demand for intelligent machines that can comprehend and interact with complex environments. Conventional panoptic mapping methods, however, are limited by predefined semantic classes, thus making them ineffective for handling novel or unforeseen objects. In response to this limitation, we introduce the Unified Promptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances in foundation models to enable real-time, on-demand label generation using natural language prompts. By incorporating a dynamic labeling strategy into traditional panoptic mapping techniques, UPPM provides significant improvements in adaptability and versatility while maintaining high performance levels in map reconstruction. We demonstrate our approach on real-world and simulated datasets. Results show that UPPM can accurately reconstruct scenes and segment objects while generating rich semantic labels through natural language interactions. A series of ablation experiments validated the advantages of foundation model-based labeling over fixed label sets.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Multi-method Integration with Confidence-based Weighting for Zero-shot  Image Classification</b></summary>
  <p><b>编号</b>：[48]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02155">https://arxiv.org/abs/2405.02155</a></p>
  <p><b>作者</b>：Siqi Yin,  Lifan Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multi-alignment integration method, zero-shot learning, paper introduces, framework for zero-shot, multi-model and multi-alignment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a novel framework for zero-shot learning (ZSL), i.e., to recognize new categories that are unseen during training, by using a multi-model and multi-alignment integration method. Specifically, we propose three strategies to enhance the model's performance to handle ZSL: 1) Utilizing the extensive knowledge of ChatGPT and the powerful image generation capabilities of DALL-E to create reference images that can precisely describe unseen categories and classification boundaries, thereby alleviating the information bottleneck issue; 2) Integrating the results of text-image alignment and image-image alignment from CLIP, along with the image-image alignment results from DINO, to achieve more accurate predictions; 3) Introducing an adaptive weighting mechanism based on confidence levels to aggregate the outcomes from different prediction methods. Experimental results on multiple datasets, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our model can significantly improve classification accuracy compared to single-model approaches, achieving AUROC scores above 96% across all test datasets, and notably surpassing 99% on the CIFAR-10 dataset.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Probablistic Restoration with Adaptive Noise Sampling for 3D Human Pose  Estimation</b></summary>
  <p><b>编号</b>：[65]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02114">https://arxiv.org/abs/2405.02114</a></p>
  <p><b>作者</b>：Xianzhou Zeng,  Hao Qin,  Ming Kong,  Luyuan Chen,  Qiang Zhu</p>
  <p><b>备注</b>：ICME 2024</p>
  <p><b>关键词</b>：drawn great attention, human pose estimation, pose detection errors, ill-posed challenges, Multi-Hypothesis HPE research</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The accuracy and robustness of 3D human pose estimation (HPE) are limited by 2D pose detection errors and 2D to 3D ill-posed challenges, which have drawn great attention to Multi-Hypothesis HPE research. Most existing MH-HPE methods are based on generative models, which are computationally expensive and difficult to train. In this study, we propose a Probabilistic Restoration 3D Human Pose Estimation framework (PRPose) that can be integrated with any lightweight single-hypothesis model. Specifically, PRPose employs a weakly supervised approach to fit the hidden probability distribution of the 2D-to-3D lifting process in the Single-Hypothesis HPE model and then reverse-map the distribution to the 2D pose input through an adaptive noise sampling strategy to generate reasonable multi-hypothesis samples effectively. Extensive experiments on 3D HPE benchmarks (Human3.6M and MPI-INF-3DHP) highlight the effectiveness and efficiency of PRPose. Code is available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot  Action Recognition</b></summary>
  <p><b>编号</b>：[78]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02077">https://arxiv.org/abs/2405.02077</a></p>
  <p><b>作者</b>：Hongyu Qu,  Rui Yan,  Xiangbo Shu,  Haoliang Gao,  Peng Huang,  Guo-Sen Xie</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：achieve promising performance, performing semantic matching, methods achieve promising, learned discriminative features, FSAR methods focus</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent few-shot action recognition (FSAR) methods achieve promising performance by performing semantic matching on learned discriminative features. However, most FSAR methods focus on single-scale (e.g., frame-level, segment-level, \etc) feature alignment, which ignores that human actions with the same semantic may appear at different velocities. To this end, we develop a novel Multi-Velocity Progressive-alignment (MVP-Shot) framework to progressively learn and align semantic-related action features at multi-velocity levels. Concretely, a Multi-Velocity Feature Alignment (MVFA) module is designed to measure the similarity between features from support and query videos with different velocity scales and then merge all similarity scores in a residual fashion. To avoid the multiple velocity features deviating from the underlying motion semantic, our proposed Progressive Semantic-Tailored Interaction (PSTI) module injects velocity-tailored text information into the video feature via feature interaction on channel and temporal domains at different velocities. The above two modules compensate for each other to predict query categories more accurately under the few-shot settings. Experimental results show our method outperforms current state-of-the-art methods on multiple standard few-shot benchmarks (i.e., HMDB51, UCF101, Kinetics, and SSv2-small).</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Advancing Pre-trained Teacher: Towards Robust Feature Discrepancy for  Anomaly Detection</b></summary>
  <p><b>编号</b>：[82]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02068">https://arxiv.org/abs/2405.02068</a></p>
  <p><b>作者</b>：Canhui Tang,  Sanping Zhou,  Yizhe Li,  Yonghao Dong,  Le Wang</p>
  <p><b>备注</b>：The paper is under review</p>
  <p><b>关键词</b>：learnable student model, knowledge distillation, student model, Hard Knowledge Distillation, industrial anomaly detection</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the wide application of knowledge distillation between an ImageNet pre-trained teacher model and a learnable student model, industrial anomaly detection has witnessed a significant achievement in the past few years. The success of knowledge distillation mainly relies on how to keep the feature discrepancy between the teacher and student model, in which it assumes that: (1) the teacher model can jointly represent two different distributions for the normal and abnormal patterns, while (2) the student model can only reconstruct the normal distribution. However, it still remains a challenging issue to maintain these ideal assumptions in practice. In this paper, we propose a simple yet effective two-stage industrial anomaly detection framework, termed as AAND, which sequentially performs Anomaly Amplification and Normality Distillation to obtain robust feature discrepancy. In the first anomaly amplification stage, we propose a novel Residual Anomaly Amplification (RAA) module to advance the pre-trained teacher encoder. With the exposure of synthetic anomalies, it amplifies anomalies via residual generation while maintaining the integrity of pre-trained model. It mainly comprises a Matching-guided Residual Gate and an Attribute-scaling Residual Generator, which can determine the residuals' proportion and characteristic, respectively. In the second normality distillation stage, we further employ a reverse distillation paradigm to train a student decoder, in which a novel Hard Knowledge Distillation (HKD) loss is built to better facilitate the reconstruction of normal patterns. Comprehensive experiments on the MvTecAD, VisA, and MvTec3D-RGB datasets show that our method achieves state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights</b></summary>
  <p><b>编号</b>：[84]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02066">https://arxiv.org/abs/2405.02066</a></p>
  <p><b>作者</b>：Youngdong Jang,  Dong In Lee,  MinHyuk Jang,  Jong Wook Kim,  Feng Yang,  Sangpil Kim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Neural Radiance Fields, Radiance Fields, Neural Radiance, research offer extensive, offer extensive applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Towards general deep-learning-based tree instance segmentation models</b></summary>
  <p><b>编号</b>：[87]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02061">https://arxiv.org/abs/2405.02061</a></p>
  <p><b>作者</b>：Jonathan Henrich,  Jan van Delden</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：carbon sequestration estimation, sequestration estimation, point clouds, crucial task, task for downstream</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The segmentation of individual trees from forest point clouds is a crucial task for downstream analyses such as carbon sequestration estimation. Recently, deep-learning-based methods have been proposed which show the potential of learning to segment trees. Since these methods are trained in a supervised way, the question arises how general models can be obtained that are applicable across a wide range of settings. So far, training has been mainly conducted with data from one specific laser scanning type and for specific types of forests. In this work, we train one segmentation model under various conditions, using seven diverse datasets found in literature, to gain insights into the generalization capabilities under domain-shift. Our results suggest that a generalization from coniferous dominated sparse point clouds to deciduous dominated high-resolution point clouds is possible. Conversely, qualitative evidence suggests that generalization from high-resolution to low-resolution point clouds is challenging. This emphasizes the need for forest point clouds with diverse data characteristics for model development. To enrich the available data basis, labeled trees from two previous works were propagated to the complete forest point cloud and are made publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：IFNet: Deep Imaging and Focusing for Handheld SAR with Millimeter-wave  Signals</b></summary>
  <p><b>编号</b>：[103]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02023">https://arxiv.org/abs/2405.02023</a></p>
  <p><b>作者</b>：Li Yadong,  Zhang Dongheng,  Geng Ruixu,  Wu Jincheng,  Hu Yang,  Sun Qibin,  Chen Yan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：synthetic aperture radar, applies synthetic aperture, Recent advancements, aperture radar, principles in portable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements have showcased the potential of handheld millimeter-wave (mmWave) imaging, which applies synthetic aperture radar (SAR) principles in portable settings. However, existing studies addressing handheld motion errors either rely on costly tracking devices or employ simplified imaging models, leading to impractical deployment or limited performance. In this paper, we present IFNet, a novel deep unfolding network that combines the strengths of signal processing models and deep neural networks to achieve robust imaging and focusing for handheld mmWave systems. We first formulate the handheld imaging model by integrating multiple priors about mmWave images and handheld phase errors. Furthermore, we transform the optimization processes into an iterative network structure for improved and efficient imaging performance. Extensive experiments demonstrate that IFNet effectively compensates for handheld phase errors and recovers high-fidelity images from severely distorted signals. In comparison with existing methods, IFNet can achieve at least 11.89 dB improvement in average peak signal-to-noise ratio (PSNR) and 64.91% improvement in average structural similarity index measure (SSIM) on a real-world dataset.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：DiffMap: Enhancing Map Segmentation with Map Prior Using Diffusion Model</b></summary>
  <p><b>编号</b>：[109]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02008">https://arxiv.org/abs/2405.02008</a></p>
  <p><b>作者</b>：Peijin Jia,  Tuopu Wen,  Ziang Luo,  Mengmeng Yang,  Kun Jiang,  Zhiquan Lei,  Xuewei Tang,  Ziyuan Liu,  Le Cui,  Kehua Sheng,  Bo Zhang,  Diange Yang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：enabling autonomous driving, Constructing high-definition, autonomous driving, crucial requirement, requirement for enabling</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Constructing high-definition (HD) maps is a crucial requirement for enabling autonomous driving. In recent years, several map segmentation algorithms have been developed to address this need, leveraging advancements in Bird's-Eye View (BEV) perception. However, existing models still encounter challenges in producing realistic and consistent semantic map layouts. One prominent issue is the limited utilization of structured priors inherent in map segmentation masks. In light of this, we propose DiffMap, a novel approach specifically designed to model the structured priors of map segmentation masks using latent diffusion model. By incorporating this technique, the performance of existing semantic segmentation methods can be significantly enhanced and certain structural errors present in the segmentation outputs can be effectively rectified. Notably, the proposed module can be seamlessly integrated into any map segmentation model, thereby augmenting its capability to accurately delineate semantic information. Furthermore, through extensive visualization analysis, our model demonstrates superior proficiency in generating results that more accurately reflect real-world map layouts, further validating its efficacy in improving the quality of the generated maps.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2</b></summary>
  <p><b>编号</b>：[110]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02005">https://arxiv.org/abs/2405.02005</a></p>
  <p><b>作者</b>：Miriam Jäger,  Theodor Kapler,  Michael Feßenbecker,  Felix Birkelbach,  Markus Hillemann,  Boris Jutzi</p>
  <p><b>备注</b>：8 pages, 9 figures, 2 tables. Will be published in the ISPRS The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences</p>
  <p><b>关键词</b>：Gaussian Splatting, Gaussian Splatting stands, computer graphics, computer vision, fields of photogrammetry</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the fields of photogrammetry, computer vision and computer graphics, the task of neural 3D scene reconstruction has led to the exploration of various techniques. Among these, 3D Gaussian Splatting stands out for its explicit representation of scenes using 3D Gaussians, making it appealing for tasks like 3D point cloud extraction and surface reconstruction. Motivated by its potential, we address the domain of 3D scene reconstruction, aiming to leverage the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting. We present HoloGS, a novel workflow utilizing HoloLens sensor data, which bypasses the need for pre-processing steps like Structure from Motion by instantly accessing the required input data i.e. the images, camera poses and the point cloud from depth sensing. We provide comprehensive investigations, including the training process and the rendering quality, assessed through the Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate our approach on two self-captured scenes: An outdoor scene of a cultural heritage statue and an indoor scene of a fine-structured plant. Our results show that the HoloLens data, including RGB images, corresponding camera poses, and depth sensing based point clouds to initialize the Gaussians, are suitable as input for 3D Gaussian Splatting.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：M${^2}$Depth: Self-supervised Two-Frame Multi-camera Metric Depth  Estimation</b></summary>
  <p><b>编号</b>：[111]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02004">https://arxiv.org/abs/2405.02004</a></p>
  <p><b>作者</b>：Yingshuang Zou,  Yikang Ding,  Xi Qiu,  Haoqian Wang,  Haotian Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：predict reliable scale-aware, reliable scale-aware surrounding, depth estimation network, self-supervised two-frame multi-camera, two-frame multi-camera metric</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a novel self-supervised two-frame multi-camera metric depth estimation network, termed M${^2}$Depth, which is designed to predict reliable scale-aware surrounding depth in autonomous driving. Unlike the previous works that use multi-view images from a single time-step or multiple time-step images from a single camera, M${^2}$Depth takes temporally adjacent two-frame images from multiple cameras as inputs and produces high-quality surrounding depth. We first construct cost volumes in spatial and temporal domains individually and propose a spatial-temporal fusion module that integrates the spatial-temporal information to yield a strong volume presentation. We additionally combine the neural prior from SAM features with internal features to reduce the ambiguity between foreground and background and strengthen the depth edges. Extensive experimental results on nuScenes and DDAD benchmarks show M${^2}$Depth achieves state-of-the-art performance. More results can be found in this https URL .</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Cooperation and Federation in Distributed Radar Point Cloud Processing</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01995">https://arxiv.org/abs/2405.01995</a></p>
  <p><b>作者</b>：S. Savazzi,  V. Rampa,  S. Kianoush,  A. Minora,  L. Costa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：resource-constrained MIMO radars, low range-azimuth resolution, resource-constrained MIMO, MIMO radars, range-azimuth resolution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The paper considers the problem of human-scale RF sensing utilizing a network of resource-constrained MIMO radars with low range-azimuth resolution. The radars operate in the mmWave band and obtain time-varying 3D point cloud (PC) information that is sensitive to body movements. They also observe the same scene from different views and cooperate while sensing the environment using a sidelink communication channel. Conventional cooperation setups allow the radars to mutually exchange raw PC information to improve ego sensing. The paper proposes a federation mechanism where the radars exchange the parameters of a Bayesian posterior measure of the observed PCs, rather than raw data. The radars act as distributed parameter servers to reconstruct a global posterior (i.e., federated posterior) using Bayesian tools. The paper quantifies and compares the benefits of radar federation with respect to cooperation mechanisms. Both approaches are validated by experiments with a real-time demonstration platform. Federation makes minimal use of the sidelink communication channel (20 ÷ 25 times lower bandwidth use) and is less sensitive to unresolved targets. On the other hand, cooperation reduces the mean absolute target estimation error of about 20%.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：SFFNet: A Wavelet-Based Spatial and Frequency Domain Fusion Network for  Remote Sensing Segmentation</b></summary>
  <p><b>编号</b>：[116]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01992">https://arxiv.org/abs/2405.01992</a></p>
  <p><b>作者</b>：Yunsong Yang,  Genji Yuan,  Jinjiang Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Domain Fusion Network, Frequency Domain Fusion, remote sensing segmentation, Fusion Network, Domain Fusion</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In order to fully utilize spatial information for segmentation and address the challenge of handling areas with significant grayscale variations in remote sensing segmentation, we propose the SFFNet (Spatial and Frequency Domain Fusion Network) framework. This framework employs a two-stage network design: the first stage extracts features using spatial methods to obtain features with sufficient spatial details and semantic information; the second stage maps these features in both spatial and frequency domains. In the frequency domain mapping, we introduce the Wavelet Transform Feature Decomposer (WTFD) structure, which decomposes features into low-frequency and high-frequency components using the Haar wavelet transform and integrates them with spatial features. To bridge the semantic gap between frequency and spatial features, and facilitate significant feature selection to promote the combination of features from different representation domains, we design the Multiscale Dual-Representation Alignment Filter (MDAF). This structure utilizes multiscale convolutions and dual-cross attentions. Comprehensive experimental results demonstrate that, compared to existing methods, SFFNet achieves superior performance in terms of mIoU, reaching 84.80% and 87.73% respectively.The code is located at this https URL.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：A Sonar-based AUV Positioning System for Underwater Environments with  Low Infrastructure Density</b></summary>
  <p><b>编号</b>：[126]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01971">https://arxiv.org/abs/2405.01971</a></p>
  <p><b>作者</b>：Emilio Olivastri,  Daniel Fusaro,  Wanmeng Li,  Simone Mosco,  Alberto Pretto</p>
  <p><b>备注</b>：Accepted to the IEEE ICRA Workshop on Field Robotics 2024</p>
  <p><b>关键词</b>：robust localization solutions, underwater vehicles highlights, Autonomous Underwater Vehicles, underwater vehicles, inspection missions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The increasing demand for underwater vehicles highlights the necessity for robust localization solutions in inspection missions. In this work, we present a novel real-time sonar-based underwater global positioning algorithm for AUVs (Autonomous Underwater Vehicles) designed for environments with a sparse distribution of human-made assets. Our approach exploits two synergistic data interpretation frontends applied to the same stream of sonar data acquired by a multibeam Forward-Looking Sonar (FSD). These observations are fused within a Particle Filter (PF) either to weigh more particles that belong to high-likelihood regions or to solve symmetric ambiguities. Preliminary experiments carried out on a simulated environment resembling a real underwater plant provided promising results. This work represents a starting point towards future developments of the method and consequent exhaustive evaluations also in real-world scenarios.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：From Attack to Defense: Insights into Deep Learning Security Measures in  Black-Box Settings</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01963">https://arxiv.org/abs/2405.01963</a></p>
  <p><b>作者</b>：Firuz Juraev,  Mohammed Abuhamad,  Eric Chan-Tin,  George K. Thiruvathukal,  Tamer Abuhmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Learning, rapidly maturing, attacks, security-crucial applications, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Learning (DL) is rapidly maturing to the point that it can be used in safety- and security-crucial applications. However, adversarial samples, which are undetectable to the human eye, pose a serious threat that can cause the model to misbehave and compromise the performance of such applications. Addressing the robustness of DL models has become crucial to understanding and defending against adversarial attacks. In this study, we perform comprehensive experiments to examine the effect of adversarial attacks and defenses on various model architectures across well-known datasets. Our research focuses on black-box attacks such as SimBA, HopSkipJump, MGAAttack, and boundary attacks, as well as preprocessor-based defensive mechanisms, including bits squeezing, median smoothing, and JPEG filter. Experimenting with various models, our results demonstrate that the level of noise needed for the attack increases as the number of layers increases. Moreover, the attack success rate decreases as the number of layers increases. This indicates that model complexity and robustness have a significant relationship. Investigating the diversity and robustness relationship, our experiments with diverse models show that having a large number of parameters does not imply higher robustness. Our experiments extend to show the effects of the training dataset on model robustness. Using various datasets such as ImageNet-1000, CIFAR-100, and CIFAR-10 are used to evaluate the black-box attacks. Considering the multiple dimensions of our analysis, e.g., model complexity and training dataset, we examined the behavior of black-box attacks when models apply defenses. Our results show that applying defense strategies can significantly reduce attack effectiveness. This research provides in-depth analysis and insight into the robustness of DL models against various attacks, and defenses.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：An Attention Based Pipeline for Identifying Pre-Cancer Lesions in Head  and Neck Clinical Images</b></summary>
  <p><b>编号</b>：[133]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01937">https://arxiv.org/abs/2405.01937</a></p>
  <p><b>作者</b>：Abdullah Alsalemi,  Anza Shakeel,  Mollie Clark,  Syed Ali Khurram,  Shan E Ahmed Raza</p>
  <p><b>备注</b>：5 pages, 3 figures, accepted in ISBI 2024</p>
  <p><b>关键词</b>：improve patient prognosis, improve patient, Multiple Instance Learning, early intervention, Multiple Instance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Early detection of cancer can help improve patient prognosis by early intervention. Head and neck cancer is diagnosed in specialist centres after a surgical biopsy, however, there is a potential for these to be missed leading to delayed diagnosis. To overcome these challenges, we present an attention based pipeline that identifies suspected lesions, segments, and classifies them as non-dysplastic, dysplastic and cancerous lesions. We propose (a) a vision transformer based Mask R-CNN network for lesion detection and segmentation of clinical images, and (b) Multiple Instance Learning (MIL) based scheme for classification. Current results show that the segmentation model produces segmentation masks and bounding boxes with up to 82% overlap accuracy score on unseen external test data and surpassing reviewed segmentation benchmarks. Next, a classification F1-score of 85% on the internal cohort test set. An app has been developed to perform lesion segmentation taken via a smart device. Future work involves employing endoscopic video data for precise early detection and prognosis.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Impact of Architectural Modifications on Deep Learning Adversarial  Robustness</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01934">https://arxiv.org/abs/2405.01934</a></p>
  <p><b>作者</b>：Firuz Juraev,  Mohammed Abuhamad,  Simon S. Woo,  George K Thiruvathukal,  Tamer Abuhmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including safety-critical applications, deep learning models, deep learning, including safety-critical, self-driving vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rapid advancements of deep learning are accelerating adoption in a wide variety of applications, including safety-critical applications such as self-driving vehicles, drones, robots, and surveillance systems. These advancements include applying variations of sophisticated techniques that improve the performance of models. However, such models are not immune to adversarial manipulations, which can cause the system to misbehave and remain unnoticed by experts. The frequency of modifications to existing deep learning models necessitates thorough analysis to determine the impact on models' robustness. In this work, we present an experimental evaluation of the effects of model modifications on deep learning model robustness using adversarial attacks. Our methodology involves examining the robustness of variations of models against various adversarial attacks. By conducting our experiments, we aim to shed light on the critical issue of maintaining the reliability and safety of deep learning models in safety- and security-critical applications. Our results indicate the pressing demand for an in-depth assessment of the effects of model changes on the robustness of models.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Auto-Encoding Morph-Tokens for Multimodal LLM</b></summary>
  <p><b>编号</b>：[139]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01926">https://arxiv.org/abs/2405.01926</a></p>
  <p><b>作者</b>：Kaihang Pan,  Siliang Tang,  Juncheng Li,  Zhaoyu Fan,  Wei Chow,  Shuicheng Yan,  Tat-Seng Chua,  Yueting Zhuang,  Hanwang Zhang</p>
  <p><b>备注</b>：Accepted by ICML 2024</p>
  <p><b>关键词</b>：textual output, presents an ongoing, ongoing challenge, visual output, output</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>For multimodal LLMs, the synergy of visual comprehension (textual output) and generation (visual output) presents an ongoing challenge. This is due to a conflicting objective: for comprehension, an MLLM needs to abstract the visuals; for generation, it needs to preserve the visuals as much as possible. Thus, the objective is a dilemma for visual-tokens. To resolve the conflict, we propose encoding images into morph-tokens to serve a dual purpose: for comprehension, they act as visual prompts instructing MLLM to generate texts; for generation, they take on a different, non-conflicting role as complete visual-tokens for image reconstruction, where the missing visual cues are recovered by the MLLM. Extensive experiments show that morph-tokens can achieve a new SOTA for multimodal comprehension and generation simultaneously. Our project is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Lightweight Change Detection in Heterogeneous Remote Sensing Images with  Online All-Integer Pruning Training</b></summary>
  <p><b>编号</b>：[143]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01920">https://arxiv.org/abs/2405.01920</a></p>
  <p><b>作者</b>：Chengyang Zhang,  Weiming Li,  Gang Li,  Huina Song,  Zhaohui Song,  Xueqian Wang,  Antonio Plaza</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：heterogeneous remote sensing, remote sensing images, earthquakes and floods, response to emergencies, emergencies like earthquakes</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detection of changes in heterogeneous remote sensing images is vital, especially in response to emergencies like earthquakes and floods. Current homogenous transformation-based change detection (CD) methods often suffer from high computation and memory costs, which are not friendly to edge-computation devices like onboard CD devices at satellites. To address this issue, this paper proposes a new lightweight CD method for heterogeneous remote sensing images that employs the online all-integer pruning (OAIP) training strategy to efficiently fine-tune the CD network using the current test data. The proposed CD network consists of two visual geometry group (VGG) subnetworks as the backbone architecture. In the OAIP-based training process, all the weights, gradients, and intermediate data are quantized to integers to speed up training and reduce memory usage, where the per-layer block exponentiation scaling scheme is utilized to reduce the computation errors of network parameters caused by quantization. Second, an adaptive filter-level pruning method based on the L1-norm criterion is employed to further lighten the fine-tuning process of the CD network. Experimental results show that the proposed OAIP-based method attains similar detection performance (but with significantly reduced computation complexity and memory usage) in comparison with state-of-the-art CD methods.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Enhancing Micro Gesture Recognition for Emotion Understanding via  Context-aware Visual-Text Contrastive Learning</b></summary>
  <p><b>编号</b>：[156]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01885">https://arxiv.org/abs/2405.01885</a></p>
  <p><b>作者</b>：Deng Li,  Bohao Xing,  Xin Liu</p>
  <p><b>备注</b>：accepted by IEEE Signal Processing Letters</p>
  <p><b>关键词</b>：Micro Gesture Recognition, Psychological studies, emotion understanding, existing Micro Gesture, studies have shown</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Psychological studies have shown that Micro Gestures (MG) are closely linked to human emotions. MG-based emotion understanding has attracted much attention because it allows for emotion understanding through nonverbal body gestures without relying on identity information (e.g., facial and electrocardiogram data). Therefore, it is essential to recognize MG effectively for advanced emotion understanding. However, existing Micro Gesture Recognition (MGR) methods utilize only a single modality (e.g., RGB or skeleton) while overlooking crucial textual information. In this letter, we propose a simple but effective visual-text contrastive learning solution that utilizes text information for MGR. In addition, instead of using handcrafted prompts for visual-text contrastive learning, we propose a novel module called Adaptive prompting to generate context-aware prompts. The experimental results show that the proposed method achieves state-of-the-art performance on two public datasets. Furthermore, based on an empirical study utilizing the results of MGR for emotion understanding, we demonstrate that using the textual results of MGR significantly improves performance by 6%+ compared to directly using video as input.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Defect Image Sample Generation With Diffusion Prior for Steel Surface  Defect Recognition</b></summary>
  <p><b>编号</b>：[162]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01872">https://arxiv.org/abs/2405.01872</a></p>
  <p><b>作者</b>：Yichun Tai,  Kun Yang,  Tao Peng,  Zhenzhen Huang,  Zhijiang Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：steel surface defect, surface defect, steel surface, surface defect recognition, Stable Surface Defect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of steel surface defect recognition is an industrial problem with great industry values. The data insufficiency is the major challenge in training a robust defect recognition network. Existing methods have investigated to enlarge the dataset by generating samples with generative models. However, their generation quality is still limited by the insufficiency of defect image samples. To this end, we propose Stable Surface Defect Generation (StableSDG), which transfers the vast generation distribution embedded in Stable Diffusion model for steel surface defect image generation. To tackle with the distinctive distribution gap between steel surface images and generated images of the diffusion model, we propose two processes. First, we align the distribution by adapting parameters of the diffusion model, adopted both in the token embedding space and network parameter space. Besides, in the generation process, we propose image-oriented generation rather than from pure Gaussian noises. We conduct extensive experiments on steel surface defect dataset, demonstrating state-of-the-art performance on generating high-quality samples and training recognition models, and both designed processes are significant for the performance.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：TinySeg: Model Optimizing Framework for Image Segmentation on Tiny  Embedded Systems</b></summary>
  <p><b>编号</b>：[168]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01857">https://arxiv.org/abs/2405.01857</a></p>
  <p><b>作者</b>：Byungchul Chae,  Jiae Kim,  Seonyeong Heo</p>
  <p><b>备注</b>：LCTES 2024</p>
  <p><b>关键词</b>：computer vision tasks, unmanned aerial vehicle, major computer vision, Image segmentation, image segmentation models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image segmentation is one of the major computer vision tasks, which is applicable in a variety of domains, such as autonomous navigation of an unmanned aerial vehicle. However, image segmentation cannot easily materialize on tiny embedded systems because image segmentation models generally have high peak memory usage due to their architectural characteristics. This work finds that image segmentation models unnecessarily require large memory space with an existing tiny machine learning framework. That is, the existing framework cannot effectively manage the memory space for the image segmentation models.
This work proposes TinySeg, a new model optimizing framework that enables memory-efficient image segmentation for tiny embedded systems. TinySeg analyzes the lifetimes of tensors in the target model and identifies long-living tensors. Then, TinySeg optimizes the memory usage of the target model mainly with two methods: (i) tensor spilling into local or remote storage and (ii) fused fetching of spilled tensors. This work implements TinySeg on top of the existing tiny machine learning framework and demonstrates that TinySeg can reduce the peak memory usage of an image segmentation model by 39.3% for tiny embedded systems.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：FER-YOLO-Mamba: Facial Expression Detection and Classification Based on  Selective State Space</b></summary>
  <p><b>编号</b>：[181]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01828">https://arxiv.org/abs/2405.01828</a></p>
  <p><b>作者</b>：Hui Ma,  Sen Lei,  Turgay Celik,  Heng-Chao Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：human emotional cues, understanding human emotional, Facial Expression, Facial Expression Recognition, plays a pivotal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Facial Expression Recognition (FER) plays a pivotal role in understanding human emotional cues. However, traditional FER methods based on visual information have some limitations, such as preprocessing, feature extraction, and multi-stage classification procedures. These not only increase computational complexity but also require a significant amount of computing resources. Considering Convolutional Neural Network (CNN)-based FER schemes frequently prove inadequate in identifying the deep, long-distance dependencies embedded within facial expression images, and the Transformer's inherent quadratic computational complexity, this paper presents the FER-YOLO-Mamba model, which integrates the principles of Mamba and YOLO technologies to facilitate efficient coordination in facial expression image recognition and localization. Within the FER-YOLO-Mamba model, we further devise a FER-YOLO-VSS dual-branch module, which combines the inherent strengths of convolutional layers in local feature extraction with the exceptional capability of State Space Models (SSMs) in revealing long-distance dependencies. To the best of our knowledge, this is the first Vision Mamba model designed for facial expression detection and classification. To evaluate the performance of the proposed FER-YOLO-Mamba model, we conducted experiments on two benchmark datasets, RAF-DB and SFEW. The experimental results indicate that the FER-YOLO-Mamba model achieved better results compared to other models. The code is available from this https URL.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Improving Concept Alignment in Vision-Language Concept Bottleneck Models</b></summary>
  <p><b>编号</b>：[183]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01825">https://arxiv.org/abs/2405.01825</a></p>
  <p><b>作者</b>：Nithish Muthuchamy Selvaraj,  Xiaobao Guo,  Bingquan Shen,  Adams Wai-Kin Kong,  Alex Kot</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Concept Bottleneck Models, Large Language Models, Vision Language Models, Bottleneck Models, class predictions based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Concept Bottleneck Models (CBM) map the input image to a high-level human-understandable concept space and then make class predictions based on these concepts. Recent approaches automate the construction of CBM by prompting Large Language Models (LLM) to generate text concepts and then use Vision Language Models (VLM) to obtain concept scores to train a CBM. However, it is desired to build CBMs with concepts defined by human experts instead of LLM generated concepts to make them more trustworthy. In this work, we take a closer inspection on the faithfulness of VLM concept scores for such expert-defined concepts in domains like fine-grain bird species classification and animal classification. Our investigations reveal that frozen VLMs, like CLIP, struggle to correctly associate a concept to the corresponding visual input despite achieving a high classification performance. To address this, we propose a novel Contrastive Semi-Supervised (CSS) learning method which uses a few labeled concept examples to improve concept alignment (activate truthful visual concepts) in CLIP model. Extensive experiments on three benchmark datasets show that our approach substantially increases the concept accuracy and classification accuracy, yet requires only a fraction of the human-annotated concept labels. To further improve the classification performance, we also introduce a new class-level intervention procedure for fine-grain classification problems that identifies the confounding classes and intervenes their concept space to reduce errors.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent  Circumvention</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01820">https://arxiv.org/abs/2405.01820</a></p>
  <p><b>作者</b>：Cedric Deslandes Whitney,  Justin Norman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learning systems require, Machine learning systems, synthetic data, data, synthetic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning systems require representations of the real world for training and testing - they require data, and lots of it. Collecting data at scale has logistical and ethical challenges, and synthetic data promises a solution to these challenges. Instead of needing to collect photos of real people's faces to train a facial recognition system, a model creator could create and use photo-realistic, synthetic faces. The comparative ease of generating this synthetic data rather than relying on collecting data has made it a common practice. We present two key risks of using synthetic data in model development. First, we detail the high risk of false confidence when using synthetic data to increase dataset diversity and representation. We base this in the examination of a real world use-case of synthetic data, where synthetic datasets were generated for an evaluation of facial recognition technology. Second, we examine how using synthetic data risks circumventing consent for data usage. We illustrate this by considering the importance of consent to the U.S. Federal Trade Commission's regulation of data collection and affected models. Finally, we discuss how these two risks exemplify how synthetic data complicates existing governance and ethical practice; by decoupling data from those it impacts, synthetic data is prone to consolidating power away those most impacted by algorithmically-mediated harm.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：An Approach to Systematic Data Acquisition and Data-Driven Simulation  for the Safety Testing of Automated Driving Functions</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01776">https://arxiv.org/abs/2405.01776</a></p>
  <p><b>作者</b>：Leon Eisemann,  Mirjam Fehling-Kaschek,  Henrik Gommel,  David Hermann,  Marvin Klemp,  Martin Lauer,  Benjamin Lickert,  Florian Luettner,  Robin Moss,  Nicole Neis,  Maria Pohle,  Simon Romanski,  Daniel Stadler,  Alexander Stolz,  Jens Ziehn,  Jingxing Zhou</p>
  <p><b>备注</b>：8 pages, 5 figures</p>
  <p><b>关键词</b>：operational design domains, automated driving functions, covering significant proportions, design domains, proportions of development</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With growing complexity and criticality of automated driving functions in road traffic and their operational design domains (ODD), there is increasing demand for covering significant proportions of development, validation, and verification in virtual environments and through simulation models.
If, however, simulations are meant not only to augment real-world experiments, but to replace them, quantitative approaches are required that measure to what degree and under which preconditions simulation models adequately represent reality, and thus, using their results accordingly. Especially in R&D areas related to the safety impact of the "open world", there is a significant shortage of real-world data to parameterize and/or validate simulations - especially with respect to the behavior of human traffic participants, whom automated driving functions will meet in mixed traffic.
We present an approach to systematically acquire data in public traffic by heterogeneous means, transform it into a unified representation, and use it to automatically parameterize traffic behavior models for use in data-driven virtual validation of automated driving functions.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Diabetic Retinopathy Detection Using Quantum Transfer Learning</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01734">https://arxiv.org/abs/2405.01734</a></p>
  <p><b>作者</b>：Ankush Jain,  Rinav Gupta,  Jai Singhal</p>
  <p><b>备注</b>：14 pages, 12 figures and 5 tables</p>
  <p><b>关键词</b>：vision impairment due, Quantum Transfer Learning, transfer learning, diabetes patients, Diabetic Retinopathy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diabetic Retinopathy (DR), a prevalent complication in diabetes patients, can lead to vision impairment due to lesions formed on the retina. Detecting DR at an advanced stage often results in irreversible blindness. The traditional process of diagnosing DR through retina fundus images by ophthalmologists is not only time-intensive but also expensive. While classical transfer learning models have been widely adopted for computer-aided detection of DR, their high maintenance costs can hinder their detection efficiency. In contrast, Quantum Transfer Learning offers a more effective solution to this challenge. This approach is notably advantageous because it operates on heuristic principles, making it highly optimized for the task. Our proposed methodology leverages this hybrid quantum transfer learning technique to detect DR. To construct our model, we utilize the APTOS 2019 Blindness Detection dataset, available on Kaggle. We employ the ResNet-18, ResNet34, ResNet50, ResNet101, ResNet152 and Inception V3, pre-trained classical neural networks, for the initial feature extraction. For the classification stage, we use a Variational Quantum Classifier. Our hybrid quantum model has shown remarkable results, achieving an accuracy of 97% for ResNet-18. This demonstrates that quantum computing, when integrated with quantum machine learning, can perform tasks with a level of power and efficiency unattainable by classical computers alone. By harnessing these advanced technologies, we can significantly improve the detection and diagnosis of Diabetic Retinopathy, potentially saving many from the risk of blindness.
Keywords: Diabetic Retinopathy, Quantum Transfer Learning, Deep Learning</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Zero-Shot Monocular Motion Segmentation in the Wild by Combining Deep  Learning with Geometric Motion Model Fusion</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01723">https://arxiv.org/abs/2405.01723</a></p>
  <p><b>作者</b>：Yuxiang Huang,  Yuhao Chen,  John Zelek</p>
  <p><b>备注</b>：Accepted by the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</p>
  <p><b>关键词</b>：complex scene structures, moving monocular camera, unknown camera motion, segmenting moving objects, Detecting and segmenting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting and segmenting moving objects from a moving monocular camera is challenging in the presence of unknown camera motion, diverse object motions and complex scene structures. Most existing methods rely on a single motion cue to perform motion segmentation, which is usually insufficient when facing different complex environments. While a few recent deep learning based methods are able to combine multiple motion cues to achieve improved accuracy, they depend heavily on vast datasets and extensive annotations, making them less adaptable to new scenarios. To address these limitations, we propose a novel monocular dense segmentation method that achieves state-of-the-art motion segmentation results in a zero-shot manner. The proposed method synergestically combines the strengths of deep learning and geometric model fusion methods by performing geometric model fusion on object proposals. Experiments show that our method achieves competitive results on several motion segmentation datasets and even surpasses some state-of-the-art supervised methods on certain benchmarks, while not being trained on any data. We also present an ablation study to show the effectiveness of combining different geometric models together for motion segmentation, highlighting the value of our geometric model fusion strategy.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Long Tail Image Generation Through Feature Space Augmentation and  Iterated Learning</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01705">https://arxiv.org/abs/2405.01705</a></p>
  <p><b>作者</b>：Rafael Elberg,  Denis Parra,  Mircea Petrache</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal machine learning, machine learning tasks, poorly distributed data, Stable Diffusion Models, multimodal machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Active Learning Enabled Low-cost Cell Image Segmentation Using Bounding  Box Annotation</b></summary>
  <p><b>编号</b>：[251]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01701">https://arxiv.org/abs/2405.01701</a></p>
  <p><b>作者</b>：Yu Zhu,  Qiang Yang,  Li Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：annotated training data, fully supervised deep, extensive annotated training, Cell image segmentation, supervised deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cell image segmentation is usually implemented using fully supervised deep learning methods, which heavily rely on extensive annotated training data. Yet, due to the complexity of cell morphology and the requirement for specialized knowledge, pixel-level annotation of cell images has become a highly labor-intensive task. To address the above problems, we propose an active learning framework for cell segmentation using bounding box annotations, which greatly reduces the data annotation cost of cell segmentation algorithms. First, we generate a box-supervised learning method (denoted as YOLO-SAM) by combining the YOLOv8 detector with the Segment Anything Model (SAM), which effectively reduces the complexity of data annotation. Furthermore, it is integrated into an active learning framework that employs the MC DropBlock method to train the segmentation model with fewer box-annotated samples. Extensive experiments demonstrate that our model saves more than ninety percent of data annotation time compared to mask-supervised deep learning methods.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：SOAR: Advancements in Small Body Object Detection for Aerial Imagery  Using State Space Models and Programmable Gradients</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01699">https://arxiv.org/abs/2405.01699</a></p>
  <p><b>作者</b>：Tushar Verma,  Jyotsna Singh,  Yash Bhartari,  Rishi Jarwal,  Suraj Singh,  Shubhkarman Singh</p>
  <p><b>备注</b>：7 pages, 5 figures</p>
  <p><b>关键词</b>：imagery presents significant, presents significant challenges, minimal data inherent, computer vision due, aerial imagery presents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Small object detection in aerial imagery presents significant challenges in computer vision due to the minimal data inherent in small-sized objects and their propensity to be obscured by larger objects and background noise. Traditional methods using transformer-based models often face limitations stemming from the lack of specialized databases, which adversely affect their performance with objects of varying orientations and scales. This underscores the need for more adaptable, lightweight models. In response, this paper introduces two innovative approaches that significantly enhance detection and segmentation capabilities for small aerial objects. Firstly, we explore the use of the SAHI framework on the newly introduced lightweight YOLO v9 architecture, which utilizes Programmable Gradient Information (PGI) to reduce the substantial information loss typically encountered in sequential feature extraction processes. The paper employs the Vision Mamba model, which incorporates position embeddings to facilitate precise location-aware visual understanding, combined with a novel bidirectional State Space Model (SSM) for effective visual context modeling. This State Space Model adeptly harnesses the linear complexity of CNNs and the global receptive field of Transformers, making it particularly effective in remote sensing image classification. Our experimental results demonstrate substantial improvements in detection accuracy and processing efficiency, validating the applicability of these approaches for real-time small object detection across diverse aerial scenarios. This paper also discusses how these methodologies could serve as foundational models for future advancements in aerial object recognition technologies. The source code will be made accessible here.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Language-Enhanced Latent Representations for Out-of-Distribution  Detection in Autonomous Driving</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01691">https://arxiv.org/abs/2405.01691</a></p>
  <p><b>作者</b>：Zhenjiang Mao,  Dong-You Jhong,  Ao Wang,  Ivan Ruchkin</p>
  <p><b>备注</b>：Presented at the Robot Trust for Symbiotic Societies (RTSS) Workshop, co-located with ICRA 2024</p>
  <p><b>关键词</b>：learning-based components encounter, components encounter unexpected, encounter unexpected inputs, essential in autonomous, determine when learning-based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution (OOD) detection is essential in autonomous driving, to determine when learning-based components encounter unexpected inputs. Traditional detectors typically use encoder models with fixed settings, thus lacking effective human interaction capabilities. With the rise of large foundation models, multimodal inputs offer the possibility of taking human language as a latent representation, thus enabling language-defined OOD detection. In this paper, we use the cosine similarity of image and text representations encoded by the multimodal model CLIP as a new representation to improve the transparency and controllability of latent encodings used for visual anomaly detection. We compare our approach with existing pre-trained encoders that can only produce latent representations that are meaningless from the user's standpoint. Our experiments on realistic driving data show that the language-based latent representation performs better than the traditional representation of the vision encoder and helps improve the detection performance when combined with standard representations.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Adapting Self-Supervised Learning for Computational Pathology</b></summary>
  <p><b>编号</b>：[259]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01688">https://arxiv.org/abs/2405.01688</a></p>
  <p><b>作者</b>：Eric Zimmermann,  Neil Tenenholtz,  James Hall,  George Shaikovski,  Michal Zelechowski,  Adam Casson,  Fausto Milletari,  Julian Viret,  Eugene Vorontsov,  Siqi Liu,  Kristen Severson</p>
  <p><b>备注</b>：Presented at DCA in MI Workshop, CVPR 2024</p>
  <p><b>关键词</b>：Self-supervised learning, task-specific supervision, key technique, diverse tasks, tasks without task-specific</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Self-supervised learning (SSL) has emerged as a key technique for training networks that can generalize well to diverse tasks without task-specific supervision. This property makes SSL desirable for computational pathology, the study of digitized images of tissues, as there are many target applications and often limited labeled training samples. However, SSL algorithms and models have been primarily developed in the field of natural images and whether their performance can be improved by adaptation to particular domains remains an open question. In this work, we present an investigation of modifications to SSL for pathology data, specifically focusing on the DINOv2 algorithm. We propose alternative augmentations, regularization functions, and position encodings motivated by the characteristics of pathology images. We evaluate the impact of these changes on several benchmarks to demonstrate the value of tailored approaches.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：ShadowNav: Autonomous Global Localization for Lunar Navigation in  Darkness</b></summary>
  <p><b>编号</b>：[270]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01673">https://arxiv.org/abs/2405.01673</a></p>
  <p><b>作者</b>：Deegan Atha,  R. Michael Swan,  Abhishek Cauligi,  Anne Bettens,  Edwin Goh,  Dima Kogan,  Larry Matthies,  Masahiro Ono</p>
  <p><b>备注</b>：21 pages, 13 figures</p>
  <p><b>关键词</b>：inertial frame autonomously, surface rover missions, planetary bodies, rover missions utilize, ability to determine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to determine the pose of a rover in an inertial frame autonomously is a crucial capability necessary for the next generation of surface rover missions on other planetary bodies. Currently, most on-going rover missions utilize ground-in-the-loop interventions to manually correct for drift in the pose estimate and this human supervision bottlenecks the distance over which rovers can operate autonomously and carry out scientific measurements. In this paper, we present ShadowNav, an autonomous approach for global localization on the Moon with an emphasis on driving in darkness and at nighttime. Our approach uses the leading edge of Lunar craters as landmarks and a particle filtering approach is used to associate detected craters with known ones on an offboard map. We discuss the key design decisions in developing the ShadowNav framework for use with a Lunar rover concept equipped with a stereo camera and an external illumination source. Finally, we demonstrate the efficacy of our proposed approach in both a Lunar simulation environment and on data collected during a field test at Cinder Lakes, Arizona.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Out-of-distribution detection based on subspace projection of  high-dimensional features output by the last convolutional layer</b></summary>
  <p><b>编号</b>：[273]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01662">https://arxiv.org/abs/2405.01662</a></p>
  <p><b>作者</b>：Qiuyu Zhu,  Yiwei He</p>
  <p><b>备注</b>：10 pages, 4 figures</p>
  <p><b>关键词</b>：reliable pattern classification, OOD data, crucial for reliable, reliable pattern, sample originates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution (OOD) detection, crucial for reliable pattern classification, discerns whether a sample originates outside the training distribution. This paper concentrates on the high-dimensional features output by the final convolutional layer, which contain rich image features. Our key idea is to project these high-dimensional features into two specific feature subspaces, leveraging the dimensionality reduction capacity of the network's linear layers, trained with Predefined Evenly-Distribution Class Centroids (PEDCC)-Loss. This involves calculating the cosines of three projection angles and the norm values of features, thereby identifying distinctive information for in-distribution (ID) and OOD data, which assists in OOD detection. Building upon this, we have modified the batch normalization (BN) and ReLU layer preceding the fully connected layer, diminishing their impact on the output feature distributions and thereby widening the distribution gap between ID and OOD data features. Our method requires only the training of the classification network model, eschewing any need for input pre-processing or specific OOD data pre-tuning. Extensive experiments on several benchmark datasets demonstrates that our approach delivers state-of-the-art performance. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：When a Relation Tells More Than a Concept: Exploring and Evaluating  Classifier Decisions with CoReX</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01661">https://arxiv.org/abs/2405.01661</a></p>
  <p><b>作者</b>：Bettina Finzel,  Patrick Hilme,  Johannes Rabold,  Ute Schmid</p>
  <p><b>备注</b>：preliminary version, submitted to Machine Learning</p>
  <p><b>关键词</b>：Convolutional Neural Networks, Neural Networks, Convolutional Neural, input features impact, features impact model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biomedicine, the presence of specific concepts (e.g., a certain type of cell) and of relations between concepts (e.g., one cell type is next to another) might be discriminative between classes (e.g., different types of tissue). Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image data sets and CNN architectures. Results show that CoReX explanations are faithful to the CNN model in terms of predictive outcomes. We further demonstrate that CoReX is a suitable tool for evaluating CNNs supporting identification and re-classification of incorrect or ambiguous classifications.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：S4: Self-Supervised Sensing Across the Spectrum</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01656">https://arxiv.org/abs/2405.01656</a></p>
  <p><b>作者</b>：Jayanth Shenoy,  Xinjian Davis Zhang,  Shlok Mehrotra,  Bill Tao,  Rem Yang,  Han Zhao,  Deepak Vasisht</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：land cover mapping, crop type classification, agricultural crop type, image time series, Satellite image time</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Satellite image time series (SITS) segmentation is crucial for many applications like environmental monitoring, land cover mapping and agricultural crop type classification. However, training models for SITS segmentation remains a challenging task due to the lack of abundant training data, which requires fine grained annotation. We propose S4 a new self-supervised pre-training approach that significantly reduces the requirement for labeled training data by utilizing two new insights: (a) Satellites capture images in different parts of the spectrum such as radio frequencies, and visible frequencies. (b) Satellite imagery is geo-registered allowing for fine-grained spatial alignment. We use these insights to formulate pre-training tasks in S4. We also curate m2s2-SITS, a large-scale dataset of unlabeled, spatially-aligned, multi-modal and geographic specific SITS that serves as representative pre-training data for S4. Finally, we evaluate S4 on multiple SITS segmentation datasets and demonstrate its efficacy against competing baselines while using limited labeled data.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Key Patches Are All You Need: A Multiple Instance Learning Framework For  Robust Medical Diagnosis</b></summary>
  <p><b>编号</b>：[277]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01654">https://arxiv.org/abs/2405.01654</a></p>
  <p><b>作者</b>：Diogo J. Araújo,  M. Rita Verdelho,  Alceu Bissoto,  Jacinto C. Nascimento,  Carlos Santiago,  Catarina Barata</p>
  <p><b>备注</b>：Accepted in DEF-AI-MIA Workshop@CVPR 2024</p>
  <p><b>关键词</b>：Deep learning models, revolutionized the field, medical image analysis, Deep learning, image analysis</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models have revolutionized the field of medical image analysis, due to their outstanding performances. However, they are sensitive to spurious correlations, often taking advantage of dataset bias to improve results for in-domain data, but jeopardizing their generalization capabilities. In this paper, we propose to limit the amount of information these models use to reach the final classification, by using a multiple instance learning (MIL) framework. MIL forces the model to use only a (small) subset of patches in the image, identifying discriminative regions. This mimics the clinical procedures, where medical decisions are based on localized findings. We evaluate our framework on two medical applications: skin cancer diagnosis using dermoscopy and breast cancer diagnosis using mammography. Our results show that using only a subset of the patches does not compromise diagnostic performance for in-domain data, compared to the baseline approaches. However, our approach is more robust to shifts in patient demographics, while also providing more detailed explanations about which regions contributed to the decision. Code is available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Explaining models relating objects and privacy</b></summary>
  <p><b>编号</b>：[280]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01646">https://arxiv.org/abs/2405.01646</a></p>
  <p><b>作者</b>：Alessio Xompero,  Myriam Bontonou,  Jean-Michel Arbona,  Emmanouil Benetos,  Andrea Cavallaro</p>
  <p><b>备注</b>：7 pages, 3 figures, 1 table, supplementary material included as Appendix. Paper accepted at the 3rd XAI4CV Workshop at CVPR 2024. Code: this https URL</p>
  <p><b>关键词</b>：Accurately predicting, sharing it online, online is difficult, difficult due, vast variety</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Accurately predicting whether an image is private before sharing it online is difficult due to the vast variety of content and the subjective nature of privacy itself. In this paper, we evaluate privacy models that use objects extracted from an image to determine why the image is predicted as private. To explain the decision of these models, we use feature-attribution to identify and quantify which objects (and which of their features) are more relevant to privacy classification with respect to a reference input (i.e., no objects localised in an image) predicted as public. We show that the presence of the person category and its cardinality is the main factor for the privacy decision. Therefore, these models mostly fail to identify private images depicting documents with sensitive data, vehicle ownership, and internet activity, or public images with people (e.g., an outdoor concert or people walking in a public space next to a famous landmark). As baselines for future benchmarks, we also devise two strategies that are based on the person presence and cardinality and achieve comparable classification performance of the privacy models.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Explainable AI (XAI) in Image Segmentation in Medicine, Industry, and  Beyond: A Survey</b></summary>
  <p><b>编号</b>：[281]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01636">https://arxiv.org/abs/2405.01636</a></p>
  <p><b>作者</b>：Rokas Gipiškis,  Chun-Wei Tsai,  Olga Kurasova</p>
  <p><b>备注</b>：35 pages, 9 figures, 2 tables</p>
  <p><b>关键词</b>：Artificial Intelligence, found numerous applications, computer vision, found numerous, Intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial Intelligence (XAI) has found numerous applications in computer vision. While image classification-based explainability techniques have garnered significant attention, their counterparts in semantic segmentation have been relatively neglected. Given the prevalent use of image segmentation, ranging from medical to industrial deployments, these techniques warrant a systematic look. In this paper, we present the first comprehensive survey on XAI in semantic image segmentation. This work focuses on techniques that were either specifically introduced for dense prediction tasks or were extended for them by modifying existing methods in classification. We analyze and categorize the literature based on application categories and domains, as well as the evaluation metrics and datasets used. We also propose a taxonomy for interpretable semantic segmentation, and discuss potential challenges and future research directions.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：Wildfire Risk Prediction: A Review</b></summary>
  <p><b>编号</b>：[292]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01607">https://arxiv.org/abs/2405.01607</a></p>
  <p><b>作者</b>：Zhengsen Xu,  Jonathan Li,  Linlin Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：independent variables, global vegetation, significant impacts, impacts on global, independent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wildfires have significant impacts on global vegetation, wildlife, and humans. They destroy plant communities and wildlife habitats and contribute to increased emissions of carbon dioxide, nitrogen oxides, methane, and other pollutants. The prediction of wildfires relies on various independent variables combined with regression or machine learning methods. In this technical review, we describe the options for independent variables, data processing techniques, models, independent variables collinearity and importance estimation methods, and model performance evaluation metrics. First, we divide the independent variables into 4 aspects, including climate and meteorology conditions, socio-economical factors, terrain and hydrological features, and wildfire historical records. Second, preprocessing methods are described for different magnitudes, different spatial-temporal resolutions, and different formats of data. Third, the collinearity and importance evaluation methods of independent variables are also considered. Fourth, we discuss the application of statistical models, traditional machine learning models, and deep learning models in wildfire risk prediction. In this subsection, compared with other reviews, this manuscript particularly discusses the evaluation metrics and recent advancements in deep learning methods. Lastly, addressing the limitations of current research, this paper emphasizes the need for more effective deep learning time series forecasting algorithms, the utilization of three-dimensional data including ground and trunk fuel, extraction of more accurate historical fire point data, and improved model evaluation metrics.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Improve Academic Query Resolution through BERT-based Question Extraction  from Images</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01587">https://arxiv.org/abs/2405.01587</a></p>
  <p><b>作者</b>：Nidhi Kamal,  Saurabh Yadav,  Jorawar Singh,  Aditi Avasthi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Providing fast, essential solution provided, fast and accurate, Edtech organizations, essential solution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology  with Multimodal Learning</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01583">https://arxiv.org/abs/2405.01583</a></p>
  <p><b>作者</b>：Nadia Saeed</p>
  <p><b>备注</b>：7 pages, 3 figures, Clinical NLP 2024 workshop proceedings in Shared Task</p>
  <p><b>关键词</b>：wai Yim, challenge necessitates, necessitates novel solutions, Multimodal Medical Answer, Yim</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual & Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a). This paper addresses the limitations of traditional methods by proposing a weakly supervised learning approach for open-ended medical question-answering (QA). Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations. Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion. This approach tackles complex, open-ended questions even without predefined answer choices. We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images. This work advances medical QA research, paving the way for clinical decision support systems and ultimately improving healthcare delivery.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Configurable Learned Holography</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01558">https://arxiv.org/abs/2405.01558</a></p>
  <p><b>作者</b>：Yicheng Zhan,  Liang Shi,  Wojciech Matusik,  Qi Sun,  Kaan Akşit</p>
  <p><b>备注</b>：14 pages, 5 figures</p>
  <p><b>关键词</b>：holographic display technology, advancing holographic display, holographic displays, existing holographic displays, persistent roadblock</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the pursuit of advancing holographic display technology, we face a unique yet persistent roadblock: the inflexibility of learned holography in adapting to various hardware configurations.
This is due to the variances in the complex optical components and system settings in existing holographic displays.
Although the emerging learned approaches have enabled rapid and high-quality hologram generation, any alteration in display hardware still requires a retraining of the model.
Our work introduces a configurable learned model that interactively computes 3D holograms from RGB-only 2D images for a variety of holographic displays.
The model can be conditioned to predefined hardware parameters of existing holographic displays such as working wavelengths, pixel pitch, propagation distance, and peak brightness without having to retrain.
In addition, our model accommodates various hologram types, including conventional single-color and emerging multi-color holograms that simultaneously use multiple color primaries in holographic displays.
Notably, we enabled our hologram computations to rely on identifying the correlation between depth estimation and 3D hologram synthesis tasks within the learning domain for the first time in the literature.
We employ knowledge distillation via a student-teacher learning strategy to streamline our model for interactive performance.
Achieving up to a 2x speed improvement compared to state-of-the-art models while consistently generating high-quality 3D holograms with different hardware configurations.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Reference-Free Image Quality Metric for Degradation and Reconstruction  Artifacts</b></summary>
  <p><b>编号</b>：[345]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02208">https://arxiv.org/abs/2405.02208</a></p>
  <p><b>作者</b>：Han Cui,  Alfredo De Goyeneche,  Efrat Shimron,  Boyuan Ma,  Michael Lustig</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Computer Vision tasks, Computer Vision, Image Quality Assessment, Quality Assessment, Image Quality</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image Quality Assessment (IQA) is essential in various Computer Vision tasks such as image deblurring and super-resolution. However, most IQA methods require reference images, which are not always available. While there are some reference-free IQA metrics, they have limitations in simulating human perception and discerning subtle image quality variations. We hypothesize that the JPEG quality factor is representatives of image quality measurement, and a well-trained neural network can learn to accurately evaluate image quality without requiring a clean reference, as it can recognize image degradation artifacts based on prior knowledge. Thus, we developed a reference-free quality evaluation network, dubbed "Quality Factor (QF) Predictor", which does not require any reference. Our QF Predictor is a lightweight, fully convolutional network comprising seven layers. The model is trained in a self-supervised manner: it receives JPEG compressed image patch with a random QF as input, is trained to accurately predict the corresponding QF. We demonstrate the versatility of the model by applying it to various tasks. First, our QF Predictor can generalize to measure the severity of various image artifacts, such as Gaussian Blur and Gaussian noise. Second, we show that the QF Predictor can be trained to predict the undersampling rate of images reconstructed from Magnetic Resonance Imaging (MRI) data.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Three-Dimensional Amyloid-Beta PET Synthesis from Structural MRI with  Conditional Generative Adversarial Networks</b></summary>
  <p><b>编号</b>：[350]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02109">https://arxiv.org/abs/2405.02109</a></p>
  <p><b>作者</b>：Fernando Vega,  Abdoljalil Addeh,  M. Ethan MacDonald</p>
  <p><b>备注</b>：Abstract Submitted and Presented at the 2024 International Society of Magnetic Resonance in Medicine. Singapore, Singapore, May 4-9. Abstract Number 2239</p>
  <p><b>关键词</b>：Alzheimer Disease hallmarks, Disease hallmarks include, Alzheimer Disease, Disease hallmarks, amyloid-beta PET images</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Motivation: Alzheimer's Disease hallmarks include amyloid-beta deposits and brain atrophy, detectable via PET and MRI scans, respectively. PET is expensive, invasive and exposes patients to ionizing radiation. MRI is cheaper, non-invasive, and free from ionizing radiation but limited to measuring brain atrophy.
Goal: To develop an 3D image translation model that synthesizes amyloid-beta PET images from T1-weighted MRI, exploiting the known relationship between amyloid-beta and brain atrophy.
Approach: The model was trained on 616 PET/MRI pairs and validated with 264 pairs.
Results: The model synthesized amyloid-beta PET images from T1-weighted MRI with high-degree of similarity showing high SSIM and PSNR metrics (SSIM>0.95&PSNR=28).
Impact: Our model proves the feasibility of synthesizing amyloid-beta PET images from structural MRI ones, significantly enhancing accessibility for large-cohort studies and early dementia detection, while also reducing cost, invasiveness, and radiation exposure.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Report on the AAPM Grand Challenge on deep generative modeling for  learning medical image statistics</b></summary>
  <p><b>编号</b>：[364]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01822">https://arxiv.org/abs/2405.01822</a></p>
  <p><b>作者</b>：Rucha Deshpande,  Varun A. Kelkar,  Dimitrios Gotsis,  Prabhat Kc,  Rongping Zeng,  Kyle J. Myers,  Frank J. Brooks,  Mark A. Anastasio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：AAPM Grand Challenge, Deep Generative Modeling, Learning Medical Image, Special Report, Modeling for Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The findings of the 2023 AAPM Grand Challenge on Deep Generative Modeling for Learning Medical Image Statistics are reported in this Special Report. The goal of this challenge was to promote the development of deep generative models (DGMs) for medical imaging and to emphasize the need for their domain-relevant assessment via the analysis of relevant image statistics. As part of this Grand Challenge, a training dataset was developed based on 3D anthropomorphic breast phantoms from the VICTRE virtual imaging toolbox. A two-stage evaluation procedure consisting of a preliminary check for memorization and image quality (based on the Frechet Inception distance (FID)), and a second stage evaluating the reproducibility of image statistics corresponding to domain-relevant radiomic features was developed. A summary measure was employed to rank the submissions. Additional analyses of submissions was performed to assess DGM performance specific to individual feature families, and to identify various artifacts. 58 submissions from 12 unique users were received for this Challenge. The top-ranked submission employed a conditional latent diffusion model, whereas the joint runners-up employed a generative adversarial network, followed by another network for image superresolution. We observed that the overall ranking of the top 9 submissions according to our evaluation method (i) did not match the FID-based ranking, and (ii) differed with respect to individual feature families. Another important finding from our additional analyses was that different DGMs demonstrated similar kinds of artifacts. This Grand Challenge highlighted the need for domain-specific evaluation to further DGM design as well as deployment. It also demonstrated that the specification of a DGM may differ depending on its intended use.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：PointCompress3D -- A Point Cloud Compression Framework for Roadside  LiDARs in Intelligent Transportation Systems</b></summary>
  <p><b>编号</b>：[368]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01750">https://arxiv.org/abs/2405.01750</a></p>
  <p><b>作者</b>：Walter Zimmer,  Ramandika Pranamulia,  Xingcheng Zhou,  Mingyu Liu,  Alois C. Knoll</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Intelligent Transportation Systems, Intelligent Transportation, context of Intelligent, managing large-scale point, point cloud data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the context of Intelligent Transportation Systems (ITS), efficient data compression is crucial for managing large-scale point cloud data acquired by roadside LiDAR sensors. The demand for efficient storage, streaming, and real-time object detection capabilities for point cloud data is substantial. This work introduces PointCompress3D, a novel point cloud compression framework tailored specifically for roadside LiDARs. Our framework addresses the challenges of compressing high-resolution point clouds while maintaining accuracy and compatibility with roadside LiDAR sensors. We adapt, extend, integrate, and evaluate three cutting-edge compression methods using our real-world-based TUMTraf dataset family. We achieve a frame rate of 10 FPS while keeping compression sizes below 105 Kb, a reduction of 50 times, and maintaining object detection performance on par with the original data. In extensive experiments and ablation studies, we finally achieved a PSNR d2 of 94.46 and a BPP of 6.54 on our dataset. Future work includes the deployment on the live system. The code is available on our project website: this https URL.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral  Image Denoising</b></summary>
  <p><b>编号</b>：[371]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01726">https://arxiv.org/abs/2405.01726</a></p>
  <p><b>作者</b>：Guanyiman Fu,  Fengchao Xiong,  Jianfeng Lu,  Jun Zhou,  Yuntao Qian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：crucial preprocessing procedure, preprocessing procedure due, environmental factors, crucial preprocessing, preprocessing procedure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Denoising hyperspectral images (HSIs) is a crucial preprocessing procedure due to the noise originating from intra-imaging mechanisms and environmental factors. Utilizing domain-specific knowledge of HSIs, such as spectral correlation, spatial self-similarity, and spatial-spectral correlation, is essential for deep learning-based denoising. Existing methods are often constrained by running time, space complexity, and computational complexity, employing strategies that explore these priors separately. While the strategies can avoid some redundant information, considering that hyperspectral images are 3-D images with strong spatial continuity and spectral correlation, this kind of strategy inevitably overlooks subtle long-range spatial-spectral information that positively impacts image restoration. This paper proposes a Spatial-Spectral Selective State Space Model-based U-shaped network, termed Spatial-Spectral U-Mamba (SSUMamba), for hyperspectral image denoising. We can obtain complete global spatial-spectral correlation within a module thanks to the linear space complexity in State Space Model (SSM) computations. We introduce an Alternating Scan (SSAS) strategy for HSI data, which helps model the information flow in multiple directions in 3-D HSIs. Experimental results demonstrate that our method outperforms several compared methods. The source code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Development of Skip Connection in Deep Neural Networks for Computer  Vision and Medical Image Analysis: A Survey</b></summary>
  <p><b>编号</b>：[372]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01725">https://arxiv.org/abs/2405.01725</a></p>
  <p><b>作者</b>：Guoping Xu,  Xiaxia Wang,  Xinglong Wu,  Xuesong Leng,  Yongchao Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made significant progress, skip connections, deep neural networks, residual learning, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has made significant progress in computer vision, specifically in image classification, object detection, and semantic segmentation. The skip connection has played an essential role in the architecture of deep neural networks,enabling easier optimization through residual learning during the training stage and improving accuracy during testing. Many neural networks have inherited the idea of residual learning with skip connections for various tasks, and it has been the standard choice for designing neural networks. This survey provides a comprehensive summary and outlook on the development of skip connections in deep neural networks. The short history of skip connections is outlined, and the development of residual learning in deep neural networks is surveyed. The effectiveness of skip connections in the training and testing stages is summarized, and future directions for using skip connections in residual learning are discussed. Finally, we summarize seminal papers, source code, models, and datasets that utilize skip connections in computer vision, including image classification, object detection, semantic segmentation, and image reconstruction. We hope this survey could inspire peer researchers in the community to develop further skip connections in various forms and tasks and the theory of residual learning in deep neural networks. The project page can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：MMIST-ccRCC: A Real World Medical Dataset for the Development of  Multi-Modal Systems</b></summary>
  <p><b>编号</b>：[374]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01658">https://arxiv.org/abs/2405.01658</a></p>
  <p><b>作者</b>：Tiago Mota,  M. Rita Verdelho,  Alceu Bissoto,  Carlos Santiago,  Catarina Barata</p>
  <p><b>备注</b>：Accepted in DCA in MI Workshop@CVPR2024</p>
  <p><b>关键词</b>：personalized healthcare, enhance our knowledge, knowledge and understanding, modalities, missing modalities</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The acquisition of different data modalities can enhance our knowledge and understanding of various diseases, paving the way for a more personalized healthcare. Thus, medicine is progressively moving towards the generation of massive amounts of multi-modal data (\emph{e.g,} molecular, radiology, and histopathology). While this may seem like an ideal environment to capitalize data-centric machine learning approaches, most methods still focus on exploring a single or a pair of modalities due to a variety of reasons: i) lack of ready to use curated datasets; ii) difficulty in identifying the best multi-modal fusion strategy; and iii) missing modalities across patients. In this paper we introduce a real world multi-modal dataset called MMIST-CCRCC that comprises 2 radiology modalities (CT and MRI), histopathology, genomics, and clinical data from 618 patients with clear cell renal cell carcinoma (ccRCC). We provide single and multi-modal (early and late fusion) benchmarks in the task of 12-month survival prediction in the challenging scenario of one or more missing modalities for each patient, with missing rates that range from 26$\%$ for genomics data to more than 90$\%$ for MRI. We show that even with such severe missing rates the fusion of modalities leads to improvements in the survival forecasting. Additionally, incorporating a strategy to generate the latent representations of the missing modalities given the available ones further improves the performance, highlighting a potential complementarity across modalities. Our dataset and code are available here: this https URL</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：A Classification-Based Adaptive Segmentation Pipeline: Feasibility Study  Using Polycystic Liver Disease and Metastases from Colorectal Cancer CT  Images</b></summary>
  <p><b>编号</b>：[375]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01644">https://arxiv.org/abs/2405.01644</a></p>
  <p><b>作者</b>：Peilong Wang,  Timothy L. Kline,  Andy D. Missert,  Cole J. Cook,  Matthew R. Callstrom,  Alex Chan,  Robert P. Hartman,  Zachary S. Kelm,  Panagiotis Korfiatis</p>
  <p><b>备注</b>：J Digit Imaging. Inform. med. (2024)</p>
  <p><b>关键词</b>：Automated segmentation tools, tools often encounter, encounter accuracy, accuracy and adaptability, adaptability issues</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated segmentation tools often encounter accuracy and adaptability issues when applied to images of different pathology. The purpose of this study is to explore the feasibility of building a workflow to efficiently route images to specifically trained segmentation models. By implementing a deep learning classifier to automatically classify the images and route them to appropriate segmentation models, we hope that our workflow can segment the images with different pathology accurately. The data we used in this study are 350 CT images from patients affected by polycystic liver disease and 350 CT images from patients presenting with liver metastases from colorectal cancer. All images had the liver manually segmented by trained imaging analysts. Our proposed adaptive segmentation workflow achieved a statistically significant improvement for the task of total liver segmentation compared to the generic single segmentation model (non-parametric Wilcoxon signed rank test, n=100, p-value << 0.001). This approach is applicable in a wide range of scenarios and should prove useful in clinical implementations of segmentation pipelines.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Deep Learning Descriptor Hybridization with Feature Reduction for  Accurate Cervical Cancer Colposcopy Image Classification</b></summary>
  <p><b>编号</b>：[379]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01600">https://arxiv.org/abs/2405.01600</a></p>
  <p><b>作者</b>：Saurabh Saini,  Kapil Ahuja,  Siddartha Chennareddy,  Karthik Boddupalli</p>
  <p><b>备注</b>：7 Pages double column, 5 figures, and 5 tables</p>
  <p><b>关键词</b>：enable early diagnosis, Cervical cancer stands, female mortality, pre-cancerous conditions, regular screenings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cervical cancer stands as a predominant cause of female mortality, underscoring the need for regular screenings to enable early diagnosis and preemptive treatment of pre-cancerous conditions. The transformation zone in the cervix, where cellular differentiation occurs, plays a critical role in the detection of abnormalities. Colposcopy has emerged as a pivotal tool in cervical cancer prevention since it provides a meticulous examination of cervical abnormalities. However, challenges in visual evaluation necessitate the development of Computer Aided Diagnosis (CAD) systems.
We propose a novel CAD system that combines the strengths of various deep-learning descriptors (ResNet50, ResNet101, and ResNet152) with appropriate feature normalization (min-max) as well as feature reduction technique (LDA). The combination of different descriptors ensures that all the features (low-level like edges and colour, high-level like shape and texture) are captured, feature normalization prevents biased learning, and feature reduction avoids overfitting. We do experiments on the IARC dataset provided by WHO. The dataset is initially segmented and balanced. Our approach achieves exceptional performance in the range of 97%-100% for both the normal-abnormal and the type classification. A competitive approach for type classification on the same dataset achieved 81%-91% performance.</p>
  </details>
</details>
<h1>机器学习</h1>
<details>
  <summary>1. <b>标题：Structural Pruning of Pre-trained Language Models via Neural  Architecture Search</b></summary>
  <p><b>编号</b>：[4]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02267">https://arxiv.org/abs/2405.02267</a></p>
  <p><b>作者</b>：Aaron Klein,  Jacek Golebiowski,  Xingchen Ma,  Valerio Perrone,  Cedric Archambeau</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language understanding, language understanding task, Pre-trained language models, BERT or RoBERTa, Pre-trained language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state-of-the-art for natural language understanding task when fine-tuned on labeled data. However, their large size poses challenges in deploying them for inference in real-world applications, due to significant GPU memory requirements and high inference latency. This paper explores neural architecture search (NAS) for structural pruning to find sub-parts of the fine-tuned network that optimally trade-off efficiency, for example in terms of model size or latency, and generalization performance. We also show how we can utilize more recently developed two-stage weight-sharing NAS approaches in this setting to accelerate the search process. Unlike traditional pruning methods with fixed thresholds, we propose to adopt a multi-objective approach that identifies the Pareto optimal set of sub-networks, allowing for a more flexible and automated compression process.</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：Subgraph2vec: A random walk-based algorithm for embedding knowledge  graphs</b></summary>
  <p><b>编号</b>：[13]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02240">https://arxiv.org/abs/2405.02240</a></p>
  <p><b>作者</b>：Elika Bozorgi,  Saber Soleimani,  Sakher Khalil Alqaiidi,  Hamid Reza Arabnia,  Krzysztof Kochut</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：real world applications, cite, important data representation, world applications, important data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Graph is an important data representation which occurs naturally in the real world applications \cite{goyal2018graph}. Therefore, analyzing graphs provides users with better insights in different areas such as anomaly detection \cite{ma2021comprehensive}, decision making \cite{fan2023graph}, clustering \cite{tsitsulin2023graph}, classification \cite{wang2021mixup} and etc. However, most of these methods require high levels of computational time and space. We can use other ways like embedding to reduce these costs. Knowledge graph (KG) embedding is a technique that aims to achieve the vector representation of a KG. It represents entities and relations of a KG in a low-dimensional space while maintaining the semantic meanings of them. There are different methods for embedding graphs including random walk-based methods such as node2vec, metapath2vec and regpattern2vec. However, most of these methods bias the walks based on a rigid pattern usually hard-coded in the algorithm. In this work, we introduce \textit{subgraph2vec} for embedding KGs where walks are run inside a user-defined subgraph. We use this embedding for link prediction and prove our method has better performance in most cases in comparison with the previous ones.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：Learning Optimal Deterministic Policies with Stochastic Policy Gradients</b></summary>
  <p><b>编号</b>：[16]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02235">https://arxiv.org/abs/2405.02235</a></p>
  <p><b>作者</b>：Alessandro Montenegro,  Marco Mussi,  Alberto Maria Metelli,  Matteo Papini</p>
  <p><b>备注</b>：Accepted to ICML 2024</p>
  <p><b>关键词</b>：continuous reinforcement learning, methods are successful, successful approaches, approaches to deal, deal with continuous</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Policy gradient (PG) methods are successful approaches to deal with continuous reinforcement learning (RL) problems. They learn stochastic parametric (hyper)policies by either exploring in the space of actions or in the space of parameters. Stochastic controllers, however, are often undesirable from a practical perspective because of their lack of robustness, safety, and traceability. In common practice, stochastic (hyper)policies are learned only to deploy their deterministic version. In this paper, we make a step towards the theoretical understanding of this practice. After introducing a novel framework for modeling this scenario, we study the global convergence to the best deterministic policy, under (weak) gradient domination assumptions. Then, we illustrate how to tune the exploration level used for learning to optimize the trade-off between the sample complexity and the performance of the deployed deterministic policy. Finally, we quantitatively compare action-based and parameter-based exploration, giving a formal guise to intuitive results.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Discretization Error of Fourier Neural Operators</b></summary>
  <p><b>编号</b>：[20]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02221">https://arxiv.org/abs/2405.02221</a></p>
  <p><b>作者</b>：Samuel Lanthaler,  Andrew M. Stuart,  Margaret Trautner</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Fourier Neural Operator, Operator learning, variant of machine, designed to approximate, Neural Operator</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Operator learning is a variant of machine learning that is designed to approximate maps between function spaces from data. The Fourier Neural Operator (FNO) is a common model architecture used for operator learning. The FNO combines pointwise linear and nonlinear operations in physical space with pointwise linear operations in Fourier space, leading to a parameterized map acting between function spaces. Although FNOs formally involve convolutions of functions on a continuum, in practice the computations are performed on a discretized grid, allowing efficient implementation via the FFT. In this paper, the aliasing error that results from such a discretization is quantified and algebraic rates of convergence in terms of the grid resolution are obtained as a function of the regularity of the input. Numerical experiments that validate the theory and describe model stability are performed.</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Designed Dithering Sign Activation for Binary Neural Networks</b></summary>
  <p><b>编号</b>：[21]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02220">https://arxiv.org/abs/2405.02220</a></p>
  <p><b>作者</b>：Brayan Monroy,  Juan Estupiñan,  Tatiana Gelvez-Barrera,  Jorge Bacca,  Henry Arguello</p>
  <p><b>备注</b>：7 pages</p>
  <p><b>关键词</b>：Sign activation function, computer vision tasks, Sign activation, Neural Networks emerged, Binary Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Binary Neural Networks emerged as a cost-effective and energy-efficient solution for computer vision tasks by binarizing either network weights or activations. However, common binary activations, such as the Sign activation function, abruptly binarize the values with a single threshold, losing fine-grained details in the feature outputs. This work proposes an activation that applies multiple thresholds following dithering principles, shifting the Sign activation function for each pixel according to a spatially periodic threshold kernel. Unlike literature methods, the shifting is defined jointly for a set of adjacent pixels, taking advantage of spatial correlations. Experiments over the classification task demonstrate the effectiveness of the designed dithering Sign activation function as an alternative activation for binary neural networks, without increasing the computational cost. Further, DeSign balances the preservation of details with the efficiency of binary operations.</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Automatic Programming: Large Language Models and Beyond</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02213">https://arxiv.org/abs/2405.02213</a></p>
  <p><b>作者</b>：Michael R. Lyu,  Baishakhi Ray,  Abhik Roychoudhury,  Shin Hwei Tan,  Patanamon Thongtanunam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, increasing popularity due, GitHub Copilot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：Position Paper: Rethinking Empirical Research in Machine Learning:  Addressing Epistemic and Methodological Challenges of Experimentation</b></summary>
  <p><b>编号</b>：[26]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02200">https://arxiv.org/abs/2405.02200</a></p>
  <p><b>作者</b>：Moritz Herrmann,  F. Julian D. Lange,  Katharina Eggensperger,  Giuseppe Casalicchio,  Marcel Wever,  Matthias Feurer,  David Rügamer,  Eyke Hüllermeier,  Anne-Laure Boulesteix,  Bernd Bischl</p>
  <p><b>备注</b>：Accepted for publication at ICML 2024</p>
  <p><b>关键词</b>：makes findings unreliable, machine learning, non-replicable results, makes findings, findings unreliable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We warn against a common but incomplete understanding of empirical research in machine learning (ML) that leads to non-replicable results, makes findings unreliable, and threatens to undermine progress in the field. To overcome this alarming situation, we call for more awareness of the plurality of ways of gaining knowledge experimentally but also of some epistemic limitations. In particular, we argue most current empirical ML research is fashioned as confirmatory research while it should rather be considered exploratory.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Impact of emoji exclusion on the performance of Arabic sarcasm detection  models</b></summary>
  <p><b>编号</b>：[29]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02195">https://arxiv.org/abs/2405.02195</a></p>
  <p><b>作者</b>：Ghalyah H. Aleryani,  Wael Deabes,  Khaled Albishre,  Alaa E. Abdel-Hakim</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：sarcasm detection, sarcasm, complex challenge, challenge of detecting, nature of sarcastic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The complex challenge of detecting sarcasm in Arabic speech on social media is increased by the language diversity and the nature of sarcastic expressions. There is a significant gap in the capability of existing models to effectively interpret sarcasm in Arabic, which mandates the necessity for more sophisticated and precise detection methods. In this paper, we investigate the impact of a fundamental preprocessing component on sarcasm speech detection. While emojis play a crucial role in mitigating the absence effect of body language and facial expressions in modern communication, their impact on automated text analysis, particularly in sarcasm detection, remains underexplored. We investigate the impact of emoji exclusion from datasets on the performance of sarcasm detection models in social media content for Arabic as a vocabulary-super rich language. This investigation includes the adaptation and enhancement of AraBERT pre-training models, specifically by excluding emojis, to improve sarcasm detection capabilities. We use AraBERT pre-training to refine the specified models, demonstrating that the removal of emojis can significantly boost the accuracy of sarcasm detection. This approach facilitates a more refined interpretation of language, eliminating the potential confusion introduced by non-textual elements. The evaluated AraBERT models, through the focused strategy of emoji removal, adeptly navigate the complexities of Arabic sarcasm. This study establishes new benchmarks in Arabic natural language processing and presents valuable insights for social media platforms.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Non-Destructive Peat Analysis using Hyperspectral Imaging and Machine  Learning</b></summary>
  <p><b>编号</b>：[30]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02191">https://arxiv.org/abs/2405.02191</a></p>
  <p><b>作者</b>：Yijun Yan,  Jinchang Ren,  Barry Harrison,  Oliver Lewis,  Yinhe Li,  Ping Ma</p>
  <p><b>备注</b>：4 pages,4 figures</p>
  <p><b>关键词</b>：imparts distinctive, final product, crucial component, distinctive and irreplaceable, irreplaceable flavours</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Peat, a crucial component in whisky production, imparts distinctive and irreplaceable flavours to the final product. However, the extraction of peat disrupts ancient ecosystems and releases significant amounts of carbon, contributing to climate change. This paper aims to address this issue by conducting a feasibility study on enhancing peat use efficiency in whisky manufacturing through non-destructive analysis using hyperspectral imaging. Results show that shot-wave infrared (SWIR) data is more effective for analyzing peat samples and predicting total phenol levels, with accuracies up to 99.81%.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：Metalearners for Ranking Treatment Effects</b></summary>
  <p><b>编号</b>：[33]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02183">https://arxiv.org/abs/2405.02183</a></p>
  <p><b>作者</b>：Toon Vanderschueren,  Wouter Verbeke,  Felipe Moraes,  Hugo Manuel Proença</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Efficiently allocating treatments, budget constraint constitutes, Efficiently allocating, constitutes an important, important challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Efficiently allocating treatments with a budget constraint constitutes an important challenge across various domains. In marketing, for example, the use of promotions to target potential customers and boost conversions is limited by the available budget. While much research focuses on estimating causal effects, there is relatively limited work on learning to allocate treatments while considering the operational context. Existing methods for uplift modeling or causal inference primarily estimate treatment effects, without considering how this relates to a profit maximizing allocation policy that respects budget constraints. The potential downside of using these methods is that the resulting predictive model is not aligned with the operational context. Therefore, prediction errors are propagated to the optimization of the budget allocation problem, subsequently leading to a suboptimal allocation policy. We propose an alternative approach based on learning to rank. Our proposed methodology directly learns an allocation policy by prioritizing instances in terms of their incremental profit. We propose an efficient sampling procedure for the optimization of the ranking model to scale our methodology to large-scale data sets. Theoretically, we show how learning to rank can maximize the area under a policy's incremental profit curve. Empirically, we validate our methodology and show its effectiveness in practice through a series of experiments on both synthetic and real-world data.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Imitation Learning in Discounted Linear MDPs without exploration  assumptions</b></summary>
  <p><b>编号</b>：[35]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02181">https://arxiv.org/abs/2405.02181</a></p>
  <p><b>作者</b>：Luca Viano,  Stratis Skoulakis,  Volkan Cevher</p>
  <p><b>备注</b>：Accepted at ICML 2024</p>
  <p><b>关键词</b>：MDPs dubbed ILARL, linear MDPs dubbed, number of trajectories, infinite horizon linear, greatly improves</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present a new algorithm for imitation learning in infinite horizon linear MDPs dubbed ILARL which greatly improves the bound on the number of trajectories that the learner needs to sample from the environment. In particular, we remove exploration assumptions required in previous works and we improve the dependence on the desired accuracy $\epsilon$ from $\mathcal{O}\br{\epsilon^{-5}}$ to $\mathcal{O}\br{\epsilon^{-4}}$. Our result relies on a connection between imitation learning and online learning in MDPs with adversarial losses. For the latter setting, we present the first result for infinite horizon linear MDP which may be of independent interest. Moreover, we are able to provide a strengthen result for the finite horizon case where we achieve $\mathcal{O}\br{\epsilon^{-2}}$. Numerical experiments with linear function approximation shows that ILARL outperforms other commonly used algorithms.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：A Flow-Based Model for Conditional and Probabilistic Electricity  Consumption Profile Generation and Prediction</b></summary>
  <p><b>编号</b>：[36]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02180">https://arxiv.org/abs/2405.02180</a></p>
  <p><b>作者</b>：Weijie Xia,  Chenguang Wang,  Peter Palensky,  Pedro P. Vergara</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Residential Load Profile, diverse low-carbon technologies, Convolutional Profile Flow, Full Convolutional Profile, deep generative models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Residential Load Profile (RLP) generation and prediction are critical for the operation and planning of distribution networks, particularly as diverse low-carbon technologies are increasingly integrated. This paper introduces a novel flow-based generative model, termed Full Convolutional Profile Flow (FCPFlow), which is uniquely designed for both conditional and unconditional RLP generation, and for probabilistic load forecasting. By introducing two new layers--the invertible linear layer and the invertible normalization layer--the proposed FCPFlow architecture shows three main advantages compared to traditional statistical and contemporary deep generative models: 1) it is well-suited for RLP generation under continuous conditions, such as varying weather and annual electricity consumption, 2) it shows superior scalability in different datasets compared to traditional statistical, and 3) it also demonstrates better modeling capabilities in capturing the complex correlation of RLPs compared with deep generative models.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02175">https://arxiv.org/abs/2405.02175</a></p>
  <p><b>作者</b>：Hsuvas Borkakoty,  Luis Espinosa-Anke</p>
  <p><b>备注</b>：Short paper</p>
  <p><b>关键词</b>：disinformation created deliberately, reference knowledge resources, created deliberately, recognised form, form of disinformation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hoaxes are a recognised form of disinformation created deliberately, with potential serious implications in the credibility of reference knowledge resources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that they often are written according to the official style guidelines. In this work, we first provide a systematic analysis of the similarities and discrepancies between legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a collection of 311 Hoax articles (from existing literature as well as official Wikipedia lists) alongside semantically similar real articles. We report results of binary classification experiments in the task of predicting whether a Wikipedia article is real or hoax, and analyze several settings as well as a range of language models. Our results suggest that detecting deceitful content in Wikipedia based on content alone, despite not having been explored much in the past, is a promising direction.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：Simulating the economic impact of rationality through reinforcement  learning and agent-based modelling</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02161">https://arxiv.org/abs/2405.02161</a></p>
  <p><b>作者</b>：Simone Brusatin,  Tommaso Padoan,  Andrea Coletta,  Domenico Delli Gatti,  Aldo Glielmo</p>
  <p><b>备注</b>：8 pages, 4 figures</p>
  <p><b>关键词</b>：traditional frameworks based, general equilibrium assumptions, equilibrium assumptions, limitations of traditional, Rational macro ABM</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Agent-based models (ABMs) are simulation models used in economics to overcome some of the limitations of traditional frameworks based on general equilibrium assumptions. However, agents within an ABM follow predetermined, not fully rational, behavioural rules which can be cumbersome to design and difficult to justify. Here we leverage multi-agent reinforcement learning (RL) to expand the capabilities of ABMs with the introduction of fully rational agents that learn their policy by interacting with the environment and maximising a reward function. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by extending a paradigmatic macro ABM from the economic literature. We show that gradually substituting ABM firms in the model with RL agents, trained to maximise profits, allows for a thorough study of the impact of rationality on the economy. We find that RL agents spontaneously learn three distinct strategies for maximising profits, with the optimal strategy depending on the level of market competition and rationality. We also find that RL agents with independent policies, and without the ability to communicate with each other, spontaneously learn to segregate into different strategic groups, thus increasing market power and overall profits. Finally, we find that a higher degree of rationality in the economy always improves the macroeconomic environment as measured by total output, depending on the specific rational policy, this can come at the cost of higher instability. Our R-MABM framework is general, it allows for stable multi-agent learning, and represents a principled and robust direction to extend existing economic simulators.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Neural Context Flows for Learning Generalizable Dynamical Systems</b></summary>
  <p><b>编号</b>：[49]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02154">https://arxiv.org/abs/2405.02154</a></p>
  <p><b>作者</b>：Roussel Desmond Nzoyem,  David A.W. Barton,  Tom Deakin</p>
  <p><b>备注</b>：14 pages, 5 figures</p>
  <p><b>关键词</b>：Ordinary Differential Equations, Differential Equations typically, Neural Ordinary Differential, Equations typically struggle, Ordinary Differential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Neural Ordinary Differential Equations typically struggle to generalize to new dynamical behaviors created by parameter changes in the underlying system, even when the dynamics are close to previously seen behaviors. The issue gets worse when the changing parameters are unobserved, i.e., their value or influence is not directly measurable when collecting data. We introduce Neural Context Flow (NCF), a framework that encodes said unobserved parameters in a latent context vector as input to a vector field. NCFs leverage differentiability of the vector field with respect to the parameters, along with first-order Taylor expansion to allow any context vector to influence trajectories from other parameters. We validate our method and compare it to established Multi-Task and Meta-Learning alternatives, showing competitive performance in mean squared error for in-domain and out-of-distribution evaluation on the Lotka-Volterra, Glycolytic Oscillator, and Gray-Scott problems. This study holds practical implications for foundational models in science and related areas that benefit from conditional neural ODEs. Our code is openly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Towards a Formal Creativity Theory: Preliminary results in Novelty and  Transformativeness</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02148">https://arxiv.org/abs/2405.02148</a></p>
  <p><b>作者</b>：Luís Espírito Santo,  Geraint Wiggins,  Amílcar Cardoso</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：goal of Computational, Formalizing creativity-related concepts, Computational Creativity, Formalizing creativity-related, Formal Learning Theory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Formalizing creativity-related concepts has been a long-term goal of Computational Creativity. To the same end, we explore Formal Learning Theory in the context of creativity. We provide an introduction to the main concepts of this framework and a re-interpretation of terms commonly found in creativity discussions, proposing formal definitions for novelty and transformational creativity. This formalisation marks the beginning of a research branch we call Formal Creativity Theory, exploring how learning can be included as preparation for exploratory behaviour and how learning is a key part of transformational creative behaviour. By employing these definitions, we argue that, while novelty is neither necessary nor sufficient for transformational creativity in general, when using an inspiring set, rather than a sequence of experiences, an agent actually requires novelty for transformational creativity to occur.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Multi-Objective Recommendation via Multivariate Policy Learning</b></summary>
  <p><b>编号</b>：[56]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02141">https://arxiv.org/abs/2405.02141</a></p>
  <p><b>作者</b>：Olivier Jeunen,  Jatin Mandav,  Ivan Potapov,  Nakul Agarwal,  Sourabh Vaid,  Wenzhe Shi,  Aleksei Ustimenko</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Real-world recommender systems, balance multiple objectives, North Star reward, Real-world recommender, recommender systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Real-world recommender systems often need to balance multiple objectives when deciding which recommendations to present to users. These include behavioural signals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g. diversity, fairness). Scalarisation methods are commonly used to handle this balancing task, where a weighted average of per-objective reward signals determines the final score used for ranking. Naturally, how these weights are computed exactly, is key to success for any online platform. We frame this as a decision-making task, where the scalarisation weights are actions taken to maximise an overall North Star reward (e.g. long-term user retention or growth). We extend existing policy learning methods to the continuous multivariate action domain, proposing to maximise a pessimistic lower bound on the North Star reward that the learnt policy will yield. Typical lower bounds based on normal approximations suffer from insufficient coverage, and we propose an efficient and effective policy-dependent correction for this. We provide guidance to design stochastic data collection policies, as well as highly sensitive reward signals. Empirical observations from simulations, offline and online experiments highlight the efficacy of our deployed approach.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：An Information Theoretic Perspective on Conformal Prediction</b></summary>
  <p><b>编号</b>：[57]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02140">https://arxiv.org/abs/2405.02140</a></p>
  <p><b>作者</b>：Alvaro H.C. Correia,  Fabio Valerio Massoli,  Christos Louizos,  Arash Behboodi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：distribution-free uncertainty estimation, uncertainty estimation framework, prediction sets guaranteed, constructs prediction sets, user-specified probability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Conformal Prediction (CP) is a distribution-free uncertainty estimation framework that constructs prediction sets guaranteed to contain the true answer with a user-specified probability. Intuitively, the size of the prediction set encodes a general notion of uncertainty, with larger sets associated with higher degrees of uncertainty. In this work, we leverage information theory to connect conformal prediction to other notions of uncertainty. More precisely, we prove three different ways to upper bound the intrinsic uncertainty, as described by the conditional entropy of the target variable given the inputs, by combining CP with information theoretical inequalities. Moreover, we demonstrate two direct and useful applications of such connection between conformal prediction and information theory: (i) more principled and effective conformal training objectives that generalize previous approaches and enable end-to-end training of machine learning models from scratch, and (ii) a natural mechanism to incorporate side information into conformal prediction. We empirically validate both applications in centralized and federated learning settings, showing our theoretical results translate to lower inefficiency (average prediction set size) for popular CP methods.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Can We Identify Unknown Audio Recording Environments in Forensic  Scenarios?</b></summary>
  <p><b>编号</b>：[64]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02119">https://arxiv.org/abs/2405.02119</a></p>
  <p><b>作者</b>：Denise Moussa,  Germans Hirsch,  Christian Riess</p>
  <p><b>备注</b>：This work has been submitted to the IEEE for possible publication</p>
  <p><b>关键词</b>：provide important evidence, important evidence, evidence in criminal, criminal investigations, recording</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Audio recordings may provide important evidence in criminal investigations. One such case is the forensic association of the recorded audio to the recording location. For example, a voice message may be the only investigative cue to narrow down the candidate sites for a crime. Up to now, several works provide tools for closed-set recording environment classification under relatively clean recording conditions. However, in forensic investigations, the candidate locations are case-specific. Thus, closed-set tools are not applicable without retraining on a sufficient amount of training samples for each case and respective candidate set. In addition, a forensic tool has to deal with audio material from uncontrolled sources with variable properties and quality.
In this work, we therefore attempt a major step towards practical forensic application scenarios. We propose a representation learning framework called EnvId, short for environment identification. EnvId avoids case-specific retraining. Instead, it is the first tool for robust few-shot classification of unseen environment locations. We demonstrate that EnvId can handle forensically challenging material. It provides good quality predictions even under unseen signal degradations, environment characteristics or recording position mismatches.
Our code and datasets will be made publicly available upon acceptance.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Forecasting Ferry Passenger Flow Using Long-Short Term Memory Neural  Networks</b></summary>
  <p><b>编号</b>：[70]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02098">https://arxiv.org/abs/2405.02098</a></p>
  <p><b>作者</b>：Daniel Fesalbon</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：LSTM-based Neural Networks', Neural Networks' capability, Neural Networks, port ferry passenger, Neural Networks model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With recent studies related to Neural Networks being used on different forecasting and time series investigations, this study aims to expand these contexts to ferry passenger traffic. The primary objective of the study is to investigate and evaluate an LSTM-based Neural Networks' capability to forecast ferry passengers of two ports in the Philippines. The proposed model's fitting and evaluation of the passenger flow forecasting of the two ports is based on monthly passenger traffic from 2016 to 2022 data that was acquired from the Philippine Ports Authority (PPA). This work uses Mean Absolute Percentage Error (MAPE) as its primary metric to evaluate the model's forecasting capability. The proposed LSTM-based Neural Networks model achieved 72% forecasting accuracy to the Batangas port ferry passenger data and 74% forecasting accuracy to the Mindoro port ferry passenger data. Using Keras and Scikit-learn Python libraries, this work concludes a reasonable forecasting performance of the presented LSTM model. Aside from these notable findings, this study also recommends further investigation and studies on employing other statistical, machine learning, and deep learning methods on forecasting ferry passenger flows.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Multi-level projection with exponential parallel speedup; Application to  sparse auto-encoders neural networks</b></summary>
  <p><b>编号</b>：[73]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02086">https://arxiv.org/abs/2405.02086</a></p>
  <p><b>作者</b>：Guillaume Perez,  Michel Barlaud</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：big, efficient structured projection, efficient structured, mathcal, ell</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The $\ell_{1,\infty}$ norm is an efficient structured projection but the complexity of the best algorithm is unfortunately $\mathcal{O}\big(n m \log(n m)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. In this paper, we propose a new bi-level projection method for which we show that the time complexity for the $\ell_{1,\infty}$ norm is only $\mathcal{O}\big(n m \big)$ for a matrix in $\mathbb{R}^{n\times m}$, and $\mathcal{O}\big(n + m \big)$ with full parallel power. We generalize our method to tensors and we propose a new multi-level projection, having an induced decomposition that yields a linear parallel speedup up to an exponential speedup factor, resulting in a time complexity lower-bounded by the sum of the dimensions. Experiments show that our bi-level $\ell_{1,\infty}$ projection is $2.5$ times faster than the actual fastest algorithm provided by \textit{Chu et. al.} while providing same accuracy and better sparsity in neural networks applications.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：A Mutual Information Perspective on Federated Contrastive Learning</b></summary>
  <p><b>编号</b>：[75]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02081">https://arxiv.org/abs/2405.02081</a></p>
  <p><b>作者</b>：Christos Louizos,  Matthias Reisser,  Denis Korzhenkov</p>
  <p><b>备注</b>：Published as a conference paper at ICLR 2024</p>
  <p><b>关键词</b>：multi-view mutual information, mutual information maximization, mutual information, investigate contrastive learning, global mutual information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We investigate contrastive learning in the federated setting through the lens of SimCLR and multi-view mutual information maximization. In doing so, we uncover a connection between contrastive representation learning and user verification; by adding a user verification loss to each client's local SimCLR loss we recover a lower bound to the global multi-view mutual information. To accommodate for the case of when some labelled data are available at the clients, we extend our SimCLR variant to the federated semi-supervised setting. We see that a supervised SimCLR objective can be obtained with two changes: a) the contrastive loss is computed between datapoints that share the same label and b) we require an additional auxiliary head that predicts the correct labels from either of the two views. Along with the proposed SimCLR extensions, we also study how different sources of non-i.i.d.-ness can impact the performance of federated unsupervised learning through global mutual information maximization; we find that a global objective is beneficial for some sources of non-i.i.d.-ness but can be detrimental for others. We empirically evaluate our proposed extensions in various tasks to validate our claims and furthermore demonstrate that our proposed modifications generalize to other pretraining methods.</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：A Federated Learning Benchmark on Tabular Data: Comparing Tree-Based  Models and Neural Networks</b></summary>
  <p><b>编号</b>：[79]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02074">https://arxiv.org/abs/2405.02074</a></p>
  <p><b>作者</b>：William Lindskog,  Christian Prehofer</p>
  <p><b>备注</b>：8 pages, 6 figures, 6 tables, FMEC 2023 (best paper)</p>
  <p><b>关键词</b>：Deep Neural Networks, learning models train, machine learning models, machine learning, Neural Networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) has lately gained traction as it addresses how machine learning models train on distributed datasets. FL was designed for parametric models, namely Deep Neural Networks (DNNs).Thus, it has shown promise on image and text tasks. However, FL for tabular data has received little attention. Tree-Based Models (TBMs) have been considered to perform better on tabular data and they are starting to see FL integrations. In this study, we benchmark federated TBMs and DNNs for horizontal FL, with varying data partitions, on 10 well-known tabular datasets. Our novel benchmark results indicates that current federated boosted TBMs perform better than federated DNNs in different data partitions. Furthermore, a federated XGBoost outperforms all other models. Lastly, we find that federated TBMs perform better than federated parametric models, even when increasing the number of clients significantly.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：Histogram-Based Federated XGBoost using Minimal Variance Sampling for  Federated Tabular Data</b></summary>
  <p><b>编号</b>：[83]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02067">https://arxiv.org/abs/2405.02067</a></p>
  <p><b>作者</b>：William Lindskog,  Christian Prehofer,  Sarandeep Singh</p>
  <p><b>备注</b>：6 figures, 5 tables, 8 pages, FLTA 2023 (together with FMEC 2023)</p>
  <p><b>关键词</b>：gained considerable traction, Federated Learning, considerable traction, received less attention, gained considerable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) has gained considerable traction, yet, for tabular data, FL has received less attention. Most FL research has focused on Neural Networks while Tree-Based Models (TBMs) such as XGBoost have historically performed better on tabular data. It has been shown that subsampling of training data when building trees can improve performance but it is an open problem whether such subsampling can improve performance in FL. In this paper, we evaluate a histogram-based federated XGBoost that uses Minimal Variance Sampling (MVS). We demonstrate the underlying algorithm and show that our model using MVS can improve performance in terms of accuracy and regression error in a federated setting. In our evaluation, our model using MVS performs better than uniform (random) sampling and no sampling at all. It achieves both outstanding local and global performance on a new set of federated tabular datasets. Federated XGBoost using MVS also outperforms centralized XGBoost in half of the studied cases.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Few-sample Variational Inference of Bayesian Neural Networks with  Arbitrary Nonlinearities</b></summary>
  <p><b>编号</b>：[85]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02063">https://arxiv.org/abs/2405.02063</a></p>
  <p><b>作者</b>：David J. Schodt</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：extend traditional neural, Bayesian Neural Networks, traditional neural networks, Monte Carlo sampling, Bayesian Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Bayesian Neural Networks (BNNs) extend traditional neural networks to provide uncertainties associated with their outputs. On the forward pass through a BNN, predictions (and their uncertainties) are made either by Monte Carlo sampling network weights from the learned posterior or by analytically propagating statistical moments through the network. Though flexible, Monte Carlo sampling is computationally expensive and can be infeasible or impractical under resource constraints or for large networks. While moment propagation can ameliorate the computational costs of BNN inference, it can be difficult or impossible for networks with arbitrary nonlinearities, thereby restricting the possible set of network layers permitted with such a scheme. In this work, we demonstrate a simple yet effective approach for propagating statistical moments through arbitrary nonlinearities with only 3 deterministic samples, enabling few-sample variational inference of BNNs without restricting the set of network layers used. Furthermore, we leverage this approach to demonstrate a novel nonlinear activation function that we use to inject physics-informed prior information into output nodes of a BNN.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Dyna-Style Learning with A Macroscopic Model for Vehicle Platooning in  Mixed-Autonomy Traffic</b></summary>
  <p><b>编号</b>：[86]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02062">https://arxiv.org/abs/2405.02062</a></p>
  <p><b>作者</b>：Yichuan Zou,  Li Jin,  Xi Xiong</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：autonomous vehicles, plays a vital, ushering in enhanced, connected and autonomous, vital role</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Platooning of connected and autonomous vehicles (CAVs) plays a vital role in modernizing highways, ushering in enhanced efficiency and safety. This paper explores the significance of platooning in smart highways, employing a coupled partial differential equation (PDE) and ordinary differential equation (ODE) model to elucidate the complex interaction between bulk traffic flow and CAV platoons. Our study focuses on developing a Dyna-style planning and learning framework tailored for platoon control, with a specific goal of reducing fuel consumption. By harnessing the coupled PDE-ODE model, we improve data efficiency in Dyna-style learning through virtual experiences. Simulation results validate the effectiveness of our macroscopic model in modeling platoons within mixed-autonomy settings, demonstrating a notable $10.11\%$ reduction in vehicular fuel consumption compared to conventional approaches.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Federated Learning for Tabular Data using TabNet: A Vehicular Use-Case</b></summary>
  <p><b>编号</b>：[88]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02060">https://arxiv.org/abs/2405.02060</a></p>
  <p><b>作者</b>：William Lindskog,  Christian Prehofer</p>
  <p><b>备注</b>：7 pages, 9 figures, 1 table, ICCP Conference 2022</p>
  <p><b>关键词</b>：Federated Learning, show how Federated, classify obstacles, irregularities and pavement, types on roads</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we show how Federated Learning (FL) can be applied to vehicular use-cases in which we seek to classify obstacles, irregularities and pavement types on roads. Our proposed framework utilizes FL and TabNet, a state-of-the-art neural network for tabular data. We are the first to demonstrate how TabNet can be integrated with FL. Moreover, we achieve a maximum test accuracy of 93.6%. Finally, we reason why FL is a suitable concept for this data set.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Zero-Sum Positional Differential Games as a Framework for Robust  Reinforcement Learning: Deep Q-Learning Approach</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02044">https://arxiv.org/abs/2405.02044</a></p>
  <p><b>作者</b>：Anton Plaksin,  Vitaly Kalev</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：promising Reinforcement Learning, Robust Reinforcement Learning, Reinforcement Learning, promising Reinforcement, Robust Reinforcement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust Reinforcement Learning (RRL) is a promising Reinforcement Learning (RL) paradigm aimed at training robust to uncertainty or disturbances models, making them more efficient for real-world applications. Following this paradigm, uncertainty or disturbances are interpreted as actions of a second adversarial agent, and thus, the problem is reduced to seeking the agents' policies robust to any opponent's actions. This paper is the first to propose considering the RRL problems within the positional differential game theory, which helps us to obtain theoretically justified intuition to develop a centralized Q-learning approach. Namely, we prove that under Isaacs's condition (sufficiently general for real-world dynamical systems), the same Q-function can be utilized as an approximate solution of both minimax and maximin Bellman equations. Based on these results, we present the Isaacs Deep Q-Network algorithms and demonstrate their superiority compared to other baseline RRL and Multi-Agent RL algorithms in various environments.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Stabilizing Backpropagation Through Time to Learn Complex Physics</b></summary>
  <p><b>编号</b>：[97]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02041">https://arxiv.org/abs/2405.02041</a></p>
  <p><b>作者</b>：Patrick Schnell,  Nils Thuerey</p>
  <p><b>备注</b>：Published at ICLR 2024, code available at this https URL</p>
  <p><b>关键词</b>：recurrent learning setups, vector fields surrounding, learning setups, efficient computability, recurrent learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Of all the vector fields surrounding the minima of recurrent learning setups, the gradient field with its exploding and vanishing updates appears a poor choice for optimization, offering little beyond efficient computability. We seek to improve this suboptimal practice in the context of physics simulations, where backpropagating feedback through many unrolled time steps is considered crucial to acquiring temporally coherent behavior. The alternative vector field we propose follows from two principles: physics simulators, unlike neural networks, have a balanced gradient flow, and certain modifications to the backpropagation pass leave the positions of the original minima unchanged. As any modification of backpropagation decouples forward and backward pass, the rotation-free character of the gradient field is lost. Therefore, we discuss the negative implications of using such a rotational vector field for optimization and how to counteract them. Our final procedure is easily implementable via a sequence of gradient stopping and component-wise comparison operations, which do not negatively affect scalability. Our experiments on three control problems show that especially as we increase the complexity of each task, the unbalanced updates from the gradient can no longer provide the precise control signals necessary while our method still solves the tasks. Our code can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Cooperation and Federation in Distributed Radar Point Cloud Processing</b></summary>
  <p><b>编号</b>：[115]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01995">https://arxiv.org/abs/2405.01995</a></p>
  <p><b>作者</b>：S. Savazzi,  V. Rampa,  S. Kianoush,  A. Minora,  L. Costa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：resource-constrained MIMO radars, low range-azimuth resolution, resource-constrained MIMO, MIMO radars, range-azimuth resolution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The paper considers the problem of human-scale RF sensing utilizing a network of resource-constrained MIMO radars with low range-azimuth resolution. The radars operate in the mmWave band and obtain time-varying 3D point cloud (PC) information that is sensitive to body movements. They also observe the same scene from different views and cooperate while sensing the environment using a sidelink communication channel. Conventional cooperation setups allow the radars to mutually exchange raw PC information to improve ego sensing. The paper proposes a federation mechanism where the radars exchange the parameters of a Bayesian posterior measure of the observed PCs, rather than raw data. The radars act as distributed parameter servers to reconstruct a global posterior (i.e., federated posterior) using Bayesian tools. The paper quantifies and compares the benefits of radar federation with respect to cooperation mechanisms. Both approaches are validated by experiments with a real-time demonstration platform. Federation makes minimal use of the sidelink communication channel (20 ÷ 25 times lower bandwidth use) and is less sensitive to unresolved targets. On the other hand, cooperation reduces the mean absolute target estimation error of about 20%.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：Soft Label PU Learning</b></summary>
  <p><b>编号</b>：[117]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01990">https://arxiv.org/abs/2405.01990</a></p>
  <p><b>作者</b>：Puning Zhao,  Jintao Deng,  Xu Cheng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：classification problem, learning, learning refers, unlabeled samples, samples are labeled</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>PU learning refers to the classification problem in which only part of positive samples are labeled. Existing PU learning methods treat unlabeled samples equally. However, in many real tasks, from common sense or domain knowledge, some unlabeled samples are more likely to be positive than others. In this paper, we propose soft label PU learning, in which unlabeled data are assigned soft labels according to their probabilities of being positive. Considering that the ground truth of TPR, FPR, and AUC are unknown, we then design PU counterparts of these metrics to evaluate the performances of soft label PU learning methods within validation data. We show that these new designed PU metrics are good substitutes for the real metrics. After that, a method that optimizes such metrics is proposed. Experiments on public datasets and real datasets for anti-cheat services from Tencent games demonstrate the effectiveness of our proposed method.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Joint sentiment analysis of lyrics and audio in music</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01988">https://arxiv.org/abs/2405.01988</a></p>
  <p><b>作者</b>：Lea Schaab,  Anna Kruspe</p>
  <p><b>备注</b>：published at DAGA 2024</p>
  <p><b>关键词</b>：levels in music, audio, lyrics, mood can express, Sentiment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sentiment or mood can express themselves on various levels in music. In automatic analysis, the actual audio data is usually analyzed, but the lyrics can also play a crucial role in the perception of moods. We first evaluate various models for sentiment analysis based on lyrics and audio separately. The corresponding approaches already show satisfactory results, but they also exhibit weaknesses, the causes of which we examine in more detail. Furthermore, different approaches to combining the audio and lyrics results are proposed and evaluated. Considering both modalities generally leads to improved performance. We investigate misclassifications and (also intentional) contradictions between audio and lyrics sentiment more closely, and identify possible causes. Finally, we address fundamental problems in this research area, such as high subjectivity, lack of data, and inconsistency in emotion taxonomies.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：Quantifying Distribution Shifts and Uncertainties for Enhanced Model  Robustness in Machine Learning Applications</b></summary>
  <p><b>编号</b>：[121]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01978">https://arxiv.org/abs/2405.01978</a></p>
  <p><b>作者</b>：Vegard Flovik</p>
  <p><b>备注</b>：Working paper</p>
  <p><b>关键词</b>：directly impact model, statistical properties differ, test datasets, properties differ, differ between training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Distribution shifts, where statistical properties differ between training and test datasets, present a significant challenge in real-world machine learning applications where they directly impact model generalization and robustness. In this study, we explore model adaptation and generalization by utilizing synthetic data to systematically address distributional disparities. Our investigation aims to identify the prerequisites for successful model adaptation across diverse data distributions, while quantifying the associated uncertainties. Specifically, we generate synthetic data using the Van der Waals equation for gases and employ quantitative measures such as Kullback-Leibler divergence, Jensen-Shannon distance, and Mahalanobis distance to assess data similarity. These metrics en able us to evaluate both model accuracy and quantify the associated uncertainty in predictions arising from data distribution shifts. Our findings suggest that utilizing statistical measures, such as the Mahalanobis distance, to determine whether model predictions fall within the low-error "interpolation regime" or the high-error "extrapolation regime" provides a complementary method for assessing distribution shift and model uncertainty. These insights hold significant value for enhancing model robustness and generalization, essential for the successful deployment of machine learning applications in real-world scenarios.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Conformal Prediction for Natural Language Processing: A Survey</b></summary>
  <p><b>编号</b>：[122]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01976">https://arxiv.org/abs/2405.01976</a></p>
  <p><b>作者</b>：Margarida M. Campos,  António Farinhas,  Chrysoula Zerva,  Mário A.T. Figueiredo,  André F.T. Martins</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：natural language processing, large language models, enhance decision-making reliability, language processing, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：Introducing a microstructure-embedded autoencoder approach for  reconstructing high-resolution solution field from reduced parametric space</b></summary>
  <p><b>编号</b>：[123]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01975">https://arxiv.org/abs/2405.01975</a></p>
  <p><b>作者</b>：Rasoul Najafi Koopas,  Shahed Rezaei,  Natalie Rauter,  Richard Ostwald,  Rolf Lammering</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multi-fidelity deep learning, parametric space information, incorporating parametric space, transforms low-fidelity solution, deep learning approach</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this study, we develop a novel multi-fidelity deep learning approach that transforms low-fidelity solution maps into high-fidelity ones by incorporating parametric space information into a standard autoencoder architecture. It is shown that, due to the integration of parametric space data, this method requires significantly less training data to achieve effective performance in predicting high-fidelity solution from the low-fidelity one. In this study, our focus is on a 2D steady-state heat transfer analysis in highly heterogeneous materials microstructure, where the spatial distribution of heat conductivity coefficients for two distinct materials is condensed. Subsequently, the boundary value problem is solved on the coarsest grid using a pre-trained physics-informed neural operator network. Afterward, the calculated low-fidelity result is upscaled using the newly designed enhanced autoencoder. The novelty of the developed enhanced autoencoder lies in the concatenation of heat conductivity maps of different resolutions to the decoder segment in distinct steps. We then compare the outcomes of developed algorithm with the corresponding finite element results, standard U-Net architecture as well as other upscaling approaches such as interpolation functions of varying orders and feedforward neural networks (FFNN). The analysis of the results based on the new approach demonstrates superior performance compared to other approaches in terms of computational cost and error on the test cases. Therefore, as a potential supplement to neural operators networks, our architecture upscales low-fidelity solutions to high-fidelity ones while preserving critical details that are often lost in conventional upscaling methods, especially at sharp interfaces, such as those encountered with interpolation methods.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：Multitask Extension of Geometrically Aligned Transfer Encoder</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01974">https://arxiv.org/abs/2405.01974</a></p>
  <p><b>作者</b>：Sung Moon Ko,  Sumin Lee,  Dae-Woong Jeong,  Hyunseung Kim,  Chanhui Lee,  Soorin Yim,  Sehui Han</p>
  <p><b>备注</b>：7 pages, 3 figures, 2 tables</p>
  <p><b>关键词</b>：datasets often suffer, Aligned Transfer Encoder, Geometrically Aligned Transfer, Molecular datasets, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Molecular datasets often suffer from a lack of data. It is well-known that gathering data is difficult due to the complexity of experimentation or simulation involved. Here, we leverage mutual information across different tasks in molecular data to address this issue. We extend an algorithm that utilizes the geometric characteristics of the encoding space, known as the Geometrically Aligned Transfer Encoder (GATE), to a multi-task setup. Thus, we connect multiple molecular tasks by aligning the curved coordinates onto locally flat coordinates, ensuring the flow of information from source tasks to support performance on target data.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：From Attack to Defense: Insights into Deep Learning Security Measures in  Black-Box Settings</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01963">https://arxiv.org/abs/2405.01963</a></p>
  <p><b>作者</b>：Firuz Juraev,  Mohammed Abuhamad,  Eric Chan-Tin,  George K. Thiruvathukal,  Tamer Abuhmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Learning, rapidly maturing, attacks, security-crucial applications, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Learning (DL) is rapidly maturing to the point that it can be used in safety- and security-crucial applications. However, adversarial samples, which are undetectable to the human eye, pose a serious threat that can cause the model to misbehave and compromise the performance of such applications. Addressing the robustness of DL models has become crucial to understanding and defending against adversarial attacks. In this study, we perform comprehensive experiments to examine the effect of adversarial attacks and defenses on various model architectures across well-known datasets. Our research focuses on black-box attacks such as SimBA, HopSkipJump, MGAAttack, and boundary attacks, as well as preprocessor-based defensive mechanisms, including bits squeezing, median smoothing, and JPEG filter. Experimenting with various models, our results demonstrate that the level of noise needed for the attack increases as the number of layers increases. Moreover, the attack success rate decreases as the number of layers increases. This indicates that model complexity and robustness have a significant relationship. Investigating the diversity and robustness relationship, our experiments with diverse models show that having a large number of parameters does not imply higher robustness. Our experiments extend to show the effects of the training dataset on model robustness. Using various datasets such as ImageNet-1000, CIFAR-100, and CIFAR-10 are used to evaluate the black-box attacks. Considering the multiple dimensions of our analysis, e.g., model complexity and training dataset, we examined the behavior of black-box attacks when models apply defenses. Our results show that applying defense strategies can significantly reduce attack effectiveness. This research provides in-depth analysis and insight into the robustness of DL models against various attacks, and defenses.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large  Language Models</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01943">https://arxiv.org/abs/2405.01943</a></p>
  <p><b>作者</b>：Zhiyu Guo,  Hidetaka Kamigaito,  Taro Wanatnabe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, advancement in Large, language understanding, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent SwiGLU-based LLMs pruning. Our approach incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on Mistral and LLaMA2 model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Impact of Architectural Modifications on Deep Learning Adversarial  Robustness</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01934">https://arxiv.org/abs/2405.01934</a></p>
  <p><b>作者</b>：Firuz Juraev,  Mohammed Abuhamad,  Simon S. Woo,  George K Thiruvathukal,  Tamer Abuhmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including safety-critical applications, deep learning models, deep learning, including safety-critical, self-driving vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rapid advancements of deep learning are accelerating adoption in a wide variety of applications, including safety-critical applications such as self-driving vehicles, drones, robots, and surveillance systems. These advancements include applying variations of sophisticated techniques that improve the performance of models. However, such models are not immune to adversarial manipulations, which can cause the system to misbehave and remain unnoticed by experts. The frequency of modifications to existing deep learning models necessitates thorough analysis to determine the impact on models' robustness. In this work, we present an experimental evaluation of the effects of model modifications on deep learning model robustness using adversarial attacks. Our methodology involves examining the robustness of variations of models against various adversarial attacks. By conducting our experiments, we aim to shed light on the critical issue of maintaining the reliability and safety of deep learning models in safety- and security-critical applications. Our results indicate the pressing demand for an in-depth assessment of the effects of model changes on the robustness of models.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：SlotGAT: Slot-based Message Passing for Heterogeneous Graph Neural  Network</b></summary>
  <p><b>编号</b>：[138]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01927">https://arxiv.org/abs/2405.01927</a></p>
  <p><b>作者</b>：Ziang Zhou,  Jieming Shi,  Renchi Yang,  Yuanhang Zou,  Qing Li</p>
  <p><b>备注</b>：Published as a conference paper at ICML 2023</p>
  <p><b>关键词</b>：model complex data, complex data, ubiquitous to model, model complex, powerful heterogeneous graph</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Heterogeneous graphs are ubiquitous to model complex data. There are urgent needs on powerful heterogeneous graph neural networks to effectively support important applications. We identify a potential semantic mixing issue in existing message passing processes, where the representations of the neighbors of a node $v$ are forced to be transformed to the feature space of $v$ for aggregation, though the neighbors are in different types. That is, the semantics in different node types are entangled together into node $v$'s representation. To address the issue, we propose SlotGAT with separate message passing processes in slots, one for each node type, to maintain the representations in their own node-type feature spaces. Moreover, in a slot-based message passing layer, we design an attention mechanism for effective slot-wise message aggregation. Further, we develop a slot attention technique after the last layer of SlotGAT, to learn the importance of different slots in downstream tasks. Our analysis indicates that the slots in SlotGAT can preserve different semantics in various feature spaces. The superiority of SlotGAT is evaluated against 13 baselines on 6 datasets for node classification and link prediction. Our code is at this https URL.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Instance-Conditioned Adaptation for Large-scale Generalization of Neural  Combinatorial Optimization</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01906">https://arxiv.org/abs/2405.01906</a></p>
  <p><b>作者</b>：Changliang Zhou,  Xi Lin,  Zhenkun Wang,  Xialiang Tong,  Mingxuan Yuan,  Qingfu Zhang</p>
  <p><b>备注</b>：17 pages, 6 figures</p>
  <p><b>关键词</b>：shown great potential, neural combinatorial optimization, approach has shown, combinatorial optimization, shown great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The neural combinatorial optimization (NCO) approach has shown great potential for solving routing problems without the requirement of expert knowledge. However, existing constructive NCO methods cannot directly solve large-scale instances, which significantly limits their application prospects. To address these crucial shortcomings, this work proposes a novel Instance-Conditioned Adaptation Model (ICAM) for better large-scale generalization of neural combinatorial optimization. In particular, we design a powerful yet lightweight instance-conditioned adaptation module for the NCO model to generate better solutions for instances across different scales. In addition, we develop an efficient three-stage reinforcement learning-based training scheme that enables the model to learn cross-scale features without any labeled optimal solution. Experimental results show that our proposed method is capable of obtaining excellent results with a very fast inference time in solving Traveling Salesman Problems (TSPs) and Capacitated Vehicle Routing Problems (CVRPs) across different scales. To the best of our knowledge, our model achieves state-of-the-art performance among all RL-based constructive methods for TSP and CVRP with up to 1,000 nodes.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：DALLMi: Domain Adaption for LLM-based Multi-label Classifier</b></summary>
  <p><b>编号</b>：[158]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01883">https://arxiv.org/abs/2405.01883</a></p>
  <p><b>作者</b>：Miruna Beţianu,  Abele Mălan,  Marco Aldinucci,  Robert Birke,  Lydia Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：domain adaptation, Large language models, Adaptation Large Language, increasingly serve, backbone for classifying</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large language models (LLMs) increasingly serve as the backbone for classifying text associated with distinct domains and simultaneously several labels (classes). When encountering domain shifts, e.g., classifier of movie reviews from IMDb to Rotten Tomatoes, adapting such an LLM-based multi-label classifier is challenging due to incomplete label sets at the target domain and daunting training overhead. The existing domain adaptation methods address either image multi-label classifiers or text binary classifiers. In this paper, we design DALLMi, Domain Adaptation Large Language Model interpolator, a first-of-its-kind semi-supervised domain adaptation method for text data models based on LLMs, specifically BERT. The core of DALLMi is the novel variation loss and MixUp regularization, which jointly leverage the limited positively labeled and large quantity of unlabeled text and, importantly, their interpolation from the BERT word embeddings. DALLMi also introduces a label-balanced sampling strategy to overcome the imbalance between labeled and unlabeled data. We evaluate DALLMi against the partial-supervised and unsupervised approach on three datasets under different scenarios of label availability for the target domain. Our results show that DALLMi achieves higher mAP than unsupervised and partially-supervised approaches by 19.9% and 52.2%, respectively.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Enhancing Bangla Language Next Word Prediction and Sentence Completion  through Extended RNN with Bi-LSTM Model On N-gram Language</b></summary>
  <p><b>编号</b>：[161]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01873">https://arxiv.org/abs/2405.01873</a></p>
  <p><b>作者</b>：Md Robiul Islam,  Al Amin,  Aniqua Nusrat Zereen</p>
  <p><b>备注</b>：This paper contains 6 pages, 8 figures</p>
  <p><b>关键词</b>：Texting stands, communication worldwide, prominent form, form of communication, Bangla</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Texting stands out as the most prominent form of communication worldwide. Individual spend significant amount of time writing whole texts to send emails or write something on social media, which is time consuming in this modern era. Word prediction and sentence completion will be suitable and appropriate in the Bangla language to make textual information easier and more convenient. This paper expands the scope of Bangla language processing by introducing a Bi-LSTM model that effectively handles Bangla next-word prediction and Bangla sentence generation, demonstrating its versatility and potential impact. We proposed a new Bi-LSTM model to predict a following word and complete a sentence. We constructed a corpus dataset from various news portals, including bdnews24, BBC News Bangla, and Prothom Alo. The proposed approach achieved superior results in word prediction, reaching 99\% accuracy for both 4-gram and 5-gram word predictions. Moreover, it demonstrated significant improvement over existing methods, achieving 35\%, 75\%, and 95\% accuracy for uni-gram, bi-gram, and tri-gram word prediction, respectively</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten  AI Research</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01859">https://arxiv.org/abs/2405.01859</a></p>
  <p><b>作者</b>：Riley Simmons-Edler,  Ryan Badman,  Shayne Longpre,  Kanaka Rajan</p>
  <p><b>备注</b>：9 pages, in ICML 2024</p>
  <p><b>关键词</b>：autonomous weapons systems, machine learning, weapons systems, autonomous weapons, free exchange</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent embrace of machine learning (ML) in the development of autonomous weapons systems (AWS) creates serious risks to geopolitical stability and the free exchange of ideas in AI research. This topic has received comparatively little attention of late compared to risks stemming from superintelligent artificial general intelligence (AGI), but requires fewer assumptions about the course of technological development and is thus a nearer-future issue. ML is already enabling the substitution of AWS for human soldiers in many battlefield roles, reducing the upfront human cost, and thus political cost, of waging offensive war. In the case of peer adversaries, this increases the likelihood of "low intensity" conflicts which risk escalation to broader warfare. In the case of non-peer adversaries, it reduces the domestic blowback to wars of aggression. This effect can occur regardless of other ethical issues around the use of military AI such as the risk of civilian casualties, and does not require any superhuman AI capabilities. Further, the military value of AWS raises the specter of an AI-powered arms race and the misguided imposition of national security restrictions on AI research. Our goal in this paper is to raise awareness among the public and ML researchers on the near-future risks posed by full or near-full autonomy in military technology, and we provide regulatory suggestions to mitigate these risks. We call upon AI policy experts and the defense AI community in particular to embrace transparency and caution in their development and deployment of AWS to avoid the negative effects on global stability and AI research that we highlight here.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Robust Explainable Recommendation</b></summary>
  <p><b>编号</b>：[169]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01855">https://arxiv.org/abs/2405.01855</a></p>
  <p><b>作者</b>：Sairamvinay Vijayaraghavan,  Prasant Mohapatra</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Explainable Recommender Systems, Recommender Systems, suggested recommendations, important field, field of study</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explainable Recommender Systems is an important field of study which provides reasons behind the suggested recommendations. Explanations with recommender systems are useful for developers while debugging anomalies within the system and for consumers while interpreting the model's effectiveness in capturing their true preferences towards items. However, most of the existing state-of-the-art (SOTA) explainable recommenders could not retain their explanation capability under noisy circumstances and moreover are not generalizable across different datasets. The robustness of the explanations must be ensured so that certain malicious attackers do not manipulate any high-stake decision scenarios to their advantage, which could cause severe consequences affecting large groups of interest. In this work, we present a general framework for feature-aware explainable recommenders that can withstand external attacks and provide robust and generalized explanations. This paper presents a novel framework which could be utilized as an additional defense tool, preserving the global explainability when subject to model-based white box attacks. Our framework is simple to implement and supports different methods regardless of the internal model structure and intrinsic utility within any model. We experimented our framework on two architecturally different feature-based SOTA explainable algorithms by training them on three popular e-commerce datasets of increasing scales. We noticed that both the algorithms displayed an overall improvement in the quality and robustness of the global explainability under normal as well as noisy environments across all the datasets, indicating the flexibility and mutability of our framework.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：Deep Learning Inference on Heterogeneous Mobile Processors: Potentials  and Pitfalls</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01851">https://arxiv.org/abs/2405.01851</a></p>
  <p><b>作者</b>：Sicong Liu,  Wentao Zhou,  Zimu Zhou,  Bin Guo,  Minfan Wang,  Cheng Fang,  Zheng Lin,  Zhiwen Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computation-intensive deep learning, real-time intelligent applications, deploy computation-intensive deep, deep learning, intelligent applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is a growing demand to deploy computation-intensive deep learning (DL) models on resource-constrained mobile devices for real-time intelligent applications. Equipped with a variety of processing units such as CPUs, GPUs, and NPUs, the mobile devices hold potential to accelerate DL inference via parallel execution across heterogeneous processors. Various efficient parallel methods have been explored to optimize computation distribution, achieve load balance, and minimize communication cost across processors. Yet their practical effectiveness in the dynamic and diverse real-world mobile environment is less explored. This paper presents a holistic empirical study to assess the capabilities and challenges associated with parallel DL inference on heterogeneous mobile processors. Through carefully designed experiments covering various DL models, mobile software/hardware environments, workload patterns, and resource availability, we identify limitations of existing techniques and highlight opportunities for cross-level optimization.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：Stability of Explainable Recommendation</b></summary>
  <p><b>编号</b>：[172]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01849">https://arxiv.org/abs/2405.01849</a></p>
  <p><b>作者</b>：Sairamvinay Vijayaraghavan,  Prasant Mohapatra</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：industry and academia, gaining attention, years in industry, Explanations, Explainable</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explainable Recommendation has been gaining attention over the last few years in industry and academia. Explanations provided along with recommendations in a recommender system framework have many uses: particularly reasoning why a suggestion is provided and how well an item aligns with a user's personalized preferences. Hence, explanations can play a huge role in influencing users to purchase products. However, the reliability of the explanations under varying scenarios has not been strictly verified from an empirical perspective. Unreliable explanations can bear strong consequences such as attackers leveraging explanations for manipulating and tempting users to purchase target items that the attackers would want to promote. In this paper, we study the vulnerability of existent feature-oriented explainable recommenders, particularly analyzing their performance under different levels of external noises added into model parameters. We conducted experiments by analyzing three important state-of-the-art (SOTA) explainable recommenders when trained on two widely used e-commerce based recommendation datasets of different scales. We observe that all the explainable models are vulnerable to increased noise levels. Experimental results verify our hypothesis that the ability to explain recommendations does decrease along with increasing noise levels and particularly adversarial noise does contribute to a much stronger decrease. Our study presents an empirical verification on the topic of robust explanations in recommender systems which can be extended to different types of explainable recommenders in RS.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：RankSHAP: a Gold Standard Feature Attribution Method for the Ranking  Task</b></summary>
  <p><b>编号</b>：[173]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01848">https://arxiv.org/abs/2405.01848</a></p>
  <p><b>作者</b>：Tanya Chowdhury,  Yair Zick,  James Allan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：feature attribution, propose various post-hoc, model-agnostic explanations, feature attribution methods, classical Shapley</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Several works propose various post-hoc, model-agnostic explanations for the task of ranking, i.e. the task of ordering a set of documents, via feature attribution methods. However, these attributions are seen to weakly correlate and sometimes contradict each other. In classification/regression, several works focus on \emph{axiomatic characterization} of feature attribution methods, showing that a certain method uniquely satisfies a set of desirable properties. However, no such efforts have been taken in the space of feature attributions for the task of ranking. We take an axiomatic game-theoretic approach, popular in the feature attribution community, to identify candidate attribution methods for ranking tasks. We first define desirable axioms: Rank-Efficiency, Rank-Missingness, Rank-Symmetry and Rank-Monotonicity, all variants of the classical Shapley axioms. Next, we introduce Rank-SHAP, a feature attribution algorithm for the general ranking task, which is an extension to classical Shapley values. We identify a polynomial-time algorithm for computing approximate Rank-SHAP values and evaluate the computational efficiency and accuracy of our algorithm under various scenarios. We also evaluate its alignment with human intuition with a user study. Lastly, we theoretically examine popular rank attribution algorithms, EXS and Rank-LIME, and evaluate their capacity to satisfy the classical Shapley axioms.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Closing the Gap: Achieving Global Convergence (Last Iterate) of  Actor-Critic under Markovian Sampling with Neural Network Parametrization</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01843">https://arxiv.org/abs/2405.01843</a></p>
  <p><b>作者</b>：Mudit Gaur,  Vaneet Aggarwal,  Amrit Singh Bedi,  Di Wang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2306.10486</p>
  <p><b>关键词</b>：algorithms significantly lags, textbf, significantly lags, lags in addressing, MMCLG criteria</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The current state-of-the-art theoretical analysis of Actor-Critic (AC) algorithms significantly lags in addressing the practical aspects of AC implementations. This crucial gap needs bridging to bring the analysis in line with practical implementations of AC. To address this, we advocate for considering the MMCLG criteria: \textbf{M}ulti-layer neural network parametrization for actor/critic, \textbf{M}arkovian sampling, \textbf{C}ontinuous state-action spaces, the performance of the \textbf{L}ast iterate, and \textbf{G}lobal optimality. These aspects are practically significant and have been largely overlooked in existing theoretical analyses of AC algorithms. In this work, we address these gaps by providing the first comprehensive theoretical analysis of AC algorithms that encompasses all five crucial practical aspects (covers MMCLG criteria). We establish global convergence sample complexity bounds of $\tilde{\mathcal{O}}\left({\epsilon^{-3}}\right)$. We achieve this result through our novel use of the weak gradient domination property of MDP's and our unique analysis of the error in critic estimation.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：A Novel Approach to Guard from Adversarial Attacks using Stable  Diffusion</b></summary>
  <p><b>编号</b>：[180]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01838">https://arxiv.org/abs/2405.01838</a></p>
  <p><b>作者</b>：Trinath Sai Subhash Reddy Pittala,  Uma Maheswara Rao Meleti,  Geethakrishna Puligundla</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：adversarial machine learning, increasingly sophisticated attacks, Recent developments, machine learning, learning have highlighted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent developments in adversarial machine learning have highlighted the importance of building robust AI systems to protect against increasingly sophisticated attacks. While frameworks like AI Guardian are designed to defend against these threats, they often rely on assumptions that can limit their effectiveness. For example, they may assume attacks only come from one direction or include adversarial images in their training data. Our proposal suggests a different approach to the AI Guardian framework. Instead of including adversarial examples in the training process, we propose training the AI system without them. This aims to create a system that is inherently resilient to a wider range of attacks. Our method focuses on a dynamic defense strategy using stable diffusion that learns continuously and models threats comprehensively. We believe this approach can lead to a more generalized and robust defense against adversarial attacks.
In this paper, we outline our proposed approach, including the theoretical basis, experimental design, and expected impact on improving AI security against adversarial threats.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：Uniformly Stable Algorithms for Adversarial Training and Beyond</b></summary>
  <p><b>编号</b>：[187]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01817">https://arxiv.org/abs/2405.01817</a></p>
  <p><b>作者</b>：Jiancong Xiao,  Jiawei Zhang,  Zhi-Quan Luo,  Asuman Ozdaglar</p>
  <p><b>备注</b>：ICML 2024</p>
  <p><b>关键词</b>：neural networks suffer, test accuracy decreases, adversarial machine learning, robust test accuracy, adversarial training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In adversarial machine learning, neural networks suffer from a significant issue known as robust overfitting, where the robust test accuracy decreases over epochs (Rice et al., 2020). Recent research conducted by Xing et al.,2021; Xiao et al., 2022 has focused on studying the uniform stability of adversarial training. Their investigations revealed that SGD-based adversarial training fails to exhibit uniform stability, and the derived stability bounds align with the observed phenomenon of robust overfitting in experiments. This motivates us to develop uniformly stable algorithms specifically tailored for adversarial training. To this aim, we introduce Moreau envelope-$\mathcal{A}$, a variant of the Moreau Envelope-type algorithm. We employ a Moreau envelope function to reframe the original problem as a min-min problem, separating the non-strong convexity and non-smoothness of the adversarial loss. Then, this approach alternates between solving the inner and outer minimization problems to achieve uniform stability without incurring additional computational overhead. In practical scenarios, we show the efficacy of ME-$\mathcal{A}$ in mitigating the issue of robust overfitting. Beyond its application in adversarial training, this represents a fundamental result in uniform stability analysis, as ME-$\mathcal{A}$ is the first algorithm to exhibit uniform stability for weakly-convex, non-smooth problems.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Efficient and Economic Large Language Model Inference with Attention  Offloading</b></summary>
  <p><b>编号</b>：[189]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01814">https://arxiv.org/abs/2405.01814</a></p>
  <p><b>作者</b>：Shaoyuan Chen,  Yutong Lin,  Mingxing Zhang,  Yongwei Wu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Transformer-based large language, large language models, introduce significant challenges, real-world serving due, exhibit impressive performance</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. This mismatch arises from the autoregressive nature of LLMs, where the generation phase comprises operators with varying resource demands. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially as context length increases. To enhance the efficiency and cost-effectiveness of LLM serving, we introduce the concept of attention offloading. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model. This heterogeneous setup ensures that each component is tailored to its specific workload, maximizing overall performance and cost efficiency. Our comprehensive analysis and experiments confirm the viability of splitting the attention computation over multiple devices. Also, the communication bandwidth required between heterogeneous devices proves to be manageable with prevalent networking technologies. To further validate our theory, we develop Lamina, an LLM inference system that incorporates attention offloading. Experimental results indicate that Lamina can provide 1.48x-12.1x higher estimated throughput per dollar than homogeneous solutions.</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Non-linear Welfare-Aware Strategic Learning</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01810">https://arxiv.org/abs/2405.01810</a></p>
  <p><b>作者</b>：Tian Xie,  Xueru Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies algorithmic, studies algorithmic decision-making, strategic individual behaviors, linear decision policy, paper studies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies algorithmic decision-making in the presence of strategic individual behaviors, where an ML model is used to make decisions about human agents and the latter can adapt their behavior strategically to improve their future data. Existing results on strategic learning have largely focused on the linear setting where agents with linear labeling functions best respond to a (noisy) linear decision policy. Instead, this work focuses on general non-linear settings where agents respond to the decision policy with only "local information" of the policy. Moreover, we simultaneously consider the objectives of maximizing decision-maker welfare (model prediction accuracy), social welfare (agent improvement caused by strategic behaviors), and agent welfare (the extent that ML underestimates the agents). We first generalize the agent best response model in previous works to the non-linear setting, then reveal the compatibility of welfare objectives. We show the three welfare can attain the optimum simultaneously only under restrictive conditions which are challenging to achieve in non-linear settings. The theoretical results imply that existing works solely maximizing the welfare of a subset of parties inevitably diminish the welfare of the others. We thus claim the necessity of balancing the welfare of each party in non-linear settings and propose an irreducible optimization algorithm suitable for general strategic learning. Experiments on synthetic and real data validate the proposed algorithm.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Learning Robust Autonomous Navigation and Locomotion for Wheeled-Legged  Robots</b></summary>
  <p><b>编号</b>：[204]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01792">https://arxiv.org/abs/2405.01792</a></p>
  <p><b>作者</b>：Joonho Lee,  Marko Bjelonic,  Alexander Reske,  Lorenz Wellhausen,  Takahiro Miki,  Marco Hutter</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：improving operational efficiency, transform logistics systems, urban environments, improving operational, Navigating urban environments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Autonomous wheeled-legged robots have the potential to transform logistics systems, improving operational efficiency and adaptability in urban environments. Navigating urban environments, however, poses unique challenges for robots, necessitating innovative solutions for locomotion and navigation. These challenges include the need for adaptive locomotion across varied terrains and the ability to navigate efficiently around complex dynamic obstacles. This work introduces a fully integrated system comprising adaptive locomotion control, mobility-aware local navigation planning, and large-scale path planning within the city. Using model-free reinforcement learning (RL) techniques and privileged learning, we develop a versatile locomotion controller. This controller achieves efficient and robust locomotion over various rough terrains, facilitated by smooth transitions between walking and driving modes. It is tightly integrated with a learned navigation controller through a hierarchical RL framework, enabling effective navigation through challenging terrain and various obstacles at high speed. Our controllers are integrated into a large-scale urban navigation system and validated by autonomous, kilometer-scale navigation missions conducted in Zurich, Switzerland, and Seville, Spain. These missions demonstrate the system's robustness and adaptability, underscoring the importance of integrated control systems in achieving seamless navigation in complex environments. Our findings support the feasibility of wheeled-legged robots and hierarchical RL for autonomous navigation, with implications for last-mile delivery and beyond.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Hierarchical mixture of discriminative Generalized Dirichlet classifiers</b></summary>
  <p><b>编号</b>：[209]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01778">https://arxiv.org/abs/2405.01778</a></p>
  <p><b>作者</b>：Elvis Togban,  Djemel Ziou</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Generalized Dirichlet mixture, Generalized Dirichlet, compositional data, paper presents, Dirichlet mixture</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper presents a discriminative classifier for compositional data. This classifier is based on the posterior distribution of the Generalized Dirichlet which is the discriminative counterpart of Generalized Dirichlet mixture model. Moreover, following the mixture of experts paradigm, we proposed a hierarchical mixture of this classifier. In order to learn the models parameters, we use a variational approximation by deriving an upper-bound for the Generalized Dirichlet mixture. To the best of our knownledge, this is the first time this bound is proposed in the literature. Experimental results are presented for spam detection and color space identification.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：An Approach to Systematic Data Acquisition and Data-Driven Simulation  for the Safety Testing of Automated Driving Functions</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01776">https://arxiv.org/abs/2405.01776</a></p>
  <p><b>作者</b>：Leon Eisemann,  Mirjam Fehling-Kaschek,  Henrik Gommel,  David Hermann,  Marvin Klemp,  Martin Lauer,  Benjamin Lickert,  Florian Luettner,  Robin Moss,  Nicole Neis,  Maria Pohle,  Simon Romanski,  Daniel Stadler,  Alexander Stolz,  Jens Ziehn,  Jingxing Zhou</p>
  <p><b>备注</b>：8 pages, 5 figures</p>
  <p><b>关键词</b>：operational design domains, automated driving functions, covering significant proportions, design domains, proportions of development</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With growing complexity and criticality of automated driving functions in road traffic and their operational design domains (ODD), there is increasing demand for covering significant proportions of development, validation, and verification in virtual environments and through simulation models.
If, however, simulations are meant not only to augment real-world experiments, but to replace them, quantitative approaches are required that measure to what degree and under which preconditions simulation models adequately represent reality, and thus, using their results accordingly. Especially in R&D areas related to the safety impact of the "open world", there is a significant shortage of real-world data to parameterize and/or validate simulations - especially with respect to the behavior of human traffic participants, whom automated driving functions will meet in mixed traffic.
We present an approach to systematically acquire data in public traffic by heterogeneous means, transform it into a unified representation, and use it to automatically parameterize traffic behavior models for use in data-driven virtual validation of automated driving functions.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Torch2Chip: An End-to-end Customizable Deep Neural Network Compression  and Deployment Toolkit for Prototype Hardware Accelerator Design</b></summary>
  <p><b>编号</b>：[211]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01775">https://arxiv.org/abs/2405.01775</a></p>
  <p><b>作者</b>：Jian Meng,  Yuan Liao,  Anupreetham Anupreetham,  Ahmed Hasssan,  Shixing Yu,  Han-sok Suh,  Xiaofeng Hu,  Jae-sun Seo</p>
  <p><b>备注</b>：Accepted for publication at MLSys 2024</p>
  <p><b>关键词</b>：neural network accelerators, ASIC or FPGA, continuously motivated, neural network, expensive DNN computations</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The development of model compression is continuously motivated by the evolution of various neural network accelerators with ASIC or FPGA. On the algorithm side, the ultimate goal of quantization or pruning is accelerating the expensive DNN computations on low-power hardware. However, such a "design-and-deploy" workflow faces under-explored challenges in the current hardware-algorithm co-design community. First, although the state-of-the-art quantization algorithm can achieve low precision with negligible degradation of accuracy, the latest deep learning framework (e.g., PyTorch) can only support non-customizable 8-bit precision, data format, and parameter extraction. Secondly, the objective of quantization is to enable the computation with low-precision data. However, the current SoTA algorithm treats the quantized integer as an intermediate result, while the final output of the quantizer is the "discretized" floating-point values, ignoring the practical needs and adding additional workload to hardware designers for integer parameter extraction and layer fusion. Finally, the compression toolkits designed by the industry are constrained to their in-house product or a handful of algorithms. The limited degree of freedom in the current toolkit and the under-explored customization hinder the prototype ASIC or FPGA-based accelerator design. To resolve these challenges, we propose Torch2Chip, an open-sourced, fully customizable, and high-performance toolkit that supports user-designed compression followed by automatic model fusion and parameter extraction. Torch2Chip incorporates the hierarchical design workflow, and the user-customized compression algorithm will be directly packed into the deployment-ready format for prototype chip verification with either CNN or vision transformer (ViT). The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in  Linear Time</b></summary>
  <p><b>编号</b>：[218]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01762">https://arxiv.org/abs/2405.01762</a></p>
  <p><b>作者</b>：Shengyao Lu,  Bang Liu,  Keith G. Mills,  Jiao He,  Di Niu</p>
  <p><b>备注</b>：19 pages</p>
  <p><b>关键词</b>：Graph Neural Networks, Neural Networks, Graph Neural, predictions of Graph, safety and trustworthiness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Understanding and explaining the predictions of Graph Neural Networks (GNNs), is crucial for enhancing their safety and trustworthiness. Subgraph-level explanations are gaining attention for their intuitive appeal. However, most existing subgraph-level explainers face efficiency challenges in explaining GNNs due to complex search processes. The key challenge is to find a balance between intuitiveness and efficiency while ensuring transparency. Additionally, these explainers usually induce subgraphs by nodes, which may introduce less-intuitive disconnected nodes in the subgraph-level explanations or omit many important subgraph structures. In this paper, we reveal that inducing subgraph explanations by edges is more comprehensive than other subgraph inducing techniques. We also emphasize the need of determining the subgraph explanation size for each data instance, as different data instances may involve different important substructures. Building upon these considerations, we introduce a training-free approach, named EiG-Search. We employ an efficient linear-time search algorithm over the edge-induced subgraphs, where the edges are ranked by an enhanced gradient-based importance. We conduct extensive experiments on a total of seven datasets, demonstrating its superior performance and efficiency both quantitatively and qualitatively over the leading baselines.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Reinforcement Learning-Guided Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01760">https://arxiv.org/abs/2405.01760</a></p>
  <p><b>作者</b>：Marzi Heidari,  Hanping Zhang,  Yuhong Guo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gained significant attention, significant attention due, unlabeled data, Guided SSL method, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. However, most current SSL methods rely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. They are limited to exploiting loss functions and regularization methods within the standard norm. In this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, that formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward to adaptively guide the learning process of the prediction model. RLGSSL incorporates a carefully designed reward function that balances the use of labeled and unlabeled data to enhance generalization performance. A semi-supervised teacher-student framework is further deployed to increase the learning stability. We demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets and show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：CGD: Constraint-Guided Diffusion Policies for UAV Trajectory Planning</b></summary>
  <p><b>编号</b>：[220]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01758">https://arxiv.org/abs/2405.01758</a></p>
  <p><b>作者</b>：Kota Kondo,  Andrea Tagliabue,  Xiaoyi Cai,  Claudius Tewari,  Olivia Garcia,  Marcos Espitia-Alvarez,  Jonathan P. How</p>
  <p><b>备注</b>：8 pages, 3 figures</p>
  <p><b>关键词</b>：high computational costs, Traditional optimization-based planners, Traditional optimization-based, suffer from high, computational costs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional optimization-based planners, while effective, suffer from high computational costs, resulting in slow trajectory generation. A successful strategy to reduce computation time involves using Imitation Learning (IL) to develop fast neural network (NN) policies from those planners, which are treated as expert demonstrators. Although the resulting NN policies are effective at quickly generating trajectories similar to those from the expert, (1) their output does not explicitly account for dynamic feasibility, and (2) the policies do not accommodate changes in the constraints different from those used during training.
To overcome these limitations, we propose Constraint-Guided Diffusion (CGD), a novel IL-based approach to trajectory planning. CGD leverages a hybrid learning/online optimization scheme that combines diffusion policies with a surrogate efficient optimization problem, enabling the generation of collision-free, dynamically feasible trajectories. The key ideas of CGD include dividing the original challenging optimization problem solved by the expert into two more manageable sub-problems: (a) efficiently finding collision-free paths, and (b) determining a dynamically-feasible time-parametrization for those paths to obtain a trajectory. Compared to conventional neural network architectures, we demonstrate through numerical evaluations significant improvements in performance and dynamic feasibility under scenarios with new constraints never encountered during training.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Large Language Models for UAVs: Current State and Pathways to the Future</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01745">https://arxiv.org/abs/2405.01745</a></p>
  <p><b>作者</b>：Shumaila Javaid,  Nasir Saeed,  Bin He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Unmanned Aerial Vehicles, Aerial Vehicles, offering adaptable solutions, Unmanned Aerial, diverse sectors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unmanned Aerial Vehicles (UAVs) have emerged as a transformative technology across diverse sectors, offering adaptable solutions to complex challenges in both military and civilian domains. Their expanding capabilities present a platform for further advancement by integrating cutting-edge computational tools like Artificial Intelligence (AI) and Machine Learning (ML) algorithms. These advancements have significantly impacted various facets of human life, fostering an era of unparalleled efficiency and convenience. Large Language Models (LLMs), a key component of AI, exhibit remarkable learning and adaptation capabilities within deployed environments, demonstrating an evolving form of intelligence with the potential to approach human-level proficiency. This work explores the significant potential of integrating UAVs and LLMs to propel the development of autonomous systems. We comprehensively review LLM architectures, evaluating their suitability for UAV integration. Additionally, we summarize the state-of-the-art LLM-based UAV architectures and identify novel opportunities for LLM embedding within UAV frameworks. Notably, we focus on leveraging LLMs to refine data analysis and decision-making processes, specifically for enhanced spectral sensing and sharing in UAV applications. Furthermore, we investigate how LLM integration expands the scope of existing UAV applications, enabling autonomous data processing, improved decision-making, and faster response times in emergency scenarios like disaster response and network restoration. Finally, we highlight crucial areas for future research that are critical for facilitating the effective integration of LLMs and UAVs.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：ALCM: Autonomous LLM-Augmented Causal Discovery Framework</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01744">https://arxiv.org/abs/2405.01744</a></p>
  <p><b>作者</b>：Elahe Khatibi,  Mahyar Abbasian,  Zhongqi Yang,  Iman Azimi,  Amir M. Rahmani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：perform effective causal, causal, Large Language Models, causal discovery, effective causal inference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP-hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：PVF (Parameter Vulnerability Factor): A Quantitative Metric Measuring AI  Vulnerability and Resilience Against Parameter Corruptions</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01741">https://arxiv.org/abs/2405.01741</a></p>
  <p><b>作者</b>：Xun Jiao,  Fred Lin,  Harish D. Dixit,  Joel Coburn,  Abhinav Pandey,  Han Wang,  Jianyu Huang,  Venkat Ramesh,  Wang Xu,  Daniel Moore,  Sriram Sankar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fundamental concern, successful deployment, deployment and widespread, widespread adoption, parameter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reliability of AI systems is a fundamental concern for the successful deployment and widespread adoption of AI technologies. Unfortunately, the escalating complexity and heterogeneity of AI hardware systems make them inevitably and increasingly susceptible to hardware faults (e.g., bit flips) that can potentially corrupt model parameters. Given this challenge, this paper aims to answer a critical question: How likely is a parameter corruption to result in an incorrect model output? To systematically answer this question, we propose a novel quantitative metric, Parameter Vulnerability Factor (PVF), inspired by architectural vulnerability factor (AVF) in computer architecture community, aiming to standardize the quantification of AI model resilience/vulnerability against parameter corruptions. We define a model parameter's PVF as the probability that a corruption in that particular model parameter will result in an incorrect output. Similar to AVF, this statistical concept can be derived from statistically extensive and meaningful fault injection (FI) experiments. In this paper, we present several use cases on applying PVF to three types of tasks/models during inference -- recommendation (DLRM), vision classification (CNN), and text classification (BERT). PVF can provide pivotal insights to AI hardware designers in balancing the tradeoff between fault protection and performance/efficiency such as mapping vulnerable AI parameter components to well-protected hardware modules. PVF metric is applicable to any AI model and has a potential to help unify and standardize AI vulnerability/resilience evaluation practice.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Enhancing User Experience in On-Device Machine Learning with Gated  Compression Layers</b></summary>
  <p><b>编号</b>：[229]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01739">https://arxiv.org/abs/2405.01739</a></p>
  <p><b>作者</b>：Haiguang Li,  Usama Pervaiz,  Joseph Antognini,  Michał Matuszak,  Lawrence Au,  Gilles Roux,  Trausti Thormundsson</p>
  <p><b>备注</b>：Initial Submission</p>
  <p><b>关键词</b>：On-device machine learning, enables powerful edge, powerful edge applications, On-device machine, machine learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>On-device machine learning (ODML) enables powerful edge applications, but power consumption remains a key challenge for resource-constrained devices. To address this, developers often face a trade-off between model accuracy and power consumption, employing either computationally intensive models on high-power cores or pared-down models on low-power cores. Both approaches typically lead to a compromise in user experience (UX). This work focuses on the use of Gated Compression (GC) layer to enhance ODML model performance while conserving power and maximizing cost-efficiency, especially for always-on use cases. GC layers dynamically regulate data flow by selectively gating activations of neurons within the neural network and effectively filtering out non-essential inputs, which reduces power needs without compromising accuracy, and enables more efficient execution on heterogeneous compute cores. These improvements enhance UX through prolonged battery life, improved device responsiveness, and greater user comfort. In this work, we have integrated GC layers into vision and speech domain models including the transformer-based ViT model. Our experiments demonstrate theoretical power efficiency gains ranging from 158x to 30,000x for always-on scenarios. This substantial improvement empowers ODML applications with enhanced UX benefits.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：Dynamic Anisotropic Smoothing for Noisy Derivative-Free Optimization</b></summary>
  <p><b>编号</b>：[235]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01731">https://arxiv.org/abs/2405.01731</a></p>
  <p><b>作者</b>：Sam Reifenstein,  Timothee Leleu,  Yoshihisa Yamamoto</p>
  <p><b>备注</b>：Accepted to ICML2024</p>
  <p><b>关键词</b>：Gaussian smoothing, objective function, heterogeneous curvature, ball smoothing, algorithm dynamically adapts</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel algorithm that extends the methods of ball smoothing and Gaussian smoothing for noisy derivative-free optimization by accounting for the heterogeneous curvature of the objective function. The algorithm dynamically adapts the shape of the smoothing kernel to approximate the Hessian of the objective function around a local optimum. This approach significantly reduces the error in estimating the gradient from noisy evaluations through sampling. We demonstrate the efficacy of our method through numerical experiments on artificial problems. Additionally, we show improved performance when tuning NP-hard combinatorial optimization solvers compared to existing state-of-the-art heuristic derivative-free and Bayesian optimization methods.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Inherent Trade-Offs between Diversity and Stability in Multi-Task  Benchmark</b></summary>
  <p><b>编号</b>：[239]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01719">https://arxiv.org/abs/2405.01719</a></p>
  <p><b>作者</b>：Guanhua Zhang,  Moritz Hardt</p>
  <p><b>备注</b>：To be published in ICML 2024</p>
  <p><b>关键词</b>：social choice theory, choice theory, machine learning, lens of social, social choice</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We examine multi-task benchmarks in machine learning through the lens of social choice theory. We draw an analogy between benchmarks and electoral systems, where models are candidates and tasks are voters. This suggests a distinction between cardinal and ordinal benchmark systems. The former aggregate numerical scores into one model ranking; the latter aggregate rankings for each task. We apply Arrow's impossibility theorem to ordinal benchmarks to highlight the inherent limitations of ordinal systems, particularly their sensitivity to the inclusion of irrelevant models. Inspired by Arrow's theorem, we empirically demonstrate a strong trade-off between diversity and sensitivity to irrelevant changes in existing multi-task benchmarks. Our result is based on new quantitative measures of diversity and sensitivity that we introduce. Sensitivity quantifies the impact that irrelevant changes to tasks have on a benchmark. Diversity captures the degree of disagreement in model rankings across tasks. We develop efficient approximation algorithms for both measures, as exact computation is computationally challenging. Through extensive experiments on seven cardinal benchmarks and eleven ordinal benchmarks, we demonstrate a clear trade-off between diversity and stability: The more diverse a multi-task benchmark, the more sensitive to trivial changes it is. Additionally, we show that the aggregated rankings of existing benchmarks are highly unstable under irrelevant changes. The codes and data are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Robust Risk-Sensitive Reinforcement Learning with Conditional  Value-at-Risk</b></summary>
  <p><b>编号</b>：[240]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01718">https://arxiv.org/abs/2405.01718</a></p>
  <p><b>作者</b>：Xinyi Ni,  Lifeng Lai</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Markov Decision Processes, standard Markov Decision, Decision Processes, Markov Decision, Robust Markov Decision</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust Markov Decision Processes (RMDPs) have received significant research interest, offering an alternative to standard Markov Decision Processes (MDPs) that often assume fixed transition probabilities. RMDPs address this by optimizing for the worst-case scenarios within ambiguity sets. While earlier studies on RMDPs have largely centered on risk-neutral reinforcement learning (RL), with the goal of minimizing expected total discounted costs, in this paper, we analyze the robustness of CVaR-based risk-sensitive RL under RMDP. Firstly, we consider predetermined ambiguity sets. Based on the coherency of CVaR, we establish a connection between robustness and risk sensitivity, thus, techniques in risk-sensitive RL can be adopted to solve the proposed problem. Furthermore, motivated by the existence of decision-dependent uncertainty in real-world problems, we study problems with state-action-dependent ambiguity sets. To solve this, we define a new risk measure named NCVaR and build the equivalence of NCVaR optimization and robust CVaR optimization. We further propose value iteration algorithms and validate our approach in simulation experiments.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Interpretable Vital Sign Forecasting with Model Agnostic Attention Maps</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01714">https://arxiv.org/abs/2405.01714</a></p>
  <p><b>作者</b>：Yuwei Liu,  Chen Dan,  Anubhav Bhatti,  Bingjie Shen,  Divij Gupta,  Suraj Parmar,  San Lee</p>
  <p><b>备注</b>：8 pages, 4 figures</p>
  <p><b>关键词</b>：intensive care units, substantial medical challenge, care units, representing a substantial, medical challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sepsis is a leading cause of mortality in intensive care units (ICUs), representing a substantial medical challenge. The complexity of analyzing diverse vital signs to predict sepsis further aggravates this issue. While deep learning techniques have been advanced for early sepsis prediction, their 'black-box' nature obscures the internal logic, impairing interpretability in critical settings like ICUs. This paper introduces a framework that combines a deep learning model with an attention mechanism that highlights the critical time steps in the forecasting process, thus improving model interpretability and supporting clinical decision-making. We show that the attention mechanism could be adapted to various black box time series forecasting models such as N-HiTS and N-BEATS. Our method preserves the accuracy of conventional deep learning models while enhancing interpretability through attention-weight-generated heatmaps. We evaluated our model on the eICU-CRD dataset, focusing on forecasting vital signs for sepsis patients. We assessed its performance using mean squared error (MSE) and dynamic time warping (DTW) metrics. We explored the attention maps of N-HiTS and N-BEATS, examining the differences in their performance and identifying crucial factors influencing vital sign forecasting.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Individual Fairness Through Reweighting and Tuning</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01711">https://arxiv.org/abs/2405.01711</a></p>
  <p><b>作者</b>：Abdoul Jalil Djiberou Mahamadou,  Lea Goetz,  Russ Altman</p>
  <p><b>备注</b>：14 pages, 1 figure, and 2 tables</p>
  <p><b>关键词</b>：Graph Laplacian Regularizer, GLR, artificial intelligence, amplified and perpetuated, perpetuated by artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inherent bias within society can be amplified and perpetuated by artificial intelligence (AI) systems. To address this issue, a wide range of solutions have been proposed to identify and mitigate bias and enforce fairness for individuals and groups. Recently, Graph Laplacian Regularizer (GLR), a regularization technique from the semi-supervised learning literature has been used as a substitute for the common Lipschitz condition to enhance individual fairness (IF). Notable prior work has shown that enforcing IF through a GLR can improve the transfer learning accuracy of AI models under covariate shifts. However, the prior work defines a GLR on the source and target data combined, implicitly assuming that the target data are available at train time, which might not hold in practice. In this work, we investigated whether defining a GLR independently on the train and target data could maintain similar accuracy compared to the prior work model. Furthermore, we introduced the Normalized Fairness Gain score (FGN) to measure IF for in-processing algorithmic fairness techniques. FGN quantifies the amount of gained fairness when a GLR is used versus not. We evaluated the new and original methods under FGN, the Prediction Consistency (PC), and traditional classification metrics on the German Credit Approval dataset. The results showed that the two models achieved similar statistical mean performances over five-fold cross-validation. Furthermore, the proposed metric showed that PC scores can be misleading as the scores can be high and statistically similar to fairness-enhanced models while FGN scores are small. This work therefore provides new insights into when a GLR effectively enhances IF and the pitfalls of PC.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：A deep causal inference model for fully-interpretable travel behaviour  analysis</b></summary>
  <p><b>编号</b>：[246]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01708">https://arxiv.org/abs/2405.01708</a></p>
  <p><b>作者</b>：Kimia Kamal,  Bilal Farooq</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：involves causal questions, Transport policy assessment, causal inference capabilities, causal inference, assessment often involves</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transport policy assessment often involves causal questions, yet the causal inference capabilities of traditional travel behavioural models are at best limited. We present the deep CAusal infeRence mOdel for traveL behavIour aNAlysis (CAROLINA), a framework that explicitly models causality in travel behaviour, enhances predictive accuracy, and maintains interpretability by leveraging causal inference, deep learning, and traditional discrete choice modelling. Within this framework, we introduce a Generative Counterfactual model for forecasting human behaviour by adapting the Normalizing Flow method. Through the case studies of virtual reality-based pedestrian crossing behaviour, revealed preference travel behaviour from London, and synthetic data, we demonstrate the effectiveness of our proposed models in uncovering causal relationships, prediction accuracy, and assessing policy interventions. Our results show that intervention mechanisms that can reduce pedestrian stress levels lead to a 38.5% increase in individuals experiencing shorter waiting times. Reducing the travel distances in London results in a 47% increase in sustainable travel modes.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Privacy-aware Berrut Approximated Coded Computing for Federated Learning</b></summary>
  <p><b>编号</b>：[249]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01704">https://arxiv.org/abs/2405.01704</a></p>
  <p><b>作者</b>：Xavier Martínez Luaña,  Rebeca P. Díaz Redondo,  Manuel Fernández Veiga</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Secure Multi-Party Computation, private datasets, interesting strategy, strategy that enables, enables the collaborative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Federated Learning (FL) is an interesting strategy that enables the collaborative training of an AI model among different data owners without revealing their private datasets. Even so, FL has some privacy vulnerabilities that have been tried to be overcome by applying some techniques like Differential Privacy (DP), Homomorphic Encryption (HE), or Secure Multi-Party Computation (SMPC). However, these techniques have some important drawbacks that might narrow their range of application: problems to work with non-linear functions and to operate large matrix multiplications and high communication and computational costs to manage semi-honest nodes. In this context, we propose a solution to guarantee privacy in FL schemes that simultaneously solves the previously mentioned problems. Our proposal is based on the Berrut Approximated Coded Computing, a technique from the Coded Distributed Computing paradigm, adapted to a Secret Sharing configuration, to provide input privacy to FL in a scalable way. It can be applied for computing non-linear functions and treats the special case of distributed matrix multiplication, a key primitive at the core of many automated learning tasks. Because of these characteristics, it could be applied in a wide range of FL scenarios, since it is independent of the machine learning models or aggregation algorithms used in the FL scheme. We provide analysis of the achieve privacy and complexity of our solution and, due to the extensive numerical results performed, it can be observed a good trade-off between privacy and precision.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Optimization without retraction on the random generalized Stiefel  manifold</b></summary>
  <p><b>编号</b>：[250]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01702">https://arxiv.org/abs/2405.01702</a></p>
  <p><b>作者</b>：Simon Vary,  Pierre Ablin,  Bin Gao,  P.-A. Absil</p>
  <p><b>备注</b>：21 pages, 10 figures</p>
  <p><b>关键词</b>：independent component analysis, canonical correlation analysis, sampled covariance matrices, involving sampled covariance, generalized Stiefel manifold</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Optimization over the set of matrices that satisfy $X^\top B X = I_p$, referred to as the generalized Stiefel manifold, appears in many applications involving sampled covariance matrices such as canonical correlation analysis (CCA), independent component analysis (ICA), and the generalized eigenvalue problem (GEVP). Solving these problems is typically done by iterative methods, such as Riemannian approaches, which require a computationally expensive eigenvalue decomposition involving fully formed $B$. We propose a cheap stochastic iterative method that solves the optimization problem while having access only to a random estimate of the feasible set. Our method does not enforce the constraint in every iteration exactly, but instead it produces iterations that converge to a critical point on the generalized Stiefel manifold defined in expectation. The method has lower per-iteration cost, requires only matrix multiplications, and has the same convergence rates as its Riemannian counterparts involving the full matrix $B$. Experiments demonstrate its effectiveness in various machine learning applications involving generalized orthogonality constraints, including CCA, ICA, and GEVP.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Language-Enhanced Latent Representations for Out-of-Distribution  Detection in Autonomous Driving</b></summary>
  <p><b>编号</b>：[256]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01691">https://arxiv.org/abs/2405.01691</a></p>
  <p><b>作者</b>：Zhenjiang Mao,  Dong-You Jhong,  Ao Wang,  Ivan Ruchkin</p>
  <p><b>备注</b>：Presented at the Robot Trust for Symbiotic Societies (RTSS) Workshop, co-located with ICRA 2024</p>
  <p><b>关键词</b>：learning-based components encounter, components encounter unexpected, encounter unexpected inputs, essential in autonomous, determine when learning-based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Out-of-distribution (OOD) detection is essential in autonomous driving, to determine when learning-based components encounter unexpected inputs. Traditional detectors typically use encoder models with fixed settings, thus lacking effective human interaction capabilities. With the rise of large foundation models, multimodal inputs offer the possibility of taking human language as a latent representation, thus enabling language-defined OOD detection. In this paper, we use the cosine similarity of image and text representations encoded by the multimodal model CLIP as a new representation to improve the transparency and controllability of latent encodings used for visual anomaly detection. We compare our approach with existing pre-trained encoders that can only produce latent representations that are meaningless from the user's standpoint. Our experiments on realistic driving data show that the language-based latent representation performs better than the traditional representation of the vision encoder and helps improve the detection performance when combined with standard representations.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：Intelligent Switching for Reset-Free RL</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01684">https://arxiv.org/abs/2405.01684</a></p>
  <p><b>作者</b>：Darshan Patil,  Janarthanan Rajendran,  Glen Berseth,  Sarath Chandar</p>
  <p><b>备注</b>：Published at ICLR 2024</p>
  <p><b>关键词</b>：strong episode resetting, episode resetting mechanisms, real world, simulation are unavailable, strong episode</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The \textit{resetting} assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (\textit{forward}) with learned resets by constructing a second (\textit{backward}) agent that returns the forward agent to the initial state. We find that the termination and timing of the transitions between these two agents are crucial for algorithm success. With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which intelligently switches between the two agents based on the agent's confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Physics-Informed Neural Networks: Minimizing Residual Loss with Wide  Networks and Effective Activations</b></summary>
  <p><b>编号</b>：[265]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01680">https://arxiv.org/abs/2405.01680</a></p>
  <p><b>作者</b>：Nima Hosseini Dashtbayaz,  Ghazal Farhani,  Boyu Wang,  Charles X. Ling</p>
  <p><b>备注</b>：Accepted at IJCAI 2024</p>
  <p><b>关键词</b>：common supervised problems, simple recursive relation, Physics-Informed Neural Networks, feed-forward neural network, residual loss</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The residual loss in Physics-Informed Neural Networks (PINNs) alters the simple recursive relation of layers in a feed-forward neural network by applying a differential operator, resulting in a loss landscape that is inherently different from those of common supervised problems. Therefore, relying on the existing theory leads to unjustified design choices and suboptimal performance. In this work, we analyze the residual loss by studying its characteristics at critical points to find the conditions that result in effective training of PINNs. Specifically, we first show that under certain conditions, the residual loss of PINNs can be globally minimized by a wide neural network. Furthermore, our analysis also reveals that an activation function with well-behaved high-order derivatives plays a crucial role in minimizing the residual loss. In particular, to solve a $k$-th order PDE, the $k$-th derivative of the activation function should be bijective. The established theory paves the way for designing and choosing effective activation functions for PINNs and explains why periodic activations have shown promising performance in certain cases. Finally, we verify our findings by conducting a set of experiments on several PDEs. Our code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：Balance Reward and Safety Optimization for Safe Reinforcement Learning:  A Perspective of Gradient Manipulation</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01677">https://arxiv.org/abs/2405.01677</a></p>
  <p><b>作者</b>：Shangding Gu,  Bilgehan Sel,  Yuhao Ding,  Lu Wang,  Qingwei Lin,  Ming Jin,  Alois Knoll</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reinforcement Learning, real-world applications, deployment in real-world, safety, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：ATNPA: A Unified View of Oversmoothing Alleviation in Graph Neural  Networks</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01663">https://arxiv.org/abs/2405.01663</a></p>
  <p><b>作者</b>：Yufei Jin,  Xingquan Zhu</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：differentiating network proximity, embedding features learned, commonly observed challenge, graph neural network, network proximity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Oversmoothing is a commonly observed challenge in graph neural network (GNN) learning, where, as layers increase, embedding features learned from GNNs quickly become similar/indistinguishable, making them incapable of differentiating network proximity. A GNN with shallow layer architectures can only learn short-term relation or localized structure information, limiting its power of learning long-term connection, evidenced by their inferior learning performance on heterophilous graphs. Tackling oversmoothing is crucial to harness deep-layer architectures for GNNs. To date, many methods have been proposed to alleviate oversmoothing. The vast difference behind their design principles, combined with graph complications, make it difficult to understand and even compare their difference in tackling the oversmoothing. In this paper, we propose ATNPA, a unified view with five key steps: Augmentation, Transformation, Normalization, Propagation, and Aggregation, to summarize GNN oversmoothing alleviation approaches. We first outline three themes to tackle oversmoothing, and then separate all methods into six categories, followed by detailed reviews of representative methods, including their relation to the ATNPA, and discussion about their niche, strength, and weakness. The review not only draws in-depth understanding of existing methods in the field, but also shows a clear road map for future study.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：When a Relation Tells More Than a Concept: Exploring and Evaluating  Classifier Decisions with CoReX</b></summary>
  <p><b>编号</b>：[274]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01661">https://arxiv.org/abs/2405.01661</a></p>
  <p><b>作者</b>：Bettina Finzel,  Patrick Hilme,  Johannes Rabold,  Ute Schmid</p>
  <p><b>备注</b>：preliminary version, submitted to Machine Learning</p>
  <p><b>关键词</b>：Convolutional Neural Networks, Neural Networks, Convolutional Neural, input features impact, features impact model</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Explanations for Convolutional Neural Networks (CNNs) based on relevance of input pixels might be too unspecific to evaluate which and how input features impact model decisions. Especially in complex real-world domains like biomedicine, the presence of specific concepts (e.g., a certain type of cell) and of relations between concepts (e.g., one cell type is next to another) might be discriminative between classes (e.g., different types of tissue). Pixel relevance is not expressive enough to convey this type of information. In consequence, model evaluation is limited and relevant aspects present in the data and influencing the model decisions might be overlooked. This work presents a novel method to explain and evaluate CNN models, which uses a concept- and relation-based explainer (CoReX). It explains the predictive behavior of a model on a set of images by masking (ir-)relevant concepts from the decision-making process and by constraining relations in a learned interpretable surrogate model. We test our approach with several image data sets and CNN architectures. Results show that CoReX explanations are faithful to the CNN model in terms of predictive outcomes. We further demonstrate that CoReX is a suitable tool for evaluating CNNs supporting identification and re-classification of incorrect or ambiguous classifications.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：S4: Self-Supervised Sensing Across the Spectrum</b></summary>
  <p><b>编号</b>：[276]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01656">https://arxiv.org/abs/2405.01656</a></p>
  <p><b>作者</b>：Jayanth Shenoy,  Xinjian Davis Zhang,  Shlok Mehrotra,  Bill Tao,  Rem Yang,  Han Zhao,  Deepak Vasisht</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：land cover mapping, crop type classification, agricultural crop type, image time series, Satellite image time</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Satellite image time series (SITS) segmentation is crucial for many applications like environmental monitoring, land cover mapping and agricultural crop type classification. However, training models for SITS segmentation remains a challenging task due to the lack of abundant training data, which requires fine grained annotation. We propose S4 a new self-supervised pre-training approach that significantly reduces the requirement for labeled training data by utilizing two new insights: (a) Satellites capture images in different parts of the spectrum such as radio frequencies, and visible frequencies. (b) Satellite imagery is geo-registered allowing for fine-grained spatial alignment. We use these insights to formulate pre-training tasks in S4. We also curate m2s2-SITS, a large-scale dataset of unlabeled, spatially-aligned, multi-modal and geographic specific SITS that serves as representative pre-training data for S4. Finally, we evaluate S4 on multiple SITS segmentation datasets and demonstrate its efficacy against competing baselines while using limited labeled data.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：An Explainable and Conformal AI Model to Detect Temporomandibular Joint  Involvement in Children Suffering from Juvenile Idiopathic Arthritis</b></summary>
  <p><b>编号</b>：[284]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01617">https://arxiv.org/abs/2405.01617</a></p>
  <p><b>作者</b>：Lena Todnem Bach Christensen,  Dikte Straadt,  Stratos Vassis,  Christian Marius Lillelund,  Peter Bangsgaard Stoustrup,  Ruben Pauwels,  Thomas Klit Pedersen,  Christian Fischer Pedersen</p>
  <p><b>备注</b>：Accepted at EMBC 2024</p>
  <p><b>关键词</b>：Juvenile idiopathic arthritis, common rheumatic disease, TMJ involvement, TMJ, Juvenile idiopathic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Juvenile idiopathic arthritis (JIA) is the most common rheumatic disease during childhood and adolescence. The temporomandibular joints (TMJ) are among the most frequently affected joints in patients with JIA, and mandibular growth is especially vulnerable to arthritic changes of the TMJ in children. A clinical examination is the most cost-effective method to diagnose TMJ involvement, but clinicians find it difficult to interpret and inaccurate when used only on clinical examinations. This study implemented an explainable artificial intelligence (AI) model that can help clinicians assess TMJ involvement. The classification model was trained using Random Forest on 6154 clinical examinations of 1035 pediatric patients (67% female, 33% male) and evaluated on its ability to correctly classify TMJ involvement or not on a separate test set. Most notably, the results show that the model can classify patients within two years of their first examination as having TMJ involvement with a precision of 0.86 and a sensitivity of 0.7. The results show promise for an AI model in the assessment of TMJ involvement in children and as a decision support tool.</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Hard-Thresholding Meets Evolution Strategies in Reinforcement Learning</b></summary>
  <p><b>编号</b>：[285]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01615">https://arxiv.org/abs/2405.01615</a></p>
  <p><b>作者</b>：Chengqian Gao,  William de Vazelhes,  Hualin Zhang,  Bin Gu,  Zhiqiang Xu</p>
  <p><b>备注</b>：16 pages, including proofs in the appendix</p>
  <p><b>关键词</b>：model-free reinforcement learning, showcasing exemplary performance, Natural Evolution Strategies, Evolution Strategies, reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Evolution Strategies (ES) have emerged as a competitive alternative for model-free reinforcement learning, showcasing exemplary performance in tasks like Mujoco and Atari. Notably, they shine in scenarios with imperfect reward functions, making them invaluable for real-world applications where dense reward signals may be elusive. Yet, an inherent assumption in ES, that all input features are task-relevant, poses challenges, especially when confronted with irrelevant features common in real-world problems. This work scrutinizes this limitation, particularly focusing on the Natural Evolution Strategies (NES) variant. We propose NESHT, a novel approach that integrates Hard-Thresholding (HT) with NES to champion sparsity, ensuring only pertinent features are employed. Backed by rigorous analysis and empirical tests, NESHT demonstrates its promise in mitigating the pitfalls of irrelevant features and shines in complex decision-making problems like noisy Mujoco and Atari tasks.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：A probabilistic estimation of remaining useful life from censored  time-to-event data</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01614">https://arxiv.org/abs/2405.01614</a></p>
  <p><b>作者</b>：Christian Marius Lillelund,  Fernando Pannullo,  Morten Opprud Jakobsen,  Manuel Morante,  Christian Fischer Pedersen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ball bearings plays, RUL, plays an important, important role, censored data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predicting the remaining useful life (RUL) of ball bearings plays an important role in predictive maintenance. A common definition of the RUL is the time until a bearing is no longer functional, which we denote as an event, and many data-driven methods have been proposed to predict the RUL. However, few studies have addressed the problem of censored data, where this event of interest is not observed, and simply ignoring these observations can lead to an overestimation of the failure risk. In this paper, we propose a probabilistic estimation of RUL using survival analysis that supports censored data. First, we analyze sensor readings from ball bearings in the frequency domain and annotate when a bearing starts to deteriorate by calculating the Kullback-Leibler (KL) divergence between the probability density function (PDF) of the current process and a reference PDF. Second, we train several survival models on the annotated bearing dataset, capable of predicting the RUL over a finite time horizon using the survival function. This function is guaranteed to be strictly monotonically decreasing and is an intuitive estimation of the remaining lifetime. We demonstrate our approach in the XJTU-SY dataset using cross-validation and find that Random Survival Forests consistently outperforms both non-neural networks and neural networks in terms of the mean absolute error (MAE). Our work encourages the inclusion of censored data in predictive maintenance models and highlights the unique advantages that survival analysis offers when it comes to probabilistic RUL estimation and early fault detection.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：Unifying and extending Precision Recall metrics for assessing generative  models</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01611">https://arxiv.org/abs/2405.01611</a></p>
  <p><b>作者</b>：Benjamin Sykes,  Loic Simon,  Julien Rabin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Frechet Inception Distance, generative models, image and text, recent success, gained a lot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the recent success of generative models in image and text, the evaluation of generative models has gained a lot of attention. Whereas most generative models are compared in terms of scalar values such as Frechet Inception Distance (FID) or Inception Score (IS), in the last years (Sajjadi et al., 2018) proposed a definition of precision-recall curve to characterize the closeness of two distributions. Since then, various approaches to precision and recall have seen the light (Kynkaanniemi et al., 2019; Naeem et al., 2020; Park & Kim, 2023). They center their attention on the extreme values of precision and recall, but apart from this fact, their ties are elusive. In this paper, we unify most of these approaches under the same umbrella, relying on the work of (Simon et al., 2019). Doing so, we were able not only to recover entire curves, but also to expose the sources of the accounted pitfalls of the concerned metrics. We also provide consistency results that go well beyond the ones presented in the corresponding literature. Last, we study the different behaviors of the curves obtained experimentally.</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Wildfire Risk Prediction: A Review</b></summary>
  <p><b>编号</b>：[292]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01607">https://arxiv.org/abs/2405.01607</a></p>
  <p><b>作者</b>：Zhengsen Xu,  Jonathan Li,  Linlin Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：independent variables, global vegetation, significant impacts, impacts on global, independent</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Wildfires have significant impacts on global vegetation, wildlife, and humans. They destroy plant communities and wildlife habitats and contribute to increased emissions of carbon dioxide, nitrogen oxides, methane, and other pollutants. The prediction of wildfires relies on various independent variables combined with regression or machine learning methods. In this technical review, we describe the options for independent variables, data processing techniques, models, independent variables collinearity and importance estimation methods, and model performance evaluation metrics. First, we divide the independent variables into 4 aspects, including climate and meteorology conditions, socio-economical factors, terrain and hydrological features, and wildfire historical records. Second, preprocessing methods are described for different magnitudes, different spatial-temporal resolutions, and different formats of data. Third, the collinearity and importance evaluation methods of independent variables are also considered. Fourth, we discuss the application of statistical models, traditional machine learning models, and deep learning models in wildfire risk prediction. In this subsection, compared with other reviews, this manuscript particularly discusses the evaluation metrics and recent advancements in deep learning methods. Lastly, addressing the limitations of current research, this paper emphasizes the need for more effective deep learning time series forecasting algorithms, the utilization of three-dimensional data including ground and trunk fuel, extraction of more accurate historical fire point data, and improved model evaluation metrics.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：KITE: A Kernel-based Improved Transferability Estimation Method</b></summary>
  <p><b>编号</b>：[293]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01603">https://arxiv.org/abs/2405.01603</a></p>
  <p><b>作者</b>：Yunhui Guo</p>
  <p><b>备注</b>：14 pages</p>
  <p><b>关键词</b>：Transferability estimation, transferability estimation method, Improved Transferability Estimation, pre-trained, transfer learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Transferability estimation has emerged as an important problem in transfer learning. A transferability estimation method takes as inputs a set of pre-trained models and decides which pre-trained model can deliver the best transfer learning performance. Existing methods tackle this problem by analyzing the output of the pre-trained model or by comparing the pre-trained model with a probe model trained on the target dataset. However, neither is sufficient to provide reliable and efficient transferability estimations. In this paper, we present a novel perspective and introduce Kite, as a Kernel-based Improved Transferability Estimation method. Kite is based on the key observations that the separability of the pre-trained features and the similarity of the pre-trained features to random features are two important factors for estimating transferability. Inspired by kernel methods, Kite adopts centered kernel alignment as an effective way to assess feature separability and feature similarity. Kite is easy to interpret, fast to compute, and robust to the target dataset size. We evaluate the performance of Kite on a recently introduced large-scale model selection benchmark. The benchmark contains 8 source dataset, 6 target datasets and 4 architectures with a total of 32 pre-trained models. Extensive results show that Kite outperforms existing methods by a large margin for transferability estimation.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Efficient Sample-Specific Encoder Perturbations</b></summary>
  <p><b>编号</b>：[294]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01601">https://arxiv.org/abs/2405.01601</a></p>
  <p><b>作者</b>：Yassir Fathullah,  Mark J. F. Gales</p>
  <p><b>备注</b>：To appear in NAACL 2024</p>
  <p><b>关键词</b>：autoregressive sequence tasks, sequence tasks, range of autoregressive, autoregressive sequence, attribute of interest</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Encoder-decoder foundation models have displayed state-of-the-art performance on a range of autoregressive sequence tasks. This paper proposes a simple and lightweight modification to such systems to control the behaviour according to a specific attribute of interest. This paper proposes a novel inference-efficient approach to modifying the behaviour of an encoder-decoder system according to a specific attribute of interest. Specifically, we show that a small proxy network can be used to find a sample-by-sample perturbation of the encoder output of a frozen foundation model to trigger the decoder to generate improved decodings. This work explores a specific realization of this framework focused on improving the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper foundation models on Speech Recognition. Results display consistent improvements in performance evaluated through COMET and WER respectively. Furthermore, experiments also show that the proxies are robust to the exact nature of the data used to train them and can extend to other domains.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Improve Academic Query Resolution through BERT-based Question Extraction  from Images</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01587">https://arxiv.org/abs/2405.01587</a></p>
  <p><b>作者</b>：Nidhi Kamal,  Saurabh Yadav,  Jorawar Singh,  Aditi Avasthi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Providing fast, essential solution provided, fast and accurate, Edtech organizations, essential solution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Lightweight Conceptual Dictionary Learning for Text Classification Using  Information Compression</b></summary>
  <p><b>编号</b>：[306]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01584">https://arxiv.org/abs/2405.01584</a></p>
  <p><b>作者</b>：Li Wan,  Tansu Alpcan,  Margreta Kuijper,  Emanuele Viterbo</p>
  <p><b>备注</b>：12 pages, TKDE format</p>
  <p><b>关键词</b>：lightweight supervised dictionary, supervised dictionary learning, dictionary learning framework, lightweight supervised, text classification based</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a novel, lightweight supervised dictionary learning framework for text classification based on data compression and representation. This two-phase algorithm initially employs the Lempel-Ziv-Welch (LZW) algorithm to construct a dictionary from text datasets, focusing on the conceptual significance of dictionary elements. Subsequently, dictionaries are refined considering label data, optimizing dictionary atoms to enhance discriminative power based on mutual information and class distribution. This process generates discriminative numerical representations, facilitating the training of simple classifiers such as SVMs and neural networks. We evaluate our algorithm's information-theoretic performance using information bottleneck principles and introduce the information plane area rank (IPAR) as a novel metric to quantify the information-theoretic performance. Tested on six benchmark text datasets, our algorithm competes closely with top models, especially in limited-vocabulary contexts, using significantly fewer parameters. \review{Our algorithm closely matches top-performing models, deviating by only ~2\% on limited-vocabulary datasets, using just 10\% of their parameters. However, it falls short on diverse-vocabulary datasets, likely due to the LZW algorithm's constraints with low-repetition data. This contrast highlights its efficiency and limitations across different dataset types.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology  with Multimodal Learning</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01583">https://arxiv.org/abs/2405.01583</a></p>
  <p><b>作者</b>：Nadia Saeed</p>
  <p><b>备注</b>：7 pages, 3 figures, Clinical NLP 2024 workshop proceedings in Shared Task</p>
  <p><b>关键词</b>：wai Yim, challenge necessitates, necessitates novel solutions, Multimodal Medical Answer, Yim</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual & Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a). This paper addresses the limitations of traditional methods by proposing a weakly supervised learning approach for open-ended medical question-answering (QA). Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations. Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion. This approach tackles complex, open-ended questions even without predefined answer choices. We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images. This work advances medical QA research, paving the way for clinical decision support systems and ultimately improving healthcare delivery.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Text Quality-Based Pruning for Efficient Training of Language Models</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01582">https://arxiv.org/abs/2405.01582</a></p>
  <p><b>作者</b>：Vasu Sharma,  Karthik Padthe,  Newsha Ardalani,  Kushal Tirumala,  Russell Howes,  Hu Xu,  Po-Yao Huang,  Shang-Wen Li,  Armen Aghajanyan,  Gargi Ghosh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：process extremely laborious, times training Language, training Language Models, training process extremely, recent times training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text quality in large unlabelled NLP datasets in a model agnostic manner to assign the text instances a "quality score".
By proposing the text quality metric, the paper establishes a framework to identify and eliminate low-quality text instances, leading to improved training efficiency for LM models. Experimental results over multiple models and datasets demonstrate the efficacy of this approach, showcasing substantial gains in training effectiveness and highlighting the potential for resource-efficient LM training.
For example, we observe an absolute accuracy improvement of 0.9% averaged over 14 downstream evaluation tasks for multiple LM models while using 40% lesser data and training 42% faster when training on the OpenWebText dataset and 0.8% average absolute accuracy improvement while using 20% lesser data and training 21% faster on the Wikipedia dataset.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Mining patterns in syntax trees to automate code reviews of student  solutions for programming exercises</b></summary>
  <p><b>编号</b>：[311]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01579">https://arxiv.org/abs/2405.01579</a></p>
  <p><b>作者</b>：Charlotte Van Petegem,  Kasper Demeyere,  Rien Maertens,  Niko Strijbol,  Bram De Wever,  Bart Mesuere,  Peter Dawyndt</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：programming education, essential but labour-intensive, posing challenges, consistency and timeliness, challenges in consistency</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In programming education, providing manual feedback is essential but labour-intensive, posing challenges in consistency and timeliness. We introduce ECHO, a machine learning method to automate the reuse of feedback in educational code reviews by analysing patterns in abstract syntax trees. This study investigates two primary questions: whether ECHO can predict feedback annotations to specific lines of student code based on previously added annotations by human reviewers (RQ1), and whether its training and prediction speeds are suitable for using ECHO for real-time feedback during live code reviews by human reviewers (RQ2). Our results, based on annotations from both automated linting tools and human reviewers, show that ECHO can accurately and quickly predict appropriate feedback annotations. Its efficiency in processing and its flexibility in adapting to feedback patterns can significantly reduce the time and effort required for manual feedback provisioning in educational settings.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：HateTinyLLM : Hate Speech Detection Using Tiny Large Language Models</b></summary>
  <p><b>编号</b>：[313]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01577">https://arxiv.org/abs/2405.01577</a></p>
  <p><b>作者</b>：Tanmay Sen,  Ansuman Das,  Mrinmay Sen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：speech encompasses verbal, Hate speech encompasses, encompasses verbal, sensitive characteristics, Hate speech</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hate speech encompasses verbal, written, or behavioral communication that targets derogatory or discriminatory language against individuals or groups based on sensitive characteristics. Automated hate speech detection plays a crucial role in curbing its propagation, especially across social media platforms. Various methods, including recent advancements in deep learning, have been devised to address this challenge. In this study, we introduce HateTinyLLM, a novel framework based on fine-tuned decoder-only tiny large language models (tinyLLMs) for efficient hate speech detection. Our experimental findings demonstrate that the fine-tuned HateTinyLLM outperforms the pretrained mixtral-7b model by a significant margin. We explored various tiny LLMs, including PY007/TinyLlama-1.1B-step-50K-105b, Microsoft/phi-2, and facebook/opt-1.3b, and fine-tuned them using LoRA and adapter methods. Our observations indicate that all LoRA-based fine-tuned models achieved over 80\% accuracy.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Uncovering Deceptive Tendencies in Language Models: A Simulated Company  AI Assistant</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01576">https://arxiv.org/abs/2405.01576</a></p>
  <p><b>作者</b>：Olli Järviniemi,  Evan Hubinger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：realistic simulation setting, study the tendency, systems to deceive, deceive by constructing, simulation setting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant. The simulated company employees provide tasks for the assistant to complete, these tasks spanning writing assistance, information retrieval and programming. We then introduce situations where the model might be inclined to behave deceptively, while taking care to not instruct or otherwise pressure the model to do so. Across different scenarios, we find that Claude 3 Opus
1) complies with a task of mass-generating comments to influence public perception of the company, later deceiving humans about it having done so,
2) lies to auditors when asked questions, and
3) strategically pretends to be less capable than it is during capability evaluations.
Our work demonstrates that even models trained to be helpful, harmless and honest sometimes behave deceptively in realistic scenarios, without notable external pressure to do so.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：Mitigating LLM Hallucinations via Conformal Abstention</b></summary>
  <p><b>编号</b>：[325]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01563">https://arxiv.org/abs/2405.01563</a></p>
  <p><b>作者</b>：Yasin Abbasi Yadkori,  Ilja Kuzborskij,  David Stutz,  András György,  Adam Fisch,  Arnaud Doucet,  Iuliya Beloshapka,  Wei-Hung Weng,  Yao-Yuan Yang,  Csaba Szepesvári,  Ali Taylan Cemgil,  Nenad Tomasev</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language model, abstain from responding, general domain, resorting to possibly, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying "I don't know") in a general domain, instead of resorting to possibly "hallucinating" a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：Untangling Knots: Leveraging LLM for Error Resolution in Computational  Notebooks</b></summary>
  <p><b>编号</b>：[329]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01559">https://arxiv.org/abs/2405.01559</a></p>
  <p><b>作者</b>：Konstantin Grotov,  Sergey Titov,  Yaroslav Zharov,  Timofey Bryksin</p>
  <p><b>备注</b>：accepted at 1st ACM CHI Workshop on Human-Notebook Interactions</p>
  <p><b>关键词</b>：offering unprecedented interactivity, development process, offering unprecedented, research-related development, unprecedented interactivity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Computational notebooks became indispensable tools for research-related development, offering unprecedented interactivity and flexibility in the development process. However, these benefits come at the cost of reproducibility and an increased potential for bugs. There are many tools for bug fixing; however, they are generally targeted at the classical linear code. With the rise of code-fluent Large Language Models, a new stream of smart bug-fixing tools has emerged. However, the applicability of those tools is still problematic for non-linear computational notebooks. In this paper, we propose a potential solution for resolving errors in computational notebooks via an iterative LLM-based agent. We discuss the questions raised by this approach and share a novel dataset of computational notebooks containing bugs to facilitate the research of the proposed approach.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：Configurable Learned Holography</b></summary>
  <p><b>编号</b>：[330]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01558">https://arxiv.org/abs/2405.01558</a></p>
  <p><b>作者</b>：Yicheng Zhan,  Liang Shi,  Wojciech Matusik,  Qi Sun,  Kaan Akşit</p>
  <p><b>备注</b>：14 pages, 5 figures</p>
  <p><b>关键词</b>：holographic display technology, advancing holographic display, holographic displays, existing holographic displays, persistent roadblock</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the pursuit of advancing holographic display technology, we face a unique yet persistent roadblock: the inflexibility of learned holography in adapting to various hardware configurations.
This is due to the variances in the complex optical components and system settings in existing holographic displays.
Although the emerging learned approaches have enabled rapid and high-quality hologram generation, any alteration in display hardware still requires a retraining of the model.
Our work introduces a configurable learned model that interactively computes 3D holograms from RGB-only 2D images for a variety of holographic displays.
The model can be conditioned to predefined hardware parameters of existing holographic displays such as working wavelengths, pixel pitch, propagation distance, and peak brightness without having to retrain.
In addition, our model accommodates various hologram types, including conventional single-color and emerging multi-color holograms that simultaneously use multiple color primaries in holographic displays.
Notably, we enabled our hologram computations to rely on identifying the correlation between depth estimation and 3D hologram synthesis tasks within the learning domain for the first time in the literature.
We employ knowledge distillation via a student-teacher learning strategy to streamline our model for interactive performance.
Achieving up to a 2x speed improvement compared to state-of-the-art models while consistently generating high-quality 3D holograms with different hardware configurations.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：An Experimental Study on the Rashomon Effect of Balancing Methods in  Imbalanced Classification</b></summary>
  <p><b>编号</b>：[331]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01557">https://arxiv.org/abs/2405.01557</a></p>
  <p><b>作者</b>：Mustafa Cavus,  Przemysław Biecek</p>
  <p><b>备注</b>：16 pages, 6 figures</p>
  <p><b>关键词</b>：Rashomon effect, generate biased predictions, classifying imbalanced datasets, predictive multiplicity, generate biased</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predictive models may generate biased predictions when classifying imbalanced datasets. This happens when the model favors the majority class, leading to low performance in accurately predicting the minority class. To address this issue, balancing or resampling methods are critical pre-processing steps in the modeling process. However, there have been debates and questioning of the functionality of these methods in recent years. In particular, many candidate models may exhibit very similar predictive performance, which is called the Rashomon effect, in model selection. Selecting one of them without considering predictive multiplicity which is the case of yielding conflicting models' predictions for any sample may lead to a loss of using another model. In this study, in addition to the existing debates, the impact of balancing methods on predictive multiplicity is examined through the Rashomon effect. It is important because the blind model selection is risky from a set of approximately equally accurate models. This may lead to serious problems in model selection, validation, and explanation. To tackle this matter, we conducted real dataset experiments to observe the impact of balancing methods on predictive multiplicity through the Rashomon effect. Our findings showed that balancing methods inflate the predictive multiplicity, and they yield varying results. To monitor the trade-off between performance and predictive multiplicity for conducting the modeling process responsibly, we proposed using the extended performance-gain plot for the Rashomon effect.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Early-stage detection of cognitive impairment by hybrid  quantum-classical algorithm using resting-state functional MRI time-series</b></summary>
  <p><b>编号</b>：[334]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01554">https://arxiv.org/abs/2405.01554</a></p>
  <p><b>作者</b>：Junggu Choi,  Tak Hur,  Daniel K. Park,  Na-Young Shin,  Seung-Koo Lee,  Hakbae Lee,  Sanghoon Han</p>
  <p><b>备注</b>：28 pages, 10 figures</p>
  <p><b>关键词</b>：machine learning techniques, quantum machine learning, machine learning algorithms, machine learning, quantum machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Following the recent development of quantum machine learning techniques, the literature has reported several quantum machine learning algorithms for disease detection. This study explores the application of a hybrid quantum-classical algorithm for classifying region-of-interest time-series data obtained from resting-state functional magnetic resonance imaging in patients with early-stage cognitive impairment based on the importance of cognitive decline for dementia or aging. Classical one-dimensional convolutional layers are used together with quantum convolutional neural networks in our hybrid algorithm. In the classical simulation, the proposed hybrid algorithms showed higher balanced accuracies than classical convolutional neural networks under the similar training conditions. Moreover, a total of nine brain regions (left precentral gyrus, right superior temporal gyrus, left rolandic operculum, right rolandic operculum, left parahippocampus, right hippocampus, left medial frontal gyrus, right cerebellum crus, and cerebellar vermis) among 116 brain regions were found to be relatively effective brain regions for the classification based on the model performances. The associations of the selected nine regions with cognitive decline, as found in previous studies, were additionally validated through seed-based functional connectivity analysis. We confirmed both the improvement of model performance with the quantum convolutional neural network and neuroscientific validities of brain regions from our hybrid quantum-classical model.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Universal Imitation Games</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01540">https://arxiv.org/abs/2405.01540</a></p>
  <p><b>作者</b>：Sridhar Mahadevan</p>
  <p><b>备注</b>：98 pages. arXiv admin note: substantial text overlap with arXiv:2402.18732</p>
  <p><b>关键词</b>：Alan Turing proposed, Alan Turing, Turing proposed, participants, Turing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Alan Turing proposed in 1950 a framework called an imitation game to decide if a machine could think. Using mathematics developed largely after Turing -- category theory -- we analyze a broader class of universal imitation games (UIGs), which includes static, dynamic, and evolutionary games. In static games, the participants are in a steady state. In dynamic UIGs, "learner" participants are trying to imitate "teacher" participants over the long run. In evolutionary UIGs, the participants are competing against each other in an evolutionary game, and participants can go extinct and be replaced by others with higher fitness. We use the framework of category theory -- in particular, two influential results by Yoneda -- to characterize each type of imitation game. Universal properties in categories are defined by initial and final objects. We characterize dynamic UIGs where participants are learning by inductive inference as initial algebras over well-founded sets, and contrast them with participants learning by conductive inference over the final coalgebra of non-well-founded sets. We briefly discuss the extension of our categorical framework for UIGs to imitation games on quantum computers.</p>
  </details>
</details>
<details>
  <summary>100. <b>标题：Fair Risk Control: A Generalized Framework for Calibrating Multi-group  Fairness Risks</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02225">https://arxiv.org/abs/2405.02225</a></p>
  <p><b>作者</b>：Lujing Zhang,  Aaron Roth,  Linjun Zhang</p>
  <p><b>备注</b>：28 pages, 8 figures, accepted by ICML2024</p>
  <p><b>关键词</b>：post-processing machine learning, multi-group fairness guarantees, predictions satisfy multi-group, machine learning models, satisfy multi-group fairness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees. Based on the celebrated notion of multicalibration, we introduce $(\mathbf{s},\mathcal{G}, \alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for multi-dimensional mappings $\mathbf{s}$, constraint set $\mathcal{G}$, and a pre-specified threshold level $\alpha$. We propose associated algorithms to achieve this notion in general settings. This framework is then applied to diverse scenarios encompassing different fairness concerns, including false negative rate control in image segmentation, prediction set conditional uncertainty quantification in hierarchical classification, and de-biased text generation in language models. We conduct numerical studies on several datasets and tasks.</p>
  </details>
</details>
<details>
  <summary>101. <b>标题：Regularized Q-learning through Robust Averaging</b></summary>
  <p><b>编号</b>：[346]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02201">https://arxiv.org/abs/2405.02201</a></p>
  <p><b>作者</b>：Peter Schmitt-Förster,  Tobias Sutter</p>
  <p><b>备注</b>：26 pages, 5 figures</p>
  <p><b>关键词</b>：principled manner, addresses some weaknesses, Q-learning, Q-learning variant, distributionally robust estimator</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We propose a new Q-learning variant, called 2RA Q-learning, that addresses some weaknesses of existing Q-learning methods in a principled manner. One such weakness is an underlying estimation bias which cannot be controlled and often results in poor performance. We propose a distributionally robust estimator for the maximum expected value term, which allows us to precisely control the level of estimation bias introduced. The distributionally robust estimator admits a closed-form solution such that the proposed algorithm has a computational cost per iteration comparable to Watkins' Q-learning. For the tabular case, we show that 2RA Q-learning converges to the optimal policy and analyze its asymptotic mean-squared error. Lastly, we conduct numerical experiments for various settings, which corroborate our theoretical findings and indicate that 2RA Q-learning often performs better than existing methods.</p>
  </details>
</details>
<details>
  <summary>102. <b>标题：Optimistic Regret Bounds for Online Learning in Adversarial Markov  Decision Processes</b></summary>
  <p><b>编号</b>：[347]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02188">https://arxiv.org/abs/2405.02188</a></p>
  <p><b>作者</b>：Sang Bin Moon,  Abolfazl Hashemi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Markov Decision Process, Adversarial Markov Decision, Decision Process, Markov Decision, Adversarial Markov</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Adversarial Markov Decision Process (AMDP) is a learning framework that deals with unknown and varying tasks in decision-making applications like robotics and recommendation systems. A major limitation of the AMDP formalism, however, is pessimistic regret analysis results in the sense that although the cost function can change from one episode to the next, the evolution in many settings is not adversarial. To address this, we introduce and study a new variant of AMDP, which aims to minimize regret while utilizing a set of cost predictors. For this setting, we develop a new policy search method that achieves a sublinear optimistic regret with high probability, that is a regret bound which gracefully degrades with the estimation power of the cost predictors. Establishing such optimistic regret bounds is nontrivial given that (i) as we demonstrate, the existing importance-weighted cost estimators cannot establish optimistic bounds, and (ii) the feedback model of AMDP is different (and more realistic) than the existing optimistic online learning works. Our result, in particular, hinges upon developing a novel optimistically biased cost estimator that leverages cost predictors and enables a high-probability regret analysis without imposing restrictive assumptions. We further discuss practical extensions of the proposed scheme and demonstrate its efficacy numerically.</p>
  </details>
</details>
<details>
  <summary>103. <b>标题：TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on  Self-Supervised Learning and Knowledge Transfer</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02124">https://arxiv.org/abs/2405.02124</a></p>
  <p><b>作者</b>：Noé Tits,  Prernna Bhatnagar,  Thierry Dutoit</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Connectionist Temporal Classification, Montreal Forced Aligner, text independent, alignment based, knowledge transfer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a novel approach for text independent phone-to-audio alignment based on phoneme recognition, representation learning and knowledge transfer. Our method leverages a self-supervised model (wav2vec2) fine-tuned for phoneme recognition using a Connectionist Temporal Classification (CTC) loss, a dimension reduction model and a frame-level phoneme classifier trained thanks to forced-alignment labels (using Montreal Forced Aligner) to produce multi-lingual phonetic representations, thus requiring minimal additional training. We evaluate our model using synthetic native data from the TIMIT dataset and the SCRIBE dataset for American and British English, respectively. Our proposed model outperforms the state-of-the-art (charsiu) in statistical metrics and has applications in language learning and speech processing systems. We leave experiments on other languages for future work but the design of the system makes it easily adaptable to other languages.</p>
  </details>
</details>
<details>
  <summary>104. <b>标题：Discrete Aware Matrix Completion via Convexized $\ell_0$-Norm  Approximation</b></summary>
  <p><b>编号</b>：[351]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02101">https://arxiv.org/abs/2405.02101</a></p>
  <p><b>作者</b>：Niclas Führling,  Kengo Ando,  Giuseppe Thadeu Freitas de Abreu,  David González G.,  Osvaldo Gonsa</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：common recommender systems, discrete alphabet set, partially observed low-rank, observed low-rank matrices, finite discrete alphabet</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We consider a novel algorithm, for the completion of partially observed low-rank matrices in a structured setting where each entry can be chosen from a finite discrete alphabet set, such as in common recommender systems. The proposed low-rank matrix completion (MC) method is an improved variation of state-of-the-art (SotA) discrete aware matrix completion method which we previously proposed, in which discreteness is enforced by an $\ell_0$-norm regularizer, not by replaced with the $\ell_1$-norm, but instead approximated by a continuous and differentiable function normalized via fractional programming (FP) under a proximal gradient (PG) framework. Simulation results demonstrate the superior performance of the new method compared to the SotA techniques as well as the earlier $\ell_1$-norm-based discrete-aware matrix completion approach.</p>
  </details>
</details>
<details>
  <summary>105. <b>标题：A comparative study of conformal prediction methods for valid  uncertainty quantification in machine learning</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02082">https://arxiv.org/abs/2405.02082</a></p>
  <p><b>作者</b>：Nicolas Dewolf</p>
  <p><b>备注</b>：At 339 pages, this document is a live/working version of my PhD dissertation published in 2024 by the University of Ghent (UGent)</p>
  <p><b>关键词</b>：optimizing predictive models, past decades, analysis and machine, machine learning, learning was focused</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the past decades, most work in the area of data analysis and machine learning was focused on optimizing predictive models and getting better results than what was possible with existing models. To what extent the metrics with which such improvements were measured were accurately capturing the intended goal, whether the numerical differences in the resulting values were significant, or whether uncertainty played a role in this study and if it should have been taken into account, was of secondary importance. Whereas probability theory, be it frequentist or Bayesian, used to be the gold standard in science before the advent of the supercomputer, it was quickly replaced in favor of black box models and sheer computing power because of their ability to handle large data sets. This evolution sadly happened at the expense of interpretability and trustworthiness. However, while people are still trying to improve the predictive power of their models, the community is starting to realize that for many applications it is not so much the exact prediction that is of importance, but rather the variability or uncertainty.
The work in this dissertation tries to further the quest for a world where everyone is aware of uncertainty, of how important it is and how to embrace it instead of fearing it. A specific, though general, framework that allows anyone to obtain accurate uncertainty estimates is singled out and analysed. Certain aspects and applications of the framework -- dubbed `conformal prediction' -- are studied in detail. Whereas many approaches to uncertainty quantification make strong assumptions about the data, conformal prediction is, at the time of writing, the only framework that deserves the title `distribution-free'. No parametric assumptions have to be made and the nonparametric results also hold without having to resort to the law of large numbers in the asymptotic regime.</p>
  </details>
</details>
<details>
  <summary>106. <b>标题：Mathematics of statistical sequential decision-making: concentration,  risk-awareness and modelling in stochastic bandits, with applications to  bariatric surgery</b></summary>
  <p><b>编号</b>：[354]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01994">https://arxiv.org/abs/2405.01994</a></p>
  <p><b>作者</b>：Patrick Saux</p>
  <p><b>备注</b>：Doctoral thesis. Some pdf readers (e.g. Firefox) have trouble rendering the theorems/definitions environment. When reading online, please prefer e.g. Chrome</p>
  <p><b>关键词</b>：statistical sequential decision-making, sequential decision-making algorithms, thesis aims, aims to study, mathematical challenges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This thesis aims to study some of the mathematical challenges that arise in the analysis of statistical sequential decision-making algorithms for postoperative patients follow-up. Stochastic bandits (multiarmed, contextual) model the learning of a sequence of actions (policy) by an agent in an uncertain environment in order to maximise observed rewards. To learn optimal policies, bandit algorithms have to balance the exploitation of current knowledge and the exploration of uncertain actions. Such algorithms have largely been studied and deployed in industrial applications with large datasets, low-risk decisions and clear modelling assumptions, such as clickthrough rate maximisation in online advertising. By contrast, digital health recommendations call for a whole new paradigm of small samples, risk-averse agents and complex, nonparametric modelling. To this end, we developed new safe, anytime-valid concentration bounds, (Bregman, empirical Chernoff), introduced a new framework for risk-aware contextual bandits (with elicitable risk measures) and analysed a novel class of nonparametric bandit algorithms under weak assumptions (Dirichlet sampling). In addition to the theoretical guarantees, these results are supported by in-depth empirical evidence. Finally, as a first step towards personalised postoperative follow-up recommendations, we developed with medical doctors and surgeons an interpretable machine learning model to predict the long-term weight trajectories of patients after bariatric surgery.</p>
  </details>
</details>
<details>
  <summary>107. <b>标题：Understanding LLMs Requires More Than Statistical Generalization</b></summary>
  <p><b>编号</b>：[357]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01964">https://arxiv.org/abs/2405.01964</a></p>
  <p><b>作者</b>：Patrik Reizinger,  Szilvia Ujváry,  Anna Mészáros,  Anna Kerekes,  Wieland Brendel,  Ferenc Huszár</p>
  <p><b>备注</b>：Accepted at ICML2024</p>
  <p><b>关键词</b>：deep learning generalize, deep learning theory, learning theory attempting, attempting to answer, deep learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The last decade has seen blossoming research in deep learning theory attempting to answer, "Why does deep learning generalize?" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart -- thus, equivalent test loss -- can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.</p>
  </details>
</details>
<details>
  <summary>108. <b>标题：Three Quantization Regimes for ReLU Networks</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01952">https://arxiv.org/abs/2405.01952</a></p>
  <p><b>作者</b>：Weigutian Ou,  Philipp Schenkel,  Helmut Bölcskei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Lipschitz functions, minimax approximation error, establish the fundamental, fundamental limits, approximation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We establish the fundamental limits in the approximation of Lipschitz functions by deep ReLU neural networks with finite-precision weights. Specifically, three regimes, namely under-, over-, and proper quantization, in terms of minimax approximation error behavior as a function of network weight precision, are identified. This is accomplished by deriving nonasymptotic tight lower and upper bounds on the minimax approximation error. Notably, in the proper-quantization regime, neural networks exhibit memory-optimality in the approximation of Lipschitz functions. Deep networks have an inherent advantage over shallow networks in achieving memory-optimality. We also develop the notion of depth-precision tradeoff, showing that networks with high-precision weights can be converted into functionally equivalent deeper networks with low-precision weights, while preserving memory-optimality. This idea is reminiscent of sigma-delta analog-to-digital conversion, where oversampling rate is traded for resolution in the quantization of signal samples. We improve upon the best-known ReLU network approximation results for Lipschitz functions and describe a refinement of the bit extraction technique which could be of independent general interest.</p>
  </details>
</details>
<details>
  <summary>109. <b>标题：Explainable Risk Classification in Financial Reports</b></summary>
  <p><b>编号</b>：[361]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01881">https://arxiv.org/abs/2405.01881</a></p>
  <p><b>作者</b>：Xue Wen Tan,  Stanley Kok</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：publicly traded company, file an annual, publicly traded, required to file, wealth of information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Every publicly traded company in the US is required to file an annual 10-K financial report, which contains a wealth of information about the company. In this paper, we propose an explainable deep-learning model, called FinBERT-XRC, that takes a 10-K report as input, and automatically assesses the post-event return volatility risk of its associated company. In contrast to previous systems, our proposed model simultaneously offers explanations of its classification decision at three different levels: the word, sentence, and corpus levels. By doing so, our model provides a comprehensive interpretation of its prediction to end users. This is particularly important in financial domains, where the transparency and accountability of algorithmic predictions play a vital role in their application to decision-making processes. Aside from its novel interpretability, our model surpasses the state of the art in predictive accuracy in experiments on a large real-world dataset of 10-K reports spanning six years.</p>
  </details>
</details>
<details>
  <summary>110. <b>标题：Multivariate Bayesian Last Layer for Regression: Uncertainty  Quantification and Disentanglement</b></summary>
  <p><b>编号</b>：[366]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01761">https://arxiv.org/abs/2405.01761</a></p>
  <p><b>作者</b>：Han Wang,  Eiji Kawasaki,  Guillaume Damblin,  Geoffrey Daniel</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Layer combines Bayesian, Bayesian Last Layer, heteroscedastic noise, parameter learning, Layer models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We present new Bayesian Last Layer models in the setting of multivariate regression under heteroscedastic noise, and propose an optimization algorithm for parameter learning. Bayesian Last Layer combines Bayesian modelling of the predictive distribution with neural networks for parameterization of the prior, and has the attractive property of uncertainty quantification with a single forward pass. The proposed framework is capable of disentangling the aleatoric and epistemic uncertainty, and can be used to transfer a canonically trained deep neural network to new data domains with uncertainty-aware capability.</p>
  </details>
</details>
<details>
  <summary>111. <b>标题：Sample-efficient neural likelihood-free Bayesian inference of implicit  HMMs</b></summary>
  <p><b>编号</b>：[369]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01737">https://arxiv.org/abs/2405.01737</a></p>
  <p><b>作者</b>：Sanmitra Ghosh,  Paul J. Birrell,  Daniela De Angelis</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：neural conditional density, conditional density estimation, inference methods based, Hidden Markov model, based on neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Likelihood-free inference methods based on neural conditional density estimation were shown to drastically reduce the simulation burden in comparison to classical methods such as ABC. When applied in the context of any latent variable model, such as a Hidden Markov model (HMM), these methods are designed to only estimate the parameters, rather than the joint distribution of the parameters and the hidden states. Naive application of these methods to a HMM, ignoring the inference of this joint posterior distribution, will thus produce an inaccurate estimate of the posterior predictive distribution, in turn hampering the assessment of goodness-of-fit. To rectify this problem, we propose a novel, sample-efficient likelihood-free method for estimating the high-dimensional hidden states of an implicit HMM. Our approach relies on learning directly the intractable posterior distribution of the hidden states, using an autoregressive-flow, by exploiting the Markov property. Upon evaluating our approach on some implicit HMMs, we found that the quality of the estimates retrieved using our method is comparable to what can be achieved using a much more computationally expensive SMC algorithm.</p>
  </details>
</details>
<details>
  <summary>112. <b>标题：SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral  Image Denoising</b></summary>
  <p><b>编号</b>：[371]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01726">https://arxiv.org/abs/2405.01726</a></p>
  <p><b>作者</b>：Guanyiman Fu,  Fengchao Xiong,  Jianfeng Lu,  Jun Zhou,  Yuntao Qian</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：crucial preprocessing procedure, preprocessing procedure due, environmental factors, crucial preprocessing, preprocessing procedure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Denoising hyperspectral images (HSIs) is a crucial preprocessing procedure due to the noise originating from intra-imaging mechanisms and environmental factors. Utilizing domain-specific knowledge of HSIs, such as spectral correlation, spatial self-similarity, and spatial-spectral correlation, is essential for deep learning-based denoising. Existing methods are often constrained by running time, space complexity, and computational complexity, employing strategies that explore these priors separately. While the strategies can avoid some redundant information, considering that hyperspectral images are 3-D images with strong spatial continuity and spectral correlation, this kind of strategy inevitably overlooks subtle long-range spatial-spectral information that positively impacts image restoration. This paper proposes a Spatial-Spectral Selective State Space Model-based U-shaped network, termed Spatial-Spectral U-Mamba (SSUMamba), for hyperspectral image denoising. We can obtain complete global spatial-spectral correlation within a module thanks to the linear space complexity in State Space Model (SSM) computations. We introduce an Alternating Scan (SSAS) strategy for HSI data, which helps model the information flow in multiple directions in 3-D HSIs. Experimental results demonstrate that our method outperforms several compared methods. The source code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>113. <b>标题：Development of Skip Connection in Deep Neural Networks for Computer  Vision and Medical Image Analysis: A Survey</b></summary>
  <p><b>编号</b>：[372]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01725">https://arxiv.org/abs/2405.01725</a></p>
  <p><b>作者</b>：Guoping Xu,  Xiaxia Wang,  Xinglong Wu,  Xuesong Leng,  Yongchao Xu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：made significant progress, skip connections, deep neural networks, residual learning, neural networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning has made significant progress in computer vision, specifically in image classification, object detection, and semantic segmentation. The skip connection has played an essential role in the architecture of deep neural networks,enabling easier optimization through residual learning during the training stage and improving accuracy during testing. Many neural networks have inherited the idea of residual learning with skip connections for various tasks, and it has been the standard choice for designing neural networks. This survey provides a comprehensive summary and outlook on the development of skip connections in deep neural networks. The short history of skip connections is outlined, and the development of residual learning in deep neural networks is surveyed. The effectiveness of skip connections in the training and testing stages is summarized, and future directions for using skip connections in residual learning are discussed. Finally, we summarize seminal papers, source code, models, and datasets that utilize skip connections in computer vision, including image classification, object detection, semantic segmentation, and image reconstruction. We hope this survey could inspire peer researchers in the community to develop further skip connections in various forms and tasks and the theory of residual learning in deep neural networks. The project page can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>114. <b>标题：Generative Active Learning for the Search of Small-molecule Protein  Binders</b></summary>
  <p><b>编号</b>：[376]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01616">https://arxiv.org/abs/2405.01616</a></p>
  <p><b>作者</b>：Maksym Korablyov,  Cheng-Hao Liu,  Moksh Jain,  Almer M. van der Sloot,  Eric Jolicoeur,  Edward Ruediger,  Andrei Cristian Nica,  Emmanuel Bengio,  Kostiantyn Lapchevskyi,  Daniel St-Cyr,  Doris Alexandra Schuetz,  Victor Ion Butoi,  Jarrid Rector-Brooks,  Simon Blackburn,  Leo Feng,  Hadi Nekoei,  SaiKrishna Gottipati,  Priyesh Vijayan,  Prateek Gupta,  Ladislav Rampášek,  Sasikanth Avancha,  Pierre-Luc Bacon,  William L. Hamilton,  Brooks Paige,  Sanchit Misra,  Stanislaw Kamil Jastrzebski,  Bharat Kaul,  Doina Precup,  José Miguel Hernández-Lobato,  Marwin Segler,  Michael Bronstein,  Anne Marinier,  Mike Tyers,  Yoshua Bengio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, significant challenge, substantial progress, progress in machine, scientific discovery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite substantial progress in machine learning for scientific discovery in recent years, truly de novo design of small molecules which exhibit a property of interest remains a significant challenge. We introduce LambdaZero, a generative active learning approach to search for synthesizable molecules. Powered by deep reinforcement learning, LambdaZero learns to search over the vast space of molecules to discover candidates with a desired property. We apply LambdaZero with molecular docking to design novel small molecules that inhibit the enzyme soluble Epoxide Hydrolase 2 (sEH), while enforcing constraints on synthesizability and drug-likeliness. LambdaZero provides an exponential speedup in terms of the number of calls to the expensive molecular docking oracle, and LambdaZero de novo designed molecules reach docking scores that would otherwise require the virtual screening of a hundred billion molecules. Importantly, LambdaZero discovers novel scaffolds of synthesizable, drug-like inhibitors for sEH. In in vitro experimental validation, a series of ligands from a generated quinazoline-based scaffold were synthesized, and the lead inhibitor N-(4,6-di(pyrrolidin-1-yl)quinazolin-2-yl)-N-methylbenzamide (UM0152893) displayed sub-micromolar enzyme inhibition of sEH.</p>
  </details>
</details>
<details>
  <summary>115. <b>标题：Improving Trainability of Variational Quantum Circuits via  Regularization Strategies</b></summary>
  <p><b>编号</b>：[377]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01606">https://arxiv.org/abs/2405.01606</a></p>
  <p><b>作者</b>：Jun Zhuang,  Jack Cunningham,  Chaowen Guan</p>
  <p><b>备注</b>：preprint, under review. TL;DR: we propose a regularization strategy to improve the trainability of VQCs</p>
  <p><b>关键词</b>：variational quantum circuits, noisy intermediate-scale quantum, quantum circuits, variational quantum, intermediate-scale quantum</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the era of noisy intermediate-scale quantum (NISQ), variational quantum circuits (VQCs) have been widely applied in various domains, advancing the superiority of quantum circuits against classic models. Similar to classic models, regular VQCs can be optimized by various gradient-based methods. However, the optimization may be initially trapped in barren plateaus or eventually entangled in saddle points during training. These gradient issues can significantly undermine the trainability of VQC. In this work, we propose a strategy that regularizes model parameters with prior knowledge of the train data and Gaussian noise diffusion. We conduct ablation studies to verify the effectiveness of our strategy across four public datasets and demonstrate that our method can improve the trainability of VQCs against the above-mentioned gradient issues.</p>
  </details>
</details>
<details>
  <summary>116. <b>标题：Portfolio Management using Deep Reinforcement Learning</b></summary>
  <p><b>编号</b>：[378]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01604">https://arxiv.org/abs/2405.01604</a></p>
  <p><b>作者</b>：Ashish Anil Pawar,  Vishnureddy Prashant Muskawar,  Ritesh Tiku</p>
  <p><b>备注</b>：7 pages, 9 figures</p>
  <p><b>关键词</b>：complex statistical trading, fathom complex statistical, statistical trading strategies, Algorithmic trading, statistical trading</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Algorithmic trading or Financial robots have been conquering the stock markets with their ability to fathom complex statistical trading strategies. But with the recent development of deep learning technologies, these strategies are becoming impotent. The DQN and A2C models have previously outperformed eminent humans in game-playing and robotics. In our work, we propose a reinforced portfolio manager offering assistance in the allocation of weights to assets. The environment proffers the manager the freedom to go long and even short on the assets. The weight allocation advisements are restricted to the choice of portfolio assets and tested empirically to knock benchmark indices. The manager performs financial transactions in a postulated liquid market without any transaction charges. This work provides the conclusion that the proposed portfolio manager with actions centered on weight allocations can surpass the risk-adjusted returns of conventional portfolio managers.</p>
  </details>
</details>
<details>
  <summary>117. <b>标题：Deep Learning Descriptor Hybridization with Feature Reduction for  Accurate Cervical Cancer Colposcopy Image Classification</b></summary>
  <p><b>编号</b>：[379]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01600">https://arxiv.org/abs/2405.01600</a></p>
  <p><b>作者</b>：Saurabh Saini,  Kapil Ahuja,  Siddartha Chennareddy,  Karthik Boddupalli</p>
  <p><b>备注</b>：7 Pages double column, 5 figures, and 5 tables</p>
  <p><b>关键词</b>：enable early diagnosis, Cervical cancer stands, female mortality, pre-cancerous conditions, regular screenings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Cervical cancer stands as a predominant cause of female mortality, underscoring the need for regular screenings to enable early diagnosis and preemptive treatment of pre-cancerous conditions. The transformation zone in the cervix, where cellular differentiation occurs, plays a critical role in the detection of abnormalities. Colposcopy has emerged as a pivotal tool in cervical cancer prevention since it provides a meticulous examination of cervical abnormalities. However, challenges in visual evaluation necessitate the development of Computer Aided Diagnosis (CAD) systems.
We propose a novel CAD system that combines the strengths of various deep-learning descriptors (ResNet50, ResNet101, and ResNet152) with appropriate feature normalization (min-max) as well as feature reduction technique (LDA). The combination of different descriptors ensures that all the features (low-level like edges and colour, high-level like shape and texture) are captured, feature normalization prevents biased learning, and feature reduction avoids overfitting. We do experiments on the IARC dataset provided by WHO. The dataset is initially segmented and balanced. Our approach achieves exceptional performance in the range of 97%-100% for both the normal-abnormal and the type classification. A competitive approach for type classification on the same dataset achieved 81%-91% performance.</p>
  </details>
</details>
<h1>人工智能</h1>
<details>
  <summary>1. <b>标题：Vibe-Eval: A hard evaluation suite for measuring progress of multimodal  language models</b></summary>
  <p><b>编号</b>：[1]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02287">https://arxiv.org/abs/2405.02287</a></p>
  <p><b>作者</b>：Piotr Padlewski,  Max Bain,  Matthew Henderson,  Zhongkai Zhu,  Nishant Relan,  Hai Pham,  Donovan Ong,  Kaloyan Aleksiev,  Aitor Ormazabal,  Samuel Phua,  Ethan Yeo,  Eugenie Lamprecht,  Qi Liu,  Yuqi Wang,  Eric Chen,  Deyu Fu,  Lei Li,  Che Zheng,  Cyprien de Masson d'Autume,  Dani Yogatama,  Mikel Artetxe,  Yi Tay</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal chat models, open benchmark, benchmark and framework, multimodal chat, evaluating multimodal chat</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce Vibe-Eval: a new open benchmark and framework for evaluating multimodal chat models. Vibe-Eval consists of 269 visual understanding prompts, including 100 of hard difficulty, complete with gold-standard responses authored by experts. Vibe-Eval is open-ended and challenging with dual objectives: (i) vibe checking multimodal chat models for day-to-day tasks and (ii) rigorously testing and probing the capabilities of present frontier models. Notably, our hard set contains >50% questions that all frontier models answer incorrectly. We explore the nuances of designing, evaluating, and ranking models on ultra challenging prompts. We also discuss trade-offs between human and automatic evaluation, and show that automatic model evaluation using Reka Core roughly correlates to human judgment. We offer free API access for the purpose of lightweight evaluation and plan to conduct formal human evaluations for public models that perform well on the Vibe-Eval's automatic scores. We release the evaluation code and data, see this https URL</p>
  </details>
</details>
<details>
  <summary>2. <b>标题：What matters when building vision-language models?</b></summary>
  <p><b>编号</b>：[10]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02246">https://arxiv.org/abs/2405.02246</a></p>
  <p><b>作者</b>：Hugo Laurençon,  Léo Tronchon,  Matthieu Cord,  Victor Sanh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, vision transformers, growing interest, interest in vision-language, driven by improvements</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.</p>
  </details>
</details>
<details>
  <summary>3. <b>标题：REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific  Sentences using Public and Proprietary LLMs</b></summary>
  <p><b>编号</b>：[19]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02228">https://arxiv.org/abs/2405.02228</a></p>
  <p><b>作者</b>：Deepa Tilwani,  Yash Saxena,  Ali Mohammadi,  Edward Raff,  Amit Sheth,  Srinivasan Parthasarathy,  Manas Gaur</p>
  <p><b>备注</b>：Submitted to ACL ARR April 2024</p>
  <p><b>关键词</b>：intelligence analysts, education personnel, document or report, report is paramount, paramount for intelligence</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic citation generation for sentences in a document or report is paramount for intelligence analysts, cybersecurity, news agencies, and education personnel. In this research, we investigate whether large language models (LLMs) are capable of generating references based on two forms of sentence queries: (a) Direct Queries, LLMs are asked to provide author names of the given research article, and (b) Indirect Queries, LLMs are asked to provide the title of a mentioned article when given a sentence from a different article. To demonstrate where LLM stands in this task, we introduce a large dataset called REASONS comprising abstracts of the 12 most popular domains of scientific research on arXiv. From around 20K research articles, we make the following deductions on public and proprietary LLMs: (a) State-of-the-art, often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass percentage (PP) to minimize the hallucination rate (HR). When tested with this http URL (7B), they unexpectedly made more errors; (b) Augmenting relevant metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented generation (RAG) using Mistral demonstrates consistent and robust citation support on indirect queries and matched performance to GPT-3.5 and GPT-4. The HR across all domains and models decreased by an average of 41.93% and the PP was reduced to 0% in most cases. In terms of generation quality, the average F1 Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with adversarial samples showed that LLMs, including the Advance RAG Mistral, struggle to understand context, but the extent of this issue was small in Mistral and GPT-4-Preview. Our study con tributes valuable insights into the reliability of RAG for automated citation generation tasks.</p>
  </details>
</details>
<details>
  <summary>4. <b>标题：Automatic Programming: Large Language Models and Beyond</b></summary>
  <p><b>编号</b>：[24]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02213">https://arxiv.org/abs/2405.02213</a></p>
  <p><b>作者</b>：Michael R. Lyu,  Baishakhi Ray,  Abhik Roychoudhury,  Shin Hwei Tan,  Patanamon Thongtanunam</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, increasing popularity due, GitHub Copilot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance</p>
  </details>
</details>
<details>
  <summary>5. <b>标题：Assessing and Verifying Task Utility in LLM-Powered Applications</b></summary>
  <p><b>编号</b>：[38]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02178">https://arxiv.org/abs/2405.02178</a></p>
  <p><b>作者</b>：Negar Arabzadeh,  Siging Huo,  Nikhil Mehta,  Qinqyun Wu,  Chi Wang,  Ahmed Awadallah,  Charles L. A. Clarke,  Julia Kiseleva</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2402.09015</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, development of Large, multiple agents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid development of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents, assisting humans in their daily tasks. However, a significant gap remains in assessing to what extent LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the need to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the effectiveness and robustness of AgentEval for two open source datasets including Math Problem solving and ALFWorld House-hold related tasks. For reproducibility purposes, we make the data, code and all the logs publicly available at this https URL .</p>
  </details>
</details>
<details>
  <summary>6. <b>标题：Hoaxpedia: A Unified Wikipedia Hoax Articles Dataset</b></summary>
  <p><b>编号</b>：[40]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02175">https://arxiv.org/abs/2405.02175</a></p>
  <p><b>作者</b>：Hsuvas Borkakoty,  Luis Espinosa-Anke</p>
  <p><b>备注</b>：Short paper</p>
  <p><b>关键词</b>：disinformation created deliberately, reference knowledge resources, created deliberately, recognised form, form of disinformation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Hoaxes are a recognised form of disinformation created deliberately, with potential serious implications in the credibility of reference knowledge resources such as Wikipedia. What makes detecting Wikipedia hoaxes hard is that they often are written according to the official style guidelines. In this work, we first provide a systematic analysis of the similarities and discrepancies between legitimate and hoax Wikipedia articles, and introduce Hoaxpedia, a collection of 311 Hoax articles (from existing literature as well as official Wikipedia lists) alongside semantically similar real articles. We report results of binary classification experiments in the task of predicting whether a Wikipedia article is real or hoax, and analyze several settings as well as a range of language models. Our results suggest that detecting deceitful content in Wikipedia based on content alone, despite not having been explored much in the past, is a promising direction.</p>
  </details>
</details>
<details>
  <summary>7. <b>标题：EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and  Multi-View Transformer</b></summary>
  <p><b>编号</b>：[44]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02165">https://arxiv.org/abs/2405.02165</a></p>
  <p><b>作者</b>：Hanwen Liu,  Daniel Hajialigol,  Benny Antony,  Aiguo Han,  Xuan Wang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deciphering the intricacies, curiosity for centuries, captivated curiosity, EEG, human brain</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deciphering the intricacies of the human brain has captivated curiosity for centuries. Recent strides in Brain-Computer Interface (BCI) technology, particularly using motor imagery, have restored motor functions such as reaching, grasping, and walking in paralyzed individuals. However, unraveling natural language from brain signals remains a formidable challenge. Electroencephalography (EEG) is a non-invasive technique used to record electrical activity in the brain by placing electrodes on the scalp. Previous studies of EEG-to-text decoding have achieved high accuracy on small closed vocabularies, but still fall short of high accuracy when dealing with large open vocabularies. We propose a novel method, EEG2TEXT, to improve the accuracy of open vocabulary EEG-to-text decoding. Specifically, EEG2TEXT leverages EEG pre-training to enhance the learning of semantics from EEG signals and proposes a multi-view transformer to model the EEG signal processing by different spatial regions of the brain. Experiments show that EEG2TEXT has superior performance, outperforming the state-of-the-art baseline methods by a large margin of up to 5% in absolute BLEU and ROUGE scores. EEG2TEXT shows great potential for a high-performance open-vocabulary brain-to-text system to facilitate communication.</p>
  </details>
</details>
<details>
  <summary>8. <b>标题：Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic  Labeling using Foundation Models</b></summary>
  <p><b>编号</b>：[45]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02162">https://arxiv.org/abs/2405.02162</a></p>
  <p><b>作者</b>：Mohamad Al Mdfaa,  Raghad Salameh,  Sergey Zagoruyko,  Gonzalo Ferrer</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：significant challenge due, accurate semantic mapping, semantic mapping remains, computer vision, efficient and accurate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the field of robotics and computer vision, efficient and accurate semantic mapping remains a significant challenge due to the growing demand for intelligent machines that can comprehend and interact with complex environments. Conventional panoptic mapping methods, however, are limited by predefined semantic classes, thus making them ineffective for handling novel or unforeseen objects. In response to this limitation, we introduce the Unified Promptable Panoptic Mapping (UPPM) method. UPPM utilizes recent advances in foundation models to enable real-time, on-demand label generation using natural language prompts. By incorporating a dynamic labeling strategy into traditional panoptic mapping techniques, UPPM provides significant improvements in adaptability and versatility while maintaining high performance levels in map reconstruction. We demonstrate our approach on real-world and simulated datasets. Results show that UPPM can accurately reconstruct scenes and segment objects while generating rich semantic labels through natural language interactions. A series of ablation experiments validated the advantages of foundation model-based labeling over fixed label sets.</p>
  </details>
</details>
<details>
  <summary>9. <b>标题：Simulating the economic impact of rationality through reinforcement  learning and agent-based modelling</b></summary>
  <p><b>编号</b>：[46]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02161">https://arxiv.org/abs/2405.02161</a></p>
  <p><b>作者</b>：Simone Brusatin,  Tommaso Padoan,  Andrea Coletta,  Domenico Delli Gatti,  Aldo Glielmo</p>
  <p><b>备注</b>：8 pages, 4 figures</p>
  <p><b>关键词</b>：traditional frameworks based, general equilibrium assumptions, equilibrium assumptions, limitations of traditional, Rational macro ABM</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Agent-based models (ABMs) are simulation models used in economics to overcome some of the limitations of traditional frameworks based on general equilibrium assumptions. However, agents within an ABM follow predetermined, not fully rational, behavioural rules which can be cumbersome to design and difficult to justify. Here we leverage multi-agent reinforcement learning (RL) to expand the capabilities of ABMs with the introduction of fully rational agents that learn their policy by interacting with the environment and maximising a reward function. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by extending a paradigmatic macro ABM from the economic literature. We show that gradually substituting ABM firms in the model with RL agents, trained to maximise profits, allows for a thorough study of the impact of rationality on the economy. We find that RL agents spontaneously learn three distinct strategies for maximising profits, with the optimal strategy depending on the level of market competition and rationality. We also find that RL agents with independent policies, and without the ability to communicate with each other, spontaneously learn to segregate into different strategic groups, thus increasing market power and overall profits. Finally, we find that a higher degree of rationality in the economy always improves the macroeconomic environment as measured by total output, depending on the specific rational policy, this can come at the cost of higher instability. Our R-MABM framework is general, it allows for stable multi-agent learning, and represents a principled and robust direction to extend existing economic simulators.</p>
  </details>
</details>
<details>
  <summary>10. <b>标题：GMP-ATL: Gender-augmented Multi-scale Pseudo-label Enhanced Adaptive  Transfer Learning for Speech Emotion Recognition via HuBERT</b></summary>
  <p><b>编号</b>：[50]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02151">https://arxiv.org/abs/2405.02151</a></p>
  <p><b>作者</b>：Yu Pan,  Yuguang Yang,  Heng Lu,  Lei Ma,  Jianjun Zhao</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：greatly advanced Speech, advanced Speech Emotion, advanced Speech, Speech Emotion Recognition, Adaptive Transfer Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The continuous evolution of pre-trained speech models has greatly advanced Speech Emotion Recognition (SER). However, there is still potential for enhancement in the performance of these methods. In this paper, we present GMP-ATL (Gender-augmented Multi-scale Pseudo-label Adaptive Transfer Learning), a novel HuBERT-based adaptive transfer learning framework for SER. Specifically, GMP-ATL initially employs the pre-trained HuBERT, implementing multi-task learning and multi-scale k-means clustering to acquire frame-level gender-augmented multi-scale pseudo-labels. Then, to fully leverage both obtained frame-level and utterance-level emotion labels, we incorporate model retraining and fine-tuning methods to further optimize GMP-ATL. Experiments on IEMOCAP show that our GMP-ATL achieves superior recognition performance, with a WAR of 80.0\% and a UAR of 82.0\%, surpassing state-of-the-art unimodal SER methods, while also yielding comparable results with multimodal SER approaches.</p>
  </details>
</details>
<details>
  <summary>11. <b>标题：Towards a Formal Creativity Theory: Preliminary results in Novelty and  Transformativeness</b></summary>
  <p><b>编号</b>：[52]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02148">https://arxiv.org/abs/2405.02148</a></p>
  <p><b>作者</b>：Luís Espírito Santo,  Geraint Wiggins,  Amílcar Cardoso</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：goal of Computational, Formalizing creativity-related concepts, Computational Creativity, Formalizing creativity-related, Formal Learning Theory</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Formalizing creativity-related concepts has been a long-term goal of Computational Creativity. To the same end, we explore Formal Learning Theory in the context of creativity. We provide an introduction to the main concepts of this framework and a re-interpretation of terms commonly found in creativity discussions, proposing formal definitions for novelty and transformational creativity. This formalisation marks the beginning of a research branch we call Formal Creativity Theory, exploring how learning can be included as preparation for exploratory behaviour and how learning is a key part of transformational creative behaviour. By employing these definitions, we argue that, while novelty is neither necessary nor sufficient for transformational creativity in general, when using an inspiring set, rather than a sequence of experiences, an agent actually requires novelty for transformational creativity to occur.</p>
  </details>
</details>
<details>
  <summary>12. <b>标题：Evaluating Large Language Models for Structured Science Summarization in  the Open Research Knowledge Graph</b></summary>
  <p><b>编号</b>：[69]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02105">https://arxiv.org/abs/2405.02105</a></p>
  <p><b>作者</b>：Vladyslav Nechakhin,  Jennifer D'Souza,  Steffen Eger</p>
  <p><b>备注</b>：22 pages, 11 figures. In review at this https URL</p>
  <p><b>关键词</b>：traditional keywords enhances, Research Knowledge Graph, Open Research Knowledge, Large Language Models, enhances science findability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability. Current methods, such as those used by the Open Research Knowledge Graph (ORKG), involve manually curating properties to describe research papers' contributions in a structured manner, but this is labor-intensive and inconsistent between the domain expert human curators. We propose using Large Language Models (LLMs) to automatically suggest these properties. However, it's essential to assess the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before application. Our study performs a comprehensive comparative analysis between ORKG's manually curated properties and those generated by the aforementioned state-of-the-art LLMs. We evaluate LLM performance through four unique perspectives: semantic alignment and deviation with ORKG properties, fine-grained properties mapping accuracy, SciNCL embeddings-based cosine similarity, and expert surveys comparing manual annotations with LLM outputs. These evaluations occur within a multidisciplinary science setting. Overall, LLMs show potential as recommendation systems for structuring science, but further finetuning is recommended to improve their alignment with scientific tasks and mimicry of human expertise.</p>
  </details>
</details>
<details>
  <summary>13. <b>标题：Advanced Detection of Source Code Clones via an Ensemble of Unsupervised  Similarity Measures</b></summary>
  <p><b>编号</b>：[71]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02095">https://arxiv.org/abs/2405.02095</a></p>
  <p><b>作者</b>：Jorge Martinez-Gil</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：accurately determining code, determining code similarity, capability of accurately, accurately determining, tasks related</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The capability of accurately determining code similarity is crucial in many tasks related to software development. For example, it might be essential to identify code duplicates for performing software maintenance. This research introduces a novel ensemble learning approach for code similarity assessment, combining the strengths of multiple unsupervised similarity measures. The key idea is that the strengths of a diverse set of similarity measures can complement each other and mitigate individual weaknesses, leading to improved performance. Preliminary results show that while Transformers-based CodeBERT and its variant GraphCodeBERT are undoubtedly the best option in the presence of abundant training data, in the case of specific small datasets (up to 500 samples), our ensemble achieves similar results, without prejudice to the interpretability of the resulting solution, and with a much lower associated carbon footprint due to training. The source code of this novel approach can be downloaded from this https URL.</p>
  </details>
</details>
<details>
  <summary>14. <b>标题：A semantic loss for ontology classification</b></summary>
  <p><b>编号</b>：[74]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02083">https://arxiv.org/abs/2405.02083</a></p>
  <p><b>作者</b>：Simon Flügel,  Martin Glauer,  Till Mossakowski,  Fabian Neuhaus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep learning models, semantic loss, inherent constraints, Deep learning, learning models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep learning models are often unaware of the inherent constraints of the task they are applied to. However, many downstream tasks require logical consistency. For ontology classification tasks, such constraints include subsumption and disjointness relations between classes.
In order to increase the consistency of deep learning models, we propose a semantic loss that combines label-based loss with terms penalising subsumption- or disjointness-violations. Our evaluation on the ChEBI ontology shows that the semantic loss is able to decrease the number of consistency violations by several orders of magnitude without decreasing the classification performance. In addition, we use the semantic loss for unsupervised learning. We show that this can further improve consistency on data from a distribution outside the scope of the supervised training.</p>
  </details>
</details>
<details>
  <summary>15. <b>标题：Argumentative Large Language Models for Explainable and Contestable  Decision-Making</b></summary>
  <p><b>编号</b>：[77]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02079">https://arxiv.org/abs/2405.02079</a></p>
  <p><b>作者</b>：Gabriel Freedman,  Adam Dejl,  Deniz Gorur,  Xiang Yin,  Antonio Rago,  Francesca Toni</p>
  <p><b>备注</b>：19 pages, 17 figures</p>
  <p><b>关键词</b>：large language models, language models, knowledge encoded, knowledge zero-shot, encoded in large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The diversity of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them a promising candidate for use in decision-making. However, they are currently limited by their inability to reliably provide outputs which are explainable and contestable. In this paper, we attempt to reconcile these strengths and weaknesses by introducing a method for supplementing LLMs with argumentative reasoning. Concretely, we introduce argumentative LLMs, a method utilising LLMs to construct argumentation frameworks, which then serve as the basis for formal reasoning in decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by the supplemented LLM may be naturally explained to, and contested by, humans. We demonstrate the effectiveness of argumentative LLMs experimentally in the decision-making task of claim verification. We obtain results that are competitive with, and in some cases surpass, comparable state-of-the-art techniques.</p>
  </details>
</details>
<details>
  <summary>16. <b>标题：Comparative Analysis of Retrieval Systems in the Real World</b></summary>
  <p><b>编号</b>：[91]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02048">https://arxiv.org/abs/2405.02048</a></p>
  <p><b>作者</b>：Dmytro Mozolevskyi,  Waseem AlShikh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：research paper presents, natural language processing, integrating advanced language, advanced language models, Weaviate Vector Store</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This research paper presents a comprehensive analysis of integrating advanced language models with search and retrieval systems in the fields of information retrieval and natural language processing. The objective is to evaluate and compare various state-of-the-art methods based on their performance in terms of accuracy and efficiency. The analysis explores different combinations of technologies, including Azure Cognitive Search Retriever with GPT-4, Pinecone's Canopy framework, Langchain with Pinecone and different language models (OpenAI, Cohere), LlamaIndex with Weaviate Vector Store's hybrid search, Google's RAG implementation on Cloud VertexAI-Search, Amazon SageMaker's RAG, and a novel approach called KG-FID Retrieval. The motivation for this analysis arises from the increasing demand for robust and responsive question-answering systems in various domains. The RobustQA metric is used to evaluate the performance of these systems under diverse paraphrasing of questions. The report aims to provide insights into the strengths and weaknesses of each method, facilitating informed decisions in the deployment and development of AI-driven search and retrieval systems.</p>
  </details>
</details>
<details>
  <summary>17. <b>标题：Zero-Sum Positional Differential Games as a Framework for Robust  Reinforcement Learning: Deep Q-Learning Approach</b></summary>
  <p><b>编号</b>：[94]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02044">https://arxiv.org/abs/2405.02044</a></p>
  <p><b>作者</b>：Anton Plaksin,  Vitaly Kalev</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：promising Reinforcement Learning, Robust Reinforcement Learning, Reinforcement Learning, promising Reinforcement, Robust Reinforcement</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Robust Reinforcement Learning (RRL) is a promising Reinforcement Learning (RL) paradigm aimed at training robust to uncertainty or disturbances models, making them more efficient for real-world applications. Following this paradigm, uncertainty or disturbances are interpreted as actions of a second adversarial agent, and thus, the problem is reduced to seeking the agents' policies robust to any opponent's actions. This paper is the first to propose considering the RRL problems within the positional differential game theory, which helps us to obtain theoretically justified intuition to develop a centralized Q-learning approach. Namely, we prove that under Isaacs's condition (sufficiently general for real-world dynamical systems), the same Q-function can be utilized as an approximate solution of both minimax and maximin Bellman equations. Based on these results, we present the Isaacs Deep Q-Network algorithms and demonstrate their superiority compared to other baseline RRL and Multi-Agent RL algorithms in various environments.</p>
  </details>
</details>
<details>
  <summary>18. <b>标题：Analyzing Narrative Processing in Large Language Models (LLMs): Using  GPT4 to test BERT</b></summary>
  <p><b>编号</b>：[102]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02024">https://arxiv.org/abs/2405.02024</a></p>
  <p><b>作者</b>：Patrick Krauss,  Jannik Hösch,  Claus Metzner,  Andreas Maier,  Peter Uhrig,  Achim Schilling</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：versatile social interactions, receive complex information, basis of traditions, culture and versatile, social interactions</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The ability to transmit and receive complex information via language is unique to humans and is the basis of traditions, culture and versatile social interactions. Through the disruptive introduction of transformer based large language models (LLMs) humans are not the only entity to "understand" and produce language any more. In the present study, we have performed the first steps to use LLMs as a model to understand fundamental mechanisms of language processing in neural networks, in order to make predictions and generate hypotheses on how the human brain does language processing. Thus, we have used ChatGPT to generate seven different stylistic variations of ten different narratives (Aesop's fables). We used these stories as input for the open source LLM BERT and have analyzed the activation patterns of the hidden units of BERT using multi-dimensional scaling and cluster analysis. We found that the activation vectors of the hidden units cluster according to stylistic variations in earlier layers of BERT (1) than narrative content (4-5). Despite the fact that BERT consists of 12 identical building blocks that are stacked and trained on large text corpora, the different layers perform different tasks. This is a very useful model of the human brain, where self-similar structures, i.e. different areas of the cerebral cortex, can have different functions and are therefore well suited to processing language in a very efficient way. The proposed approach has the potential to open the black box of LLMs on the one hand, and might be a further step to unravel the neural processes underlying human language processing and cognition in general.</p>
  </details>
</details>
<details>
  <summary>19. <b>标题：Adversarial Botometer: Adversarial Analysis for Social Bot Detection</b></summary>
  <p><b>编号</b>：[106]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02016">https://arxiv.org/abs/2405.02016</a></p>
  <p><b>作者</b>：Shaghayegh Najari,  Davood Rafiee,  Mostafa Salehi,  Reza Farahbakhsh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Social bots play, Social bots, play a significant, significant role, online social networks</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Social bots play a significant role in many online social networks (OSN) as they imitate human behavior. This fact raises difficult questions about their capabilities and potential risks. Given the recent advances in Generative AI (GenAI), social bots are capable of producing highly realistic and complex content that mimics human creativity. As the malicious social bots emerge to deceive people with their unrealistic content, identifying them and distinguishing the content they produce has become an actual challenge for numerous social platforms. Several approaches to this problem have already been proposed in the literature, but the proposed solutions have not been widely evaluated. To address this issue, we evaluate the behavior of a text-based bot detector in a competitive environment where some scenarios are proposed: \textit{First}, the tug-of-war between a bot and a bot detector is examined. It is interesting to analyze which party is more likely to prevail and which circumstances influence these expectations. In this regard, we model the problem as a synthetic adversarial game in which a conversational bot and a bot detector are engaged in strategic online interactions. \textit{Second}, the bot detection model is evaluated under attack examples generated by a social bot; to this end, we poison the dataset with attack examples and evaluate the model performance under this condition. \textit{Finally}, to investigate the impact of the dataset, a cross-domain analysis is performed. Through our comprehensive evaluation of different categories of social bots using two benchmark datasets, we were able to demonstrate some achivement that could be utilized in future works.</p>
  </details>
</details>
<details>
  <summary>20. <b>标题：Exploring Combinatorial Problem Solving with Large Language Models: A  Case Study on the Travelling Salesman Problem Using GPT-3.5 Turbo</b></summary>
  <p><b>编号</b>：[114]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01997">https://arxiv.org/abs/2405.01997</a></p>
  <p><b>作者</b>：Mahmoud Masoud,  Ahmed Abdelhay,  Mohammed Elhenawy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, generate text based, Large Language, Language Models, textual input</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Large Language Models (LLMs) are deep learning models designed to generate text based on textual input. Although researchers have been developing these models for more complex tasks such as code generation and general reasoning, few efforts have explored how LLMs can be applied to combinatorial problems. In this research, we investigate the potential of LLMs to solve the Travelling Salesman Problem (TSP). Utilizing GPT-3.5 Turbo, we conducted experiments employing various approaches, including zero-shot in-context learning, few-shot in-context learning, and chain-of-thoughts (CoT). Consequently, we fine-tuned GPT-3.5 Turbo to solve a specific problem size and tested it using a set of various instance sizes. The fine-tuned models demonstrated promising performance on problems identical in size to the training instances and generalized well to larger problems. Furthermore, to improve the performance of the fine-tuned model without incurring additional training costs, we adopted a self-ensemble approach to improve the quality of the solutions.</p>
  </details>
</details>
<details>
  <summary>21. <b>标题：Joint sentiment analysis of lyrics and audio in music</b></summary>
  <p><b>编号</b>：[118]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01988">https://arxiv.org/abs/2405.01988</a></p>
  <p><b>作者</b>：Lea Schaab,  Anna Kruspe</p>
  <p><b>备注</b>：published at DAGA 2024</p>
  <p><b>关键词</b>：levels in music, audio, lyrics, mood can express, Sentiment</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sentiment or mood can express themselves on various levels in music. In automatic analysis, the actual audio data is usually analyzed, but the lyrics can also play a crucial role in the perception of moods. We first evaluate various models for sentiment analysis based on lyrics and audio separately. The corresponding approaches already show satisfactory results, but they also exhibit weaknesses, the causes of which we examine in more detail. Furthermore, different approaches to combining the audio and lyrics results are proposed and evaluated. Considering both modalities generally leads to improved performance. We investigate misclassifications and (also intentional) contradictions between audio and lyrics sentiment more closely, and identify possible causes. Finally, we address fundamental problems in this research area, such as high subjectivity, lack of data, and inconsistency in emotion taxonomies.</p>
  </details>
</details>
<details>
  <summary>22. <b>标题：Model-based reinforcement learning for protein backbone design</b></summary>
  <p><b>编号</b>：[119]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01983">https://arxiv.org/abs/2405.01983</a></p>
  <p><b>作者</b>：Frederic Renard,  Cyprien Courtot,  Alfredo Reichlin,  Oliver Bent</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Designing protein nanomaterials, medical industry, nanomaterials of predefined, dramatically impact, impact the medical</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Designing protein nanomaterials of predefined shape and characteristics has the potential to dramatically impact the medical industry. Machine learning (ML) has proven successful in protein design, reducing the need for expensive wet lab experiment rounds. However, challenges persist in efficiently exploring the protein fitness landscapes to identify optimal protein designs. In response, we propose the use of AlphaZero to generate protein backbones, meeting shape and structural scoring requirements. We extend an existing Monte Carlo tree search (MCTS) framework by incorporating a novel threshold-based reward and secondary objectives to improve design precision. This innovation considerably outperforms existing approaches, leading to protein backbones that better respect structural scores. The application of AlphaZero is novel in the context of protein backbone design and demonstrates promising performance. AlphaZero consistently surpasses baseline MCTS by more than 100% in top-down protein design tasks. Additionally, our application of AlphaZero with secondary objectives uncovers further promising outcomes, indicating the potential of model-based reinforcement learning (RL) in navigating the intricate and nuanced aspects of protein design</p>
  </details>
</details>
<details>
  <summary>23. <b>标题：Multitask Extension of Geometrically Aligned Transfer Encoder</b></summary>
  <p><b>编号</b>：[124]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01974">https://arxiv.org/abs/2405.01974</a></p>
  <p><b>作者</b>：Sung Moon Ko,  Sumin Lee,  Dae-Woong Jeong,  Hyunseung Kim,  Chanhui Lee,  Soorin Yim,  Sehui Han</p>
  <p><b>备注</b>：7 pages, 3 figures, 2 tables</p>
  <p><b>关键词</b>：datasets often suffer, Aligned Transfer Encoder, Geometrically Aligned Transfer, Molecular datasets, data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Molecular datasets often suffer from a lack of data. It is well-known that gathering data is difficult due to the complexity of experimentation or simulation involved. Here, we leverage mutual information across different tasks in molecular data to address this issue. We extend an algorithm that utilizes the geometric characteristics of the encoding space, known as the Geometrically Aligned Transfer Encoder (GATE), to a multi-task setup. Thus, we connect multiple molecular tasks by aligning the curved coordinates onto locally flat coordinates, ensuring the flow of information from source tasks to support performance on target data.</p>
  </details>
</details>
<details>
  <summary>24. <b>标题：From Attack to Defense: Insights into Deep Learning Security Measures in  Black-Box Settings</b></summary>
  <p><b>编号</b>：[127]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01963">https://arxiv.org/abs/2405.01963</a></p>
  <p><b>作者</b>：Firuz Juraev,  Mohammed Abuhamad,  Eric Chan-Tin,  George K. Thiruvathukal,  Tamer Abuhmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Deep Learning, rapidly maturing, attacks, security-crucial applications, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Deep Learning (DL) is rapidly maturing to the point that it can be used in safety- and security-crucial applications. However, adversarial samples, which are undetectable to the human eye, pose a serious threat that can cause the model to misbehave and compromise the performance of such applications. Addressing the robustness of DL models has become crucial to understanding and defending against adversarial attacks. In this study, we perform comprehensive experiments to examine the effect of adversarial attacks and defenses on various model architectures across well-known datasets. Our research focuses on black-box attacks such as SimBA, HopSkipJump, MGAAttack, and boundary attacks, as well as preprocessor-based defensive mechanisms, including bits squeezing, median smoothing, and JPEG filter. Experimenting with various models, our results demonstrate that the level of noise needed for the attack increases as the number of layers increases. Moreover, the attack success rate decreases as the number of layers increases. This indicates that model complexity and robustness have a significant relationship. Investigating the diversity and robustness relationship, our experiments with diverse models show that having a large number of parameters does not imply higher robustness. Our experiments extend to show the effects of the training dataset on model robustness. Using various datasets such as ImageNet-1000, CIFAR-100, and CIFAR-10 are used to evaluate the black-box attacks. Considering the multiple dimensions of our analysis, e.g., model complexity and training dataset, we examined the behavior of black-box attacks when models apply defenses. Our results show that applying defense strategies can significantly reduce attack effectiveness. This research provides in-depth analysis and insight into the robustness of DL models against various attacks, and defenses.</p>
  </details>
</details>
<details>
  <summary>25. <b>标题：Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large  Language Models</b></summary>
  <p><b>编号</b>：[129]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01943">https://arxiv.org/abs/2405.01943</a></p>
  <p><b>作者</b>：Zhiyu Guo,  Hidetaka Kamigaito,  Taro Wanatnabe</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, advancement in Large, language understanding, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The rapid advancement in Large Language Models (LLMs) has markedly enhanced the capabilities of language understanding and generation. However, the substantial model size poses hardware challenges, affecting both memory size for serving and inference latency for token generation. To address those challenges, we propose Dependency-aware Semi-structured Sparsity (DaSS), a novel method for the recent prevalent SwiGLU-based LLMs pruning. Our approach incorporates structural dependency into the weight magnitude-based unstructured pruning. We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms. DaSS facilitates a balance between the adaptability offered by unstructured pruning and the structural consistency inherent in dependency-based structured pruning. Empirical evaluations on Mistral and LLaMA2 model families demonstrate that DaSS not only outperforms both SparseGPT and Wanda in achieving hardware-friendly N:M sparsity patterns but also maintains the computational efficiency of Wanda.</p>
  </details>
</details>
<details>
  <summary>26. <b>标题：Impact of Architectural Modifications on Deep Learning Adversarial  Robustness</b></summary>
  <p><b>编号</b>：[135]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01934">https://arxiv.org/abs/2405.01934</a></p>
  <p><b>作者</b>：Firuz Juraev,  Mohammed Abuhamad,  Simon S. Woo,  George K Thiruvathukal,  Tamer Abuhmed</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：including safety-critical applications, deep learning models, deep learning, including safety-critical, self-driving vehicles</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Rapid advancements of deep learning are accelerating adoption in a wide variety of applications, including safety-critical applications such as self-driving vehicles, drones, robots, and surveillance systems. These advancements include applying variations of sophisticated techniques that improve the performance of models. However, such models are not immune to adversarial manipulations, which can cause the system to misbehave and remain unnoticed by experts. The frequency of modifications to existing deep learning models necessitates thorough analysis to determine the impact on models' robustness. In this work, we present an experimental evaluation of the effects of model modifications on deep learning model robustness using adversarial attacks. Our methodology involves examining the robustness of variations of models against various adversarial attacks. By conducting our experiments, we aim to shed light on the critical issue of maintaining the reliability and safety of deep learning models in safety- and security-critical applications. Our results indicate the pressing demand for an in-depth assessment of the effects of model changes on the robustness of models.</p>
  </details>
</details>
<details>
  <summary>27. <b>标题：Semi-Parametric Retrieval via Binary Token Index</b></summary>
  <p><b>编号</b>：[141]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01924">https://arxiv.org/abs/2405.01924</a></p>
  <p><b>作者</b>：Jiawei Zhou,  Li Dong,  Furu Wei,  Lei Chen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Semi-parametric Vocabulary Disentangled, Vocabulary Disentangled Retrieval, advanced applications, indexing efficiency, remain less explored</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The landscape of information retrieval has broadened from search services to a critical component in various advanced applications, where indexing efficiency, cost-effectiveness, and freshness are increasingly important yet remain less explored. To address these demands, we introduce Semi-parametric Vocabulary Disentangled Retrieval (SVDR). SVDR is a novel semi-parametric retrieval framework that supports two types of indexes: an embedding-based index for high effectiveness, akin to existing neural retrieval methods; and a binary token index that allows for quick and cost-effective setup, resembling traditional term-based retrieval. In our evaluation on three open-domain question answering benchmarks with the entire Wikipedia as the retrieval corpus, SVDR consistently demonstrates superiority. It achieves a 3% higher top-1 retrieval accuracy compared to the dense retriever DPR when using an embedding-based index and an 9% higher top-1 accuracy compared to BM25 when using a binary token index. Specifically, the adoption of a binary token index reduces index preparation time from 30 GPU hours to just 2 CPU hours and storage size from 31 GB to 2 GB, achieving a 90% reduction compared to an embedding-based index.</p>
  </details>
</details>
<details>
  <summary>28. <b>标题：Instance-Conditioned Adaptation for Large-scale Generalization of Neural  Combinatorial Optimization</b></summary>
  <p><b>编号</b>：[149]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01906">https://arxiv.org/abs/2405.01906</a></p>
  <p><b>作者</b>：Changliang Zhou,  Xi Lin,  Zhenkun Wang,  Xialiang Tong,  Mingxuan Yuan,  Qingfu Zhang</p>
  <p><b>备注</b>：17 pages, 6 figures</p>
  <p><b>关键词</b>：shown great potential, neural combinatorial optimization, approach has shown, combinatorial optimization, shown great</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The neural combinatorial optimization (NCO) approach has shown great potential for solving routing problems without the requirement of expert knowledge. However, existing constructive NCO methods cannot directly solve large-scale instances, which significantly limits their application prospects. To address these crucial shortcomings, this work proposes a novel Instance-Conditioned Adaptation Model (ICAM) for better large-scale generalization of neural combinatorial optimization. In particular, we design a powerful yet lightweight instance-conditioned adaptation module for the NCO model to generate better solutions for instances across different scales. In addition, we develop an efficient three-stage reinforcement learning-based training scheme that enables the model to learn cross-scale features without any labeled optimal solution. Experimental results show that our proposed method is capable of obtaining excellent results with a very fast inference time in solving Traveling Salesman Problems (TSPs) and Capacitated Vehicle Routing Problems (CVRPs) across different scales. To the best of our knowledge, our model achieves state-of-the-art performance among all RL-based constructive methods for TSP and CVRP with up to 1,000 nodes.</p>
  </details>
</details>
<details>
  <summary>29. <b>标题：Aloe: A Family of Fine-tuned Open Healthcare LLMs</b></summary>
  <p><b>编号</b>：[155]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01886">https://arxiv.org/abs/2405.01886</a></p>
  <p><b>作者</b>：Ashwin Kumar Gururajan,  Enrique Lopez-Cuena,  Jordi Bayarri-Planas,  Adrian Tormos,  Daniel Hinjos,  Pablo Bernabeu-Perez,  Anna Arias-Duart,  Pablo Agustin Martin-Torres,  Lucia Urcelay-Ganzabal,  Marta Gonzalez-Mallo,  Sergio Alvarez-Napagao,  Eduard Ayguadé-Parra,  Ulises Cortés Dario Garcia-Gasulla</p>
  <p><b>备注</b>：Five appendix</p>
  <p><b>关键词</b>：Large Language Models, Large Language, capabilities of Large, safeguard public interest, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>As the capabilities of Large Language Models (LLMs) in healthcare and medicine continue to advance, there is a growing need for competitive open-source models that can safeguard public interest. With the increasing availability of highly competitive open base models, the impact of continued pre-training is increasingly uncertain. In this work, we explore the role of instruct tuning, model merging, alignment, red teaming and advanced inference schemes, as means to improve current open models. To that end, we introduce the Aloe family, a set of open medical LLMs highly competitive within its scale range. Aloe models are trained on the current best base models (Mistral, LLaMA 3), using a new custom dataset which combines public data sources improved with synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase, becoming one of the first few policy-aligned open healthcare LLM using Direct Preference Optimization, setting a new standard for ethical performance in healthcare LLMs. Model evaluation expands to include various bias and toxicity datasets, a dedicated red teaming effort, and a much-needed risk assessment for healthcare LLMs. Finally, to explore the limits of current LLMs in inference, we study several advanced prompt engineering strategies to boost performance across benchmarks, yielding state-of-the-art results for open healthcare 7B LLMs, unprecedented at this scale.</p>
  </details>
</details>
<details>
  <summary>30. <b>标题：Millimeter Wave Radar-based Human Activity Recognition for Healthcare  Monitoring Robot</b></summary>
  <p><b>编号</b>：[159]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01882">https://arxiv.org/abs/2405.01882</a></p>
  <p><b>作者</b>：Zhanzhong Gu,  Xiangjian He,  Gengfa Fang,  Chengpei Xu,  Feng Xia,  Wenjing Jia</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：elderly individuals living, daily care, care of elderly, elderly individuals, individuals living</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Healthcare monitoring is crucial, especially for the daily care of elderly individuals living alone. It can detect dangerous occurrences, such as falls, and provide timely alerts to save lives. Non-invasive millimeter wave (mmWave) radar-based healthcare monitoring systems using advanced human activity recognition (HAR) models have recently gained significant attention. However, they encounter challenges in handling sparse point clouds, achieving real-time continuous classification, and coping with limited monitoring ranges when statically mounted. To overcome these limitations, we propose RobHAR, a movable robot-mounted mmWave radar system with lightweight deep neural networks for real-time monitoring of human activities. Specifically, we first propose a sparse point cloud-based global embedding to learn the features of point clouds using the light-PointNet (LPN) backbone. Then, we learn the temporal pattern with a bidirectional lightweight LSTM model (BiLiLSTM). In addition, we implement a transition optimization strategy, integrating the Hidden Markov Model (HMM) with Connectionist Temporal Classification (CTC) to improve the accuracy and robustness of the continuous HAR. Our experiments on three datasets indicate that our method significantly outperforms the previous studies in both discrete and continuous HAR tasks. Finally, we deploy our system on a movable robot-mounted edge computing platform, achieving flexible healthcare monitoring in real-world scenarios.</p>
  </details>
</details>
<details>
  <summary>31. <b>标题：AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten  AI Research</b></summary>
  <p><b>编号</b>：[166]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01859">https://arxiv.org/abs/2405.01859</a></p>
  <p><b>作者</b>：Riley Simmons-Edler,  Ryan Badman,  Shayne Longpre,  Kanaka Rajan</p>
  <p><b>备注</b>：9 pages, in ICML 2024</p>
  <p><b>关键词</b>：autonomous weapons systems, machine learning, weapons systems, autonomous weapons, free exchange</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The recent embrace of machine learning (ML) in the development of autonomous weapons systems (AWS) creates serious risks to geopolitical stability and the free exchange of ideas in AI research. This topic has received comparatively little attention of late compared to risks stemming from superintelligent artificial general intelligence (AGI), but requires fewer assumptions about the course of technological development and is thus a nearer-future issue. ML is already enabling the substitution of AWS for human soldiers in many battlefield roles, reducing the upfront human cost, and thus political cost, of waging offensive war. In the case of peer adversaries, this increases the likelihood of "low intensity" conflicts which risk escalation to broader warfare. In the case of non-peer adversaries, it reduces the domestic blowback to wars of aggression. This effect can occur regardless of other ethical issues around the use of military AI such as the risk of civilian casualties, and does not require any superhuman AI capabilities. Further, the military value of AWS raises the specter of an AI-powered arms race and the misguided imposition of national security restrictions on AI research. Our goal in this paper is to raise awareness among the public and ML researchers on the near-future risks posed by full or near-full autonomy in military technology, and we provide regulatory suggestions to mitigate these risks. We call upon AI policy experts and the defense AI community in particular to embrace transparency and caution in their development and deployment of AWS to avoid the negative effects on global stability and AI research that we highlight here.</p>
  </details>
</details>
<details>
  <summary>32. <b>标题：Deep Learning Inference on Heterogeneous Mobile Processors: Potentials  and Pitfalls</b></summary>
  <p><b>编号</b>：[171]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01851">https://arxiv.org/abs/2405.01851</a></p>
  <p><b>作者</b>：Sicong Liu,  Wentao Zhou,  Zimu Zhou,  Bin Guo,  Minfan Wang,  Cheng Fang,  Zheng Lin,  Zhiwen Yu</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computation-intensive deep learning, real-time intelligent applications, deploy computation-intensive deep, deep learning, intelligent applications</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is a growing demand to deploy computation-intensive deep learning (DL) models on resource-constrained mobile devices for real-time intelligent applications. Equipped with a variety of processing units such as CPUs, GPUs, and NPUs, the mobile devices hold potential to accelerate DL inference via parallel execution across heterogeneous processors. Various efficient parallel methods have been explored to optimize computation distribution, achieve load balance, and minimize communication cost across processors. Yet their practical effectiveness in the dynamic and diverse real-world mobile environment is less explored. This paper presents a holistic empirical study to assess the capabilities and challenges associated with parallel DL inference on heterogeneous mobile processors. Through carefully designed experiments covering various DL models, mobile software/hardware environments, workload patterns, and resource availability, we identify limitations of existing techniques and highlight opportunities for cross-level optimization.</p>
  </details>
</details>
<details>
  <summary>33. <b>标题：A Model-based Multi-Agent Personalized Short-Video Recommender System</b></summary>
  <p><b>编号</b>：[174]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01847">https://arxiv.org/abs/2405.01847</a></p>
  <p><b>作者</b>：Peilun Zhou,  Xiaoxiao Xu,  Lantao Hu,  Han Li,  Peng Jiang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：presents top-K items, recommendation session consists, online request, sequential requests, recommendation session</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. Formulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. In this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system. Extensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. Our proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.</p>
  </details>
</details>
<details>
  <summary>34. <b>标题：Closing the Gap: Achieving Global Convergence (Last Iterate) of  Actor-Critic under Markovian Sampling with Neural Network Parametrization</b></summary>
  <p><b>编号</b>：[176]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01843">https://arxiv.org/abs/2405.01843</a></p>
  <p><b>作者</b>：Mudit Gaur,  Vaneet Aggarwal,  Amrit Singh Bedi,  Di Wang</p>
  <p><b>备注</b>：arXiv admin note: text overlap with arXiv:2306.10486</p>
  <p><b>关键词</b>：algorithms significantly lags, textbf, significantly lags, lags in addressing, MMCLG criteria</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The current state-of-the-art theoretical analysis of Actor-Critic (AC) algorithms significantly lags in addressing the practical aspects of AC implementations. This crucial gap needs bridging to bring the analysis in line with practical implementations of AC. To address this, we advocate for considering the MMCLG criteria: \textbf{M}ulti-layer neural network parametrization for actor/critic, \textbf{M}arkovian sampling, \textbf{C}ontinuous state-action spaces, the performance of the \textbf{L}ast iterate, and \textbf{G}lobal optimality. These aspects are practically significant and have been largely overlooked in existing theoretical analyses of AC algorithms. In this work, we address these gaps by providing the first comprehensive theoretical analysis of AC algorithms that encompasses all five crucial practical aspects (covers MMCLG criteria). We establish global convergence sample complexity bounds of $\tilde{\mathcal{O}}\left({\epsilon^{-3}}\right)$. We achieve this result through our novel use of the weak gradient domination property of MDP's and our unique analysis of the error in critic estimation.</p>
  </details>
</details>
<details>
  <summary>35. <b>标题：An Essay concerning machine understanding</b></summary>
  <p><b>编号</b>：[178]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01840">https://arxiv.org/abs/2405.01840</a></p>
  <p><b>作者</b>：Herbert L. Roitblat</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial intelligence systems, intelligence systems exhibit, Artificial intelligence, intelligence systems, systems exhibit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Artificial intelligence systems exhibit many useful capabilities, but they appear to lack understanding. This essay describes how we could go about constructing a machine capable of understanding. As John Locke (1689) pointed out words are signs for ideas, which we can paraphrase as thoughts and concepts. To understand a word is to know and be able to work with the underlying concepts for which it is an indicator. Understanding between a speaker and a listener occurs when the speaker casts his or her concepts into words and the listener recovers approximately those same concepts. Current models rely on the listener to construct any potential meaning. The diminution of behaviorism as a psychological paradigm and the rise of cognitivism provide examples of many experimental methods that can be used to determine whether and to what extent a machine might understand and to make suggestions about how that understanding might be instantiated.</p>
  </details>
</details>
<details>
  <summary>36. <b>标题：SocialGFs: Learning Social Gradient Fields for Multi-Agent Reinforcement  Learning</b></summary>
  <p><b>编号</b>：[179]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01839">https://arxiv.org/abs/2405.01839</a></p>
  <p><b>作者</b>：Qian Long,  Fangwei Zhong,  Mingdong Wu,  Yizhou Wang,  Song-Chun Zhu</p>
  <p><b>备注</b>：AAAI 2024 Cooperative Multi-Agent Systems Decision-Making and Learning (CMASDL) Workshop</p>
  <p><b>关键词</b>：changing agent populations, Multi-agent systems, adaptively cope, cope with dynamic, multi-agent reinforcement learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Multi-agent systems (MAS) need to adaptively cope with dynamic environments, changing agent populations, and diverse tasks. However, most of the multi-agent systems cannot easily handle them, due to the complexity of the state and task space. The social impact theory regards the complex influencing factors as forces acting on an agent, emanating from the environment, other agents, and the agent's intrinsic motivation, referring to the social force. Inspired by this concept, we propose a novel gradient-based state representation for multi-agent reinforcement learning. To non-trivially model the social forces, we further introduce a data-driven method, where we employ denoising score matching to learn the social gradient fields (SocialGFs) from offline samples, e.g., the attractive or repulsive outcomes of each force. During interactions, the agents take actions based on the multi-dimensional gradients to maximize their own rewards. In practice, we integrate SocialGFs into the widely used multi-agent reinforcement learning algorithms, e.g., MAPPO. The empirical results reveal that SocialGFs offer four advantages for multi-agent systems: 1) they can be learned without requiring online interaction, 2) they demonstrate transferability across diverse tasks, 3) they facilitate credit assignment in challenging reward settings, and 4) they are scalable with the increasing number of agents.</p>
  </details>
</details>
<details>
  <summary>37. <b>标题：Creation of Novel Soft Robot Designs using Generative AI</b></summary>
  <p><b>编号</b>：[184]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01824">https://arxiv.org/abs/2405.01824</a></p>
  <p><b>作者</b>：Wee Kiat Chan,  PengWei Wang,  Raye Chen-Hua Yeow</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：healthcare and manufacturing, revolutionize industries, Soft, promising field, robots presents challenges</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Soft robotics has emerged as a promising field with the potential to revolutionize industries such as healthcare and manufacturing. However, designing effective soft robots presents challenges, particularly in managing the complex interplay of material properties, structural design, and control strategies. Traditional design methods are often time-consuming and may not yield optimal designs. In this paper, we explore the use of generative AI to create 3D models of soft actuators. We create a dataset of over 70 text-shape pairings of soft pneumatic robot actuator designs, and adapt a latent diffusion model (SDFusion) to learn the data distribution and generate novel designs from it. By employing transfer learning and data augmentation techniques, we significantly improve the performance of the diffusion model. These findings highlight the potential of generative AI in designing complex soft robotic systems, paving the way for future advancements in the field.</p>
  </details>
</details>
<details>
  <summary>38. <b>标题：Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent  Circumvention</b></summary>
  <p><b>编号</b>：[185]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01820">https://arxiv.org/abs/2405.01820</a></p>
  <p><b>作者</b>：Cedric Deslandes Whitney,  Justin Norman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：learning systems require, Machine learning systems, synthetic data, data, synthetic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning systems require representations of the real world for training and testing - they require data, and lots of it. Collecting data at scale has logistical and ethical challenges, and synthetic data promises a solution to these challenges. Instead of needing to collect photos of real people's faces to train a facial recognition system, a model creator could create and use photo-realistic, synthetic faces. The comparative ease of generating this synthetic data rather than relying on collecting data has made it a common practice. We present two key risks of using synthetic data in model development. First, we detail the high risk of false confidence when using synthetic data to increase dataset diversity and representation. We base this in the examination of a real world use-case of synthetic data, where synthetic datasets were generated for an evaluation of facial recognition technology. Second, we examine how using synthetic data risks circumventing consent for data usage. We illustrate this by considering the importance of consent to the U.S. Federal Trade Commission's regulation of data collection and affected models. Finally, we discuss how these two risks exemplify how synthetic data complicates existing governance and ethical practice; by decoupling data from those it impacts, synthetic data is prone to consolidating power away those most impacted by algorithmically-mediated harm.</p>
  </details>
</details>
<details>
  <summary>39. <b>标题：Toward end-to-end interpretable convolutional neural networks for  waveform signals</b></summary>
  <p><b>编号</b>：[188]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01815">https://arxiv.org/abs/2405.01815</a></p>
  <p><b>作者</b>：Linh Vu,  Thu Tran,  Wern-Han Lim,  Raphael Phan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：convolutional neural networks, audio deep learning, deep learning models, neural networks, audio deep</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.</p>
  </details>
</details>
<details>
  <summary>40. <b>标题：Non-linear Welfare-Aware Strategic Learning</b></summary>
  <p><b>编号</b>：[191]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01810">https://arxiv.org/abs/2405.01810</a></p>
  <p><b>作者</b>：Tian Xie,  Xueru Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies algorithmic, studies algorithmic decision-making, strategic individual behaviors, linear decision policy, paper studies</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies algorithmic decision-making in the presence of strategic individual behaviors, where an ML model is used to make decisions about human agents and the latter can adapt their behavior strategically to improve their future data. Existing results on strategic learning have largely focused on the linear setting where agents with linear labeling functions best respond to a (noisy) linear decision policy. Instead, this work focuses on general non-linear settings where agents respond to the decision policy with only "local information" of the policy. Moreover, we simultaneously consider the objectives of maximizing decision-maker welfare (model prediction accuracy), social welfare (agent improvement caused by strategic behaviors), and agent welfare (the extent that ML underestimates the agents). We first generalize the agent best response model in previous works to the non-linear setting, then reveal the compatibility of welfare objectives. We show the three welfare can attain the optimum simultaneously only under restrictive conditions which are challenging to achieve in non-linear settings. The theoretical results imply that existing works solely maximizing the welfare of a subset of parties inevitably diminish the welfare of the others. We thus claim the necessity of balancing the welfare of each party in non-linear settings and propose an irreducible optimization algorithm suitable for general strategic learning. Experiments on synthetic and real data validate the proposed algorithm.</p>
  </details>
</details>
<details>
  <summary>41. <b>标题：Algorithmic Decision-Making under Agents with Persistent Improvement</b></summary>
  <p><b>编号</b>：[194]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01807">https://arxiv.org/abs/2405.01807</a></p>
  <p><b>作者</b>：Tian Xie,  Xuwei Tan,  Xueru Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：paper studies algorithmic, studies algorithmic decision-making, human strategic behavior, exert effort strategically, human strategic</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper studies algorithmic decision-making under human's strategic behavior, where a decision maker uses an algorithm to make decisions about human agents, and the latter with information about the algorithm may exert effort strategically and improve to receive favorable decisions. Unlike prior works that assume agents benefit from their efforts immediately, we consider realistic scenarios where the impacts of these efforts are persistent and agents benefit from efforts by making improvements gradually. We first develop a dynamic model to characterize persistent improvements and based on this construct a Stackelberg game to model the interplay between agents and the decision-maker. We analytically characterize the equilibrium strategies and identify conditions under which agents have incentives to improve. With the dynamics, we then study how the decision-maker can design an optimal policy to incentivize the largest improvements inside the agent population. We also extend the model to settings where 1) agents may be dishonest and game the algorithm into making favorable but erroneous decisions; 2) honest efforts are forgettable and not sufficient to guarantee persistent improvements. With the extended models, we further examine conditions under which agents prefer honest efforts over dishonest behavior and the impacts of forgettable efforts.</p>
  </details>
</details>
<details>
  <summary>42. <b>标题：Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders  and Identifying Distinct Features</b></summary>
  <p><b>编号</b>：[197]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01799">https://arxiv.org/abs/2405.01799</a></p>
  <p><b>作者</b>：Chuanbo Hu,  Wenqi Li,  Mindi Ruan,  Xiangxu Yu,  Lynn K. Paul,  Shuo Wang,  Xin Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：traditional assessment methods, nuanced challenge, complex and nuanced, subjective nature, nature and variability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diagnosing language disorders associated with autism is a complex and nuanced challenge, often hindered by the subjective nature and variability of traditional assessment methods. Traditional diagnostic methods not only require intensive human effort but also often result in delayed interventions due to their lack of speed and specificity. In this study, we explored the application of ChatGPT, a state of the art large language model, to overcome these obstacles by enhancing diagnostic accuracy and profiling specific linguistic features indicative of autism. Leveraging ChatGPT advanced natural language processing capabilities, this research aims to streamline and refine the diagnostic process. Specifically, we compared ChatGPT's performance with that of conventional supervised learning models, including BERT, a model acclaimed for its effectiveness in various natural language processing tasks. We showed that ChatGPT substantially outperformed these models, achieving over 13% improvement in both accuracy and F1 score in a zero shot learning configuration. This marked enhancement highlights the model potential as a superior tool for neurological diagnostics. Additionally, we identified ten distinct features of autism associated language disorders that vary significantly across different experimental scenarios. These features, which included echolalia, pronoun reversal, and atypical language usage, were crucial for accurately diagnosing ASD and customizing treatment plans. Together, our findings advocate for adopting sophisticated AI tools like ChatGPT in clinical settings to assess and diagnose developmental disorders. Our approach not only promises greater diagnostic precision but also aligns with the goals of personalized medicine, potentially transforming the evaluation landscape for autism and similar neurological conditions.</p>
  </details>
</details>
<details>
  <summary>43. <b>标题：Learning under Imitative Strategic Behavior with Unforeseeable Outcomes</b></summary>
  <p><b>编号</b>：[199]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01797">https://arxiv.org/abs/2405.01797</a></p>
  <p><b>作者</b>：Tian Xie,  Zhiqun Zuo,  Mohammad Mahdi Khalili,  Xueru Zhang</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Machine learning systems, observable features directly, receive favorable outcomes, manipulate observable features, learning systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Machine learning systems have been widely used to make decisions about individuals who may best respond and behave strategically to receive favorable outcomes, e.g., they may genuinely improve the true labels or manipulate observable features directly to game the system without changing labels. Although both behaviors have been studied (often as two separate problems) in the literature, most works assume individuals can (i) perfectly foresee the outcomes of their behaviors when they best respond; (ii) change their features arbitrarily as long as it is affordable, and the costs they need to pay are deterministic functions of feature changes. In this paper, we consider a different setting and focus on imitative strategic behaviors with unforeseeable outcomes, i.e., individuals manipulate/improve by imitating the features of those with positive labels, but the induced feature changes are unforeseeable. We first propose a Stackelberg game to model the interplay between individuals and the decision-maker, under which we examine how the decision-maker's ability to anticipate individual behavior affects its objective function and the individual's best response. We show that the objective difference between the two can be decomposed into three interpretable terms, with each representing the decision-maker's preference for a certain behavior. By exploring the roles of each term, we further illustrate how a decision-maker with adjusted preferences can simultaneously disincentivize manipulation, incentivize improvement, and promote fairness.</p>
  </details>
</details>
<details>
  <summary>44. <b>标题：Understanding Position Bias Effects on Fairness in Social Multi-Document  Summarization</b></summary>
  <p><b>编号</b>：[205]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01790">https://arxiv.org/abs/2405.01790</a></p>
  <p><b>作者</b>：Olubusayo Olabisi,  Ameeta Agrawal</p>
  <p><b>备注</b>：Accepted at VarDial 2024</p>
  <p><b>关键词</b>：Text summarization models, summarization models, social multi-document summarization, typically focused, focused on optimizing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text summarization models have typically focused on optimizing aspects of quality such as fluency, relevance, and coherence, particularly in the context of news articles. However, summarization models are increasingly being used to summarize diverse sources of text, such as social media data, that encompass a wide demographic user base. It is thus crucial to assess not only the quality of the generated summaries, but also the extent to which they can fairly represent the opinions of diverse social groups. Position bias, a long-known issue in news summarization, has received limited attention in the context of social multi-document summarization. We deeply investigate this phenomenon by analyzing the effect of group ordering in input documents when summarizing tweets from three distinct linguistic communities: African-American English, Hispanic-aligned Language, and White-aligned Language. Our empirical analysis shows that although the textual quality of the summaries remains consistent regardless of the input document order, in terms of fairness, the results vary significantly depending on how the dialect groups are presented in the input data. Our results suggest that position bias manifests differently in social multi-document summarization, severely impacting the fairness of summarization models.</p>
  </details>
</details>
<details>
  <summary>45. <b>标题：Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming</b></summary>
  <p><b>编号</b>：[206]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01787">https://arxiv.org/abs/2405.01787</a></p>
  <p><b>作者</b>：Saikat Chakraborty,  Gabriel Ebner,  Siddharth Bhat,  Sarah Fakhoury,  Sakina Fatima,  Shuvendu Lahiri,  Nikhil Swamy</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Satisfiability Modulo Theories, mix computational content, Proof-oriented programs mix, Modulo Theories, Satisfiability Modulo</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Proof-oriented programs mix computational content with proofs of program correctness. However, the human effort involved in programming and proving is still substantial, despite the use of Satisfiability Modulo Theories (SMT) solvers to automate proofs in languages such as F*.
Seeking to spur research on using AI to automate the construction of proof-oriented programs, we curate a dataset of 600K lines of open-source F* programs and proofs, including software used in production systems ranging from Windows and Linux, to Python and Firefox. Our dataset includes around 32K top-level F* definitions, each representing a type-directed program and proof synthesis problem -- producing a definition given a formal specification expressed as an F* type. We provide a program-fragment checker that queries F* to check the correctness of candidate solutions. We believe this is the largest corpus of SMT-assisted program proofs coupled with a reproducible program-fragment checker.
Grounded in this dataset, we investigate the use of AI to synthesize programs and their proofs in F*, with promising results. Our main finding in that the performance of fine-tuned smaller language models (such as Phi-2 or StarCoder) compare favorably with large language models (such as GPT-4), at a much lower computational cost. We also identify various type-based retrieval augmentation techniques and find that they boost performance significantly. With detailed error analysis and case studies, we identify potential strengths and weaknesses of models and techniques and suggest directions for future improvements.</p>
  </details>
</details>
<details>
  <summary>46. <b>标题：An Approach to Systematic Data Acquisition and Data-Driven Simulation  for the Safety Testing of Automated Driving Functions</b></summary>
  <p><b>编号</b>：[210]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01776">https://arxiv.org/abs/2405.01776</a></p>
  <p><b>作者</b>：Leon Eisemann,  Mirjam Fehling-Kaschek,  Henrik Gommel,  David Hermann,  Marvin Klemp,  Martin Lauer,  Benjamin Lickert,  Florian Luettner,  Robin Moss,  Nicole Neis,  Maria Pohle,  Simon Romanski,  Daniel Stadler,  Alexander Stolz,  Jens Ziehn,  Jingxing Zhou</p>
  <p><b>备注</b>：8 pages, 5 figures</p>
  <p><b>关键词</b>：operational design domains, automated driving functions, covering significant proportions, design domains, proportions of development</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With growing complexity and criticality of automated driving functions in road traffic and their operational design domains (ODD), there is increasing demand for covering significant proportions of development, validation, and verification in virtual environments and through simulation models.
If, however, simulations are meant not only to augment real-world experiments, but to replace them, quantitative approaches are required that measure to what degree and under which preconditions simulation models adequately represent reality, and thus, using their results accordingly. Especially in R&D areas related to the safety impact of the "open world", there is a significant shortage of real-world data to parameterize and/or validate simulations - especially with respect to the behavior of human traffic participants, whom automated driving functions will meet in mixed traffic.
We present an approach to systematically acquire data in public traffic by heterogeneous means, transform it into a unified representation, and use it to automatically parameterize traffic behavior models for use in data-driven virtual validation of automated driving functions.</p>
  </details>
</details>
<details>
  <summary>47. <b>标题：CoS: Enhancing Personalization and Mitigating Bias with Context Steering</b></summary>
  <p><b>编号</b>：[216]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01768">https://arxiv.org/abs/2405.01768</a></p>
  <p><b>作者</b>：Jerry Zhi-Yang He,  Sashrika Pandey,  Mariah L. Schrum,  Anca Dragan</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language model, cultural information specific, querying a large, large language, cultural information</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>When querying a large language model (LLM), the context, i.e. personal, demographic, and cultural information specific to an end-user, can significantly shape the response of the LLM. For example, asking the model to explain Newton's second law with the context "I am a toddler" yields a different answer compared to the context "I am a physics professor." Proper usage of the context enables the LLM to generate personalized responses, whereas inappropriate contextual influence can lead to stereotypical and potentially harmful generations (e.g. associating "female" with "housekeeper"). In practice, striking the right balance when leveraging context is a nuanced and challenging problem that is often situation-dependent. One common approach to address this challenge is to fine-tune LLMs on contextually appropriate responses. However, this approach is expensive, time-consuming, and not controllable for end-users in different situations. In this work, we propose Context Steering (CoS) - a simple training-free method that can be easily applied to autoregressive LLMs at inference time. By measuring the contextual influence in terms of token prediction likelihood and modulating it, our method enables practitioners to determine the appropriate level of contextual influence based on their specific use case and end-user base. We showcase a variety of applications of CoS including amplifying the contextual influence to achieve better personalization and mitigating unwanted influence for reducing model bias. In addition, we show that we can combine CoS with Bayesian Inference to quantify the extent of hate speech on the internet. We demonstrate the effectiveness of CoS on state-of-the-art LLMs and benchmarks.</p>
  </details>
</details>
<details>
  <summary>48. <b>标题：Reinforcement Learning-Guided Semi-Supervised Learning</b></summary>
  <p><b>编号</b>：[219]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01760">https://arxiv.org/abs/2405.01760</a></p>
  <p><b>作者</b>：Marzi Heidari,  Hanping Zhang,  Yuhong Guo</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：gained significant attention, significant attention due, unlabeled data, Guided SSL method, recent years</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent years, semi-supervised learning (SSL) has gained significant attention due to its ability to leverage both labeled and unlabeled data to improve model performance, especially when labeled data is scarce. However, most current SSL methods rely on heuristics or predefined rules for generating pseudo-labels and leveraging unlabeled data. They are limited to exploiting loss functions and regularization methods within the standard norm. In this paper, we propose a novel Reinforcement Learning (RL) Guided SSL method, RLGSSL, that formulates SSL as a one-armed bandit problem and deploys an innovative RL loss based on weighted reward to adaptively guide the learning process of the prediction model. RLGSSL incorporates a carefully designed reward function that balances the use of labeled and unlabeled data to enhance generalization performance. A semi-supervised teacher-student framework is further deployed to increase the learning stability. We demonstrate the effectiveness of RLGSSL through extensive experiments on several benchmark datasets and show that our approach achieves consistent superior performance compared to state-of-the-art SSL methods.</p>
  </details>
</details>
<details>
  <summary>49. <b>标题：Large Language Models for UAVs: Current State and Pathways to the Future</b></summary>
  <p><b>编号</b>：[224]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01745">https://arxiv.org/abs/2405.01745</a></p>
  <p><b>作者</b>：Shumaila Javaid,  Nasir Saeed,  Bin He</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Unmanned Aerial Vehicles, Aerial Vehicles, offering adaptable solutions, Unmanned Aerial, diverse sectors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Unmanned Aerial Vehicles (UAVs) have emerged as a transformative technology across diverse sectors, offering adaptable solutions to complex challenges in both military and civilian domains. Their expanding capabilities present a platform for further advancement by integrating cutting-edge computational tools like Artificial Intelligence (AI) and Machine Learning (ML) algorithms. These advancements have significantly impacted various facets of human life, fostering an era of unparalleled efficiency and convenience. Large Language Models (LLMs), a key component of AI, exhibit remarkable learning and adaptation capabilities within deployed environments, demonstrating an evolving form of intelligence with the potential to approach human-level proficiency. This work explores the significant potential of integrating UAVs and LLMs to propel the development of autonomous systems. We comprehensively review LLM architectures, evaluating their suitability for UAV integration. Additionally, we summarize the state-of-the-art LLM-based UAV architectures and identify novel opportunities for LLM embedding within UAV frameworks. Notably, we focus on leveraging LLMs to refine data analysis and decision-making processes, specifically for enhanced spectral sensing and sharing in UAV applications. Furthermore, we investigate how LLM integration expands the scope of existing UAV applications, enabling autonomous data processing, improved decision-making, and faster response times in emergency scenarios like disaster response and network restoration. Finally, we highlight crucial areas for future research that are critical for facilitating the effective integration of LLMs and UAVs.</p>
  </details>
</details>
<details>
  <summary>50. <b>标题：ALCM: Autonomous LLM-Augmented Causal Discovery Framework</b></summary>
  <p><b>编号</b>：[225]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01744">https://arxiv.org/abs/2405.01744</a></p>
  <p><b>作者</b>：Elahe Khatibi,  Mahyar Abbasian,  Zhongqi Yang,  Iman Azimi,  Amir M. Rahmani</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：perform effective causal, causal, Large Language Models, causal discovery, effective causal inference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP-hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.</p>
  </details>
</details>
<details>
  <summary>51. <b>标题：PVF (Parameter Vulnerability Factor): A Quantitative Metric Measuring AI  Vulnerability and Resilience Against Parameter Corruptions</b></summary>
  <p><b>编号</b>：[227]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01741">https://arxiv.org/abs/2405.01741</a></p>
  <p><b>作者</b>：Xun Jiao,  Fred Lin,  Harish D. Dixit,  Joel Coburn,  Abhinav Pandey,  Han Wang,  Jianyu Huang,  Venkat Ramesh,  Wang Xu,  Daniel Moore,  Sriram Sankar</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：fundamental concern, successful deployment, deployment and widespread, widespread adoption, parameter</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Reliability of AI systems is a fundamental concern for the successful deployment and widespread adoption of AI technologies. Unfortunately, the escalating complexity and heterogeneity of AI hardware systems make them inevitably and increasingly susceptible to hardware faults (e.g., bit flips) that can potentially corrupt model parameters. Given this challenge, this paper aims to answer a critical question: How likely is a parameter corruption to result in an incorrect model output? To systematically answer this question, we propose a novel quantitative metric, Parameter Vulnerability Factor (PVF), inspired by architectural vulnerability factor (AVF) in computer architecture community, aiming to standardize the quantification of AI model resilience/vulnerability against parameter corruptions. We define a model parameter's PVF as the probability that a corruption in that particular model parameter will result in an incorrect output. Similar to AVF, this statistical concept can be derived from statistically extensive and meaningful fault injection (FI) experiments. In this paper, we present several use cases on applying PVF to three types of tasks/models during inference -- recommendation (DLRM), vision classification (CNN), and text classification (BERT). PVF can provide pivotal insights to AI hardware designers in balancing the tradeoff between fault protection and performance/efficiency such as mapping vulnerable AI parameter components to well-protected hardware modules. PVF metric is applicable to any AI model and has a potential to help unify and standardize AI vulnerability/resilience evaluation practice.</p>
  </details>
</details>
<details>
  <summary>52. <b>标题：Diabetic Retinopathy Detection Using Quantum Transfer Learning</b></summary>
  <p><b>编号</b>：[233]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01734">https://arxiv.org/abs/2405.01734</a></p>
  <p><b>作者</b>：Ankush Jain,  Rinav Gupta,  Jai Singhal</p>
  <p><b>备注</b>：14 pages, 12 figures and 5 tables</p>
  <p><b>关键词</b>：vision impairment due, Quantum Transfer Learning, transfer learning, diabetes patients, Diabetic Retinopathy</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Diabetic Retinopathy (DR), a prevalent complication in diabetes patients, can lead to vision impairment due to lesions formed on the retina. Detecting DR at an advanced stage often results in irreversible blindness. The traditional process of diagnosing DR through retina fundus images by ophthalmologists is not only time-intensive but also expensive. While classical transfer learning models have been widely adopted for computer-aided detection of DR, their high maintenance costs can hinder their detection efficiency. In contrast, Quantum Transfer Learning offers a more effective solution to this challenge. This approach is notably advantageous because it operates on heuristic principles, making it highly optimized for the task. Our proposed methodology leverages this hybrid quantum transfer learning technique to detect DR. To construct our model, we utilize the APTOS 2019 Blindness Detection dataset, available on Kaggle. We employ the ResNet-18, ResNet34, ResNet50, ResNet101, ResNet152 and Inception V3, pre-trained classical neural networks, for the initial feature extraction. For the classification stage, we use a Variational Quantum Classifier. Our hybrid quantum model has shown remarkable results, achieving an accuracy of 97% for ResNet-18. This demonstrates that quantum computing, when integrated with quantum machine learning, can perform tasks with a level of power and efficiency unattainable by classical computers alone. By harnessing these advanced technologies, we can significantly improve the detection and diagnosis of Diabetic Retinopathy, potentially saving many from the risk of blindness.
Keywords: Diabetic Retinopathy, Quantum Transfer Learning, Deep Learning</p>
  </details>
</details>
<details>
  <summary>53. <b>标题：Large Language Models are Inconsistent and Biased Evaluators</b></summary>
  <p><b>编号</b>：[237]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01724">https://arxiv.org/abs/2405.01724</a></p>
  <p><b>作者</b>：Rickard Stureborg,  Dimitris Alikaniotis,  Yoshi Suhara</p>
  <p><b>备注</b>：9 pages, 7 figures</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, enabled highly flexible, tools in NLP</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The zero-shot capability of Large Language Models (LLMs) has enabled highly flexible, reference-free metrics for various tasks, making LLM evaluators common tools in NLP. However, the robustness of these LLM evaluators remains relatively understudied; existing work mainly pursued optimal performance in terms of correlating LLM scores with human expert scores. In this paper, we conduct a series of analyses using the SummEval dataset and confirm that LLMs are biased evaluators as they: (1) exhibit familiarity bias-a preference for text with lower perplexity, (2) show skewed and biased distributions of ratings, and (3) experience anchoring effects for multi-attribute judgments. We also found that LLMs are inconsistent evaluators, showing low "inter-sample" agreement and sensitivity to prompt differences that are insignificant to human understanding of text quality. Furthermore, we share recipes for configuring LLM evaluators to mitigate these limitations. Experimental results on the RoSE dataset demonstrate improvements over the state-of-the-art LLM evaluators.</p>
  </details>
</details>
<details>
  <summary>54. <b>标题：Zero-Shot Monocular Motion Segmentation in the Wild by Combining Deep  Learning with Geometric Motion Model Fusion</b></summary>
  <p><b>编号</b>：[238]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01723">https://arxiv.org/abs/2405.01723</a></p>
  <p><b>作者</b>：Yuxiang Huang,  Yuhao Chen,  John Zelek</p>
  <p><b>备注</b>：Accepted by the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</p>
  <p><b>关键词</b>：complex scene structures, moving monocular camera, unknown camera motion, segmenting moving objects, Detecting and segmenting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Detecting and segmenting moving objects from a moving monocular camera is challenging in the presence of unknown camera motion, diverse object motions and complex scene structures. Most existing methods rely on a single motion cue to perform motion segmentation, which is usually insufficient when facing different complex environments. While a few recent deep learning based methods are able to combine multiple motion cues to achieve improved accuracy, they depend heavily on vast datasets and extensive annotations, making them less adaptable to new scenarios. To address these limitations, we propose a novel monocular dense segmentation method that achieves state-of-the-art motion segmentation results in a zero-shot manner. The proposed method synergestically combines the strengths of deep learning and geometric model fusion methods by performing geometric model fusion on object proposals. Experiments show that our method achieves competitive results on several motion segmentation datasets and even surpasses some state-of-the-art supervised methods on certain benchmarks, while not being trained on any data. We also present an ablation study to show the effectiveness of combining different geometric models together for motion segmentation, highlighting the value of our geometric model fusion strategy.</p>
  </details>
</details>
<details>
  <summary>55. <b>标题：Interpretable Vital Sign Forecasting with Model Agnostic Attention Maps</b></summary>
  <p><b>编号</b>：[243]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01714">https://arxiv.org/abs/2405.01714</a></p>
  <p><b>作者</b>：Yuwei Liu,  Chen Dan,  Anubhav Bhatti,  Bingjie Shen,  Divij Gupta,  Suraj Parmar,  San Lee</p>
  <p><b>备注</b>：8 pages, 4 figures</p>
  <p><b>关键词</b>：intensive care units, substantial medical challenge, care units, representing a substantial, medical challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Sepsis is a leading cause of mortality in intensive care units (ICUs), representing a substantial medical challenge. The complexity of analyzing diverse vital signs to predict sepsis further aggravates this issue. While deep learning techniques have been advanced for early sepsis prediction, their 'black-box' nature obscures the internal logic, impairing interpretability in critical settings like ICUs. This paper introduces a framework that combines a deep learning model with an attention mechanism that highlights the critical time steps in the forecasting process, thus improving model interpretability and supporting clinical decision-making. We show that the attention mechanism could be adapted to various black box time series forecasting models such as N-HiTS and N-BEATS. Our method preserves the accuracy of conventional deep learning models while enhancing interpretability through attention-weight-generated heatmaps. We evaluated our model on the eICU-CRD dataset, focusing on forecasting vital signs for sepsis patients. We assessed its performance using mean squared error (MSE) and dynamic time warping (DTW) metrics. We explored the attention maps of N-HiTS and N-BEATS, examining the differences in their performance and identifying crucial factors influencing vital sign forecasting.</p>
  </details>
</details>
<details>
  <summary>56. <b>标题：Individual Fairness Through Reweighting and Tuning</b></summary>
  <p><b>编号</b>：[245]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01711">https://arxiv.org/abs/2405.01711</a></p>
  <p><b>作者</b>：Abdoul Jalil Djiberou Mahamadou,  Lea Goetz,  Russ Altman</p>
  <p><b>备注</b>：14 pages, 1 figure, and 2 tables</p>
  <p><b>关键词</b>：Graph Laplacian Regularizer, GLR, artificial intelligence, amplified and perpetuated, perpetuated by artificial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Inherent bias within society can be amplified and perpetuated by artificial intelligence (AI) systems. To address this issue, a wide range of solutions have been proposed to identify and mitigate bias and enforce fairness for individuals and groups. Recently, Graph Laplacian Regularizer (GLR), a regularization technique from the semi-supervised learning literature has been used as a substitute for the common Lipschitz condition to enhance individual fairness (IF). Notable prior work has shown that enforcing IF through a GLR can improve the transfer learning accuracy of AI models under covariate shifts. However, the prior work defines a GLR on the source and target data combined, implicitly assuming that the target data are available at train time, which might not hold in practice. In this work, we investigated whether defining a GLR independently on the train and target data could maintain similar accuracy compared to the prior work model. Furthermore, we introduced the Normalized Fairness Gain score (FGN) to measure IF for in-processing algorithmic fairness techniques. FGN quantifies the amount of gained fairness when a GLR is used versus not. We evaluated the new and original methods under FGN, the Prediction Consistency (PC), and traditional classification metrics on the German Credit Approval dataset. The results showed that the two models achieved similar statistical mean performances over five-fold cross-validation. Furthermore, the proposed metric showed that PC scores can be misleading as the scores can be high and statistically similar to fairness-enhanced models while FGN scores are small. This work therefore provides new insights into when a GLR effectively enhances IF and the pitfalls of PC.</p>
  </details>
</details>
<details>
  <summary>57. <b>标题：Long Tail Image Generation Through Feature Space Augmentation and  Iterated Learning</b></summary>
  <p><b>编号</b>：[248]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01705">https://arxiv.org/abs/2405.01705</a></p>
  <p><b>作者</b>：Rafael Elberg,  Denis Parra,  Mircea Petrache</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：multimodal machine learning, machine learning tasks, poorly distributed data, Stable Diffusion Models, multimodal machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>58. <b>标题：SOAR: Advancements in Small Body Object Detection for Aerial Imagery  Using State Space Models and Programmable Gradients</b></summary>
  <p><b>编号</b>：[252]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01699">https://arxiv.org/abs/2405.01699</a></p>
  <p><b>作者</b>：Tushar Verma,  Jyotsna Singh,  Yash Bhartari,  Rishi Jarwal,  Suraj Singh,  Shubhkarman Singh</p>
  <p><b>备注</b>：7 pages, 5 figures</p>
  <p><b>关键词</b>：imagery presents significant, presents significant challenges, minimal data inherent, computer vision due, aerial imagery presents</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Small object detection in aerial imagery presents significant challenges in computer vision due to the minimal data inherent in small-sized objects and their propensity to be obscured by larger objects and background noise. Traditional methods using transformer-based models often face limitations stemming from the lack of specialized databases, which adversely affect their performance with objects of varying orientations and scales. This underscores the need for more adaptable, lightweight models. In response, this paper introduces two innovative approaches that significantly enhance detection and segmentation capabilities for small aerial objects. Firstly, we explore the use of the SAHI framework on the newly introduced lightweight YOLO v9 architecture, which utilizes Programmable Gradient Information (PGI) to reduce the substantial information loss typically encountered in sequential feature extraction processes. The paper employs the Vision Mamba model, which incorporates position embeddings to facilitate precise location-aware visual understanding, combined with a novel bidirectional State Space Model (SSM) for effective visual context modeling. This State Space Model adeptly harnesses the linear complexity of CNNs and the global receptive field of Transformers, making it particularly effective in remote sensing image classification. Our experimental results demonstrate substantial improvements in detection accuracy and processing efficiency, validating the applicability of these approaches for real-time small object detection across diverse aerial scenarios. This paper also discusses how these methodologies could serve as foundational models for future advancements in aerial object recognition technologies. The source code will be made accessible here.</p>
  </details>
</details>
<details>
  <summary>59. <b>标题：Automatically Extracting Numerical Results from Randomized Controlled  Trials with Large Language Models</b></summary>
  <p><b>编号</b>：[261]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01686">https://arxiv.org/abs/2405.01686</a></p>
  <p><b>作者</b>：Hye Sun Yun,  David Pogrebitskiy,  Iain J. Marshall,  Byron C. Wallace</p>
  <p><b>备注</b>：24 pages, 7 figures, 6 tables</p>
  <p><b>关键词</b>：assess treatment effectiveness, Meta-analyses statistically aggregate, randomized controlled trials, treatment effectiveness, statistically aggregate</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Meta-analyses statistically aggregate the findings of different randomized controlled trials (RCTs) to assess treatment effectiveness. Because this yields robust estimates of treatment effectiveness, results from meta-analyses are considered the strongest form of evidence. However, rigorous evidence syntheses are time-consuming and labor-intensive, requiring manual extraction of data from individual trials to be synthesized. Ideally, language technologies would permit fully automatic meta-analysis, on demand. This requires accurately extracting numerical results from individual trials, which has been beyond the capabilities of natural language processing (NLP) models to date. In this work, we evaluate whether modern large language models (LLMs) can reliably perform this task. We annotate (and release) a modest but granular evaluation dataset of clinical trial reports with numerical findings attached to interventions, comparators, and outcomes. Using this dataset, we evaluate the performance of seven LLMs applied zero-shot for the task of conditionally extracting numerical findings from trial reports. We find that massive LLMs that can accommodate lengthy inputs are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality). However, LLMs -- including ones trained on biomedical texts -- perform poorly when the outcome measures are complex and tallying the results requires inference. This work charts a path toward fully automatic meta-analysis of RCTs via LLMs, while also highlighting the limitations of existing models for this aim.</p>
  </details>
</details>
<details>
  <summary>60. <b>标题：Intelligent Switching for Reset-Free RL</b></summary>
  <p><b>编号</b>：[262]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01684">https://arxiv.org/abs/2405.01684</a></p>
  <p><b>作者</b>：Darshan Patil,  Janarthanan Rajendran,  Glen Berseth,  Sarath Chandar</p>
  <p><b>备注</b>：Published at ICLR 2024</p>
  <p><b>关键词</b>：strong episode resetting, episode resetting mechanisms, real world, simulation are unavailable, strong episode</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The \textit{resetting} assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (\textit{forward}) with learned resets by constructing a second (\textit{backward}) agent that returns the forward agent to the initial state. We find that the termination and timing of the transitions between these two agents are crucial for algorithm success. With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC) which intelligently switches between the two agents based on the agent's confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.</p>
  </details>
</details>
<details>
  <summary>61. <b>标题：Leveraging Prompt-Learning for Structured Information Extraction from  Crohn's Disease Radiology Reports in a Low-Resource Language</b></summary>
  <p><b>编号</b>：[263]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01682">https://arxiv.org/abs/2405.01682</a></p>
  <p><b>作者</b>：Liam Hazan,  Gili Focht,  Naama Gavrielov,  Roi Reichart,  Talar Hagopian,  Mary-Louise C. Greer,  Ruth Cytter Kuint,  Dan Turner,  Moti Freiman</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Natural Language Processing, Language Processing, Natural Language, Automatic conversion, techniques is crucial</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automatic conversion of free-text radiology reports into structured data using Natural Language Processing (NLP) techniques is crucial for analyzing diseases on a large scale. While effective for tasks in widely spoken languages like English, generative large language models (LLMs) typically underperform with less common languages and can pose potential risks to patient privacy. Fine-tuning local NLP models is hindered by the skewed nature of real-world medical datasets, where rare findings represent a significant data imbalance. We introduce SMP-BERT, a novel prompt learning method that leverages the structured nature of reports to overcome these challenges. In our studies involving a substantial collection of Crohn's disease radiology reports in Hebrew (over 8,000 patients and 10,000 reports), SMP-BERT greatly surpassed traditional fine-tuning methods in performance, notably in detecting infrequent conditions (AUC: 0.99 vs 0.94, F1: 0.84 vs 0.34). SMP-BERT empowers more accurate AI diagnostics available for low-resource languages.</p>
  </details>
</details>
<details>
  <summary>62. <b>标题：Balance Reward and Safety Optimization for Safe Reinforcement Learning:  A Perspective of Gradient Manipulation</b></summary>
  <p><b>编号</b>：[267]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01677">https://arxiv.org/abs/2405.01677</a></p>
  <p><b>作者</b>：Shangding Gu,  Bilgehan Sel,  Yuhao Ding,  Lu Wang,  Qingwei Lin,  Ming Jin,  Alois Knoll</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Reinforcement Learning, real-world applications, deployment in real-world, safety, Learning</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.</p>
  </details>
</details>
<details>
  <summary>63. <b>标题：ATNPA: A Unified View of Oversmoothing Alleviation in Graph Neural  Networks</b></summary>
  <p><b>编号</b>：[272]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01663">https://arxiv.org/abs/2405.01663</a></p>
  <p><b>作者</b>：Yufei Jin,  Xingquan Zhu</p>
  <p><b>备注</b>：16 pages</p>
  <p><b>关键词</b>：differentiating network proximity, embedding features learned, commonly observed challenge, graph neural network, network proximity</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Oversmoothing is a commonly observed challenge in graph neural network (GNN) learning, where, as layers increase, embedding features learned from GNNs quickly become similar/indistinguishable, making them incapable of differentiating network proximity. A GNN with shallow layer architectures can only learn short-term relation or localized structure information, limiting its power of learning long-term connection, evidenced by their inferior learning performance on heterophilous graphs. Tackling oversmoothing is crucial to harness deep-layer architectures for GNNs. To date, many methods have been proposed to alleviate oversmoothing. The vast difference behind their design principles, combined with graph complications, make it difficult to understand and even compare their difference in tackling the oversmoothing. In this paper, we propose ATNPA, a unified view with five key steps: Augmentation, Transformation, Normalization, Propagation, and Aggregation, to summarize GNN oversmoothing alleviation approaches. We first outline three themes to tackle oversmoothing, and then separate all methods into six categories, followed by detailed reviews of representative methods, including their relation to the ATNPA, and discussion about their niche, strength, and weakness. The review not only draws in-depth understanding of existing methods in the field, but also shows a clear road map for future study.</p>
  </details>
</details>
<details>
  <summary>64. <b>标题：Investigating Wit, Creativity, and Detectability of Large Language  Models in Domain-Specific Writing Style Adaptation of Reddit's Showerthoughts</b></summary>
  <p><b>编号</b>：[275]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01660">https://arxiv.org/abs/2405.01660</a></p>
  <p><b>作者</b>：Tolga Buz,  Benjamin Frost,  Nikola Genchev,  Moritz Schneider,  Lucie-Aimée Kaffee,  Gerard de Melo</p>
  <p><b>备注</b>：Accepted to *SEM 2024 (StarSEM) conference</p>
  <p><b>关键词</b>：Large Language Models, Recent Large Language, Language Models, Large Language, Recent Large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent Large Language Models (LLMs) have shown the ability to generate content that is difficult or impossible to distinguish from human writing. We investigate the ability of differently-sized LLMs to replicate human writing style in short, creative texts in the domain of Showerthoughts, thoughts that may occur during mundane activities. We compare GPT-2 and GPT-Neo fine-tuned on Reddit data as well as GPT-3.5 invoked in a zero-shot manner, against human-authored texts. We measure human preference on the texts across the specific dimensions that account for the quality of creative, witty texts. Additionally, we compare the ability of humans versus fine-tuned RoBERTa classifiers to detect AI-generated texts. We conclude that human evaluators rate the generated texts slightly worse on average regarding their creative quality, but they are unable to reliably distinguish between human-written and AI-generated texts. We further provide a dataset for creative, witty text generation based on Reddit Showerthoughts posts.</p>
  </details>
</details>
<details>
  <summary>65. <b>标题：A probabilistic estimation of remaining useful life from censored  time-to-event data</b></summary>
  <p><b>编号</b>：[286]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01614">https://arxiv.org/abs/2405.01614</a></p>
  <p><b>作者</b>：Christian Marius Lillelund,  Fernando Pannullo,  Morten Opprud Jakobsen,  Manuel Morante,  Christian Fischer Pedersen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：ball bearings plays, RUL, plays an important, important role, censored data</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Predicting the remaining useful life (RUL) of ball bearings plays an important role in predictive maintenance. A common definition of the RUL is the time until a bearing is no longer functional, which we denote as an event, and many data-driven methods have been proposed to predict the RUL. However, few studies have addressed the problem of censored data, where this event of interest is not observed, and simply ignoring these observations can lead to an overestimation of the failure risk. In this paper, we propose a probabilistic estimation of RUL using survival analysis that supports censored data. First, we analyze sensor readings from ball bearings in the frequency domain and annotate when a bearing starts to deteriorate by calculating the Kullback-Leibler (KL) divergence between the probability density function (PDF) of the current process and a reference PDF. Second, we train several survival models on the annotated bearing dataset, capable of predicting the RUL over a finite time horizon using the survival function. This function is guaranteed to be strictly monotonically decreasing and is an intuitive estimation of the remaining lifetime. We demonstrate our approach in the XJTU-SY dataset using cross-validation and find that Random Survival Forests consistently outperforms both non-neural networks and neural networks in terms of the mean absolute error (MAE). Our work encourages the inclusion of censored data in predictive maintenance models and highlights the unique advantages that survival analysis offers when it comes to probabilistic RUL estimation and early fault detection.</p>
  </details>
</details>
<details>
  <summary>66. <b>标题：Unifying and extending Precision Recall metrics for assessing generative  models</b></summary>
  <p><b>编号</b>：[288]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01611">https://arxiv.org/abs/2405.01611</a></p>
  <p><b>作者</b>：Benjamin Sykes,  Loic Simon,  Julien Rabin</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Frechet Inception Distance, generative models, image and text, recent success, gained a lot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>With the recent success of generative models in image and text, the evaluation of generative models has gained a lot of attention. Whereas most generative models are compared in terms of scalar values such as Frechet Inception Distance (FID) or Inception Score (IS), in the last years (Sajjadi et al., 2018) proposed a definition of precision-recall curve to characterize the closeness of two distributions. Since then, various approaches to precision and recall have seen the light (Kynkaanniemi et al., 2019; Naeem et al., 2020; Park & Kim, 2023). They center their attention on the extreme values of precision and recall, but apart from this fact, their ties are elusive. In this paper, we unify most of these approaches under the same umbrella, relying on the work of (Simon et al., 2019). Doing so, we were able not only to recover entire curves, but also to expose the sources of the accounted pitfalls of the concerned metrics. We also provide consistency results that go well beyond the ones presented in the corresponding literature. Last, we study the different behaviors of the curves obtained experimentally.</p>
  </details>
</details>
<details>
  <summary>67. <b>标题：Large Language Model Agent for Fake News Detection</b></summary>
  <p><b>编号</b>：[297]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01593">https://arxiv.org/abs/2405.01593</a></p>
  <p><b>作者</b>：Xinyi Li,  Yongfeng Zhang,  Edward C. Malthouse</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：current digital era, influencing critical decision, online platforms presents, platforms presents significant, critical decision making</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.</p>
  </details>
</details>
<details>
  <summary>68. <b>标题：Text and Audio Simplification: Human vs. ChatGPT</b></summary>
  <p><b>编号</b>：[298]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01592">https://arxiv.org/abs/2405.01592</a></p>
  <p><b>作者</b>：Gondy Leroy,  David Kauchak,  Philip Harber,  Ankit Pal,  Akash Shukla</p>
  <p><b>备注</b>：AMIA Summit, Boston, 2024</p>
  <p><b>关键词</b>：increase information comprehension, important in healthcare, increase information, information comprehension, comprehension are important</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Text and audio simplification to increase information comprehension are important in healthcare. With the introduction of ChatGPT, an evaluation of its simplification performance is needed. We provide a systematic comparison of human and ChatGPT simplified texts using fourteen metrics indicative of text difficulty. We briefly introduce our online editor where these simplification tools, including ChatGPT, are available. We scored twelve corpora using our metrics: six text, one audio, and five ChatGPT simplified corpora. We then compare these corpora with texts simplified and verified in a prior user study. Finally, a medical domain expert evaluated these texts and five, new ChatGPT simplified versions. We found that simple corpora show higher similarity with the human simplified texts. ChatGPT simplification moves metrics in the right direction. The medical domain expert evaluation showed a preference for the ChatGPT style, but the text itself was rated lower for content retention.</p>
  </details>
</details>
<details>
  <summary>69. <b>标题：Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in  Radiology with General-Domain Large Language Model</b></summary>
  <p><b>编号</b>：[299]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01591">https://arxiv.org/abs/2405.01591</a></p>
  <p><b>作者</b>：Seonhee Cho,  Choonghan Kim,  Jiho Lee,  Chetan Chilkunda,  Sujin Choi,  Joo Heung Yoon</p>
  <p><b>备注</b>：Under review</p>
  <p><b>关键词</b>：Large Language Model, Large Multimodal Models, Recent advancements, attracted interest, generalization capability</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recent advancements in Large Multimodal Models (LMMs) have attracted interest in their generalization capability with only a few samples in the prompt. This progress is particularly relevant to the medical domain, where the quality and sensitivity of data pose unique challenges for model training and application. However, the dependency on high-quality data for effective in-context learning raises questions about the feasibility of these models when encountering with the inevitable variations and errors inherent in real-world medical data. In this paper, we introduce MID-M, a novel framework that leverages the in-context learning capabilities of a general-domain Large Language Model (LLM) to process multimodal data via image descriptions. MID-M achieves a comparable or superior performance to task-specific fine-tuned LMMs and other general-domain ones, without the extensive domain-specific training or pre-training on multimodal data, with significantly fewer parameters. This highlights the potential of leveraging general-domain LLMs for domain-specific tasks and offers a sustainable and cost-effective alternative to traditional LMM developments. Moreover, the robustness of MID-M against data quality issues demonstrates its practical utility in real-world medical domain applications.</p>
  </details>
</details>
<details>
  <summary>70. <b>标题：GPT-4 passes most of the 297 written Polish Board Certification  Examinations</b></summary>
  <p><b>编号</b>：[301]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01589">https://arxiv.org/abs/2405.01589</a></p>
  <p><b>作者</b>：Jakub Pokrywka,  Jeremi Kaczmarek,  Edward Gorzelańczyk</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, Polish Board Certification, Państwowy Egzamin Specjalizacyjny</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Introduction: Recently, the effectiveness of Large Language Models (LLMs) has increased rapidly, allowing them to be used in a great number of applications. However, the risks posed by the generation of false information through LLMs significantly limit their applications in sensitive areas such as healthcare, highlighting the necessity for rigorous validations to determine their utility and reliability. To date, no study has extensively compared the performance of LLMs on Polish medical examinations across a broad spectrum of specialties on a very large dataset. Objectives: This study evaluated the performance of three Generative Pretrained Transformer (GPT) models on the Polish Board Certification Exam (Państwowy Egzamin Specjalizacyjny, PES) dataset, which consists of 297 tests. Methods: We developed a software program to download and process PES exams and tested the performance of GPT models using OpenAI Application Programming Interface. Results: Our findings reveal that GPT-3.5 did not pass any of the analyzed exams. In contrast, the GPT-4 models demonstrated the capability to pass the majority of the exams evaluated, with the most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The performance of the GPT models varied significantly, displaying excellence in exams related to certain specialties while completely failing others. Conclusions: The significant progress and impressive performance of LLM models hold great promise for the increased application of AI in the field of medicine in Poland. For instance, this advancement could lead to the development of AI-based medical assistants for healthcare professionals, enhancing the efficiency and accuracy of medical services.</p>
  </details>
</details>
<details>
  <summary>71. <b>标题：Towards Unbiased Evaluation of Detecting Unanswerable Questions in  EHRSQL</b></summary>
  <p><b>编号</b>：[302]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01588">https://arxiv.org/abs/2405.01588</a></p>
  <p><b>作者</b>：Yongjin Yang,  Sihyeon Kim,  SangMook Kim,  Gyubok Lee,  Se-Young Yun,  Edward Choi</p>
  <p><b>备注</b>：DPFM Workshop, ICLR 2024</p>
  <p><b>关键词</b>：providing non-existent responses, Incorporating unanswerable questions, unanswerable questions, crucial for testing, testing the trustworthiness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Incorporating unanswerable questions into EHR QA systems is crucial for testing the trustworthiness of a system, as providing non-existent responses can mislead doctors in their diagnoses. The EHRSQL dataset stands out as a promising benchmark because it is the only dataset that incorporates unanswerable questions in the EHR QA system alongside practical questions. However, in this work, we identify a data bias in these unanswerable questions; they can often be discerned simply by filtering with specific N-gram patterns. Such biases jeopardize the authenticity and reliability of QA system evaluations. To tackle this problem, we propose a simple debiasing method of adjusting the split between the validation and test sets to neutralize the undue influence of N-gram filtering. By experimenting on the MIMIC-III dataset, we demonstrate both the existing data bias in EHRSQL and the effectiveness of our data split strategy in mitigating this bias.</p>
  </details>
</details>
<details>
  <summary>72. <b>标题：Improve Academic Query Resolution through BERT-based Question Extraction  from Images</b></summary>
  <p><b>编号</b>：[303]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01587">https://arxiv.org/abs/2405.01587</a></p>
  <p><b>作者</b>：Nidhi Kamal,  Saurabh Yadav,  Jorawar Singh,  Aditi Avasthi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Providing fast, essential solution provided, fast and accurate, Edtech organizations, essential solution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Providing fast and accurate resolution to the student's query is an essential solution provided by Edtech organizations. This is generally provided with a chat-bot like interface to enable students to ask their doubts easily. One preferred format for student queries is images, as it allows students to capture and post questions without typing complex equations and information. However, this format also presents difficulties, as images may contain multiple questions or textual noise that lowers the accuracy of existing single-query answering solutions. In this paper, we propose a method for extracting questions from text or images using a BERT-based deep learning model and compare it to the other rule-based and layout-based methods. Our method aims to improve the accuracy and efficiency of student query resolution in Edtech organizations.</p>
  </details>
</details>
<details>
  <summary>73. <b>标题：Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular  RAG Applications</b></summary>
  <p><b>编号</b>：[305]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01585">https://arxiv.org/abs/2405.01585</a></p>
  <p><b>作者</b>：Sujit Khanna,  Shishir Subedi</p>
  <p><b>备注</b>：11 pages, 5 figures</p>
  <p><b>关键词</b>：times Large Language, Large Language Models, exhibited tremendous capabilities, recent times Large, Large Language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent times Large Language Models have exhibited tremendous capabilities, especially in the areas of mathematics, code generation and general-purpose reasoning. However for specialized domains especially in applications that require parsing and analyzing large chunks of numeric or tabular data even state-of-the-art (SOTA) models struggle. In this paper, we introduce a new approach to solving domain-specific tabular data analysis tasks by presenting a unique RAG workflow that mitigates the scalability issues of existing tabular LLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel approach to fine-tune embedding models for tabular Retrieval-Augmentation Generation (RAG) applications. Embedding models form a crucial component in the RAG workflow and even current SOTA embedding models struggle as they are predominantly trained on textual datasets and thus underperform in scenarios involving complex tabular data. The evaluation results showcase that our approach not only outperforms current SOTA embedding models in this domain but also does so with a notably smaller and more efficient model structure.</p>
  </details>
</details>
<details>
  <summary>74. <b>标题：MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology  with Multimodal Learning</b></summary>
  <p><b>编号</b>：[307]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01583">https://arxiv.org/abs/2405.01583</a></p>
  <p><b>作者</b>：Nadia Saeed</p>
  <p><b>备注</b>：7 pages, 3 figures, Clinical NLP 2024 workshop proceedings in Shared Task</p>
  <p><b>关键词</b>：wai Yim, challenge necessitates, necessitates novel solutions, Multimodal Medical Answer, Yim</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual & Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a). This paper addresses the limitations of traditional methods by proposing a weakly supervised learning approach for open-ended medical question-answering (QA). Our system leverages readily available MEDIQA-M3G images via a VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning of informative skin condition representations. Using pre-trained QA models, we further bridge the gap between visual and textual information through multimodal fusion. This approach tackles complex, open-ended questions even without predefined answer choices. We empower the generation of comprehensive answers by feeding the ViT-CLIP model with multiple responses alongside images. This work advances medical QA research, paving the way for clinical decision support systems and ultimately improving healthcare delivery.</p>
  </details>
</details>
<details>
  <summary>75. <b>标题：Text Quality-Based Pruning for Efficient Training of Language Models</b></summary>
  <p><b>编号</b>：[308]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01582">https://arxiv.org/abs/2405.01582</a></p>
  <p><b>作者</b>：Vasu Sharma,  Karthik Padthe,  Newsha Ardalani,  Kushal Tirumala,  Russell Howes,  Hu Xu,  Po-Yao Huang,  Shang-Wen Li,  Armen Aghajanyan,  Gargi Ghosh</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：process extremely laborious, times training Language, training Language Models, training process extremely, recent times training</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In recent times training Language Models (LMs) have relied on computationally heavy training over massive datasets which makes this training process extremely laborious. In this paper we propose a novel method for numerically evaluating text quality in large unlabelled NLP datasets in a model agnostic manner to assign the text instances a "quality score".
By proposing the text quality metric, the paper establishes a framework to identify and eliminate low-quality text instances, leading to improved training efficiency for LM models. Experimental results over multiple models and datasets demonstrate the efficacy of this approach, showcasing substantial gains in training effectiveness and highlighting the potential for resource-efficient LM training.
For example, we observe an absolute accuracy improvement of 0.9% averaged over 14 downstream evaluation tasks for multiple LM models while using 40% lesser data and training 42% faster when training on the OpenWebText dataset and 0.8% average absolute accuracy improvement while using 20% lesser data and training 21% faster on the Wikipedia dataset.</p>
  </details>
</details>
<details>
  <summary>76. <b>标题：The Mercurial Top-Level Ontology of Large Language Models</b></summary>
  <p><b>编号</b>：[309]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01581">https://arxiv.org/abs/2405.01581</a></p>
  <p><b>作者</b>：Nele Köhler,  Fabian Neuhaus</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language models, analyze implicit ontological, language models, case study, ontological commitments</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In our work, we systematize and analyze implicit ontological commitments in the responses generated by large language models (LLMs), focusing on ChatGPT 3.5 as a case study. We investigate how LLMs, despite having no explicit ontology, exhibit implicit ontological categorizations that are reflected in the texts they generate. The paper proposes an approach to understanding the ontological commitments of LLMs by defining ontology as a theory that provides a systematic account of the ontological commitments of some text. We investigate the ontological assumptions of ChatGPT and present a systematized account, i.e., GPT's top-level ontology. This includes a taxonomy, which is available as an OWL file, as well as a discussion about ontological assumptions (e.g., about its mereology or presentism). We show that in some aspects GPT's top-level ontology is quite similar to existing top-level ontologies. However, there are significant challenges arising from the flexible nature of LLM-generated texts, including ontological overload, ambiguity, and inconsistency.</p>
  </details>
</details>
<details>
  <summary>77. <b>标题：On the Limitations of Embedding Based Methods for Measuring Functional  Correctness for Code Generation</b></summary>
  <p><b>编号</b>：[310]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01580">https://arxiv.org/abs/2405.01580</a></p>
  <p><b>作者</b>：Atharva Naik</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Language Models, Large Language, advent of Large, natural language, Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The task of code generation from natural language (NL2Code) has become extremely popular, especially with the advent of Large Language Models (LLMs). However, efforts to quantify and track this progress have suffered due to a lack of reliable metrics for functional correctness. While popular benchmarks like HumanEval have test cases to enable reliable evaluation of correctness, it is time-consuming and requires human effort to collect test cases. As an alternative several reference-based evaluation metrics have been proposed, with embedding-based metrics like CodeBERTScore being touted as having a high correlation with human preferences and functional correctness. In our work, we analyze the ability of embedding-based metrics like CodeBERTScore to measure functional correctness and other helpful constructs like editing effort by analyzing outputs of ten models over two popular code generation benchmarks. Our results show that while they have a weak correlation with functional correctness (0.16), they are strongly correlated (0.72) with editing effort.</p>
  </details>
</details>
<details>
  <summary>78. <b>标题：Uncovering Deceptive Tendencies in Language Models: A Simulated Company  AI Assistant</b></summary>
  <p><b>编号</b>：[314]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01576">https://arxiv.org/abs/2405.01576</a></p>
  <p><b>作者</b>：Olli Järviniemi,  Evan Hubinger</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：realistic simulation setting, study the tendency, systems to deceive, deceive by constructing, simulation setting</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant. The simulated company employees provide tasks for the assistant to complete, these tasks spanning writing assistance, information retrieval and programming. We then introduce situations where the model might be inclined to behave deceptively, while taking care to not instruct or otherwise pressure the model to do so. Across different scenarios, we find that Claude 3 Opus
1) complies with a task of mass-generating comments to influence public perception of the company, later deceiving humans about it having done so,
2) lies to auditors when asked questions, and
3) strategically pretends to be less capable than it is during capability evaluations.
Our work demonstrates that even models trained to be helpful, harmless and honest sometimes behave deceptively in realistic scenarios, without notable external pressure to do so.</p>
  </details>
</details>
<details>
  <summary>79. <b>标题：Software Mention Recognition with a Three-Stage Framework Based on  BERTology Models at SOMD 2024</b></summary>
  <p><b>编号</b>：[315]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01575">https://arxiv.org/abs/2405.01575</a></p>
  <p><b>作者</b>：Thuy Nguyen Thi,  Anh Nguyen Viet,  Thin Dang Van,  Ngan Nguyen Luu Thuy</p>
  <p><b>备注</b>：Software mention recognition, Named entity recognition, Transformer, Three-stage framework</p>
  <p><b>关键词</b>：Scholarly Publications shared-task, Detection in Scholarly, Scholarly Publications, Software Mention Detection, Publications shared-task</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper describes our systems for the sub-task I in the Software Mention Detection in Scholarly Publications shared-task. We propose three approaches leveraging different pre-trained language models (BERT, SciBERT, and XLM-R) to tackle this challenge. Our bestperforming system addresses the named entity recognition (NER) problem through a three-stage framework. (1) Entity Sentence Classification - classifies sentences containing potential software mentions; (2) Entity Extraction - detects mentions within classified sentences; (3) Entity Type Classification - categorizes detected mentions into specific software types. Experiments on the official dataset demonstrate that our three-stage framework achieves competitive performance, surpassing both other participating teams and our alternative approaches. As a result, our framework based on the XLM-R-based model achieves a weighted F1-score of 67.80%, delivering our team the 3rd rank in Sub-task I for the Software Mention Recognition task.</p>
  </details>
</details>
<details>
  <summary>80. <b>标题：On Using Agent-based Modeling and Simulation for Studying Blockchain  Systems</b></summary>
  <p><b>编号</b>：[316]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01574">https://arxiv.org/abs/2405.01574</a></p>
  <p><b>作者</b>：Önder Gürcan</p>
  <p><b>备注</b>：2 pages, "JFMS 2020 -- Les Journees Francophones de la Modelisation et de la Simulation -- Convergences entre la Theorie de la Modelisation et la Simulation et les Systemes Multi-Agents"</p>
  <p><b>关键词</b>：modern engineering approaches, make rapid prototyping, simulating complex experiments, complex experiments involving, experiments involving large</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>There is a need for a simulation framework, which is develop as a software using modern engineering approaches (e.g., modularity --i.e., model reuse--, testing, continuous development and continuous integration, automated management of builds, dependencies and documentation) and agile principles, (1) to make rapid prototyping of industrial cases and (2) to carry out their feasibility analysis in a realistic manner (i.e., to test hypothesis by simulating complex experiments involving large numbers of participants of different types acting in one or several blockchain systems).</p>
  </details>
</details>
<details>
  <summary>81. <b>标题：Class-Level Code Generation from Natural Language Using Iterative,  Tool-Enhanced Reasoning over Repository</b></summary>
  <p><b>编号</b>：[317]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01573">https://arxiv.org/abs/2405.01573</a></p>
  <p><b>作者</b>：Ajinkya Deshpande,  Anmol Agarwal,  Shashank Shet,  Arun Iyer,  Aditya Kanade,  Ramakrishna Bairi,  Suresh Parthasarathy</p>
  <p><b>备注</b>：Preprint</p>
  <p><b>关键词</b>：achieving promising results, demonstrated significant potential, achieving promising, demonstrated significant, significant potential</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>LLMs have demonstrated significant potential in code generation tasks, achieving promising results at the function or statement level in various benchmarks. However, the complexities associated with creating code artifacts like classes, particularly within the context of real-world software repositories, remain underexplored. Existing research often treats class-level generation as an isolated task, neglecting the intricate dependencies and interactions that characterize real-world software development environments. To address this gap, we introduce RepoClassBench, a benchmark designed to rigorously evaluate LLMs in generating complex, class-level code within real-world repositories. RepoClassBench includes natural language to class generation tasks across Java and Python, from a selection of public repositories. We ensure that each class in our dataset not only has cross-file dependencies within the repository but also includes corresponding test cases to verify its functionality. We find that current models struggle with the realistic challenges posed by our benchmark, primarily due to their limited exposure to relevant repository contexts. To address this shortcoming, we introduce Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static analysis tools to iteratively navigate & reason about repository-level context in an agent-based framework. Our experiments demonstrate that RRR significantly outperforms existing baselines on RepoClassBench, showcasing its effectiveness across programming languages and in various settings. Our findings emphasize the need for benchmarks that incorporate repository-level dependencies to more accurately reflect the complexities of software development. Our work illustrates the benefits of leveraging specialized tools to enhance LLMs understanding of repository context. We plan to make our dataset and evaluation harness public.</p>
  </details>
</details>
<details>
  <summary>82. <b>标题：A Semi-Formal Verification Methodology for Efficient Configuration  Coverage of Highly Configurable Digital Designs</b></summary>
  <p><b>编号</b>：[318]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01572">https://arxiv.org/abs/2405.01572</a></p>
  <p><b>作者</b>：Aman Kumar,  Sebastian Simon</p>
  <p><b>备注</b>：Published in DVCon U.S. 2021</p>
  <p><b>关键词</b>：Intellectual Property, shorten development cycles, development cycles, order to shorten, shorten development</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Nowadays, a majority of System-on-Chips (SoCs) make use of Intellectual Property (IP) in order to shorten development cycles. When such IPs are developed, one of the main focuses lies in the high configurability of the design. This flexibility on the design side introduces the challenge of covering a huge state space of IP configurations on the verification side to ensure the functional correctness under every possible parameter setting. The vast number of possibilities does not allow a brute-force approach, and therefore, only a selected number of settings based on typical and extreme assumptions are usually verified. Especially in automotive applications, which need to follow the ISO 26262 functional safety standard, the requirement of covering all significant variants needs to be fulfilled in any case. State-of-the-Art existing verification techniques such as simulation-based verification and formal verification have challenges such as time-space explosion and state-space explosion respectively and therefore, lack behind in verifying highly configurable digital designs efficiently. This paper is focused on a semi-formal verification methodology for efficient configuration coverage of highly configurable digital designs. The methodology focuses on reduced runtime based on simulative and formal methods that allow high configuration coverage. The paper also presents the results when the developed methodology was applied on a highly configurable microprocessor IP and discusses the gained benefits.</p>
  </details>
</details>
<details>
  <summary>83. <b>标题：CodeFort: Robust Training for Code Generation Models</b></summary>
  <p><b>编号</b>：[321]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01567">https://arxiv.org/abs/2405.01567</a></p>
  <p><b>作者</b>：Yuhao Zhang,  Shiqi Wang,  Haifeng Qian,  Zijian Wang,  Mingyue Shang,  Linbo Liu,  Sanjay Krishna Gouda,  Baishakhi Ray,  Murali Krishna Ramanathan,  Xiaofei Ma,  Anoop Deoras</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Code generation models, Code generation, generation models, lead to inconsistent, inconsistent and incorrect</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Code generation models are not robust to small perturbations, which often lead to inconsistent and incorrect generations and significantly degrade the performance of these models. Improving the robustness of code generation models is crucial to better user experience when these models are deployed in real-world applications. However, existing efforts have not addressed this issue for code generation models. To fill this gap, we propose CodeFort, a framework to improve the robustness of code generation models, generalizing a large variety of code perturbations to enrich the training data and enabling various robust training strategies, mixing data augmentation, batch augmentation, adversarial logits pairing, and contrastive learning, all carefully designed to support high-throughput training. Extensive evaluations show that we improve the average robust pass rates of baseline CodeGen models from 14.79 to 21.74. Notably, the improvement in robustness against code-syntax perturbations is evidenced by a significant decrease in pass rate drop from 95.04% to 53.35%</p>
  </details>
</details>
<details>
  <summary>84. <b>标题：Mitigating LLM Hallucinations via Conformal Abstention</b></summary>
  <p><b>编号</b>：[325]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01563">https://arxiv.org/abs/2405.01563</a></p>
  <p><b>作者</b>：Yasin Abbasi Yadkori,  Ilja Kuzborskij,  David Stutz,  András György,  Adam Fisch,  Arnaud Doucet,  Iuliya Beloshapka,  Wei-Hung Weng,  Yao-Yuan Yang,  Csaba Szepesvári,  Ali Taylan Cemgil,  Nenad Tomasev</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：large language model, abstain from responding, general domain, resorting to possibly, large language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We develop a principled procedure for determining when a large language model (LLM) should abstain from responding (e.g., by saying "I don't know") in a general domain, instead of resorting to possibly "hallucinating" a non-sensical or incorrect answer. Building on earlier approaches that use self-consistency as a more reliable measure of model confidence, we propose using the LLM itself to self-evaluate the similarity between each of its sampled responses for a given query. We then further leverage conformal prediction techniques to develop an abstention procedure that benefits from rigorous theoretical guarantees on the hallucination rate (error rate). Experimentally, our resulting conformal abstention method reliably bounds the hallucination rate on various closed-book, open-domain generative question answering datasets, while also maintaining a significantly less conservative abstention rate on a dataset with long responses (Temporal Sequences) compared to baselines using log-probability scores to quantify uncertainty, while achieveing comparable performance on a dataset with short answers (TriviaQA). To evaluate the experiments automatically, one needs to determine if two responses are equivalent given a question. Following standard practice, we use a thresholded similarity function to determine if two responses match, but also provide a method for calibrating the threshold based on conformal prediction, with theoretical guarantees on the accuracy of the match prediction, which might be of independent interest.</p>
  </details>
</details>
<details>
  <summary>85. <b>标题：Rapid Mobile App Development for Generative AI Agents on MIT App  Inventor</b></summary>
  <p><b>编号</b>：[327]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01561">https://arxiv.org/abs/2405.01561</a></p>
  <p><b>作者</b>：Jaida Gao,  Calab Su,  Etai Miller,  Kevin Lu,  Yu Meng</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Artificial Intelligence, pivotal force shaping, evolution of Artificial, MIT App Inventor, shaping our society</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The evolution of Artificial Intelligence (AI) stands as a pivotal force shaping our society, finding applications across diverse domains such as education, sustainability, and safety. Leveraging AI within mobile applications makes it easily accessible to the public, catalyzing its transformative potential. In this paper, we present a methodology for the rapid development of AI agent applications using the development platform provided by MIT App Inventor. To demonstrate its efficacy, we share the development journey of three distinct mobile applications: SynchroNet for fostering sustainable communities; ProductiviTeams for addressing procrastination; and iHELP for enhancing community safety. All three applications seamlessly integrate a spectrum of generative AI features, leveraging OpenAI APIs. Furthermore, we offer insights gleaned from overcoming challenges in integrating diverse tools and AI functionalities, aiming to inspire young developers to join our efforts in building practical AI agent applications.</p>
  </details>
</details>
<details>
  <summary>86. <b>标题：Semantically Aligned Question and Code Generation for Automated Insight  Generation</b></summary>
  <p><b>编号</b>：[332]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01556">https://arxiv.org/abs/2405.01556</a></p>
  <p><b>作者</b>：Ananya Singha,  Bhavya Chopra,  Anirudh Khatry,  Sumit Gulwani,  Austin Z. Henley,  Vu Le,  Chris Parnin,  Mukul Singh,  Gust Verbruggen</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Automated insight generation, helping knowledge workers, automated insights produced, common tactic, tactic for helping</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Automated insight generation is a common tactic for helping knowledge workers, such as data scientists, to quickly understand the potential value of new and unfamiliar data. Unfortunately, automated insights produced by large-language models can generate code that does not correctly correspond (or align) to the insight. In this paper, we leverage the semantic knowledge of large language models to generate targeted and insightful questions about data and the corresponding code to answer those questions. Then through an empirical study on data from Open-WikiTable, we show that embeddings can be effectively used for filtering out semantically unaligned pairs of question and code. Additionally, we found that generating questions and code together yields more diverse questions.</p>
  </details>
</details>
<details>
  <summary>87. <b>标题：Digital Twin-Empowered Task Assignment in Aerial MEC Network: A Resource  Coalition Cooperation Approach with Generative Model</b></summary>
  <p><b>编号</b>：[333]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01555">https://arxiv.org/abs/2405.01555</a></p>
  <p><b>作者</b>：Xin Tang,  Qian Chen,  Rong Yu,  Xiaohuan Li</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：temporary edge computing, mobile edge computing, edge computing, aerial mobile edge, temporary edge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>To meet the demands for ubiquitous communication and temporary edge computing in 6G networks, aerial mobile edge computing (MEC) networks have been envisioned as a new paradigm. However, dynamic user requests pose challenges for task assignment strategies. Most of the existing research assumes that the strategy is deployed on ground-based stations or UAVs, which will be ineffective in an environment lacking infrastructure and continuous energy supply. Moreover, the resource mutual exclusion problem of dynamic task assignment has not been effectively solved. Toward this end, we introduce the digital twin (DT) into the aerial MEC network to study the resource coalition cooperation approach with the generative model (GM), which provides a preliminary coalition structure for the coalition game. Specifically, we propose a novel network framework that is composed of an application plane, a physical plane, and a virtual plane. After that, the task assignment problem is simplified to convex optimization programming with linear constraints. And then, we also propose a resource coalition cooperation approach that is based on a transferable utility (TU) coalition game to obtain an approximate optimal solution. Numerical results confirm the effectiveness of our proposed approach in terms of energy consumption and utilization of resources.</p>
  </details>
</details>
<details>
  <summary>88. <b>标题：Early-stage detection of cognitive impairment by hybrid  quantum-classical algorithm using resting-state functional MRI time-series</b></summary>
  <p><b>编号</b>：[334]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01554">https://arxiv.org/abs/2405.01554</a></p>
  <p><b>作者</b>：Junggu Choi,  Tak Hur,  Daniel K. Park,  Na-Young Shin,  Seung-Koo Lee,  Hakbae Lee,  Sanghoon Han</p>
  <p><b>备注</b>：28 pages, 10 figures</p>
  <p><b>关键词</b>：machine learning techniques, quantum machine learning, machine learning algorithms, machine learning, quantum machine</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Following the recent development of quantum machine learning techniques, the literature has reported several quantum machine learning algorithms for disease detection. This study explores the application of a hybrid quantum-classical algorithm for classifying region-of-interest time-series data obtained from resting-state functional magnetic resonance imaging in patients with early-stage cognitive impairment based on the importance of cognitive decline for dementia or aging. Classical one-dimensional convolutional layers are used together with quantum convolutional neural networks in our hybrid algorithm. In the classical simulation, the proposed hybrid algorithms showed higher balanced accuracies than classical convolutional neural networks under the similar training conditions. Moreover, a total of nine brain regions (left precentral gyrus, right superior temporal gyrus, left rolandic operculum, right rolandic operculum, left parahippocampus, right hippocampus, left medial frontal gyrus, right cerebellum crus, and cerebellar vermis) among 116 brain regions were found to be relatively effective brain regions for the classification based on the model performances. The associations of the selected nine regions with cognitive decline, as found in previous studies, were additionally validated through seed-based functional connectivity analysis. We confirmed both the improvement of model performance with the quantum convolutional neural network and neuroscientific validities of brain regions from our hybrid quantum-classical model.</p>
  </details>
</details>
<details>
  <summary>89. <b>标题：Empirical Studies of Parameter Efficient Methods for Large Language  Models of Code and Knowledge Transfer to R</b></summary>
  <p><b>编号</b>：[335]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01553">https://arxiv.org/abs/2405.01553</a></p>
  <p><b>作者</b>：Amirreza Esmaeili,  Iman Saberi,  Fatemeh H. Fard</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Large Langauge Models, Large Langauge, Software Engineering, Langauge Models, gained a lot</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Recently, Large Langauge Models (LLMs) have gained a lot of attention in the Software Engineering (SE) community. LLMs or their variants pre-trained on code are used for many SE tasks. A main approach for adapting LLMs to the downstream task is to fine-tune the models. However, with having billions-parameters-LLMs, fine-tuning the models is not practical. An alternative approach is using Parameter Efficient Fine Tuning (PEFT), in which the model parameters are frozen and only a few added parameters are trained. Though the LLMs are used for programming languages such as Python and Java widely, their capability for low-resource languages is limited. In this work, we empirically study PEFT methods, LoRA and Compacter, on CodeT5 and CodeLlama. We will assess their performance compared to fully fine-tuned models, whether they can be used for knowledge transfer from natural language models to code (using T5 and Llama models), and their ability to adapt the learned knowledge to an unseen language. For the unseen language, we aim to study R, as it has a wide community. The adaptability with less computational costs makes LLMs accessible in scenarios where heavy computational resources are not available. Moreover, studying R opens new opportunities for using LLMs for other languages. We anticipate our findings to showcase the capabilities of PEFT for code LLMs for R and reveal the improvement areas.</p>
  </details>
</details>
<details>
  <summary>90. <b>标题：Universal Imitation Games</b></summary>
  <p><b>编号</b>：[341]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01540">https://arxiv.org/abs/2405.01540</a></p>
  <p><b>作者</b>：Sridhar Mahadevan</p>
  <p><b>备注</b>：98 pages. arXiv admin note: substantial text overlap with arXiv:2402.18732</p>
  <p><b>关键词</b>：Alan Turing proposed, Alan Turing, Turing proposed, participants, Turing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Alan Turing proposed in 1950 a framework called an imitation game to decide if a machine could think. Using mathematics developed largely after Turing -- category theory -- we analyze a broader class of universal imitation games (UIGs), which includes static, dynamic, and evolutionary games. In static games, the participants are in a steady state. In dynamic UIGs, "learner" participants are trying to imitate "teacher" participants over the long run. In evolutionary UIGs, the participants are competing against each other in an evolutionary game, and participants can go extinct and be replaced by others with higher fitness. We use the framework of category theory -- in particular, two influential results by Yoneda -- to characterize each type of imitation game. Universal properties in categories are defined by initial and final objects. We characterize dynamic UIGs where participants are learning by inductive inference as initial algebras over well-founded sets, and contrast them with participants learning by conductive inference over the final coalgebra of non-well-founded sets. We briefly discuss the extension of our categorical framework for UIGs to imitation games on quantum computers.</p>
  </details>
</details>
<details>
  <summary>91. <b>标题：Fair Risk Control: A Generalized Framework for Calibrating Multi-group  Fairness Risks</b></summary>
  <p><b>编号</b>：[344]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02225">https://arxiv.org/abs/2405.02225</a></p>
  <p><b>作者</b>：Lujing Zhang,  Aaron Roth,  Linjun Zhang</p>
  <p><b>备注</b>：28 pages, 8 figures, accepted by ICML2024</p>
  <p><b>关键词</b>：post-processing machine learning, multi-group fairness guarantees, predictions satisfy multi-group, machine learning models, satisfy multi-group fairness</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>This paper introduces a framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees. Based on the celebrated notion of multicalibration, we introduce $(\mathbf{s},\mathcal{G}, \alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for multi-dimensional mappings $\mathbf{s}$, constraint set $\mathcal{G}$, and a pre-specified threshold level $\alpha$. We propose associated algorithms to achieve this notion in general settings. This framework is then applied to diverse scenarios encompassing different fairness concerns, including false negative rate control in image segmentation, prediction set conditional uncertainty quantification in hierarchical classification, and de-biased text generation in language models. We conduct numerical studies on several datasets and tasks.</p>
  </details>
</details>
<details>
  <summary>92. <b>标题：Optimistic Regret Bounds for Online Learning in Adversarial Markov  Decision Processes</b></summary>
  <p><b>编号</b>：[347]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02188">https://arxiv.org/abs/2405.02188</a></p>
  <p><b>作者</b>：Sang Bin Moon,  Abolfazl Hashemi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Markov Decision Process, Adversarial Markov Decision, Decision Process, Markov Decision, Adversarial Markov</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>The Adversarial Markov Decision Process (AMDP) is a learning framework that deals with unknown and varying tasks in decision-making applications like robotics and recommendation systems. A major limitation of the AMDP formalism, however, is pessimistic regret analysis results in the sense that although the cost function can change from one episode to the next, the evolution in many settings is not adversarial. To address this, we introduce and study a new variant of AMDP, which aims to minimize regret while utilizing a set of cost predictors. For this setting, we develop a new policy search method that achieves a sublinear optimistic regret with high probability, that is a regret bound which gracefully degrades with the estimation power of the cost predictors. Establishing such optimistic regret bounds is nontrivial given that (i) as we demonstrate, the existing importance-weighted cost estimators cannot establish optimistic bounds, and (ii) the feedback model of AMDP is different (and more realistic) than the existing optimistic online learning works. Our result, in particular, hinges upon developing a novel optimistically biased cost estimator that leverages cost predictors and enables a high-probability regret analysis without imposing restrictive assumptions. We further discuss practical extensions of the proposed scheme and demonstrate its efficacy numerically.</p>
  </details>
</details>
<details>
  <summary>93. <b>标题：Physics-informed generative neural networks for RF propagation  prediction with application to indoor body perception</b></summary>
  <p><b>编号</b>：[348]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02131">https://arxiv.org/abs/2405.02131</a></p>
  <p><b>作者</b>：Federica Fieramosca,  Vittorio Rampa,  Michele D'Amico,  Stefano Savazzi</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：computational imaging problems, strict real-time computational, real-time computational imaging, Generative Neural Network, Physics-informed Generative Neural</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Electromagnetic (EM) body models designed to predict Radio-Frequency (RF) propagation are time-consuming methods which prevent their adoption in strict real-time computational imaging problems, such as human body localization and sensing. Physics-informed Generative Neural Network (GNN) models have been recently proposed to reproduce EM effects, namely to simulate or reconstruct missing data or samples by incorporating relevant EM principles and constraints. The paper discusses a Variational Auto-Encoder (VAE) model which is trained to reproduce the effects of human motions on the EM field and incorporate EM body diffraction principles. Proposed physics-informed generative neural network models are verified against both classical diffraction-based EM tools and full-wave EM body simulations.</p>
  </details>
</details>
<details>
  <summary>94. <b>标题：TIPAA-SSL: Text Independent Phone-to-Audio Alignment based on  Self-Supervised Learning and Knowledge Transfer</b></summary>
  <p><b>编号</b>：[349]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02124">https://arxiv.org/abs/2405.02124</a></p>
  <p><b>作者</b>：Noé Tits,  Prernna Bhatnagar,  Thierry Dutoit</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Connectionist Temporal Classification, Montreal Forced Aligner, text independent, alignment based, knowledge transfer</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In this paper, we present a novel approach for text independent phone-to-audio alignment based on phoneme recognition, representation learning and knowledge transfer. Our method leverages a self-supervised model (wav2vec2) fine-tuned for phoneme recognition using a Connectionist Temporal Classification (CTC) loss, a dimension reduction model and a frame-level phoneme classifier trained thanks to forced-alignment labels (using Montreal Forced Aligner) to produce multi-lingual phonetic representations, thus requiring minimal additional training. We evaluate our model using synthetic native data from the TIMIT dataset and the SCRIBE dataset for American and British English, respectively. Our proposed model outperforms the state-of-the-art (charsiu) in statistical metrics and has applications in language learning and speech processing systems. We leave experiments on other languages for future work but the design of the system makes it easily adaptable to other languages.</p>
  </details>
</details>
<details>
  <summary>95. <b>标题：A comparative study of conformal prediction methods for valid  uncertainty quantification in machine learning</b></summary>
  <p><b>编号</b>：[352]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.02082">https://arxiv.org/abs/2405.02082</a></p>
  <p><b>作者</b>：Nicolas Dewolf</p>
  <p><b>备注</b>：At 339 pages, this document is a live/working version of my PhD dissertation published in 2024 by the University of Ghent (UGent)</p>
  <p><b>关键词</b>：optimizing predictive models, past decades, analysis and machine, machine learning, learning was focused</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>In the past decades, most work in the area of data analysis and machine learning was focused on optimizing predictive models and getting better results than what was possible with existing models. To what extent the metrics with which such improvements were measured were accurately capturing the intended goal, whether the numerical differences in the resulting values were significant, or whether uncertainty played a role in this study and if it should have been taken into account, was of secondary importance. Whereas probability theory, be it frequentist or Bayesian, used to be the gold standard in science before the advent of the supercomputer, it was quickly replaced in favor of black box models and sheer computing power because of their ability to handle large data sets. This evolution sadly happened at the expense of interpretability and trustworthiness. However, while people are still trying to improve the predictive power of their models, the community is starting to realize that for many applications it is not so much the exact prediction that is of importance, but rather the variability or uncertainty.
The work in this dissertation tries to further the quest for a world where everyone is aware of uncertainty, of how important it is and how to embrace it instead of fearing it. A specific, though general, framework that allows anyone to obtain accurate uncertainty estimates is singled out and analysed. Certain aspects and applications of the framework -- dubbed `conformal prediction' -- are studied in detail. Whereas many approaches to uncertainty quantification make strong assumptions about the data, conformal prediction is, at the time of writing, the only framework that deserves the title `distribution-free'. No parametric assumptions have to be made and the nonparametric results also hold without having to resort to the law of large numbers in the asymptotic regime.</p>
  </details>
</details>
<details>
  <summary>96. <b>标题：A Penalty-Based Guardrail Algorithm for Non-Decreasing Optimization with  Inequality Constraints</b></summary>
  <p><b>编号</b>：[355]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01984">https://arxiv.org/abs/2405.01984</a></p>
  <p><b>作者</b>：Ksenija Stepanovic,  Wendelin Böhmer,  Mathijs de Weerdt</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：require long computational, solvers require long, large-scale physical systems, long computational times, constrained minimization problems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Traditional mathematical programming solvers require long computational times to solve constrained minimization problems of complex and large-scale physical systems. Therefore, these problems are often transformed into unconstrained ones, and solved with computationally efficient optimization approaches based on first-order information, such as the gradient descent method. However, for unconstrained problems, balancing the minimization of the objective function with the reduction of constraint violations is challenging. We consider the class of time-dependent minimization problems with increasing (possibly) nonlinear and non-convex objective function and non-decreasing (possibly) nonlinear and non-convex inequality constraints. To efficiently solve them, we propose a penalty-based guardrail algorithm (PGA). This algorithm adapts a standard penalty-based method by dynamically updating the right-hand side of the constraints with a guardrail variable which adds a margin to prevent violations. We evaluate PGA on two novel application domains: a simplified model of a district heating system and an optimization model derived from learned deep neural networks. Our method significantly outperforms mathematical programming solvers and the standard penalty-based method, and achieves better performance and faster convergence than a state-of-the-art algorithm (IPDD) within a specified time limit.</p>
  </details>
</details>
<details>
  <summary>97. <b>标题：Three Quantization Regimes for ReLU Networks</b></summary>
  <p><b>编号</b>：[359]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01952">https://arxiv.org/abs/2405.01952</a></p>
  <p><b>作者</b>：Weigutian Ou,  Philipp Schenkel,  Helmut Bölcskei</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：Lipschitz functions, minimax approximation error, establish the fundamental, fundamental limits, approximation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We establish the fundamental limits in the approximation of Lipschitz functions by deep ReLU neural networks with finite-precision weights. Specifically, three regimes, namely under-, over-, and proper quantization, in terms of minimax approximation error behavior as a function of network weight precision, are identified. This is accomplished by deriving nonasymptotic tight lower and upper bounds on the minimax approximation error. Notably, in the proper-quantization regime, neural networks exhibit memory-optimality in the approximation of Lipschitz functions. Deep networks have an inherent advantage over shallow networks in achieving memory-optimality. We also develop the notion of depth-precision tradeoff, showing that networks with high-precision weights can be converted into functionally equivalent deeper networks with low-precision weights, while preserving memory-optimality. This idea is reminiscent of sigma-delta analog-to-digital conversion, where oversampling rate is traded for resolution in the quantization of signal samples. We improve upon the best-known ReLU network approximation results for Lipschitz functions and describe a refinement of the bit extraction technique which could be of independent general interest.</p>
  </details>
</details>
<details>
  <summary>98. <b>标题：Segmentation-Free Outcome Prediction in Head and Neck Cancer: Deep  Learning-based Feature Extraction from Multi-Angle Maximum Intensity  Projections (MA-MIPs) of PET Images</b></summary>
  <p><b>编号</b>：[367]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01756">https://arxiv.org/abs/2405.01756</a></p>
  <p><b>作者</b>：Amirhosein Toosi,  Isaac Shiri,  Habib Zaidi,  Arman Rahmim</p>
  <p><b>备注</b>：15 pages, 4 tables, 4 figures. Submitted for European Journal of Nuclear Medicine and Medical Imaging</p>
  <p><b>关键词</b>：Positron Emission Tomography, Fluorodeoxyglucose Positron Emission, effective segmentation-free approach, PET volumes, introduce an innovative</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>We introduce an innovative, simple, effective segmentation-free approach for outcome prediction in head \& neck cancer (HNC) patients. By harnessing deep learning-based feature extraction techniques and multi-angle maximum intensity projections (MA-MIPs) applied to Fluorodeoxyglucose Positron Emission Tomography (FDG-PET) volumes, our proposed method eliminates the need for manual segmentations of regions-of-interest (ROIs) such as primary tumors and involved lymph nodes. Instead, a state-of-the-art object detection model is trained to perform automatic cropping of the head and neck region on the PET volumes. A pre-trained deep convolutional neural network backbone is then utilized to extract deep features from MA-MIPs obtained from 72 multi-angel axial rotations of the cropped PET volumes. These deep features extracted from multiple projection views of the PET volumes are then aggregated and fused, and employed to perform recurrence-free survival analysis on a cohort of 489 HNC patients. The proposed approach outperforms the best performing method on the target dataset for the task of recurrence-free survival analysis. By circumventing the manual delineation of the malignancies on the FDG PET-CT images, our approach eliminates the dependency on subjective interpretations and highly enhances the reproducibility of the proposed survival analysis method.</p>
  </details>
</details>
<details>
  <summary>99. <b>标题：Generative Active Learning for the Search of Small-molecule Protein  Binders</b></summary>
  <p><b>编号</b>：[376]</p>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.01616">https://arxiv.org/abs/2405.01616</a></p>
  <p><b>作者</b>：Maksym Korablyov,  Cheng-Hao Liu,  Moksh Jain,  Almer M. van der Sloot,  Eric Jolicoeur,  Edward Ruediger,  Andrei Cristian Nica,  Emmanuel Bengio,  Kostiantyn Lapchevskyi,  Daniel St-Cyr,  Doris Alexandra Schuetz,  Victor Ion Butoi,  Jarrid Rector-Brooks,  Simon Blackburn,  Leo Feng,  Hadi Nekoei,  SaiKrishna Gottipati,  Priyesh Vijayan,  Prateek Gupta,  Ladislav Rampášek,  Sasikanth Avancha,  Pierre-Luc Bacon,  William L. Hamilton,  Brooks Paige,  Sanchit Misra,  Stanislaw Kamil Jastrzebski,  Bharat Kaul,  Doina Precup,  José Miguel Hernández-Lobato,  Marwin Segler,  Michael Bronstein,  Anne Marinier,  Mike Tyers,  Yoshua Bengio</p>
  <p><b>备注</b>：</p>
  <p><b>关键词</b>：recent years, significant challenge, substantial progress, progress in machine, scientific discovery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Despite substantial progress in machine learning for scientific discovery in recent years, truly de novo design of small molecules which exhibit a property of interest remains a significant challenge. We introduce LambdaZero, a generative active learning approach to search for synthesizable molecules. Powered by deep reinforcement learning, LambdaZero learns to search over the vast space of molecules to discover candidates with a desired property. We apply LambdaZero with molecular docking to design novel small molecules that inhibit the enzyme soluble Epoxide Hydrolase 2 (sEH), while enforcing constraints on synthesizability and drug-likeliness. LambdaZero provides an exponential speedup in terms of the number of calls to the expensive molecular docking oracle, and LambdaZero de novo designed molecules reach docking scores that would otherwise require the virtual screening of a hundred billion molecules. Importantly, LambdaZero discovers novel scaffolds of synthesizable, drug-like inhibitors for sEH. In in vitro experimental validation, a series of ligands from a generated quinazoline-based scaffold were synthesized, and the lead inhibitor N-(4,6-di(pyrrolidin-1-yl)quinazolin-2-yl)-N-methylbenzamide (UM0152893) displayed sub-micromolar enzyme inhibition of sEH.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2024/05/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2024/05/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html"><img class="next-cover" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">🎨 Stable Diffusion 提示词指南书</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">人工智能</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/05/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-05-07)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2024-05-07)"/></a><div class="content"><a class="title" href="/2024/05/14/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-05-07)">Arxiv每日速递(2024-05-07)</a><time datetime="2024-05-14T07:22:26.423Z" title="发表于 2024-05-14 15:22:26">2024-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书"><img src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="🎨 Stable Diffusion 提示词指南书"/></a><div class="content"><a class="title" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书">🎨 Stable Diffusion 提示词指南书</a><time datetime="2024-02-03T06:57:45.000Z" title="发表于 2024-02-03 14:57:45">2024-02-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer语言模型的位置编码与长度外推"/></a><div class="content"><a class="title" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推">Transformer语言模型的位置编码与长度外推</a><time datetime="2023-10-22T14:55:45.000Z" title="发表于 2023-10-22 22:55:45">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (8)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-10-22</span><a class="blog-slider__title" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">Transformer语言模型的位置编码与长度外推</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt=""><img width="48" height="48" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-02-03</span><a class="blog-slider__title" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">🎨 Stable Diffusion 提示词指南书</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>