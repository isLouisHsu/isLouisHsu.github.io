<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2024-06-06) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新524篇论文，其中：  自然语言处理93篇 信息检索20篇 计算机视觉106篇  自然语言处理    1. 【2406.02543】o Believe or Not to Believe Your LLM   链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2024-06-06)">
<meta property="og:url" content="http://louishsu.xyz/2024/06/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新524篇论文，其中：  自然语言处理93篇 信息检索20篇 计算机视觉106篇  自然语言处理    1. 【2406.02543】o Believe or Not to Believe Your LLM   链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2024-06-06T00:55:18.334Z">
<meta property="article:modified_time" content="2024-06-06T00:57:01.026Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2024/06/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-06-06 08:57:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2024-06-06)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-06T00:55:18.334Z" title="发表于 2024-06-06 08:55:18">2024-06-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-06T00:57:01.026Z" title="更新于 2024-06-06 08:57:01">2024-06-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">58.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>352分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/06/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。</p>
<h1>统计</h1>
<p>今日共更新<strong>524</strong>篇论文，其中：</p>
<ul>
<li>自然语言处理<strong>93</strong>篇</li>
<li>信息检索<strong>20</strong>篇</li>
<li>计算机视觉<strong>106</strong>篇</li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>【2406.02543】o Believe or Not to Believe Your LLM</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02543">https://arxiv.org/abs/2406.02543</a></p>
  <p><b>作者</b>：Yasin Abbasi Yadkori,Ilja Kuzborskij,András György,Csaba Szepesvári</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, explore uncertainty quantification, goal to identify, epistemic uncertainty, explore uncertainty</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2406.02539】Parrot: Multilingual Visual Instruction Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02539">https://arxiv.org/abs/2406.02539</a></p>
  <p><b>作者</b>：Hai-Long Sun,Da-Wei Zhou,Yang Li,Shiyin Lu,Chao Yi,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,De-Chuan Zhan,Han-Jia Ye</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, artificial general intelligence, Language Models, Multimodal Large</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V has marked a significant step towards artificial general intelligence. Existing methods mainly focus on aligning vision encoders with LLMs through supervised fine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs' inherent ability to react to multiple languages progressively deteriorate as the training process evolves. We empirically find that the imbalanced SFT datasets, primarily composed of English-centric image-text pairs, lead to significantly reduced performance in non-English languages. This is due to the failure of aligning the vision encoder and LLM with multilingual tokens during the SFT process. In this paper, we introduce Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level. Parrot makes the visual tokens condition on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens. Specifically, to enhance non-English visual tokens alignment, we compute the cross-attention using the initial visual features and textual embeddings, the result of which is then fed into the MoE router to select the most relevant experts. The selected experts subsequently convert the initial visual tokens into language-specific visual tokens. Moreover, considering the current lack of benchmarks for evaluating multilingual capabilities within the field, we collect and make available a Massive Multilingual Multimodal Benchmark which includes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our method not only demonstrates state-of-the-art performance on multilingual MMBench and MMMB, but also excels across a broad range of multimodal tasks. Both the source code and the training dataset of Parrot will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2406.02537】opViewRS: Vision-Language Models as Top-View Spatial Reasoners</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02537">https://arxiv.org/abs/2406.02537</a></p>
  <p><b>作者</b>：Chengzu Li,Caiqi Zhang,Han Zhou,Nigel Collier,Anna Korhonen,Ivan Vulić</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Top-view perspective denotes, large Vision-Language Models, perspective denotes, denotes a typical, vital for localization</p>
  <p><b>备注</b>： 9 pages, 3 figures, 3 tables (21 pages, 4 figures, 15 tables including references and appendices)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored. In this work, we thus study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2406.02536】Mitigate Position Bias in Large Language Models via Scaling a Single Dimension</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02536">https://arxiv.org/abs/2406.02536</a></p>
  <p><b>作者</b>：Yijiong Yu,Huiqiang Jiang,Xufang Luo,Qianhui Wu,Chin-Yew Lin,Dongsheng Li,Yuqing Yang,Yongfeng Huang,Lili Qiu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, robust generative abilities, excellent generalization capabilities, real-world scenarios due</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as "lost in the middle", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2406.02532】SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02532">https://arxiv.org/abs/2406.02532</a></p>
  <p><b>作者</b>：Ruslan Svirschevski,Avner May,Zhuoming Chen,Beidi Chen,Zhihao Jia,Max Ryabinin</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：gain widespread adoption, language models gain, models gain widespread, large language models, widespread adoption</p>
  <p><b>备注</b>： preprint. arXiv admin note: text overlap with [arXiv:2312.17238](https://arxiv.org/abs/2312.17238) by other authors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As large language models gain widespread adoption, running them efficiently becomes crucial. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models (50B+ parameters) and must offload them to RAM or SSD. When running with offloaded parameters, the inference engine can process batches of hundreds or thousands of tokens at the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families. It utilizes the high spikiness of the token probabilities distribution in modern LLMs and a high degree of alignment between model output probabilities. SpecExec takes the most probable tokens continuation from the draft model to build a "cache" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens per second with 16-bit weights.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2406.02528】Scalable MatMul-free Language Modeling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02528">https://arxiv.org/abs/2406.02528</a></p>
  <p><b>作者</b>：Rui-Jie Zhu,Yu Zhang,Ethan Sifferman,Tyler Sheaves,Yiqiao Wang,Dustin Richmond,Peng Zhou,Jason K. Eshraghian</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Matrix multiplication, large language models, typically dominates, large language, Matrix</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2406.02524】CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02524">https://arxiv.org/abs/2406.02524</a></p>
  <p><b>作者</b>：Maciej Besta,Lorenzo Paleari,Ales Kubicek,Piotr Nyczyk,Robert Gerstenberger,Patrick Iff,Tomasz Lehmann,Hubert Niewiadomski,Torsten Hoefler</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, intricate open-ended tasks, revolutionizing various domains</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) are revolutionizing various domains, yet verifying their answers remains a significant challenge, especially for intricate open-ended tasks such as consolidation, summarization, and extraction of knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and simple LLM verification approach. CheckEmbed is driven by a straightforward yet powerful idea: in order to compare LLM solutions to one another or to the ground-truth, compare their corresponding answer-level embeddings obtained with a model such as GPT Text Embedding Large. This reduces a complex textual answer to a single embedding, facilitating straightforward, fast, and meaningful verification. We develop a comprehensive verification pipeline implementing the CheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for assessing the truthfulness of the LLM answers, such as embedding heatmaps and their summaries. We show how to use these metrics for deploying practical engines that decide whether an LLM answer is satisfactory or not. We apply the pipeline to real-world document analysis tasks, including term extraction and document summarization, showcasing significant improvements in accuracy, cost-effectiveness, and runtime performance compared to existing token-, sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2406.02517】Deterministic Reversible Data Augmentation for Neural Machine Translation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02517">https://arxiv.org/abs/2406.02517</a></p>
  <p><b>作者</b>：Jiashu Yao,Heyan Huang,Zeming Liu,Yuhang Guo</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：introduce semantic inconsistency, subword sampling procedures, effective data augmentation, data augmentation method, Reversible Data Augmentation</p>
  <p><b>备注</b>： Findings of ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Data augmentation is an effective way to diversify corpora in machine translation, but previous methods may introduce semantic inconsistency between original and augmented data because of irreversible operations and random subword sampling procedures. To generate both symbolically diverse and semantically consistent augmentation data, we propose Deterministic Reversible Data Augmentation (DRDA), a simple but effective data augmentation method for neural machine translation. DRDA adopts deterministic segmentations and reversible operations to generate multi-granularity subword representations and pulls them closer together with multi-view techniques. With no extra corpora or model changes required, DRDA outperforms strong baselines on several translation tasks with a clear margin (up to 4.3 BLEU gain over Transformer) and exhibits good robustness in noisy, low-resource, and cross-domain datasets.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2406.02481】Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02481">https://arxiv.org/abs/2406.02481</a></p>
  <p><b>作者</b>：Jakub Hoscilowicz,Pawel Popiolek,Jan Rudkowski,Jedrzej Bieniasz,Artur Janicki</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Cryptography and Security (cs.CR)</p>
  <p><b>关键词</b>：Unconditional Token Forcing, LLM, artificially embed hidden, Unconditional Token, Token Forcing</p>
  <p><b>备注</b>： Work in progress. Code is available at [this https URL](https://github.com/j-hoscilowic/zurek-stegano) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the help of simple fine-tuning, one can artificially embed hidden text into large language models (LLMs). This text is revealed only when triggered by a specific query to the LLM. Two primary applications are LLM fingerprinting and steganography. In the context of LLM fingerprinting, a unique text identifier (fingerprint) is embedded within the model to verify licensing compliance. In the context of steganography, the LLM serves as a carrier for hidden messages that can be disclosed through a designated trigger.
Our work demonstrates that embedding hidden text in the LLM via fine-tuning, though seemingly secure due to the vast number of potential triggers (any sequence of characters or tokens could serve as a trigger), is susceptible to extraction through analysis of the LLM's output decoding process. We propose a novel approach to extraction called Unconditional Token Forcing. It is premised on the hypothesis that iteratively feeding each token from the LLM's vocabulary into the model should reveal sequences with abnormally high token probabilities, indicating potential embedded text candidates. Additionally, our experiments show that when the first token of a hidden fingerprint is used as an input, the LLM not only produces an output sequence with high token probabilities, but also repetitively generates the fingerprint itself. We also present a method to hide text in such a way that it is resistant to Unconditional Token Forcing, which we named Unconditional Token Forcing Confusion.
</p><p>Comments:<br>
Work in progress. Code is available at this https URL</p>
<p>Subjects:</p>
<p>Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>)</p>
<p>Cite as:<br>
arXiv:2406.02481 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]</p>
<p>(or<br>
arXiv:2406.02481v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2406.02481">https://doi.org/10.48550/arXiv.2406.02481</a></p>
<p>Focus to learn more</p>
<pre><code>            arXiv-issued DOI via DataCite&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>10. <b>【2406.02472】Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02472">https://arxiv.org/abs/2406.02472</a></p>
  <p><b>作者</b>：Zhihan Zhang,Yixin Cao,Chenchen Ye,Yunshan Ma,Lizi Liao,Tat-Seng Chua</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：complex events, Temporal Complex Event, digital landscape, landscape is rapidly, rapidly evolving</p>
  <p><b>备注</b>： Accepted to ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2406.02469】Landscape-Aware Growing: The Power of a Little LAG</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02469">https://arxiv.org/abs/2406.02469</a></p>
  <p><b>作者</b>：Stefani Karp,Nikunj Saunshi,Sobhan Miryoosefi,Sashank J. Reddi,Sanjiv Kumar</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：efficient pretraining paradigms, training Transformer-based models, Transformer-based models, training Transformer-based, increasing interest</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, there has been increasing interest in efficient pretraining paradigms for training Transformer-based models. Several recent approaches use smaller models to initialize larger models in order to save computation (e.g., stacking and fusion). In this work, we study the fundamental question of how to select the best growing strategy from a given pool of growing strategies. Prior works have extensively focused on loss- and/or function-preserving behavior at initialization or simply performance at the end of training. Instead, we identify that behavior at initialization can be misleading as a predictor of final performance and present an alternative perspective based on early training dynamics, which we call "landscape-aware growing (LAG)". We perform extensive analysis of correlation of the final performance with performance in the initial steps of training and find early and more accurate predictions of the optimal growing strategy (i.e., with only a small "lag" after initialization). This perspective also motivates an adaptive strategy for gradual stacking.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2406.02449】Representations as Language: An Information-Theoretic Framework for Interpretability</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02449">https://arxiv.org/abs/2406.02449</a></p>
  <p><b>作者</b>：Henry Conklin,Kenny Smith</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large scale neural, Large scale, neural models show, models show impressive, scale neural models</p>
  <p><b>备注</b>： 6 pages, 3 Figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large scale neural models show impressive performance across a wide array of linguistic tasks. Despite this they remain, largely, black-boxes - inducing vector-representations of their input that prove difficult to interpret. This limits our ability to understand what they learn, and when the learn it, or describe what kinds of representations generalise well out of distribution. To address this we introduce a novel approach to interpretability that looks at the mapping a model learns from sentences to representations as a kind of language in its own right. In doing so we introduce a set of information-theoretic measures that quantify how structured a model's representations are with respect to its input, and when during training that structure arises. Our measures are fast to compute, grounded in linguistic theory, and can predict which models will generalise best based on their representations. We use these measures to describe two distinct phases of training a transformer: an initial phase of in-distribution learning which reduces task loss, then a second stage where representations becoming robust to noise. Generalisation performance begins to increase during this second phase, drawing a link between generalisation and robustness to noise. Finally we look at how model size affects the structure of the representational space, showing that larger models ultimately compress their representations more than their smaller counterparts.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2406.02396】he Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02396">https://arxiv.org/abs/2406.02396</a></p>
  <p><b>作者</b>：Kenneth Enevoldsen,Márton Kardos,Niklas Muennighoff,Kristoffer Laigaard Nielbo</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：English text embeddings, English text, Scandinavian Embedding Benchmark, transitioned from evaluating, evaluating a handful</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The evaluation of English text embeddings has transitioned from evaluating a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB. However, this is not the case for multilingual text embeddings due to a lack of available benchmarks. To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB). SEB is a comprehensive framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26 models, uncovering significant performance disparities between public and commercial solutions not previously captured by MTEB. We open-source SEB and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2406.02394】Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02394">https://arxiv.org/abs/2406.02394</a></p>
  <p><b>作者</b>：Maxime Griot,Jean Vanderdonckt,Demet Yuksel,Coralie Hemptinne</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：ChatGPT demonstrate significant, demonstrate significant potential, Large Language Models, Large Language, ChatGPT demonstrate</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) like ChatGPT demonstrate significant potential in the medical field, often evaluated using multiple-choice questions (MCQs) similar to those found on the USMLE. Despite their prevalence in medical education, MCQs have limitations that might be exacerbated when assessing LLMs. To evaluate the effectiveness of MCQs in assessing the performance of LLMs, we developed a fictional medical benchmark focused on a non-existent gland, the Glianorex. This approach allowed us to isolate the knowledge of the LLM from its test-taking abilities. We used GPT-4 to generate a comprehensive textbook on the Glianorex in both English and French and developed corresponding multiple-choice questions in both languages. We evaluated various open-source, proprietary, and domain-specific LLMs using these questions in a zero-shot setting. The models achieved average scores around 67%, with minor performance differences between larger and smaller models. Performance was slightly higher in English than in French. Fine-tuned medical models showed some improvement over their base versions in English but not in French. The uniformly high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs' clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills. This study underscores the need for more robust evaluation methods to better assess the true capabilities of LLMs in medical contexts.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2406.02378】On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02378">https://arxiv.org/abs/2406.02378</a></p>
  <p><b>作者</b>：Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Kristen Johnson,Jiliang Tang,Rongrong Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, self-correction, self-correction capability</p>
  <p><b>备注</b>： 22 pages, 7 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) can improve their responses when instructed to do so, a capability known as self-correction. When these instructions lack specific details about the issues in the response, this is referred to as leveraging the intrinsic self-correction capability. The empirical success of self-correction can be found in various applications, e.g., text detoxification and social bias mitigation. However, leveraging this self-correction capability may not always be effective, as it has the potential to revise an initially correct response into an incorrect one. In this paper, we endeavor to understand how and why leveraging the self-correction capability is effective. We identify that appropriate instructions can guide LLMs to a convergence state, wherein additional self-correction steps do not yield further performance improvements. We empirically demonstrate that model uncertainty and activated latent concepts jointly characterize the effectiveness of self-correction. Furthermore, we provide a mathematical formulation indicating that the activated latent concept drives the convergence of the model uncertainty and self-correction performance. Our analysis can also be generalized to the self-correction behaviors observed in Vision-Language Models (VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from our principle in terms of selecting effective fine-tuning samples. Such initial success demonstrates the potential extensibility for better instruction tuning and safety alignment.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2406.02377】XRec: Large Language Models for Explainable Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02377">https://arxiv.org/abs/2406.02377</a></p>
  <p><b>作者</b>：Qiyao Ma,Xubin Ren,Chao Huang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：navigate information overload, providing personalized recommendations, personalized recommendations aligned, users navigate information, Recommender systems</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recommender systems help users navigate information overload by providing personalized recommendations aligned with their preferences. Collaborative Filtering (CF) is a widely adopted approach, but while advanced techniques like graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced CF models for better user representations, they often lack the ability to provide explanations for the recommended items. Explainable recommendations aim to address this gap by offering transparency and insights into the recommendation decision-making process, enhancing users' understanding. This work leverages the language capabilities of Large Language Models (LLMs) to push the boundaries of explainable recommender systems. We introduce a model-agnostic framework called XRec, which enables LLMs to provide comprehensive explanations for user behaviors in recommender systems. By integrating collaborative signals and designing a lightweight collaborative adaptor, the framework empowers LLMs to understand complex patterns in user-item interactions and gain a deeper understanding of user preferences. Our extensive experiments demonstrate the effectiveness of XRec, showcasing its ability to generate comprehensive and meaningful explanations that outperform baseline approaches in explainable recommender systems. We open-source our model implementation at this https URL.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2406.02376】Retaining Key Information under High Compression Ratios: Query-Guided Compressor for LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02376">https://arxiv.org/abs/2406.02376</a></p>
  <p><b>作者</b>：Zhiwei Cao,Qian Cao,Yu Lu,Ningxin Peng,Luyang Huang,Shanbo Cheng,Jinsong Su</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, Language, popularity of Large</p>
  <p><b>备注</b>： Accepted to ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression ratios increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression ratios. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can consistently perform well even at high compression ratios, which also offers significant benefits in terms of inference cost and throughput.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2406.02368】Large Language Models Make Sample-Efficient Recommender Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02368">https://arxiv.org/abs/2406.02368</a></p>
  <p><b>作者</b>：Jianghao Lin,Xinyi Dai,Rong Shan,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：achieved remarkable progress, natural language processing, resembles human language, demonstrating remarkable abilities, recommender systems</p>
  <p><b>备注</b>： Accepted by Frontier of Computer Science</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have achieved remarkable progress in the field of natural language processing (NLP), demonstrating remarkable abilities in producing text that resembles human language for various tasks. This opens up new opportunities for employing them in recommender systems (RSs). In this paper, we specifically examine the sample efficiency of LLM-enhanced recommender systems, which pertains to the model's capacity to attain superior performance with a limited quantity of training data. Conventional recommendation models (CRMs) often need a large amount of training data because of the sparsity of features and interactions. Hence, we propose and verify our core viewpoint: Large Language Models Make Sample-Efficient Recommender Systems. We propose a simple yet effective framework (i.e., Laser) to validate the viewpoint from two aspects: (1) LLMs themselves are sample-efficient recommenders; and (2) LLMs, as feature generators and encoders, make CRMs more sample-efficient. Extensive experiments on two public datasets show that Laser requires only a small fraction of training samples to match or even surpass CRMs that are trained on the entire training set, demonstrating superior sample efficiency.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2406.02356】Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02356">https://arxiv.org/abs/2406.02356</a></p>
  <p><b>作者</b>：Andrew Gambardella,Yusuke Iwasawa,Yutaka Matsuo</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, perform arithmetic tasks, language models, practical debate, large language</p>
  <p><b>备注</b>： In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The ability (and inability) of large language models (LLMs) to perform arithmetic tasks has been the subject of much theoretical and practical debate. We show that LLMs are frequently able to correctly and confidently predict the first digit of n-digit by m-digit multiplication tasks without using chain of thought reasoning, despite these tasks require compounding operations to solve. Simultaneously, LLMs in practice often fail to correctly or confidently predict the last digit of an n-digit by m-digit multiplication, a task equivalent to 1-digit by 1-digit multiplication which can be easily learned or memorized. We show that the latter task can be solved more robustly when the LLM is conditioned on all of the correct higher-order digits, which on average increases the confidence of the correct last digit on 5-digit by 5-digit multiplication tasks using Llama 2-13B by over 230% (0.13 to 0.43) and Mistral-7B by 150% (0.22 to 0.55).</p>
  </details>
</details>
<details>
  <summary>20. <b>【2406.02350】LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02350">https://arxiv.org/abs/2406.02350</a></p>
  <p><b>作者</b>：Maojun Sun</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：shown amazing capabilities, Extended Classification Integration, memorization and present, Classification Integration, shown amazing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have shown amazing capabilities in knowledge memorization and present. However, when it comes to domain-specific knowledge and downstream tasks like medical, general LLMs are often unable to give precise answers. In addition, when people want LLMs to answer classification questions, they usually go through instruction tuning first, however, LLMs do not always give a direct index of the categorization after instruction tuning. In this paper, we proposed LlamaCare, a fine-tuned medical language model, and Extended Classification Integration(ECI), a module to handle classification problems of LLMs. Our contributions are : (i) We fine-tuned a large language model of medical knowledge with very low carbon emissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We solved the problem of redundant categorical answers and improved the performance of LLMs by proposing a new module called Extended Classification Integration. (iii) We released our processed data for one-shot and few-shot training for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method achieves a close effect with the state-of-the-art model in benchmarks while costing lower GPU resources compared to LLMs with the same quantity of parameters. Our models, codes, and datasets can be found in this https URL</p>
  </details>
</details>
<details>
  <summary>21. <b>【2406.02338】Linguistic Fingerprint in Transformer Models: How Language Variation Influences Parameter Selection in Irony Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02338">https://arxiv.org/abs/2406.02338</a></p>
  <p><b>作者</b>：Michele Mastromattei,Fabio Massimo Zanzotto</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：transformer model architectures, sentiment analysis, paper explores, explores the correlation, analysis and transformer</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper explores the correlation between linguistic diversity, sentiment analysis and transformer model architectures. We aim to investigate how different English variations impact transformer-based models for irony detection. To conduct our study, we used the EPIC corpus to extract five diverse English variation-specific datasets and applied the KEN pruning algorithm on five different architectures. Our results reveal several similarities between optimal subnetworks, which provide insights into the linguistic variations that share strong resemblances and those that exhibit greater dissimilarities. We discovered that optimal subnetworks across models share at least 60% of their parameters, emphasizing the significance of parameter values in capturing and interpreting linguistic variations. This study highlights the inherent structural similarities between models trained on different variants of the same language and also the critical role of parameter values in capturing these nuances.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2406.02335】Probing the Category of Verbal Aspect in Transformer Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02335">https://arxiv.org/abs/2406.02335</a></p>
  <p><b>作者</b>：Anisia Katinskaia,Roman Yangarber</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：pretrained language models, investigate how pretrained, grammatical category, category of verbal, aspect</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We investigate how pretrained language models (PLM) encode the grammatical category of verbal aspect in Russian. Encoding of aspect in transformer LMs has not been studied previously in any language. A particular challenge is posed by "alternative contexts": where either the perfective or the imperfective aspect is suitable grammatically and semantically. We perform probing using BERT and RoBERTa on alternative and non-alternative contexts. First, we assess the models' performance on aspect prediction, via behavioral probing. Next, we examine the models' performance when their contextual representations are substituted with counterfactual representations, via causal probing. These counterfactuals alter the value of the "boundedness" feature--a semantic feature, which characterizes the action in the context. Experiments show that BERT and RoBERTa do encode aspect--mostly in their final layers. The counterfactual interventions affect perfective and imperfective in opposite ways, which is consistent with grammar: perfective is positively affected by adding the meaning of boundedness, and vice versa. The practical implications of our probing results are that fine-tuning only the last layers of BERT on predicting aspect is faster and more effective than fine-tuning the whole model. The model has high predictive uncertainty about aspect in alternative contexts, which tend to lack explicit hints about the boundedness of the described action.</p>
  </details>
</details>
<details>
  <summary>23. <b>【2406.02332】Extended Mind Transformers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02332">https://arxiv.org/abs/2406.02332</a></p>
  <p><b>作者</b>：Phoebe Klett,Thomas Ahle</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Pre-trained language models, long inputs quickly, Pre-trained language, demonstrate general intelligence, language models demonstrate</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Pre-trained language models demonstrate general intelligence and common sense, but long inputs quickly become a bottleneck for memorizing information at inference time. We resurface a simple method, Memorizing Transformers (Wu et al., 2022), that gives the model access to a bank of pre-computed memories. We show that it is possible to fix many of the shortcomings of the original method, such as the need for fine-tuning, by critically assessing how positional encodings should be updated for the keys and values retrieved. This intuitive method uses the model's own key/query system to select and attend to the most relevant memories at each generation step, rather than using external embeddings. We demonstrate the importance of external information being retrieved in a majority of decoder layers, contrary to previous work. We open source a new counterfactual long-range retrieval benchmark, and show that Extended Mind Transformers outperform today's state of the art by 6% on average.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2406.02331】ranslation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02331">https://arxiv.org/abs/2406.02331</a></p>
  <p><b>作者</b>：ChaeHun Park,Koanho Lee,Hyesu Lim,Jaeseok Kim,Junmo Park,Yu-Jung Heo,Du-Seong Chang,Jaegul Choo</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：visual question answering, reliable visual question, Building a reliable, question answering, challenging problem</p>
  <p><b>备注</b>： ACL 2024 Findings Accepted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Building a reliable visual question answering~(VQA) system across different languages is a challenging problem, primarily due to the lack of abundant samples for training. To address this challenge, recent studies have employed machine translation systems for the cross-lingual VQA task. This involves translating the evaluation samples into a source language (usually English) and using monolingual models (i.e., translate-test). However, our analysis reveals that translated texts contain unique characteristics distinct from human-written ones, referred to as translation artifacts. We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes. In light of this, we present a simple data augmentation strategy that can alleviate the adverse impacts of translation artifacts.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2406.02329】On Affine Homotopy between Language Encoders</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02329">https://arxiv.org/abs/2406.02329</a></p>
  <p><b>作者</b>：Robin SM Chan,Reda Boumasmoud,Anej Svete,Yuxin Ren,Qipeng Guo,Zhijing Jin,Shauli Ravfogel,Mrinmaya Sachan,Bernhard Schölkopf,Mennatallah El-Assady,Ryan Cotterell</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：NLP tasks, functions that represent, text as vectors, represent text, integral component</p>
  <p><b>备注</b>： 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Pre-trained language encoders -- functions that represent text as vectors -- are an integral component of many NLP tasks. We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that a faithful measure of similarity needs to be \emph{intrinsic}, that is, task-independent, yet still be informative of \emph{extrinsic} similarity -- the performance on downstream tasks. It is common to consider two encoders similar if they are \emph{homotopic}, i.e., if they can be aligned through some transformation. In this spirit, we study the properties of \emph{affine} alignment of language encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2406.02325】chnical Language Processing for Telecommunications Specifications</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02325">https://arxiv.org/abs/2406.02325</a></p>
  <p><b>作者</b>：Felipe A. Rodriguez Y.</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, Generative Pre-Trained Transformer, real-world technical documentation</p>
  <p><b>备注</b>： Still not published</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) are continuously being applied in a more diverse set of contexts. At their current state, however, even state-of-the-art LLMs such as Generative Pre-Trained Transformer 4 (GTP-4) have challenges when extracting information from real-world technical documentation without a heavy preprocessing. One such area with real-world technical documentation is telecommunications engineering, which could greatly benefit from domain-specific LLMs. The unique format and overall structure of telecommunications internal specifications differs greatly from standard English and thus it is evident that the application of out-of-the-box Natural Language Processing (NLP) tools is not a viable option. In this article, we outline the limitations of out-of-the-box NLP tools for processing technical information generated by telecommunications experts, and expand the concept of Technical Language Processing (TLP) to the telecommunication domain. Additionally, we explore the effect of domain-specific LLMs in the work of Specification Engineers, emphasizing the potential benefits of adopting domain-specific LLMs to speed up the training of experts in different telecommunications fields.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2406.02301】mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02301">https://arxiv.org/abs/2406.02301</a></p>
  <p><b>作者</b>：Huiyuan Lai,Malvina Nissim</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large language models, Large language, downstream tasks, recently emerged, powerful technique</p>
  <p><b>备注</b>： Accepted to ACL 2024 main</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) with Chain-of-thought (CoT) have recently emerged as a powerful technique for eliciting reasoning to improve various downstream tasks. As most research mainly focuses on English, with few explorations in a multilingual context, the question of how reliable this reasoning capability is in different languages is still open. To address it directly, we study multilingual reasoning consistency across multiple languages, using popular open-source LLMs. First, we compile the first large-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven diverse languages. Then, we introduce multilingual CoT instruction tuning to boost reasoning capability across languages, thereby improving model consistency. While existing LLMs show substantial variation across the languages we consider, and especially low performance for lesser resourced languages, our 7B parameter model mCoT achieves impressive consistency across languages, and superior or comparable performance to close- and open-source models even of much larger sizes.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2406.02267】Prompting Large Language Models with Human Error Markings for Self-Correcting Machine Translation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02267">https://arxiv.org/abs/2406.02267</a></p>
  <p><b>作者</b>：Nathaniel Berger,Stefan Riezler,Miriam Exel,Matthias Huck</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：general domain texts, large language models, unpaired language data, enhance term translation, term translation quality</p>
  <p><b>备注</b>： To appear at The 25th Annual Conference of the European Association for Machine Translation (EAMT 2024)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:While large language models (LLMs) pre-trained on massive amounts of unpaired language data have reached the state-of-the-art in machine translation (MT) of general domain texts, post-editing (PE) is still required to correct errors and to enhance term translation quality in specialized domains. In this paper we present a pilot study of enhancing translation memories (TM) produced by PE (source segments, machine translations, and reference translations, henceforth called PE-TM) for the needs of correct and consistent term translation in technical domains.
We investigate a light-weight two-step scenario where, at inference time, a human translator marks errors in the first translation step, and in a second step a few similar examples are extracted from the PE-TM to prompt an LLM. Our experiment shows that the additional effort of augmenting translations with human error markings guides the LLM to focus on a correction of the marked errors, yielding consistent improvements over automatic PE (APE) and MT from scratch.
</p><p>Comments:<br>
To appear at The 25th Annual Conference of the European Association for Machine Translation (EAMT 2024)</p>
<p>Subjects:</p>
<p>Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)</p>
<p>Cite as:<br>
arXiv:2406.02267 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]</p>
<p>(or<br>
arXiv:2406.02267v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2406.02267">https://doi.org/10.48550/arXiv.2406.02267</a></p>
<p>Focus to learn more</p>
<pre><code>            arXiv-issued DOI via DataCite&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>29. <b>【2406.02266】Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning Compressor</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02266">https://arxiv.org/abs/2406.02266</a></p>
  <p><b>作者</b>：Chuankai Xu,Dongming Zhao,Bo Wang,Hanwen Xing</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：retrieval-augmented language models, tasks remains challenging, document-based tasks remains, remains challenging, language model responses</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite the prevalence of retrieval-augmented language models (RALMs), the seamless integration of these models with retrieval mechanisms to enhance performance in document-based tasks remains challenging. While some post-retrieval processing Retrieval-Augmented Generation (RAG) methods have achieved success, most still lack the ability to distinguish pertinent from extraneous information, leading to potential inconsistencies and reduced precision in the generated output, which subsequently affects the truthfulness of the language model's responses. To address these limitations, this work proposes a novel two-stage consistency learning approach for retrieved information compression in retrieval-augmented language models to enhance performance. By incorporating consistency learning, the aim is to generate summaries that maintain coherence and alignment with the intended semantic representations of a teacher model while improving faithfulness to the original retrieved documents. The proposed method is empirically validated across multiple datasets, demonstrating notable enhancements in precision and efficiency for question-answering tasks. It outperforms existing baselines and showcases the synergistic effects of combining contrastive and consistency learning paradigms within the retrieval-augmented generation framework.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2406.02265】Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02265">https://arxiv.org/abs/2406.02265</a></p>
  <p><b>作者</b>：Wenyan Li,Jiaang Li,Rita Ramos,Raphael Tang,Desmond Elliott</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：strong domain-transfer capabilities, Recent advancements, image captioning highlight, retrieving related captions, domain-transfer capabilities</p>
  <p><b>备注</b>： 9 pages, long paper at ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in retrieval-augmented models for image captioning highlight the significance of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice. Retrieved information can sometimes mislead the model generation, negatively impacting performance. In this paper, we analyze the robustness of the SmallCap retrieval-augmented captioning model. Our analysis shows that SmallCap is sensitive to tokens that appear in the majority of the retrieved captions, and integrated gradients attribution shows that those tokens are likely copied into the final caption. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This reduces the probability that the model learns to copy majority tokens and improves both in-domain and cross-domain performance effectively.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2406.02251】Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02251">https://arxiv.org/abs/2406.02251</a></p>
  <p><b>作者</b>：Lukas Christ,Shahin Amiriparian,Manuel Milling,Ilhan Aslan,Björn W. Schuller</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Telling stories, integral part, part of human, human communication, influence the affective</p>
  <p><b>备注</b>： Accepted to ACL 2024 Findings. arXiv admin note: text overlap with [arXiv:2212.11382](https://arxiv.org/abs/2212.11382) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Telling stories is an integral part of human communication which can evoke emotions and influence the affective states of the audience. Automatically modeling emotional trajectories in stories has thus attracted considerable scholarly interest. However, as most existing works have been limited to unsupervised dictionary-based approaches, there is no benchmark for this task. We address this gap by introducing continuous valence and arousal labels for an existing dataset of children's stories originally annotated with discrete emotion categories. We collect additional annotations for this data and map the categorical labels to the continuous valence and arousal space. For predicting the thus obtained emotionality signals, we fine-tune a DeBERTa model and improve upon this baseline via a weakly supervised learning approach. The best configuration achieves a Concordance Correlation Coefficient (CCC) of $.8221$ for valence and $.7125$ for arousal on the test set, demonstrating the efficacy of our proposed approach. A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story. In addition, we uncover the weaknesses of our approach by investigating examples that prove to be difficult to predict.</p>
  </details>
</details>
<details>
  <summary>32. <b>【2406.02245】Description Boosting for Zero-Shot Entity and Relation Classification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02245">https://arxiv.org/abs/2406.02245</a></p>
  <p><b>作者</b>：Gabriele Picco,Leopold Fuchs,Marcos Martínez Galindo,Alberto Purpura,Vanessa López,Hoang Thanh Lam</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：annotate input text, input text data, leverage available external, external information, information of unseen</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Zero-shot entity and relation classification models leverage available external information of unseen classes -- e.g., textual descriptions -- to annotate input text data. Thanks to the minimum data requirement, Zero-Shot Learning (ZSL) methods have high value in practice, especially in applications where labeled data is scarce. Even though recent research in ZSL has demonstrated significant results, our analysis reveals that those methods are sensitive to provided textual descriptions of entities (or relations). Even a minor modification of descriptions can lead to a change in the decision boundary between entity (or relation) classes. In this paper, we formally define the problem of identifying effective descriptions for zero shot inference. We propose a strategy for generating variations of an initial description, a heuristic for ranking them and an ensemble method capable of boosting the predictions of zero-shot models through description enhancement. Empirical results on four different entity and relation classification datasets show that our proposed method outperform existing approaches and achieve new SOTA results on these datasets under the ZSL settings. The source code of the proposed solutions and the evaluation framework are open-sourced.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2406.02237】Self-Modifying State Modeling for Simultaneous Machine Translation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02237">https://arxiv.org/abs/2406.02237</a></p>
  <p><b>作者</b>：Donglei Yu,Xiaomian Kang,Yuchen Liu,Yu Zhou,Chengqing Zong</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：generates target outputs, Simultaneous Machine Translation, receiving stream source, Simultaneous Machine, generates target</p>
  <p><b>备注</b>： Accept to ACL 2024 main conference. 15 pages, 13 figures, 9 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Simultaneous Machine Translation (SiMT) generates target outputs while receiving stream source inputs and requires a read/write policy to decide whether to wait for the next source token or generate a new target token, whose decisions form a \textit{decision path}. Existing SiMT methods, which learn the policy by exploring various decision paths in training, face inherent limitations. These methods not only fail to precisely optimize the policy due to the inability to accurately assess the individual impact of each decision on SiMT performance, but also cannot sufficiently explore all potential paths because of their vast number. Besides, building decision paths requires unidirectional encoders to simulate streaming source inputs, which impairs the translation quality of SiMT models. To solve these issues, we propose \textbf{S}elf-\textbf{M}odifying \textbf{S}tate \textbf{M}odeling (SM$^2$), a novel training paradigm for SiMT task. Without building decision paths, SM$^2$ individually optimizes decisions at each state during training. To precisely optimize the policy, SM$^2$ introduces Self-Modifying process to independently assess and adjust decisions at each state. For sufficient exploration, SM$^2$ proposes Prefix Sampling to efficiently traverse all potential states. Moreover, SM$^2$ ensures compatibility with bidirectional encoders, thus achieving higher translation quality. Experiments show that SM$^2$ outperforms strong baselines. Furthermore, SM$^2$ allows offline machine translation models to acquire SiMT ability with fine-tuning.</p>
  </details>
</details>
<details>
  <summary>34. <b>【2406.02224】FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02224">https://arxiv.org/abs/2406.02224</a></p>
  <p><b>作者</b>：Tao Fan,Guoqiang Ma,Yan Kang,Hanlin Gu,Lixin Fan,Qiang Yang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Recent research, small language models, language models, locally deployed homogeneous, large language models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, heterogeneous, homogeneous, and one-to-one, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLP text generation tasks. Empirical results demonstrate significant performance improvements in clients' SLMs with the aid of the LLM. Furthermore, the LLM optimized by FedMKT achieves a performance comparable to that achieved through direct fine-tuning based on clients' data, highlighting the effectiveness and adaptability of FedMKT.</p>
  </details>
</details>
<details>
  <summary>35. <b>【2406.02208】Why Only Text: Empowering Vision-and-Language Navigation with Multi-modal Prompts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02208">https://arxiv.org/abs/2406.02208</a></p>
  <p><b>作者</b>：Haodong Hong,Sen Wang,Zi Huang,Qi Wu,Jiajun Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：employ textual instructions, Current, Prompts, employ textual, textual instructions</p>
  <p><b>备注</b>： IJCAI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Current Vision-and-Language Navigation (VLN) tasks mainly employ textual instructions to guide agents. However, being inherently abstract, the same textual instruction can be associated with different visual signals, causing severe ambiguity and limiting the transfer of prior knowledge in the vision domain from the user to the agent. To fill this gap, we propose Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task augmenting traditional VLN by integrating both natural language and images in instructions. VLN-MP not only maintains backward compatibility by effectively handling text-only prompts but also consistently shows advantages with different quantities and relevance of visual prompts. Possible forms of visual prompts include both exact and similar object images, providing adaptability and versatility in diverse navigation scenarios. To evaluate VLN-MP under a unified framework, we implement a new benchmark that offers: (1) a training-free pipeline to transform textual instructions into multi-modal forms with landmark images; (2) diverse datasets with multi-modal instructions for different downstream tasks; (3) a novel module designed to process various image prompts for seamless integration with state-of-the-art VLN models. Extensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show that incorporating visual prompts significantly boosts navigation performance. While maintaining efficiency with text-only prompts, VLN-MP enables agents to navigate in the pre-explore setting and outperform text-based models, showing its broader applicability.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2406.02169】A multilingual dataset for offensive language and hate speech detection for hausa, yoruba and igbo languages</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02169">https://arxiv.org/abs/2406.02169</a></p>
  <p><b>作者</b>：Saminu Mohammad Aliyu,Gregory Maksha Wajiga,Muhammad Murtala</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：effective detection mechanisms, multilingual contexts, offensive language detection, online offensive language, offensive language necessitates</p>
  <p><b>备注</b>： 9 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The proliferation of online offensive language necessitates the development of effective detection mechanisms, especially in multilingual contexts. This study addresses the challenge by developing and introducing novel datasets for offensive language detection in three major Nigerian languages: Hausa, Yoruba, and Igbo. We collected data from Twitter and manually annotated it to create datasets for each of the three languages, using native speakers. We used pre-trained language models to evaluate their efficacy in detecting offensive language in our datasets. The best-performing model achieved an accuracy of 90\%. To further support research in offensive language detection, we plan to make the dataset and our models publicly available.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2406.02166】Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition via Weakly Phonetic Supervision</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02166">https://arxiv.org/abs/2406.02166</a></p>
  <p><b>作者</b>：Saierdaer Yusuyin,Te Ma,Hao Huang,Wenbo Zhao,Zhijian Ou</p>
  <p><b>类目</b>：ound (cs.SD); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：International Phonetic Alphabet, MCL-ASR, supervised pre-training, phonetic, self-supervised pre-training</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pre-training with phonetic or graphemic transcription, and self-supervised pre-training. We find that pre-training with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pre-training with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training this http URL is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. To support reproducibility and promote future research along this direction, we will release the code, models and data for the whole pipeline of Whistle at this https URL upon publication.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2406.02148】Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02148">https://arxiv.org/abs/2406.02148</a></p>
  <p><b>作者</b>：Qingkai Min,Qipeng Guo,Xiangkun Hu,Songfang Huang,Zheng Zhang,Yue Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Cross-document event coreference, involves clustering event, event coreference resolution, clustering event mentions, Cross-document event</p>
  <p><b>备注</b>： Accepted to ACL-24 Main</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events. Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions. However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences. Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks. In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM. The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting. Then, the SLM refines its learning of event representations based on these insights during fine-tuning. Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage. Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2406.02143】Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02143">https://arxiv.org/abs/2406.02143</a></p>
  <p><b>作者</b>：Ruichao Yang,Wei Gao,Jing Ma,Hongzhan Lin,Bo Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Learning multi-task models, poses challenges due, verifying rumors poses, rumors poses challenges, jointly detecting stance</p>
  <p><b>备注</b>： ACL 2024 (Findings)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Learning multi-task models for jointly detecting stance and verifying rumors poses challenges due to the need for training data of stance at post level and rumor veracity at claim level, which are difficult to obtain. To address this issue, we leverage large language models (LLMs) as the foundation annotators for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed as JSDRV. We introduce a novel reinforcement tuning framework to enhance the joint predictive capabilities of LLM-based SD and RV components. Specifically, we devise a policy for selecting LLM-annotated data at the two levels, employing a hybrid reward mechanism to choose high-quality labels for effective LLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the capabilities of LLMs in the joint tasks, not only outperforming state-of-the-art methods but also generalizing to non-LLMs accommodated as task models.</p>
  </details>
</details>
<details>
  <summary>40. <b>【2406.02135】Robust Interaction-based Relevance Modeling for Online E-Commerce and LLM-based Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02135">https://arxiv.org/abs/2406.02135</a></p>
  <p><b>作者</b>：Ben Chen,Huangyu Dai,Xiang Ma,Wen Jiang,Wei Ning</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：items selected closely, selected closely align, Semantic relevance calculation, items selected, selected closely</p>
  <p><b>备注</b>： Accepted by ECML-PKDD'24 as Outstanding Paper. 8 pages, 2 figures, 7 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Semantic relevance calculation is crucial for e-commerce search engines, as it ensures that the items selected closely align with customer intent. Inadequate attention to this aspect can detrimentally affect user experience and engagement. Traditional text-matching techniques are prevalent but often fail to capture the nuances of search intent accurately, so neural networks now have become a preferred solution to processing such complex text matching. Existing methods predominantly employ representation-based architectures, which strike a balance between high traffic capacity and low latency. However, they exhibit significant shortcomings in generalization and robustness when compared to interaction-based architectures. In this work, we introduce a robust interaction-based modeling paradigm to address these shortcomings. It encompasses 1) a dynamic length representation scheme for expedited inference, 2) a professional terms recognition method to identify subjects and core attributes from complex sentence structures, and 3) a contrastive adversarial training protocol to bolster the model's robustness and matching capabilities. Extensive offline evaluations demonstrate the superior robustness and effectiveness of our approach, and online A/B testing confirms its ability to improve relevance in the same exposure position, resulting in more clicks and conversions. To the best of our knowledge, this method is the first interaction-based approach for large e-commerce search relevance calculation. Notably, we have deployed it for the entire search traffic on this http URL, the largest B2B e-commerce platform in the world.</p>
  </details>
</details>
<details>
  <summary>41. <b>【2406.02134】he current status of large language models in summarizing radiology report impressions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02134">https://arxiv.org/abs/2406.02134</a></p>
  <p><b>作者</b>：Danqing Hu,Shanyuan Zhang,Qing Liu,Xiaofeng Zhu,Bing Liu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large language models, language processing tasks, natural language processing, Large language, ChatGPT show excellent</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) like ChatGPT show excellent capabilities in various natural language processing tasks, especially for text generation. The effectiveness of LLMs in summarizing radiology report impressions remains unclear. In this study, we explore the capability of eight LLMs on the radiology report impression summarization. Three types of radiology reports, i.e., CT, PET-CT, and Ultrasound reports, are collected from Peking University Cancer Hospital and Institute. We use the report findings to construct the zero-shot, one-shot, and three-shot prompts with complete example reports to generate the impressions. Besides the automatic quantitative evaluation metrics, we define five human evaluation metrics, i.e., completeness, correctness, conciseness, verisimilitude, and replaceability, to evaluate the semantics of the generated impressions. Two thoracic surgeons (ZSY and LB) and one radiologist (LQ) compare the generated impressions with the reference impressions and score each impression under the five human evaluation metrics. Experimental results show that there is a gap between the generated impressions and reference impressions. Although the LLMs achieve comparable performance in completeness and correctness, the conciseness and verisimilitude scores are not very high. Using few-shot prompts can improve the LLMs' performance in conciseness and verisimilitude, but the clinicians still think the LLMs can not replace the radiologists in summarizing the radiology impressions.</p>
  </details>
</details>
<details>
  <summary>42. <b>【2406.02128】Iteration Head: A Mechanistic Study of Chain-of-Thought</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02128">https://arxiv.org/abs/2406.02128</a></p>
  <p><b>作者</b>：Vivien Cabannes,Charles Arnal,Wassim Bouaziz,Alice Yang,Francois Charton,Julia Kempe</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, improve Large Language, theoretical approximation power, Large Language, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power. However, our understanding of the inner workings and conditions of apparition of CoT capabilities remains limited. This paper helps fill this gap by demonstrating how CoT reasoning emerges in transformers in a controlled and interpretable setting. In particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined "iteration heads". We track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.</p>
  </details>
</details>
<details>
  <summary>43. <b>【2406.02120】Diver: Large Language Model Decoding with Span-Level Mutual Information Verification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02120">https://arxiv.org/abs/2406.02120</a></p>
  <p><b>作者</b>：Jinliang Lu,Chen Wang,Jiajun Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large language models, shown impressive capabilities, Large language, language models, task-specific instructions</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have shown impressive capabilities in adapting to various tasks when provided with task-specific instructions. However, LLMs using standard decoding strategies often struggle with deviations from the inputs. Intuitively, compliant LLM outputs should reflect the information present in the input, which can be measured by point-wise mutual information (PMI) scores. Therefore, we propose Diver, a novel approach that enhances LLM Decoding through span-level PMI verification. During inference, Diver first identifies divergence steps that may lead to multiple candidate spans. Subsequently, it calculates the PMI scores by assessing the log-likelihood gains of the input if the candidate spans are generated. Finally, the optimal span is selected based on the PMI re-ranked output distributions. We evaluate our method across various downstream tasks, and empirical results demonstrate that Diver significantly outperforms existing decoding methods in both performance and versatility.</p>
  </details>
</details>
<details>
  <summary>44. <b>【2406.02110】UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02110">https://arxiv.org/abs/2406.02110</a></p>
  <p><b>作者</b>：Zhuoyang Li,Liran Deng,Hui Liu,Qiaoqiao Liu,Junzhao Du</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：extensive Chinese open-domain, Chinese open-domain knowledge, extensive Chinese, Chinese open-domain, recent times</p>
  <p><b>备注</b>： 10 pages, 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:OwnThink stands as the most extensive Chinese open-domain knowledge graph introduced in recent times. Despite prior attempts in question answering over OwnThink (OQA), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering. In this paper, we introduce UniOQA, a unified framework that integrates two complementary parallel workflows. Unlike conventional approaches, UniOQA harnesses large language models (LLMs) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement. Initially, to bolster representation capacity, we fine-tune an LLM to translate questions into the Cypher query language (CQL), tackling issues associated with restricted semantic understanding and hallucinations. Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated CQL. Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation (RAG) process to the knowledge graph. Ultimately, we optimize answer accuracy through a dynamic decision algorithm. Experimental findings illustrate that UniOQA notably advances SpCQL Logical Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new state-of-the-art results on this benchmark. Through ablation experiments, we delve into the superior representation capacity of UniOQA and quantify its performance breakthrough.</p>
  </details>
</details>
<details>
  <summary>45. <b>【2406.02106】MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02106">https://arxiv.org/abs/2406.02106</a></p>
  <p><b>作者</b>：Weiqi Wang,Yangqiu Song</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：enable Large Language, Large Language Models, Large Language, enable Large, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents. Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning. We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step. These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities. Our data and models are publicly accessible at this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>【2406.02100】Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02100">https://arxiv.org/abs/2406.02100</a></p>
  <p><b>作者</b>：Haolong Li,Yu Ma,Yinqi Zhang,Chen Ye,Jie Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, complex multi-step reasoning, language understanding, multi-step reasoning problems</p>
  <p><b>备注</b>： Accept by Findings of ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have shown excellent performance in language understanding, text generation, code synthesis, and many other tasks, while they still struggle in complex multi-step reasoning problems, such as mathematical reasoning. In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine-tuning on high-quality synthetic data. Experimental results with the open-llama-3B model on three different test datasets show that not only the model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also demonstrates certain generalization capabilities on the out-of-domain datasets. Specifically, this paper has designed two out-of-domain datasets in the form of extending the numerical range and the composing components of the arithmetical puzzle problem separately. The fine-tuned models have shown encouraging performance on these two far more difficult tasks with the zero-shot pass@1 at 0.33 and 0.35, respectively.</p>
  </details>
</details>
<details>
  <summary>47. <b>【2406.02080】LongSSM: On the Length Extension of State-space Models in Language Modelling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02080">https://arxiv.org/abs/2406.02080</a></p>
  <p><b>作者</b>：Shida Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Dynamical Systems (math.DS)</p>
  <p><b>关键词</b>：language modeling, Length extension, investigate the length-extension, Length, extension</p>
  <p><b>备注</b>： 23 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we investigate the length-extension of state-space models (SSMs) in language modeling. Length extension involves training models on short sequences and testing them on longer ones. We show that state-space models trained with zero hidden states initialization have difficulty doing length extension. We explain this difficulty by pointing out the length extension is equivalent to polynomial extrapolation. Based on the theory, we propose a simple yet effective method - changing the hidden states initialization scheme - to improve the length extension. Moreover, our method shows that using long training sequence length is beneficial but not necessary to length extension. Changing the hidden state initialization enables the efficient training of long-memory model with a smaller training context length.</p>
  </details>
</details>
<details>
  <summary>48. <b>【2406.02079】Assessing the Performance of Chinese Open Source Large Language Models in Information Extraction Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02079">https://arxiv.org/abs/2406.02079</a></p>
  <p><b>作者</b>：Yida Cai,Hao Sun,Hsiu-Yuan Huang,Yunfang Wu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Natural Language Processing, facilitating seamless integration, extracting structured information, Named Entity Recognition, Language Processing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Information Extraction (IE) plays a crucial role in Natural Language Processing (NLP) by extracting structured information from unstructured text, thereby facilitating seamless integration with various real-world applications that rely on structured data. Despite its significance, recent experiments focusing on English IE tasks have shed light on the challenges faced by Large Language Models (LLMs) in achieving optimal performance, particularly in sub-tasks like Named Entity Recognition (NER). In this paper, we delve into a comprehensive investigation of the performance of mainstream Chinese open-source LLMs in tackling IE tasks, specifically under zero-shot conditions where the models are not fine-tuned for specific tasks. Additionally, we present the outcomes of several few-shot experiments to further gauge the capability of these models. Moreover, our study includes a comparative analysis between these open-source LLMs and ChatGPT, a widely recognized language model, on IE performance. Through meticulous experimentation and analysis, we aim to provide insights into the strengths, limitations, and potential enhancements of existing Chinese open-source LLMs in the domain of Information Extraction within the context of NLP.</p>
  </details>
</details>
<details>
  <summary>49. <b>【2406.02069】PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02069">https://arxiv.org/abs/2406.02069</a></p>
  <p><b>作者</b>：Zefan Cai.,Yichi Zhang,Bofei Gao,Tianyu Liu,Keming Lu,Wayne Xiong,Yue Dong,Baobao Chang,Junjie Hu,Wen Xiao</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：flow inside large, inside large language, attention-based information flow, information flow inside, long context processing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusin on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques achieving up to a 20.5 absolute accuracy improvement on TREC.</p>
  </details>
</details>
<details>
  <summary>50. <b>【2406.02061】Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02061">https://arxiv.org/abs/2406.02061</a></p>
  <p><b>作者</b>：Marianna Nezhurina,Lucia Cipolina-Kun,Mehdi Cherti,Jenia Jitsev</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, exhibiting scaling laws, predict function improvement, Large Language, zero-shot manner</p>
  <p><b>备注</b>： v1</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical "reasoning"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at this https URL</p>
  </details>
</details>
<details>
  <summary>51. <b>【2406.02060】I've got the "Answer"! Interpretation of LLMs Hidden States in Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02060">https://arxiv.org/abs/2406.02060</a></p>
  <p><b>作者</b>：Valeriya Goloviznina,Evgeny Kotelnikov</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：large language models, Interpretability and explainability, increasingly important, important in light, rapid development</p>
  <p><b>备注</b>： Accepted for NLDB-2024 conference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Interpretability and explainability of AI are becoming increasingly important in light of the rapid development of large language models (LLMs). This paper investigates the interpretation of LLMs in the context of the knowledge-based question answering. The main hypothesis of the study is that correct and incorrect model behavior can be distinguished at the level of hidden states. The quantized models LLaMA-2-7B-Chat, Mistral-7B, Vicuna-7B and the MuSeRC question-answering dataset are used to test this hypothesis. The results of the analysis support the proposed hypothesis. We also identify the layers which have a negative effect on the model's behavior. As a prospect of practical application of the hypothesis, we propose to train such "weak" layers additionally in order to improve the quality of the task solution.</p>
  </details>
</details>
<details>
  <summary>52. <b>【2406.02050】Analyzing Social Biases in Japanese Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02050">https://arxiv.org/abs/2406.02050</a></p>
  <p><b>作者</b>：Hitomi Yanaka,Namgi Han,Ryoma Kumon,Jie Lu,Masashi Takeshita,Ryo Sekizawa,Taisei Kato,Hiromi Arai</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, development of Large, social biases, Japanese LLMs</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the development of Large Language Models (LLMs), social biases in the LLMs have become a crucial issue. While various benchmarks for social biases have been provided across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, and analyze social biases in Japanese LLMs. The results show that while current Japanese LLMs improve their accuracies on JBBQ by instruction-tuning, their bias scores become larger. In addition, augmenting their prompts with warning about social biases reduces the effect of biases in some models.</p>
  </details>
</details>
<details>
  <summary>53. <b>【2406.02044】QROA: A Black-Box Query-Response Optimization Attack on LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02044">https://arxiv.org/abs/2406.02044</a></p>
  <p><b>作者</b>：Hussein Jawad,Nicolas J.-B. BRUNEL(LaMME)</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, recent months, surged in popularity</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have surged in popularity in recent months, yet they possess concerning capabilities for generating harmful content when manipulated. This study introduces the Query-Response Optimization Attack (QROA), an optimization-based strategy designed to exploit LLMs through a black-box, query-only interaction. QROA adds an optimized trigger to a malicious instruction to compel the LLM to generate harmful content. Unlike previous approaches, QROA does not require access to the model's logit information or any other internal data and operates solely through the standard query-response interface of LLMs. Inspired by deep Q-learning and Greedy coordinate descent, the method iteratively updates tokens to maximize a designed reward function. We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\%. We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed. This study demonstrates the feasibility of generating jailbreak attacks against deployed LLMs in the public domain using black-box optimization methods, enabling more comprehensive safety testing of LLMs.</p>
  </details>
</details>
<details>
  <summary>54. <b>【2406.02030】Multimodal Reasoning with Multimodal Knowledge Graph</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02030">https://arxiv.org/abs/2406.02030</a></p>
  <p><b>作者</b>：Junlin Lee,Yequan Wang,Jing Li,Min Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：large language models, Multimodal reasoning, Multimodal, knowledge, Multimodal Knowledge Graph</p>
  <p><b>备注</b>： Accepted by ACL 2024 (Main Conference)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM's parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models.</p>
  </details>
</details>
<details>
  <summary>55. <b>【2406.02018】Why Would You Suggest That? Human Trust in Language Model Responses</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02018">https://arxiv.org/abs/2406.02018</a></p>
  <p><b>作者</b>：Manasi Sharma,Ho Chit Siu,Rohan Paleja,Jaime D. Peña</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, creative decision-making scenarios, emergence of Large, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.</p>
  </details>
</details>
<details>
  <summary>56. <b>【2406.02004】Efficiently Train ASR Models that Memorize Less and Perform Better with Per-core Clipping</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02004">https://arxiv.org/abs/2406.02004</a></p>
  <p><b>作者</b>：Lun Wang,Om Thakkar,Zhong Meng,Nicole Rafidi,Rohit Prabhavalkar,Arun Narayanan</p>
  <p><b>类目</b>：Cryptography and Security (cs.CR); Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：automatic speech recognition, large-scale automatic speech, training large-scale automatic, speech recognition, Gradient clipping plays</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Gradient clipping plays a vital role in training large-scale automatic speech recognition (ASR) models. It is typically applied to minibatch gradients to prevent gradient explosion, and to the individual sample gradients to mitigate unintended memorization. This work systematically investigates the impact of a specific granularity of gradient clipping, namely per-core clip-ping (PCC), across training a wide range of ASR models. We empirically demonstrate that PCC can effectively mitigate unintended memorization in ASR models. Surprisingly, we find that PCC positively influences ASR performance metrics, leading to improved convergence rates and reduced word error rates. To avoid tuning the additional hyperparameter introduced by PCC, we further propose a novel variant, adaptive per-core clipping (APCC), for streamlined optimization. Our findings highlight the multifaceted benefits of PCC as a strategy for robust, privacy-forward ASR model training.</p>
  </details>
</details>
<details>
  <summary>57. <b>【2406.02002】Position Debiasing Fine-Tuning for Causal Perception in Long-Term Dialogue</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02002">https://arxiv.org/abs/2406.02002</a></p>
  <p><b>作者</b>：Shixuan Fan,Wei Wei,Wendi Li,Xian-Ling Mao,Wenfeng Xie,Dangyang Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：extensive dialogue history, dialogue, relevant, human-like responses based, dialogue system</p>
  <p><b>备注</b>： Accepted to IJCAI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The core of the dialogue system is to generate relevant, informative, and human-like responses based on extensive dialogue history. Recently, dialogue generation domain has seen mainstream adoption of large language models (LLMs), due to its powerful capability in generating utterances. However, there is a natural deficiency for such models, that is, inherent position bias, which may lead them to pay more attention to the nearby utterances instead of causally relevant ones, resulting in generating irrelevant and generic responses in long-term dialogue. To alleviate such problem, in this paper, we propose a novel method, named Causal Perception long-term Dialogue framework (CPD), which employs perturbation-based causal variable discovery method to extract casually relevant utterances from the dialogue history and enhances model causal perception during fine-tuning. Specifically, a local-position awareness method is proposed in CPD for inter-sentence position correlation elimination, which helps models extract causally relevant utterances based on perturbations. Then, a casual-perception fine-tuning strategy is also proposed, to enhance the capability of discovering the causal invariant factors, by differently perturbing causally relevant and non-casually relevant ones for response generation. Experimental results on two datasets prove that our proposed method can effectively alleviate the position bias for multiple LLMs and achieve significant progress compared with existing baselines.</p>
  </details>
</details>
<details>
  <summary>58. <b>【2406.01988】Personalized Topic Selection Model for Topic-Grounded Dialogue</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01988">https://arxiv.org/abs/2406.01988</a></p>
  <p><b>作者</b>：Shixuan Fan,Wei Wei,Xiaofei Wen,Xianling Mao,Jixiong Chen,Dangyang Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：accomplish specific tasks, actively guide users, topic-guided conversations, textbf, increasingly popular</p>
  <p><b>备注</b>： Accepted to ACL 2024 Findings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, the topic-grounded dialogue (TGD) system has become increasingly popular as its powerful capability to actively guide users to accomplish specific tasks through topic-guided conversations. Most existing works utilize side information (\eg topics or personas) in isolation to enhance the topic selection ability. However, due to disregarding the noise within these auxiliary information sources and their mutual influence, current models tend to predict user-uninteresting and contextually irrelevant topics. To build user-engaging and coherent dialogue agent, we propose a \textbf{P}ersonalized topic s\textbf{E}lection model for \textbf{T}opic-grounded \textbf{D}ialogue, named \textbf{PETD}, which takes account of the interaction of side information to selectively aggregate such information for more accurately predicting subsequent topics. Specifically, we evaluate the correlation between global topics and personas and selectively incorporate the global topics aligned with user personas. Furthermore, we propose a contrastive learning based persona selector to filter out irrelevant personas under the constraint of lacking pertinent persona annotations. Throughout the selection and generation, diverse relevant side information is considered. Extensive experiments demonstrate that our proposed method can generate engaging and diverse responses, outperforming state-of-the-art baselines across various evaluation metrics.</p>
  </details>
</details>
<details>
  <summary>59. <b>【2406.01983】RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01983">https://arxiv.org/abs/2406.01983</a></p>
  <p><b>作者</b>：Bichen Wang,Yuzhe Zi,Yixin Sun,Yanyan Zhao,Bing Qin</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：language model training, large language models, model training datasets, large language, training datasets</p>
  <p><b>备注</b>： Work is in progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \textbf{R}everse \textbf{KL}-Divergence-based Knowledge \textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.</p>
  </details>
</details>
<details>
  <summary>60. <b>【2406.01981】Zyda: A 1.3T Dataset for Open Language Modeling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01981">https://arxiv.org/abs/2406.01981</a></p>
  <p><b>作者</b>：Yury Tokpanov,Beren Millidge,Paolo Glorioso,Jonathan Pilault,Adam Ibrahim,James Whittington,Quentin Anthony</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：large language models, surged correspondingly, scaled dramatically, dramatically in recent, recent years</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.</p>
  </details>
</details>
<details>
  <summary>61. <b>【2406.01976】Conditional Language Learning with Context</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01976">https://arxiv.org/abs/2406.01976</a></p>
  <p><b>作者</b>：Xiao Zhang,Miao Li,Ji Wu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：fitting raw text, sophisticated language understanding, language understanding skills, raw text, learn sophisticated language</p>
  <p><b>备注</b>： To appear at the 41st International Conference on Machine Learning (ICML 2024)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Language models can learn sophisticated language understanding skills from fitting raw text. They also unselectively learn useless corpus statistics and biases, especially during finetuning on domain-specific corpora. In this paper, we propose a simple modification to causal language modeling called conditional finetuning, which performs language modeling conditioned on a context. We show that a context can "explain away" certain corpus statistics and make the model avoid learning them. In this fashion, conditional finetuning achieves selective learning from a corpus, learning knowledge useful for downstream tasks while avoiding learning useless corpus statistics like topic biases. This selective learning effect leads to less forgetting and better stability-plasticity tradeoff in domain finetuning, potentially benefitting lifelong learning with language models.</p>
  </details>
</details>
<details>
  <summary>62. <b>【2406.01946】Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01946">https://arxiv.org/abs/2406.01946</a></p>
  <p><b>作者</b>：Tong Zhou,Xuandong Zhao,Xiaolin Xu,Shaolei Ren</p>
  <p><b>类目</b>：Cryptography and Security (cs.CR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, harmful content, forge harmful content, language models, machine-generated content</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.</p>
  </details>
</details>
<details>
  <summary>63. <b>【2406.01943】Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01943">https://arxiv.org/abs/2406.01943</a></p>
  <p><b>作者</b>：Nik Bear Brown</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, Word Error Rate, Character Error Rate</p>
  <p><b>备注</b>： An extensive survey of the literature specifying algorithms and techniques enhancing the trustworthiness and understanding of Large Language Models (LLMs)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper surveys evaluation techniques to enhance the trustworthiness and understanding of Large Language Models (LLMs). As reliance on LLMs grows, ensuring their reliability, fairness, and transparency is crucial. We explore algorithmic methods and metrics to assess LLM performance, identify weaknesses, and guide development towards more trustworthy applications. Key evaluation metrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR, BERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot Learning Performance, Transfer Learning Evaluation, Adversarial Testing, and Fairness and Bias Evaluation. We introduce innovative approaches like LLMMaps for stratified evaluation, Benchmarking and Leaderboards for competitive assessment, Stratified Analysis for in-depth understanding, Visualization of Blooms Taxonomy for cognitive level accuracy distribution, Hallucination Score for quantifying inaccuracies, Knowledge Stratification Strategy for hierarchical analysis, and Machine Learning Models for Hierarchy Generation. Human Evaluation is highlighted for capturing nuances that automated metrics may miss. These techniques form a framework for evaluating LLMs, aiming to enhance transparency, guide development, and establish user trust. Future papers will describe metric visualization and demonstrate each approach on practical examples.</p>
  </details>
</details>
<details>
  <summary>64. <b>【2406.01940】Process-Driven Autoformalization in Lean 4</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01940">https://arxiv.org/abs/2406.01940</a></p>
  <p><b>作者</b>：Jianqiao Lu,Zhengying Liu,Yingjia Wan,Yinya Huang,Haiming Wang,Zhicheng Yang,Jing Tang,Zhijiang Guo</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)</p>
  <p><b>关键词</b>：advancing mathematical reasoning, textbf, natural language mathematics, offers significant potential, mathematical reasoning</p>
  <p><b>备注</b>： 22 pages, 1 figures, 11 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Autoformalization, the conversion of natural language mathematics into formal languages, offers significant potential for advancing mathematical reasoning. However, existing efforts are limited to formal languages with substantial online corpora and struggle to keep pace with rapidly evolving languages like Lean 4. To bridge this gap, we propose a new benchmark \textbf{Form}alization for \textbf{L}ean~\textbf{4} (\textbf{\name}) designed to evaluate the autoformalization capabilities of large language models (LLMs). This benchmark encompasses a comprehensive assessment of questions, answers, formal statements, and proofs. Additionally, we introduce a \textbf{P}rocess-\textbf{S}upervised \textbf{V}erifier (\textbf{PSV}) model that leverages the precise feedback from Lean 4 compilers to enhance autoformalization. Our experiments demonstrate that the PSV method improves autoformalization, enabling higher accuracy using less filtered training data. Furthermore, when fine-tuned with data containing detailed process information, PSV can leverage the data more effectively, leading to more significant improvements in autoformalization for Lean 4. Our dataset and code are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>65. <b>【2406.01934】Optimal Transport Guided Correlation Assignment for Multimodal Entity Linking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01934">https://arxiv.org/abs/2406.01934</a></p>
  <p><b>作者</b>：Zefeng Zhang,Jiawei Sheng,Chuang Zhang,Yunzhi Liang,Wenyuan Zhang,Siqi Wang,Tingwen Liu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Multimodal Entity Linking, Entity Linking, link ambiguous mentions, Multimodal Entity, aims to link</p>
  <p><b>备注</b>： Findings of ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal Entity Linking (MEL) aims to link ambiguous mentions in multimodal contexts to entities in a multimodal knowledge graph. A pivotal challenge is to fully leverage multi-element correlations between mentions and entities to bridge modality gap and enable fine-grained semantic matching. Existing methods attempt several local correlative mechanisms, relying heavily on the automatically learned attention weights, which may over-concentrate on partial correlations. To mitigate this issue, we formulate the correlation assignment problem as an optimal transport (OT) problem, and propose a novel MEL framework, namely OT-MEL, with OT-guided correlation assignment. Thereby, we exploit the correlation between multimodal features to enhance multimodal fusion, and the correlation between mentions and entities to enhance fine-grained matching. To accelerate model prediction, we further leverage knowledge distillation to transfer OT assignment knowledge to attention mechanism. Experimental results show that our model significantly outperforms previous state-of-the-art baselines and confirm the effectiveness of the OT-guided correlation assignment.</p>
  </details>
</details>
<details>
  <summary>66. <b>【2406.01931】Dishonesty in Helpful and Harmless Alignment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01931">https://arxiv.org/abs/2406.01931</a></p>
  <p><b>作者</b>：Youcheng Huang,Jingkun Tang,Duanyu Feng,Zheng Zhang,Wenqiang Lei,Jiancheng Lv,Anthony G. Cohn</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：People tell lies, seeking rewards, People, Large language models, satisfy human preference</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:People tell lies when seeking rewards. Large language models (LLMs) are aligned to human values with reinforcement learning where they get rewards if they satisfy human preference. We find that this also induces dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses. Using the latest interpreting tools, we detect dishonesty, show how LLMs can be harmful if their honesty is increased, and analyze such conflicts at the parameter-level. Given these preliminaries and the hypothesis that reward-seeking stimulates dishonesty, we theoretically show that the dishonesty can in-turn decrease the alignment performances and augment reward-seeking alignment with representation regularization. Extensive results, including GPT-4 annotated win-rates, perplexities, and cases studies demonstrate that we can train more honest, helpful, and harmless LLMs. We will make all our codes and results be open-sourced upon this paper's acceptance.</p>
  </details>
</details>
<details>
  <summary>67. <b>【2406.01919】OTTAWA: Optimal TransporT Adaptive Word Aligner for Hallucination and Omission Translation Errors Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01919">https://arxiv.org/abs/2406.01919</a></p>
  <p><b>作者</b>：Chenyang Huang,Abbas Ghaddar,Ivan Kobyzev,Mehdi Rezagholizadeh,Osmar R. Zaiane,Boxing Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Machine Translation, omissions in Machine, system internal states, considerable attention, attention on detecting</p>
  <p><b>备注</b>： Accepted by ACL 2024 Findings</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, there has been considerable attention on detecting hallucinations and omissions in Machine Translation (MT) systems. The two dominant approaches to tackle this task involve analyzing the MT system's internal states or relying on the output of external tools, such as sentence similarity or MT quality estimators. In this work, we introduce OTTAWA, a novel Optimal Transport (OT)-based word aligner specifically designed to enhance the detection of hallucinations and omissions in MT systems. Our approach explicitly models the missing alignments by introducing a "null" vector, for which we propose a novel one-side constrained OT setting to allow an adaptive null alignment. Our approach yields competitive results compared to state-of-the-art methods across 18 language pairs on the HalOmi benchmark. In addition, it shows promising features, such as the ability to distinguish between both error types and perform word-level detection without accessing the MT system's internal states.</p>
  </details>
</details>
<details>
  <summary>68. <b>【2406.01914】HPE-CogVLM: New Head Pose Grounding Task Exploration on Vision Language Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01914">https://arxiv.org/abs/2406.01914</a></p>
  <p><b>作者</b>：Yu Tian,Tianqi Shao,Tsukasa Demizu,Xuyang Wu,Hsin-Tai Wu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：roll Euler angles, Head pose estimation, precise numerical output, Euler angles, roll Euler</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Head pose estimation (HPE) task requires a sophisticated understanding of 3D spatial relationships and precise numerical output of yaw, pitch, and roll Euler angles. Previous HPE studies are mainly based on Non-large language models (Non-LLMs), which rely on close-up human heads cropped from the full image as inputs and lack robustness in real-world scenario. In this paper, we present a novel framework to enhance the HPE prediction task by leveraging the visual grounding capability of CogVLM. CogVLM is a vision language model (VLM) with grounding capability of predicting object bounding boxes (BBoxes), which enables HPE training and prediction using full image information input. To integrate the HPE task into the VLM, we first cop with the catastrophic forgetting problem in large language models (LLMs) by investigating the rehearsal ratio in the data rehearsal method. Then, we propose and validate a LoRA layer-based model merging method, which keeps the integrity of parameters, to enhance the HPE performance in the framework. The results show our HPE-CogVLM achieves a 31.5\% reduction in Mean Absolute Error for HPE prediction over the current Non-LLM based state-of-the-art in cross-dataset evaluation. Furthermore, we compare our LoRA layer-based model merging method with LoRA fine-tuning only and other merging methods in CogVLM. The results demonstrate our framework outperforms them in all HPE metrics.</p>
  </details>
</details>
<details>
  <summary>69. <b>【2406.01895】Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01895">https://arxiv.org/abs/2406.01895</a></p>
  <p><b>作者</b>：Mahdi Sabbaghi,George Pappas,Hamed Hassani,Surbhi Goel</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：basic arithmetic tasks, code generation, language understanding, logical reasoning, basic arithmetic</p>
  <p><b>备注</b>： 32 pages, 16 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite the success of Transformers on language understanding, code generation, and logical reasoning, they still fail to generalize over length on basic arithmetic tasks such as addition and multiplication. A major reason behind this failure is the vast difference in structure between numbers and text; For example, the numbers are typically parsed from right to left, and there is a correspondence between digits at the same position across different numbers. In contrast, for text, such symmetries are quite unnatural. In this work, we propose to encode these semantics explicitly into the model via modified number formatting and custom positional encodings. Empirically, our method allows a Transformer trained on numbers with at most 5-digits for addition and multiplication to generalize up to 50-digit numbers, without using additional data for longer sequences. We further demonstrate that traditional absolute positional encodings (APE) fail to generalize to longer sequences, even when trained with augmented data that captures task symmetries. To elucidate the importance of explicitly encoding structure, we prove that explicit incorporation of structure via positional encodings is necessary for out-of-distribution generalization. Finally, we pinpoint other challenges inherent to length generalization beyond capturing symmetries, in particular complexity of the underlying task, and propose changes in the training distribution to address them.</p>
  </details>
</details>
<details>
  <summary>70. <b>【2406.01879】Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01879">https://arxiv.org/abs/2406.01879</a></p>
  <p><b>作者</b>：Haiming Wu,Hanqing Zhang,Richeng Xuan,Dawei Song</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Chinese Spelling Check, Spelling Check, Chinese Spelling, correct potentially misspelled, potentially misspelled characters</p>
  <p><b>备注</b>： 12 pages, 6 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Chinese Spelling Check (CSC) aims to detect and correct potentially misspelled characters in Chinese sentences. Naturally, it involves the detection and correction subtasks, which interact with each other dynamically. Such interactions are bi-directional, i.e., the detection result would help reduce the risk of over-correction and under-correction while the knowledge learnt from correction would help prevent false detection. Current CSC approaches are of two types: correction-only or single-directional detection-to-correction interactive frameworks. Nonetheless, they overlook the bi-directional interactions between detection and correction. This paper aims to fill the gap by proposing a Bi-directional Detector-Corrector framework for CSC (Bi-DCSpell). Notably, Bi-DCSpell contains separate detection and correction encoders, followed by a novel interactive learning module facilitating bi-directional feature interactions between detection and correction to improve each other's representation learning. Extensive experimental results demonstrate a robust correction performance of Bi-DCSpell on widely used benchmarking datasets while possessing a satisfactory detection ability.</p>
  </details>
</details>
<details>
  <summary>71. <b>【2406.01876】GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01876">https://arxiv.org/abs/2406.01876</a></p>
  <p><b>作者</b>：Xuanqing Liu,Luyang Kong,Runhui Wang,Patrick Song,Austin Nevins,Henrik Johnson,Nimish Amlathe,Davor Golac</p>
  <p><b>类目</b>：Databases (cs.DB); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：data ingestion process, Schema matching constitutes, contemporary database systems, constitutes a pivotal, pivotal phase</p>
  <p><b>备注</b>： KDD 2024 Camera Ready; 11 pages, 8 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems. Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table. This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights. Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s. This study revisits this foundational problem within the context of large language models. Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table. We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy. The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain.</p>
  </details>
</details>
<details>
  <summary>72. <b>【2406.01873】CR-UTP: Certified Robustness against Universal Text Perturbations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01873">https://arxiv.org/abs/2406.01873</a></p>
  <p><b>作者</b>：Qian Lou,Xin Liang,Jiaqi Xue,Yancheng Zhang,Rui Xie,Mengxin Zheng</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Universal Text Perturbations, minor input variations, language model robustness, language model, language prediction</p>
  <p><b>备注</b>： Accepted by ACL Findings 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:It is imperative to ensure the stability of every prediction made by a language model; that is, a language's prediction should remain consistent despite minor input variations, like word substitutions. In this paper, we investigate the problem of certifying a language model's robustness against Universal Text Perturbations (UTPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing certified robustness based on random smoothing has shown considerable promise in certifying the input-specific text perturbations (ISTPs), operating under the assumption that any random alteration of a sample's clean or adversarial words would negate the impact of sample-wise perturbations. However, with UTPs, masking only the adversarial words can eliminate the attack. A naive method is to simply increase the masking ratio and the likelihood of masking attack tokens, but it leads to a significant reduction in both certified accuracy and the certified radius due to input corruption by extensive masking. To solve this challenge, we introduce a novel approach, the superior prompt search method, designed to identify a superior prompt that maintains higher certified accuracy under extensive masking. Additionally, we theoretically motivate why ensembles are a particularly suitable choice as base prompts for random smoothing. The method is denoted by superior prompt ensembling technique. We also empirically confirm this technique, obtaining state-of-the-art results in multiple settings. These methodologies, for the first time, enable high certified accuracy against both UTPs and ISTPs. The source code of CR-UTP is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>73. <b>【2406.01866】#EpiTwitter: Public Health Messaging During the COVID-19 Pandemic</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01866">https://arxiv.org/abs/2406.01866</a></p>
  <p><b>作者</b>：Ashwin Rao,Nazanin Sabri,Siyi Guo,Louiqa Raschid,Kristina Lerman</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Computers and Society (cs.CY); Social and Information Networks (cs.SI)</p>
  <p><b>关键词</b>：social media serving, Effective communication, health crises, crises is critical, moral language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Effective communication during health crises is critical, with social media serving as a key platform for public health experts (PHEs) to engage with the public. However, it also amplifies pseudo-experts promoting contrarian views. Despite its importance, the role of emotional and moral language in PHEs' communication during COVID-19 remains under explored. This study examines how PHEs and pseudo-experts communicated on Twitter during the pandemic, focusing on emotional and moral language and their engagement with political elites. Analyzing tweets from 489 PHEs and 356 pseudo-experts from January 2020 to January 2021, alongside public responses, we identified key priorities and differences in messaging strategy. PHEs prioritize masking, healthcare, education, and vaccines, using positive emotional language like optimism. In contrast, pseudo-experts discuss therapeutics and lockdowns more frequently, employing negative emotions like pessimism and disgust. Negative emotional and moral language tends to drive engagement, but positive language from PHEs fosters positivity in public responses. PHEs exhibit liberal partisanship, expressing more positivity towards liberals and negativity towards conservative elites, while pseudo-experts show conservative partisanship. These findings shed light on the polarization of COVID-19 discourse and underscore the importance of strategic use of emotional and moral language by experts to mitigate polarization and enhance public trust.</p>
  </details>
</details>
<details>
  <summary>74. <b>【2406.01863】owards Effective Time-Aware Language Representation: Exploring Enhanced Temporal Understanding in Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01863">https://arxiv.org/abs/2406.01863</a></p>
  <p><b>作者</b>：Jiexin Wang,Adam Jatowt,Yi Cai</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Natural Language Processing, field of Natural, Language Processing, Natural Language, increasingly crucial</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In the evolving field of Natural Language Processing, understanding the temporal context of text is increasingly crucial. This study investigates methods to incorporate temporal information during pre-training, aiming to achieve effective time-aware language representation for improved performance on time-related tasks. In contrast to common pre-trained models like BERT, which rely on synchronic document collections such as BookCorpus and Wikipedia, our research introduces BiTimeBERT 2.0, a novel language model pre-trained on a temporal news article collection. BiTimeBERT 2.0 utilizes this temporal news collection, focusing on three innovative pre-training objectives: Time-Aware Masked Language Modeling (TAMLM), Document Dating (DD), and Time-Sensitive Entity Replacement (TSER). Each objective targets a unique aspect of temporal information. TAMLM is designed to enhance the understanding of temporal contexts and relations, DD integrates document timestamps as chronological markers, and TSER focuses on the temporal dynamics of "Person" entities, recognizing their inherent temporal significance. The experimental results consistently demonstrate that BiTimeBERT 2.0 outperforms models like BERT and other existing pre-trained models, achieving substantial gains across a variety of downstream NLP tasks and applications where time plays a pivotal role.</p>
  </details>
</details>
<details>
  <summary>75. <b>【2406.01860】Eliciting the Priors of Large Language Models using Iterated In-Context Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01860">https://arxiv.org/abs/2406.01860</a></p>
  <p><b>作者</b>：Jian-Qiao Zhu,Thomas L. Griffiths</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, decisions is critical, increasingly deployed</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As Large Language Models (LLMs) are increasingly deployed in real-world settings, understanding the knowledge they implicitly use when making decisions is critical. One way to capture this knowledge is in the form of Bayesian prior distributions. We develop a prompt-based workflow for eliciting prior distributions from LLMs. Our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution. We validated our method in settings where iterated learning has previously been used to estimate the priors of human participants -- causal learning, proportion estimation, and predicting everyday quantities. We found that priors elicited from GPT-4 qualitatively align with human priors in these settings. We then used the same method to elicit priors from GPT-4 for a variety of speculative events, such as the timing of the development of superhuman AI.</p>
  </details>
</details>
<details>
  <summary>76. <b>【2406.01855】ruthEval: A Dataset to Evaluate LLM Truthfulness and Reliability</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01855">https://arxiv.org/abs/2406.01855</a></p>
  <p><b>作者</b>：Aisha Khatun,Daniel G. Brown</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Model, Large Language, Language Model, existing benchmarks proving, areas of research</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Model (LLM) evaluation is currently one of the most important areas of research, with existing benchmarks proving to be insufficient and not completely representative of LLMs' various capabilities. We present a curated collection of challenging statements on sensitive topics for LLM benchmarking called TruthEval. These statements were curated by hand and contain known truth values. The categories were chosen to distinguish LLMs' abilities from their stochastic nature. We perform some initial analyses using this dataset and find several instances of LLMs failing in simple tasks showing their inability to understand simple questions.</p>
  </details>
</details>
<details>
  <summary>77. <b>【2406.01835】An Open Multilingual System for Scoring Readability of Wikipedia</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01835">https://arxiv.org/abs/2406.01835</a></p>
  <p><b>作者</b>：Mykola Trokhymovych,Indira Sen,Martin Gerlach</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：freely accessible knowledge, Wikipedia, accessible knowledge, largest platform, platform for open</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With over 60M articles, Wikipedia has become the largest platform for open and freely accessible knowledge. While it has more than 15B monthly visits, its content is believed to be inaccessible to many readers due to the lack of readability of its text. However, previous investigations of the readability of Wikipedia have been restricted to English only, and there are currently no systems supporting the automatic readability assessment of the 300+ languages in Wikipedia. To bridge this gap, we develop a multilingual model to score the readability of Wikipedia articles. To train and evaluate this model, we create a novel multilingual dataset spanning 14 languages, by matching articles from Wikipedia to simplified Wikipedia and online children encyclopedias. We show that our model performs well in a zero-shot scenario, yielding a ranking accuracy of more than 80% across 14 languages and improving upon previous benchmarks. These results demonstrate the applicability of the model at scale for languages in which there is no ground-truth data available for model fine-tuning. Furthermore, we provide the first overview on the state of readability in Wikipedia beyond English.</p>
  </details>
</details>
<details>
  <summary>78. <b>【2406.01806】Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01806">https://arxiv.org/abs/2406.01806</a></p>
  <p><b>作者</b>：Zhen Lin,Shubhendu Trivedi,Jimeng Sun</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：large language models, numerous natural language, natural language generation, language models, large language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The advent of large language models (LLMs) has dramatically advanced the state-of-the-art in numerous natural language generation tasks. For LLMs to be applied reliably, it is essential to have an accurate measure of their confidence. Currently, the most commonly used confidence score function is the likelihood of the generated sequence, which, however, conflates semantic and syntactic components. For instance, in question-answering (QA) tasks, an awkward phrasing of the correct answer might result in a lower probability prediction. Additionally, different tokens should be weighted differently depending on the context. In this work, we propose enhancing the predicted sequence probability by assigning different weights to various tokens using attention values elicited from the base LLM. By employing a validation set, we can identify the relevant attention heads, thereby significantly improving the reliability of the vanilla sequence probability confidence measure. We refer to this new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to implement, fast to compute, and offers considerable potential for further improvement with task-specific prompts. Across several QA datasets and a diverse array of LLMs, CSL has demonstrated significantly higher reliability than state-of-the-art baselines in predicting generation quality, as measured by the AUROC or AUARC.</p>
  </details>
</details>
<details>
  <summary>79. <b>【2406.01789】AI-based Classification of Customer Support Tickets: State of the Art and Implementation with AutoML</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01789">https://arxiv.org/abs/2406.01789</a></p>
  <p><b>作者</b>：Mario Truss,Stephan Boehm</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：shortening resolution time, improve customer support, customer inquiries, crucial to improve, shortening resolution</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Automation of support ticket classification is crucial to improve customer support performance and shortening resolution time for customer inquiries. This research aims to test the applicability of automated machine learning (AutoML) as a technology to train a machine learning model (ML model) that can classify support tickets. The model evaluation conducted in this research shows that AutoML can be used to train ML models with good classification performance. Moreover, this paper fills a research gap by providing new insights into developing AI solutions without a dedicated professional by utilizing AutoML, which makes this technology more accessible for companies without specialized AI departments and staff.</p>
  </details>
</details>
<details>
  <summary>80. <b>【2406.01775】OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01775">https://arxiv.org/abs/2406.01775</a></p>
  <p><b>作者</b>：Kerim Büyükakyüz</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：generating human-like text, enabling unprecedented capabilities, human-like text, advent of large, unprecedented capabilities</p>
  <p><b>备注</b>： 10 pages, 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The advent of large language models (LLMs) has revolutionized natural language processing, enabling unprecedented capabilities in understanding and generating human-like text. However, the computational cost and convergence times associated with fine-tuning these models remain significant challenges. Low-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these issues by introducing efficient fine-tuning techniques with a reduced number of trainable parameters. In this paper, we present OLoRA, an enhancement to the LoRA method that leverages orthonormal matrix initialization through QR decomposition. OLoRA significantly accelerates the convergence of LLM training while preserving the efficiency benefits of LoRA, such as the number of trainable parameters and GPU memory footprint. Our empirical evaluations demonstrate that OLoRA not only converges faster but also exhibits improved performance compared to standard LoRA across a variety of language modeling tasks. This advancement opens new avenues for more efficient and accessible fine-tuning of LLMs, potentially enabling broader adoption and innovation in natural language applications.</p>
  </details>
</details>
<details>
  <summary>81. <b>【2406.01771】LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01771">https://arxiv.org/abs/2406.01771</a></p>
  <p><b>作者</b>：Wen Lai,Mohsen Mesgar,Alexander Fraser</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, democratize large language, models capable, democratize large, imperative to make</p>
  <p><b>备注</b>： Accepted to Findings of ACL 2024. The code, datasets, and models are publicly available at [this https URL](https://github.com/boschresearch/ACL24-MLLM) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low-resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.</p>
  </details>
</details>
<details>
  <summary>82. <b>【2406.01749】owards Harnessing Large Language Models for Comprehension of Conversational Grounding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01749">https://arxiv.org/abs/2406.01749</a></p>
  <p><b>作者</b>：Kristiina Jokinen,Phillip Schneider,Taiga Mori</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：establishing mutual knowledge, large language models, collaborative mechanism, mechanism for establishing, establishing mutual</p>
  <p><b>备注</b>： Accepted to IWSDS 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Conversational grounding is a collaborative mechanism for establishing mutual knowledge among participants engaged in a dialogue. This experimental study analyzes information-seeking conversations to investigate the capabilities of large language models in classifying dialogue turns related to explicit or implicit grounding and predicting grounded knowledge elements. Our experimental results reveal challenges encountered by large language models in the two tasks and discuss ongoing research efforts to enhance large language model-based conversational grounding comprehension through pipeline architectures and knowledge bases. These initiatives aim to develop more effective dialogue systems that are better equipped to handle the intricacies of grounded knowledge in conversations.</p>
  </details>
</details>
<details>
  <summary>83. <b>【2406.01721】Rotation and Permutation for Advanced Outlier Management and Efficient Quantization of LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01721">https://arxiv.org/abs/2406.01721</a></p>
  <p><b>作者</b>：Haokun Lin,Haobo Xu,Yichen Wu,Jingzhi Cui,Yingtao Zhang,Linzhan Mou,Linqi Song,Zhenan Sun,Ying Wei</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Quantizing large language, presents significant challenges, Quantizing large, large language models, solving Normal Outliers-activations</p>
  <p><b>备注</b>： 26 pages, 13 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Quantizing large language models (LLMs) presents significant challenges, primarily due to outlier activations that compromise the efficiency of low-bit representation. Traditional approaches mainly focus on solving Normal Outliers-activations with consistently high magnitudes across all tokens. However, these techniques falter when dealing with Massive Outliers, which are significantly higher in value and often cause substantial performance losses during low-bit quantization. In this study, we propose DuQuant, an innovative quantization strategy employing rotation and permutation transformations to more effectively eliminate both types of outliers. Initially, DuQuant constructs rotation matrices informed by specific outlier dimensions, redistributing these outliers across adjacent channels within different rotation blocks. Subsequently, a zigzag permutation is applied to ensure a balanced distribution of outliers among blocks, minimizing block-wise variance. An additional rotation further enhances the smoothness of the activation landscape, thereby improving model performance. DuQuant streamlines the quantization process and demonstrates superior outlier management, achieving top-tier results in multiple tasks with various LLM architectures even under 4-bit weight-activation quantization. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>84. <b>【2406.01638】meCMA: Towards LLM-Empowered Time Series Forecasting via Cross-Modality Alignment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01638">https://arxiv.org/abs/2406.01638</a></p>
  <p><b>作者</b>：Chenxi Liu,Qianxiong Xu,Hao Miao,Sun Yang,Lingzheng Zhang,Cheng Long,Ziyue Li,Rui Zhao</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：scalable mobile sensing, time series, series, time, time series forecasting</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The widespread adoption of scalable mobile sensing has led to large amounts of time series data for real-world applications. A fundamental application is multivariate time series forecasting (MTSF), which aims to predict future time series values based on historical observations. Existing MTSF methods suffer from limited parameterization and small-scale training data. Recently, Large language models (LLMs) have been introduced in time series, which achieve promising forecasting performance but incur heavy computational costs. To solve these challenges, we propose TimeCMA, an LLM-empowered framework for time series forecasting with cross-modality alignment. We design a dual-modality encoding module with two branches, where the time series encoding branch extracts relatively low-quality yet pure embeddings of time series through an inverted Transformer. In addition, the LLM-empowered encoding branch wraps the same time series as prompts to obtain high-quality yet entangled prompt embeddings via a Pre-trained LLM. Then, we design a cross-modality alignment module to retrieve high-quality and pure time series embeddings from the prompt embeddings. Moreover, we develop a time series forecasting module to decode the aligned embeddings while capturing dependencies among multiple variables for forecasting. Notably, we tailor the prompt to encode sufficient temporal information into a last token and design the last token embedding storage to reduce computational costs. Extensive experiments on real data offer insight into the accuracy and efficiency of the proposed framework.</p>
  </details>
</details>
<details>
  <summary>85. <b>【2406.01633】On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01633">https://arxiv.org/abs/2406.01633</a></p>
  <p><b>作者</b>：Christine Herlihy,Jennifer Neville,Tobias Schnabel,Adith Swaminathan</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Model, power recommender systems, Language Model, Large Language, Observed Decision Processes</p>
  <p><b>备注</b>： Preprint of UAI'24 conference publication</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We explore the use of Large Language Model (LLM-based) chatbots to power recommender systems. We observe that the chatbots respond poorly when they encounter under-specified requests (e.g., they make incorrect assumptions, hedge with a long response, or refuse to answer). We conjecture that such miscalibrated response tendencies (i.e., conversational priors) can be attributed to LLM fine-tuning using annotators -- single-turn annotations may not capture multi-turn conversation utility, and the annotators' preferences may not even be representative of users interacting with a recommender system.
We first analyze public LLM chat logs to conclude that query under-specification is common. Next, we study synthetic recommendation problems with configurable latent item utilities and frame them as Partially Observed Decision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for PODPs and derive better policies that clarify under-specified queries when appropriate. Then, we re-calibrate LLMs by prompting them with learned control messages to approximate the improved policy. Finally, we show empirically that our lightweight learning approach effectively uses logged conversation data to re-calibrate the response strategies of LLM-based chatbots for recommendation tasks.
</p><p>Comments:<br>
Preprint of UAI’24 conference publication</p>
<p>Subjects:</p>
<p>Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)</p>
<p>Cite as:<br>
arXiv:2406.01633 [<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>]</p>
<p>(or<br>
arXiv:2406.01633v1 [<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2406.01633">https://doi.org/10.48550/arXiv.2406.01633</a></p>
<p>Focus to learn more</p>
<pre><code>            arXiv-issued DOI via DataCite&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>86. <b>【2406.01609】Judgement Citation Retrieval using Contextual Similarity</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01609">https://arxiv.org/abs/2406.01609</a></p>
  <p><b>作者</b>：Akshat Mohan Dasula,Hrushitha Tigulla,Preethika Bhukya</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：demanded manual effort, keyword-based search applications, understanding legal jargon, Legal case descriptions, intricate case descriptions</p>
  <p><b>备注</b>： 14 pages, 16 images, Submitted to Multimedia Tools and Applications Springer journal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Traditionally in the domain of legal research, the retrieval of pertinent citations from intricate case descriptions has demanded manual effort and keyword-based search applications that mandate expertise in understanding legal jargon. Legal case descriptions hold pivotal information for legal professionals and researchers, necessitating more efficient and automated approaches. We propose a methodology that combines natural language processing (NLP) and machine learning techniques to enhance the organization and utilization of legal case descriptions. This approach revolves around the creation of textual embeddings with the help of state-of-art embedding models. Our methodology addresses two primary objectives: unsupervised clustering and supervised citation retrieval, both designed to automate the citation extraction process. Although the proposed methodology can be used for any dataset, we employed the Supreme Court of The United States (SCOTUS) dataset, yielding remarkable results. Our methodology achieved an impressive accuracy rate of 90.9%. By automating labor-intensive processes, we pave the way for a more efficient, time-saving, and accessible landscape in legal research, benefiting legal professionals, academics, and researchers.</p>
  </details>
</details>
<details>
  <summary>87. <b>【2406.01608】Detecting Deceptive Dark Patterns in E-commerce Platforms</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01608">https://arxiv.org/abs/2406.01608</a></p>
  <p><b>作者</b>：Arya Ramteke,Sankalp Tembhurne,Gunesh Sonawane,Ratnmala N. Bhimanpallewar</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：deceptive user interfaces, user interfaces employed, manipulate user behavior, Dark patterns, deceptive user</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Dark patterns are deceptive user interfaces employed by e-commerce websites to manipulate user's behavior in a way that benefits the website, often unethically. This study investigates the detection of such dark patterns. Existing solutions include UIGuard, which uses computer vision and natural language processing, and approaches that categorize dark patterns based on detectability or utilize machine learning models trained on datasets. We propose combining web scraping techniques with fine-tuned BERT language models and generative capabilities to identify dark patterns, including outliers. The approach scrapes textual content, feeds it into the BERT model for detection, and leverages BERT's bidirectional analysis and generation abilities. The study builds upon research on automatically detecting and explaining dark patterns, aiming to raise awareness and protect consumers.</p>
  </details>
</details>
<details>
  <summary>88. <b>【2406.01607】Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01607">https://arxiv.org/abs/2406.01607</a></p>
  <p><b>作者</b>：Hongliu Cao</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：academic fields due, natural language processing, Text embedding methods, language processing tasks, universal text embeddings</p>
  <p><b>备注</b>： 45 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text embedding methods have become increasingly popular in both industrial and academic fields due to their critical role in a variety of natural language processing tasks. The significance of universal text embeddings has been further highlighted with the rise of Large Language Models (LLMs) applications such as Retrieval-Augmented Systems (RAGs). While previous models have attempted to be general-purpose, they often struggle to generalize across tasks and domains. However, recent advancements in training data quantity, quality and diversity; synthetic data generation from LLMs as well as using LLMs as backbones encourage great improvements in pursuing universal text embeddings. In this paper, we provide an overview of the recent advances in universal text embedding models with a focus on the top performing text embeddings on Massive Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we highlight the key contributions and limitations in this area, and propose potentially inspiring future research directions.</p>
  </details>
</details>
<details>
  <summary>89. <b>【2406.01606】SymTax: Symbiotic Relationship and Taxonomy Fusion for Effective Citation Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01606">https://arxiv.org/abs/2406.01606</a></p>
  <p><b>作者</b>：Karan Goyal,Mayank Goel,Vikram Goyal,Mukesh Mohania</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Citing pertinent literature, Citing pertinent, scientific document, pertinent literature, literature is pivotal</p>
  <p><b>备注</b>： Accepted in ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Citing pertinent literature is pivotal to writing and reviewing a scientific document. Existing techniques mainly focus on the local context or the global context for recommending citations but fail to consider the actual human citation behaviour. We propose SymTax, a three-stage recommendation architecture that considers both the local and the global context, and additionally the taxonomical representations of query-candidate tuples and the Symbiosis prevailing amongst them. SymTax learns to embed the infused taxonomies in the hyperbolic space and uses hyperbolic separation as a latent feature to compute query-candidate similarity. We build a novel and large dataset ArSyTa containing 8.27 million citation contexts and describe the creation process in detail. We conduct extensive experiments and ablation studies to demonstrate the effectiveness and design choice of each module in our framework. Also, combinatorial analysis from our experiments shed light on the choice of language models (LMs) and fusion embedding, and the inclusion of section heading as a signal. Our proposed module that captures the symbiotic relationship solely leads to performance gains of 26.66% and 39.25% in Recall@5 w.r.t. SOTA on ACL-200 and RefSeer datasets, respectively. The complete framework yields a gain of 22.56% in Recall@5 wrt SOTA on our proposed dataset. The code and dataset are available at this https URL</p>
  </details>
</details>
<details>
  <summary>90. <b>【2406.02488】Language-Universal Speech Attributes Modeling for Zero-Shot Multilingual Spoken Keyword Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02488">https://arxiv.org/abs/2406.02488</a></p>
  <p><b>作者</b>：Hao Yen,Pin-Jui Ku,Sabato Marco Siniscalchi,Chin-Hui Lee</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Sound (cs.SD)</p>
  <p><b>关键词</b>：self-supervised pre-trained model, automatic spoken keyword, spoken keyword recognition, universal speech attributes, manner and place</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose a novel language-universal approach to end-to-end automatic spoken keyword recognition (SKR) leveraging upon (i) a self-supervised pre-trained model, and (ii) a set of universal speech attributes (manner and place of articulation). Specifically, Wav2Vec2.0 is used to generate robust speech representations, followed by a linear output layer to produce attribute sequences. A non-trainable pronunciation model then maps sequences of attributes into spoken keywords in a multilingual setting. Experiments on the Multilingual Spoken Words Corpus show comparable performances to character- and phoneme-based SKR in seen languages. The inclusion of domain adversarial training (DAT) improves the proposed framework, outperforming both character- and phoneme-based SKR approaches with 13.73% and 17.22% relative word error rate (WER) reduction in seen languages, and achieves 32.14% and 19.92% WER reduction for unseen languages in zero-shot settings.</p>
  </details>
</details>
<details>
  <summary>91. <b>【2406.02133】SimulTron: On-Device Simultaneous Speech to Speech Translation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02133">https://arxiv.org/abs/2406.02133</a></p>
  <p><b>作者</b>：Alex Agranovich,Eliya Nachmani,Oleg Rybakov,Yifan Ding,Ye Jia,Nadav Bar,Heiga Zen,Michelle Tadmor Ramanovich</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)</p>
  <p><b>关键词</b>：enabling fluid conversations, holds the promise, conversations across languages, promise of breaking, breaking down communication</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Simultaneous speech-to-speech translation (S2ST) holds the promise of breaking down communication barriers and enabling fluid conversations across languages. However, achieving accurate, real-time translation through mobile devices remains a major challenge. We introduce SimulTron, a novel S2ST architecture designed to tackle this task. SimulTron is a lightweight direct S2ST model that uses the strengths of the Translatotron framework while incorporating key modifications for streaming operation, and an adjustable fixed delay. Our experiments show that SimulTron surpasses Translatotron 2 in offline evaluations. Furthermore, real-time evaluations reveal that SimulTron improves upon the performance achieved by Translatotron 1. Additionally, SimulTron achieves superior BLEU scores and latency compared to previous real-time S2ST method on the MuST-C dataset. Significantly, we have successfully deployed SimulTron on a Pixel 7 Pro device, show its potential for simultaneous S2ST on-device.</p>
  </details>
</details>
<details>
  <summary>92. <b>【2406.02009】Phonetic Enhanced Language Modeling for Text-to-Speech Synthesis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02009">https://arxiv.org/abs/2406.02009</a></p>
  <p><b>作者</b>：Kun Zhou,Shengkui Zhao,Yukun Ma,Chong Zhang,Hao Wang,Dianwen Ng,Chongjia Ni,Nguyen Trung Hieu,Jia Qi Yip,Bin Ma</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Sound (cs.SD)</p>
  <p><b>关键词</b>：Recent language model-based, frameworks demonstrate scalability, in-context learning capabilities, Recent language, frameworks demonstrate</p>
  <p><b>备注</b>： Accepted by Interspeech 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent language model-based text-to-speech (TTS) frameworks demonstrate scalability and in-context learning capabilities. However, they suffer from robustness issues due to the accumulation of errors in speech unit predictions during autoregressive language modeling. In this paper, we propose a phonetic enhanced language modeling method to improve the performance of TTS models. We leverage self-supervised representations that are phonetically rich as the training target for the autoregressive language model. Subsequently, a non-autoregressive model is employed to predict discrete acoustic codecs that contain fine-grained acoustic details. The TTS model focuses solely on linguistic modeling during autoregressive training, thereby reducing the error propagation that occurs in non-autoregressive training. Both objective and subjective evaluations validate the effectiveness of our proposed method.</p>
  </details>
</details>
<details>
  <summary>93. <b>【2406.01624】Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01624">https://arxiv.org/abs/2406.01624</a></p>
  <p><b>作者</b>：Alaa Nfissi,Wassim Bouachir,Nizar Bouguila,Brian Mishara</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)</p>
  <p><b>关键词</b>：gained significant attention, significant attention due, Speech emotion recognition, SER, SER systems</p>
  <p><b>备注</b>： Published in: Springer Nature International Journal of Applied Intelligence (2024)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Speech emotion recognition (SER) has gained significant attention due to its several application fields, such as mental health, education, and human-computer interaction. However, the accuracy of SER systems is hindered by high-dimensional feature sets that may contain irrelevant and redundant information. To overcome this challenge, this study proposes an iterative feature boosting approach for SER that emphasizes feature relevance and explainability to enhance machine learning model performance. Our approach involves meticulous feature selection and analysis to build efficient SER systems. In addressing our main problem through model explainability, we employ a feature evaluation loop with Shapley values to iteratively refine feature sets. This process strikes a balance between model performance and transparency, which enables a comprehensive understanding of the model's predictions. The proposed approach offers several advantages, including the identification and removal of irrelevant and redundant features, leading to a more effective model. Additionally, it promotes explainability, facilitating comprehension of the model's predictions and the identification of crucial features for emotion determination. The effectiveness of the proposed method is validated on the SER benchmarks of the Toronto emotional speech set (TESS), Berlin Database of Emotional Speech (EMO-DB), Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion (SAVEE) datasets, outperforming state-of-the-art methods. These results highlight the potential of the proposed technique in developing accurate and explainable SER systems. To the best of our knowledge, this is the first work to incorporate model explainability into an SER framework.</p>
  </details>
</details>
<h1>信息检索</h1>
<details>
  <summary>1. <b>【2406.02377】XRec: Large Language Models for Explainable Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02377">https://arxiv.org/abs/2406.02377</a></p>
  <p><b>作者</b>：Qiyao Ma,Xubin Ren,Chao Huang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：navigate information overload, providing personalized recommendations, personalized recommendations aligned, users navigate information, Recommender systems</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recommender systems help users navigate information overload by providing personalized recommendations aligned with their preferences. Collaborative Filtering (CF) is a widely adopted approach, but while advanced techniques like graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced CF models for better user representations, they often lack the ability to provide explanations for the recommended items. Explainable recommendations aim to address this gap by offering transparency and insights into the recommendation decision-making process, enhancing users' understanding. This work leverages the language capabilities of Large Language Models (LLMs) to push the boundaries of explainable recommender systems. We introduce a model-agnostic framework called XRec, which enables LLMs to provide comprehensive explanations for user behaviors in recommender systems. By integrating collaborative signals and designing a lightweight collaborative adaptor, the framework empowers LLMs to understand complex patterns in user-item interactions and gain a deeper understanding of user preferences. Our extensive experiments demonstrate the effectiveness of XRec, showcasing its ability to generate comprehensive and meaningful explanations that outperform baseline approaches in explainable recommender systems. We open-source our model implementation at this https URL.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2406.02368】Large Language Models Make Sample-Efficient Recommender Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02368">https://arxiv.org/abs/2406.02368</a></p>
  <p><b>作者</b>：Jianghao Lin,Xinyi Dai,Rong Shan,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：achieved remarkable progress, natural language processing, resembles human language, demonstrating remarkable abilities, recommender systems</p>
  <p><b>备注</b>： Accepted by Frontier of Computer Science</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have achieved remarkable progress in the field of natural language processing (NLP), demonstrating remarkable abilities in producing text that resembles human language for various tasks. This opens up new opportunities for employing them in recommender systems (RSs). In this paper, we specifically examine the sample efficiency of LLM-enhanced recommender systems, which pertains to the model's capacity to attain superior performance with a limited quantity of training data. Conventional recommendation models (CRMs) often need a large amount of training data because of the sparsity of features and interactions. Hence, we propose and verify our core viewpoint: Large Language Models Make Sample-Efficient Recommender Systems. We propose a simple yet effective framework (i.e., Laser) to validate the viewpoint from two aspects: (1) LLMs themselves are sample-efficient recommenders; and (2) LLMs, as feature generators and encoders, make CRMs more sample-efficient. Extensive experiments on two public datasets show that Laser requires only a small fraction of training samples to match or even surpass CRMs that are trained on the entire training set, demonstrating superior sample efficiency.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2406.02245】Description Boosting for Zero-Shot Entity and Relation Classification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02245">https://arxiv.org/abs/2406.02245</a></p>
  <p><b>作者</b>：Gabriele Picco,Leopold Fuchs,Marcos Martínez Galindo,Alberto Purpura,Vanessa López,Hoang Thanh Lam</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：annotate input text, input text data, leverage available external, external information, information of unseen</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Zero-shot entity and relation classification models leverage available external information of unseen classes -- e.g., textual descriptions -- to annotate input text data. Thanks to the minimum data requirement, Zero-Shot Learning (ZSL) methods have high value in practice, especially in applications where labeled data is scarce. Even though recent research in ZSL has demonstrated significant results, our analysis reveals that those methods are sensitive to provided textual descriptions of entities (or relations). Even a minor modification of descriptions can lead to a change in the decision boundary between entity (or relation) classes. In this paper, we formally define the problem of identifying effective descriptions for zero shot inference. We propose a strategy for generating variations of an initial description, a heuristic for ranking them and an ensemble method capable of boosting the predictions of zero-shot models through description enhancement. Empirical results on four different entity and relation classification datasets show that our proposed method outperform existing approaches and achieve new SOTA results on these datasets under the ZSL settings. The source code of the proposed solutions and the evaluation framework are open-sourced.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2406.02163】Pairwise Ranking Loss for Multi-Task Learning in Recommender Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02163">https://arxiv.org/abs/2406.02163</a></p>
  <p><b>作者</b>：Furkan Durmus,Hasan Saribas,Said Aldemir,Junyan Yang,Hakan Cevikalp</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：minimizing resource consumption, achieve robust representations, real-world advertising applications, plays a crucial, aiming to achieve</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-Task Learning (MTL) plays a crucial role in real-world advertising applications such as recommender systems, aiming to achieve robust representations while minimizing resource consumption. MTL endeavors to simultaneously optimize multiple tasks to construct a unified model serving diverse objectives. In online advertising systems, tasks like Click-Through Rate (CTR) and Conversion Rate (CVR) are often treated as MTL problems concurrently. However, it has been overlooked that a conversion ($y_{cvr}=1$) necessitates a preceding click ($y_{ctr}=1$). In other words, while certain CTR tasks are associated with corresponding conversions, others lack such associations. Moreover, the likelihood of noise is significantly higher in CTR tasks where conversions do not occur compared to those where they do, and existing methods lack the ability to differentiate between these two scenarios. In this study, exposure labels corresponding to conversions are regarded as definitive indicators, and a novel task-specific loss is introduced by calculating a \textbf{p}air\textbf{wise} \textbf{r}anking (PWiseR) loss between model predictions, manifesting as pairwise ranking loss, to encourage the model to rely more on them. To demonstrate the effect of the proposed loss function, experiments were conducted on different MTL and Single-Task Learning (STL) models using four distinct public MTL datasets, namely Alibaba FR, NL, US, and CCP, along with a proprietary industrial dataset. The results indicate that our proposed loss function outperforms the BCE loss function in most cases in terms of the AUC metric.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2406.02135】Robust Interaction-based Relevance Modeling for Online E-Commerce and LLM-based Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02135">https://arxiv.org/abs/2406.02135</a></p>
  <p><b>作者</b>：Ben Chen,Huangyu Dai,Xiang Ma,Wen Jiang,Wei Ning</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：items selected closely, selected closely align, Semantic relevance calculation, items selected, selected closely</p>
  <p><b>备注</b>： Accepted by ECML-PKDD'24 as Outstanding Paper. 8 pages, 2 figures, 7 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Semantic relevance calculation is crucial for e-commerce search engines, as it ensures that the items selected closely align with customer intent. Inadequate attention to this aspect can detrimentally affect user experience and engagement. Traditional text-matching techniques are prevalent but often fail to capture the nuances of search intent accurately, so neural networks now have become a preferred solution to processing such complex text matching. Existing methods predominantly employ representation-based architectures, which strike a balance between high traffic capacity and low latency. However, they exhibit significant shortcomings in generalization and robustness when compared to interaction-based architectures. In this work, we introduce a robust interaction-based modeling paradigm to address these shortcomings. It encompasses 1) a dynamic length representation scheme for expedited inference, 2) a professional terms recognition method to identify subjects and core attributes from complex sentence structures, and 3) a contrastive adversarial training protocol to bolster the model's robustness and matching capabilities. Extensive offline evaluations demonstrate the superior robustness and effectiveness of our approach, and online A/B testing confirms its ability to improve relevance in the same exposure position, resulting in more clicks and conversions. To the best of our knowledge, this method is the first interaction-based approach for large e-commerce search relevance calculation. Notably, we have deployed it for the entire search traffic on this http URL, the largest B2B e-commerce platform in the world.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2406.02048】Auto-Encoding or Auto-Regression? A Reality Check on Causality of Self-Attention-Based Sequential Recommenders</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02048">https://arxiv.org/abs/2406.02048</a></p>
  <p><b>作者</b>：Yueqi Wang,Zhankui He,Zhenrui Yue,Julian McAuley,Dong Wang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：increasingly important topic, increasingly important, important topic, topic with recent, recent advances</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The comparison between Auto-Encoding (AE) and Auto-Regression (AR) has become an increasingly important topic with recent advances in sequential recommendation. At the heart of this discussion lies the comparison of BERT4Rec and SASRec, which serve as representative AE and AR models for self-attentive sequential recommenders. Yet the conclusion of this debate remains uncertain due to: (1) the lack of fair and controlled environments for experiments and evaluations; and (2) the presence of numerous confounding factors w.r.t. feature selection, modeling choices and optimization algorithms. In this work, we aim to answer this question by conducting a series of controlled experiments. We start by tracing the AE/AR debate back to its origin through a systematic re-evaluation of SASRec and BERT4Rec, discovering that AR models generally surpass AE models in sequential recommendation. In addition, we find that AR models further outperforms AE models when using a customized design space that includes additional features, modeling approaches and optimization techniques. Furthermore, the performance advantage of AR models persists in the broader HuggingFace transformer ecosystems. Lastly, we provide potential explanations and insights into AE/AR performance from two key perspectives: low-rank approximation and inductive bias. We make our code and data available at this https URL</p>
  </details>
</details>
<details>
  <summary>7. <b>【2406.01906】ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01906">https://arxiv.org/abs/2406.01906</a></p>
  <p><b>作者</b>：Chen Mao,Jingqi Hu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：computer vision tasks, augmented reality, autonomous driving, visual geo-localization datasets, Visual Geo-localization</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Visual Geo-localization (VG) refers to the process to identify the location described in query images, which is widely applied in robotics field and computer vision tasks, such as autonomous driving, metaverse, augmented reality, and SLAM. In fine-grained images lacking specific text descriptions, directly applying pure visual methods to represent neighborhood features often leads to the model focusing on overly fine-grained features, unable to fully mine the semantic information in the images. Therefore, we propose a two-stage training method to enhance visual performance and use contrastive learning to mine challenging samples. We first leverage the multi-modal description capability of CLIP (Contrastive Language-Image Pretraining) to create a set of learnable text prompts for each geographic image feature to form vague descriptions. Then, by utilizing dynamic text prompts to assist the training of the image encoder, we enable the image encoder to learn better and more generalizable visual features. This strategy of applying text to purely visual tasks addresses the challenge of using multi-modal models for geographic images, which often suffer from a lack of precise descriptions, making them difficult to utilize widely. We validate the effectiveness of the proposed strategy on several large-scale visual geo-localization datasets, and our method achieves competitive results on multiple visual geo-localization datasets. Our code and model are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2406.01876】GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01876">https://arxiv.org/abs/2406.01876</a></p>
  <p><b>作者</b>：Xuanqing Liu,Luyang Kong,Runhui Wang,Patrick Song,Austin Nevins,Henrik Johnson,Nimish Amlathe,Davor Golac</p>
  <p><b>类目</b>：Databases (cs.DB); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：data ingestion process, Schema matching constitutes, contemporary database systems, constitutes a pivotal, pivotal phase</p>
  <p><b>备注</b>： KDD 2024 Camera Ready; 11 pages, 8 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Schema matching constitutes a pivotal phase in the data ingestion process for contemporary database systems. Its objective is to discern pairwise similarities between two sets of attributes, each associated with a distinct data table. This challenge emerges at the initial stages of data analytics, such as when incorporating a third-party table into existing databases to inform business insights. Given its significance in the realm of database systems, schema matching has been under investigation since the 2000s. This study revisits this foundational problem within the context of large language models. Adhering to increasingly stringent data security policies, our focus lies on the zero-shot and few-shot scenarios: the model should analyze only a minimal amount of customer data to execute the matching task, contrasting with the conventional approach of scrutinizing the entire data table. We emphasize that the zero-shot or few-shot assumption is imperative to safeguard the identity and privacy of customer data, even at the potential cost of accuracy. The capability to accurately match attributes under such stringent requirements distinguishes our work from previous literature in this domain.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2406.01702】Session Context Embedding for Intent Understanding in Product Search</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01702">https://arxiv.org/abs/2406.01702</a></p>
  <p><b>作者</b>：Navid Mehrdad,Vishal Rathi,Sravanthi Rajanala</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：single query-item pair, query-item pair relevance, pair relevance training, noted that single, single query-item</p>
  <p><b>备注</b>： 5 pages, 1 Figure, 5 Tables, SIGIR 2024, LLM for Individuals, Groups, and Society</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:It is often noted that single query-item pair relevance training in search does not capture the customer intent. User intent can be better deduced from a series of engagements (Clicks, ATCs, Orders) in a given search session. We propose a novel method for vectorizing session context for capturing and utilizing context in retrieval and rerank. In the runtime, session embedding is an alternative to query embedding, saved and updated after each request in the session, it can be used for retrieval and ranking. We outline session embedding's solution to session-based intent understanding and its architecture, the background to this line of thought in search and recommendation, detail the methodologies implemented, and finally present the results of an implementation of session embedding for query product type classification. We demonstrate improvements over strategies ignoring session context in the runtime for user intent understanding.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2406.01633】On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01633">https://arxiv.org/abs/2406.01633</a></p>
  <p><b>作者</b>：Christine Herlihy,Jennifer Neville,Tobias Schnabel,Adith Swaminathan</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Model, power recommender systems, Language Model, Large Language, Observed Decision Processes</p>
  <p><b>备注</b>： Preprint of UAI'24 conference publication</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We explore the use of Large Language Model (LLM-based) chatbots to power recommender systems. We observe that the chatbots respond poorly when they encounter under-specified requests (e.g., they make incorrect assumptions, hedge with a long response, or refuse to answer). We conjecture that such miscalibrated response tendencies (i.e., conversational priors) can be attributed to LLM fine-tuning using annotators -- single-turn annotations may not capture multi-turn conversation utility, and the annotators' preferences may not even be representative of users interacting with a recommender system.
We first analyze public LLM chat logs to conclude that query under-specification is common. Next, we study synthetic recommendation problems with configurable latent item utilities and frame them as Partially Observed Decision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for PODPs and derive better policies that clarify under-specified queries when appropriate. Then, we re-calibrate LLMs by prompting them with learned control messages to approximate the improved policy. Finally, we show empirically that our lightweight learning approach effectively uses logged conversation data to re-calibrate the response strategies of LLM-based chatbots for recommendation tasks.
</p><p>Comments:<br>
Preprint of UAI’24 conference publication</p>
<p>Subjects:</p>
<p>Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)</p>
<p>Cite as:<br>
arXiv:2406.01633 [<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>]</p>
<p>(or<br>
arXiv:2406.01633v1 [<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2406.01633">https://doi.org/10.48550/arXiv.2406.01633</a></p>
<p>Focus to learn more</p>
<pre><code>            arXiv-issued DOI via DataCite&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>11. <b>【2406.01631】An LLM-based Recommender System Environment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01631">https://arxiv.org/abs/2406.01631</a></p>
  <p><b>作者</b>：Nathan Corecco,Giorgio Piatti,Luca A. Lanzendörfer,Flint Xiaofeng Fan,Roger Wattenhofer</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：discovering relevant content, optimize long-term rewards, Reinforcement learning, recommender systems due, recommender systems</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Reinforcement learning (RL) has gained popularity in the realm of recommender systems due to its ability to optimize long-term rewards and guide users in discovering relevant content. However, the successful implementation of RL in recommender systems is challenging because of several factors, including the limited availability of online data for training on-policy methods. This scarcity requires expensive human interaction for online model training. Furthermore, the development of effective evaluation frameworks that accurately reflect the quality of models remains a fundamental challenge in recommender systems. To address these challenges, we propose a comprehensive framework for synthetic environments that simulate human behavior by harnessing the capabilities of large language models (LLMs). We complement our framework with in-depth ablation studies and demonstrate its effectiveness with experiments on movie and book recommendations. By utilizing LLMs as synthetic users, this work introduces a modular and novel framework for training RL-based recommender systems. The software, including the RL environment, is publicly available.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2406.01629】RecDiff: Diffusion Model for Social Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01629">https://arxiv.org/abs/2406.01629</a></p>
  <p><b>作者</b>：Zongwei Li,Lianghao Xia,Chao Huang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)</p>
  <p><b>关键词</b>：friend relations observed, online social platforms, friend relations, relations observed, observed in online</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Social recommendation has emerged as a powerful approach to enhance personalized recommendations by leveraging the social connections among users, such as following and friend relations observed in online social platforms. The fundamental assumption of social recommendation is that socially-connected users exhibit homophily in their preference patterns. This means that users connected by social ties tend to have similar tastes in user-item activities, such as rating and purchasing. However, this assumption is not always valid due to the presence of irrelevant and false social ties, which can contaminate user embeddings and adversely affect recommendation accuracy. To address this challenge, we propose a novel diffusion-based social denoising framework for recommendation (RecDiff). Our approach utilizes a simple yet effective hidden-space diffusion paradigm to alleivate the noisy effect in the compressed and dense representation space. By performing multi-step noise diffusion and removal, RecDiff possesses a robust ability to identify and eliminate noise from the encoded user representations, even when the noise levels vary. The diffusion module is optimized in a downstream task-aware manner, thereby maximizing its ability to enhance the recommendation process. We conducted extensive experiments to evaluate the efficacy of our framework, and the results demonstrate its superiority in terms of recommendation accuracy, training efficiency, and denoising effectiveness. The source code for the model implementation is publicly available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2406.01618】FinEmbedDiff: A Cost-Effective Approach of Classifying Financial Documents with Vector Sampling using Multi-modal Embedding Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01618">https://arxiv.org/abs/2406.01618</a></p>
  <p><b>作者</b>：Anjanava Biswas,Wrick Talukdar</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Accurate classification, crucial but challenging, Accurate, documents, multi-modal</p>
  <p><b>备注</b>： 10 pages, 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Accurate classification of multi-modal financial documents, containing text, tables, charts, and images, is crucial but challenging. Traditional text-based approaches often fail to capture the complex multi-modal nature of these documents. We propose FinEmbedDiff, a cost-effective vector sampling method that leverages pre-trained multi-modal embedding models to classify financial documents. Our approach generates multi-modal embedding vectors for documents, and compares new documents with pre-computed class embeddings using vector similarity measures. Evaluated on a large dataset, FinEmbedDiff achieves competitive classification accuracy compared to state-of-the-art baselines while significantly reducing computational costs. The method exhibits strong generalization capabilities, making it a practical and scalable solution for real-world financial applications.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2406.01611】System-2 Recommenders: Disentangling Utility and Engagement in Recommendation Systems via Temporal Point-Processes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01611">https://arxiv.org/abs/2406.01611</a></p>
  <p><b>作者</b>：Arpit Agarwal,Nicolas Usunier,Alessandro Lazaric,Maximilian Nickel</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Machine Learning (cs.LG); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：modern human experience, important part, modern human, human experience, experience whose influence</p>
  <p><b>备注</b>： Accepted at FAccT'24</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recommender systems are an important part of the modern human experience whose influence ranges from the food we eat to the news we read. Yet, there is still debate as to what extent recommendation platforms are aligned with the user goals. A core issue fueling this debate is the challenge of inferring a user utility based on engagement signals such as likes, shares, watch time etc., which are the primary metric used by platforms to optimize content. This is because users utility-driven decision-processes (which we refer to as System-2), e.g., reading news that are relevant for them, are often confounded by their impulsive decision-processes (which we refer to as System-1), e.g., spend time on click-bait news. As a result, it is difficult to infer whether an observed engagement is utility-driven or impulse-driven. In this paper we explore a new approach to recommender systems where we infer user utility based on their return probability to the platform rather than engagement signals. Our intuition is that users tend to return to a platform in the long run if it creates utility for them, while pure engagement-driven interactions that do not add utility, may affect user return in the short term but will not have a lasting effect. We propose a generative model in which past content interactions impact the arrival rates of users based on a self-exciting Hawkes process. These arrival rates to the platform are a combination of both System-1 and System-2 decision processes. The System-2 arrival intensity depends on the utility and has a long lasting effect, while the System-1 intensity depends on the instantaneous gratification and tends to vanish rapidly. We show analytically that given samples it is possible to disentangle System-1 and System-2 and allow content optimization based on user utility. We conduct experiments on synthetic data to demonstrate the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2406.01609】Judgement Citation Retrieval using Contextual Similarity</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01609">https://arxiv.org/abs/2406.01609</a></p>
  <p><b>作者</b>：Akshat Mohan Dasula,Hrushitha Tigulla,Preethika Bhukya</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：demanded manual effort, keyword-based search applications, understanding legal jargon, Legal case descriptions, intricate case descriptions</p>
  <p><b>备注</b>： 14 pages, 16 images, Submitted to Multimedia Tools and Applications Springer journal</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Traditionally in the domain of legal research, the retrieval of pertinent citations from intricate case descriptions has demanded manual effort and keyword-based search applications that mandate expertise in understanding legal jargon. Legal case descriptions hold pivotal information for legal professionals and researchers, necessitating more efficient and automated approaches. We propose a methodology that combines natural language processing (NLP) and machine learning techniques to enhance the organization and utilization of legal case descriptions. This approach revolves around the creation of textual embeddings with the help of state-of-art embedding models. Our methodology addresses two primary objectives: unsupervised clustering and supervised citation retrieval, both designed to automate the citation extraction process. Although the proposed methodology can be used for any dataset, we employed the Supreme Court of The United States (SCOTUS) dataset, yielding remarkable results. Our methodology achieved an impressive accuracy rate of 90.9%. By automating labor-intensive processes, we pave the way for a more efficient, time-saving, and accessible landscape in legal research, benefiting legal professionals, academics, and researchers.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2406.01608】Detecting Deceptive Dark Patterns in E-commerce Platforms</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01608">https://arxiv.org/abs/2406.01608</a></p>
  <p><b>作者</b>：Arya Ramteke,Sankalp Tembhurne,Gunesh Sonawane,Ratnmala N. Bhimanpallewar</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：deceptive user interfaces, user interfaces employed, manipulate user behavior, Dark patterns, deceptive user</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Dark patterns are deceptive user interfaces employed by e-commerce websites to manipulate user's behavior in a way that benefits the website, often unethically. This study investigates the detection of such dark patterns. Existing solutions include UIGuard, which uses computer vision and natural language processing, and approaches that categorize dark patterns based on detectability or utilize machine learning models trained on datasets. We propose combining web scraping techniques with fine-tuned BERT language models and generative capabilities to identify dark patterns, including outliers. The approach scrapes textual content, feeds it into the BERT model for detection, and leverages BERT's bidirectional analysis and generation abilities. The study builds upon research on automatically detecting and explaining dark patterns, aiming to raise awareness and protect consumers.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2406.01607】Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01607">https://arxiv.org/abs/2406.01607</a></p>
  <p><b>作者</b>：Hongliu Cao</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：academic fields due, natural language processing, Text embedding methods, language processing tasks, universal text embeddings</p>
  <p><b>备注</b>： 45 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text embedding methods have become increasingly popular in both industrial and academic fields due to their critical role in a variety of natural language processing tasks. The significance of universal text embeddings has been further highlighted with the rise of Large Language Models (LLMs) applications such as Retrieval-Augmented Systems (RAGs). While previous models have attempted to be general-purpose, they often struggle to generalize across tasks and domains. However, recent advancements in training data quantity, quality and diversity; synthetic data generation from LLMs as well as using LLMs as backbones encourage great improvements in pursuing universal text embeddings. In this paper, we provide an overview of the recent advances in universal text embedding models with a focus on the top performing text embeddings on Massive Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we highlight the key contributions and limitations in this area, and propose potentially inspiring future research directions.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2406.01606】SymTax: Symbiotic Relationship and Taxonomy Fusion for Effective Citation Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01606">https://arxiv.org/abs/2406.01606</a></p>
  <p><b>作者</b>：Karan Goyal,Mayank Goel,Vikram Goyal,Mukesh Mohania</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Citing pertinent literature, Citing pertinent, scientific document, pertinent literature, literature is pivotal</p>
  <p><b>备注</b>： Accepted in ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Citing pertinent literature is pivotal to writing and reviewing a scientific document. Existing techniques mainly focus on the local context or the global context for recommending citations but fail to consider the actual human citation behaviour. We propose SymTax, a three-stage recommendation architecture that considers both the local and the global context, and additionally the taxonomical representations of query-candidate tuples and the Symbiosis prevailing amongst them. SymTax learns to embed the infused taxonomies in the hyperbolic space and uses hyperbolic separation as a latent feature to compute query-candidate similarity. We build a novel and large dataset ArSyTa containing 8.27 million citation contexts and describe the creation process in detail. We conduct extensive experiments and ablation studies to demonstrate the effectiveness and design choice of each module in our framework. Also, combinatorial analysis from our experiments shed light on the choice of language models (LMs) and fusion embedding, and the inclusion of section heading as a signal. Our proposed module that captures the symbiotic relationship solely leads to performance gains of 26.66% and 39.25% in Recall@5 w.r.t. SOTA on ACL-200 and RefSeer datasets, respectively. The complete framework yields a gain of 22.56% in Recall@5 wrt SOTA on our proposed dataset. The code and dataset are available at this https URL</p>
  </details>
</details>
<details>
  <summary>19. <b>【2406.01604】An Empirical Study of Excitation and Aggregation Design Adaptions in CLIP4Clip for Video-Text Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01604">https://arxiv.org/abs/2406.01604</a></p>
  <p><b>作者</b>：Xiaolun Jing,Genke Yang,Jian Chu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：clip retrieval task, video clip retrieval, video-text retrieval domain, clip retrieval, frame representations aggregation</p>
  <p><b>备注</b>： 20 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:CLIP4Clip model transferred from the CLIP has been the de-factor standard to solve the video clip retrieval task from frame-level input, triggering the surge of CLIP4Clip-based models in the video-text retrieval domain. In this work, we rethink the inherent limitation of widely-used mean pooling operation in the frame features aggregation and investigate the adaptions of excitation and aggregation design for discriminative video representation generation. We present a novel excitationand-aggregation design, including (1) The excitation module is available for capturing non-mutuallyexclusive relationships among frame features and achieving frame-wise features recalibration, and (2) The aggregation module is applied to learn exclusiveness used for frame representations aggregation. Similarly, we employ the cascade of sequential module and aggregation design to generate discriminative video representation in the sequential type. Besides, we adopt the excitation design in the tight type to obtain representative frame features for multi-modal interaction. The proposed modules are evaluated on three benchmark datasets of MSR-VTT, ActivityNet and DiDeMo, achieving MSR-VTT (43.9 R@1), ActivityNet (44.1 R@1) and DiDeMo (31.0 R@1). They outperform the CLIP4Clip results by +1.2% (+0.5%), +4.5% (+1.9%) and +9.5% (+2.7%) relative (absolute) improvements, demonstrating the superiority of our proposed excitation and aggregation designs. We hope our work will serve as an alternative for frame representations aggregation and facilitate future research.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2406.01603】Privacy-preserving recommender system using the data collaboration analysis for distributed datasets</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01603">https://arxiv.org/abs/2406.01603</a></p>
  <p><b>作者</b>：Tomoya Yanagi,Shunnosuke Ikeda,Noriyoshi Sukegawa,Yuichi Takano</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Cryptography and Security (cs.CR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：provide high-quality recommendations, multiple datasets held, integrate multiple datasets, recommendations for users, order to provide</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In order to provide high-quality recommendations for users, it is desirable to share and integrate multiple datasets held by different parties. However, when sharing such distributed datasets, we need to protect personal and confidential information contained in the datasets. To this end, we establish a framework for privacy-preserving recommender systems using the data collaboration analysis of distributed datasets. Numerical experiments with two public rating datasets demonstrate that our privacy-preserving method for rating prediction can improve the prediction accuracy for distributed datasets. This study opens up new possibilities for privacy-preserving techniques in recommender systems.</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>【2406.02552】VHS: High-Resolution Iterative Stereo Matching with Visual Hull Priors</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02552">https://arxiv.org/abs/2406.02552</a></p>
  <p><b>作者</b>：Markus Plack,Hannah Dröge,Leif Van Holland,Matthias B. Hullin</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：present a stereo-matching, memory-efficient technique, stereo-matching method, correlation computation, high-resolution images</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present a stereo-matching method for depth estimation from high-resolution images using visual hulls as priors, and a memory-efficient technique for the correlation computation. Our method uses object masks extracted from supplementary views of the scene to guide the disparity estimation, effectively reducing the search space for matches. This approach is specifically tailored to stereo rigs in volumetric capture systems, where an accurate depth plays a key role in the downstream reconstruction task. To enable training and regression at high resolutions targeted by recent systems, our approach extends a sparse correlation computation into a hybrid sparse-dense scheme suitable for application in leading recurrent network architectures. We evaluate the performance-efficiency trade-off of our method compared to state-of-the-art methods, and demonstrate the efficacy of the visual hull guidance. In addition, we propose a training scheme for a further reduction of memory requirements during optimization, facilitating training on high-resolution data.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2406.02549】Dreamguider: Improved Training free Diffusion-based Conditional Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02549">https://arxiv.org/abs/2406.02549</a></p>
  <p><b>作者</b>：Nithin Gopalakrishnan Nair,Vishal M Patel</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：training-free conditional generation.However, conditional generation.However, formidable tool, tool for training-free, training-free conditional</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have emerged as a formidable tool for training-free conditional generation.However, a key hurdle in inference-time guidance techniques is the need for compute-heavy backpropagation through the diffusion network for estimating the guidance direction. Moreover, these techniques often require handcrafted parameter tuning on a case-by-case basis. Although some recent works have introduced minimal compute methods for linear inverse problems, a generic lightweight guidance solution to both linear and non-linear guidance problems is still missing. To this end, we propose Dreamguider, a method that enables inference-time guidance without compute-heavy backpropagation through the diffusion network. The key idea is to regulate the gradient flow through a time-varying factor. Moreover, we propose an empirical guidance scale that works for a wide variety of tasks, hence removing the need for handcrafted parameter tuning. We further introduce an effective lightweight augmentation strategy that significantly boosts the performance during inference-time guidance. We present experiments using Dreamguider on multiple tasks across multiple datasets and models to show the effectiveness of the proposed modules. To facilitate further research, we will make the code public after the review process.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2406.02548】Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02548">https://arxiv.org/abs/2406.02548</a></p>
  <p><b>作者</b>：Mohamed El Amine Boudjoghra,Angela Dai,Jean Lahoud,Hisham Cholakkal,Rao Muhammad Anwer,Salman Khan,Fahad Shahbaz Khan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：show strong promise, high computation requirements, segmentation show strong, Recent works, high computation cost</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent works on open-vocabulary 3D instance segmentation show strong promise, but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on 3D clip features, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP for multi-view aggregation into 3D. As a consequence, this hampers their applicability in many real-world applications that require both fast and accurate predictions. To this end, we propose a fast yet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that effectively leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation. We address this task by generating class-agnostic 3D masks for objects in the scene and associating them with text prompts. We observe that the projection of class-agnostic 3D point cloud instances already holds instance information; thus, using SAM might only result in redundancy that unnecessarily increases the inference time. We empirically find that a better performance of matching text prompts to 3D masks can be achieved in a faster fashion with a 2D object detector. We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network. Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to $\sim$16$\times$ speedup compared to the best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7\% while operating at 22 seconds per scene. Code and model are available at this http URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2406.02547】Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02547">https://arxiv.org/abs/2406.02547</a></p>
  <p><b>作者</b>：Alex Jinpeng Wang,Linjie Li,Yiqi Lin,Min Li,Lijuan Wang,Mike Zheng Shou</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：substantial GPU memory, in-context text, in-context text length, multimodal model due, computational costs</p>
  <p><b>备注</b>： 12 pages. The website is \url{ [this https URL](https://fingerrec.github.io/visincontext) }</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Training models with longer in-context lengths is a significant challenge for multimodal model due to substantial GPU memory and computational costs. This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently. We present Visualized In-Context Text Processing (VisInContext), which processes long in-context text using visual tokens. This technique significantly reduces GPU memory usage and floating point operations (FLOPs) for both training and inferenceing stage. For instance, our method expands the pre-training in-context text length from 256 to 2048 tokens with nearly same FLOPs for a 56 billion parameter MOE model. Experimental results demonstrate that model trained with VisInContext delivers superior performance on common downstream benchmarks for in-context few-shot evaluation. Additionally, VisInContext is complementary to existing methods for increasing in-context text length and enhances document understanding capabilities, showing great potential in document QA tasks and sequential document retrieval.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2406.02541】Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02541">https://arxiv.org/abs/2406.02541</a></p>
  <p><b>作者</b>：Inkyu Shin,Qihang Yu,Xiaohui Shen,In So Kweon,Kuk-Jin Yoon,Liang-Chieh Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Recent advancements, achieving high temporal, video, high temporal consistency, Gaussian Splatting</p>
  <p><b>备注</b>： Project page at this [this https URL](https://video-3dgs-project.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in zero-shot video diffusion models have shown promise for text-driven video editing, but challenges remain in achieving high temporal consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting (3DGS)-based video refiner designed to enhance temporal consistency in zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian optimizing process tailored for editing dynamic monocular videos. In the first stage, Video-3DGS employs an improved version of COLMAP, referred to as MC-COLMAP, which processes original videos using a Masked and Clipped approach. For each video clip, MC-COLMAP generates the point clouds for dynamic foreground objects and complex backgrounds. These point clouds are utilized to initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent foreground and background views. Both foreground and background views are then merged with a 2D learnable parameter map to reconstruct full views. In the second stage, we leverage the reconstruction ability developed in the first stage to impose the temporal constraints on the video diffusion model. To demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive experiments across two related tasks: Video Reconstruction and Video Editing. Video-3DGS trained with 3k iterations significantly improves video reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency (x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring temporal consistency across 58 dynamic monocular videos.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2406.02540】ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02540">https://arxiv.org/abs/2406.02540</a></p>
  <p><b>作者</b>：Tianchen Zhao,Tongcheng Fang,Enshu Liu,Wan Rui,Widyadewi Soedarmadji,Shiyao Li,Zinan Lin,Guohao Dai,Shengen Yan,Huazhong Yang,Xuefei Ning,Yu Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：exhibited remarkable performance, generating realistic images, textual instructions, Diffusion transformers, quantizing diffusion transformers</p>
  <p><b>备注</b>： Project Page: [this https URL](https://a-suozhang.xyz/viditq.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion transformers (DiTs) have exhibited remarkable performance in visual generation tasks, such as generating realistic images or videos based on textual instructions. However, larger model sizes and multi-frame processing for video generation lead to increased computational and memory costs, posing challenges for practical deployment on edge devices. Post-Training Quantization (PTQ) is an effective method for reducing memory costs and computational complexity. When quantizing diffusion transformers, we find that applying existing diffusion quantization methods designed for U-Net faces challenges in preserving quality. After analyzing the major challenges for quantizing diffusion transformers, we design an improved quantization scheme: "ViDiT-Q": Video and Image Diffusion Transformer Quantization) to address these issues. Furthermore, we identify highly sensitive layers and timesteps hinder quantization for lower bit-widths. To tackle this, we improve ViDiT-Q with a novel metric-decoupled mixed-precision quantization method (ViDiT-Q-MP). We validate the effectiveness of ViDiT-Q across a variety of text-to-image and video models. While baseline quantization methods fail at W8A8 and produce unreadable content at W4A8, ViDiT-Q achieves lossless W8A8 quantization. ViDiTQ-MP achieves W4A8 with negligible visual quality degradation, resulting in a 2.5x memory optimization and a 1.5x latency speedup.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2406.02539】Parrot: Multilingual Visual Instruction Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02539">https://arxiv.org/abs/2406.02539</a></p>
  <p><b>作者</b>：Hai-Long Sun,Da-Wei Zhou,Yang Li,Shiyin Lu,Chao Yi,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,De-Chuan Zhan,Han-Jia Ye</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, artificial general intelligence, Language Models, Multimodal Large</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V has marked a significant step towards artificial general intelligence. Existing methods mainly focus on aligning vision encoders with LLMs through supervised fine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs' inherent ability to react to multiple languages progressively deteriorate as the training process evolves. We empirically find that the imbalanced SFT datasets, primarily composed of English-centric image-text pairs, lead to significantly reduced performance in non-English languages. This is due to the failure of aligning the vision encoder and LLM with multilingual tokens during the SFT process. In this paper, we introduce Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level. Parrot makes the visual tokens condition on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens. Specifically, to enhance non-English visual tokens alignment, we compute the cross-attention using the initial visual features and textual embeddings, the result of which is then fed into the MoE router to select the most relevant experts. The selected experts subsequently convert the initial visual tokens into language-specific visual tokens. Moreover, considering the current lack of benchmarks for evaluating multilingual capabilities within the field, we collect and make available a Massive Multilingual Multimodal Benchmark which includes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our method not only demonstrates state-of-the-art performance on multilingual MMBench and MMMB, but also excels across a broad range of multimodal tasks. Both the source code and the training dataset of Parrot will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2406.02537】opViewRS: Vision-Language Models as Top-View Spatial Reasoners</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02537">https://arxiv.org/abs/2406.02537</a></p>
  <p><b>作者</b>：Chengzu Li,Caiqi Zhang,Han Zhou,Nigel Collier,Anna Korhonen,Ivan Vulić</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Top-view perspective denotes, large Vision-Language Models, perspective denotes, denotes a typical, vital for localization</p>
  <p><b>备注</b>： 9 pages, 3 figures, 3 tables (21 pages, 4 figures, 15 tables including references and appendices)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Top-view perspective denotes a typical way in which humans read and reason over different types of maps, and it is vital for localization and navigation of humans as well as of `non-human' agents, such as the ones backed by large Vision-Language Models (VLMs). Nonetheless, spatial reasoning capabilities of modern VLMs remain unattested and underexplored. In this work, we thus study their capability to understand and reason over spatial relations from the top view. The focus on top view also enables controlled evaluations at different granularity of spatial reasoning; we clearly disentangle different abilities (e.g., recognizing particular objects versus understanding their relative positions). We introduce the TopViewRS (Top-View Reasoning in Space) dataset, consisting of 11,384 multiple-choice questions with either realistic or semantic top-view map as visual input. We then use it to study and evaluate VLMs across 4 perception and reasoning tasks with different levels of complexity. Evaluation of 10 representative open- and closed-source VLMs reveals the gap of more than 50% compared to average human performance, and it is even lower than the random baseline in some cases. Although additional experiments show that Chain-of-Thought reasoning can boost model capabilities by 5.82% on average, the overall performance of VLMs remains limited. Our findings underscore the critical need for enhanced model capability in top-view spatial reasoning and set a foundation for further research towards human-level proficiency of VLMs in real-world multimodal tasks.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2406.02535】Enhancing 2D Representation Learning with a 3D Prior</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02535">https://arxiv.org/abs/2406.02535</a></p>
  <p><b>作者</b>：Mehmet Aygün,Prithviraj Dhar,Zhicheng Yan,Oisin Mac Aodha,Rakesh Ranjan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：fundamental task, task in computer, data, Abstract, Learning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Learning robust and effective representations of visual data is a fundamental task in computer vision. Traditionally, this is achieved by training models with labeled data which can be expensive to obtain. Self-supervised learning attempts to circumvent the requirement for labeled data by learning representations from raw unlabeled visual data alone. However, unlike humans who obtain rich 3D information from their binocular vision and through motion, the majority of current self-supervised methods are tasked with learning from monocular 2D image collections. This is noteworthy as it has been demonstrated that shape-centric visual processing is more robust compared to texture-biased automated methods. Inspired by this, we propose a new approach for strengthening existing self-supervised methods by explicitly enforcing a strong 3D structural prior directly into the model during training. Through experiments, across a range of datasets, we demonstrate that our 3D aware representations are more robust compared to conventional self-supervised baselines.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2406.02533】SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection Ensembles for Satellite Feature Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02533">https://arxiv.org/abs/2406.02533</a></p>
  <p><b>作者</b>：Van Minh Nguyen,Emma Sandidge,Trupti Mahendrakar,Ryan T. White</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：active debris removal, On-orbit servicing, inspection of spacecraft, debris removal, active debris</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:On-orbit servicing (OOS), inspection of spacecraft, and active debris removal (ADR). Such missions require precise rendezvous and proximity operations in the vicinity of non-cooperative, possibly unknown, resident space objects. Safety concerns with manned missions and lag times with ground-based control necessitate complete autonomy. In this article, we present an approach for mapping geometries and high-confidence detection of components of unknown, non-cooperative satellites on orbit. We implement accelerated 3D Gaussian splatting to learn a 3D representation of the satellite, render virtual views of the target, and ensemble the YOLOv5 object detector over the virtual views, resulting in reliable, accurate, and precise satellite component detections. The full pipeline capable of running on-board and stand to enable downstream machine intelligence tasks necessary for autonomous guidance, navigation, and control tasks.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2406.02518】DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02518">https://arxiv.org/abs/2406.02518</a></p>
  <p><b>作者</b>：Zhongpai Gao,Benjamin Planche,Meng Zheng,Xiao Chen,Terrence Chen,Ziyan Wu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：physics-based Monte Carlo, Digitally reconstructed radiographs, heavy physics-based Monte, Monte Carlo methods, Monte Carlo</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images generated from 3D CT volumes, widely used in preoperative settings but limited in intraoperative applications due to computational bottlenecks, especially for accurate but heavy physics-based Monte Carlo methods. While analytical DRR renderers offer greater efficiency, they overlook anisotropic X-ray image formation phenomena, such as Compton scattering. We present a novel approach that marries realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS). Our direction-disentangled 3DGS (DDGS) method separates the radiosity contribution into isotropic and direction-dependent components, approximating complex anisotropic interactions without intricate runtime simulations. Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency. Our method outperforms state-of-the-art techniques in image accuracy. Furthermore, our DDGS shows promise for intraoperative applications and inverse problems such as pose registration, delivering superior registration accuracy and runtime performance compared to analytical DRR methods.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2406.02511】V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02511">https://arxiv.org/abs/2406.02511</a></p>
  <p><b>作者</b>：Cong Wang,Kuan Tian,Jun Zhang,Yonghang Guan,Feng Luo,Fei Shen,Zhiwei Jiang,Qing Gu,Xiao Han,Wei Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：increasingly prevalent, portrait video generation, portrait, signals, reference image</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In the field of portrait video generation, the use of single images to generate portrait videos has become increasingly prevalent. A common approach involves leveraging generative models to enhance adapters for controlled generation. However, control signals (e.g., text, audio, reference image, pose, depth map, etc.) can vary in strength. Among these, weaker conditions often struggle to be effective due to interference from stronger conditions, posing a challenge in balancing these conditions. In our work on portrait video generation, we identified audio signals as particularly weak, often overshadowed by stronger signals such as facial pose and reference image. However, direct training with weak signals often leads to difficulties in convergence. To address this, we propose V-Express, a simple method that balances different control signals through the progressive training and the conditional dropout operation. Our method gradually enables effective control by weak conditions, thereby achieving generation capabilities that simultaneously take into account the facial pose, reference image, and audio. The experimental results demonstrate that our method can effectively generate portrait videos controlled by audio. Furthermore, a potential solution is provided for the simultaneous and effective use of conditions of varying strengths.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2406.02509】CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02509">https://arxiv.org/abs/2406.02509</a></p>
  <p><b>作者</b>：Dejia Xu,Weili Nie,Chao Liu,Sifei Liu,Jan Kautz,Zhangyang Wang,Arash Vahdat</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Recently video diffusion, expressive generative tools, content creation readily, high-quality video content, video content creation</p>
  <p><b>备注</b>： Project page: [this https URL](https://ir1d.github.io/CamCo/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users. However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control. To address this issue, we introduce CamCo, which allows fine-grained Camera pose Control for image-to-video generation. We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Plücker coordinates. To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion. Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>14. <b>【2406.02507】Guiding a Diffusion Model with a Bad Version of Itself</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02507">https://arxiv.org/abs/2406.02507</a></p>
  <p><b>作者</b>：Tero Karras,Miika Aittala,Tuomas Kynkäänniemi,Jaakko Lehtinen,Timo Aila,Samuli Laine</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：results align, primary axes, axes of interest, interest in image-generating, class label</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2406.02506】An Open-Source Tool for Mapping War Destruction at Scale in Ukraine using Sentinel-1 Time Series</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02506">https://arxiv.org/abs/2406.02506</a></p>
  <p><b>作者</b>：Olivier Dietrich,Torben Peters,Vivien Sainte Fare Garnot,Valerie Sticher,Thao Ton-That Whelan,Konrad Schindler,Jan Dirk Wegner</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：effectively assist populations, Access to detailed, effectively assist, assist populations, populations most affected</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Access to detailed war impact assessments is crucial for humanitarian organizations to effectively assist populations most affected by armed conflicts. However, maintaining a comprehensive understanding of the situation on the ground is challenging, especially in conflicts that cover vast territories and extend over long periods. This study presents a scalable and transferable method for estimating war-induced damage to buildings. We first train a machine learning model to output pixel-wise probability of destruction from Synthetic Aperture Radar (SAR) satellite image time series, leveraging existing, manual damage assessments as ground truth and cloud-based geospatial analysis tools for large-scale inference. We further post-process these assessments using open building footprints to obtain a final damage estimate per building. We introduce an accessible, open-source tool that allows users to adjust the confidence interval based on their specific requirements and use cases. Our approach enables humanitarian organizations and other actors to rapidly screen large geographic regions for war impacts. We provide two publicly accessible dashboards: a Ukraine Damage Explorer to dynamically view our pre-computed estimates, and a Rapid Damage Mapping Tool to easily run our method and produce custom maps.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2406.02495】GenS: Generalizable Neural Surface Reconstruction from Multi-View Images</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02495">https://arxiv.org/abs/2406.02495</a></p>
  <p><b>作者</b>：Rui Peng,Xiaodong Gu,Luyang Tang,Shihe Shen,Fanqi Yu,Ronggang Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：signed distance function, Combining the signed, differentiable volume rendering, distance function, signed distance</p>
  <p><b>备注</b>： NeurIPS 2023 Accepted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Combining the signed distance function (SDF) and differentiable volume rendering has emerged as a powerful paradigm for surface reconstruction from multi-view images without 3D supervision. However, current methods are impeded by requiring long-time per-scene optimizations and cannot generalize to new scenes. In this paper, we present GenS, an end-to-end generalizable neural surface reconstruction model. Unlike coordinate-based methods that train a separate network for each scene, we construct a generalized multi-scale volume to directly encode all scenes. Compared with existing solutions, our representation is more powerful, which can recover high-frequency details while maintaining global smoothness. Meanwhile, we introduce a multi-scale feature-metric consistency to impose the multi-view consistency in a more discriminative multi-scale feature space, which is robust to the failures of the photometric consistency. And the learnable feature can be self-enhanced to continuously improve the matching accuracy and mitigate aggregation ambiguity. Furthermore, we design a view contrast loss to force the model to be robust to those regions covered by few viewpoints through distilling the geometric prior from dense input to sparse input. Extensive experiments on popular benchmarks show that our model can generalize well to new scenes and outperform existing state-of-the-art methods even those employing ground-truth depth supervision. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2406.02485】Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02485">https://arxiv.org/abs/2406.02485</a></p>
  <p><b>作者</b>：Jiajun Wang,Morteza Ghahremani,Yitong Li,Björn Ommer,Christian Wachinger</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：generating high-quality visual, high-quality visual content, shown impressive performance, shown impressive, generating high-quality</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions. Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures. To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis. We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons. Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the model's precision in capturing intricate pose details. We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around 13% improvement over the established technique ControlNet. The project link and code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2406.02468】DL-KDD: Dual-Light Knowledge Distillation for Action Recognition in the Dark</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02468">https://arxiv.org/abs/2406.02468</a></p>
  <p><b>作者</b>：Chi-Jui Chang,Oscar Tai-Yuan Chen,Vincent S. Tseng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：video, Human action recognition, action recognition, computer vision, original</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Human action recognition in dark videos is a challenging task for computer vision. Recent research focuses on applying dark enhancement methods to improve the visibility of the video. However, such video processing results in the loss of critical information in the original (un-enhanced) video. Conversely, traditional two-stream methods are capable of learning information from both original and processed videos, but it can lead to a significant increase in the computational cost during the inference phase in the task of video classification. To address these challenges, we propose a novel teacher-student video classification framework, named Dual-Light KnowleDge Distillation for Action Recognition in the Dark (DL-KDD). This framework enables the model to learn from both original and enhanced video without introducing additional computational cost during inference. Specifically, DL-KDD utilizes the strategy of knowledge distillation during training. The teacher model is trained with enhanced video, and the student model is trained with both the original video and the soft target generated by the teacher model. This teacher-student framework allows the student model to predict action using only the original input video during inference. In our experiments, the proposed DL-KDD framework outperforms state-of-the-art methods on the ARID, ARID V1.5, and Dark-48 datasets. We achieve the best performance on each dataset and up to a 4.18% improvement on Dark-48, using only original video inputs, thus avoiding the use of two-stream framework or enhancement modules for inference. We further validate the effectiveness of the distillation strategy in ablative experiments. The results highlight the advantages of our knowledge distillation framework in dark human action recognition.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2406.02465】An Empirical Study into Clustering of Unseen Datasets with Self-Supervised Encoders</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02465">https://arxiv.org/abs/2406.02465</a></p>
  <p><b>作者</b>：Scott C. Lowe,Joakim Bruslund Haurum,Sageev Oore,Thomas B. Moeslund,Graham W. Taylor</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：pretrained models generalize, models, models generalize, Abstract, pretrained</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Can pretrained models generalize to new datasets without any retraining? We deploy pretrained image models on datasets they were not trained for, and investigate whether their embeddings form meaningful clusters. Our suite of benchmarking experiments use encoders pretrained solely on ImageNet-1k with either supervised or self-supervised training techniques, deployed on image datasets that were not seen during training, and clustered with conventional clustering algorithms. This evaluation provides new insights into the embeddings of self-supervised models, which prioritize different features to supervised models. Supervised encoders typically offer more utility than SSL encoders within the training domain, and vice-versa far outside of it, however, fine-tuned encoders demonstrate the opposite trend. Clustering provides a way to evaluate the utility of self-supervised learned representations orthogonal to existing methods such as kNN. Additionally, we find the silhouette score when measured in a UMAP-reduced space is highly correlated with clustering performance, and can therefore be used as a proxy for clustering performance on data with no ground truth labels. Our code implementation is available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2406.02462】Learning Image Priors through Patch-based Diffusion Models for Solving Inverse Problems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02462">https://arxiv.org/abs/2406.02462</a></p>
  <p><b>作者</b>：Jason Hu,Bowen Song,Xiaojian Xu,Liyue Shen,Jeffrey A. Fessler</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：underlying data distribution, process is computationally, computationally expensive, expensive and requires, requires lots</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems, but the training process is computationally expensive and requires lots of data. Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images. This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images. Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems. First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiency while still maintaining the capability to generate entire images via positional encoding. Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS). We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors. Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2406.02461】RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02461">https://arxiv.org/abs/2406.02461</a></p>
  <p><b>作者</b>：Qi Wang,Ruijie Lu,Xudong Xu,Jingbo Wang,Michael Yu Wang,Bo Dai,Gang Zeng,Dan Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：advancement of diffusion, diffusion models, models has pushed, pushed the boundary, object generation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The advancement of diffusion models has pushed the boundary of text-to-3D object generation. While it is straightforward to composite objects into a scene with reasonable geometry, it is nontrivial to texture such a scene perfectly due to style inconsistency and occlusions between objects. To tackle these problems, we propose a coarse-to-fine 3D scene texturing framework, referred to as RoomTex, to generate high-fidelity and style-consistent textures for untextured compositional scene meshes. In the coarse stage, RoomTex first unwraps the scene mesh to a panoramic depth map and leverages ControlNet to generate a room panorama, which is regarded as the coarse reference to ensure the global texture consistency. In the fine stage, based on the panoramic image and perspective depth maps, RoomTex will refine and texture every single object in the room iteratively along a series of selected camera views, until this object is completely painted. Moreover, we propose to maintain superior alignment between RGB and depth spaces via subtle edge detection methods. Extensive experiments show our method is capable of generating high-quality and diverse room textures, and more importantly, supporting interactive fine-grained texture control and flexible scene editing thanks to our inpainting-based framework and compositional mesh input. Our project page is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2406.02435】Generative Active Learning for Long-tailed Instance Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02435">https://arxiv.org/abs/2406.02435</a></p>
  <p><b>作者</b>：Muzhi Zhu,Chengxiang Fan,Hao Chen,Yang Liu,Weian Mao,Xiaogang Xu,Chunhua Shen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：large-scale language-image generative, gained widespread attention, language-image generative models, generated data, large-scale language-image</p>
  <p><b>备注</b>： Accepted by ICML 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, large-scale language-image generative models have gained widespread attention and many works have utilized generated data from these models to further enhance the performance of perception tasks. However, not all generated data can positively impact downstream models, and these methods do not thoroughly explore how to better select and utilize generated data. On the other hand, there is still a lack of research oriented towards active learning on generated data. In this paper, we explore how to perform active learning specifically for generated data in the long-tailed instance segmentation task. Subsequently, we propose BSGAL, a new algorithm that online estimates the contribution of the generated data based on gradient cache. BSGAL can handle unlimited generated data and complex downstream segmentation tasks effectively. Experiments show that BSGAL outperforms the baseline approach and effectually improves the performance of long-tailed segmentation. Our code can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>23. <b>【2406.02425】CoNav: A Benchmark for Human-Centered Collaborative Navigation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02425">https://arxiv.org/abs/2406.02425</a></p>
  <p><b>作者</b>：Changhao Li,Xinyu Sun,Peihao Chen,Jugang Fan,Zixu Wang,Yanxia Liu,Jinhui Zhu,Chuang Gan,Mingkui Tan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：robot intelligently assists, Human-robot collaboration, appealing objective, human, robot intelligently</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Human-robot collaboration, in which the robot intelligently assists the human with the upcoming task, is an appealing objective. To achieve this goal, the agent needs to be equipped with a fundamental collaborative navigation ability, where the agent should reason human intention by observing human activities and then navigate to the human's intended destination in advance of the human. However, this vital ability has not been well studied in previous literature. To fill this gap, we propose a collaborative navigation (CoNav) benchmark. Our CoNav tackles the critical challenge of constructing a 3D navigation environment with realistic and diverse human activities. To achieve this, we design a novel LLM-based humanoid animation generation framework, which is conditioned on both text descriptions and environmental context. The generated humanoid trajectory obeys the environmental context and can be easily integrated into popular simulators. We empirically find that the existing navigation methods struggle in CoNav task since they neglect the perception of human intention. To solve this problem, we propose an intention-aware agent for reasoning both long-term and short-term human intention. The agent predicts navigation action based on the predicted intention and panoramic observation. The emergent agent behavior including observing humans, avoiding human collision, and navigation reveals the efficiency of the proposed datasets and agents.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2406.02411】Decoupling of neural network calibration measures</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02411">https://arxiv.org/abs/2406.02411</a></p>
  <p><b>作者</b>：Dominik Werner Wolf,Prasannavenkatesh Balaji,Alexander Braun,Markus Ulrich</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：autonomous driving systems, safeguarding autonomous driving, deep neural networks, Uncertainty Calibration Error, neural network calibration</p>
  <p><b>备注</b>： Submitted to the German Conference on Pattern Recognition (GCPR) 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:A lot of effort is currently invested in safeguarding autonomous driving systems, which heavily rely on deep neural networks for computer vision. We investigate the coupling of different neural network calibration measures with a special focus on the Area Under the Sparsification Error curve (AUSE) metric. We elaborate on the well-known inconsistency in determining optimal calibration using the Expected Calibration Error (ECE) and we demonstrate similar issues for the AUSE, the Uncertainty Calibration Score (UCS), as well as the Uncertainty Calibration Error (UCE). We conclude that the current methodologies leave a degree of freedom, which prevents a unique model calibration for the homologation of safety-critical functionalities. Furthermore, we propose the AUSE as an indirect measure for the residual uncertainty, which is irreducible for a fixed network architecture and is driven by the stochasticity in the underlying data generation process (aleatoric contribution) as well as the limitation in the hypothesis space (epistemic contribution).</p>
  </details>
</details>
<details>
  <summary>25. <b>【2406.02407】WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo Collections</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02407">https://arxiv.org/abs/2406.02407</a></p>
  <p><b>作者</b>：Yuze Wang,Junyi Wang,Yue Qi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：unconstrained photo collections, computer graphics, challenging in computer, photo collections, Gaussian Splatting</p>
  <p><b>备注</b>： Our project page is available at [this https URL](https://yuzewang1998.github.io/we-gs.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Novel View Synthesis (NVS) from unconstrained photo collections is challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has shown promise for photorealistic and real-time NVS of static scenes. Building on 3DGS, we propose an efficient point-based differentiable rendering framework for scene reconstruction from photo collections. Our key innovation is a residual-based spherical harmonic coefficients transfer module that adapts 3DGS to varying lighting conditions and photometric post-processing. This lightweight module can be pre-computed and ensures efficient gradient propagation from rendered images to 3D Gaussian attributes. Additionally, we observe that the appearance encoder and the transient mask predictor, the two most critical parts of NVS from unconstrained photo collections, can be mutually beneficial. We introduce a plug-and-play lightweight spatial attention module to simultaneously predict transient occluders and latent appearance representation for each image. After training and preprocessing, our method aligns with the standard 3DGS format and rendering pipeline, facilitating seamlessly integration into various 3DGS applications. Extensive experiments on diverse datasets show our approach outperforms existing approaches on the rendering quality of novel view and appearance synthesis with high converge and rendering speed.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2406.02395】GrootVL: Tree Topology is All You Need in State Space Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02395">https://arxiv.org/abs/2406.02395</a></p>
  <p><b>作者</b>：Yicheng Xiao,Lin Song,Shaoli Huang,Jiangshan Wang,Siyu Song,Yixiao Ge,Xiu Li,Ying Shan</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：employing recursively propagated, comparable to Transformer, recursively propagated features, employing recursively, superior efficiency</p>
  <p><b>备注</b>： The code is available at [this https URL](https://github.com/EasonXiao-888/GrootVL) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The state space models, employing recursively propagated features, demonstrate strong representation capabilities comparable to Transformer models and superior efficiency. However, constrained by the inherent geometric constraints of sequences, it still falls short in modeling long-range dependencies. To address this issue, we propose the GrootVL network, which first dynamically generates a tree topology based on spatial relationships and input features. Then, feature propagation is performed based on this graph, thereby breaking the original sequence constraints to achieve stronger representation capabilities. Additionally, we introduce a linear complexity dynamic programming algorithm to enhance long-range interactions without increasing computational cost. GrootVL is a versatile multimodal framework that can be applied to both visual and textual tasks. Extensive experiments demonstrate that our method significantly outperforms existing structured state space models on image classification, object detection and segmentation. Besides, by fine-tuning large language models, our approach achieves consistent improvements in multiple textual tasks at minor training cost.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2406.02385】Low-Rank Adaption on Transformer-based Oriented Object Detector for Satellite Onboard Processing of Remote Sensing Images</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02385">https://arxiv.org/abs/2406.02385</a></p>
  <p><b>作者</b>：Xinyang Pu,Feng Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Deep learning models, remote sensing images, Deep learning, conserving communication resources, satellite onboard real-time</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deep learning models in satellite onboard enable real-time interpretation of remote sensing images, reducing the need for data transmission to the ground and conserving communication resources. As satellite numbers and observation frequencies increase, the demand for satellite onboard real-time image interpretation grows, highlighting the expanding importance and development of this technology. However, updating the extensive parameters of models deployed on the satellites for spaceborne object detection model is challenging due to the limitations of uplink bandwidth in wireless satellite communications. To address this issue, this paper proposes a method based on parameter-efficient fine-tuning technology with low-rank adaptation (LoRA) module. It involves training low-rank matrix parameters and integrating them with the original model's weight matrix through multiplication and summation, thereby fine-tuning the model parameters to adapt to new data distributions with minimal weight updates. The proposed method combines parameter-efficient fine-tuning with full fine-tuning in the parameter update strategy of the oriented object detection algorithm architecture. This strategy enables model performance improvements close to full fine-tuning effects with minimal parameter updates. In addition, low rank approximation is conducted to pick an optimal rank value for LoRA matrices. Extensive experiments verify the effectiveness of the proposed method. By fine-tuning and updating only 12.4$\%$ of the model's total parameters, it is able to achieve 97$\%$ to 100$\%$ of the performance of full fine-tuning models. Additionally, the reduced number of trainable parameters accelerates model training iterations and enhances the generalization and robustness of the oriented object detection model. The source code is available at: \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2406.02383】Learning to Edit Visual Programs with Self-Supervision</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02383">https://arxiv.org/abs/2406.02383</a></p>
  <p><b>作者</b>：R. Kenny Jones,Renhao Zhang,Aditya Ganeshan,Daniel Ritchie</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：design a system, system that learns, edit network, edit, visual programs</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We design a system that learns how to edit visual programs. Our edit network consumes a complete input program and a visual target. From this input, we task our network with predicting a local edit operation that could be applied to the input program to improve its similarity to the target. In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in one-shot. Our joint finetuning scheme, when coupled with an inference procedure that initializes a population from the one-shot model and evolves members of this population with the edit network, helps to infer more accurate visual programs. Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages.</p>
  </details>
</details>
<details>
  <summary>29. <b>【2406.02380】EUFCC-340K: A Faceted Hierarchical Dataset for Metadata Annotation in GLAM Collections</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02380">https://arxiv.org/abs/2406.02380</a></p>
  <p><b>作者</b>：Francesc Net,Marc Folia,Pep Casals,Andrew D. Bagdanov,Lluis Gomez</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：automatic metadata annotation, Art Architecture Thesaurus, Europeana portal, domain of Galleries, address the challenges</p>
  <p><b>备注</b>： 23 pages, 13 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we address the challenges of automatic metadata annotation in the domain of Galleries, Libraries, Archives, and Museums (GLAMs) by introducing a novel dataset, EUFCC340K, collected from the Europeana portal. Comprising over 340,000 images, the EUFCC340K dataset is organized across multiple facets: Materials, Object Types, Disciplines, and Subjects, following a hierarchical structure based on the Art  Architecture Thesaurus (AAT). We developed several baseline models, incorporating multiple heads on a ConvNeXT backbone for multi-label image tagging on these facets, and fine-tuning a CLIP model with our image text pairs. Our experiments to evaluate model robustness and generalization capabilities in two different test scenarios demonstrate the utility of the dataset in improving multi-label classification tools that have the potential to alleviate cataloging tasks in the cultural heritage sector.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2406.02355】FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02355">https://arxiv.org/abs/2406.02355</a></p>
  <p><b>作者</b>：Seongyoon Kim,Minchan Jeong,Sungnyun Kim,Sungwoo Cho,Sumyeong Ahn,Se-Young Yun</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Federated Learning, non-iid data distribution, pivotal framework, non-iid data, Federated</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Federated Learning (FL) has emerged as a pivotal framework for the development of effective global models (global FL) or personalized models (personalized FL) across clients with heterogeneous, non-iid data distribution. A key challenge in FL is client drift, where data heterogeneity impedes the aggregation of scattered knowledge. Recent studies have tackled the client drift issue by identifying significant divergence in the last classifier layer. To mitigate this divergence, strategies such as freezing the classifier weights and aligning the feature extractor accordingly have proven effective. Although the local alignment between classifier and feature extractor has been studied as a crucial factor in FL, we observe that it may lead the model to overemphasize the observed classes within each client. Thus, our objectives are twofold: (1) enhancing local alignment while (2) preserving the representation of unseen class samples. This approach aims to effectively integrate knowledge from individual clients, thereby improving performance for both global and personalized FL. To achieve this, we introduce a novel algorithm named FedDr+, which empowers local model alignment using dot-regression loss. FedDr+ freezes the classifier as a simplex ETF to align the features and improves aggregated global models by employing a feature distillation mechanism to retain information about unseen/missing classes. Consequently, we provide empirical evidence demonstrating that our algorithm surpasses existing methods that use a frozen classifier to boost alignment across the diverse distribution.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2406.02349】CADE: Cosine Annealing Differential Evolution for Spiking Neural Network</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02349">https://arxiv.org/abs/2406.02349</a></p>
  <p><b>作者</b>：Runhua Jiang,Guodong Du,Shuyang Yu,Yifei Guo,Sim Kuan Goh,Ho-Kin Tang</p>
  <p><b>类目</b>：Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Spiking neural networks, Spiking Element Wise, energy-efficient artificial intelligence, Annealing Differential Evolution, Cosine Annealing Differential</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Spiking neural networks (SNNs) have gained prominence for their potential in neuromorphic computing and energy-efficient artificial intelligence, yet optimizing them remains a formidable challenge for gradient-based methods due to their discrete, spike-based computation. This paper attempts to tackle the challenges by introducing Cosine Annealing Differential Evolution (CADE), designed to modulate the mutation factor (F) and crossover rate (CR) of differential evolution (DE) for the SNN model, i.e., Spiking Element Wise (SEW) ResNet. Extensive empirical evaluations were conducted to analyze CADE. CADE showed a balance in exploring and exploiting the search space, resulting in accelerated convergence and improved accuracy compared to existing gradient-based and DE-based methods. Moreover, an initialization method based on a transfer learning setting was developed, pretraining on a source dataset (i.e., CIFAR-10) and fine-tuning the target dataset (i.e., CIFAR-100), to improve population diversity. It was found to further enhance CADE for SNN. Remarkably, CADE elevates the performance of the highest accuracy SEW model by an additional 0.52 percentage points, underscoring its effectiveness in fine-tuning and enhancing SNNs. These findings emphasize the pivotal role of a scheduler for F and CR adjustment, especially for DE-based SNN. Source Code on Github: this https URL.</p>
  </details>
</details>
<details>
  <summary>32. <b>【2406.02347】Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02347">https://arxiv.org/abs/2406.02347</a></p>
  <p><b>作者</b>：Clement Chadebec,Onur Tasar,Eyal Benaroche,Benjamin Aubin</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：pre-trained diffusion models, Flash Diffusion, versatile distillation method, diffusion models, pre-trained diffusion</p>
  <p><b>备注</b>： 16 pages + 16 pages appendices</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we propose an efficient, fast, and versatile distillation method to accelerate the generation of pre-trained diffusion models: Flash Diffusion. The method reaches state-of-the-art performances in terms of FID and CLIP-Score for few steps image generation on the COCO2014 and COCO2017 datasets, while requiring only several GPU hours of training and fewer trainable parameters than existing methods. In addition to its efficiency, the versatility of the method is also exposed across several tasks such as text-to-image, inpainting, face-swapping, super-resolution and using different backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\alpha$), as well as adapters. In all cases, the method allowed to reduce drastically the number of sampling steps while maintaining very high-quality image generation. The official implementation is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2406.02345】Progressive Confident Masking Attention Network for Audio-Visual Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02345">https://arxiv.org/abs/2406.02345</a></p>
  <p><b>作者</b>：Yuxuan Wang,Feng Dong,Jinchao Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：typically occur simultaneously, signals typically occur, occur simultaneously, typically occur, humans possess</p>
  <p><b>备注</b>： 10 pages, 9 figures, submitted to IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Audio and visual signals typically occur simultaneously, and humans possess an innate ability to correlate and synchronize information from these two modalities. Recently, a challenging problem known as Audio-Visual Segmentation (AVS) has emerged, intending to produce segmentation maps for sounding objects within a scene. However, the methods proposed so far have not sufficiently integrated audio and visual information, and the computational costs have been extremely high. Additionally, the outputs of different stages have not been fully utilized. To facilitate this research, we introduce a novel Progressive Confident Masking Attention Network (PMCANet). It leverages attention mechanisms to uncover the intrinsic correlations between audio signals and visual frames. Furthermore, we design an efficient and effective cross-attention module to enhance semantic perception by selecting query tokens. This selection is determined through confidence-driven units based on the network's multi-stage predictive outputs. Experiments demonstrate that our network outperforms other AVS methods while requiring less computational resources.</p>
  </details>
</details>
<details>
  <summary>34. <b>【2406.02343】Cluster-Aware Similarity Diffusion for Instance Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02343">https://arxiv.org/abs/2406.02343</a></p>
  <p><b>作者</b>：Jifei Luo,Hantao Yao,Changsheng Xu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Diffusion-based re-ranking, performing similarity propagation, nearest neighbor graph, common method, similarity</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion-based re-ranking is a common method used for retrieving instances by performing similarity propagation in a nearest neighbor graph. However, existing techniques that construct the affinity graph based on pairwise instances can lead to the propagation of misinformation from outliers and other manifolds, resulting in inaccurate results. To overcome this issue, we propose a novel Cluster-Aware Similarity (CAS) diffusion for instance retrieval. The primary concept of CAS is to conduct similarity diffusion within local clusters, which can reduce the influence from other manifolds explicitly. To obtain a symmetrical and smooth similarity matrix, our Bidirectional Similarity Diffusion strategy introduces an inverse constraint term to the optimization objective of local cluster diffusion. Additionally, we have optimized a Neighbor-guided Similarity Smoothing approach to ensure similarity consistency among the local neighbors of each instance. Evaluations in instance retrieval and object re-identification validate the effectiveness of the proposed CAS, our code is publicly available.</p>
  </details>
</details>
<details>
  <summary>35. <b>【2406.02327】Continual Unsupervised Out-of-Distribution Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02327">https://arxiv.org/abs/2406.02327</a></p>
  <p><b>作者</b>：Lars Doorenbos,Raphael Sznitman,Pablo Márquez-Neila</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：testing data, OOD, learning models excel, aligns with testing, Deep learning models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deep learning models excel when the data distribution during training aligns with testing data. Yet, their performance diminishes when faced with out-of-distribution (OOD) samples, leading to great interest in the field of OOD detection. Current approaches typically assume that OOD samples originate from an unconcentrated distribution complementary to the training distribution. While this assumption is appropriate in the traditional unsupervised OOD (U-OOD) setting, it proves inadequate when considering the place of deployment of the underlying deep learning model. To better reflect this real-world scenario, we introduce the novel setting of continual U-OOD detection. To tackle this new setting, we propose a method that starts from a U-OOD detector, which is agnostic to the OOD distribution, and slowly updates during deployment to account for the actual OOD distribution. Our method uses a new U-OOD scoring function that combines the Mahalanobis distance with a nearest-neighbor approach. Furthermore, we design a confidence-scaled few-shot OOD detector that outperforms previous methods. We show our method greatly improves upon strong baselines from related fields.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2406.02287】Optimised ProPainter for Video Diminished Reality Inpainting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02287">https://arxiv.org/abs/2406.02287</a></p>
  <p><b>作者</b>：Pengze Li,Lihao Liu,Carola-Bibiane Schönlieb,Angelica I Aviles-Rivero</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：inpainting technique optimised, DREAMING Challenge, refined video inpainting, video inpainting technique, Reality for Emerging</p>
  <p><b>备注</b>： Accepted to ISBI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, part of the DREAMING Challenge - Diminished Reality for Emerging Applications in Medicine through Inpainting, we introduce a refined video inpainting technique optimised from the ProPainter method to meet the specialised demands of medical imaging, specifically in the context of oral and maxillofacial surgery. Our enhanced algorithm employs the zero-shot ProPainter, featuring optimized parameters and pre-processing, to adeptly manage the complex task of inpainting surgical video sequences, without requiring any training process. It aims to produce temporally coherent and detail-rich reconstructions of occluded regions, facilitating clearer views of operative fields. The efficacy of our approach is evaluated using comprehensive metrics, positioning it as a significant advancement in the application of diminished reality for medical purposes.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2406.02265】Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02265">https://arxiv.org/abs/2406.02265</a></p>
  <p><b>作者</b>：Wenyan Li,Jiaang Li,Rita Ramos,Raphael Tang,Desmond Elliott</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：strong domain-transfer capabilities, Recent advancements, image captioning highlight, retrieving related captions, domain-transfer capabilities</p>
  <p><b>备注</b>： 9 pages, long paper at ACL 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in retrieval-augmented models for image captioning highlight the significance of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice. Retrieved information can sometimes mislead the model generation, negatively impacting performance. In this paper, we analyze the robustness of the SmallCap retrieval-augmented captioning model. Our analysis shows that SmallCap is sensitive to tokens that appear in the majority of the retrieved captions, and integrated gradients attribution shows that those tokens are likely copied into the final caption. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This reduces the probability that the model learns to copy majority tokens and improves both in-domain and cross-domain performance effectively.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2406.02264】Image contrast enhancement based on the Schr\"odinger operator spectrum</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02264">https://arxiv.org/abs/2406.02264</a></p>
  <p><b>作者</b>：Juan M. Vargas,Taous-Meriem Laleg-Kirati</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：dimensional Schrödinger operator, Schrödinger operator, dimensional Schrödinger, gamma, enhancement method based</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study proposes a novel image contrast enhancement method based on image projection onto the squared eigenfunctions of the two dimensional Schrödinger operator. This projection depends on a design parameter \texorpdfstring{\(\gamma\)}{gamma} which is proposed to control the pixel intensity during image reconstruction. The performance of the proposed method is investigated through its application to color images. The selection of \texorpdfstring{\(\gamma\)}{gamma} values is performed using k-means, which helps preserve the image spatial adjacency information. Furthermore, multi-objective optimization using the Non dominated Sorting Genetic Algorithm II (NSAG2) algorithm is proposed to select the optimal values of \texorpdfstring{\(\gamma\)}{gamma} and the semi-classical parameter h from the 2DSCSA. The results demonstrate the effectiveness of the proposed method for enhancing image contrast while preserving the inherent characteristics of the original image, producing the desired enhancement with almost no artifacts.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2406.02263】M3DM-NR: RGB-3D Noisy-Resistant Industrial Anomaly Detection via Multimodal Denoising</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02263">https://arxiv.org/abs/2406.02263</a></p>
  <p><b>作者</b>：Chengjie Wang,Haokun Zhu,Jinlong Peng,Yue Wang,Ran Yi,Yunsheng Wu,Lizhuang Ma,Jiangning Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Existing industrial anomaly, pristine RGB images, Existing industrial, Suspected Anomaly Map, anomaly detection</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing industrial anomaly detection methods primarily concentrate on unsupervised learning with pristine RGB images. Yet, both RGB and 3D data are crucial for anomaly detection, and the datasets are seldom completely clean in practical scenarios. To address above challenges, this paper initially delves into the RGB-3D multi-modal noisy anomaly detection, proposing a novel noise-resistant M3DM-NR framework to leveraging strong multi-modal discriminative capabilities of CLIP. M3DM-NR consists of three stages: Stage-I introduces the Suspected References Selection module to filter a few normal samples from the training dataset, using the multimodal features extracted by the Initial Feature Extraction, and a Suspected Anomaly Map Computation module to generate a suspected anomaly map to focus on abnormal regions as reference. Stage-II uses the suspected anomaly maps of the reference samples as reference, and inputs image, point cloud, and text information to achieve denoising of the training samples through intra-modal comparison and multi-scale aggregation operations. Finally, Stage-III proposes the Point Feature Alignment, Unsupervised Feature Fusion, Noise Discriminative Coreset Selection, and Decision Layer Fusion modules to learn the pattern of the training dataset, enabling anomaly detection and segmentation while filtering out noise. Extensive experiments show that M3DM-NR outperforms state-of-the-art methods in 3D-RGB multi-modal noisy anomaly detection.</p>
  </details>
</details>
<details>
  <summary>40. <b>【2406.02253】PuFace: Defending against Facial Cloaking Attacks for Facial Recognition Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02253">https://arxiv.org/abs/2406.02253</a></p>
  <p><b>作者</b>：Jing Wen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)</p>
  <p><b>关键词</b>：add invisible perturbation, facial recognition models, recently proposed facial, attacks add invisible, unauthorized facial recognition</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The recently proposed facial cloaking attacks add invisible perturbation (cloaks) to facial images to protect users from being recognized by unauthorized facial recognition models. However, we show that the "cloaks" are not robust enough and can be removed from images.
This paper introduces PuFace, an image purification system leveraging the generalization ability of neural networks to diminish the impact of cloaks by pushing the cloaked images towards the manifold of natural (uncloaked) images before the training process of facial recognition models. Specifically, we devise a purifier that takes all the training images including both cloaked and natural images as input and generates the purified facial images close to the manifold where natural images lie. To meet the defense goal, we propose to train the purifier on particularly amplified cloaked images with a loss function that combines image loss and feature loss. Our empirical experiment shows PuFace can effectively defend against two state-of-the-art facial cloaking attacks and reduces the attack success rate from 69.84\% to 7.61\% on average without degrading the normal accuracy for various facial recognition models. Moreover, PuFace is a model-agnostic defense mechanism that can be applied to any facial recognition model without modifying the model structure.
</p><p>Subjects:</p>
<p>Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>)</p>
<p>Cite as:<br>
arXiv:2406.02253 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]</p>
<p>(or<br>
arXiv:2406.02253v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2406.02253">https://doi.org/10.48550/arXiv.2406.02253</a></p>
<p>Focus to learn more</p>
<pre><code>            arXiv-issued DOI via DataCite&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>41. <b>【2406.02230】I4VGen: Image as Stepping Stone for Text-to-Video Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02230">https://arxiv.org/abs/2406.02230</a></p>
  <p><b>作者</b>：Xiefan Guo,Jinlin Liu,Miaomiao Cui,Di Huang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：limited video-text datasets, video-text datasets, diversity due, complexity of spatio-temporal, spatio-temporal modeling</p>
  <p><b>备注</b>： Project page: [this https URL](https://xiefan-guo.github.io/i4vgen) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text-to-video generation has lagged behind text-to-image synthesis in quality and diversity due to the complexity of spatio-temporal modeling and limited video-text datasets. This paper presents I4VGen, a training-free and plug-and-play video diffusion inference framework, which enhances text-to-video generation by leveraging robust image techniques. Specifically, following text-to-image-to-video, I4VGen decomposes the text-to-video generation into two stages: anchor image synthesis and anchor image-guided video synthesis. Correspondingly, a well-designed generation-selection pipeline is employed to achieve visually-realistic and semantically-faithful anchor image, and an innovative Noise-Invariant Video Score Distillation Sampling is incorporated to animate the image to a dynamic video, followed by a video regeneration process to refine the video. This inference strategy effectively mitigates the prevalent issue of non-zero terminal signal-to-noise ratio. Extensive evaluations show that I4VGen not only produces videos with higher visual realism and textual fidelity but also integrates seamlessly into existing image-to-video diffusion models, thereby improving overall video quality.</p>
  </details>
</details>
<details>
  <summary>42. <b>【2406.02223】SMCL: Saliency Masked Contrastive Learning for Long-tailed Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02223">https://arxiv.org/abs/2406.02223</a></p>
  <p><b>作者</b>：Sanglee Park,Seung-won Hwang,Jungmin So</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Real-world data, high imbalance, Real-world, contrastive learning, classes</p>
  <p><b>备注</b>： accepted at ICASSP 2023</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Real-world data often follow a long-tailed distribution with a high imbalance in the number of samples between classes. The problem with training from imbalanced data is that some background features, common to all classes, can be unobserved in classes with scarce samples. As a result, this background correlates to biased predictions into ``major" classes. In this paper, we propose saliency masked contrastive learning, a new method that uses saliency masking and contrastive learning to mitigate the problem and improve the generalizability of a model. Our key idea is to mask the important part of an image using saliency detection and use contrastive learning to move the masked image towards minor classes in the feature space, so that background features present in the masked image are no longer correlated with the original class. Experiment results show that our method achieves state-of-the-art level performance on benchmark long-tailed datasets.</p>
  </details>
</details>
<details>
  <summary>43. <b>【2406.02208】Why Only Text: Empowering Vision-and-Language Navigation with Multi-modal Prompts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02208">https://arxiv.org/abs/2406.02208</a></p>
  <p><b>作者</b>：Haodong Hong,Sen Wang,Zi Huang,Qi Wu,Jiajun Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：employ textual instructions, Current, Prompts, employ textual, textual instructions</p>
  <p><b>备注</b>： IJCAI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Current Vision-and-Language Navigation (VLN) tasks mainly employ textual instructions to guide agents. However, being inherently abstract, the same textual instruction can be associated with different visual signals, causing severe ambiguity and limiting the transfer of prior knowledge in the vision domain from the user to the agent. To fill this gap, we propose Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task augmenting traditional VLN by integrating both natural language and images in instructions. VLN-MP not only maintains backward compatibility by effectively handling text-only prompts but also consistently shows advantages with different quantities and relevance of visual prompts. Possible forms of visual prompts include both exact and similar object images, providing adaptability and versatility in diverse navigation scenarios. To evaluate VLN-MP under a unified framework, we implement a new benchmark that offers: (1) a training-free pipeline to transform textual instructions into multi-modal forms with landmark images; (2) diverse datasets with multi-modal instructions for different downstream tasks; (3) a novel module designed to process various image prompts for seamless integration with state-of-the-art VLN models. Extensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show that incorporating visual prompts significantly boosts navigation performance. While maintaining efficiency with text-only prompts, VLN-MP enables agents to navigate in the pre-explore setting and outperform text-based models, showing its broader applicability.</p>
  </details>
</details>
<details>
  <summary>44. <b>【2406.02202】Can CLIP help CLIP in learning 3D?</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02202">https://arxiv.org/abs/2406.02202</a></p>
  <p><b>作者</b>：Cristian Sbrolli,Matteo Matteucci</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：explore an alternative, textual descriptions, leverage CLIP knowledge, enhance contrastive, alternative approach</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this study, we explore an alternative approach to enhance contrastive text-image-3D alignment in the absence of textual descriptions for 3D objects. We introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP knowledge about textual and 2D data to compute the neural perceived similarity between two 3D samples. We employ the proposed methods to mine 3D hard negatives, establishing a multimodal contrastive pipeline with hard negative weighting via a custom loss function. We train on different configurations of the proposed hard negative mining approach, and we evaluate the accuracy of our models in 3D classification and on the cross-modal retrieval benchmark, testing image-to-shape and shape-to-image retrieval. Results demonstrate that our approach, even without explicit text alignment, achieves comparable or superior performance on zero-shot and standard 3D classification, while significantly improving both image-to-shape and shape-to-image retrieval compared to previous methods.</p>
  </details>
</details>
<details>
  <summary>45. <b>【2406.02184】GraVITON: Graph based garment warping with attention guided inversion for Virtual-tryon</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02184">https://arxiv.org/abs/2406.02184</a></p>
  <p><b>作者</b>：Sanhita Pathak,Vinay Kaushik,Brejesh Lall</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：rapidly evolving field, improving customer experiences, computer vision, human body, rapidly evolving</p>
  <p><b>备注</b>： 18 pages, 7 Figures and 6 Tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Virtual try-on, a rapidly evolving field in computer vision, is transforming e-commerce by improving customer experiences through precise garment warping and seamless integration onto the human body. While existing methods such as TPS and flow address the garment warping but overlook the finer contextual details. In this paper, we introduce a novel graph based warping technique which emphasizes the value of context in garment flow. Our graph based warping module generates warped garment as well as a coarse person image, which is utilised by a simple refinement network to give a coarse virtual tryon image. The proposed work exploits latent diffusion model to generate the final tryon, treating garment transfer as an inpainting task. The diffusion model is conditioned with decoupled cross attention based inversion of visual and textual information. We introduce an occlusion aware warping constraint that generates dense warped garment, without any holes and occlusion. Our method, validated on VITON-HD and Dresscode datasets, showcases substantial state-of-the-art qualitative and quantitative results showing considerable improvement in garment warping, texture preservation, and overall realism.</p>
  </details>
</details>
<details>
  <summary>46. <b>【2406.02158】Radar Spectra-Language Model for Automotive Scene Parsing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02158">https://arxiv.org/abs/2406.02158</a></p>
  <p><b>作者</b>：Mariia Pushkareva,Yuri Feldman,Csaba Domokos,Kilian Rambach,Dotan Di Castro</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Radar, radar spectra, low cost, sensors are low, spectra</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Radar sensors are low cost, long-range, and weather-resilient. Therefore, they are widely used for driver assistance functions, and are expected to be crucial for the success of autonomous driving in the future. In many perception tasks only pre-processed radar point clouds are considered. In contrast, radar spectra are a raw form of radar measurements and contain more information than radar point clouds. However, radar spectra are rather difficult to interpret. In this work, we aim to explore the semantic information contained in spectra in the context of automated driving, thereby moving towards better interpretability of radar spectra. To this end, we create a radar spectra-language model, allowing us to query radar spectra measurements for the presence of scene elements using free text. We overcome the scarcity of radar spectra data by matching the embedding space of an existing vision-language model (VLM). Finally, we explore the benefit of the learned representation for scene parsing, and obtain improvements in free space segmentation and object detection merely by injecting the spectra embedding into a baseline model.</p>
  </details>
</details>
<details>
  <summary>47. <b>【2406.02153】Analyzing the Feature Extractor Networks for Face Image Synthesis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02153">https://arxiv.org/abs/2406.02153</a></p>
  <p><b>作者</b>：Erdi Sarıtaş,Hazım Kemal Ekenel</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Advancements like Generative, Generative Adversarial, Adversarial Networks, Networks have attracted</p>
  <p><b>备注</b>： Accepted at 18th International Conference on Automatic Face and Gesture Recognition (FG) on 1st SD-FGA Workshop 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Advancements like Generative Adversarial Networks have attracted the attention of researchers toward face image synthesis to generate ever more realistic images. Thereby, the need for the evaluation criteria to assess the realism of the generated images has become apparent. While FID utilized with InceptionV3 is one of the primary choices for benchmarking, concerns about InceptionV3's limitations for face images have emerged. This study investigates the behavior of diverse feature extractors -- InceptionV3, CLIP, DINOv2, and ArcFace -- considering a variety of metrics -- FID, KID, Precision\Recall. While the FFHQ dataset is used as the target domain, as the source domains, the CelebA-HQ dataset and the synthetic datasets generated using StyleGAN2 and Projected FastGAN are used. Experiments include deep-down analysis of the features: $L_2$ normalization, model attention during extraction, and domain distributions in the feature space. We aim to give valuable insights into the behavior of feature extractors for evaluating face image synthesis methodologies. The code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>48. <b>【2406.02147】UA-Track: Uncertainty-Aware End-to-End 3D Multi-Object Tracking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02147">https://arxiv.org/abs/2406.02147</a></p>
  <p><b>作者</b>：Lijun Zhou,Tao Tang,Pengkun Hao,Zihang He,Kalok Ho,Shuo Gu,Wenbo Hou,Zhihui Hao,Haiyang Sun,Kun Zhan,Peng Jia,Xianpeng Lang,Xiaodan Liang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：autonomous driving perception, plays a crucial, driving perception, multiple object tracking, crucial role</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception. Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task. However, existing methods overlook the uncertainty issue, which refers to the lack of precise confidence about the state and location of tracked objects. Uncertainty arises owing to various factors during motion observation by cameras, especially occlusions and the small size of target objects, resulting in an inaccurate estimation of the object's position, label, and identity. To this end, we propose an Uncertainty-Aware 3D MOT framework, UA-Track, which tackles the uncertainty problem from multiple aspects. Specifically, we first introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty in object prediction with probabilistic attention. Secondly, we propose an Uncertainty-guided Query Denoising strategy to further enhance the training process. We also utilize Uncertainty-reduced Query Initialization, which leverages predicted 2D object location and depth information to reduce query uncertainty. As a result, our UA-Track achieves state-of-the-art performance on the nuScenes benchmark, i.e., 66.3% AMOTA on the test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA.</p>
  </details>
</details>
<details>
  <summary>49. <b>【2406.02142】Analyzing the Effect of Combined Degradations on Face Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02142">https://arxiv.org/abs/2406.02142</a></p>
  <p><b>作者</b>：Erdi Sarıtaş,Hazım Kemal Ekenel</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：controlled environments, typically trained, trained on large, collected from controlled, real-world</p>
  <p><b>备注</b>： Accepted at 18th International Conference on Automatic Face and Gesture Recognition (FG) on 2nd PrivAAL Workshop 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:A face recognition model is typically trained on large datasets of images that may be collected from controlled environments. This results in performance discrepancies when applied to real-world scenarios due to the domain gap between clean and in-the-wild images. Therefore, some researchers have investigated the robustness of these models by analyzing synthetic degradations. Yet, existing studies have mostly focused on single degradation factors, which may not fully capture the complexity of real-world degradations. This work addresses this problem by analyzing the impact of both single and combined degradations using a real-world degradation pipeline extended with under/over-exposure conditions. We use the LFW dataset for our experiments and assess the model's performance based on verification accuracy. Results reveal that single and combined degradations show dissimilar model behavior. The combined effect of degradation significantly lowers performance even if its single effect is negligible. This work emphasizes the importance of accounting for real-world complexity to assess the robustness of face recognition models in real-world settings. The code is publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>50. <b>【2406.02125】Domain Game: Disentangle Anatomical Feature for Single Domain Generalized Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02125">https://arxiv.org/abs/2406.02125</a></p>
  <p><b>作者</b>：Hao Chen,Hongrun Zhang,U Wang Chan,Rui Yin,Xiaofei Wang,Chao Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Single domain generalization, domain generalization aims, Single domain, generalization aims, generalization problem</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Single domain generalization aims to address the challenge of out-of-distribution generalization problem with only one source domain available. Feature distanglement is a classic solution to this purpose, where the extracted task-related feature is presumed to be resilient to domain shift. However, the absence of references from other domains in a single-domain scenario poses significant uncertainty in feature disentanglement (ill-posedness). In this paper, we propose a new framework, named \textit{Domain Game}, to perform better feature distangling for medical image segmentation, based on the observation that diagnostic relevant features are more sensitive to geometric transformations, whilist domain-specific features probably will remain invariant to such operations. In domain game, a set of randomly transformed images derived from a singular source image is strategically encoded into two separate feature sets to represent diagnostic features and domain-specific features, respectively, and we apply forces to pull or repel them in the feature space, accordingly. Results from cross-site test domain evaluation showcase approximately an ~11.8% performance boost in prostate segmentation and around ~10.5% in brain tumor segmentation compared to the second-best method.</p>
  </details>
</details>
<details>
  <summary>51. <b>【2406.02074】FaceCom: Towards High-fidelity 3D Facial Shape Completion via Optimization and Inpainting Guidance</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02074">https://arxiv.org/abs/2406.02074</a></p>
  <p><b>作者</b>：Yinglong Li,Hongyu Wu,Xiaogang Wang,Qingzhao Qin,Yijiao Zhao,Yong wang,Aimin Hao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：arbitrary forms, delivers high-fidelity results, delivers high-fidelity, shape completion, facial</p>
  <p><b>备注</b>： accepted to CVPR2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose FaceCom, a method for 3D facial shape completion, which delivers high-fidelity results for incomplete facial inputs of arbitrary forms. Unlike end-to-end shape completion methods based on point clouds or voxels, our approach relies on a mesh-based generative network that is easy to optimize, enabling it to handle shape completion for irregular facial scans. We first train a shape generator on a mixed 3D facial dataset containing 2405 identities. Based on the incomplete facial input, we fit complete faces using an optimization approach under image inpainting guidance. The completion results are refined through a post-processing step. FaceCom demonstrates the ability to effectively and naturally complete facial scan data with varying missing regions and degrees of missing areas. Our method can be used in medical prosthetic fabrication and the registration of deficient scanning data. Our experimental results demonstrate that FaceCom achieves exceptional performance in fitting and shape completion tasks. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>52. <b>【2406.02064】Advancing Generalized Transfer Attack with Initialization Derived Bilevel Optimization and Dynamic Sequence Truncation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02064">https://arxiv.org/abs/2406.02064</a></p>
  <p><b>作者</b>：Yaohua Liu,Jiaxin Gao,Xuan Liu,Xianghao Jiao,Xin Fan,Risheng Liu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Transfer attacks generate, generate significant interest, real-world black-box applications, crafting transferable adversarial, attacks generate significant</p>
  <p><b>备注</b>： Accepted by IJCAI 2024. 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Transfer attacks generate significant interest for real-world black-box applications by crafting transferable adversarial examples through surrogate models. Whereas, existing works essentially directly optimize the single-level objective w.r.t. the surrogate model, which always leads to poor interpretability of attack mechanism and limited generalization performance over unknown victim models. In this work, we propose the \textbf{B}il\textbf{E}vel \textbf{T}ransfer \textbf{A}ttac\textbf{K} (BETAK) framework by establishing an initialization derived bilevel optimization paradigm, which explicitly reformulates the nested constraint relationship between the Upper-Level (UL) pseudo-victim attacker and the Lower-Level (LL) surrogate attacker. Algorithmically, we introduce the Hyper Gradient Response (HGR) estimation as an effective feedback for the transferability over pseudo-victim attackers, and propose the Dynamic Sequence Truncation (DST) technique to dynamically adjust the back-propagation path for HGR and reduce computational overhead simultaneously. Meanwhile, we conduct detailed algorithmic analysis and provide convergence guarantee to support non-convexity of the LL surrogate attacker. Extensive evaluations demonstrate substantial improvement of BETAK (e.g., $\mathbf{53.41}$\% increase of attack success rates against IncRes-v$2_{ens}$) against different victims and defense methods in targeted and untargeted attack scenarios. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>53. <b>【2406.02058】OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02058">https://arxiv.org/abs/2406.02058</a></p>
  <p><b>作者</b>：Yanmin Wu,Jiarui Meng,Haijie Li,Chenming Wu,Yahao Shi,Xinhua Cheng,Chen Zhao,Haocheng Feng,Errui Ding,Jingdong Wang,Jian Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：Gaussian Splatting, paper introduces OpenGaussian, point-level open vocabulary, open vocabulary understanding, open vocabulary</p>
  <p><b>备注</b>： technical report, 15 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) capable of 3D point-level open vocabulary understanding. Our primary motivation stems from observing that existing 3DGS-based open vocabulary methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations. To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency. These features exhibit both intra-object consistency and inter-object distinction. Then, we propose a two-stage codebook to discretize these features from coarse to fine levels. At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level. Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features. Extensive experiments, including open vocabulary-based 3D object selection, 3D point cloud understanding, click-based 3D object selection, and ablation studies, demonstrate the effectiveness of our proposed method. Project page: this https URL</p>
  </details>
</details>
<details>
  <summary>54. <b>【2406.02038】Leveraging Predicate and Triplet Learning for Scene Graph Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02038">https://arxiv.org/abs/2406.02038</a></p>
  <p><b>作者</b>：Jiankai Li,Yunhong Wang,Xiefan Guo,Ruijie Yang,Weixin Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Scene Graph Generation, Graph Generation, Scene Graph, visual scenes, textless subject</p>
  <p><b>备注</b>： CVPR 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Scene Graph Generation (SGG) aims to identify entities and predict the relationship triplets \textit{\textless subject, predicate, object\textgreater } in visual scenes. Given the prevalence of large visual variations of subject-object pairs even in the same predicate, it can be quite challenging to model and refine predicate representations directly across such pairs, which is however a common strategy adopted by most existing SGG methods. We observe that visual variations within the identical triplet are relatively small and certain relation cues are shared in the same type of triplet, which can potentially facilitate the relation learning in SGG. Moreover, for the long-tail problem widely studied in SGG task, it is also crucial to deal with the limited types and quantity of triplets in tail predicates. Accordingly, in this paper, we propose a Dual-granularity Relation Modeling (DRM) network to leverage fine-grained triplet cues besides the coarse-grained predicate ones. DRM utilizes contexts and semantics of predicate and triplet with Dual-granularity Constraints, generating compact and balanced representations from two perspectives to facilitate relation recognition. Furthermore, a Dual-granularity Knowledge Transfer (DKT) strategy is introduced to transfer variation from head predicates/triplets to tail ones, aiming to enrich the pattern diversity of tail classes to alleviate the long-tail problem. Extensive experiments demonstrate the effectiveness of our method, which establishes new state-of-the-art performance on Visual Genome, Open Image, and GQA datasets. Our code is available at \url{this https URL}</p>
  </details>
</details>
<details>
  <summary>55. <b>【2406.02037】Multi-Scale Direction-Aware Network for Infrared Small Target Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02037">https://arxiv.org/abs/2406.02037</a></p>
  <p><b>作者</b>：Jinmiao Zhao,Zelin Shi,Chuang Yu,Yunpeng Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：high-frequency directional features, target detection faces, high-frequency directional, small target detection, Infrared small target</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target. Existing deep learning-based methods focus on appearance features and ignore high-frequency directional features. Therefore, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks. Specifically, an innovative multi-directional feature awareness (MDFA) module is constructed, which fully utilizes the prior knowledge of targets and emphasizes the focus on high-frequency directional features. On this basis, combined with the multi-scale local relation learning (MLRL) module, a multi-scale direction-aware (MSDA) module is further constructed. The MSDA module promotes the full extraction of local relations at different scales and the full perception of key features in different directions. Meanwhile, a high-frequency direction injection (HFDI) module without training parameters is constructed to inject the high-frequency directional information of the original image into the network. This helps guide the network to pay attention to detailed information such as target edges and shapes. In addition, we propose a feature aggregation (FA) structure that aggregates multi-level features to solve the problem of small targets disappearing in deep feature maps. Furthermore, a lightweight feature alignment fusion (FAF) module is constructed, which can effectively alleviate the pixel offset existing in multi-level feature map fusion. Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on the public NUDT-SIRST, SIRST and IRSTD-1k datasets.</p>
  </details>
</details>
<details>
  <summary>56. <b>【2406.02027】Inference Attacks in Machine Learning as a Service: A Taxonomy, Review, and Promising Directions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02027">https://arxiv.org/abs/2406.02027</a></p>
  <p><b>作者</b>：Feng Wu,Lei Cui,Shaowen Yao,Shui Yu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：brought people concerns, inference attacks, inference, brought people, people concerns</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The prosperity of machine learning has also brought people's concerns about data privacy. Among them, inference attacks can implement privacy breaches in various MLaaS scenarios and model training/prediction phases. Specifically, inference attacks can perform privacy inference on undisclosed target training sets based on outputs of the target model, including but not limited to statistics, membership, semantics, data representation, etc. For instance, infer whether the target data has the characteristics of AIDS. In addition, the rapid development of the machine learning community in recent years, especially the surge of model types and application scenarios, has further stimulated the inference attacks' research. Thus, studying inference attacks and analyzing them in depth is urgent and significant. However, there is still a gap in the systematic discussion of inference attacks from taxonomy, global perspective, attack, and defense perspectives. This survey provides an in-depth and comprehensive inference of attacks and corresponding countermeasures in ML-as-a-service based on taxonomy and the latest researches. Without compromising researchers' intuition, we first propose the 3MP taxonomy based on the community research status, trying to normalize the confusing naming system of inference attacks. Also, we analyze the pros and cons of each type of inference attack, their workflow, countermeasure, and how they interact with other attacks. In the end, we point out several promising directions for researchers from a more comprehensive and novel perspective.</p>
  </details>
</details>
<details>
  <summary>57. <b>【2406.02021】MetaMixer Is All You Need</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02021">https://arxiv.org/abs/2406.02021</a></p>
  <p><b>作者</b>：Seokju Yun,Dongheon Lee,Youngmin Ro</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：revolutionized the landscape, Transformer, Feed-Forward Network, network design, FFN</p>
  <p><b>备注</b>： Code: [this https URL](https://github.com/ysj9909/FFNet) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Transformer, composed of self-attention and Feed-Forward Network, has revolutionized the landscape of network design across various vision tasks. FFN is a versatile operator seamlessly integrated into nearly all AI models to effectively harness rich representations. Recent works also show that FFN functions like key-value memories. Thus, akin to the query-key-value mechanism within self-attention, FFN can be viewed as a memory network, where the input serves as query and the two projection weights operate as keys and values, respectively. We hypothesize that the importance lies in query-key-value framework itself rather than in self-attention. To verify this, we propose converting self-attention into a more FFN-like efficient token mixer with only convolutions while retaining query-key-value framework, namely FFNification. Specifically, FFNification replaces query-key and attention coefficient-value interactions with large kernel convolutions and adopts GELU activation function instead of softmax. The derived token mixer, FFNified attention, serves as key-value memories for detecting locally distributed spatial patterns, and operates in the opposite dimension to the ConvNeXt block within each corresponding sub-operation of the query-key-value framework. Building upon the above two modules, we present a family of Fast-Forward Networks. Our FFNet achieves remarkable performance improvements over previous state-of-the-art methods across a wide range of tasks. The strong and general performance of our proposed method validates our hypothesis and leads us to introduce MetaMixer, a general mixer architecture that does not specify sub-operations within the query-key-value framework. We show that using only simple operations like convolution and GELU in the MetaMixer can achieve superior performance.</p>
  </details>
</details>
<details>
  <summary>58. <b>【2406.01996】Bayesian Mesh Optimization for Graph Neural Networks to Enhance Engineering Performance Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01996">https://arxiv.org/abs/2406.01996</a></p>
  <p><b>作者</b>：Jangseop Park,Namwoo Kang</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)</p>
  <p><b>关键词</b>：replace computationally expensive, leveraging design variables, computationally expensive simulations, design variables, widely employed</p>
  <p><b>备注</b>： 17 pages, 8 figures, 3 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In engineering design, surrogate models are widely employed to replace computationally expensive simulations by leveraging design variables and geometric parameters from computer-aided design (CAD) models. However, these models often lose critical information when simplified to lower dimensions and face challenges in parameter definition, especially with the complex 3D shapes commonly found in industrial datasets. To address these limitations, we propose a Bayesian graph neural network (GNN) framework for a 3D deep-learning-based surrogate model that predicts engineering performance by directly learning geometric features from CAD using mesh representation. Our framework determines the optimal size of mesh elements through Bayesian optimization, resulting in a high-accuracy surrogate model. Additionally, it effectively handles the irregular and complex structures of 3D CADs, which differ significantly from the regular and uniform pixel structures of 2D images typically used in deep learning. Experimental results demonstrate that the quality of the mesh significantly impacts the prediction accuracy of the surrogate model, with an optimally sized mesh achieving superior performance. We compare the performance of models based on various 3D representations such as voxel, point cloud, and graph, and evaluate the computational costs of Monte Carlo simulation and Bayesian optimization methods to find the optimal mesh size. We anticipate that our proposed framework has the potential to be applied to mesh-based simulations across various engineering fields, leveraging physics-based information commonly used in computer-aided engineering.</p>
  </details>
</details>
<details>
  <summary>59. <b>【2406.01994】3D Imaging of Complex Specular Surfaces by Fusing Polarimetric and Deflectometric Information</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01994">https://arxiv.org/abs/2406.01994</a></p>
  <p><b>作者</b>：Jiazhang Wang,Oliver Cossairt,Florian Willomitzer</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Optics (physics.optics)</p>
  <p><b>关键词</b>：poses major challenges, Accurate and fast, optical measurement principles, poses major, major challenges</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Accurate and fast 3D imaging of specular surfaces still poses major challenges for state-of-the-art optical measurement principles. Frequently used methods, such as phase-measuring deflectometry (PMD) or shape-from-polarization (SfP), rely on strong assumptions about the measured objects, limiting their generalizability in broader application areas like medical imaging, industrial inspection, virtual reality, or cultural heritage analysis. In this paper, we introduce a measurement principle that utilizes a novel technique to effectively encode and decode the information contained in a light field reflected off a specular surface. We combine polarization cues from SfP with geometric information obtained from PMD to resolve all arising ambiguities in the 3D measurement. Moreover, our approach removes the unrealistic orthographic imaging assumption for SfP, which significantly improves the respective results. We showcase our new technique by demonstrating single-shot and multi-shot measurements on complex-shaped specular surfaces, displaying an evaluated accuracy of surface normals below $0.6^\circ$.</p>
  </details>
</details>
<details>
  <summary>60. <b>【2406.01987】Dealing with All-stage Missing Modality: Towards A Universal Model with Robust Reconstruction and Personalization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01987">https://arxiv.org/abs/2406.01987</a></p>
  <p><b>作者</b>：Yunpeng Zhao,Cheng Chen,Qing You Pang,Quanzheng Li,Carol Tang,Beng-Ti Ang,Yueming Jin</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Addressing missing modalities, Addressing missing, missing modalities presents, presents a critical, critical challenge</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Addressing missing modalities presents a critical challenge in multimodal learning. Current approaches focus on developing models that can handle modality-incomplete inputs during inference, assuming that the full set of modalities are available for all the data during training. This reliance on full-modality data for training limits the use of abundant modality-incomplete samples that are often encountered in practical settings. In this paper, we propose a robust universal model with modality reconstruction and model personalization, which can effectively tackle the missing modality at both training and testing stages. Our method leverages a multimodal masked autoencoder to reconstruct the missing modality and masked patches simultaneously, incorporating an innovative distribution approximation mechanism to fully utilize both modality-complete and modality-incomplete data. The reconstructed modalities then contributes to our designed data-model co-distillation scheme to guide the model learning in the presence of missing modalities. Moreover, we propose a CLIP-driven hyper-network to personalize partial model parameters, enabling the model to adapt to each distinct missing modality scenario. Our method has been extensively validated on two brain tumor segmentation benchmarks. Experimental results demonstrate the promising performance of our method, which consistently exceeds previous state-of-the-art approaches under the all-stage missing modality settings with different missing ratios. Code will be available.</p>
  </details>
</details>
<details>
  <summary>61. <b>【2406.01975】Can Dense Connectivity Benefit Outlier Detection? An Odyssey with NAS</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01975">https://arxiv.org/abs/2406.01975</a></p>
  <p><b>作者</b>：Hao Fu,Tunhou Zhang,Hai Li,Yiran Chen</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Convolutional Neural Networks, real world applications, Recent advances, Neural Networks, deployment of Convolutional</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advances in Out-of-Distribution (OOD) Detection is the driving force behind safe and reliable deployment of Convolutional Neural Networks (CNNs) in real world applications. However, existing studies focus on OOD detection through confidence score and deep generative model-based methods, without considering the impact of DNN structures, especially dense connectivity in architecture fabrications. In addition, existing outlier detection approaches exhibit high variance in generalization performance, lacking stability and confidence in evaluating and ranking different outlier detectors. In this work, we propose a novel paradigm, Dense Connectivity Search of Outlier Detector (DCSOD), that automatically explore the dense connectivity of CNN architectures on near-OOD detection task using Neural Architecture Search (NAS). We introduce a hierarchical search space containing versatile convolution operators and dense connectivity, allowing a flexible exploration of CNN architectures with diverse connectivity patterns. To improve the quality of evaluation on OOD detection during search, we propose evolving distillation based on our multi-view feature learning explanation. Evolving distillation stabilizes training for OOD detection evaluation, thus improves the quality of search. We thoroughly examine DCSOD on CIFAR benchmarks under OOD detection protocol. Experimental results show that DCSOD achieve remarkable performance over widely used architectures and previous NAS baselines. Notably, DCSOD achieves state-of-the-art (SOTA) performance on CIFAR benchmark, with AUROC improvement of $\sim$1.0%.</p>
  </details>
</details>
<details>
  <summary>62. <b>【2406.01970】he Crystal Ball Hypothesis in diffusion models: Anticipating object positions from initial noise</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01970">https://arxiv.org/abs/2406.01970</a></p>
  <p><b>作者</b>：Yuanhao Ban,Ruochen Wang,Tianyi Zhou,Boqing Gong,Cho-Jui Hsieh,Minhao Cheng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：achieved remarkable success, Diffusion models, rarely explored, initial noise, models have achieved</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have achieved remarkable success in text-to-image generation tasks; however, the role of initial noise has been rarely explored. In this study, we identify specific regions within the initial noise image, termed trigger patches, that play a key role for object generation in the resulting images. Notably, these patches are ``universal'' and can be generalized across various positions, seeds, and prompts. To be specific, extracting these patches from one noise and injecting them into another noise leads to object generation in targeted areas. We identify these patches by analyzing the dispersion of object bounding boxes across generated images, leading to the development of a posterior analysis technique. Furthermore, we create a dataset consisting of Gaussian noises labeled with bounding boxes corresponding to the objects appearing in the generated images and train a detector that identifies these patches from the initial noise. To explain the formation of these patches, we reveal that they are outliers in Gaussian noise, and follow distinct distributions through two-sample tests. Finally, we find the misalignment between prompts and the trigger patch patterns can result in unsuccessful image generations. The study proposes a reject-sampling strategy to obtain optimal noise, aiming to improve prompt adherence and positional diversity in image generation.</p>
  </details>
</details>
<details>
  <summary>63. <b>【2406.01961】Exploring Real World Map Change Generalization of Prior-Informed HD Map Prediction Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01961">https://arxiv.org/abs/2406.01961</a></p>
  <p><b>作者</b>：Samuel M. Bateman,Ning Xu,H. Charles Zhao,Yael Ben Shalom,Vince Gong,Greg Long,Will Maddern</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Building and maintaining, maintaining High-Definition, represents a large, large barrier, autonomous vehicle deployment</p>
  <p><b>备注</b>： Accepted to CVPR 2024, Workshop on Autonomous Driving</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Building and maintaining High-Definition (HD) maps represents a large barrier to autonomous vehicle deployment. This, along with advances in modern online map detection models, has sparked renewed interest in the online mapping problem. However, effectively predicting online maps at a high enough quality to enable safe, driverless deployments remains a significant challenge. Recent work on these models proposes training robust online mapping systems using low quality map priors with synthetic perturbations in an attempt to simulate out-of-date HD map priors. In this paper, we investigate how models trained on these synthetically perturbed map priors generalize to performance on deployment-scale, real world map changes. We present a large-scale experimental study to determine which synthetic perturbations are most useful in generalizing to real world HD map changes, evaluated using multiple years of real-world autonomous driving data. We show there is still a substantial sim2real gap between synthetic prior perturbations and observed real-world changes, which limits the utility of current prior-informed HD map prediction models.</p>
  </details>
</details>
<details>
  <summary>64. <b>【2406.01956】Enhance Image-to-Image Generation with LLaVA Prompt and Negative Prompt</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01956">https://arxiv.org/abs/2406.01956</a></p>
  <p><b>作者</b>：Zhicheng Ding,Panfeng Li,Qikai Yang,Siyang Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Vision Assistant, Large Language, Language and Vision, approach to enhance, paper presents</p>
  <p><b>备注</b>： Accepted by 2024 5th International Conference on Information Science, Parallel and Distributed Systems</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper presents a novel approach to enhance image-to-image generation by leveraging the multimodal capabilities of the Large Language and Vision Assistant (LLaVA). We propose a framework where LLaVA analyzes input images and generates textual descriptions, hereinafter LLaVA-generated prompts. These prompts, along with the original image, are fed into the image-to-image generation pipeline. This enriched representation guides the generation process towards outputs that exhibit a stronger resemblance to the input image. Extensive experiments demonstrate the effectiveness of LLaVA-generated prompts in promoting image similarity. We observe a significant improvement in the visual coherence between the generated and input images compared to traditional methods. Future work will explore fine-tuning LLaVA prompts for increased control over the creative process. By providing more specific details within the prompts, we aim to achieve a delicate balance between faithfulness to the original image and artistic expression in the generated outputs.</p>
  </details>
</details>
<details>
  <summary>65. <b>【2406.01954】Plug-and-Play Diffusion Distillation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01954">https://arxiv.org/abs/2406.01954</a></p>
  <p><b>作者</b>：Yi-Ting Hsiao,Siavash Khodadadeh,Kevin Duarte,Wei-An Lin,Hui Qu,Mingi Kwon,Ratheesh Kalarot</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：shown tremendous results, shown tremendous, Diffusion models, Diffusion, model</p>
  <p><b>备注</b>： IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have shown tremendous results in image generation. However, due to the iterative nature of the diffusion process and its reliance on classifier-free guidance, inference times are slow. In this paper, we propose a new distillation approach for guided diffusion models in which an external lightweight guide model is trained while the original text-to-image model remains frozen. We show that our method reduces the inference computation of classifier-free guided latent-space diffusion models by almost half, and only requires 1\% trainable parameters of the base model. Furthermore, once trained, our guide model can be applied to various fine-tuned, domain-specific versions of the base diffusion model without the need for additional training: this "plug-and-play" functionality drastically improves inference computation while maintaining the visual fidelity of generated images. Empirically, we show that our approach is able to produce visually appealing results and achieve a comparable FID score to the teacher with as few as 8 to 16 steps.</p>
  </details>
</details>
<details>
  <summary>66. <b>【2406.01938】Nutrition Estimation for Dietary Management: A Transformer Approach with Depth Sensing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01938">https://arxiv.org/abs/2406.01938</a></p>
  <p><b>作者</b>：Zhengyi Kwan,Wei Zhang,Zhengkui Wang,Aik Beng Ng,Simon See</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：Nutrition estimation, health and well-being, crucial for effective, effective dietary management, effective dietary</p>
  <p><b>备注</b>： 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Nutrition estimation is crucial for effective dietary management and overall health and well-being. Existing methods often struggle with sub-optimal accuracy and can be time-consuming. In this paper, we propose NuNet, a transformer-based network designed for nutrition estimation that utilizes both RGB and depth information from food images. We have designed and implemented a multi-scale encoder and decoder, along with two types of feature fusion modules, specialized for estimating five nutritional factors. These modules effectively balance the efficiency and effectiveness of feature extraction with flexible usage of our customized attention mechanisms and fusion strategies. Our experimental study shows that NuNet outperforms its variants and existing solutions significantly for nutrition estimation. It achieves an error rate of 15.65%, the lowest known to us, largely due to our multi-scale architecture and fusion modules. This research holds practical values for dietary management with huge potential for transnational research and deployment and could inspire other applications involving multiple data types with varying degrees of importance.</p>
  </details>
</details>
<details>
  <summary>67. <b>【2406.01932】Detecting Endangered Marine Species in Autonomous Underwater Vehicle Imagery Using Point Annotations and Few-Shot Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01932">https://arxiv.org/abs/2406.01932</a></p>
  <p><b>作者</b>：Heather Doig,Oscar Pizarro,Jacquomo Monk,Stefan Williams</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：Autonomous Underwater Vehicles, Underwater Vehicles, Autonomous Underwater, common marine species, marine species</p>
  <p><b>备注</b>： 7 pages, 5 figures. Submitted to the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:One use of Autonomous Underwater Vehicles (AUVs) is the monitoring of habitats associated with threatened, endangered and protected marine species, such as the handfish of Tasmania, Australia. Seafloor imagery collected by AUVs can be used to identify individuals within their broader habitat context, but the sheer volume of imagery collected can overwhelm efforts to locate rare or cryptic individuals. Machine learning models can be used to identify the presence of a particular species in images using a trained object detector, but the lack of training examples reduces detection performance, particularly for rare species that may only have a small number of examples in the wild. In this paper, inspired by recent work in few-shot learning, images and annotations of common marine species are exploited to enhance the ability of the detector to identify rare and cryptic species. Annotated images of six common marine species are used in two ways. Firstly, the common species are used in a pre-training step to allow the backbone to create rich features for marine species. Secondly, a copy-paste operation is used with the common species images to augment the training data. While annotations for more common marine species are available in public datasets, they are often in point format, which is unsuitable for training an object detector. A popular semantic segmentation model efficiently generates bounding box annotations for training from the available point annotations. Our proposed framework is applied to AUV images of handfish, increasing average precision by up to 48\% compared to baseline object detection training. This approach can be applied to other objects with low numbers of annotations and promises to increase the ability to actively monitor threatened, endangered and protected species.</p>
  </details>
</details>
<details>
  <summary>68. <b>【2406.01920】CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01920">https://arxiv.org/abs/2406.01920</a></p>
  <p><b>作者</b>：Junho Kim,Hyunjun Kim,Yeonju Kim,Yong Man Ro</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Multi-modal Models, Large Multi-modal, recently demonstrated remarkable, demonstrated remarkable abilities, visual context understanding</p>
  <p><b>备注</b>： Project page: [this https URL](https://ivy-lvlm.github.io/CODE/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Multi-modal Models (LMMs) have recently demonstrated remarkable abilities in visual context understanding and coherent response generation. However, alongside these advancements, the issue of hallucinations has emerged as a significant challenge, producing erroneous responses that are unrelated to the visual contents. In this paper, we introduce a novel contrastive-based decoding method, COuntering DEscription Contrastive Decoding (CODE), which leverages self-generated descriptions as contrasting references during the decoding phase of LMMs to address hallucination issues. CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content. By dynamically adjusting the information flow and distribution of next-token predictions in the LMM's vocabulary, CODE enhances the coherence and informativeness of generated responses. Extensive experiments demonstrate that our method significantly reduces hallucinations and improves cross-modal consistency across various benchmarks and cutting-edge LMMs. Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training.</p>
  </details>
</details>
<details>
  <summary>69. <b>【2406.01917】GOMAA-Geo: GOal Modality Agnostic Active Geo-localization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01917">https://arxiv.org/abs/2406.01917</a></p>
  <p><b>作者</b>：Anindya Sarkar,Srikumar Sastry,Aleksis Pirinen,Chongjie Zhang,Nathan Jacobs,Yevgeniy Vorobeychik</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：sequence of visual, find a target, visual cues observed, goal, AGL task</p>
  <p><b>备注</b>： 23 pages, 17 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We consider the task of active geo-localization (AGL) in which an agent uses a sequence of visual cues observed during aerial navigation to find a target specified through multiple possible modalities. This could emulate a UAV involved in a search-and-rescue operation navigating through an area, observing a stream of aerial images as it goes. The AGL task is associated with two important challenges. Firstly, an agent must deal with a goal specification in one of multiple modalities (e.g., through a natural language description) while the search cues are provided in other modalities (aerial imagery). The second challenge is limited localization time (e.g., limited battery life, urgency) so that the goal must be localized as efficiently as possible, i.e. the agent must effectively leverage its sequentially observed aerial views when searching for the goal. To address these challenges, we propose GOMAA-Geo - a goal modality agnostic active geo-localization agent - for zero-shot generalization between different goal modalities. Our approach combines cross-modality contrastive learning to align representations across modalities with supervised foundation model pretraining and reinforcement learning to obtain highly effective navigation and localization policies. Through extensive evaluations, we show that GOMAA-Geo outperforms alternative learnable approaches and that it generalizes across datasets - e.g., to disaster-hit areas without seeing a single disaster scenario during training - and goal modalities - e.g., to ground-level imagery or textual descriptions, despite only being trained with goals specified as aerial views. Code and models are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>70. <b>【2406.01916】FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01916">https://arxiv.org/abs/2406.01916</a></p>
  <p><b>作者</b>：Yuzhou Ji,He Zhu,Junshu Tang,Wuyi Liu,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：scene understanding applications, semantically interactive radiance, interactive radiance field, automated real-world, scene understanding</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The semantically interactive radiance field has always been an appealing task for its potential to facilitate user-friendly and automated real-world 3D scene understanding applications. However, it is a challenging task to achieve high quality, efficiency and zero-shot ability at the same time with semantics in radiance fields. In this work, we present FastLGS, an approach that supports real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high resolution. We propose the semantic feature grid to save multi-view CLIP features which are extracted based on Segment Anything Model (SAM) masks, and map the grids to low dimensional features for semantic field training through 3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through feature grids from rendered features for open-vocabulary queries. Comparisons with other state-of-the-art methods prove that FastLGS can achieve the first place performance concerning both speed and accuracy, where FastLGS is 98x faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that FastLGS is adaptive and compatible with many downstream tasks, such as 3D segmentation and 3D object inpainting, which can be easily applied to other 3D manipulation systems.</p>
  </details>
</details>
<details>
  <summary>71. <b>【2406.01914】HPE-CogVLM: New Head Pose Grounding Task Exploration on Vision Language Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01914">https://arxiv.org/abs/2406.01914</a></p>
  <p><b>作者</b>：Yu Tian,Tianqi Shao,Tsukasa Demizu,Xuyang Wu,Hsin-Tai Wu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：roll Euler angles, Head pose estimation, precise numerical output, Euler angles, roll Euler</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Head pose estimation (HPE) task requires a sophisticated understanding of 3D spatial relationships and precise numerical output of yaw, pitch, and roll Euler angles. Previous HPE studies are mainly based on Non-large language models (Non-LLMs), which rely on close-up human heads cropped from the full image as inputs and lack robustness in real-world scenario. In this paper, we present a novel framework to enhance the HPE prediction task by leveraging the visual grounding capability of CogVLM. CogVLM is a vision language model (VLM) with grounding capability of predicting object bounding boxes (BBoxes), which enables HPE training and prediction using full image information input. To integrate the HPE task into the VLM, we first cop with the catastrophic forgetting problem in large language models (LLMs) by investigating the rehearsal ratio in the data rehearsal method. Then, we propose and validate a LoRA layer-based model merging method, which keeps the integrity of parameters, to enhance the HPE performance in the framework. The results show our HPE-CogVLM achieves a 31.5\% reduction in Mean Absolute Error for HPE prediction over the current Non-LLM based state-of-the-art in cross-dataset evaluation. Furthermore, we compare our LoRA layer-based model merging method with LoRA fine-tuning only and other merging methods in CogVLM. The results demonstrate our framework outperforms them in all HPE metrics.</p>
  </details>
</details>
<details>
  <summary>72. <b>【2406.01906】ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01906">https://arxiv.org/abs/2406.01906</a></p>
  <p><b>作者</b>：Chen Mao,Jingqi Hu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：computer vision tasks, augmented reality, autonomous driving, visual geo-localization datasets, Visual Geo-localization</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Visual Geo-localization (VG) refers to the process to identify the location described in query images, which is widely applied in robotics field and computer vision tasks, such as autonomous driving, metaverse, augmented reality, and SLAM. In fine-grained images lacking specific text descriptions, directly applying pure visual methods to represent neighborhood features often leads to the model focusing on overly fine-grained features, unable to fully mine the semantic information in the images. Therefore, we propose a two-stage training method to enhance visual performance and use contrastive learning to mine challenging samples. We first leverage the multi-modal description capability of CLIP (Contrastive Language-Image Pretraining) to create a set of learnable text prompts for each geographic image feature to form vague descriptions. Then, by utilizing dynamic text prompts to assist the training of the image encoder, we enable the image encoder to learn better and more generalizable visual features. This strategy of applying text to purely visual tasks addresses the challenge of using multi-modal models for geographic images, which often suffer from a lack of precise descriptions, making them difficult to utilize widely. We validate the effectiveness of the proposed strategy on several large-scale visual geo-localization datasets, and our method achieves competitive results on multiple visual geo-localization datasets. Our code and model are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>73. <b>【2406.01900】Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01900">https://arxiv.org/abs/2406.01900</a></p>
  <p><b>作者</b>：Yue Ma,Hongyu Liu,Hongfa Wang,Heng Pan,Yingqing He,Junkun Yuan,Ailing Zeng,Chengfei Cai,Heung-Yeung Shum,Wei Liu,Qifeng Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：target landmark sequences, reference portrait, diffusion-based framework, portrait, reference</p>
  <p><b>备注</b>： Project Page: [this https URL](https://follow-your-emoji.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present Follow-Your-Emoji, a diffusion-based framework for portrait animation, which animates a reference portrait with target landmark sequences. The main challenge of portrait animation is to preserve the identity of the reference portrait and transfer the target expression to this portrait while maintaining temporal consistency and fidelity. To address these challenges, Follow-Your-Emoji equipped the powerful Stable Diffusion model with two well-designed technologies. Specifically, we first adopt a new explicit motion signal, namely expression-aware landmark, to guide the animation process. We discover this landmark can not only ensure the accurate motion alignment between the reference portrait and target motion during inference but also increase the ability to portray exaggerated expressions (i.e., large pupil movements) and avoid identity leakage. Then, we propose a facial fine-grained loss to improve the model's ability of subtle expression perception and reference portrait appearance reconstruction by using both expression and facial masks. Accordingly, our method demonstrates significant performance in controlling the expression of freestyle portraits, including real humans, cartoons, sculptures, and even animals. By leveraging a simple and effective progressive generation strategy, we extend our model to stable long-term animation, thus increasing its potential application value. To address the lack of a benchmark for this field, we introduce EmojiBench, a comprehensive benchmark comprising diverse portrait images, driving videos, and landmarks. We show extensive evaluations on EmojiBench to verify the superiority of Follow-Your-Emoji.</p>
  </details>
</details>
<details>
  <summary>74. <b>【2406.01894】SVASTIN: Sparse Video Adversarial Attack via Spatio-Temporal Invertible Neural Networks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01894">https://arxiv.org/abs/2406.01894</a></p>
  <p><b>作者</b>：Yi Pan,Jun-Jie Huang,Zihan Chen,Wentao Zhao,Ziyue Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Robust and imperceptible, Invertible Neural Networks, adversarial video attack, Spatio-Temporal Invertible Neural, imperceptible adversarial video</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Robust and imperceptible adversarial video attack is challenging due to the spatial and temporal characteristics of videos. The existing video adversarial attack methods mainly take a gradient-based approach and generate adversarial videos with noticeable perturbations. In this paper, we propose a novel Sparse Adversarial Video Attack via Spatio-Temporal Invertible Neural Networks (SVASTIN) to generate adversarial videos through spatio-temporal feature space information exchanging. It consists of a Guided Target Video Learning (GTVL) module to balance the perturbation budget and optimization speed and a Spatio-Temporal Invertible Neural Network (STIN) module to perform spatio-temporal feature space information exchanging between a source video and the target feature tensor learned by GTVL module. Extensive experiments on UCF-101 and Kinetics-400 demonstrate that our proposed SVASTIN can generate adversarial examples with higher imperceptibility than the state-of-the-art methods with the higher fooling rate. Code is available at \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>75. <b>【2406.01884】Rank-based No-reference Quality Assessment for Face Swapping</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01884">https://arxiv.org/abs/2406.01884</a></p>
  <p><b>作者</b>：Xinghui Zhou,Wenbo Zhou,Tianyi Wei,Shen Chen,Taiping Yao,Shouhong Ding,Weiming Zhang,Nenghai Yu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：rapid technological advancements, prominent research area, image processing due, image quality assessment, Face swapping</p>
  <p><b>备注</b>： 8 pages, 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Face swapping has become a prominent research area in computer vision and image processing due to rapid technological advancements. The metric of measuring the quality in most face swapping methods relies on several distances between the manipulated images and the source image, or the target image, i.e., there are suitable known reference face images. Therefore, there is still a gap in accurately assessing the quality of face interchange in reference-free scenarios. In this study, we present a novel no-reference image quality assessment (NR-IQA) method specifically designed for face swapping, addressing this issue by constructing a comprehensive large-scale dataset, implementing a method for ranking image quality based on multiple facial attributes, and incorporating a Siamese network based on interpretable qualitative comparisons. Our model demonstrates the state-of-the-art performance in the quality assessment of swapped faces, providing coarse- and fine-grained. Enhanced by this metric, an improved face-swapping model achieved a more advanced level with respect to expressions and poses. Extensive experiments confirm the superiority of our method over existing general no-reference image quality assessment metrics and the latest metric of facial image quality assessment, making it well suited for evaluating face swapping images in real-world scenarios.</p>
  </details>
</details>
<details>
  <summary>76. <b>【2406.01869】Fruit Classification System with Deep Learning and Neural Architecture Search</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01869">https://arxiv.org/abs/2406.01869</a></p>
  <p><b>作者</b>：Christine Dewi,Dhananjay Thiruvady,Nayyar Zaidi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：identification process involves, process involves analyzing, fruit identification process, visual characteristics, identification process</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The fruit identification process involves analyzing and categorizing different types of fruits based on their visual characteristics. This activity can be achieved using a range of methodologies, encompassing manual examination, conventional computer vision methodologies, and more sophisticated methodologies employing machine learning and deep learning. Our study identified a total of 15 distinct categories of fruit, consisting of class Avocado, Banana, Cherry, Apple Braeburn, Apple golden 1, Apricot, Grape, Kiwi, Mango, Orange, Papaya, Peach, Pineapple, Pomegranate and Strawberry. Neural Architecture Search (NAS) is a technological advancement employed within the realm of deep learning and artificial intelligence, to automate conceptualizing and refining neural network topologies. NAS aims to identify neural network structures that are highly suitable for tasks, such as the detection of fruits. Our suggested model with 99.98% mAP increased the detection performance of the preceding research study that used Fruit datasets. In addition, after the completion of the study, a comparative analysis was carried out to assess the findings in conjunction with those of another research that is connected to the topic. When compared to the findings of earlier studies, the detector that was proposed exhibited higher performance in terms of both its accuracy and its precision.</p>
  </details>
</details>
<details>
  <summary>77. <b>【2406.01867】MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by Adversarial Training</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01867">https://arxiv.org/abs/2406.01867</a></p>
  <p><b>作者</b>：Kengo Uchida,Takashi Shibuya,Yuhta Takida,Naoki Murata,Shusuke Takahashi,Yuki Mitsufuji</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：latent diffusion model, diffusion model, editing tasks, quality and speed, editing</p>
  <p><b>备注</b>： 12 pages, 6 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In motion generation, controllability as well as generation quality and speed is becoming more and more important. There are various motion editing tasks, such as in-betweening, upper body editing, and path-following, but existing methods perform motion editing with a data-space diffusion model, which is slow in inference compared to a latent diffusion model. In this paper, we propose MoLA, which provides fast and high-quality motion generation and also can deal with multiple editing tasks in a single framework. For high-quality and fast generation, we employ a variational autoencoder and latent diffusion model, and improve the performance with adversarial training. In addition, we apply a training-free guided generation framework to achieve various editing tasks with motion control inputs. We quantitatively show the effectiveness of adversarial learning in text-to-motion generation, and demonstrate the applicability of our editing framework to multiple editing tasks in the motion domain.</p>
  </details>
</details>
<details>
  <summary>78. <b>【2406.01843】L-MAGIC: Language Model Assisted Generation of Images with Coherence</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01843">https://arxiv.org/abs/2406.01843</a></p>
  <p><b>作者</b>：Zhipeng Cai,Matthias Mueller,Reiner Birkl,Diana Wofk,Shao-Yen Tseng,JunDa Cheng,Gabriela Ben-Melech Stan,Vasudev Lal,Michael Paulitsch</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：single input image, input image remains, generative AI breakthroughs, key challenge, current era</p>
  <p><b>备注</b>： accepted to CVPR 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In the current era of generative AI breakthroughs, generating panoramic scenes from a single input image remains a key challenge. Most existing methods use diffusion-based iterative or simultaneous multi-view inpainting. However, the lack of global scene layout priors leads to subpar outputs with duplicated objects (e.g., multiple beds in a bedroom) or requires time-consuming human text inputs for each view. We propose L-MAGIC, a novel method leveraging large language models for guidance while diffusing multiple coherent views of 360 degree panoramic scenes. L-MAGIC harnesses pre-trained diffusion and language models without fine-tuning, ensuring zero-shot performance. The output quality is further enhanced by super-resolution and multi-view fusion techniques. Extensive experiments demonstrate that the resulting panoramic scenes feature better scene layouts and perspective view rendering quality compared to related works, with 70% preference in human evaluations. Combined with conditional diffusion models, L-MAGIC can accept various input modalities, including but not limited to text, depth maps, sketches, and colored scripts. Applying depth estimation further enables 3D point cloud generation and dynamic scene exploration with fluid camera motion. Code is available at this https URL. The video presentation is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>79. <b>【2406.01837】Boosting Vision-Language Models with Transduction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01837">https://arxiv.org/abs/2406.01837</a></p>
  <p><b>作者</b>：Maxime Zanella,Benoît Gérin,Ismail Ben Ayed</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：boost predictive accuracy, predictive accuracy, powerful paradigm, paradigm that leverages, leverages the structure</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Transduction is a powerful paradigm that leverages the structure of unlabeled data to boost predictive accuracy. We present TransCLIP, a novel and computationally efficient transductive approach designed for Vision-Language Models (VLMs). TransCLIP is applicable as a plug-and-play module on top of popular inductive zero- and few-shot models, consistently improving their performances. Our new objective function can be viewed as a regularized maximum-likelihood estimation, constrained by a KL divergence penalty that integrates the text-encoder knowledge and guides the transductive learning process. We further derive an iterative Block Majorize-Minimize (BMM) procedure for optimizing our objective, with guaranteed convergence and decoupled sample-assignment updates, yielding computationally efficient transduction for large-scale datasets. We report comprehensive evaluations, comparisons, and ablation studies that demonstrate: (i) Transduction can greatly enhance the generalization capabilities of inductive pretrained zero- and few-shot VLMs; (ii) TransCLIP substantially outperforms standard transductive few-shot learning methods relying solely on vision features, notably due to the KL-based language constraint.</p>
  </details>
</details>
<details>
  <summary>80. <b>【2406.01829】FacAID: A Transformer Model for Neuro-Symbolic Facade Reconstruction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01829">https://arxiv.org/abs/2406.01829</a></p>
  <p><b>作者</b>：Aleksander Płocharski,Jan Swidzinski,Joanna Porter-Sobieraj,Przemyslaw Musialski</p>
  <p><b>类目</b>：Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：custom-designed split grammar, split grammar, custom-designed split, segmented facade structures, semi-complex split grammar</p>
  <p><b>备注</b>： 11 pages, 10 figures, preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We introduce a neuro-symbolic transformer-based model that converts flat, segmented facade structures into procedural definitions using a custom-designed split grammar. To facilitate this, we first develop a semi-complex split grammar tailored for architectural facades and then generate a dataset comprising of facades alongside their corresponding procedural representations. This dataset is used to train our transformer model to convert segmented, flat facades into the procedural language of our grammar. During inference, the model applies this learned transformation to new facade segmentations, providing a procedural representation that users can adjust to generate varied facade designs. This method not only automates the conversion of static facade images into dynamic, editable procedural formats but also enhances the design flexibility, allowing for easy modifications and variations by architects and designers. Our approach sets a new standard in facade design by combining the precision of procedural generation with the adaptability of neuro-symbolic learning.</p>
  </details>
</details>
<details>
  <summary>81. <b>【2406.01820】Finding Lottery Tickets in Vision Models via Data-driven Spectral Foresight Pruning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01820">https://arxiv.org/abs/2406.01820</a></p>
  <p><b>作者</b>：Leonardo Iurada,Marco Ciccone,Tatiana Tommasi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Neural Tangent Kernel, Recent advances, deep learning models, neural network pruning, memory demands</p>
  <p><b>备注</b>： Accepted CVPR 2024 - [this https URL](https://iurada.github.io/PX) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advances in neural network pruning have shown how it is possible to reduce the computational costs and memory demands of deep learning models before training. We focus on this framework and propose a new pruning at initialization algorithm that leverages the Neural Tangent Kernel (NTK) theory to align the training dynamics of the sparse network with that of the dense one. Specifically, we show how the usually neglected data-dependent component in the NTK's spectrum can be taken into account by providing an analytical upper bound to the NTK's trace obtained by decomposing neural networks into individual paths. This leads to our Path eXclusion (PX), a foresight pruning method designed to preserve the parameters that mostly influence the NTK's trace. PX is able to find lottery tickets (i.e. good paths) even at high sparsity levels and largely reduces the need for additional training. When applied to pre-trained models it extracts subnetworks directly usable for several downstream tasks, resulting in performance comparable to those of the dense counterpart but with substantial cost and computational savings. Code available at: this https URL</p>
  </details>
</details>
<details>
  <summary>82. <b>【2406.01815】Deep asymmetric mixture model for unsupervised cell segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01815">https://arxiv.org/abs/2406.01815</a></p>
  <p><b>作者</b>：Yang Nan,Guang Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Automated cell segmentation, Automated cell, drug discovery, laborious and subjective, Deep Gaussian mixture</p>
  <p><b>备注</b>： 5 pages, 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Automated cell segmentation has become increasingly crucial for disease diagnosis and drug discovery, as manual delineation is excessively laborious and subjective. To address this issue with limited manual annotation, researchers have developed semi/unsupervised segmentation approaches. Among these approaches, the Deep Gaussian mixture model plays a vital role due to its capacity to facilitate complex data distributions. However, these models assume that the data follows symmetric normal distributions, which is inapplicable for data that is asymmetrically distributed. These models also obstacles weak generalization capacity and are sensitive to outliers. To address these issues, this paper presents a novel asymmetric mixture model for unsupervised cell segmentation. This asymmetric mixture model is built by aggregating certain multivariate Gaussian mixture models with log-likelihood and self-supervised-based optimization functions. The proposed asymmetric mixture model outperforms (nearly 2-30% gain in dice coefficient, p0.05) the existing state-of-the-art unsupervised models on cell segmentation including the segment anything.</p>
  </details>
</details>
<details>
  <summary>83. <b>【2406.01797】he Empirical Impact of Forgetting and Transfer in Continual Visual Odometry</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01797">https://arxiv.org/abs/2406.01797</a></p>
  <p><b>作者</b>：Paolo Cudrano,Xiaoyu Luo,Matteo Matteucci</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：embodied agents increases, continues to advance, continuously-learning embodied agents, adaptive and continuously-learning, realm of assistance</p>
  <p><b>备注</b>： Accepted to CoLLAs 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As robotics continues to advance, the need for adaptive and continuously-learning embodied agents increases, particularly in the realm of assistance robotics. Quick adaptability and long-term information retention are essential to operate in dynamic environments typical of humans' everyday lives. A lifelong learning paradigm is thus required, but it is scarcely addressed by current robotics literature. This study empirically investigates the impact of catastrophic forgetting and the effectiveness of knowledge transfer in neural networks trained continuously in an embodied setting. We focus on the task of visual odometry, which holds primary importance for embodied agents in enabling their self-localization. We experiment on the simple continual scenario of discrete transitions between indoor locations, akin to a robot navigating different apartments. In this regime, we observe initial satisfactory performance with high transferability between environments, followed by a specialization phase where the model prioritizes current environment-specific knowledge at the expense of generalization. Conventional regularization strategies and increased model capacity prove ineffective in mitigating this phenomenon. Rehearsal is instead mildly beneficial but with the addition of a substantial memory cost. Incorporating action information, as commonly done in embodied settings, facilitates quicker convergence but exacerbates specialization, making the model overly reliant on its motion expectations and less adept at correctly interpreting visual cues. These findings emphasize the open challenges of balancing adaptation and memory retention in lifelong robotics and contribute valuable insights into the application of a lifelong paradigm on embodied agents.</p>
  </details>
</details>
<details>
  <summary>84. <b>【2406.01791】Hybrid-Learning Video Moment Retrieval across Multi-Domain Labels</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01791">https://arxiv.org/abs/2406.01791</a></p>
  <p><b>作者</b>：Weitong Cai,Jiabo Huang,Shaogang Gong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：text query description, untrimmed raw video, Video moment retrieval, query description, Video moment</p>
  <p><b>备注</b>： Accepted by BMVC2022</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Video moment retrieval (VMR) is to search for a visual temporal moment in an untrimmed raw video by a given text query description (sentence). Existing studies either start from collecting exhaustive frame-wise annotations on the temporal boundary of target moments (fully-supervised), or learn with only the video-level video-text pairing labels (weakly-supervised). The former is poor in generalisation to unknown concepts and/or novel scenes due to restricted dataset scale and diversity under expensive annotation costs; the latter is subject to visual-textual mis-correlations from incomplete labels. In this work, we introduce a new approach called hybrid-learning video moment retrieval to solve the problem by knowledge transfer through adapting the video-text matching relationships learned from a fully-supervised source domain to a weakly-labelled target domain when they do not share a common label space. Our aim is to explore shared universal knowledge between the two domains in order to improve model learning in the weakly-labelled target domain. Specifically, we introduce a multiplE branch Video-text Alignment model (EVA) that performs cross-modal (visual-textual) matching information sharing and multi-modal feature alignment to optimise domain-invariant visual and textual features as well as per-task discriminative joint video-text representations. Experiments show EVA's effectiveness in exploring temporal segment annotations in a source domain to help learn video moment retrieval without temporal labels in a target domain.</p>
  </details>
</details>
<details>
  <summary>85. <b>【2406.01765】Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01765">https://arxiv.org/abs/2406.01765</a></p>
  <p><b>作者</b>：Fatemeh Nourilenjan Nokabadi,Jean-François Lalonde,Christian Gagné</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：demonstrated strong performance, transformer trackers, trackers, demonstrated strong, transformer</p>
  <p><b>备注</b>： Published in Transactions on Machine Learning Research (05/2024): [this https URL](https://openreview.net/forum?id=FEEKR0Vl9s) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:New transformer networks have been integrated into object tracking pipelines and have demonstrated strong performance on the latest benchmarks. This paper focuses on understanding how transformer trackers behave under adversarial attacks and how different attacks perform on tracking datasets as their parameters change. We conducted a series of experiments to evaluate the effectiveness of existing adversarial attacks on object trackers with transformer and non-transformer backbones. We experimented on 7 different trackers, including 3 that are transformer-based, and 4 which leverage other architectures. These trackers are tested against 4 recent attack methods to assess their performance and robustness on VOT2022ST, UAV123 and GOT10k datasets. Our empirical study focuses on evaluating adversarial robustness of object trackers based on bounding box versus binary mask predictions, and attack methods at different levels of perturbations. Interestingly, our study found that altering the perturbation level may not significantly affect the overall object tracking results after the attack. Similarly, the sparsity and imperceptibility of the attack perturbations may remain stable against perturbation level shifts. By applying a specific attack on all transformer trackers, we show that new transformer trackers having a stronger cross-attention modeling achieve a greater adversarial robustness on tracking datasets, such as VOT2022ST and GOT10k. Our results also indicate the necessity for new attack methods to effectively tackle the latest types of transformer trackers. The codes necessary to reproduce this study are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>86. <b>【2406.01764】An approximation-based approach versus an AI one for the study of CT images of abdominal aorta aneurysms</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01764">https://arxiv.org/abs/2406.01764</a></p>
  <p><b>作者</b>：Lucrezia Rinelli,Arianna Travaglini,Nicolò Vescera,Gianluca Vinti</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：abdominal aortic aneurysm, tools of Approximation, Artificial Intelligence, Approximation Theory, based on tools</p>
  <p><b>备注</b>： 28 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study evaluates two approaches applied to computed tomography (CT) images of patients with abdominal aortic aneurysm: one deterministic, based on tools of Approximation Theory, and one based on Artificial Intelligence. Both aim to segment the basal CT images to extract the patent area of the aortic vessel, in order to propose an alternative to nephrotoxic contrast agents for diagnosing this pathology. While the deterministic approach employs sampling Kantorovich operators and the theory behind, leveraging the reconstruction and enhancement capabilities of these operators applied to images, the artificial intelligence-based approach lays on a U-net neural network. The results obtained from testing the two methods have been compared numerically and visually to assess their performances, demonstrating that both models yield accurate results.</p>
  </details>
</details>
<details>
  <summary>87. <b>【2406.01733】Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01733">https://arxiv.org/abs/2406.01733</a></p>
  <p><b>作者</b>：Xinyin Ma,Gongfan Fang,Michael Bi Mi,Xinchao Wang</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：recently demonstrated unprecedented, demonstrated unprecedented generative, unprecedented generative capabilities, recently demonstrated, demonstrated unprecedented</p>
  <p><b>备注</b>： Code is available at [this https URL](https://github.com/horseee/learning-to-cache) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion Transformers have recently demonstrated unprecedented generative capabilities for various tasks. The encouraging results, however, come with the cost of slow inference, since each denoising step requires inference on a transformer model with a large scale of parameters. In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68% of the computation in the cache steps (46.84% for all steps), with less than 0.01 drop in FID. To achieve this, we introduce a novel scheme, named Learning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for diffusion transformers. Specifically, by leveraging the identical structure of layers in transformers and the sequential nature of diffusion, we explore redundant computations between timesteps by treating each layer as the fundamental unit for caching. To address the challenge of the exponential search space in deep models for identifying layers to cache and remove, we propose a novel differentiable optimization objective. An input-invariant yet timestep-variant router is then optimized, which can finally produce a static computation graph. Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed.</p>
  </details>
</details>
<details>
  <summary>88. <b>【2406.01708】Model for Peanuts: Hijacking ML Models without Training Access is Possible</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01708">https://arxiv.org/abs/2406.01708</a></p>
  <p><b>作者</b>：Mahmoud Ghorbel,Halima Bouzidi,Ioan Marius Bilasco,Ihsen Alouani</p>
  <p><b>类目</b>：Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Machine Learning, deployment of Machine, Model, Model hijacking, invasion of privacy</p>
  <p><b>备注</b>： 17 pages, 14 figures, 7 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The massive deployment of Machine Learning (ML) models has been accompanied by the emergence of several attacks that threaten their trustworthiness and raise ethical and societal concerns such as invasion of privacy, discrimination risks, and lack of accountability. Model hijacking is one of these attacks, where the adversary aims to hijack a victim model to execute a different task than its original one. Model hijacking can cause accountability and security risks since a hijacked model owner can be framed for having their model offering illegal or unethical services. Prior state-of-the-art works consider model hijacking as a training time attack, whereby an adversary requires access to the ML model training to execute their attack. In this paper, we consider a stronger threat model where the attacker has no access to the training phase of the victim model. Our intuition is that ML models, typically over-parameterized, might (unintentionally) learn more than the intended task for they are trained. We propose a simple approach for model hijacking at inference time named SnatchML to classify unknown input samples using distance measures in the latent space of the victim model to previously known samples associated with the hijacking task classes. SnatchML empirically shows that benign pre-trained models can execute tasks that are semantically related to the initial task. Surprisingly, this can be true even for hijacking tasks unrelated to the original task. We also explore different methods to mitigate this risk. We first propose a novel approach we call meta-unlearning, designed to help the model unlearn a potentially malicious task while training on the original task dataset. We also provide insights on over-parameterization as one possible inherent factor that makes model hijacking easier, and we accordingly propose a compression-based countermeasure against this attack.</p>
  </details>
</details>
<details>
  <summary>89. <b>【2406.01662】Few-Shot Classification of Interactive Activities of Daily Living (InteractADL)</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01662">https://arxiv.org/abs/2406.01662</a></p>
  <p><b>作者</b>：Zane Durante,Robathan Harries,Edward Vendrow,Zelun Luo,Yuta Kyuragi,Kazuki Kozuka,Li Fei-Fei,Ehsan Adeli</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Daily Living, Activities of Daily, including assistive robots, applications including assistive, Understanding Activities</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Understanding Activities of Daily Living (ADLs) is a crucial step for different applications including assistive robots, smart homes, and healthcare. However, to date, few benchmarks and methods have focused on complex ADLs, especially those involving multi-person interactions in home environments. In this paper, we propose a new dataset and benchmark, InteractADL, for understanding complex ADLs that involve interaction between humans (and objects). Furthermore, complex ADLs occurring in home environments comprise a challenging long-tailed distribution due to the rarity of multi-person interactions, and pose fine-grained visual recognition tasks due to the presence of semantically and visually similar classes. To address these issues, we propose a novel method for fine-grained few-shot video classification called Name Tuning that enables greater semantic separability by learning optimal class name vectors. We show that Name Tuning can be combined with existing prompt tuning strategies to learn the entire input text (rather than only learning the prompt or class names) and demonstrate improved performance for few-shot classification on InteractADL and 4 other fine-grained visual classification benchmarks. For transparency and reproducibility, we release our code at this https URL.</p>
  </details>
</details>
<details>
  <summary>90. <b>【2406.01658】Proxy Denoising for Source-Free Domain Adaptation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01658">https://arxiv.org/abs/2406.01658</a></p>
  <p><b>作者</b>：Song Tang,Wenxin Su,Mao Ye,Jianwei Zhang,Xiatian Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Source-free Domain Adaptation, pre-trained source model, unlabeled target domain, source data, Source-free Domain</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of pre-trained large vision-language (ViL) models in many other applications, the latest SFDA methods have also validated the benefit of ViL models by leveraging their predictions as pseudo supervision. However, we observe that ViL's predictions could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, in this paper, we introduce a novel Proxy Denoising (ProDe) approach. Specifically, we leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Critically, we design a proxy denoising mechanism for correcting ViL's predictions. This is grounded on a novel proxy confidence theory by modeling elegantly the domain adaption effect of the proxy's divergence against the domain-invariant space. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that our ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set and generalized SFDA settings. The code will release soon.</p>
  </details>
</details>
<details>
  <summary>91. <b>【2406.01604】An Empirical Study of Excitation and Aggregation Design Adaptions in CLIP4Clip for Video-Text Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01604">https://arxiv.org/abs/2406.01604</a></p>
  <p><b>作者</b>：Xiaolun Jing,Genke Yang,Jian Chu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：clip retrieval task, video clip retrieval, video-text retrieval domain, clip retrieval, frame representations aggregation</p>
  <p><b>备注</b>： 20 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:CLIP4Clip model transferred from the CLIP has been the de-factor standard to solve the video clip retrieval task from frame-level input, triggering the surge of CLIP4Clip-based models in the video-text retrieval domain. In this work, we rethink the inherent limitation of widely-used mean pooling operation in the frame features aggregation and investigate the adaptions of excitation and aggregation design for discriminative video representation generation. We present a novel excitationand-aggregation design, including (1) The excitation module is available for capturing non-mutuallyexclusive relationships among frame features and achieving frame-wise features recalibration, and (2) The aggregation module is applied to learn exclusiveness used for frame representations aggregation. Similarly, we employ the cascade of sequential module and aggregation design to generate discriminative video representation in the sequential type. Besides, we adopt the excitation design in the tight type to obtain representative frame features for multi-modal interaction. The proposed modules are evaluated on three benchmark datasets of MSR-VTT, ActivityNet and DiDeMo, achieving MSR-VTT (43.9 R@1), ActivityNet (44.1 R@1) and DiDeMo (31.0 R@1). They outperform the CLIP4Clip results by +1.2% (+0.5%), +4.5% (+1.9%) and +9.5% (+2.7%) relative (absolute) improvements, demonstrating the superiority of our proposed excitation and aggregation designs. We hope our work will serve as an alternative for frame representations aggregation and facilitate future research.</p>
  </details>
</details>
<details>
  <summary>92. <b>【2406.01598】D2E-An Autonomous Decision-making Dataset involving Driver States and Human Evaluation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01598">https://arxiv.org/abs/2406.01598</a></p>
  <p><b>作者</b>：Zehong Ke,Yanbo Jiang,Yuning Wang,Hao Cheng,Jinhao Li,Jianqiang Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Databases (cs.DB); Robotics (cs.RO)</p>
  <p><b>关键词</b>：deep learning technology, datasets greatly influenced, learning technology, model performance, advancement of deep</p>
  <p><b>备注</b>： Submit for ITSC 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the advancement of deep learning technology, data-driven methods are increasingly used in the decision-making of autonomous driving, and the quality of datasets greatly influenced the model performance. Although current datasets have made significant progress in the collection of vehicle and environment data, emphasis on human-end data including the driver states and human evaluation is not sufficient. In addition, existing datasets consist mostly of simple scenarios such as car following, resulting in low interaction levels. In this paper, we introduce the Driver to Evaluation dataset (D2E), an autonomous decision-making dataset that contains data on driver states, vehicle states, environmental situations, and evaluation scores from human reviewers, covering a comprehensive process of vehicle decision-making. Apart from regular agents and surrounding environment information, we not only collect driver factor data including first-person view videos, physiological signals, and eye attention data, but also provide subjective rating scores from 40 human volunteers. The dataset is mixed of driving simulator scenes and real-road ones. High-interaction situations are designed and filtered to ensure behavior diversity. Through data organization, analysis, and preprocessing, D2E contains over 1100 segments of interactive driving case data covering from human driver factor to evaluation results, supporting the development of data-driven decision-making related algorithms.</p>
  </details>
</details>
<details>
  <summary>93. <b>【2406.01597】End-to-End Rate-Distortion Optimized 3D Gaussian Representation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01597">https://arxiv.org/abs/2406.01597</a></p>
  <p><b>作者</b>：Henan Wang,Hanxin Zhu,Tianyu He,Runsen Feng,Jiajun Deng,Jiang Bian,Zhibo Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)</p>
  <p><b>关键词</b>：Gaussian Splatting, representation and image, image rendering, emerging technique, technique with remarkable</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable potential in 3D representation and image rendering. However, the substantial storage overhead of 3DGS significantly impedes its practical applications. In this work, we formulate the compact 3D Gaussian learning as an end-to-end Rate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can achieve flexible and continuous rate control. RDO-Gaussian addresses two main issues that exist in current schemes: 1) Different from prior endeavors that minimize the rate under the fixed distortion, we introduce dynamic pruning and entropy-constrained vector quantization (ECVQ) that optimize the rate and distortion at the same time. 2) Previous works treat the colors of each Gaussian equally, while we model the colors of different regions and materials with learnable numbers of parameters. We verify our method on both real and synthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D Gaussian over 40x, and surpasses existing methods in rate-distortion performance.</p>
  </details>
</details>
<details>
  <summary>94. <b>【2405.14785】EditWorld: Simulating World Dynamics for Instruction-Following Image Editing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.14785">https://arxiv.org/abs/2405.14785</a></p>
  <p><b>作者</b>：Ling Yang,Bohan Zeng,Jiaming Liu,Hong Li,Minghao Xu,Wentao Zhang,Shuicheng Yan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Diffusion models, improved the performance, image editing, editing, Diffusion</p>
  <p><b>备注</b>： Project: [this https URL](https://github.com/YangLing0818/EditWorld) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have significantly improved the performance of image editing. Existing methods realize various approaches to achieve high-quality image editing, including but not limited to text control, dragging operation, and mask-and-inpainting. Among these, instruction-based editing stands out for its convenience and effectiveness in following human instructions across diverse scenarios. However, it still focuses on simple editing operations like adding, replacing, or deleting, and falls short of understanding aspects of world dynamics that convey the realistic dynamic nature in the physical world. Therefore, this work, EditWorld, introduces a new editing task, namely world-instructed image editing, which defines and categorizes the instructions grounded by various world scenarios. We curate a new image editing dataset with world instructions using a set of large pretrained models (e.g., GPT-3.5, Video-LLava and SDXL). To enable sufficient simulation of world dynamics for image editing, our EditWorld trains model in the curated dataset, and improves instruction-following ability with designed post-edit strategy. Extensive experiments demonstrate our method significantly outperforms existing editing methods in this new task. Our dataset and code will be available at this https URL</p>
  </details>
</details>
<details>
  <summary>95. <b>【2402.12908】RealCompo: Balancing Realism and Compositionality Improves Text-to-Image Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.12908">https://arxiv.org/abs/2402.12908</a></p>
  <p><b>作者</b>：Xinchen Zhang,Ling Yang,Yaqi Cai,Zhaochen Yu,Kai-Ni Wang,Jiake Xie,Ye Tian,Minkai Xu,Yong Tang,Yujiu Yang,Bin Cui</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：achieved remarkable advancements, image diffusion models, spatial-aware image diffusion, Diffusion models, image diffusion</p>
  <p><b>备注</b>： Project: [this https URL](https://github.com/YangLing0818/RealCompo) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose RealCompo, a new training-free and transferred-friendly text-to-image generation framework, which aims to leverage the respective advantages of text-to-image models and spatial-aware image diffusion models (e.g., layout, keypoints and segmentation maps) to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and spatial-aware image diffusion models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Notably, our RealCompo can be seamlessly extended with a wide range of spatial-aware image diffusion models and stylized diffusion models. Our code is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>96. <b>【2401.11708】Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.11708">https://arxiv.org/abs/2401.11708</a></p>
  <p><b>作者</b>：Ling Yang,Zhaochen Yu,Chenlin Meng,Minkai Xu,Stefano Ermon,Bin Cui</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：exhibit exceptional performance, exceptional performance, Diffusion models, Plan and Generate, RPG</p>
  <p><b>备注</b>： ICML 2024. Project: [this https URL](https://github.com/YangLing0818/RPG-DiffusionMaster) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>97. <b>【2401.02015】Improving Diffusion-Based Image Synthesis with Context Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.02015">https://arxiv.org/abs/2401.02015</a></p>
  <p><b>作者</b>：Ling Yang,Jingwei Liu,Shenda Hong,Zhilong Zhang,Zhilin Huang,Zheming Cai,Wentao Zhang,Bin Cui</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：dramatically promoted image, quality and diversity, class of generative, dramatically promoted, unprecedented quality</p>
  <p><b>备注</b>： Accepted by NeurIPS 2023</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by preserving its semantic connections with neighborhood context. This new paradigm of ConPreDiff can generalize to arbitrary discrete and continuous diffusion backbones without introducing extra parameters in sampling procedure. Extensive experiments are conducted on unconditional image generation, text-to-image generation and image inpainting tasks. Our ConPreDiff consistently outperforms previous methods and achieves a new SOTA text-to-image generation results on MS-COCO, with a zero-shot FID score of 6.21.</p>
  </details>
</details>
<details>
  <summary>98. <b>【2406.02534】Enhancing predictive imaging biomarker discovery through treatment effect analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02534">https://arxiv.org/abs/2406.02534</a></p>
  <p><b>作者</b>：Shuhan Xiao,Lukas Klein,Jens Petersen,Philipp Vollmuth,Paul F. Jaeger,Klaus H. Maier-Hein</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：individual treatment effectiveness, Identifying predictive biomarkers, forecast individual treatment, Identifying predictive, predictive imaging biomarkers</p>
  <p><b>备注</b>： 19 pages, 12 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Identifying predictive biomarkers, which forecast individual treatment effectiveness, is crucial for personalized medicine and informs decision-making across diverse disciplines. These biomarkers are extracted from pre-treatment data, often within randomized controlled trials, and have to be distinguished from prognostic biomarkers, which are independent of treatment assignment. Our study focuses on the discovery of predictive imaging biomarkers, aiming to leverage pre-treatment images to unveil new causal relationships. Previous approaches relied on labor-intensive handcrafted or manually derived features, which may introduce biases. In response, we present a new task of discovering predictive imaging biomarkers directly from the pre-treatment images to learn relevant image features. We propose an evaluation protocol for this task to assess a model's ability to identify predictive imaging biomarkers and differentiate them from prognostic ones. It employs statistical testing and a comprehensive analysis of image feature attribution. We explore the suitability of deep learning models originally designed for estimating the conditional average treatment effect (CATE) for this task, which previously have been primarily assessed for the precision of CATE estimation, overlooking the evaluation of imaging biomarker discovery. Our proof-of-concept analysis demonstrates promising results in discovering and validating predictive imaging biomarkers from synthetic outcomes and real-world image datasets.</p>
  </details>
</details>
<details>
  <summary>99. <b>【2406.02529】ReLUs Are Sufficient for Learning Implicit Neural Representations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02529">https://arxiv.org/abs/2406.02529</a></p>
  <p><b>作者</b>：Joseph Shenouda,Yamin Zhou,Robert D. Nowak</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Rectified Linear Unit, Linear Unit, Rectified Linear, learning implicit neural, employ the Rectified</p>
  <p><b>备注</b>： Accepted to ICML 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Motivated by the growing theoretical understanding of neural networks that employ the Rectified Linear Unit (ReLU) as their activation function, we revisit the use of ReLU activation functions for learning implicit neural representations (INRs). Inspired by second order B-spline wavelets, we incorporate a set of simple constraints to the ReLU neurons in each layer of a deep neural network (DNN) to remedy the spectral bias. This in turn enables its use for various INR tasks. Empirically, we demonstrate that, contrary to popular belief, one can learn state-of-the-art INRs based on a DNN composed of only ReLU neurons. Next, by leveraging recent theoretical works which characterize the kinds of functions ReLU neural networks learn, we provide a way to quantify the regularity of the learned function. This offers a principled approach to selecting the hyperparameters in INR architectures. We substantiate our claims through experiments in signal representation, super resolution, and computed tomography, demonstrating the versatility and effectiveness of our method. The code for all experiments can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>100. <b>【2406.02480】Fairness Evolution in Continual Learning for Medical Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02480">https://arxiv.org/abs/2406.02480</a></p>
  <p><b>作者</b>：Marina Ceccon,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：achieving remarkable results, made significant strides, Deep Learning, Chest X-ray images, recent years</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deep Learning (DL) has made significant strides in various medical applications in recent years, achieving remarkable results. In the field of medical imaging, DL models can assist doctors in disease diagnosis by classifying pathologies in Chest X-ray images. However, training on new data to expand model capabilities and adapt to distribution shifts is a notable challenge these models face. Continual Learning (CL) has emerged as a solution to this challenge, enabling models to adapt to new data while retaining knowledge gained from previous experiences. Previous studies have analyzed the behavior of CL strategies in medical imaging regarding classification performance. However, when considering models that interact with sensitive information, such as in the medical domain, it is imperative to disaggregate the performance of socially salient groups. Indeed, DL algorithms can exhibit biases against certain sub-populations, leading to discrepancies in predictive performance across different groups identified by sensitive attributes such as age, race/ethnicity, sex/gender, and socioeconomic status. In this study, we go beyond the typical assessment of classification performance in CL and study bias evolution over successive tasks with domain-specific fairness metrics. Specifically, we evaluate the CL strategies using the well-known CheXpert (CXP) and ChestX-ray14 (NIH) datasets. We consider a class incremental scenario of five tasks with 12 pathologies. We evaluate the Replay, Learning without Forgetting (LwF), LwF Replay, and Pseudo-Label strategies. LwF and Pseudo-Label exhibit optimal classification performance, but when including fairness metrics in the evaluation, it is clear that Pseudo-Label is less biased. For this reason, this strategy should be preferred when considering real-world scenarios in which it is crucial to consider the fairness of the model.</p>
  </details>
</details>
<details>
  <summary>101. <b>【2406.02477】Inpainting Pathology in Lumbar Spine MRI with Latent Diffusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02477">https://arxiv.org/abs/2406.02477</a></p>
  <p><b>作者</b>：Colin Hansen,Simas Glinskis,Ashwin Raju,Micha Kornreich,JinHyeong Park,Jayashri Pawar,Richard Herzog,Li Zhang,Benjamin Odry</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：imbalanced datasets due, expert annotations, automated diagnosis, diagnosis in radiology, radiology suffer</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Data driven models for automated diagnosis in radiology suffer from insufficient and imbalanced datasets due to low representation of pathology in a population and the cost of expert annotations. Datasets can be bolstered through data augmentation. However, even when utilizing a full suite of transformations during model training, typical data augmentations do not address variations in human anatomy. An alternative direction is to synthesize data using generative models, which can potentially craft datasets with specific attributes. While this holds promise, commonly used generative models such as Generative Adversarial Networks may inadvertently produce anatomically inaccurate features. On the other hand, diffusion models, which offer greater stability, tend to memorize training data, raising concerns about privacy and generative diversity. Alternatively, inpainting has the potential to augment data through directly inserting pathology in medical images. However, this approach introduces a new challenge: accurately merging the generated pathological features with the surrounding anatomical context. While inpainting is a well established method for addressing simple lesions, its application to pathologies that involve complex structural changes remains relatively unexplored. We propose an efficient method for inpainting pathological features onto healthy anatomy in MRI through voxelwise noise scheduling in a latent diffusion model. We evaluate the method's ability to insert disc herniation and central canal stenosis in lumbar spine sagittal T2 MRI, and it achieves superior Frechet Inception Distance compared to state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>102. <b>【2406.02422】IterMask2: Iterative Unsupervised Anomaly Segmentation via Spatial and Frequency Masking for Brain Lesions in MRI</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02422">https://arxiv.org/abs/2406.02422</a></p>
  <p><b>作者</b>：Ziyun Liang,Xiaoqing Guo,J. Alison Noble,Konstantinos Kamnitsas</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Unsupervised anomaly segmentation, Unsupervised anomaly, healthy subjects, anomaly segmentation approaches, pathology segmentation train</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Unsupervised anomaly segmentation approaches to pathology segmentation train a model on images of healthy subjects, that they define as the 'normal' data distribution. At inference, they aim to segment any pathologies in new images as 'anomalies', as they exhibit patterns that deviate from those in 'normal' training data. Prevailing methods follow the 'corrupt-and-reconstruct' paradigm. They intentionally corrupt an input image, reconstruct it to follow the learned 'normal' distribution, and subsequently segment anomalies based on reconstruction error. Corrupting an input image, however, inevitably leads to suboptimal reconstruction even of normal regions, causing false positives. To alleviate this, we propose a novel iterative spatial mask-refining strategy IterMask2. We iteratively mask areas of the image, reconstruct them, and update the mask based on reconstruction error. This iterative process progressively adds information about areas that are confidently normal as per the model. The increasing content guides reconstruction of nearby masked areas, improving reconstruction of normal tissue under these areas, reducing false positives. We also use high-frequency image content as an auxiliary input to provide additional structural information for masked areas. This further improves reconstruction error of normal in comparison to anomalous areas, facilitating segmentation of the latter. We conduct experiments on several brain lesion datasets and demonstrate effectiveness of our method. Code is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>103. <b>【2406.02077】Multi-target stain normalization for histology slides</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.02077">https://arxiv.org/abs/2406.02077</a></p>
  <p><b>作者</b>：Desislav Ivanov,Carlo Alberto Barbano,Marco Grangetto</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Traditional staining normalization, single representative reference, staining normalization approaches, diverse staining patterns, representative reference image</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Traditional staining normalization approaches, e.g. Macenko, typically rely on the choice of a single representative reference image, which may not adequately account for the diverse staining patterns of datasets collected in practical scenarios. In this study, we introduce a novel approach that leverages multiple reference images to enhance robustness against stain variation. Our method is parameter-free and can be adopted in existing computational pathology pipelines with no significant changes. We evaluate the effectiveness of our method through experiments using a deep-learning pipeline for automatic nuclei segmentation on colorectal images. Our results show that by leveraging multiple reference images, better results can be achieved when generalizing to external data, where the staining can widely differ from the training set.</p>
  </details>
</details>
<details>
  <summary>104. <b>【2406.01993】Choroidal Vessel Segmentation on Indocyanine Green Angiography Images via Human-in-the-Loop Labeling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01993">https://arxiv.org/abs/2406.01993</a></p>
  <p><b>作者</b>：Ruoyu Chen(1),Ziwei Zhao(1),Mayinuer Yusufu(4 and 5),Xianwen Shang(1),Danli Shi(1 and 2),Mingguang He(1,2 and 3) ((1) School of Optometry, The Hong Kong Polytechnic University, Kowloon, Hong Kong SAR, China. (2) Research Centre for SHARP Vision, The Hong Kong Polytechnic University, Kowloon, Hong Kong SAR, China.(3) Centre for Eye and Vision Research (CEVR), 17W Hong Kong Science Park, Hong Kong SAR, China.(4) Centre for Eye Research Australia, Royal Victorian Eye and Ear Hospital, East Melbourne, Australia.(5) Department of Surgery (Ophthalmology), The University of Melbourne, Melbourne, Australia)</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：medical image processing, ICGA, degree view ICGA, field of medical, choroidal</p>
  <p><b>备注</b>： 25 pages,4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Human-in-the-loop (HITL) strategy has been recently introduced into the field of medical image processing. Indocyanine green angiography (ICGA) stands as a well-established examination for visualizing choroidal vasculature and detecting chorioretinal diseases. However, the intricate nature of choroidal vascular networks makes large-scale manual segmentation of ICGA images challenging. Thus, the study aims to develop a high-precision choroidal vessel segmentation model with limited labor using HITL framework. We utilized a multi-source ICGA dataset, including 55 degree view and ultra-widefield ICGA (UWF-ICGA) images for model development. The choroidal vessel network was pre-segmented by a pre-trained vessel segmentation model, and then manually modified by two ophthalmologists. Choroidal vascular diameter, density, complexity, tortuosity, and branching angle were automatically quantified based on the segmentation. We finally conducted four cycles of HITL. One hundred and fifty 55 degree view ICGA images were used for the first three cycles (50 images per cycle), and twenty UWF-ICGA images for the last cycle. The average time needed to manually correct a pre-segmented ICGA image per cycle reduced from 20 minutes to 1 minute. High segmentation accuracy has been achieved on both 55 degree view ICGA and UWF-ICGA images. Additionally, the multi-dimensional choroidal vascular parameters were significantly associated with various chorioretinal diseases. Our study not only demonstrated the feasibility of the HITL strategy in improving segmentation performance with reduced manual labeling, but also innovatively introduced several risk predictors for choroidal abnormalities.</p>
  </details>
</details>
<details>
  <summary>105. <b>【2406.01613】QuST: QuPath Extension for Integrative Whole Slide Image and Spatial Transcriptomics Analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01613">https://arxiv.org/abs/2406.01613</a></p>
  <p><b>作者</b>：Chao-Hui Huang</p>
  <p><b>类目</b>：Quantitative Methods (q-bio.QM); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：WSI analysis, WSI analysis tools, DL-based WSI analysis, analysis, WSI</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, various technologies have been introduced into digital pathology, including artificial intelligence (AI) driven methods, in both areas of pathological whole slide image (WSI) analysis and spatial transcriptomics (ST) analysis. AI-driven WSI analysis utilizes the power of deep learning (DL), expands the field of view for histopathological image analysis. On the other hand, ST bridges the gap between tissue spatial analysis and biological signals, offering the possibility to understand the spatial biology. However, a major bottleneck in DL-based WSI analysis is the preparation of training patterns, as hematoxylin \ eosin (H\E) staining does not provide direct biological evidence, such as gene expression, for determining the category of a biological component. On the other hand, as of now, the resolution in ST is far beyond that of WSI, resulting the challenge of further spatial analysis. Although various WSI analysis tools, including QuPath, have cited the use of WSI analysis tools in the context of ST analysis, its usage is primarily focused on initial image analysis, with other tools being utilized for more detailed transcriptomic analysis. As a result, the information hidden beneath WSI has not yet been fully utilized to support ST analysis.
To bridge this gap, we introduce QuST, a QuPath extension designed to bridge the gap between H\E WSI and ST analyzing tasks. In this paper, we highlight the importance of integrating DL-based WSI analysis and ST analysis in understanding disease biology and the challenges in integrating these modalities due to differences in data formats and analytical methods. The QuST source code is hosted on GitHub and documentation is available at this https URL.
</p><p>Subjects:</p>
<p>Quantitative Methods (q-bio.QM); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Image and Video Processing (eess.IV)</p>
<p>Cite as:<br>
arXiv:2406.01613 [q-bio.QM]</p>
<p>(or<br>
arXiv:2406.01613v1 [q-bio.QM] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2406.01613">https://doi.org/10.48550/arXiv.2406.01613</a></p>
<p>Focus to learn more</p>
<pre><code>            arXiv-issued DOI via DataCite&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>106. <b>【2406.01605】An Enhanced Encoder-Decoder Network Architecture for Reducing Information Loss in Image Semantic Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.01605">https://arxiv.org/abs/2406.01605</a></p>
  <p><b>作者</b>：Zijun Gao,Qi Wang,Taiyuan Mei,Xiaohan Cheng,Yun Zi,Haowei Yang</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：commonly encounters significant, encounters significant information, architecture commonly encounters, significant information loss, sampling process</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The traditional SegNet architecture commonly encounters significant information loss during the sampling process, which detrimentally affects its accuracy in image semantic segmentation tasks. To counter this challenge, we introduce an innovative encoder-decoder network structure enhanced with residual connections. Our approach employs a multi-residual connection strategy designed to preserve the intricate details across various image scales more effectively, thus minimizing the information loss inherent to down-sampling procedures. Additionally, to enhance the convergence rate of network training and mitigate sample imbalance issues, we have devised a modified cross-entropy loss function incorporating a balancing factor. This modification optimizes the distribution between positive and negative samples, thus improving the efficiency of model training. Experimental evaluations of our model demonstrate a substantial reduction in information loss and improved accuracy in semantic segmentation. Notably, our proposed network architecture demonstrates a substantial improvement in the finely annotated mean Intersection over Union (mIoU) on the dataset compared to the conventional SegNet. The proposed network structure not only reduces operational costs by decreasing manual inspection needs but also scales up the deployment of AI-driven image analysis across different sectors.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2024/06/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2024/06/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html"><img class="next-cover" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">🎨 Stable Diffusion 提示词指南书</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">信息检索</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">计算机视觉</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/06/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-06-06)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2024-06-06)"/></a><div class="content"><a class="title" href="/2024/06/06/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-06-06)">Arxiv每日速递(2024-06-06)</a><time datetime="2024-06-06T00:55:18.334Z" title="发表于 2024-06-06 08:55:18">2024-06-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书"><img src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="🎨 Stable Diffusion 提示词指南书"/></a><div class="content"><a class="title" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书">🎨 Stable Diffusion 提示词指南书</a><time datetime="2024-02-03T06:57:45.000Z" title="发表于 2024-02-03 14:57:45">2024-02-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer语言模型的位置编码与长度外推"/></a><div class="content"><a class="title" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推">Transformer语言模型的位置编码与长度外推</a><time datetime="2023-10-22T14:55:45.000Z" title="发表于 2023-10-22 22:55:45">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (8)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-10-22</span><a class="blog-slider__title" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">Transformer语言模型的位置编码与长度外推</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt=""><img width="48" height="48" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-02-03</span><a class="blog-slider__title" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">🎨 Stable Diffusion 提示词指南书</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>