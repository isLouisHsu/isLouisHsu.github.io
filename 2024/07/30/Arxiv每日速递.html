<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2024-07-30) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新329篇论文，其中：  自然语言处理40篇 信息检索12篇 计算机视觉59篇  自然语言处理    1. 【2407.18908】Wolf: Captioning Everything with a World Summarization Framework   链接：">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2024-07-30)">
<meta property="og:url" content="http://louishsu.xyz/2024/07/30/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新329篇论文，其中：  自然语言处理40篇 信息检索12篇 计算机视觉59篇  自然语言处理    1. 【2407.18908】Wolf: Captioning Everything with a World Summarization Framework   链接：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2024-07-30T00:58:26.472Z">
<meta property="article:modified_time" content="2024-07-30T01:00:04.173Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2024/07/30/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-07-30 09:00:04'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2024-07-30)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-30T00:58:26.472Z" title="发表于 2024-07-30 08:58:26">2024-07-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-30T01:00:04.173Z" title="更新于 2024-07-30 09:00:04">2024-07-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">29.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>177分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/07/30/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。</p>
<h1>统计</h1>
<p>今日共更新<strong>329</strong>篇论文，其中：</p>
<ul>
<li>自然语言处理<strong>40</strong>篇</li>
<li>信息检索<strong>12</strong>篇</li>
<li>计算机视觉<strong>59</strong>篇</li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>【2407.18908】Wolf: Captioning Everything with a World Summarization Framework</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18908">https://arxiv.org/abs/2407.18908</a></p>
  <p><b>作者</b>：Boyi Li,Ligeng Zhu,Ran Tian,Shuhan Tan,Yuxiao Chen,Yao Lu,Yin Cui,Sushant Veer,Max Ehrlich,Jonah Philion,Xinshuo Weng,Fuzhao Xue,Andrew Tao,Ming-Yu Liu,Sanja Fidler,Boris Ivanovic,Trevor Darrell,Jitendra Malik,Song Han,Marco Pavone</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：WOrLd summarization Framework, Vision Language Models, WOrLd summarization, Vision Language, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose Wolf, a WOrLd summarization Framework for accurate video captioning. Wolf is an automated captioning framework that adopts a mixture-of-experts approach, leveraging complementary strengths of Vision Language Models (VLMs). By utilizing both image and video models, our framework captures different levels of information and summarizes them efficiently. Our approach can be applied to enhance video understanding, auto-labeling, and captioning. To evaluate caption quality, we introduce CapScore, an LLM-based metric to assess the similarity and quality of generated captions compared to the ground truth captions. We further build four human-annotated datasets in three domains: autonomous driving, general scenes, and robotics, to facilitate comprehensive comparisons. We show that Wolf achieves superior captioning performance compared to state-of-the-art approaches from the research community (VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally, we establish a benchmark for video captioning and introduce a leaderboard, aiming to accelerate advancements in video understanding, captioning, and data alignment. Leaderboard: this https URL.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2407.18901】AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18901">https://arxiv.org/abs/2407.18901</a></p>
  <p><b>作者</b>：Harsh Trivedi,Tushar Khot,Mareike Hartmann,Ruskin Manku,Vinty Dong,Edward Li,Shashank Gupta,Ashish Sabharwal,Niranjan Balasubramanian</p>
  <p><b>类目</b>：oftware Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：complex control flow, iterative manner based, operate multiple apps, ordering groceries, operate multiple</p>
  <p><b>备注</b>： ACL'24 Camera Ready</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls.
To remedy this gap, we built $\textbf{AppWorld Engine}$, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created $\textbf{AppWorld Benchmark}$ (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our 'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least 16% fewer. This highlights the benchmark's difficulty and AppWorld's potential to push the frontiers of interactive coding agents. The project website is available at https://appworld.dev/.
</p><p>Comments:<br>
ACL’24 Camera Ready</p>
<p>Subjects:</p>
<p>Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)</p>
<p>Cite as:<br>
arXiv:2407.18901 [<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>]</p>
<p>(or<br>
arXiv:2407.18901v1 [<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>] for this version)</p><p></p>
  </details>
</details>
<details>
  <summary>3. <b>【2407.18887】Embedding And Clustering Your Data Can Improve Contrastive Pretraining</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18887">https://arxiv.org/abs/2407.18887</a></p>
  <p><b>作者</b>：Luke Merrick</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：embedding domain show, text embedding model, text embedding domain, Recent studies, single-source minibatches</p>
  <p><b>备注</b>： 16 pages, 3 figures, 2 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent studies of large-scale contrastive pretraining in the text embedding domain show that using single-source minibatches, rather than mixed-source minibatches, can substantially improve overall model accuracy. In this work, we explore extending training data stratification beyond source granularity by leveraging a pretrained text embedding model and the classic k-means clustering algorithm to further split training data apart by the semantic clusters within each source. Experimentally, we observe a notable increase in NDCG@10 when pretraining a BERT-based text embedding model on query-passage pairs from the MSMARCO passage retrieval dataset. Additionally, we conceptually connect our clustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B methodology and the nearest-neighbor-based hard-negative mining aspect of the ANCE methodology and discuss how this unified view motivates future lines of research on the organization of contrastive pretraining data.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2407.18789】Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18789">https://arxiv.org/abs/2407.18789</a></p>
  <p><b>作者</b>：Doan Nam Long Vu,Timour Igamberdiev,Ivan Habernal</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Applying differential privacy, protect individual data, individual data points, Applying differential, popular in NLP</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Applying differential privacy (DP) by means of the DP-SGD algorithm to protect individual data points during training is becoming increasingly popular in NLP. However, the choice of granularity at which DP is applied is often neglected. For example, neural machine translation (NMT) typically operates on the sentence-level granularity. From the perspective of DP, this setup assumes that each sentence belongs to a single person and any two sentences in the training dataset are independent. This assumption is however violated in many real-world NMT datasets, e.g. those including dialogues. For proper application of DP we thus must shift from sentences to entire documents. In this paper, we investigate NMT at both the sentence and document levels, analyzing the privacy/utility trade-off for both scenarios, and evaluating the risks of not using the appropriate privacy granularity in terms of leaking personally identifiable information (PII). Our findings indicate that the document-level NMT system is more resistant to membership inference attacks, emphasizing the significance of using the appropriate granularity when working with DP.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2407.18786】he power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18786">https://arxiv.org/abs/2407.18786</a></p>
  <p><b>作者</b>：Aleix Sant,Carlos Escolano,Audrey Mash,Francesca De Luca Fornaciari,Maite Melero</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Neural Machine Translation, lens of Large, paper studies gender</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper studies gender bias in machine translation through the lens of Large Language Models (LLMs). Four widely-used test sets are employed to benchmark various base LLMs, comparing their translation quality and gender bias against state-of-the-art Neural Machine Translation (NMT) models for English to Catalan (En $\rightarrow$ Ca) and English to Spanish (En $\rightarrow$ Es) translation directions. Our findings reveal pervasive gender bias across all models, with base LLMs exhibiting a higher degree of bias compared to NMT models. To combat this bias, we explore prompting engineering techniques applied to an instruction-tuned LLM. We identify a prompt structure that significantly reduces gender bias by up to 12% on the WinoMT evaluation dataset compared to more straightforward prompts. These results significantly reduce the gender bias accuracy gap between LLMs and traditional NMT systems.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2407.18752】Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18752">https://arxiv.org/abs/2407.18752</a></p>
  <p><b>作者</b>：Yuni Susanti,Michael Färber</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Small Language Models, Causal discovery aims, estimate causal structures, Causal discovery, Large Language Models</p>
  <p><b>备注</b>： accepted at ISWC'24</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Causal discovery aims to estimate causal structures among variables based on observational data. Large Language Models (LLMs) offer a fresh perspective to tackle the causal discovery problem by reasoning on the metadata associated with variables rather than their actual data values, an approach referred to as knowledge-based causal discovery. In this paper, we investigate the capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1 billion parameters) with prompt-based learning for knowledge-based causal discovery. Specifically, we present KG Structure as Prompt, a novel approach for integrating structural information from a knowledge graph, such as common neighbor nodes and metapaths, into prompt-based learning to enhance the capabilities of SLMs. Experimental results on three types of biomedical and open-domain datasets under few-shot settings demonstrate the effectiveness of our approach, surpassing most baselines and even conventional fine-tuning approaches trained on full datasets. Our findings further highlight the strong capabilities of SLMs: in combination with knowledge graphs and prompt-based learning, SLMs demonstrate the potential to surpass LLMs with larger number of parameters. Our code and datasets are available on GitHub.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2407.18743】owards Effective and Efficient Continual Pre-training of Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18743">https://arxiv.org/abs/2407.18743</a></p>
  <p><b>作者</b>：Jie Chen,Zhipeng Chen,Jiapeng Wang,Kun Zhou,Yutao Zhu,Jinhao Jiang,Yingqian Min,Wayne Xin Zhao,Zhicheng Dou,Jiaxin Mao,Yankai Lin,Ruihua Song,Jun Xu,Xu Chen,Rui Yan,Zhewei Wei,Di Hu,Wenbing Huang,Ji-Rong Wen</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Continual pre-training, Chinese language ability, adapting language models, scientific reasoning ability, domains or tasks</p>
  <p><b>备注</b>： 16 pages, 10 figures, 16 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Continual pre-training (CPT) has been an important approach for adapting language models to specific domains or tasks. To make the CPT approach more traceable, this paper presents a technical report for continually pre-training Llama-3 (8B), which significantly enhances the Chinese language ability and scientific reasoning ability of the backbone model. To enhance the new abilities while retaining the original abilities, we design specific data mixture and curriculum strategies by utilizing existing datasets and synthesizing high-quality datasets. Specifically, we synthesize multidisciplinary scientific question and answer (QA) pairs based on related web pages, and subsequently incorporate these synthetic data to improve the scientific reasoning ability of Llama-3. We refer to the model after CPT as Llama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning experiments with a relatively small model -- TinyLlama, and employ the derived findings to train the backbone model. Extensive experiments on a number of evaluation benchmarks show that our approach can largely improve the performance of the backbone models, including both the general abilities (+8.81 on C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on MATH and +4.13 on SciEval), without hurting the original capacities. Our model, data, and codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2407.18738】owards Generalized Offensive Language Identification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18738">https://arxiv.org/abs/2407.18738</a></p>
  <p><b>作者</b>：Alphaeus Dmonte,Tejas Arya,Tharindu Ranasinghe,Marcos Zampieri</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：encompassing hate speech, pervasive issue worldwide, encompassing hate, speech and cyberbullying, issue worldwide</p>
  <p><b>备注</b>： Accepted to ASONAM 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The prevalence of offensive content on the internet, encompassing hate speech and cyberbullying, is a pervasive issue worldwide. Consequently, it has garnered significant attention from the machine learning (ML) and natural language processing (NLP) communities. As a result, numerous systems have been developed to automatically identify potentially harmful content and mitigate its impact. These systems can follow two approaches; (1) Use publicly available models and application endpoints, including prompting large language models (LLMs) (2) Annotate datasets and train ML models on them. However, both approaches lack an understanding of how generalizable they are. Furthermore, the applicability of these systems is often questioned in off-domain and practical environments. This paper empirically evaluates the generalizability of offensive language detection models and datasets across a novel generalized benchmark. We answer three research questions on generalizability. Our findings will be useful in creating robust real-world offensive language detection systems.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2407.18730】Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of Shakespeare and Milton</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18730">https://arxiv.org/abs/2407.18730</a></p>
  <p><b>作者</b>：Manex Agirrezabal</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：William Shakespeare, Shakespeare and John, John Milton, public domain, work we present</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this work we present a corpus of poems by William Shakespeare and John Milton that have been enriched with readings from the public domain. We have aligned all the lines with their respective audio segments, at the line, word, syllable and phone level, and we have included their scansion. We make a basic visualization platform for these poems and we conclude by conjecturing possible future directions.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2407.18716】ChatSchema: A pipeline of extracting structured information with Large Multimodal Models based on schema</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18716">https://arxiv.org/abs/2407.18716</a></p>
  <p><b>作者</b>：Fei Wang,Yuewen Zheng,Qin Li,Jingyi Wu,Pengfei Li,Luxia Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Multimodal Models, Optical Character Recognition, Multimodal Models, Character Recognition, Large Multimodal</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Objective: This study introduces ChatSchema, an effective method for extracting and structuring information from unstructured data in medical paper reports using a combination of Large Multimodal Models (LMMs) and Optical Character Recognition (OCR) based on the schema. By integrating predefined schema, we intend to enable LMMs to directly extract and standardize information according to the schema specifications, facilitating further data entry. Method: Our approach involves a two-stage process, including classification and extraction for categorizing report scenarios and structuring information. We established and annotated a dataset to verify the effectiveness of ChatSchema, and evaluated key extraction using precision, recall, F1-score, and accuracy metrics. Based on key extraction, we further assessed value extraction. We conducted ablation studies on two LMMs to illustrate the improvement of structured information extraction with different input modals and methods. Result: We analyzed 100 medical reports from Peking University First Hospital and established a ground truth dataset with 2,945 key-value pairs. We evaluated ChatSchema using GPT-4o and Gemini 1.5 Pro and found a higher overall performance of GPT-4o. The results are as follows: For the result of key extraction, key-precision was 98.6%, key-recall was 98.5%, key-F1-score was 98.6%. For the result of value extraction based on correct key extraction, the overall accuracy was 97.2%, precision was 95.8%, recall was 95.8%, and F1-score was 95.8%. An ablation study demonstrated that ChatSchema achieved significantly higher overall accuracy and overall F1-score of key-value extraction, compared to the Baseline, with increases of 26.9% overall accuracy and 27.4% overall F1-score, respectively.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2407.18712】Cluster-norm for Unsupervised Probing of Knowledge</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18712">https://arxiv.org/abs/2407.18712</a></p>
  <p><b>作者</b>：Walter Laurito,Sharan Maiya,Grégoire Dhimoïla,Owen(Ho Wan)Yeung,Kaarel Hänni</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：generating reliable information, language models brings, models brings challenges, reliable information, language models</p>
  <p><b>备注</b>： 34 pages, 35 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The deployment of language models brings challenges in generating reliable information, especially when these models are fine-tuned using human preferences. To extract encoded knowledge without (potentially) biased human labels, unsupervised probing techniques like Contrast-Consistent Search (CCS) have been developed (Burns et al., 2022). However, salient but unrelated features in a given dataset can mislead these probes (Farquhar et al., 2023). Addressing this, we propose a cluster normalization method to minimize the impact of such features by clustering and normalizing activations of contrast pairs before applying unsupervised probing techniques. While this approach does not address the issue of differentiating between knowledge in general and simulated knowledge - a major issue in the literature of latent knowledge elicitation (Christiano et al., 2021) - it significantly improves the ability of unsupervised probes to identify the intended knowledge amidst distractions.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2407.18698】Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18698">https://arxiv.org/abs/2407.18698</a></p>
  <p><b>作者</b>：Esteban Garces Arias,Julian Rodemann,Meimingwei Li,Christian Heumann,Matthias Aßenmacher</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：distributions of large, complex challenge, contrastive search, produce high-quality text, large language models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Decoding from the output distributions of large language models to produce high-quality text is a complex challenge in language modeling. Various approaches, such as beam search, sampling with temperature, $k-$sampling, nucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive search, have been proposed to address this problem, aiming to improve coherence, diversity, as well as resemblance to human-generated text. In this study, we introduce adaptive contrastive search, a novel decoding strategy extending contrastive search by incorporating an adaptive degeneration penalty, guided by the estimated uncertainty of the model at each generation step. This strategy is designed to enhance both the creativity and diversity of the language modeling process while at the same time producing coherent and high-quality generated text output. Our findings indicate performance enhancement in both aspects, across different model architectures and datasets, underscoring the effectiveness of our method in text generation tasks. Our code base, datasets, and models are publicly available.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2407.18689】he BIAS Detection Framework: Bias Detection in Word Embeddings and Language Models for European Languages</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18689">https://arxiv.org/abs/2407.18689</a></p>
  <p><b>作者</b>：Alexandre Puttick,Leander Rankwiler,Catherine Ikae,Mascha Kurpicz-Briki</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Mitigating Diversity Biases, Swiss State Secretariat, Research and Innovation, Mitigating Diversity, Secretariat for Education</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The project BIAS: Mitigating Diversity Biases of AI in the Labor Market is a four-year project funded by the European commission and supported by the Swiss State Secretariat for Education, Research and Innovation (SERI). As part of the project, novel bias detection methods to identify societal bias in language models and word embeddings in European languages are developed, with particular attention to linguistic and geographic particularities. This technical report describes the overall architecture and components of the BIAS Detection Framework. The code described in this technical report is available and will be updated and expanded continuously with upcoming results from the BIAS project. The details about the datasets for the different languages are described in corresponding papers at scientific venues.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2407.18626】Every Part Matters: Integrity Verification of Scientific Figures Based on Multimodal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18626">https://arxiv.org/abs/2407.18626</a></p>
  <p><b>作者</b>：Xiang Shi,Jiawei Liu,Yinpeng Liu,Qikai Cheng,Wei Lu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Digital Libraries (cs.DL); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：paper tackles, tackles a key, key issue, scientific figures, Large Language Models</p>
  <p><b>备注</b>： 28 pages, 11 figures, under review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper tackles a key issue in the interpretation of scientific figures: the fine-grained alignment of text and figures. It advances beyond prior research that primarily dealt with straightforward, data-driven visualizations such as bar and pie charts and only offered a basic understanding of diagrams through captioning and classification. We introduce a novel task, Figure Integrity Verification, designed to evaluate the precision of technologies in aligning textual knowledge with visual elements in scientific figures. To support this, we develop a semi-automated method for constructing a large-scale dataset, Figure-seg, specifically designed for this task. Additionally, we propose an innovative framework, Every Part Matters (EPM), which leverages Multimodal Large Language Models (MLLMs) to not only incrementally improve the alignment and verification of text-figure integrity but also enhance integrity through analogical reasoning. Our comprehensive experiments show that these innovations substantially improve upon existing methods, allowing for more precise and thorough analysis of complex scientific figures. This progress not only enhances our understanding of multimodal technologies but also stimulates further research and practical applications across fields requiring the accurate interpretation of complex visual data.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2407.18581】Dynamic Language Group-Based MoE: Enhancing Efficiency and Flexibility for Code-Switching Speech Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18581">https://arxiv.org/abs/2407.18581</a></p>
  <p><b>作者</b>：Hukai Huang,Shenghui Lu,Yahui Shan,He Qu,Wenhao Guan,Qingyang Hong,Lin Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：approach is ideally, multilingual and code-switching, challenges due, multi-expert architecture, ideally suited</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The Mixture of Experts (MoE) approach is ideally suited for tackling multilingual and code-switching (CS) challenges due to its multi-expert architecture. This work introduces the DLG-MoE, which is optimized for bilingual and CS scenarios. Our novel Dynamic Language Group-based MoE layer features a language router with shared weights for explicit language modeling, while independent unsupervised routers within the language group handle attributes beyond language. This structure not only enhances expert extension capabilities but also supports dynamic top-k training, allowing for flexible inference across various top-k values and improving overall performance. The model requires no pre-training and supports streaming recognition, achieving state-of-the-art (SOTA) results with unmatched flexibility compared to other methods. The Code will be released.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2407.18562】Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18562">https://arxiv.org/abs/2407.18562</a></p>
  <p><b>作者</b>：Chaoyi Ai,Yong Jiang,Shen Huang,Pengjun Xie,Kewei Tu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Optical Character Recognition, Character Recognition processes, Named entity recognition, Optical Character, Character Recognition</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Named entity recognition (NER) models often struggle with noisy inputs, such as those with spelling mistakes or errors generated by Optical Character Recognition processes, and learning a robust NER model is challenging. Existing robust NER models utilize both noisy text and its corresponding gold text for training, which is infeasible in many real-world applications in which gold text is not available. In this paper, we consider a more realistic setting in which only noisy text and its NER labels are available. We propose to retrieve relevant text of the noisy text from a knowledge corpus and use it to enhance the representation of the original noisy input. We design three retrieval methods: sparse retrieval based on lexicon similarity, dense retrieval based on semantic similarity, and self-retrieval based on task-specific text. After retrieving relevant text, we concatenate the retrieved text with the original noisy text and encode them with a transformer network, utilizing self-attention to enhance the contextual token representations of the noisy text using the retrieved text. We further employ a multi-view training framework that improves robust NER without retrieving text during inference. Experiments show that our retrieval-augmented model achieves significant improvements in various noisy NER settings.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2407.18552】Multimodal Emotion Recognition using Audio-Video Transformer Fusion with Cross Attention</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18552">https://arxiv.org/abs/2407.18552</a></p>
  <p><b>作者</b>：Joe Dhanith P R,Shravan Venkatraman,Vigya Sharma,Santhosh Malarvannan</p>
  <p><b>类目</b>：Multimedia (cs.MM); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：human communication, fundamental aspect, aspect of human, Cross Attention, single data source</p>
  <p><b>备注</b>： 38 Pages, 9 Tables, 12 Figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Understanding emotions is a fundamental aspect of human communication. Integrating audio and video signals offers a more comprehensive understanding of emotional states compared to traditional methods that rely on a single data source, such as speech or facial expressions. Despite its potential, multimodal emotion recognition faces significant challenges, particularly in synchronization, feature extraction, and fusion of diverse data sources. To address these issues, this paper introduces a novel transformer-based model named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA model employs a transformer fusion approach to effectively capture and synchronize interlinked features from both audio and video inputs, thereby resolving synchronization problems. Additionally, the Cross Attention mechanism within AVT-CA selectively extracts and emphasizes critical features while discarding irrelevant ones from both modalities, addressing feature extraction and fusion challenges. Extensive experimental analysis conducted on the CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the proposed model. The results underscore the importance of AVT-CA in developing precise and reliable multimodal emotion recognition systems for practical applications.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2407.18540】A Universal Prompting Strategy for Extracting Process Model Information from Natural Language Text using Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18540">https://arxiv.org/abs/2407.18540</a></p>
  <p><b>作者</b>：Julian Neuberger,Lars Ackermann,Han van der Aa,Stefan Jablonski</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Business Process Management, extensive research efforts, textual process descriptions, Process Management domain, past decade</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Over the past decade, extensive research efforts have been dedicated to the extraction of information from textual process descriptions. Despite the remarkable progress witnessed in natural language processing (NLP), information extraction within the Business Process Management domain remains predominantly reliant on rule-based systems and machine learning methodologies. Data scarcity has so far prevented the successful application of deep learning techniques. However, the rapid progress in generative large language models (LLMs) makes it possible to solve many NLP tasks with very high quality without the need for extensive data. Therefore, we systematically investigate the potential of LLMs for extracting information from textual process descriptions, targeting the detection of process elements such as activities and actors, and relations between them. Using a heuristic algorithm, we demonstrate the suitability of the extracted information for process model generation. Based on a novel prompting strategy, we show that LLMs are able to outperform state-of-the-art machine learning approaches with absolute performance improvements of up to 8\% $F_1$ score across three different datasets. We evaluate our prompting strategy on eight different LLMs, showing it is universally applicable, while also analyzing the impact of certain prompt parts on extraction quality. The number of example texts, the specificity of definitions, and the rigour of format instructions are identified as key for improving the accuracy of extracted information. Our code, prompts, and data are publicly available.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2407.18538】owards a Multidimensional Evaluation Framework for Empathetic Conversational Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18538">https://arxiv.org/abs/2407.18538</a></p>
  <p><b>作者</b>：Aravind Sesagiri Raamkumar,Siyuan Brandon Loh</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Empathetic Conversational Systems, Empathetic Conversational, Conversational Systems, emotions and sentiments, application domain</p>
  <p><b>备注</b>： 13 pages, 4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Empathetic Conversational Systems (ECS) are built to respond empathetically to the user's emotions and sentiments, regardless of the application domain. Current ECS studies evaluation approaches are restricted to offline evaluation experiments primarily for gold standard comparison  benchmarking, and user evaluation studies for collecting human ratings on specific constructs. These methods are inadequate in measuring the actual quality of empathy in conversations. In this paper, we propose a multidimensional empathy evaluation framework with three new methods for measuring empathy at (i) structural level using three empathy-related dimensions, (ii) behavioral level using empathy behavioral types, and (iii) overall level using an empathy lexicon, thereby fortifying the evaluation process. Experiments were conducted with the state-of-the-art ECS models and large language models (LLMs) to show the framework's usefulness.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2407.18525】Is larger always better? Evaluating and prompting large language models for non-generative medical tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18525">https://arxiv.org/abs/2407.18525</a></p>
  <p><b>作者</b>：Yinghao Zhu,Junyi Gao,Zixiang Wang,Weibin Liao,Xiaochen Zheng,Lifang Liang,Yasha Wang,Chengwei Pan,Ewen M. Harrison,Liantao Ma</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Electronic Health Record, structured Electronic Health, Electronic Health, Large Language Models, Health Record</p>
  <p><b>备注</b>： arXiv admin note: text overlap with [arXiv:2402.01713](https://arxiv.org/abs/2402.01713) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The use of Large Language Models (LLMs) in medicine is growing, but their ability to handle both structured Electronic Health Record (EHR) data and unstructured clinical notes is not well-studied. This study benchmarks various models, including GPT-based LLMs, BERT-based models, and traditional clinical predictive models, for non-generative medical tasks utilizing renowned datasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7 traditional predictive models using the MIMIC dataset (ICU patient records) and the TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality and readmission prediction, disease hierarchy reconstruction, and biomedical sentence matching, comparing both zero-shot and finetuned performance. Results indicated that LLMs exhibited robust zero-shot predictive capabilities on structured EHR data when using well-designed prompting strategies, frequently surpassing traditional models. However, for unstructured medical texts, LLMs did not outperform finetuned BERT models, which excelled in both supervised and unsupervised tasks. Consequently, while LLMs are effective for zero-shot learning on structured data, finetuned BERT models are more suitable for unstructured texts, underscoring the importance of selecting models based on specific task requirements and data characteristics to optimize the application of NLP technology in healthcare.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2407.18501】he formation of perceptual space in early phonetic acquisition: a cross-linguistic modeling approach</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18501">https://arxiv.org/abs/2407.18501</a></p>
  <p><b>作者</b>：Frank Lihui Tan,Youngah Do</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：learners organize perceptual, advancing previous studies, key aspects, early phonetic acquisition, study investigates</p>
  <p><b>备注</b>： 51 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study investigates how learners organize perceptual space in early phonetic acquisition by advancing previous studies in two key aspects. Firstly, it examines the shape of the learned hidden representation as well as its ability to categorize phonetic categories. Secondly, it explores the impact of training models on context-free acoustic information, without involving contextual cues, on phonetic acquisition, closely mimicking the early language learning stage. Using a cross-linguistic modeling approach, autoencoder models are trained on English and Mandarin and evaluated in both native and non-native conditions, following experimental conditions used in infant language perception studies. The results demonstrate that unsupervised bottom-up training on context-free acoustic information leads to comparable learned representations of perceptual space between native and non-native conditions for both English and Mandarin, resembling the early stage of universal listening in infants. These findings provide insights into the organization of perceptual space during early phonetic acquisition and contribute to our understanding of the formation and representation of phonetic categories.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2407.18498】A Reliable Common-Sense Reasoning Socialbot Built Using LLMs and Goal-Directed ASP</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18498">https://arxiv.org/abs/2407.18498</a></p>
  <p><b>作者</b>：Yankai Zeng,Abhiramon Rajashekharan,Kinjal Basu,Huaduo Wang,Joaquín Arias,Gopal Gupta</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)</p>
  <p><b>关键词</b>：development of large, enabled the construction, receiving a lot, lot of attention, ability to simulate</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The development of large language models (LLMs), such as GPT, has enabled the construction of several socialbots, like ChatGPT, that are receiving a lot of attention for their ability to simulate a human conversation. However, the conversation is not guided by a goal and is hard to control. In addition, because LLMs rely more on pattern recognition than deductive reasoning, they can give confusing answers and have difficulty integrating multiple topics into a cohesive response. These limitations often lead the LLM to deviate from the main topic to keep the conversation interesting. We propose AutoCompanion, a socialbot that uses an LLM model to translate natural language into predicates (and vice versa) and employs commonsense reasoning based on Answer Set Programming (ASP) to hold a social conversation with a human. In particular, we rely on s(CASP), a goal-directed implementation of ASP as the backend. This paper presents the framework design and how an LLM is used to parse user messages and generate a response from the s(CASP) engine output. To validate our proposal, we describe (real) conversations in which the chatbot's goal is to keep the user entertained by talking about movies and books, and s(CASP) ensures (i) correctness of answers, (ii) coherence (and precision) during the conversation, which it dynamically regulates to achieve its specific purpose, and (iii) no deviation from the main topic.</p>
  </details>
</details>
<details>
  <summary>23. <b>【2407.18496】owards More Accurate Prediction of Human Empathy and Emotion in Text and Multi-turn Conversations by Combining Advanced NLP, Transformers-based Networks, and Linguistic Methodologies</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18496">https://arxiv.org/abs/2407.18496</a></p>
  <p><b>作者</b>：Manisha Singh,Divy Sharma,Alonso Ma,Nora Goldfine</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：personal distress displayed, Emotion Classification, displayed in essays, Feed-Forward Neural Network, Neural Network</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion Classification, we predict the level of empathic concern and personal distress displayed in essays. For the first stage of this project we implemented a Feed-Forward Neural Network using sentence-level embeddings as features. We experimented with four different embedding models for generating the inputs to the neural network. The subsequent stage builds upon the previous work and we have implemented three types of revisions. The first revision focuses on the enhancements to the model architecture and the training approach. The second revision focuses on handling class imbalance using stratified data sampling. The third revision focuses on leveraging lexical resources, where we apply four different resources to enrich the features associated with the dataset. During the final stage of this project, we have created the final end-to-end system for the primary task using an ensemble of models to revise primary task performance. Additionally, as part of the final stage, these approaches have been adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality Detection in Interactions, in which the empathic concern, emotion polarity, and emotion intensity in dyadic text conversations are predicted.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2407.18483】A Role-specific Guided Large Language Model for Ophthalmic Consultation Based on Stylistic Differentiation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18483">https://arxiv.org/abs/2407.18483</a></p>
  <p><b>作者</b>：Laiyi Fu,Binbin Fan,Hongkai Du,Yanxiang Feng,Chunhua Li,Huping Song</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：preventing eye diseases, crucial for diagnosing, preventing eye, consultations, Ophthalmology consultations</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Ophthalmology consultations are crucial for diagnosing, treating, and preventing eye diseases. However, the growing demand for consultations exceeds the availability of ophthalmologists. By leveraging large pre-trained language models, we can design effective dialogues for specific scenarios, aiding in consultations. Traditional fine-tuning strategies for question-answering tasks are impractical due to increasing model size and often ignoring patient-doctor role function during consultations. In this paper, we propose EyeDoctor, an ophthalmic medical questioning large language model that enhances accuracy through doctor-patient role perception guided and an augmented knowledge base with external disease information. Experimental results show EyeDoctor achieves higher question-answering precision in ophthalmology consultations. Notably, EyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16% improvement in F1 scores on multi-round datasets compared to second best model ChatGPT, highlighting the importance of doctor-patient role differentiation and dynamic knowledge base expansion for intelligent medical consultations. EyeDoc also serves as a free available web based service and souce code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2407.18479】Multi-turn Response Selection with Commonsense-enhanced Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18479">https://arxiv.org/abs/2407.18479</a></p>
  <p><b>作者</b>：Yuandong Wang,Xuhui Ren,Tong Chen,Yuxiao Dong,Nguyen Quoc Viet Hung,Jie Tang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：advanced artificial intelligence, pre-trained language models, Graph neural network, pre-trained language, dialogue systems</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As a branch of advanced artificial intelligence, dialogue systems are prospering. Multi-turn response selection is a general research problem in dialogue systems. With the assistance of background information and pre-trained language models, the performance of state-of-the-art methods on this problem gains impressive improvement. However, existing studies neglect the importance of external commonsense knowledge. Hence, we design a Siamese network where a pre-trained Language model merges with a Graph neural network (SinLG). SinLG takes advantage of Pre-trained Language Models (PLMs) to catch the word correlations in the context and response candidates and utilizes a Graph Neural Network (GNN) to reason helpful common sense from an external knowledge graph. The GNN aims to assist the PLM in fine-tuning, and arousing its related memories to attain better performance. Specifically, we first extract related concepts as nodes from an external knowledge graph to construct a subgraph with the context response pair as a super node for each sample. Next, we learn two representations for the context response pair via both the PLM and GNN. A similarity loss between the two representations is utilized to transfer the commonsense knowledge from the GNN to the PLM. Then only the PLM is used to infer online so that efficiency can be guaranteed. Finally, we conduct extensive experiments on two variants of the PERSONA-CHAT dataset, which proves that our solution can not only improve the performance of the PLM but also achieve an efficient inference.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2407.18471】Constructing the CORD-19 Vaccine Dataset</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18471">https://arxiv.org/abs/2407.18471</a></p>
  <p><b>作者</b>：Manisha Singh,Divy Sharma,Alonso Ma,Bridget Tyree,Margaret Mitchell</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：cater to scientists, scientists specifically, Latent Dirichlet Allocation, author demography, paper</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We introduce new dataset 'CORD-19-Vaccination' to cater to scientists specifically looking into COVID-19 vaccine-related research. This dataset is extracted from CORD-19 dataset [Wang et al., 2020] and augmented with new columns for language detail, author demography, keywords, and topic per paper. Facebook's fastText model is used to identify languages [Joulin et al., 2016]. To establish author demography (author affiliation, lab/institution location, and lab/institution country columns) we processed the JSON file for each paper and then further enhanced using Google's search API to determine country values. 'Yake' was used to extract keywords from the title, abstract, and body of each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to add topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset, we demonstrate a question-answering task like the one used in the CORD-19 Kaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential sentence classification was performed on each paper's abstract using the model from Dernoncourt et al. [2016]. We partially hand annotated the training dataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination' contains 30k research papers and can be immensely valuable for NLP research such as text mining, information extraction, and question answering, specific to the domain of COVID-19 vaccine research.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2407.18461】Enhancing Dysarthric Speech Recognition for Unseen Speakers via Prototype-Based Adaptation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18461">https://arxiv.org/abs/2407.18461</a></p>
  <p><b>作者</b>：Shiyao Wang,Shiwan Zhao,Jiaming Zhou,Aobo Kong,Yong Qin</p>
  <p><b>类目</b>：ound (cs.SD); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：inherent inter-speaker variability, formidable challenge due, Dysarthric speech recognition, applying DSR models, severe performance degradation</p>
  <p><b>备注</b>： accepted by Interspeech 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Dysarthric speech recognition (DSR) presents a formidable challenge due to inherent inter-speaker variability, leading to severe performance degradation when applying DSR models to new dysarthric speakers. Traditional speaker adaptation methodologies typically involve fine-tuning models for each speaker, but this strategy is cost-prohibitive and inconvenient for disabled users, requiring substantial data collection. To address this issue, we introduce a prototype-based approach that markedly improves DSR performance for unseen dysarthric speakers without additional fine-tuning. Our method employs a feature extractor trained with HuBERT to produce per-word prototypes that encapsulate the characteristics of previously unseen speakers. These prototypes serve as the basis for classification. Additionally, we incorporate supervised contrastive learning to refine feature extraction. By enhancing representation quality, we further improve DSR performance, enabling effective personalized DSR. We release our code at this https URL.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2407.18454】Fairness Definitions in Language Models Explained</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18454">https://arxiv.org/abs/2407.18454</a></p>
  <p><b>作者</b>：Thang Viet Doan,Zhibo Chu,Zichong Wang,Wenbin Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Natural Language Processing, Language Models, Language Processing, Natural Language, demonstrated exceptional performance</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts (\textit{e.g.,} medium-sized LMs versus large-sized LMs) and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their foundational principles and operational distinctions. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The implementation and additional resources are publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>29. <b>【2407.18442】Guidance-Based Prompt Data Augmentation in Specialized Domains for Named Entity Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18442">https://arxiv.org/abs/2407.18442</a></p>
  <p><b>作者</b>：Hyeonseok Kang,Hyein Seo,Jeesu Jung,Sangkeun Jung,Du-Seong Chang,Riwoo Chung</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：natural language processing, specialized data types, data types continue, finding quality data, language processing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:While the abundance of rich and vast datasets across numerous fields has facilitated the advancement of natural language processing, sectors in need of specialized data types continue to struggle with the challenge of finding quality data. Our study introduces a novel guidance data augmentation technique utilizing abstracted context and sentence structures to produce varied sentences while maintaining context-entity relationships, addressing data scarcity challenges. By fostering a closer relationship between context, sentence structure, and role of entities, our method enhances data augmentation's effectiveness. Consequently, by showcasing diversification in both entity-related vocabulary and overall sentence structure, and simultaneously improving the training performance of named entity recognition task.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2407.18421】Self-Directed Synthetic Dialogues and Revisions Technical Report</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18421">https://arxiv.org/abs/2407.18421</a></p>
  <p><b>作者</b>：Nathan Lambert,Hailey Schoelkopf,Aaron Gokaslan,Luca Soldaini,Valentina Pyatkin,Louis Castricato</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：solve complex problems, Directed Synthetic Dialogues, complex problems, important tool, instructions and solve</p>
  <p><b>备注</b>： 25 pages, 3 figures, 4 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Synthetic data has become an important tool in the fine-tuning of language models to follow instructions and solve complex problems. Nevertheless, the majority of open data to date is often lacking multi-turn data and collected on closed models, limiting progress on advancing open fine-tuning methods. We introduce Self Directed Synthetic Dialogues (SDSD), an experimental dataset consisting of guided conversations of language models talking to themselves. The dataset consists of multi-turn conversations generated with DBRX, Llama 2 70B, and Mistral Large, all instructed to follow a conversation plan generated prior to the conversation. We also explore including principles from Constitutional AI and other related works to create synthetic preference data via revisions to the final conversation turn. We hope this work encourages further exploration in multi-turn data and the use of open models for expanding the impact of synthetic data.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2407.18418】he Art of Refusal: A Survey of Abstention in Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18418">https://arxiv.org/abs/2407.18418</a></p>
  <p><b>作者</b>：Bingbing Wen,Jihan Yao,Shangbin Feng,Chenjun Xu,Yulia Tsvetkov,Bill Howe,Lucy Lu Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, building LLM systems, provide an answer, refusal of large, large language</p>
  <p><b>备注</b>： preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Abstention, the refusal of large language models (LLMs) to provide an answer, is increasingly recognized for its potential to mitigate hallucinations and enhance safety in building LLM systems. In this survey, we introduce a framework to examine abstention behavior from three perspectives: the query, the model, and human values. We review the literature on abstention methods (categorized based on the development stages of LLMs), benchmarks, and evaluation metrics, and discuss the merits and limitations of prior work. We further identify and motivate areas for future research, such as encouraging the study of abstention as a meta-capability across tasks and customizing abstention abilities based on context. In doing so, we aim to broaden the scope and impact of abstention methodologies in AI systems.</p>
  </details>
</details>
<details>
  <summary>32. <b>【2407.18416】PersonaGym: Evaluating Persona Agents and LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18416">https://arxiv.org/abs/2407.18416</a></p>
  <p><b>作者</b>：Vinay Samuel,Henry Peng Zou,Yue Zhou,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Ameet Deshpande,Karthik Narasimhan,Vishvak Murahari</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：demonstrated impressive contextual, impressive contextual response, Persona agents, Persona, persona agent capabilities</p>
  <p><b>备注</b>： 21 pages, 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Persona agents, which are LLM agents that act according to an assigned persona, have demonstrated impressive contextual response capabilities across various applications. These persona agents offer significant enhancements across diverse sectors, such as education, healthcare, and entertainment, where model developers can align agent responses to different user requirements thereby broadening the scope of agent applications. However, evaluating persona agent performance is incredibly challenging due to the complexity of assessing persona adherence in free-form interactions across various environments that are relevant to each persona agent. We introduce PersonaGym, the first dynamic evaluation framework for assessing persona agents, and PersonaScore, the first automated human-aligned metric grounded in decision theory for comprehensive large-scale evaluation of persona agents. Our evaluation of 6 open and closed-source LLMs, using a benchmark encompassing 200 personas and 10,000 questions, reveals significant opportunities for advancement in persona agent capabilities across state-of-the-art models. For example, Claude 3.5 Sonnet only has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite being a much more advanced model. Importantly, we find that increased model size and complexity do not necessarily imply enhanced persona agent capabilities thereby highlighting the pressing need for algorithmic and architectural invention towards faithful and performant persona agents.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2407.18376】Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18376">https://arxiv.org/abs/2407.18376</a></p>
  <p><b>作者</b>：Azmine Toushik Wasi,Raima Islam,Mst Rafia Islam,Taki Hasan Rafi,Dong-Kyu Chae</p>
  <p><b>类目</b>：Human-Computer Interaction (cs.HC); Computation and Language (cs.CL); Computers and Society (cs.CY); Multimedia (cs.MM); Social and Information Networks (cs.SI)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, massive technological impact, low-resource languages</p>
  <p><b>备注</b>： 10 Pages, 4 Figures. Accepted to the 1st Human-centered Evaluation and Auditing of Language Models Workshop at CHI 2024 (Workshop website: [this https URL](https://heal-workshop.github.io/#:~:text=Exploring%20Bengali%20Religious%20Dialect%20Biases%20in%20Large%20Language%20Models%20with%20Evaluation%20Perspectives) )</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages. This can be of great ethical concern when dealing with sensitive topics such as religion. As a means toward making LLMS more fair, we explore bias from a religious perspective in Bengali, focusing specifically on two main religious dialects: Hindu and Muslim-majority dialects. Here, we perform different experiments and audit showing the comparative analysis of different sentences using three commonly used LLMs: ChatGPT, Gemini, and Microsoft Copilot, pertaining to the Hindu and Muslim dialects of specific words and showcasing which ones catch the social biases and which do not. Furthermore, we analyze our findings and relate them to potential reasons and evaluation perspectives, considering their global impact with over 300 million speakers worldwide. With this work, we hope to establish the rigor for creating more fairness in LLMs, as these are widely used as creative writing agents.</p>
  </details>
</details>
<details>
  <summary>34. <b>【2407.18370】rust or Escalate: LLM Judges with Provable Guarantees for Human Agreement</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18370">https://arxiv.org/abs/2407.18370</a></p>
  <p><b>作者</b>：Jaehun Jung,Faeze Brahman,Yejin Choi</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：provide LLM-based evaluation, Cascaded Selective Evaluation, selective evaluation, human agreement, evaluation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this selective evaluation framework, human agreement can be provably guaranteed -- such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce Simulated Annotators, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose Cascaded Selective Evaluation, where we use cheaper models as initial judges and escalate to stronger models only when necessary -- again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, guarantees over 80% human agreement with almost 80% test coverage.</p>
  </details>
</details>
<details>
  <summary>35. <b>【2407.18369】AI Safety in Generative AI Large Language Models: A Survey</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18369">https://arxiv.org/abs/2407.18369</a></p>
  <p><b>作者</b>：Jaymari Chua,Yun Li,Shiyi Yang,Chen Wang,Lina Yao</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Model, facing accelerated adoption, Large Language, generative language models, adoption and innovation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Model (LLMs) such as ChatGPT that exhibit generative AI capabilities are facing accelerated adoption and innovation. The increased presence of Generative AI (GAI) inevitably raises concerns about the risks and safety associated with these models. This article provides an up-to-date survey of recent trends in AI safety research of GAI-LLMs from a computer scientist's perspective: specific and technical. In this survey, we explore the background and motivation for the identified harms and risks in the context of LLMs being generative language models; our survey differentiates by emphasising the need for unified theories of the distinct safety challenges in the research development and applications of LLMs. We start our discussion with a concise introduction to the workings of LLMs, supported by relevant literature. Then we discuss earlier research that has pointed out the fundamental constraints of generative models, or lack of understanding thereof (e.g., performance and safety trade-offs as LLMs scale in number of parameters). We provide a sufficient coverage of LLM alignment -- delving into various approaches, contending methods and present challenges associated with aligning LLMs with human preferences. By highlighting the gaps in the literature and possible implementation oversights, our aim is to create a comprehensive analysis that provides insights for addressing AI safety in LLMs and encourages the development of aligned and secure models. We conclude our survey by discussing future directions of LLMs for AI safety, offering insights into ongoing research in this critical area.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2407.18367】Robust Claim Verification Through Fact Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18367">https://arxiv.org/abs/2407.18367</a></p>
  <p><b>作者</b>：Nazanin Jafari,James Allan</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Claim verification, Claim, verification, Large Language Models, scientific claim verification</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Claim verification can be a challenging task. In this paper, we present a method to enhance the robustness and reasoning capabilities of automated claim verification through the extraction of short facts from evidence. Our novel approach, FactDetect, leverages Large Language Models (LLMs) to generate concise factual statements from evidence and label these facts based on their semantic relevance to the claim and evidence. The generated facts are then combined with the claim and evidence. To train a lightweight supervised model, we incorporate a fact-detection task into the claim verification process as a multitasking approach to improve both performance and explainability. We also show that augmenting FactDetect in the claim verification prompt enhances performance in zero-shot claim verification using LLMs. Our method demonstrates competitive results in the supervised claim verification model by 15% on the F1 score when evaluated for challenging scientific claim verification datasets. We also demonstrate that FactDetect can be augmented with claim and evidence for zero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show that AugFactDetect outperforms the baseline with statistical significance on three challenging scientific claim verification datasets with an average of 17.3% performance gain compared to the best performing baselines.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2407.18328】Unveiling Scoring Processes: Dissecting the Differences between LLMs and Human Graders in Automatic Scoring</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18328">https://arxiv.org/abs/2407.18328</a></p>
  <p><b>作者</b>：Xuansheng Wu,Padmaja Pravin Saraf,Gyeong-Geon Lee,Ehsan Latif,Ninghao Liu,Xiaoming Zhai</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Computers and Society (cs.CY)</p>
  <p><b>关键词</b>：Large language models, demonstrated strong potential, Large language, constructed response assessments, language models</p>
  <p><b>备注</b>： Non-archival Presenting at EDM 2024 Workshop on Large Language Models</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have demonstrated strong potential in performing automatic scoring for constructed response assessments. While constructed responses graded by humans are usually based on given grading rubrics, the methods by which LLMs assign scores remain largely unclear. It is also uncertain how closely AI's scoring process mirrors that of humans, or if it adheres to the same grading criteria. To address this gap, this paper uncovers the grading rubrics that LLMs used to score students' written responses to science tasks and their alignment with human scores. We also examine whether enhancing the alignments can improve scoring accuracy. Specifically, we prompt LLMs to generate analytic rubrics that they use to assign scores and study the alignment gap with human grading rubrics. Based on a series of experiments with various configurations of LLM settings, we reveal a notable alignment gap between human and LLM graders. While LLMs can adapt quickly to scoring tasks, they often resort to shortcuts, bypassing deeper logical reasoning expected in human grading. We found that incorporating high-quality analytical rubrics designed to reflect human grading logic can mitigate this gap and enhance LLMs' scoring accuracy. These results caution against the simplistic application of LLMs in science education and highlight the importance of aligning LLM outputs with human expectations to ensure efficient and accurate automatic scoring.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2407.18324】AMA-LSTM: Pioneering Robust and Fair Financial Audio Analysis for Stock Volatility Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18324">https://arxiv.org/abs/2407.18324</a></p>
  <p><b>作者</b>：Shengkun Wang,Taoran Ji,Jianfeng He,Mariam Almutairi,Dan Wang,Linhan Wang,Min Zhang,Chang-Tien Lu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS); Computational Finance (q-fin.CP); Statistical Finance (q-fin.ST)</p>
  <p><b>关键词</b>：earnings calls, Stock volatility prediction, Stock volatility, earnings, financial industry</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Stock volatility prediction is an important task in the financial industry. Recent advancements in multimodal methodologies, which integrate both textual and auditory data, have demonstrated significant improvements in this domain, such as earnings calls (Earnings calls are public available and often involve the management team of a public company and interested parties to discuss the company's earnings). However, these multimodal methods have faced two drawbacks. First, they often fail to yield reliable models and overfit the data due to their absorption of stochastic information from the stock market. Moreover, using multimodal models to predict stock volatility suffers from gender bias and lacks an efficient way to eliminate such bias. To address these aforementioned problems, we use adversarial training to generate perturbations that simulate the inherent stochasticity and bias, by creating areas resistant to random information around the input space to improve model robustness and fairness. Our comprehensive experiments on two real-world financial audio datasets reveal that this method exceeds the performance of current state-of-the-art solution. This confirms the value of adversarial training in reducing stochasticity and bias for stock volatility prediction tasks.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2407.18322】he Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18322">https://arxiv.org/abs/2407.18322</a></p>
  <p><b>作者</b>：Joe B Hakim,Jeffery L Painter,Darmendra Ramcharran,Vijay Kara,Greg Powell,Paulina Sobczak,Chiho Sato,Andrew Bate,Andrew Beam</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large language models, performing specific types, Large language, capacity for performing, performing specific</p>
  <p><b>备注</b>： 27 pages, 6 figures, 4 tables and supplementary material provided</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) are useful tools with the capacity for performing specific types of knowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical domains pose unique challenges, notably the issue of ``hallucination,'' where LLMs can generate fabricated information. This is particularly concerning in settings such as drug safety, where inaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety, and potentially applicable to other medical safety-critical contexts. These guardrails include mechanisms to detect anomalous documents to prevent the ingestion of inappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty in generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, which involves converting both structured and unstructured data within adverse event reports into natural language. This method was applied to translate individual case safety reports, demonstrating effective application in a pharmacovigilance processing task. Our guardrail framework offers a set of tools with broad applicability across various domains, ensuring LLMs can be safely used in high-risk situations by eliminating the occurrence of key errors, including the generation of incorrect pharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in medical safety-critical environments.</p>
  </details>
</details>
<details>
  <summary>40. <b>【2407.18332】Analyzing Speech Unit Selection for Textless Speech-to-Speech Translation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18332">https://arxiv.org/abs/2407.18332</a></p>
  <p><b>作者</b>：Jarod Duret(LIA),Yannick Estève(LIA),Titouan Parcollet(CAM)</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)</p>
  <p><b>关键词</b>：self-supervised learning techniques, Recent advancements, advancements in textless, learning techniques, adoption of self-supervised</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in textless speech-to-speech translation systems have been driven by the adoption of self-supervised learning techniques.     Although most state-of-the-art systems adopt a similar architecture to transform source language speech into sequences of discrete representations in the target language, the criteria for selecting these target speech units remains an open question.    This work explores the selection process through a study of downstream tasks such as automatic speech recognition, speech synthesis, speaker recognition, and emotion recognition.      Interestingly, our findings reveal a discrepancy in the optimization of discrete speech units: units that perform well in resynthesis performance do not necessarily correlate with those that enhance translation efficacy.      This discrepancy underscores the nuanced complexity of target feature selection and its impact on the overall performance of speech-to-speech translation systems.</p>
  </details>
</details>
<h1>信息检索</h1>
<details>
  <summary>1. <b>【2407.18910】Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE for Efficient Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18910">https://arxiv.org/abs/2407.18910</a></p>
  <p><b>作者</b>：Weizhi Zhang,Liangwei Yang,Zihe Song,Henry Peng Zou,Ke Xu,Henry Peng Zou,Liancheng Fang,Philip S. Yu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：training recommender systems, graph convolution networks, post-training graph convolution, graph convolution, Light Post-Training Graph</p>
  <p><b>备注</b>： Accepted to CIKM 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The efficiency and scalability of graph convolution networks (GCNs) in training recommender systems (RecSys) have been persistent concerns, hindering their deployment in real-world applications. This paper presents a critical examination of the necessity of graph convolutions during the training phase and introduces an innovative alternative: the Light Post-Training Graph Ordinary-Differential-Equation (LightGODE). Our investigation reveals that the benefits of GCNs are more pronounced during testing rather than training. Motivated by this, LightGODE utilizes a novel post-training graph convolution method that bypasses the computation-intensive message passing of GCNs and employs a non-parametric continuous graph ordinary-differential-equation (ODE) to dynamically model node representations. This approach drastically reduces training time while achieving fine-grained post-training graph convolution to avoid the distortion of the original training embedding space, termed the embedding discrepancy issue. We validate our model across several real-world datasets of different scales, demonstrating that LightGODE not only outperforms GCN-based models in terms of efficiency and effectiveness but also significantly mitigates the embedding discrepancy commonly associated with deeper graph convolution layers. Our LightGODE challenges the prevailing paradigms in RecSys training and suggests re-evaluating the role of graph convolutions, potentially guiding future developments of efficient large-scale graph-based RecSys.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2407.18898】A Flexible and Scalable Approach for Collecting Wildlife Advertisements on the Web</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18898">https://arxiv.org/abs/2407.18898</a></p>
  <p><b>作者</b>：Juliana Barbosa,Sunandan Chakraborty,Juliana Freire</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Databases (cs.DB)</p>
  <p><b>关键词</b>：activities in cyberspace, traffickers are increasingly, increasingly carrying, Wildlife traffickers, Wildlife</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Wildlife traffickers are increasingly carrying out their activities in cyberspace. As they advertise and sell wildlife products in online marketplaces, they leave digital traces of their activity. This creates a new opportunity: by analyzing these traces, we can obtain insights into how trafficking networks work as well as how they can be disrupted. However, collecting such information is difficult. Online marketplaces sell a very large number of products and identifying ads that actually involve wildlife is a complex task that is hard to automate. Furthermore, given that the volume of data is staggering, we need scalable mechanisms to acquire, filter, and store the ads, as well as to make them available for analysis. In this paper, we present a new approach to collect wildlife trafficking data at scale. We propose a data collection pipeline that combines scoped crawlers for data discovery and acquisition with foundational models and machine learning classifiers to identify relevant ads. We describe a dataset we created using this pipeline which is, to the best of our knowledge, the largest of its kind: it contains almost a million ads obtained from 41 marketplaces, covering 235 species and 20 languages. The source code is publicly available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2407.18827】Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18827">https://arxiv.org/abs/2407.18827</a></p>
  <p><b>作者</b>：Mutahar Safdar,Jiarui Xie,Andrei Mircea,Yaoyao Fiona Zhao</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：research in Additive, gained significant success, Additive Manufacturing, gained significant, extract scientific information</p>
  <p><b>备注</b>： 11 pages, 5 Figures, 3 Tables. This paper has been accepted to be published in the proceedings of IDETC-CIE 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. It requires substantial effort and time to extract scientific information from these works. AM domain experts have contributed over two dozen review papers to summarize these works. However, information specific to AM and AI contexts still requires manual effort to extract. The recent success of foundation models such as BERT (Bidirectional Encoder Representations for Transformers) or GPT (Generative Pre-trained Transformers) on textual data has opened the possibility of expediting scientific information extraction. We propose a framework that enables collaboration between AM and AI experts to continuously extract scientific information from data-driven AM literature. A demonstration tool is implemented based on the proposed framework and a case study is conducted to extract information relevant to the datasets, modeling, sensing, and AM system categories. We show the ability of LLMs (Large Language Models) to expedite the extraction of relevant information from data-driven AM literature. In the future, the framework can be used to extract information from the broader design and manufacturing literature in the engineering discipline.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2407.18735】AutoRDF2GML: Facilitating RDF Integration in Graph Machine Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18735">https://arxiv.org/abs/2407.18735</a></p>
  <p><b>作者</b>：Michael Färber,David Lamprecht,Yuni Susanti</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：graph machine learning, machine learning, machine learning tasks, graph machine, convert RDF data</p>
  <p><b>备注</b>： accepted at ISWC'24</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we introduce AutoRDF2GML, a framework designed to convert RDF data into data representations tailored for graph machine learning tasks. AutoRDF2GML enables, for the first time, the creation of both content-based features -- i.e., features based on RDF datatype properties -- and topology-based features -- i.e., features based on RDF object properties. Characterized by automated feature extraction, AutoRDF2GML makes it possible even for users less familiar with RDF and SPARQL to generate data representations ready for graph machine learning tasks, such as link prediction, node classification, and graph classification. Furthermore, we present four new benchmark datasets for graph machine learning, created from large RDF knowledge graphs using our framework. These datasets serve as valuable resources for evaluating graph machine learning approaches, such as graph neural networks. Overall, our framework effectively bridges the gap between the Graph Machine Learning and Semantic Web communities, paving the way for RDF-based machine learning applications.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2407.18646】Decoding Knowledge Claims: The Evaluation of Scientific Publication Contributions through Semantic Analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18646">https://arxiv.org/abs/2407.18646</a></p>
  <p><b>作者</b>：Luca D'Aniello,Nicolas Robinson-Garcia,Massimo Aria,Corrado Cuccurullo</p>
  <p><b>类目</b>：Digital Libraries (cs.DL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Word Mover Distance, Relaxed Word Mover, scientific publications challenges, sheer quantity, publications challenges</p>
  <p><b>备注</b>： This paper was submitted to STI 2024 - 28th International Conference on Science, Technology and Innovation Indicators STI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The surge in scientific publications challenges the use of publication counts as a measure of scientific progress, requiring alternative metrics that emphasize the quality and novelty of scientific contributions rather than sheer quantity. This paper proposes the use of Relaxed Word Mover's Distance (RWMD), a semantic text similarity measure, to evaluate the novelty of scientific papers. We hypothesize that RWMD can more effectively gauge the growth of scientific knowledge. To test such an assumption, we apply RWMD to evaluate seminal papers, with Hirsch's H-Index paper as a primary case study. We compare RWMD results across three groups: 1) H-Index-related papers, 2) scientometric studies, and 3) unrelated papers, aiming to discern redundant literature and hype from genuine innovations. Findings suggest that emphasizing knowledge claims offers a deeper insight into scientific contributions, marking RWMD as a promising alternative method to traditional citation metrics, thus better tracking significant scientific breakthroughs.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2407.18553】REAPER: Reasoning based Retrieval Planning for Complex RAG Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18553">https://arxiv.org/abs/2407.18553</a></p>
  <p><b>作者</b>：Ashutosh Joshi,Sheikh Muhammad Sarwar,Samarth Varshney,Sreyashi Nag,Shrivats Agrawal,Juhi Naik</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：facilitate factual responses, Retrieval Augmented Generation, factual responses, Complex dialog systems, Augmented Generation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from massive heterogeneous data stores that are usually architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one or a small subset of possible retrieval sources. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders will need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models (100B parameters) this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, though this means that a (small) classification model dictates the performance of a large language model. In this work we present REAPER (REAsoning-based PlannER) - an LLM based planner to generate retrieval plans in conversational systems. We show significant gains in latency over Agent-based systems and are able to scale easily to new and unseen use cases as compared to classification-based planning. Though our method can be applied to any RAG system, we show our results in the context of Rufus -- Amazon's conversational shopping assistant.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2407.18472】FedUD: Exploiting Unaligned Data for Cross-Platform Federated Click-Through Rate Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18472">https://arxiv.org/abs/2407.18472</a></p>
  <p><b>作者</b>：Wentao Ouyang,Rui Dong,Ri Tao,Xiangzheng Liu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Click-through rate, CTR prediction, federated CTR prediction, CTR, online advertising platforms</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Click-through rate (CTR) prediction plays an important role in online advertising platforms. Most existing methods use data from the advertising platform itself for CTR prediction. As user behaviors also exist on many other platforms, e.g., media platforms, it is beneficial to further exploit such complementary information for better modeling user interest and for improving CTR prediction performance. However, due to privacy concerns, data from different platforms cannot be uploaded to a server for centralized model training. Vertical federated learning (VFL) provides a possible solution which is able to keep the raw data on respective participating parties and learn a collaborative model in a privacy-preserving way. However, traditional VFL methods only utilize aligned data with common keys across parties, which strongly restricts their application scope. In this paper, we propose FedUD, which is able to exploit unaligned data, in addition to aligned data, for more accurate federated CTR prediction. FedUD contains two steps. In the first step, FedUD utilizes aligned data across parties like traditional VFL, but it additionally includes a knowledge distillation module. This module distills useful knowledge from the guest party's high-level representations and guides the learning of a representation transfer network. In the second step, FedUD applies the learned knowledge to enrich the representations of the host party's unaligned data such that both aligned and unaligned data can contribute to federated model training. Experiments on two real-world datasets demonstrate the superior performance of FedUD for federated CTR prediction.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2407.18471】Constructing the CORD-19 Vaccine Dataset</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18471">https://arxiv.org/abs/2407.18471</a></p>
  <p><b>作者</b>：Manisha Singh,Divy Sharma,Alonso Ma,Bridget Tyree,Margaret Mitchell</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：cater to scientists, scientists specifically, Latent Dirichlet Allocation, author demography, paper</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We introduce new dataset 'CORD-19-Vaccination' to cater to scientists specifically looking into COVID-19 vaccine-related research. This dataset is extracted from CORD-19 dataset [Wang et al., 2020] and augmented with new columns for language detail, author demography, keywords, and topic per paper. Facebook's fastText model is used to identify languages [Joulin et al., 2016]. To establish author demography (author affiliation, lab/institution location, and lab/institution country columns) we processed the JSON file for each paper and then further enhanced using Google's search API to determine country values. 'Yake' was used to extract keywords from the title, abstract, and body of each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to add topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset, we demonstrate a question-answering task like the one used in the CORD-19 Kaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential sentence classification was performed on each paper's abstract using the model from Dernoncourt et al. [2016]. We partially hand annotated the training dataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination' contains 30k research papers and can be immensely valuable for NLP research such as text mining, information extraction, and question answering, specific to the domain of COVID-19 vaccine research.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2407.18470】Synergizing Knowledge Graphs with Large Language Models: A Comprehensive Review and Future Prospects</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18470">https://arxiv.org/abs/2407.18470</a></p>
  <p><b>作者</b>：DaiFeng Li,Fan Xu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, prodigious linguistic capabilities, Large Language, shortcomings including factual</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements have witnessed the ascension of Large Language Models (LLMs), endowed with prodigious linguistic capabilities, albeit marred by shortcomings including factual inconsistencies and opacity. Conversely, Knowledge Graphs (KGs) harbor verifiable knowledge and symbolic reasoning prowess, thereby complementing LLMs' deficiencies. Against this backdrop, the synergy between KGs and LLMs emerges as a pivotal research direction. Our contribution in this paper is a comprehensive dissection of the latest developments in integrating KGs with LLMs. Through meticulous analysis of their confluence points and methodologies, we introduce a unifying framework designed to elucidate and stimulate further exploration among scholars engaged in cognate disciplines. This framework serves a dual purpose: it consolidates extant knowledge while simultaneously delineating novel avenues for real-world deployment, thereby amplifying the translational impact of academic research.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2407.18383】Supporting Evidence-Based Medicine by Finding Both Relevant and Significant Works</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18383">https://arxiv.org/abs/2407.18383</a></p>
  <p><b>作者</b>：Sameh Frihat,Norbert Fuhr</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：approach to improving, underlying empirical evidence, distinct levels based, medical publications, LoE</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we present a new approach to improving the relevance and reliability of medical IR, which builds upon the concept of Level of Evidence (LoE). LoE framework categorizes medical publications into 7 distinct levels based on the underlying empirical evidence. Despite LoE framework's relevance in medical research and evidence-based practice, only few medical publications explicitly state their LoE. Therefore, we develop a classification model for automatically assigning LoE to medical publications, which successfully classifies over 26 million documents in MEDLINE database into LoE classes. The subsequent retrieval experiments on TREC PM datasets show substantial improvements in retrieval relevance, when LoE is used as a search filter.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2407.18331】Using Bibliometrics to Detect Unconventional Authorship Practices and Examine Their Impact on Global Research Metrics, 2019-2023</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18331">https://arxiv.org/abs/2407.18331</a></p>
  <p><b>作者</b>：Lokman I. Meho,Elie A. Akl</p>
  <p><b>类目</b>：Digital Libraries (cs.DL); Computers and Society (cs.CY); Information Retrieval (cs.IR); Physics and Society (physics.soc-ph)</p>
  <p><b>关键词</b>：sixteen universities increased, alongside significant, rise in hyperprolific, increased multi-affiliations, research output</p>
  <p><b>备注</b>： 17 pages, 6 tables, 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Between 2019 and 2023, sixteen universities increased their research output by over fifteen times the global average, alongside significant changes in authorship dynamics (e.g., decreased first authorship, rise in hyperprolific authors, increased multi-affiliations, and increased authors per publication rate). Using bibliometric methods, this study detected patterns suggesting a reliance on unconventional authorship practices, such as gift, honorary, and sold authorship, to inflate publication metrics. The study underscores the need for reforms by universities, policymakers, funding agencies, ranking agencies, accreditation bodies, scholarly publishers, and researchers to maintain academic integrity and ensure the reliability of global ranking systems.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2407.18327】he Structure of Financial Equity Research Reports -- Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18327">https://arxiv.org/abs/2407.18327</a></p>
  <p><b>作者</b>：Adria Pop,Jan Spörer,Siegfried Handschuh</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Computational Engineering, Finance, and Science (cs.CE); Information Retrieval (cs.IR); Computational Finance (q-fin.CP)</p>
  <p><b>关键词</b>：dissects financial equity, research dissects financial, financial equity research, ERRs, dissects financial</p>
  <p><b>备注</b>： JEL classes: C45; G11; G12; G14</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This research dissects financial equity research reports (ERRs) by mapping their content into categories. There is insufficient empirical analysis of the questions answered in ERRs. In particular, it is not understood how frequently certain information appears, what information is considered essential, and what information requires human judgment to distill into an ERR. The study analyzes 72 ERRs sentence-by-sentence, classifying their 4940 sentences into 169 unique question archetypes. We did not predefine the questions but derived them solely from the statements in the ERRs. This approach provides an unbiased view of the content of the observed ERRs. Subsequently, we used public corporate reports to classify the questions' potential for automation. Answers were labeled "text-extractable" if the answers to the question were accessible in corporate reports. 78.7% of the questions in ERRs can be automated. Those automatable question consist of 48.2% text-extractable (suited to processing by large language models, LLMs) and 30.5% database-extractable questions. Only 21.3% of questions require human judgment to answer. We empirically validate using Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language generation and information extraction enable the automation of approximately 80% of the statements in ERRs. Surprisingly, the models complement each other's strengths and weaknesses well. The research confirms that the current writing process of ERRs can likely benefit from additional automation, improving quality and efficiency. The research thus allows us to quantify the potential impacts of introducing large language models in the ERR writing process. The full question list, including the archetypes and their frequency, will be made available online after peer review.</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>【2407.18914】Floating No More: Object-Ground Reconstruction from a Single Image</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18914">https://arxiv.org/abs/2407.18914</a></p>
  <p><b>作者</b>：Yunze Man,Yichen Sheng,Jianming Zhang,Liang-Yan Gui,Yu-Xiong Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Recent advancements, primarily focused, focused on improving, improving the accuracy, object</p>
  <p><b>备注</b>： Project Page: [this https URL](https://yunzeman.github.io/ORG/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in 3D object reconstruction from single images have primarily focused on improving the accuracy of object shapes. Yet, these techniques often fail to accurately capture the inter-relation between the object, ground, and camera. As a result, the reconstructed objects often appear floating or tilted when placed on flat surfaces. This limitation significantly affects 3D-aware image editing applications like shadow rendering and object pose manipulation. To address this issue, we introduce ORG (Object Reconstruction with Ground), a novel task aimed at reconstructing 3D object geometry in conjunction with the ground surface. Our method uses two compact pixel-level representations to depict the relationship between camera, object, and ground. Experiments show that the proposed ORG model can effectively reconstruct object-ground geometry on unseen data, significantly enhancing the quality of shadow generation and pose manipulation compared to conventional single-image 3D reconstruction techniques.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2407.18911】HRP: Human Affordances for Robotic Pre-Training</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18911">https://arxiv.org/abs/2407.18911</a></p>
  <p><b>作者</b>：Mohan Kumar Srirama,Sudeep Dasari,Shikhar Bahl,Abhinav Gupta</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：predict optimal actions, dimensional vision inputs, high dimensional vision, predict optimal, optimal actions</p>
  <p><b>备注</b>： Accepted to Robotics Science and Systems 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In order to *generalize* to various tasks in the wild, robotic agents will need a suitable representation (i.e., vision network) that enables the robot to predict optimal actions given high dimensional vision inputs. However, learning such a representation requires an extreme amount of diverse training data, which is prohibitively expensive to collect on a real robot. How can we overcome this problem? Instead of collecting more robot data, this paper proposes using internet-scale, human videos to extract "affordances," both at the environment and agent level, and distill them into a pre-trained representation. We present a simple framework for pre-training representations on hand, object, and contact "affordance labels" that highlight relevant objects in images and how to interact with them. These affordances are automatically extracted from human video data (with the help of off-the-shelf computer vision modules) and used to fine-tune existing representations. Our approach can efficiently fine-tune *any* existing representation, and results in models with stronger downstream robotic performance across the board. We experimentally demonstrate (using 3000+ robot trials) that this affordance pre-training scheme boosts performance by a minimum of 15% on 5 real-world tasks, which consider three diverse robot morphologies (including a dexterous hand). Unlike prior works in the space, these representations improve performance across 3 different camera views. Quantitatively, we find that our approach leads to higher levels of generalization in out-of-distribution settings. For code, weights, and data check: this https URL</p>
  </details>
</details>
<details>
  <summary>3. <b>【2407.18908】Wolf: Captioning Everything with a World Summarization Framework</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18908">https://arxiv.org/abs/2407.18908</a></p>
  <p><b>作者</b>：Boyi Li,Ligeng Zhu,Ran Tian,Shuhan Tan,Yuxiao Chen,Yao Lu,Yin Cui,Sushant Veer,Max Ehrlich,Jonah Philion,Xinshuo Weng,Fuzhao Xue,Andrew Tao,Ming-Yu Liu,Sanja Fidler,Boris Ivanovic,Trevor Darrell,Jitendra Malik,Song Han,Marco Pavone</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：WOrLd summarization Framework, Vision Language Models, WOrLd summarization, Vision Language, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose Wolf, a WOrLd summarization Framework for accurate video captioning. Wolf is an automated captioning framework that adopts a mixture-of-experts approach, leveraging complementary strengths of Vision Language Models (VLMs). By utilizing both image and video models, our framework captures different levels of information and summarizes them efficiently. Our approach can be applied to enhance video understanding, auto-labeling, and captioning. To evaluate caption quality, we introduce CapScore, an LLM-based metric to assess the similarity and quality of generated captions compared to the ground truth captions. We further build four human-annotated datasets in three domains: autonomous driving, general scenes, and robotics, to facilitate comprehensive comparisons. We show that Wolf achieves superior captioning performance compared to state-of-the-art approaches from the research community (VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For instance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise by 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally, we establish a benchmark for video captioning and introduce a leaderboard, aiming to accelerate advancements in video understanding, captioning, and data alignment. Leaderboard: this https URL.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2407.18907】SHIC: Shape-Image Correspondences with no Keypoint Supervision</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18907">https://arxiv.org/abs/2407.18907</a></p>
  <p><b>作者</b>：Aleksandar Shtedritski,Christian Rupprecht,Andrea Vedaldi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：surface mapping generalizes, mapping generalizes keypoint, generalizes keypoint detection, Canonical surface mapping, surface mapping</p>
  <p><b>备注</b>： ECCV 2024. Project website [this https URL](https://www.robots.ox.ac.uk/~vgg/research/shic/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Canonical surface mapping generalizes keypoint detection by assigning each pixel of an object to a corresponding point in a 3D template. Popularised by DensePose for the analysis of humans, authors have since attempted to apply the concept to more categories, but with limited success due to the high cost of manual supervision. In this work, we introduce SHIC, a method to learn canonical maps without manual supervision which achieves better results than supervised methods for most categories. Our idea is to leverage foundation computer vision models such as DINO and Stable Diffusion that are open-ended and thus possess excellent priors over natural categories. SHIC reduces the problem of estimating image-to-template correspondences to predicting image-to-image correspondences using features from the foundation models. The reduction works by matching images of the object to non-photorealistic renders of the template, which emulates the process of collecting manual annotations for this task. These correspondences are then used to supervise high-quality canonical maps for any object of interest. We also show that image generators can further improve the realism of the template views, which provide an additional source of supervision for the model.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2407.18906】A Scalable Quantum Non-local Neural Network for Image Classification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18906">https://arxiv.org/abs/2407.18906</a></p>
  <p><b>作者</b>：Sparsh Gupta,Debanjan Konar,Vaneet Aggarwal</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG); Quantum Physics (quant-ph)</p>
  <p><b>关键词</b>：traditional convolution operations, non-local neural network, Non-local operations play, computer vision enabling, non-local neural</p>
  <p><b>备注</b>： draft, 13 pages (including references and appendix), 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Non-local operations play a crucial role in computer vision enabling the capture of long-range dependencies through weighted sums of features across the input, surpassing the constraints of traditional convolution operations that focus solely on local neighborhoods. Non-local operations typically require computing pairwise relationships between all elements in a set, leading to quadratic complexity in terms of time and memory. Due to the high computational and memory demands, scaling non-local neural networks to large-scale problems can be challenging. This article introduces a hybrid quantum-classical scalable non-local neural network, referred to as Quantum Non-Local Neural Network (QNL-Net), to enhance pattern recognition. The proposed QNL-Net relies on inherent quantum parallelism to allow the simultaneous processing of a large number of input features enabling more efficient computations in quantum-enhanced feature space and involving pairwise relationships through quantum entanglement. We benchmark our proposed QNL-Net with other quantum counterparts to binary classification with datasets MNIST and CIFAR-10. The simulation findings showcase our QNL-Net achieves cutting-edge accuracy levels in binary image classification among quantum classifiers while utilizing fewer qubits.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2407.18899】Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18899">https://arxiv.org/abs/2407.18899</a></p>
  <p><b>作者</b>：Mengyao Lyu,Tianxiang Hao,Xinhao Xu,Hui Chen,Zijia Lin,Jungong Han,Guiguang Ding</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：related target domain, Active Domain Adaptation, Domain Adaptation, target domain, Domain</p>
  <p><b>备注</b>： ECCV 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Domain Adaptation (DA) facilitates knowledge transfer from a source domain to a related target domain. This paper investigates a practical DA paradigm, namely Source data-Free Active Domain Adaptation (SFADA), where source data becomes inaccessible during adaptation, and a minimum amount of annotation budget is available in the target domain. Without referencing the source data, new challenges emerge in identifying the most informative target samples for labeling, establishing cross-domain alignment during adaptation, and ensuring continuous performance improvements through the iterative query-and-adaptation process. In response, we present learn from the learnt (LFTL), a novel paradigm for SFADA to leverage the learnt knowledge from the source pretrained model and actively iterated models without extra overhead. We propose Contrastive Active Sampling to learn from the hypotheses of the preceding model, thereby querying target samples that are both informative to the current model and persistently challenging throughout active learning. During adaptation, we learn from features of actively selected anchors obtained from previous intermediate models, so that the Visual Persistence-guided Adaptation can facilitate feature distribution alignment and active sample exploitation. Extensive experiments on three widely-used benchmarks show that our LFTL achieves state-of-the-art performance, superior computational efficiency and continuous improvements as the annotation budget increases. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2407.18854】Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18854">https://arxiv.org/abs/2407.18854</a></p>
  <p><b>作者</b>：Yuze Zheng,Zixuan Li,Xiangxian Li,Jinxing Liu,Yuqing Wang,Xiangxu Meng,Lei Meng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：real-world applications due, differing visual perspectives, driven by differing, lighting discrepancies, real-world applications</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image classification models often demonstrate unstable performance in real-world applications due to variations in image information, driven by differing visual perspectives of subject objects and lighting discrepancies. To mitigate these challenges, existing studies commonly incorporate additional modal information matching the visual data to regularize the model's learning process, enabling the extraction of high-quality visual features from complex image regions. Specifically, in the realm of multimodal learning, cross-modal alignment is recognized as an effective strategy, harmonizing different modal information by learning a domain-consistent latent feature space for visual and semantic features. However, this approach may face limitations due to the heterogeneity between multimodal information, such as differences in feature distribution and structure. To address this issue, we introduce a Multimodal Alignment and Reconstruction Network (MARNet), designed to enhance the model's resistance to visual noise. Importantly, MARNet includes a cross-modal diffusion reconstruction module for smoothly and stably blending information across different domains. Experiments conducted on two benchmark datasets, Vireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves the quality of image information extracted by the model. It is a plug-and-play framework that can be rapidly integrated into various image classification frameworks, boosting model performance.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2407.18839】Scalable Group Choreography via Variational Phase Manifold Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18839">https://arxiv.org/abs/2407.18839</a></p>
  <p><b>作者</b>：Nhat Le,Khoa Do,Xuan Bui,Tuong Do,Erman Tjiputra,Quang D.Tran,Anh Nguyen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Generating group dance, Generating group, challenging task, industrial applications, Generating</p>
  <p><b>备注</b>： Accepted at ECCV 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generating group dance motion from the music is a challenging task with several industrial applications. Although several methods have been proposed to tackle this problem, most of them prioritize optimizing the fidelity in dancing movement, constrained by predetermined dancer counts in datasets. This limitation impedes adaptability to real-world applications. Our study addresses the scalability problem in group choreography while preserving naturalness and synchronization. In particular, we propose a phase-based variational generative model for group dance generation on learning a generative manifold. Our method achieves high-fidelity group dance motion and enables the generation with an unlimited number of dancers while consuming only a minimal and constant amount of memory. The intensive experiments on two public datasets show that our proposed method outperforms recent state-of-the-art approaches by a large margin and is scalable to a great number of dancers beyond the training data.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2407.18821】Deep Companion Learning: Enhancing Generalization Through Historical Consistency</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18821">https://arxiv.org/abs/2407.18821</a></p>
  <p><b>作者</b>：Ruizhao Zhu,Venkatesh Saligrama</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Deep Neural Networks, Deep Companion Learning, propose Deep Companion, Neural Networks, Deep Neural</p>
  <p><b>备注</b>： ECCV 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose Deep Companion Learning (DCL), a novel training method for Deep Neural Networks (DNNs) that enhances generalization by penalizing inconsistent model predictions compared to its historical performance. To achieve this, we train a deep-companion model (DCM), by using previous versions of the model to provide forecasts on new inputs. This companion model deciphers a meaningful latent semantic structure within the data, thereby providing targeted supervision that encourages the primary model to address the scenarios it finds most challenging. We validate our approach through both theoretical analysis and extensive experimentation, including ablation studies, on a variety of benchmark datasets (CIFAR-100, Tiny-ImageNet, ImageNet-1K) using diverse architectural models (ShuffleNetV2, ResNet, Vision Transformer, etc.), demonstrating state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2407.18798】Predicting 3D Rigid Body Dynamics with Deep Residual Network</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18798">https://arxiv.org/abs/2407.18798</a></p>
  <p><b>作者</b>：Abiodun Finbarrs Oketunji</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：three-dimensional rigid bodies, deep residual networks, study investigates, investigates the application, deep residual</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study investigates the application of deep residual networks for predicting the dynamics of interacting three-dimensional rigid bodies. We present a framework combining a 3D physics simulator implemented in C++ with a deep learning model constructed using PyTorch. The simulator generates training data encompassing linear and angular motion, elastic collisions, fluid friction, gravitational effects, and damping. Our deep residual network, consisting of an input layer, multiple residual blocks, and an output layer, is designed to handle the complexities of 3D dynamics. We evaluate the network's performance using a datasetof 10,000 simulated scenarios, each involving 3-5 interacting rigid bodies. The model achieves a mean squared error of 0.015 for position predictions and 0.022 for orientation predictions, representing a 25% improvement over baseline methods. Our results demonstrate the network's ability to capture intricate physical interactions, with particular success in predicting elastic collisions and rotational dynamics. This work significantly contributes to physics-informed machine learning by showcasing the immense potential of deep residual networks in modeling complex 3D physical systems. We discuss our approach's limitations and propose future directions for improving generalization to more diverse object shapes and materials.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2407.18792】Benchmarking Dependence Measures to Prevent Shortcut Learning in Medical Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18792">https://arxiv.org/abs/2407.18792</a></p>
  <p><b>作者</b>：Sarah Müller,Louisa Fay,Lisa M. Koch,Sergios Gatidis,Thomas Küstner,Philipp Berens</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：hospital sites, patient backgrounds, acquisition devices, Medical imaging cohorts, Medical imaging</p>
  <p><b>备注</b>： Accepted to the 15th International Workshop on Machine Learning in Medical Imaging (MLMI 2024)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Medical imaging cohorts are often confounded by factors such as acquisition devices, hospital sites, patient backgrounds, and many more. As a result, deep learning models tend to learn spurious correlations instead of causally related features, limiting their generalizability to new and unseen data. This problem can be addressed by minimizing dependence measures between intermediate representations of task-related and non-task-related variables. These measures include mutual information, distance correlation, and the performance of adversarial classifiers. Here, we benchmark such dependence measures for the task of preventing shortcut learning. We study a simplified setting using Morpho-MNIST and a medical imaging task with CheXpert chest radiographs. Our results provide insights into how to mitigate confounding factors in medical imaging.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2407.18715】BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18715">https://arxiv.org/abs/2407.18715</a></p>
  <p><b>作者</b>：Peng Hao,Xiaobing Wang,Yingying Jiang,Hanchao Jia,Xiaoshuai Hao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：challenging task due, Scene Graph Generation, remains a challenging, compositional property, Scene Graph</p>
  <p><b>备注</b>： 9 pages, 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Scene Graph Generation (SGG) remains a challenging task due to its compositional property. Previous approaches improve prediction efficiency by learning in an end-to-end manner. However, these methods exhibit limited performance as they assume unidirectional conditioning between entities and predicates, leading to insufficient information interaction. To address this limitation, we propose a novel bidirectional conditioning factorization for SGG, introducing efficient interaction between entities and predicates. Specifically, we develop an end-to-end scene graph generation model, Bidirectional Conditioning Transformer (BCTR), to implement our factorization. BCTR consists of two key modules. First, the Bidirectional Conditioning Generator (BCG) facilitates multi-stage interactive feature augmentation between entities and predicates, enabling mutual benefits between the two predictions. Second, Random Feature Alignment (RFA) regularizes the feature space by distilling multi-modal knowledge from pre-trained models, enhancing BCTR's ability on tailed categories without relying on statistical priors. We conduct a series of experiments on Visual Genome and Open Image V6, demonstrating that BCTR achieves state-of-the-art performance on both benchmarks. The code will be available upon acceptance of the paper.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2407.18695】PIV3CAMS: a multi-camera dataset for multiple computer vision problems and its application to novel view-point synthesis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18695">https://arxiv.org/abs/2407.18695</a></p>
  <p><b>作者</b>：Sohyeong Kim,Martin Danelljan,Radu Timofte,Luc Van Gool,Jean-Philippe Thiran</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：tasks significantly rely, computer vision tasks, vision tasks significantly, machine learning, computer vision</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The modern approaches for computer vision tasks significantly rely on machine learning, which requires a large number of quality images. While there is a plethora of image datasets with a single type of images, there is a lack of datasets collected from multiple cameras. In this thesis, we introduce Paired Image and Video data from three CAMeraS, namely PIV3CAMS, aimed at multiple computer vision tasks. The PIV3CAMS dataset consists of 8385 pairs of images and 82 pairs of videos taken from three different cameras: Canon D5 Mark IV, Huawei P20, and ZED stereo camera. The dataset includes various indoor and outdoor scenes from different locations in Zurich (Switzerland) and Cheonan (South Korea). Some of the computer vision applications that can benefit from the PIV3CAMS dataset are image/video enhancement, view interpolation, image matching, and much more. We provide a careful explanation of the data collection process and detailed analysis of the data. The second part of this thesis studies the usage of depth information in the view synthesizing task. In addition to the regeneration of a current state-of-the-art algorithm, we investigate several proposed alternative models that integrate depth information geometrically. Through extensive experiments, we show that the effect of depth is crucial in small view changes. Finally, we apply our model to the introduced PIV3CAMS dataset to synthesize novel target views as an example application of PIV3CAMS.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2407.18682】Rapid Object Annotation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18682">https://arxiv.org/abs/2407.18682</a></p>
  <p><b>作者</b>：Misha Denil</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：problem of rapidly, rapidly annotating, annotating a video, video with bounding, bounding boxes</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this report we consider the problem of rapidly annotating a video with bounding boxes for a novel object. We describe a UI and associated workflow designed to make this process fast for an arbitrary novel target.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2407.18673】A Survey on Cell Nuclei Instance Segmentation and Classification: Leveraging Context and Attention</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18673">https://arxiv.org/abs/2407.18673</a></p>
  <p><b>作者</b>：João D. Nunes,Diana Montezuma,Domingos Oliveira,Tania Pereira,Jaime S. Cardoso</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：nuclei instance segmentation, cell nuclei instance, Hematoxylin and Eosin, Slide Images, Manually annotating nuclei</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Manually annotating nuclei from the gigapixel Hematoxylin and Eosin (HE)-stained Whole Slide Images (WSIs) is a laborious and costly task, meaning automated algorithms for cell nuclei instance segmentation and classification could alleviate the workload of pathologists and clinical researchers and at the same time facilitate the automatic extraction of clinically interpretable features. But due to high intra- and inter-class variability of nuclei morphological and chromatic features, as well as HE-stains susceptibility to artefacts, state-of-the-art algorithms cannot correctly detect and classify instances with the necessary performance. In this work, we hypothesise context and attention inductive biases in artificial neural networks (ANNs) could increase the generalization of algorithms for cell nuclei instance segmentation and classification. We conduct a thorough survey on context and attention methods for cell nuclei instance segmentation and classification from HE-stained microscopy imaging, while providing a comprehensive discussion of the challenges being tackled with context and attention. Besides, we illustrate some limitations of current approaches and present ideas for future research. As a case study, we extend both a general instance segmentation and classification method (Mask-RCNN) and a tailored cell nuclei instance segmentation and classification model (HoVer-Net) with context- and attention-based mechanisms, and do a comparative analysis on a multi-centre colon nuclei identification and counting dataset. Although pathologists rely on context at multiple levels while paying attention to specific Regions of Interest (RoIs) when analysing and annotating WSIs, our findings suggest translating that domain knowledge into algorithm design is no trivial task, but to fully exploit these mechanisms, the scientific understanding of these methods should be addressed.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2407.18667】A Labeled Ophthalmic Ultrasound Dataset with Medical Report Generation Based on Cross-modal Deep Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18667">https://arxiv.org/abs/2407.18667</a></p>
  <p><b>作者</b>：Jing Wang,Junyan Fan,Meng Zhou,Yanzhu Zhang,Mingyu Shi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：reveals eye morphology, imaging reveals eye, treating eye diseases, reveals eye, eye morphology</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Ultrasound imaging reveals eye morphology and aids in diagnosing and treating eye diseases. However, interpreting diagnostic reports requires specialized physicians. We present a labeled ophthalmic dataset for the precise analysis and the automated exploration of medical images along with their associated reports. It collects three modal data, including the ultrasound images, blood flow information and examination reports from 2,417 patients at an ophthalmology hospital in Shenyang, China, during the year 2018, in which the patient information is de-identified for privacy protection. To the best of our knowledge, it is the only ophthalmic dataset that contains the three modal information simultaneously. It incrementally consists of 4,858 images with the corresponding free-text reports, which describe 15 typical imaging findings of intraocular diseases and the corresponding anatomical locations. Each image shows three kinds of blood flow indices at three specific arteries, i.e., nine parameter values to describe the spectral characteristics of blood flow distribution. The reports were written by ophthalmologists during the clinical care. The proposed dataset is applied to generate medical report based on the cross-modal deep learning model. The experimental results demonstrate that our dataset is suitable for training supervised models concerning cross-modal medical data.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2407.18665】Local Binary Pattern(LBP) Optimization for Feature Extraction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18665">https://arxiv.org/abs/2407.18665</a></p>
  <p><b>作者</b>：Zeinab Sedaghatjoo,Hossein Hosseinzadeh,Bahram Sadeghi Bigham</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Numerical Analysis (math.NA)</p>
  <p><b>关键词</b>：computer vision techniques, advanced image processing, vision techniques, rapid growth, development of advanced</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rapid growth of image data has led to the development of advanced image processing and computer vision techniques, which are crucial in various applications such as image classification, image segmentation, and pattern recognition. Texture is an important feature that has been widely used in many image processing tasks. Therefore, analyzing and understanding texture plays a pivotal role in image analysis and understanding.Local binary pattern (LBP) is a powerful operator that describes the local texture features of images. This paper provides a novel mathematical representation of the LBP by separating the operator into three matrices, two of which are always fixed and do not depend on the input data. These fixed matrices are analyzed in depth, and a new algorithm is proposed to optimize them for improved classification performance. The optimization process is based on the singular value decomposition (SVD) algorithm. As a result, the authors present optimal LBPs that effectively describe the texture of human face images. Several experiment results presented in this paper convincingly verify the efficiency and superiority of the optimized LBPs for face detection and facial expression recognition tasks.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2407.18658】Adversarial Robustification via Text-to-Image Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18658">https://arxiv.org/abs/2407.18658</a></p>
  <p><b>作者</b>：Daewon Choi,Jongheon Jeong,Huiwon Jang,Jinwoo Shin</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Adversarial robustness, neural networks, requiring plenty, training data, conventionally believed</p>
  <p><b>备注</b>： Code is available at [this https URL](https://github.com/ChoiDae1/robustify-T2I) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Adversarial robustness has been conventionally believed as a challenging property to encode for neural networks, requiring plenty of training data. In the recent paradigm of adopting off-the-shelf models, however, access to their training data is often infeasible or not practical, while most of such models are not originally trained concerning adversarial robustness. In this paper, we develop a scalable and model-agnostic solution to achieve adversarial robustness without using any data. Our intuition is to view recent text-to-image diffusion models as "adaptable" denoisers that can be optimized to specify target tasks. Based on this, we propose: (a) to initiate a denoise-and-classify pipeline that offers provable guarantees against adversarial attacks, and (b) to leverage a few synthetic reference images generated from the text-to-image model that enables novel adaptation schemes. Our experiments show that our data-free scheme applied to the pre-trained CLIP could improve the (provable) adversarial robustness of its diverse zero-shot classification derivatives (while maintaining their accuracy), significantly surpassing prior approaches that utilize the full training data. Not only for CLIP, we also demonstrate that our framework is easily applicable for robustifying other visual classifiers efficiently.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2407.18656】Auto DragGAN: Editing the Generative Image Manifold in an Autoregressive Manner</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18656">https://arxiv.org/abs/2407.18656</a></p>
  <p><b>作者</b>：Pengxiang Cai,Zhiwei Liu,Guibo Zhu,Yunfang Niu,Jinqiao Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：open challenge, remains an open, inference speed, points, target points</p>
  <p><b>备注</b>： This paper has been accepted as a poster paper for ACM Multimedia 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Pixel-level fine-grained image editing remains an open challenge. Previous works fail to achieve an ideal trade-off between control granularity and inference speed. They either fail to achieve pixel-level fine-grained control, or their inference speed requires optimization. To address this, this paper for the first time employs a regression-based network to learn the variation patterns of StyleGAN latent codes during the image dragging process. This method enables pixel-level precision in dragging editing with little time cost. Users can specify handle points and their corresponding target points on any GAN-generated images, and our method will move each handle point to its corresponding target point. Through experimental analysis, we discover that a short movement distance from handle points to target points yields a high-fidelity edited image, as the model only needs to predict the movement of a small portion of pixels. To achieve this, we decompose the entire movement process into multiple sub-processes. Specifically, we develop a transformer encoder-decoder based network named 'Latent Predictor' to predict the latent code motion trajectories from handle points to target points in an autoregressive manner. Moreover, to enhance the prediction stability, we introduce a component named 'Latent Regularizer', aimed at constraining the latent code motion within the distribution of natural images. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) inference speed and image editing performance at the pixel-level granularity.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2407.18637】DynamicTrack: Advancing Gigapixel Tracking in Crowded Scenes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18637">https://arxiv.org/abs/2407.18637</a></p>
  <p><b>作者</b>：Yunqi Zhao,Yuchen Guo,Zheng Cao,Kai Ni,Ruqi Huang,Lu Fang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：scenarios holds numerous, holds numerous potential, numerous potential applications, gigapixel scenarios holds, scenarios holds</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Tracking in gigapixel scenarios holds numerous potential applications in video surveillance and pedestrian analysis. Existing algorithms attempt to perform tracking in crowded scenes by utilizing multiple cameras or group relationships. However, their performance significantly degrades when confronted with complex interaction and occlusion inherent in gigapixel images. In this paper, we introduce DynamicTrack, a dynamic tracking framework designed to address gigapixel tracking challenges in crowded scenes. In particular, we propose a dynamic detector that utilizes contrastive learning to jointly detect the head and body of pedestrians. Building upon this, we design a dynamic association algorithm that effectively utilizes head and body information for matching purposes. Extensive experiments show that our tracker achieves state-of-the-art performance on widely used tracking benchmarks specifically designed for gigapixel crowded scenes.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2407.18626】Every Part Matters: Integrity Verification of Scientific Figures Based on Multimodal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18626">https://arxiv.org/abs/2407.18626</a></p>
  <p><b>作者</b>：Xiang Shi,Jiawei Liu,Yinpeng Liu,Qikai Cheng,Wei Lu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Digital Libraries (cs.DL); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：paper tackles, tackles a key, key issue, scientific figures, Large Language Models</p>
  <p><b>备注</b>： 28 pages, 11 figures, under review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper tackles a key issue in the interpretation of scientific figures: the fine-grained alignment of text and figures. It advances beyond prior research that primarily dealt with straightforward, data-driven visualizations such as bar and pie charts and only offered a basic understanding of diagrams through captioning and classification. We introduce a novel task, Figure Integrity Verification, designed to evaluate the precision of technologies in aligning textual knowledge with visual elements in scientific figures. To support this, we develop a semi-automated method for constructing a large-scale dataset, Figure-seg, specifically designed for this task. Additionally, we propose an innovative framework, Every Part Matters (EPM), which leverages Multimodal Large Language Models (MLLMs) to not only incrementally improve the alignment and verification of text-figure integrity but also enhance integrity through analogical reasoning. Our comprehensive experiments show that these innovations substantially improve upon existing methods, allowing for more precise and thorough analysis of complex scientific figures. This progress not only enhances our understanding of multimodal technologies but also stimulates further research and practical applications across fields requiring the accurate interpretation of complex visual data.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2407.18616】MOoSE: Multi-Orientation Sharing Experts for Open-set Scene Text Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18616">https://arxiv.org/abs/2407.18616</a></p>
  <p><b>作者</b>：Chang Liu,Simon Corbillé,Elisa H Barney Smith</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Open-set text recognition, text recognition field, text recognition, Open-set text, text</p>
  <p><b>备注</b>： Accepted in ICDAR2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Open-set text recognition, which aims to address both novel characters and previously seen ones, is one of the rising subtopics in the text recognition field. However, the current open-set text recognition solutions only focuses on horizontal text, which fail to model the real-life challenges posed by the variety of writing directions in real-world scene text. Multi-orientation text recognition, in general, faces challenges from the diverse image aspect ratios, significant imbalance in data amount, and domain gaps between orientations. In this work, we first propose a Multi-Oriented Open-Set Text Recognition task (MOOSTR) to model the challenges of both novel characters and writing direction variety. We then propose a Multi-Orientation Sharing Experts (MOoSE) framework as a strong baseline solution. MOoSE uses a mixture-of-experts scheme to alleviate the domain gaps between orientations, while exploiting common structural knowledge among experts to alleviate the data scarcity that some experts face. The proposed MOoSE framework is validated by ablative experiments, and also tested for feasibility on the existing open-set benchmark. Code, models, and documents are available at: this https URL</p>
  </details>
</details>
<details>
  <summary>23. <b>【2407.18614】LookupForensics: A Large-Scale Multi-Task Dataset for Multi-Phase Image-Based Fact Verification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18614">https://arxiv.org/abs/2407.18614</a></p>
  <p><b>作者</b>：Shuhan Cui,Huy H. Nguyen,Trung-Nghia Le,Chun-Shien Lu,Isao Echizen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：identify forged content, Amid the proliferation, forged content, identify forged, notably the tsunami</p>
  <p><b>备注</b>： Pages 1-13 are the main body of the paper, and pages 14-16 are the supplementary material</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Amid the proliferation of forged images, notably the tsunami of deepfake content, extensive research has been conducted on using artificial intelligence (AI) to identify forged content in the face of continuing advancements in counterfeiting technologies. We have investigated the use of AI to provide the original authentic image after deepfake detection, which we believe is a reliable and persuasive solution. We call this "image-based automated fact verification," a name that originated from a text-based fact-checking system used by journalists. We have developed a two-phase open framework that integrates detection and retrieval components. Additionally, inspired by a dataset proposed by Meta Fundamental AI Research, we further constructed a large-scale dataset that is specifically designed for this task. This dataset simulates real-world conditions and includes both content-preserving and content-aware manipulations that present a range of difficulty levels and have potential for ongoing research. This multi-task dataset is fully annotated, enabling it to be utilized for sub-tasks within the forgery identification and fact retrieval domains. This paper makes two main contributions: (1) We introduce a new task, "image-based automated fact verification," and present a novel two-phase open framework combining "forgery identification" and "fact retrieval." (2) We present a large-scale dataset tailored for this new task that features various hand-crafted image edits and machine learning-driven manipulations, with extensive annotations suitable for various sub-tasks. Extensive experimental results validate its practicality for fact verification research and clarify its difficulty levels for various sub-tasks.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2407.18613】Dilated Strip Attention Network for Image Restoration</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18613">https://arxiv.org/abs/2407.18613</a></p>
  <p><b>作者</b>：Fangwei Hao,Jiesheng Wu,Ji Du,Yinjie Wang,Jing Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：latent sharp image, image restoration tasks, Image restoration, deteriorated counterpart, seeks to recover</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image restoration is a long-standing task that seeks to recover the latent sharp image from its deteriorated counterpart. Due to the robust capacity of self-attention to capture long-range dependencies, transformer-based methods or some attention-based convolutional neural networks have demonstrated promising results on many image restoration tasks in recent years. However, existing attention modules encounters limited receptive fields or abundant parameters. In order to integrate contextual information more effectively and efficiently, in this paper, we propose a dilated strip attention network (DSAN) for image restoration. Specifically, to gather more contextual information for each pixel from its neighboring pixels in the same row or column, a dilated strip attention (DSA) mechanism is elaborately proposed. By employing the DSA operation horizontally and vertically, each location can harvest the contextual information from a much wider region. In addition, we utilize multi-scale receptive fields across different feature groups in DSA to improve representation learning. Extensive experiments show that our DSAN outperforms state-of-the-art algorithms on several image restoration tasks.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2407.18611】IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18611">https://arxiv.org/abs/2407.18611</a></p>
  <p><b>作者</b>：Jingpeng Xie,Shiyu Tan,Yuanlei Wang,Yizhen Lao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Urban-level three-dimensional reconstruction, minimizing computational costs, modern applications demands, applications demands high, Neural Radiance Fields</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Urban-level three-dimensional reconstruction for modern applications demands high rendering fidelity while minimizing computational costs. The advent of Neural Radiance Fields (NeRF) has enhanced 3D reconstruction, yet it exhibits artifacts under multiple viewpoints. In this paper, we propose a new NeRF framework method to address these issues. Our method uses image content and pose data to iteratively plan the next best view. A crucial aspect of this method involves uncertainty estimation, guiding the selection of views with maximum information gain from a candidate set. This iterative process enhances rendering quality over time. Simultaneously, we introduce the Vonoroi diagram and threshold sampling together with flight classifier to boost the efficiency, while keep the original NeRF network intact. It can serve as a plug-in tool to assist in better rendering, outperforming baselines and similar prior works.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2407.18595】LinguaLinker: Audio-Driven Portraits Animation with Implicit Facial Control Enhancement</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18595">https://arxiv.org/abs/2407.18595</a></p>
  <p><b>作者</b>：Rui Zhang,Yixiao Fang,Zhengnan Lu,Pei Cheng,Zebiao Huang,Bin Fu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：synchronizing facial dynamics, multilingual audio inputs, audio-driven visual synthesis, visually compelling, study delves</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study delves into the intricacies of synchronizing facial dynamics with multilingual audio inputs, focusing on the creation of visually compelling, time-synchronized animations through diffusion-based techniques. Diverging from traditional parametric models for facial animation, our approach, termed LinguaLinker, adopts a holistic diffusion-based framework that integrates audio-driven visual synthesis to enhance the synergy between auditory stimuli and visual responses. We process audio features separately and derive the corresponding control gates, which implicitly govern the movements in the mouth, eyes, and head, irrespective of the portrait's origin. The advanced audio-driven visual synthesis mechanism provides nuanced control but keeps the compatibility of output video and input audio, allowing for a more tailored and effective portrayal of distinct personas across different languages. The significant improvements in the fidelity of animated portraits, the accuracy of lip-syncing, and the appropriate motion variations achieved by our method render it a versatile tool for animating any portrait in any language.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2407.18593】Content-driven Magnitude-Derivative Spectrum Complementary Learning for Hyperspectral Image Classification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18593">https://arxiv.org/abs/2407.18593</a></p>
  <p><b>作者</b>：Huiyan Bai,Tingfa Xu,Huan Chen,Peifu Liu,Jianan Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：HSI classification, complex spectral details, hyperspectral image, classification is pivotal, HSI</p>
  <p><b>备注</b>： accepted by TGRS</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Extracting discriminative information from complex spectral details in hyperspectral image (HSI) for HSI classification is pivotal. While current prevailing methods rely on spectral magnitude features, they could cause confusion in certain classes, resulting in misclassification and decreased accuracy. We find that the derivative spectrum proves more adept at capturing concealed information, thereby offering a distinct advantage in separating these confusion classes. Leveraging the complementarity between spectral magnitude and derivative features, we propose a Content-driven Spectrum Complementary Network based on Magnitude-Derivative Dual Encoder, employing these two features as combined inputs. To fully utilize their complementary information, we raise a Content-adaptive Point-wise Fusion Module, enabling adaptive fusion of dual-encoder features in a point-wise selective manner, contingent upon feature representation. To preserve a rich source of complementary information while extracting more distinguishable features, we introduce a Hybrid Disparity-enhancing Loss that enhances the differential expression of the features from the two branches and increases the inter-class distance. As a result, our method achieves state-of-the-art results on the extensive WHU-OHS dataset and eight other benchmark datasets.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2407.18590】From 2D to 3D: AISG-SLA Visual Localization Challenge</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18590">https://arxiv.org/abs/2407.18590</a></p>
  <p><b>作者</b>：Jialin Gao,Bill Ong,Darld Lwi,Zhen Hao Ng,Xun Wei Yee,Mun-Thye Mak,Wee Siong Ng,See-Kiong Ng,Hui Ying Teo,Victor Khoo,Georg Bökman,Johan Edstedt,Kirill Brodt,Clémentin Boittiaux,Maxime Ferrera,Stepan Konev</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：smart city applications, mapping is crucial, city applications, cost of acquiring, hinders progress</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Research in 3D mapping is crucial for smart city applications, yet the cost of acquiring 3D data often hinders progress. Visual localization, particularly monocular camera position estimation, offers a solution by determining the camera's pose solely through visual cues. However, this task is challenging due to limited data from a single camera. To tackle these challenges, we organized the AISG-SLA Visual Localization Challenge (VLC) at IJCAI 2023 to explore how AI can accurately extract camera pose data from 2D images in 3D space. The challenge attracted over 300 participants worldwide, forming 50+ teams. Winning teams achieved high accuracy in pose estimation using images from a car-mounted camera with low frame rates. The VLC dataset is available for research purposes upon request via vlc-dataset@aisingapore.org.</p>
  </details>
</details>
<details>
  <summary>29. <b>【2407.18589】HICEScore: A Hierarchical Metric for Image Captioning Evaluation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18589">https://arxiv.org/abs/2407.18589</a></p>
  <p><b>作者</b>：Zequn Zeng,Jianqiao Sun,Hao Zhang,Tiansheng Wen,Yudi Su,Yan Xie,Zhengjue Wang,Bo Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Image captioning evaluation, Image captioning, captioning evaluation, Hierarchical Image Captioning, reference-free metrics</p>
  <p><b>备注</b>： Accepted by ACM MM2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image captioning evaluation metrics can be divided into two categories, reference-based metrics and reference-free metrics. However, reference-based approaches may struggle to evaluate descriptive captions with abundant visual details produced by advanced multimodal large language models, due to their heavy reliance on limited human-annotated references. In contrast, previous reference-free metrics have been proven effective via CLIP cross-modality similarity. Nonetheless, CLIP-based metrics, constrained by their solution of global image-text compatibility, often have a deficiency in detecting local textual hallucinations and are insensitive to small visual objects. Besides, their single-scale designs are unable to provide an interpretable evaluation process such as pinpointing the position of caption mistakes and identifying visual regions that have not been described. To move forward, we propose a novel reference-free metric for image captioning evaluation, dubbed Hierarchical Image Captioning Evaluation Score (HICE-S). By detecting local visual regions and textual phrases, HICE-S builds an interpretable hierarchical scoring mechanism, breaking through the barriers of the single-scale structure of existing reference-free metrics. Comprehensive experiments indicate that our proposed metric achieves the SOTA performance on several benchmarks, outperforming existing reference-free metrics like CLIP-S and PAC-S, and reference-based metrics like METEOR and CIDEr. Moreover, several case studies reveal that the assessment process of HICE-S on detailed captions closely resembles interpretable human judgments.Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2407.18574】Learning to Enhance Aperture Phasor Field for Non-Line-of-Sight Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18574">https://arxiv.org/abs/2407.18574</a></p>
  <p><b>作者</b>：In Cho,Hyunbo Shim,Seon Joo Kim</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：practical NLOS imaging, NLOS imaging, scan areas, practical NLOS, paper aims</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper aims to facilitate more practical NLOS imaging by reducing the number of samplings and scan areas. To this end, we introduce a phasor-based enhancement network that is capable of predicting clean and full measurements from noisy partial observations. We leverage a denoising autoencoder scheme to acquire rich and noise-robust representations in the measurement space. Through this pipeline, our enhancement network is trained to accurately reconstruct complete measurements from their corrupted and partial counterparts. However, we observe that the \naive application of denoising often yields degraded and over-smoothed results, caused by unnecessary and spurious frequency signals present in measurements. To address this issue, we introduce a phasor-based pipeline designed to limit the spectrum of our network to the frequency range of interests, where the majority of informative signals are detected. The phasor wavefronts at the aperture, which are band-limited signals, are employed as inputs and outputs of the network, guiding our network to learn from the frequency range of interests and discard unnecessary information. The experimental results in more practical acquisition scenarios demonstrate that we can look around the corners with $16\times$ or $64\times$ fewer samplings and $4\times$ smaller apertures. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2407.18568】Learning Spectral-Decomposed Tokens for Domain Generalized Semantic Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18568">https://arxiv.org/abs/2407.18568</a></p>
  <p><b>作者</b>：Jingjun Yi,Qi Bi,Hao Zheng,Haolan Zhan,Wei Ji,Yawen Huang,Yuexiang Li,Yefeng Zheng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Vision Foundation Model, brings inherent out-domain, Foundation Model, inherent out-domain generalization, Vision Foundation</p>
  <p><b>备注</b>： accecpted by ACM MM2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rapid development of Vision Foundation Model (VFM) brings inherent out-domain generalization for a variety of down-stream tasks. Among them, domain generalized semantic segmentation (DGSS) holds unique challenges as the cross-domain images share common pixel-wise content information but vary greatly in terms of the style. In this paper, we present a novel Spectral-dEcomposed Token (SET) learning framework to advance the frontier. Delving into further than existing fine-tuning token  frozen backbone paradigm, the proposed SET especially focuses on the way learning style-invariant features from these learnable tokens. Particularly, the frozen VFM features are first decomposed into the phase and amplitude components in the frequency space, which mainly contain the information of content and style, respectively, and then separately processed by learnable tokens for task-specific information extraction. After the decomposition, style variation primarily impacts the token-based feature enhancement within the amplitude branch. To address this issue, we further develop an attention optimization method to bridge the gap between style-affected representation and static tokens during inference. Extensive cross-domain experiments show its state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>32. <b>【2407.18559】VSSD: Vision Mamba with Non-Casual State Space Duality</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18559">https://arxiv.org/abs/2407.18559</a></p>
  <p><b>作者</b>：Yuheng Shi,Minjing Dong,Mingjia Li,Chang Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：global receptive field, offering robust modeling, robust modeling capabilities, State Space Duality, State Space</p>
  <p><b>备注</b>： 16 pages, 5 figures, 7 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Vision transformers have significantly advanced the field of computer vision, offering robust modeling capabilities and global receptive field. However, their high computational demands limit their applicability in processing long sequences. To tackle this issue, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. Recently, State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. However, the inherent causal nature of SSD/SSMs restricts their applications in non-causal vision tasks. To address this limitation, we introduce Visual State Space Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we propose to discard the magnitude of interactions between the hidden state and tokens while preserving their relative weights, which relieves the dependencies of token contribution on previous tokens. Together with the involvement of multi-scan strategies, we show that the scanning results can be integrated to achieve non-causality, which not only improves the performance of SSD in vision tasks but also enhances its efficiency. We conduct extensive experiments on various benchmarks including image classification, detection, and segmentation, where VSSD surpasses existing state-of-the-art SSM-based models. Code and weights are available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2407.18554】Skin Cancer Detection utilizing Deep Learning: Classification of Skin Lesion Images using a Vision Transformer</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18554">https://arxiv.org/abs/2407.18554</a></p>
  <p><b>作者</b>：Carolin Flosdorf,Justin Engelker,Igor Keller,Nicolas Mohr</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：represents a major, major challenge, Common detection methods, Skin cancer detection, detection still represents</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Skin cancer detection still represents a major challenge in healthcare. Common detection methods can be lengthy and require human assistance which falls short in many countries. Previous research demonstrates how convolutional neural networks (CNNs) can help effectively through both automation and an accuracy that is comparable to the human level. However, despite the progress in previous decades, the precision is still limited, leading to substantial misclassifications that have a serious impact on people's health. Hence, we employ a Vision Transformer (ViT) that has been developed in recent years based on the idea of a self-attention mechanism, specifically two configurations of a pre-trained ViT. We generally find superior metrics for classifying skin lesions after comparing them to base models such as decision tree classifier and k-nearest neighbor (KNN) classifier, as well as to CNNs and less complex ViTs. In particular, we attach greater importance to the performance of melanoma, which is the most lethal type of skin cancer. The ViT-L32 model achieves an accuracy of 91.57% and a melanoma recall of 58.54%, while ViT-L16 achieves an accuracy of 92.79% and a melanoma recall of 56.10%. This offers a potential tool for faster and more accurate diagnoses and an overall improvement for the healthcare sector.</p>
  </details>
</details>
<details>
  <summary>34. <b>【2407.18552】Multimodal Emotion Recognition using Audio-Video Transformer Fusion with Cross Attention</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18552">https://arxiv.org/abs/2407.18552</a></p>
  <p><b>作者</b>：Joe Dhanith P R,Shravan Venkatraman,Vigya Sharma,Santhosh Malarvannan</p>
  <p><b>类目</b>：Multimedia (cs.MM); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：human communication, fundamental aspect, aspect of human, Cross Attention, single data source</p>
  <p><b>备注</b>： 38 Pages, 9 Tables, 12 Figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Understanding emotions is a fundamental aspect of human communication. Integrating audio and video signals offers a more comprehensive understanding of emotional states compared to traditional methods that rely on a single data source, such as speech or facial expressions. Despite its potential, multimodal emotion recognition faces significant challenges, particularly in synchronization, feature extraction, and fusion of diverse data sources. To address these issues, this paper introduces a novel transformer-based model named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA model employs a transformer fusion approach to effectively capture and synchronize interlinked features from both audio and video inputs, thereby resolving synchronization problems. Additionally, the Cross Attention mechanism within AVT-CA selectively extracts and emphasizes critical features while discarding irrelevant ones from both modalities, addressing feature extraction and fusion challenges. Extensive experimental analysis conducted on the CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the proposed model. The results underscore the importance of AVT-CA in developing precise and reliable multimodal emotion recognition systems for practical applications.</p>
  </details>
</details>
<details>
  <summary>35. <b>【2407.18534】Boosting Cross-Domain Point Classification via Distilling Relational Priors from 2D Transformers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18534">https://arxiv.org/abs/2407.18534</a></p>
  <p><b>作者</b>：Longkun Zou,Wanru Zhu,Ke Chen,Lihua Guo,Kailing Guo,Kui Jia,Yaowei Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Semantic pattern, local geometries, Semantic, local, point cloud</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Semantic pattern of an object point cloud is determined by its topological configuration of local geometries. Learning discriminative representations can be challenging due to large shape variations of point sets in local regions and incomplete surface in a global perspective, which can be made even more severe in the context of unsupervised domain adaptation (UDA). In specific, traditional 3D networks mainly focus on local geometric details and ignore the topological structure between local geometries, which greatly limits their cross-domain generalization. Recently, the transformer-based models have achieved impressive performance gain in a range of image-based tasks, benefiting from its strong generalization capability and scalability stemming from capturing long range correlation across local patches. Inspired by such successes of visual transformers, we propose a novel Relational Priors Distillation (RPD) method to extract relational priors from the well-trained transformers on massive images, which can significantly empower cross-domain representations with consistent topological priors of objects. To this end, we establish a parameter-frozen pre-trained transformer module shared between 2D teacher and 3D student models, complemented by an online knowledge distillation strategy for semantically regularizing the 3D student model. Furthermore, we introduce a novel self-supervised task centered on reconstructing masked point cloud patches using corresponding masked multi-view image features, thereby empowering the model with incorporating 3D geometric information. Experiments on the PointDA-10 and the Sim-to-Real datasets verify that the proposed method consistently achieves the state-of-the-art performance of UDA for point cloud classification. The source code of this work is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2407.18524】She Works, He Works: A Curious Exploration of Gender Bias in AI-Generated Imagery</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18524">https://arxiv.org/abs/2407.18524</a></p>
  <p><b>作者</b>：Amalia Foka</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：paper examines gender, examines gender bias, construction workers, highlighting discrepancies, Griselda Pollock theories</p>
  <p><b>备注</b>： 11 pages, 8 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper examines gender bias in AI-generated imagery of construction workers, highlighting discrepancies in the portrayal of male and female figures. Grounded in Griselda Pollock's theories on visual culture and gender, the analysis reveals that AI models tend to sexualize female figures while portraying male figures as more authoritative and competent. These findings underscore AI's potential to mirror and perpetuate societal biases, emphasizing the need for critical engagement with AI-generated content. The project contributes to discussions on the ethical implications of AI in creative practices and its broader impact on cultural perceptions of gender.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2407.18520】xt-Region Matching for Multi-Label Image Recognition with Missing Labels</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18520">https://arxiv.org/abs/2407.18520</a></p>
  <p><b>作者</b>：Leilei Ma,Hongxing Xie,Lei Wang,Yanping Fu,Dengdi Sun,Haifeng Zhao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：demonstrated impressive performance, large-scale visual language, visual language pre-trained, language pre-trained, models have demonstrated</p>
  <p><b>备注</b>： Accepted to ACM International Conference on Multimedia (ACM MM) 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, large-scale visual language pre-trained (VLP) models have demonstrated impressive performance across various downstream tasks. Motivated by these advancements, pioneering efforts have emerged in multi-label image recognition with missing labels, leveraging VLP prompt-tuning technology. However, they usually cannot match text and vision features well, due to complicated semantics gaps and missing labels in a multi-label image. To tackle this challenge, we propose \textbf{T}ext-\textbf{R}egion \textbf{M}atching for optimizing \textbf{M}ulti-\textbf{L}abel prompt tuning, namely TRM-ML, a novel method for enhancing meaningful cross-modal matching. Compared to existing methods, we advocate exploring the information of category-aware regions rather than the entire image or pixels, which contributes to bridging the semantic gap between textual and visual representations in a one-to-one matching manner. Concurrently, we further introduce multimodal contrastive learning to narrow the semantic gap between textual and visual modalities and establish intra-class and inter-class relationships. Additionally, to deal with missing labels, we propose a multimodal category prototype that leverages intra- and inter-category semantic relationships to estimate unknown labels, facilitating pseudo-label generation. Extensive experiments on the MS-COCO, PASCAL VOC, Visual Genome, NUS-WIDE, and CUB-200-211 benchmark datasets demonstrate that our proposed framework outperforms the state-of-the-art methods by a significant margin. Our code is available here\href{this https URL}{\raisebox{-1pt}{\faGithub}}.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2407.18500】Revisit Event Generation Model: Self-Supervised Learning of Event-to-Video Reconstruction with Implicit Neural Representations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18500">https://arxiv.org/abs/2407.18500</a></p>
  <p><b>作者</b>：Zipeng Wang,Yunfan Lu,Lin Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：frame-based computer vision, Reconstructing intensity frames, maintaining high temporal, high temporal resolution, Reconstructing intensity</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Reconstructing intensity frames from event data while maintaining high temporal resolution and dynamic range is crucial for bridging the gap between event-based and frame-based computer vision. Previous approaches have depended on supervised learning on synthetic data, which lacks interpretability and risk over-fitting to the setting of the event simulator. Recently, self-supervised learning (SSL) based methods, which primarily utilize per-frame optical flow to estimate intensity via photometric constancy, has been actively investigated. However, they are vulnerable to errors in the case of inaccurate optical flow. This paper proposes a novel SSL event-to-video reconstruction approach, dubbed EvINR, which eliminates the need for labeled data or optical flow estimation. Our core idea is to reconstruct intensity frames by directly addressing the event generation model, essentially a partial differential equation (PDE) that describes how events are generated based on the time-varying brightness signals. Specifically, we utilize an implicit neural representation (INR), which takes in spatiotemporal coordinate $(x, y, t)$ and predicts intensity values, to represent the solution of the event generation equation. The INR, parameterized as a fully-connected Multi-layer Perceptron (MLP), can be optimized with its temporal derivatives supervised by events. To make EvINR feasible for online requisites, we propose several acceleration techniques that substantially expedite the training process. Comprehensive experiments demonstrate that our EvINR surpasses previous SSL methods by 38% w.r.t. Mean Squared Error (MSE) and is comparable or superior to SoTA supervised methods. Project page: this https URL.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2407.18497】Answerability Fields: Answerable Location Estimation via Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18497">https://arxiv.org/abs/2407.18497</a></p>
  <p><b>作者</b>：Daichi Azuma,Taiki Miyanishi,Shuhei Kurita,Koya Sakamoto,Motoaki Kawanabe</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：critical research endeavor, Answerability Fields, intelligence and robotics, enabling machines, research endeavor</p>
  <p><b>备注</b>： IROS2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In an era characterized by advancements in artificial intelligence and robotics, enabling machines to interact with and understand their environment is a critical research endeavor. In this paper, we propose Answerability Fields, a novel approach to predicting answerability within complex indoor environments. Leveraging a 3D question answering dataset, we construct a comprehensive Answerability Fields dataset, encompassing diverse scenes and questions from ScanNet. Using a diffusion model, we successfully infer and evaluate these Answerability Fields, demonstrating the importance of objects and their locations in answering questions within a scene. Our results showcase the efficacy of Answerability Fields in guiding scene-understanding tasks, laying the foundation for their application in enhancing interactions between intelligent agents and their environments.</p>
  </details>
</details>
<details>
  <summary>40. <b>【2407.18492】Neural Modulation Alteration to Positive and Negative Emotions in Depressed Patients: Insights from fMRI Using Positive/Negative Emotion Atlas</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18492">https://arxiv.org/abs/2407.18492</a></p>
  <p><b>作者</b>：Yu Feng,Weiming Zeng,Yifan Xie,Hongyu Chen,Lei Wang,Yingying Wang,Hongjie Yan,Kaile Zhang,Ran Tao,Wai Ting Siok,Nizhuan Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：emotions remain elusive, negative emotions remain, depressed patients show, remain elusive, precise neural modulation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Background: Although it has been noticed that depressed patients show differences in processing emotions, the precise neural modulation mechanisms of positive and negative emotions remain elusive. FMRI is a cutting-edge medical imaging technology renowned for its high spatial resolution and dynamic temporal information, making it particularly suitable for the neural dynamics of depression research. Methods: To address this gap, our study firstly leveraged fMRI to delineate activated regions associated with positive and negative emotions in healthy individuals, resulting in the creation of positive emotion atlas (PEA) and negative emotion atlas (NEA). Subsequently, we examined neuroimaging changes in depression patients using these atlases and evaluated their diagnostic performance based on machine learning. Results: Our findings demonstrate that the classification accuracy of depressed patients based on PEA and NEA exceeded 0.70, a notable improvement compared to the whole-brain atlases. Furthermore, ALFF analysis unveiled significant differences between depressed patients and healthy controls in eight functional clusters during the NEA, focusing on the left cuneus, cingulate gyrus, and superior parietal lobule. In contrast, the PEA revealed more pronounced differences across fifteen clusters, involving the right fusiform gyrus, parahippocampal gyrus, and inferior parietal lobule. Limitations: Due to the limited sample size and subtypes of depressed patients, the efficacy may need further validation in future. Conclusions: These findings emphasize the complex interplay between emotion modulation and depression, showcasing significant alterations in both PEA and NEA among depression patients. This research enhances our understanding of emotion modulation in depression, with implications for diagnosis and treatment evaluation.</p>
  </details>
</details>
<details>
  <summary>41. <b>【2407.18487】SMPISD-MTPNet: Scene Semantic Prior-Assisted Infrared Ship Detection Using Multi-Task Perception Networks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18487">https://arxiv.org/abs/2407.18487</a></p>
  <p><b>作者</b>：Chen Hu,Xiaogang Dong,Yian Huang Lele Wang,Liang Xu,Tian Pu,Zhenming Peng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：scene semantic extraction, received increasing attention, Scene Semantic, Scene Segmentation Module, recent years due</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Infrared ship detection (IRSD) has received increasing attention in recent years due to the robustness of infrared images to adverse weather. However, a large number of false alarms may occur in complex scenes. To address these challenges, we propose the Scene Semantic Prior-Assisted Multi-Task Perception Network (SMPISD-MTPNet), which includes three stages: scene semantic extraction, deep feature extraction, and prediction. In the scene semantic extraction stage, we employ a Scene Semantic Extractor (SSE) to guide the network by the features extracted based on expert knowledge. In the deep feature extraction stage, a backbone network is employed to extract deep features. These features are subsequently integrated by a fusion network, enhancing the detection capabilities across targets of varying sizes. In the prediction stage, we utilize the Multi-Task Perception Module, which includes the Gradient-based Module and the Scene Segmentation Module, enabling precise detection of small and dim targets within complex scenes. For the training process, we introduce the Soft Fine-tuning training strategy to suppress the distortion caused by data augmentation. Besides, due to the lack of a publicly available dataset labelled for scenes, we introduce the Infrared Ship Dataset with Scene Segmentation (IRSDSS). Finally, we evaluate the network and compare it with state-of-the-art (SOTA) methods, indicating that SMPISD-MTPNet outperforms existing approaches. The source code and dataset for this research can be accessed at this https URL.</p>
  </details>
</details>
<details>
  <summary>42. <b>【2407.18466】A Progressive Single-Modality to Multi-Modality Classification Framework for Alzheimer's Disease Sub-type Diagnosis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18466">https://arxiv.org/abs/2407.18466</a></p>
  <p><b>作者</b>：Yuxiao Liu,Mianxin Liu,Yuanwang Zhang,Kaicong Sun,Dinggang Shen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Alzheimer disease, involves multiple modalities, diagnosis, multiple modalities, distinct usage</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The current clinical diagnosis framework of Alzheimer's disease (AD) involves multiple modalities acquired from multiple diagnosis stages, each with distinct usage and cost. Previous AD diagnosis research has predominantly focused on how to directly fuse multiple modalities for an end-to-end one-stage diagnosis, which practically requires a high cost in data acquisition. Moreover, a significant part of these methods diagnose AD without considering clinical guideline and cannot offer accurate sub-type diagnosis. In this paper, by exploring inter-correlation among multiple modalities, we propose a novel progressive AD sub-type diagnosis framework, aiming to give diagnosis results based on easier-to-access modalities in earlier low-cost stages, instead of modalities from all stages. Specifically, first, we design 1) a text disentanglement network for better processing tabular data collected in the initial stage, and 2) a modality fusion module for fusing multi-modality features separately. Second, we align features from modalities acquired in earlier low-cost stage(s) with later high-cost stage(s) to give accurate diagnosis without actual modality acquisition in later-stage(s) for saving cost. Furthermore, we follow the clinical guideline to align features at each stage for achieving sub-type diagnosis. Third, we leverage a progressive classifier that can progressively include additional acquired modalities (if needed) for diagnosis, to achieve the balance between diagnosis cost and diagnosis performance. We evaluate our proposed framework on large diverse public and in-home datasets (8280 in total) and achieve superior performance over state-of-the-art methods. Our codes will be released after the acceptance.</p>
  </details>
</details>
<details>
  <summary>43. <b>【2407.18450】xtile Anomaly Detection: Evaluation of the State-of-the-Art for Automated Quality Inspection of Carpet</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18450">https://arxiv.org/abs/2407.18450</a></p>
  <p><b>作者</b>：Briony Forsberg,Dr Henry Williams,Prof Bruce MacDonald,Tracy Chen,Dr Kirstine Hulse</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：automated anomaly inspection, unsupervised detection models, purpose of automated, automated anomaly, wool carpets</p>
  <p><b>备注</b>： Accepted at the 2023 Australasian Conference on Robotics and Automation (ACRA 2023) Publication url [this https URL](https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184380272&partnerID=40&md5=74fde263f4a24a1bff75d6560b423994) ISSN: 14482053 Contains 10 pages and three figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this study, state-of-the-art unsupervised detection models were evaluated for the purpose of automated anomaly inspection of wool carpets. A custom dataset of four unique types of carpet textures was created to thoroughly test the models and their robustness in detecting subtle anomalies in complex textures. Due to the requirements of an inline inspection system in a manufacturing use case, the metrics of importance in this study were accuracy in detecting anomalous areas, the number of false detections, and the inference times of each model for real-time performance. Of the evaluated models, the student-teacher network based methods were found on average to yield the highest detection accuracy and lowest false detection rates. When trained on a multi-class dataset the models were found to yield comparable if not better results than single-class training. Finally, in terms of detection speed, with exception to the generative model, all other evaluated models were found to have comparable inference times on a GPU, with an average of 0.16s per image. On a CPU, most of these models typically produced results between 1.5 to 2 times the respective GPU inference times.</p>
  </details>
</details>
<details>
  <summary>44. <b>【2407.18443】HybridDepth: Robust Depth Fusion for Mobile AR by Leveraging Depth from Focus and Single-Image Priors</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18443">https://arxiv.org/abs/2407.18443</a></p>
  <p><b>作者</b>：Ashkan Ganj,Hang Su,Tian Guo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：hardware heterogeneity, depth, robust depth estimation, addresses the unique, unique challenges</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose HYBRIDDEPTH, a robust depth estimation pipeline that addresses the unique challenges of depth estimation for mobile AR, such as scale ambiguity, hardware heterogeneity, and generalizability. HYBRIDDEPTH leverages the camera features available on mobile devices. It effectively combines the scale accuracy inherent in Depth from Focus (DFF) methods with the generalization capabilities enabled by strong single-image depth priors. By utilizing the focal planes of a mobile camera, our approach accurately captures depth values from focused pixels and applies these values to compute scale and shift parameters for transforming relative depths into metric depths. We test our pipeline as an end-to-end system, with a newly developed mobile client to capture focal stacks, which are then sent to a GPU-powered server for depth estimation.
Through comprehensive quantitative and qualitative analyses, we demonstrate that HYBRIDDEPTH not only outperforms state-of-the-art (SOTA) models in common datasets (DDFF12, NYU Depth v2) and a real-world AR dataset ARKitScenes but also demonstrates strong zero-shot generalization. For example, HYBRIDDEPTH trained on NYU Depth v2 achieves comparable performance on the DDFF12 to existing models trained on DDFF12. it also outperforms all the SOTA models in zero-shot performance on the ARKitScenes dataset. Additionally, we conduct a qualitative comparison between our model and the ARCore framework, demonstrating that our models output depth maps are significantly more accurate in terms of structural details and metric accuracy. The source code of this project is available at github.
</p><p>Subjects:</p>
<p>Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)</p>
<p>Cite as:<br>
arXiv:2407.18443 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]</p>
<p>(or<br>
arXiv:2407.18443v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)</p><p></p>
  </details>
</details>
<details>
  <summary>45. <b>【2407.18437】Mixed Non-linear Quantization for Vision Transformers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18437">https://arxiv.org/abs/2407.18437</a></p>
  <p><b>作者</b>：Gihwan Kim,Jemin Lee,Sihyeong Park,Yongin Kwon,Hyungshin Kim</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Vision Transformers, size of Vision, non-linear operations, proposed to reduce, quantization</p>
  <p><b>备注</b>： 16 pages, 4 figures, under review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The majority of quantization methods have been proposed to reduce the model size of Vision Transformers, yet most of them have overlooked the quantization of non-linear operations. Only a few works have addressed quantization for non-linear operations, but they applied a single quantization method across all non-linear operations. We believe that this can be further improved by employing a different quantization method for each non-linear operation. Therefore, to assign the most error-minimizing quantization method from the known methods to each non-linear layer, we propose a mixed non-linear quantization that considers layer-wise quantization sensitivity measured by SQNR difference metric. The results show that our method outperforms I-BERT, FQ-ViT, and I-ViT in both 8-bit and 6-bit settings for ViT, DeiT, and Swin models by an average of 0.6%p and 19.6%p, respectively. Our method outperforms I-BERT and I-ViT by 0.6%p and 20.8%p, respectively, when training time is limited. We plan to release our code at this https URL.</p>
  </details>
</details>
<details>
  <summary>46. <b>【2407.18428】Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18428">https://arxiv.org/abs/2407.18428</a></p>
  <p><b>作者</b>：Gina Wong,Joshua Gleason,Rama Chellappa,Yoav Wald,Anqi Liu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：text, inv, invariant, invariant covariate shift, promising approach</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Learning models whose predictions are invariant under multiple environments is a promising approach for out-of-distribution generalization. Such models are trained to extract features $X_{\text{inv}}$ where the conditional distribution $Y \mid X_{\text{inv}}$ of the label given the extracted features does not change across environments. Invariant models are also supposed to generalize to shifts in the marginal distribution $p(X_{\text{inv}})$ of the extracted features $X_{\text{inv}}$, a type of shift we call an $\textit{invariant covariate shift}$. However, we show that proposed methods for learning invariant models underperform under invariant covariate shift, either failing to learn invariant models$\unicode{x2014}$even for data generated from simple and well-studied linear-Gaussian models$\unicode{x2014}$or having poor finite-sample performance. To alleviate these problems, we propose $\textit{weighted risk invariance}$ (WRI). Our framework is based on imposing invariance of the loss across environments subject to appropriate reweightings of the training examples. We show that WRI provably learns invariant models, i.e. discards spurious correlations, in linear-Gaussian settings. We propose a practical algorithm to implement WRI by learning the density $p(X_{\text{inv}})$ and the model parameters simultaneously, and we demonstrate empirically that WRI outperforms previous invariant learning methods under invariant covariate shift.</p>
  </details>
</details>
<details>
  <summary>47. <b>【2407.18392】A Reference-Based 3D Semantic-Aware Framework for Accurate Local Facial Attribute Editing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18392">https://arxiv.org/abs/2407.18392</a></p>
  <p><b>作者</b>：Yu-Kai Huang,Yutong Zheng,Yen-Shuo Su,Anudeepsekhar Bolimera,Han Zhang,Fangyi Chen,Marios Savvides</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：role in synthesizing, specific characteristics, maintaining realistic appearances, synthesizing realistic faces, crucial role</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Facial attribute editing plays a crucial role in synthesizing realistic faces with specific characteristics while maintaining realistic appearances. Despite advancements, challenges persist in achieving precise, 3D-aware attribute modifications, which are crucial for consistent and accurate representations of faces from different angles. Current methods struggle with semantic entanglement and lack effective guidance for incorporating attributes while maintaining image integrity. To address these issues, we introduce a novel framework that merges the strengths of latent-based and reference-based editing methods. Our approach employs a 3D GAN inversion technique to embed attributes from the reference image into a tri-plane space, ensuring 3D consistency and realistic viewing from multiple perspectives. We utilize blending techniques and predicted semantic masks to locate precise edit regions, merging them with the contextual guidance from the reference image. A coarse-to-fine inpainting strategy is then applied to preserve the integrity of untargeted areas, significantly enhancing realism. Our evaluations demonstrate superior performance across diverse editing tasks, validating our framework's effectiveness in realistic and applicable facial attribute editing.</p>
  </details>
</details>
<details>
  <summary>48. <b>【2407.18391】UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18391">https://arxiv.org/abs/2407.18391</a></p>
  <p><b>作者</b>：Xinyu Pi,Mingyuan Wu,Jize Jiang,Haozhen Zheng,Beitong Tian,Chengxiang Zhai,Klara Nahrstedt,Zhiting Hu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Smaller-scale Vision-Langauge Models, general-domain visual grounding, Smaller-scale Vision-Langauge, Vision-Langauge Models, larger models</p>
  <p><b>备注</b>： 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Smaller-scale Vision-Langauge Models (VLMs) often claim to perform on par with larger models in general-domain visual grounding and question-answering benchmarks while offering advantages in computational efficiency and storage. However, their ability to handle rare objects, which fall into the long tail of data distributions, is less understood. To rigorously evaluate this aspect, we introduce the "Uncontextualized Uncommon Objects" (UOUO) benchmark. This benchmark focuses on systematically testing VLMs with both large and small parameter counts on rare and specialized objects. Our comprehensive analysis reveals that while smaller VLMs maintain competitive performance on common datasets, they significantly underperform on tasks involving uncommon objects. We also propose an advanced, scalable pipeline for data collection and cleaning, ensuring the UOUO benchmark provides high-quality, challenging instances. These findings highlight the need to consider long-tail distributions when assessing the true capabilities of VLMs.</p>
  </details>
</details>
<details>
  <summary>49. <b>【2407.18381】Neural Surface Detection for Unsigned Distance Fields</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18381">https://arxiv.org/abs/2407.18381</a></p>
  <p><b>作者</b>：Federico Stella,Nicolas Talabot,Hieu Le,Pascal Fua</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Signed Distance Fields, Marching Cubes, Distance Fields, Signed Distance, Extracting surfaces</p>
  <p><b>备注</b>： Accepted to ECCV 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Extracting surfaces from Signed Distance Fields (SDFs) can be accomplished using traditional algorithms, such as Marching Cubes. However, since they rely on sign flips across the surface, these algorithms cannot be used directly on Unsigned Distance Fields (UDFs). In this work, we introduce a deep-learning approach to taking a UDF and turning it locally into an SDF, so that it can be effectively triangulated using existing algorithms. We show that it achieves better accuracy in surface detection than existing methods. Furthermore it generalizes well to unseen shapes and datasets, while being parallelizable. We also demonstrate the flexibily of the method by using it in conjunction with DualMeshUDF, a state of the art dual meshing method that can operate on UDFs, improving its results and removing the need to tune its parameters.</p>
  </details>
</details>
<details>
  <summary>50. <b>【2407.18363】KI-Bilder und die Widerst\"andigkeit der Medienkonvergenz: Von prim\"arer zu sekund\"arer Intermedialit\"at?</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18363">https://arxiv.org/abs/2407.18363</a></p>
  <p><b>作者</b>：Lukas R.A. Wilde</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：potential media form, media, media form, media convergence, potential media</p>
  <p><b>备注</b>： in German language</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The article presents some current observations (as of April 10, 2024) on the integration of AI-generated images within processes of media convergence. It draws on two different concepts of intermediality. Primary intermediality concepts are motivated by the object when a new type of technology develops the potential to become socially relevant as a media form and thus a socially, politically, or culturally important communicative factor. Due to their uncertain 'measurements' within the wider media ecology, however, the new, still potential media form appears hybrid. The "inter-" or "between-" of this initial intermediality moment thus refers to the questionable "site" and the questionable description of the potential media form between already existing technologies and cultural forms and their conceptual measurements. For secondary concepts of intermediality, in contrast, it can be assumed that the boundaries of media forms and their application have already been drawn and are reasonably undisputed. This then raises the question of intentional and staged references to AI imagery within other media forms and pictures. The article discusses indicators of both intermediality moments using current examples and controversies surrounding AI images. The thesis is that there can be no talk of a seamless 'integration' of AI images into the wider media landscape at the moment (within films, comic books, or video games, for example) - as one of countless other image production techniques - and that the medial 'site' of AI image circulation - at least where it is not a matter of deception, but rather their conscious use as AI images - especially in social media communication and in fan cultures, but with repercussions for the more general media ecology and image interpretation, insofar as the suspicion that an image could be AI-generated is now increasingly present as a "hermeneutics of suspicion".</p>
  </details>
</details>
<details>
  <summary>51. <b>【2407.18338】SMiCRM: A Benchmark Dataset of Mechanistic Molecular Images</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18338">https://arxiv.org/abs/2407.18338</a></p>
  <p><b>作者</b>：Ching Ting Leung,Yufan Chen,Hanyu Gao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV); Biomolecules (q-bio.BM)</p>
  <p><b>关键词</b>：graph or SMILES, Optical chemical structure, Optical chemical, systems aim, chemical</p>
  <p><b>备注</b>： Under Submission</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Optical chemical structure recognition (OCSR) systems aim to extract the molecular structure information, usually in the form of molecular graph or SMILES, from images of chemical molecules. While many tools have been developed for this purpose, challenges still exist due to different types of noises that might exist in the images. Specifically, we focus on the 'arrow-pushing' diagrams, a typical type of chemical images to demonstrate electron flow in mechanistic steps. We present Structural molecular identifier of Molecular images in Chemical Reaction Mechanisms (SMiCRM), a dataset designed to benchmark machine recognition capabilities of chemical molecules with arrow-pushing annotations. Comprising 453 images, it spans a broad array of organic chemical reactions, each illustrated with molecular structures and mechanistic arrows. SMiCRM offers a rich collection of annotated molecule images for enhancing the benchmarking process for OCSR methods. This dataset includes a machine-readable molecular identity for each image as well as mechanistic arrows showing electron flow during chemical reactions. It presents a more authentic and challenging task for testing molecular recognition technologies, and achieving this task can greatly enrich the mechanisitic information in computer-extracted chemical reaction data.</p>
  </details>
</details>
<details>
  <summary>52. <b>【2407.18290】Several questions of visual generation in 2024</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18290">https://arxiv.org/abs/2407.18290</a></p>
  <p><b>作者</b>：Shuyang Gu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：author personal understanding, visual generation based, Visual Signal Decomposition, personal understanding, signal decomposition</p>
  <p><b>备注</b>： 12 pages, 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper does not propose any new algorithms but instead outlines various problems in the field of visual generation based on the author's personal understanding. The core of these problems lies in how to decompose visual signals, with all other issues being closely related to this central problem and stemming from unsuitable approaches to signal decomposition. This paper aims to draw researchers' attention to the significance of Visual Signal Decomposition.</p>
  </details>
</details>
<details>
  <summary>53. <b>【2407.18289】MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18289">https://arxiv.org/abs/2407.18289</a></p>
  <p><b>作者</b>：Zsófia Katona,Seyed Sahand Mohammadi Ziabari,Fatemeh Karimi Najadasl</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：prey play, play an essential, essential role, rarity makes, makes them difficult</p>
  <p><b>备注</b>： This is an MSc thesis by Zsofia Katona, supervised by the two other authors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Encounters between predator and prey play an essential role in ecosystems, but their rarity makes them difficult to detect in video recordings. Although advances in action recognition (AR) and temporal action detection (AD), especially transformer-based models and vision foundation models, have achieved high performance on human action datasets, animal videos remain relatively under-researched. This thesis addresses this gap by proposing the model MARINE, which utilizes motion-based frame selection designed for fast animal actions and DINOv2 feature extraction with a trainable classification head for action recognition. MARINE outperforms VideoMAE in identifying predator attacks in videos of fish, both on a small and specific coral reef dataset (81.53\% against 52.64\% accuracy), and on a subset of the more extensive Animal Kingdom dataset (94.86\% against 83.14\% accuracy). In a multi-label setting on a representative sample of Animal Kingdom, MARINE achieves 23.79\% mAP, positioning it mid-field among existing benchmarks. Furthermore, in an AD task on the coral reef dataset, MARINE achieves 80.78\% AP (against VideoMAE's 34.89\%) although at a lowered t-IoU threshold of 25\%. Therefore, despite room for improvement, MARINE offers an effective starter framework to apply to AR and AD tasks on animal recordings and thus contribute to the study of natural ecosystems.</p>
  </details>
</details>
<details>
  <summary>54. <b>【2407.18288】Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18288">https://arxiv.org/abs/2407.18288</a></p>
  <p><b>作者</b>：Niels G. Faber,Seyed Sahand Mohammadi Ziabari,Fatemeh Karimi Najadasl</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Multiple Object Tracking, computer vision task, Multiple Object, Object Tracking, variety of sectors</p>
  <p><b>备注</b>： This is an MSc thesis by Niels Faber, supervised by the two other authors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements in certain scenarios, it does not consistently outperform the original FairMOT model. These findings highlight the potential and limitations of applying foundation models in knowledge</p>
  </details>
</details>
<details>
  <summary>55. <b>【2407.18555】How To Segment in 3D Using 2D Models: Automated 3D Segmentation of Prostate Cancer Metastatic Lesions on PET Volumes Using Multi-Angle Maximum Intensity Projections and Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18555">https://arxiv.org/abs/2407.18555</a></p>
  <p><b>作者</b>：Amirhosein Toosi,Sara Harsini,François Bénard,Carlos Uribe,Arman Rahmim</p>
  <p><b>类目</b>：Medical Physics (physics.med-ph); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Prostate specific membrane, positron emission tomography, specific membrane antigen, tremendously exciting frontier, prostate cancer</p>
  <p><b>备注</b>： 11 pages, 2 figures, accepted in the DGM4MICCAI workshop, MICCAI, 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Prostate specific membrane antigen (PSMA) positron emission tomography/computed tomography (PET/CT) imaging provides a tremendously exciting frontier in visualization of prostate cancer (PCa) metastatic lesions. However, accurate segmentation of metastatic lesions is challenging due to low signal-to-noise ratios and variable sizes, shapes, and locations of the lesions. This study proposes a novel approach for automated segmentation of metastatic lesions in PSMA PET/CT 3D volumetric images using 2D denoising diffusion probabilistic models (DDPMs). Instead of 2D trans-axial slices or 3D volumes, the proposed approach segments the lesions on generated multi-angle maximum intensity projections (MA-MIPs) of the PSMA PET images, then obtains the final 3D segmentation masks from 3D ordered subset expectation maximization (OSEM) reconstruction of 2D MA-MIPs segmentations. Our proposed method achieved superior performance compared to state-of-the-art 3D segmentation approaches in terms of accuracy and robustness in detecting and segmenting small metastatic PCa lesions. The proposed method has significant potential as a tool for quantitative analysis of metastatic burden in PCa patients.</p>
  </details>
</details>
<details>
  <summary>56. <b>【2407.18456】Lensless fiber endomicroscopic phase imaging with speckle-conditioned diffusion model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18456">https://arxiv.org/abs/2407.18456</a></p>
  <p><b>作者</b>：Zhaoqing Chen,Jiawei Sun,Xinyi Ye,Bin Zhao,Xuelong Li</p>
  <p><b>类目</b>：Optics (physics.optics); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Lensless fiber endomicroscope, enhance image contrast, Lensless fiber, fiber endomicroscope, phase reconstruction</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Lensless fiber endomicroscope is an emerging tool for in-vivo microscopic imaging, where quantitative phase imaging (QPI) can be utilized as a label-free method to enhance image contrast. However, existing single-shot phase reconstruction methods through lensless fiber endomicroscope typically perform well on simple images but struggle with complex microscopic structures. Here, we propose a speckle-conditioned diffusion model (SpecDiffusion), which reconstructs phase images directly from speckles captured at the detection side of a multi-core fiber (MCF). Unlike conventional neural networks, SpecDiffusion employs iterative phase denoising steps for speckle-driven phase reconstruction. The iteration scheme allows SpecDiffusion to break down the phase reconstruction process into multiple steps, gradually building up to the final phase image. This attribute alleviates the computation challenge at each step and enables the reconstruction of rich details in complex microscopic images. To validate its efficacy, we build an optical system to capture speckles from MCF and construct a dataset consisting of 100,000 paired images. SpecDiffusion provides high-fidelity phase reconstruction results and shows powerful generalization capacity for unseen objects, such as test charts and biological tissues, reducing the average mean absolute error of the reconstructed tissue images by 7 times. Furthermore, the reconstructed tissue images using SpecDiffusion shows higher accuracy in zero-shot cell segmentation tasks compared to the conventional method, demonstrating the potential for further cell morphology analysis through the learning-based lensless fiber endomicroscope. SpecDiffusion offers a precise and generalized method to phase reconstruction through scattering media, including MCFs, opening new perspective in lensless fiber endomicroscopic imaging.</p>
  </details>
</details>
<details>
  <summary>57. <b>【2407.18449】owards A Generalizable Pathology Foundation Model via Unified Knowledge Distillation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18449">https://arxiv.org/abs/2407.18449</a></p>
  <p><b>作者</b>：Jiabo Ma,Zhengrui Guo,Fengtao Zhou,Yihui Wang,Yingxue Xu,Yu Cai,Zhengjie Zhu,Cheng Jin,Yi Lin Xinrui Jiang,Anjia Han,Li Liang,Ronald Cheong Kin Chan,Jiguang Wang,Kwang-Ting Cheng,Hao Chen</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Foundation models, Foundation, pathology foundation models, clinical tasks, tasks</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Foundation models pretrained on large-scale datasets are revolutionizing the field of computational pathology (CPath). The generalization ability of foundation models is crucial for the success in various downstream clinical tasks. However, current foundation models have only been evaluated on a limited type and number of tasks, leaving their generalization ability and overall performance unclear. To address this gap, we established a most comprehensive benchmark to evaluate the performance of off-the-shelf foundation models across six distinct clinical task types, encompassing a total of 39 specific tasks. Our findings reveal that existing foundation models excel at certain task types but struggle to effectively handle the full breadth of clinical tasks. To improve the generalization of pathology foundation models, we propose a unified knowledge distillation framework consisting of both expert and self knowledge distillation, where the former allows the model to learn from the knowledge of multiple expert models, while the latter leverages self-distillation to enable image representation learning via local-global alignment. Based on this framework, a Generalizable Pathology Foundation Model (GPFM) is pretrained on a large-scale dataset consisting of 190 million images from around 86,000 public H\E whole slides across 34 major tissue types. Evaluated on the established benchmark, GPFM achieves an impressive average rank of 1.36, with 29 tasks ranked 1st, while the the second-best model, UNI, attains an average rank of 2.96, with only 4 tasks ranked 1st. The superior generalization of GPFM demonstrates its exceptional modeling capabilities across a wide range of clinical tasks, positioning it as a new cornerstone for feature representation in CPath.</p>
  </details>
</details>
<details>
  <summary>58. <b>【2407.18390】Adapting Mouse Pathological Model to Human Glomerular Lesion Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18390">https://arxiv.org/abs/2407.18390</a></p>
  <p><b>作者</b>：Lining Yu,Mengmeng Yin,Ruining Deng,Quan Liu,Tianyuan Yao,Can Cui,Yu Wang,Yaohong Wang,Shilin Zhao,Haichun Yang,Yuankai Huo</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：preclinical research encompasses, Moving from animal, medical science, preclinical research, research encompasses</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Moving from animal models to human applications in preclinical research encompasses a broad spectrum of disciplines in medical science. A fundamental element in the development of new drugs, treatments, diagnostic methods, and in deepening our understanding of disease processes is the accurate measurement of kidney tissues. Past studies have demonstrated the viability of translating glomeruli segmentation techniques from mouse models to human applications. Yet, these investigations tend to neglect the complexities involved in segmenting pathological glomeruli affected by different lesions. Such lesions present a wider range of morphological variations compared to healthy glomerular tissue, which are arguably more valuable than normal glomeruli in clinical practice. Furthermore, data on lesions from animal models can be more readily scaled up from disease models and whole kidney biopsies. This brings up a question: ``\textit{Can a pathological segmentation model trained on mouse models be effectively applied to human patients?}" To answer this question, we introduced GLAM, a deep learning study for fine-grained segmentation of human kidney lesions using a mouse model, addressing mouse-to-human transfer learning, by evaluating different learning strategies for segmenting human pathological lesions using zero-shot transfer learning and hybrid learning by leveraging mouse samples. From the results, the hybrid learning model achieved superior performance.</p>
  </details>
</details>
<details>
  <summary>59. <b>【2407.18362】Retinal IPA: Iterative KeyPoints Alignment for Multimodal Retinal Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.18362">https://arxiv.org/abs/2407.18362</a></p>
  <p><b>作者</b>：Jiacheng Wang,Hao Li,Dewei Hu,Rui Xu,Xing Yao,Yuankai K. Tao,Ipek Oguz</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：learning cross-modality features, designed for learning, learning cross-modality, enhance matching, matching and registration</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose a novel framework for retinal feature point alignment, designed for learning cross-modality features to enhance matching and registration across multi-modality retinal images. Our model draws on the success of previous learning-based feature detection and description methods. To better leverage unlabeled data and constrain the model to reproduce relevant keypoints, we integrate a keypoint-based segmentation task. It is trained in a self-supervised manner by enforcing segmentation consistency between different augmentations of the same image. By incorporating a keypoint augmented self-supervised layer, we achieve robust feature extraction across modalities. Extensive evaluation on two public datasets and one in-house dataset demonstrates significant improvements in performance for modality-agnostic retinal feature alignment. Our code and model weights are publicly available at \url{this https URL}.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2024/07/30/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2024/07/30/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html"><img class="next-cover" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">🎨 Stable Diffusion 提示词指南书</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">信息检索</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">计算机视觉</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/07/30/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-07-30)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2024-07-30)"/></a><div class="content"><a class="title" href="/2024/07/30/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-07-30)">Arxiv每日速递(2024-07-30)</a><time datetime="2024-07-30T00:58:26.472Z" title="发表于 2024-07-30 08:58:26">2024-07-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书"><img src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="🎨 Stable Diffusion 提示词指南书"/></a><div class="content"><a class="title" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书">🎨 Stable Diffusion 提示词指南书</a><time datetime="2024-02-03T06:57:45.000Z" title="发表于 2024-02-03 14:57:45">2024-02-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer语言模型的位置编码与长度外推"/></a><div class="content"><a class="title" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推">Transformer语言模型的位置编码与长度外推</a><time datetime="2023-10-22T14:55:45.000Z" title="发表于 2023-10-22 22:55:45">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (8)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-10-22</span><a class="blog-slider__title" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">Transformer语言模型的位置编码与长度外推</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt=""><img width="48" height="48" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-02-03</span><a class="blog-slider__title" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">🎨 Stable Diffusion 提示词指南书</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>