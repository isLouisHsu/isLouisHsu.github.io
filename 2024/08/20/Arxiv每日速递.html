<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2024-08-20) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新299篇论文，其中：  自然语言处理45篇 信息检索10篇 计算机视觉75篇  自然语言处理    1. 【2408.08872】xGen-MM (BLIP-3): A Family of Open Large Multimodal Models   链接：https:">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2024-08-20)">
<meta property="og:url" content="http://louishsu.xyz/2024/08/20/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新299篇论文，其中：  自然语言处理45篇 信息检索10篇 计算机视觉75篇  自然语言处理    1. 【2408.08872】xGen-MM (BLIP-3): A Family of Open Large Multimodal Models   链接：https:">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2024-08-20T00:58:32.990Z">
<meta property="article:modified_time" content="2024-08-20T01:00:12.610Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2024/08/20/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-08-20 09:00:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2024-08-20)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-20T00:58:32.990Z" title="发表于 2024-08-20 08:58:32">2024-08-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-20T01:00:12.610Z" title="更新于 2024-08-20 09:00:12">2024-08-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">35.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>211分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/08/20/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。</p>
<h1>统计</h1>
<p>今日共更新<strong>299</strong>篇论文，其中：</p>
<ul>
<li>自然语言处理<strong>45</strong>篇</li>
<li>信息检索<strong>10</strong>篇</li>
<li>计算机视觉<strong>75</strong>篇</li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>【2408.08872】xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08872">https://arxiv.org/abs/2408.08872</a></p>
  <p><b>作者</b>：Le Xue,Manli Shu,Anas Awadalla,Jun Wang,An Yan,Senthil Purushwalkam,Honglu Zhou,Viraj Prabhu,Yutong Dai,Michael S Ryoo,Shrikant Kendre,Jieyu Zhang,Can Qin,Shu Zhang,Chia-Chih Chen,Ning Yu,Juntao Tan,Tulika Manoj Awalgaonkar,Shelby Heinecke,Huan Wang,Yejin Choi,Ludwig Schmidt,Zeyuan Chen,Silvio Savarese,Juan Carlos Niebles,Caiming Xiong,Ran Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：developing Large Multimodal, Large Multimodal Models, Large Multimodal, developing Large, Multimodal Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, we introduce a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. We open-source our models, curated large-scale datasets, and our fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on our project page above.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2408.08869】PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08869">https://arxiv.org/abs/2408.08869</a></p>
  <p><b>作者</b>：Sumanth Prabhu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, demonstrated remarkable gains, diverse reasoning paths</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Self-ensembling techniques with diverse reasoning paths such as Self-Consistency have demonstrated remarkable gains in accuracy for Large Language Models (LLMs). However, such techniques depend on the availability of an accurate answer extraction process to aggregate across multiple outputs. Moreover, they acquire higher inference cost, in comparison to Greedy Decoding, due to generation of relatively higher number of output tokens. Research has shown that the free form text outputs from Self-Consistency can be aggregated reliably using LLMs to produce the final output. Additionally, recent advancements in LLM inference have demonstrated that usage of diverse exemplars in prompts have the ability to induce diversity in the LLM outputs. Such proven techniques can be easily extended to self-ensembling based approaches to achieve enhanced results in text generation. In this paper, we introduce PEDAL (Prompts based on Exemplar Diversity Aggregated using LLMs), a hybrid self-ensembling approach, that combines the strengths of diverse exemplar based prompts and LLM based aggregation to achieve improvement in overall performance. On the publicly available SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve better accuracy than Greedy Decoding based strategies with lower inference cost compared to Self Consistency based approaches.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2408.08848】PsychoLex: Unveiling the Psychological Mind of Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08848">https://arxiv.org/abs/2408.08848</a></p>
  <p><b>作者</b>：Mohammad Amin Abbasi,Farnaz Sadat Mirnezami,Hassan Naderi</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, specialized Large Language, Large Language, Language Models, paper explores</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper explores the intersection of psychology and artificial intelligence through the development and evaluation of specialized Large Language Models (LLMs). We introduce PsychoLex, a suite of resources designed to enhance LLMs' proficiency in psychological tasks in both Persian and English. Key contributions include the PsychoLexQA dataset for instructional content and the PsychoLexEval dataset for rigorous evaluation of LLMs in complex psychological scenarios. Additionally, we present the PsychoLexLLaMA model, optimized specifically for psychological applications, demonstrating superior performance compared to general-purpose models. The findings underscore the potential of tailored LLMs for advancing psychological research and applications, while also highlighting areas for further refinement. This research offers a foundational step towards integrating LLMs into specialized psychological domains, with implications for future advancements in AI-driven psychological practice.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2408.08841】FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08841">https://arxiv.org/abs/2408.08841</a></p>
  <p><b>作者</b>：Xuanliang Zhang,Dingzirui Wang,Longxu Dou,Baoxin Wang,Dayong Wu,Qingfu Zhu,Wanxiang Che</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：reasoning task aims, Large Language Models, table reasoning task, table reasoning, task aims</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The table reasoning task aims to answer the question according to the given table. Currently, using Large Language Models (LLMs) is the predominant method for table reasoning. Most existing methods employ a fixed tabular format to represent the table, which could limit the performance. Given that each instance requires different capabilities and models possess varying abilities, we assert that different instances and models suit different tabular formats. We prove the aforementioned claim through quantitative analysis of experimental results, where different instances and models achieve different performances using various tabular formats. Building on this discussion, we propose FLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by employing flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a classifier to predict the most suitable tabular format based on the instance and the LLM. (ii) FLEXTAF-Vote integrates the results across different formats. Our experiments on WikiTableQuestions and TabFact reveal significant improvements, with average gains of 2.3% and 4.8% compared to the best performance achieved using a fixed tabular format with greedy decoding and self-consistency decoding, thereby validating the effectiveness of our methods.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2408.08805】CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational Dialogue Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08805">https://arxiv.org/abs/2408.08805</a></p>
  <p><b>作者</b>：Joanito Agili Lopo,Marina Indah Prasasti,Alma Permatasari</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：dialogue systems powered, Gemma Language model, language model size, Language model, smaller language model</p>
  <p><b>备注</b>： This paper is the result of the final project of the Natural Language Processing course, Master of Artificial Intelligence, Universitas Gadjah Mada</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this study, we introduce CIKMar, an efficient approach to educational dialogue systems powered by the Gemma Language model. By leveraging a Dual-Encoder ranking system that incorporates both BERT and SBERT model, we have designed CIKMar to deliver highly relevant and accurate responses, even with the constraints of a smaller language model size. Our evaluation reveals that CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore metrics. However, we have identified a significant challenge: the Dual-Encoder tends to prioritize theoretical responses over practical ones. These findings underscore the potential of compact and efficient models like Gemma in democratizing access to advanced educational AI systems, ensuring effective and contextually appropriate responses.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2408.08803】Leveraging FourierKAN Classification Head for Pre-Trained Transformer-based Text Classification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08803">https://arxiv.org/abs/2408.08803</a></p>
  <p><b>作者</b>：Abdullah Al Imran,Md Farhan Ishmam</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Multi-layer Perceptron, text classification tasks, Perceptron, transformer-based pre-trained models, Multi-layer</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:For many years, transformer-based pre-trained models with Multi-layer Perceptron (MLP) heads have been the standard for text classification tasks. However, the fixed non-linear functions employed by MLPs often fall short of capturing the intricacies of the contextualized embeddings produced by pre-trained encoders. Furthermore, MLPs usually require a significant number of training parameters, which can be computationally expensive. In this work, we introduce FourierKAN (FR-KAN), a variant of the promising MLP alternative called Kolmogorov-Arnold Networks (KANs), as classification heads for transformer-based encoders. Our studies reveal an average increase of 10% in accuracy and 11% in F1-score when incorporating FR-KAN heads instead of traditional MLP heads for several transformer-based pre-trained models across multiple text classification tasks. Beyond improving model accuracy, FR-KAN heads train faster and require fewer parameters. Our research opens new grounds for broader applications of KAN across several Natural Language Processing (NLP) tasks.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2408.08782】EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08782">https://arxiv.org/abs/2408.08782</a></p>
  <p><b>作者</b>：Chenwei Wan,Matthieu Labeau,Chloé Clavel</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Designing emotionally intelligent, emotionally intelligent conversational, people experiencing distress, Designing emotionally, intelligent conversational systems</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Previous efforts have focused on developing modular dialogue systems that treat socio-emotional strategy prediction as an auxiliary task and generate strategy-conditioned responses with customized decoders. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit socio-emotional strategy prediction steps have become prevalent. However, despite their excellence in language generation, recent studies show that LLMs' inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy predictor, EmoDynamiX, which models the discourse dynamics between user emotions and system strategies using a heterogeneous graph. Additionally, we make use of the Emotion Recognition in Conversations (ERC) task and design a flexible mixed-emotion module to capture fine-grained emotional states of the user. Experimental results on two ESC datasets show EmoDynamiX outperforms previous state-of-the-art methods with a significant margin.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2408.08781】Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08781">https://arxiv.org/abs/2408.08781</a></p>
  <p><b>作者</b>：Bhuvanashree Murugadoss,Christian Poelitz,Ian Drosos,Vu Le,Nick McKenna,Carina Suzana Negreanu,Chris Parnin,Advait Sarkar</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：recently popularized method, replaces human judgements, Reinforcement Learning, recently popularized, human judgements</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:LLMs-as-a-judge is a recently popularized method which replaces human judgements in task evaluation (Zheng et al. 2024) with automatic evaluation using LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human Feedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have strong alignment with human preferences when prompted for a quality judgement, such as the coherence of a text. While this seems beneficial, it is not clear whether the assessments by an LLM-as-a-judge constitute only an evaluation based on the instructions in the prompts, or reflect its preference for high-quality data similar to its fine-tune data. To investigate how much influence prompting the LLMs-as-a-judge has on the alignment of AI judgements to human judgements, we analyze prompts with increasing levels of instructions about the target quality of an evaluation, for several LLMs-as-a-judge. Further, we compare to a prompt-free method using model perplexity as a quality measure instead. We aggregate a taxonomy of quality criteria commonly used across state-of-the-art evaluations with LLMs and provide this as a rigorous benchmark of models as judges. Overall, we show that the LLMs-as-a-judge benefit only little from highly detailed instructions in prompts and that perplexity can sometimes align better with human judgements than prompting, especially on textual quality.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2408.08780】Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08780">https://arxiv.org/abs/2408.08780</a></p>
  <p><b>作者</b>：Chenming Tang,Zhixiang Wang,Yunfang Wu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, achieved impressive performance, large language, language models, achieved impressive</p>
  <p><b>备注</b>： 10 pages, 6 figures, 3 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the help of in-context learning (ICL), large language models (LLMs) have achieved impressive performance across various tasks. However, the function of descriptive instructions during ICL remains under-explored. In this work, we propose an ensemble prompt framework to describe the selection criteria of multiple in-context examples, and preliminary experiments on machine translation (MT) across six translation directions confirm that this framework boosts ICL perfromance. But to our surprise, LLMs might not necessarily care what the descriptions actually say, and the performance gain is primarily caused by the ensemble format, since the framework could lead to improvement even with random descriptive nouns. We further apply this new ensemble prompt on a range of commonsense, math, logical reasoning and hallucination tasks with three LLMs and achieve promising results, suggesting again that designing a proper prompt format would be much more effective and efficient than paying effort into specific descriptions. Our code will be publicly available once this paper is published.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2408.08779】DAC: Decomposed Automation Correction for Text-to-SQL</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08779">https://arxiv.org/abs/2408.08779</a></p>
  <p><b>作者</b>：Dingzirui Wang,Longxu Dou,Xuanliang Zhang,Qingfu Zhu,Wanxiang Che</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：people obtain information, Large Language Models, generating SQL queries, automatically generating SQL, important task</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text-to-SQL is an important task that helps people obtain information from databases by automatically generating SQL queries. Considering the brilliant performance, approaches based on Large Language Models (LLMs) become the mainstream for text-to-SQL. Among these approaches, automated correction is an effective approach that further enhances performance by correcting the mistakes in the generated results. The existing correction methods require LLMs to directly correct with generated SQL, while previous research shows that LLMs do not know how to detect mistakes, leading to poor performance. Therefore, in this paper, we propose to employ the decomposed correction to enhance text-to-SQL performance. We first demonstrate that decomposed correction outperforms direct correction since detecting and fixing mistakes with the results of the decomposed sub-tasks is easier than with SQL. Based on this analysis, we introduce Decomposed Automation Correction (DAC), which corrects SQL by decomposing text-to-SQL into entity linking and skeleton parsing. DAC first generates the entity and skeleton corresponding to the question and then compares the differences between the initial SQL and the generated entities and skeleton as feedback for correction. Experimental results show that our method improves performance by $3.7\%$ on average of Spider, Bird, and KaggleDBQA compared with the baseline method, demonstrating the effectiveness of DAC.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2408.08769】Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08769">https://arxiv.org/abs/2408.08769</a></p>
  <p><b>作者</b>：Dingwei Chen,Feiteng Fang,Shiwen Ni,Feng Liang,Ruifeng Xu,Min Yang,Chengming Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, language processing tasks, natural language processing, Large Language, demonstrated exceptional performance</p>
  <p><b>备注</b>： 9 pages, 4 figures, 5 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have demonstrated exceptional performance across various natural language processing tasks, yet they occasionally tend to yield content that factually inaccurate or discordant with the expected output, a phenomenon empirically referred to as "hallucination". To tackle this issue, recent works have investigated contrastive decoding between the original model and an amateur model with induced hallucination, which has shown promising results. Nonetheless, this method may undermine the output distribution of the original LLM caused by its coarse contrast and simplistic subtraction operation, potentially leading to errors in certain cases. In this paper, we introduce a novel contrastive decoding framework termed LOL (LOwer Layer Matters). Our approach involves concatenating the contrastive decoding of both the final and lower layers between the original model and the amateur model, thereby achieving multi-layer fusion to aid in the mitigation of hallucination. Additionally, we incorporate a truthfulness refocused module that leverages contextual guidance to enhance factual encoding, further capturing truthfulness during contrastive decoding. Extensive experiments conducted on two publicly available datasets illustrate that our proposed LOL framework can substantially alleviate hallucination while surpassing existing baselines in most cases. Compared with the best baseline, we improve by average 4.5 points on all metrics of TruthfulQA. The source code is coming soon.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2408.08724】ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target Language</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08724">https://arxiv.org/abs/2408.08724</a></p>
  <p><b>作者</b>：Yongkang Liu,Feng Shi,Daling Wang,Yifei Zhang,Hinrich Schütze</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：show amazing capabilities, LLMs fall short, exciting applications discovered, show amazing, amazing capabilities</p>
  <p><b>备注</b>： ECAI2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Although large language models(LLMs) show amazing capabilities, among various exciting applications discovered for LLMs fall short in other low-resource languages. Besides, most existing methods depend on large-scale dialogue corpora and thus building systems for dialogue generation in a zero-shot scenario remains a considerable challenge. To address this challenge, we propose a novel end-to-end zero-shot dialogue generation model ChatZero based on cross-lingual code-switching method. First, we construct code-switching language and pseudo-target language with placeholders. Then for cross-lingual semantic transfer, we employ unsupervised contrastive learning to minimize the semantics gap of the source language, code-switching language, and pseudo-target language that are mutually positive examples in the high dimensional semantic space. Experiments on the multilingual DailyDialog and DSTC7-AVSD datasets demonstrate that ChatZero can achieve more than 90\% of the original performance under the zero-shot case compared to supervised learning, and achieve state-of-the-art performance compared with other baselines.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2408.08696】urning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08696">https://arxiv.org/abs/2408.08696</a></p>
  <p><b>作者</b>：Xianzhen Luo,Yixuan Wang,Qingfu Zhu,Zhiming Zhang,Xuanyu Zhang,Qing Yang,Dongliang Xu,Wanxiang Che</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：limiting broader application, made inference latency, fundamental bottleneck, limiting broader, rapid growth</p>
  <p><b>备注</b>： under review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rapid growth in the parameters of large language models (LLMs) has made inference latency a fundamental bottleneck, limiting broader application of LLMs. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm, leveraging the parallel capabilities of modern hardware. Some speculative decoding methods rely on additional structures to guess draft tokens, such as small models or parameter-efficient architectures, which need extra training before use. Alternatively, retrieval-based train-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. This approach stores candidate tokens in an adjacency matrix and employs a breadth-first search (BFS)-like algorithm on the matrix to construct a draft tree. The tree is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a training method by 25\%. It can be directly applied to any existing LLMs and tasks without the need for adaptation.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2408.08694】Quantifying the Effectiveness of Student Organization Activities using Natural Language Processing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08694">https://arxiv.org/abs/2408.08694</a></p>
  <p><b>作者</b>：Lyberius Ennio F. Taruc,Arvin R. De La Cruz</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET)</p>
  <p><b>关键词</b>：extracurricular activities play, students' educational experiences, Natural Language Processing, Student extracurricular activities, Large Language Model</p>
  <p><b>备注</b>： 11 pages, 4 figures, presented in International Conference on Generative Al and its Applications (ICGAIA-24) last 22nd - 23rd, July, 2024 at Jakarta, Indonesia</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Student extracurricular activities play an important role in enriching the students' educational experiences. With the increasing popularity of Machine Learning and Natural Language Processing, it becomes a logical step that incorporating ML-NLP in improving extracurricular activities is a potential focus of study in Artificial Intelligence (AI). This research study aims to develop a machine learning workflow that will quantify the effectiveness of student-organized activities based on student emotional responses using sentiment analysis. The study uses the Bidirectional Encoder Representations from Transformers (BERT) Large Language Model (LLM) called via the pysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data set from Organization C, a Recognized Student Organization (RSO) of a higher educational institute in the Philippines, College X, was used to develop the workflow. The workflow consisted of data preprocessing, key feature selection, LLM feature processing, and score aggregation, resulting in an Event Score for each data set. The results show that the BERT LLM can also be used effectively in analyzing sentiment beyond product reviews and post comments. For the student affairs offices of educational institutions, this study can provide a practical example of how NLP can be applied to real-world scenarios, showcasing the potential impact of data-driven decision making.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2408.08693】Med-PMC: Medical Personalized Multi-modal Consultation with a Proactive Ask-First-Observe-Next Paradigm</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08693">https://arxiv.org/abs/2408.08693</a></p>
  <p><b>作者</b>：Hongcheng Liu,Yusheng Liao,Siqv Ou,Yuhao Wang,Heyang Liu,Yanfeng Wang,Yu Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Multi-modal Large Language, Language Models, Large Language, Multi-modal Large</p>
  <p><b>备注</b>： 26 pages, 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The application of the Multi-modal Large Language Models (MLLMs) in medical clinical scenarios remains underexplored. Previous benchmarks only focus on the capacity of the MLLMs in medical visual question-answering (VQA) or report generation and fail to assess the performance of the MLLMs on complex clinical multi-modal tasks. In this paper, we propose a novel Medical Personalized Multi-modal Consultation (Med-PMC) paradigm to evaluate the clinical capacity of the MLLMs. Med-PMC builds a simulated clinical environment where the MLLMs are required to interact with a patient simulator to complete the multi-modal information-gathering and decision-making task. Specifically, the patient simulator is decorated with personalized actors to simulate diverse patients in real scenarios. We conduct extensive experiments to access 12 types of MLLMs, providing a comprehensive view of the MLLMs' clinical performance. We found that current MLLMs fail to gather multimodal information and show potential bias in the decision-making task when consulted with the personalized patient simulators. Further analysis demonstrates the effectiveness of Med-PMC, showing the potential to guide the development of robust and reliable clinical MLLMs. Code and data are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2408.08688】he Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08688">https://arxiv.org/abs/2408.08688</a></p>
  <p><b>作者</b>：Samee Arif,Sualeha Farid,Abdul Hameed Azeemi,Awais Athar,Agha Ali Raza</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：synthetic Preference Optimization, Preference Optimization, synthetic Preference, LLM Feedback Loop, response evaluation module</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper presents and evaluates multi-agent workflows for synthetic Preference Optimization (PO) dataset generation. PO dataset generation requires two modules: (1) response evaluation, and (2) response generation. In the response evaluation module, the responses from Large Language Models (LLMs) are evaluated and ranked - a task typically carried out by human annotators that we automate using LLMs. We assess the response evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators using three distinct prompting strategies. In step 2, we apply the winning prompting strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we use inter-rater agreement using Cohen's Kappa between human annotators and LLMs. For the response generation module, we compare different configurations for the LLM Feedback Loop using the identified LLM evaluator configuration. We use the win rate (the fraction of times a generation framework is selected as the best by an LLM evaluator) to determine the best multi-agent configuration for generation. After identifying the best configurations for both modules, we use models from the GPT, Gemma, and Llama families to generate our PO datasets using the above pipeline. We generate two types of PO datasets, one to improve the generation capabilities of individual LLM and the other to improve the multi-agent workflow. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across datasets when the candidate responses do not include responses from the GPT family. Additionally, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2408.08682】LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08682">https://arxiv.org/abs/2408.08682</a></p>
  <p><b>作者</b>：Yuqi Ye,Wei Gao</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：point cloud, point cloud geometry, point cloud compression, robust context model, context model consistent</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The key to effective point cloud compression is to obtain a robust context model consistent with complex 3D data structures. Recently, the advancement of large language models (LLMs) has highlighted their capabilities not only as powerful generators for in-context learning and generation but also as effective compressors. These dual attributes of LLMs make them particularly well-suited to meet the demands of data compression. Therefore, this paper explores the potential of using LLM for compression tasks, focusing on lossless point cloud geometry compression (PCGC) experiments. However, applying LLM directly to PCGC tasks presents some significant challenges, i.e., LLM does not understand the structure of the point cloud well, and it is a difficult task to fill the gap between text and point cloud through text description, especially for large complicated and small shapeless point clouds. To address these problems, we introduce a novel architecture, namely the Large Language Model-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to compress point cloud geometry information without any text description or aligning operation. By utilizing different adaptation techniques for cross-modality representation alignment and semantic consistency, including clustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA), the proposed method can translate LLM to a compressor/generator for point cloud. To the best of our knowledge, this is the first structure to employ LLM as a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC outperforms the other existing methods significantly, by achieving -40.213% bit rate reduction compared to the reference software of MPEG Geometry-based Point Cloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction compared to the state-of-the-art learning-based method.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2408.08661】MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08661">https://arxiv.org/abs/2408.08661</a></p>
  <p><b>作者</b>：Wenjie Fu,Huandong Wang,Chen Gao,Guanghua Liu,Yong Li,Tao Jiang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：large language models, language models, highlight the urgent, increasing parameters, parameters and expansive</p>
  <p><b>备注</b>： code and dataset: [this https URL](https://github.com/wjfu99/MIA-Tuner) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The increasing parameters and expansive dataset of large language models (LLMs) highlight the urgent demand for a technical solution to audit the underlying privacy risks and copyright issues associated with LLMs. Existing studies have partially addressed this need through an exploration of the pre-training data detection problem, which is an instance of a membership inference attack (MIA). This problem involves determining whether a given piece of text has been used during the pre-training phase of the target LLM. Although existing methods have designed various sophisticated MIA score functions to achieve considerable detection performance in pre-trained LLMs, how to achieve high-confidence detection and how to perform MIA on aligned LLMs remain challenging. In this paper, we propose MIA-Tuner, a novel instruction-based MIA method, which instructs LLMs themselves to serve as a more precise pre-training data detector internally, rather than design an external MIA score function. Furthermore, we design two instruction-based safeguards to respectively mitigate the privacy risks brought by the existing methods and MIA-Tuner. To comprehensively evaluate the most recent state-of-the-art LLMs, we collect a more up-to-date MIA benchmark dataset, named WIKIMIA-24, to replace the widely adopted benchmark WIKIMIA. We conduct extensive experiments across various aligned and unaligned LLMs over the two benchmark datasets. The results demonstrate that MIA-Tuner increases the AUC of MIAs from 0.7 to a significantly high level of 0.9.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2408.08656】LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08656">https://arxiv.org/abs/2408.08656</a></p>
  <p><b>作者</b>：Do Xuan Long,Hai Nguyen Ngoc,Tiviatis Sim,Hieu Dao,Shafiq Joty,Kenji Kawaguchi,Nancy F. Chen,Min-Yen Kan</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, systematic evaluation examining, format bias, language models, format</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present the first systematic evaluation examining format bias in performance of large language models (LLMs). Our approach distinguishes between two categories of an evaluation metric under format constraints to reliably and accurately assess performance: one measures performance when format constraints are adhered to, while the other evaluates performance regardless of constraint adherence. We then define a metric for measuring the format bias of LLMs and establish effective strategies to reduce it. Subsequently, we present our empirical format bias evaluation spanning four commonly used categories -- multiple-choice question-answer, wrapping, list, and mapping -- covering 15 widely-used formats. Our evaluation on eight generation tasks uncovers significant format bias across state-of-the-art LLMs. We further discover that improving the format-instruction following capabilities of LLMs across formats potentially reduces format bias. Based on our evaluation findings, we study prompting and fine-tuning with synthesized format data techniques to mitigate format bias. Our methods successfully reduce the variance in ChatGPT's performance among wrapping formats from 235.33 to 0.71 (%$^2$).</p>
  </details>
</details>
<details>
  <summary>20. <b>【2408.08651】Reasoning Beyond Bias: A Study on Counterfactual Prompting and Chain of Thought Reasoning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08651">https://arxiv.org/abs/2408.08651</a></p>
  <p><b>作者</b>：Kyle Moore,Jesse Roberts,Thao Pham,Douglas Fisher</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Counterfactual Prompting, Multi-Task Language Understanding, Massive Multi-Task Language, training data, leading to predictions</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings reveal that differences in learned regularities across answer options are predictive of model preferences and mirror human test-taking strategies. To address this issue, we introduce two novel methods: Counterfactual Prompting with Chain of Thought (CoT) and Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, our novel Primed Counterfactual Prompting with CoT approach effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a "System-2" like process and that CoT reasoning is susceptible to confirmation bias under some prompting methodologies. Our contributions offer practical solutions for developing more robust and fair language models.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2408.08650】An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08650">https://arxiv.org/abs/2408.08650</a></p>
  <p><b>作者</b>：Peiming Guo,Sinuo Liu,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Meishan Zhang,Min Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Multi-modal dialogue generation, Photo-Sharing Multi-modal dialogue, generate text responses, model, Multi-modal dialogue</p>
  <p><b>备注</b>： Work in progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Photo-Sharing Multi-modal dialogue generation requires a dialogue agent not only to generate text responses but also to share photos at the proper moment. Using image text caption as the bridge, a pipeline model integrates an image caption model, a text generation model, and an image generation model to handle this complex multi-modal task. However, representing the images with text captions may loss important visual details and information and cause error propagation in the complex dialogue system. Besides, the pipeline model isolates the three models separately because discrete image text captions hinder end-to-end gradient propagation. We propose the first end-to-end model for photo-sharing multi-modal dialogue generation, which integrates an image perceptron and an image generator with a large language model. The large language model employs the Q-Former to perceive visual images in the input end. For image generation in the output end, we propose a dynamic vocabulary transformation matrix and use straight-through and gumbel-softmax techniques to align the large language model and stable diffusion model and achieve end-to-end gradient propagation. We perform experiments on PhotoChat and DialogCC datasets to evaluate our end-to-end model. Compared with pipeline models, the end-to-end model gains state-of-the-art performances on various metrics of text and image generation. More analysis experiments also verify the effectiveness of the end-to-end model for photo-sharing multi-modal dialogue generation.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2408.08648】Understanding Enthymemes in Argument Maps: Bridging Argument Mining and Logic-based Argumentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08648">https://arxiv.org/abs/2408.08648</a></p>
  <p><b>作者</b>：Jonathan Ben-Naim,Victor David,Anthony Hunter</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：argument map, processing technology aimed, Argument, arguments, language processing technology</p>
  <p><b>备注</b>： Research note</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Argument mining is natural language processing technology aimed at identifying arguments in text. Furthermore, the approach is being developed to identify the premises and claims of those arguments, and to identify the relationships between arguments including support and attack relationships. In this paper, we assume that an argument map contains the premises and claims of arguments, and support and attack relationships between them, that have been identified by argument mining. So from a piece of text, we assume an argument map is obtained automatically by natural language processing. However, to understand and to automatically analyse that argument map, it would be desirable to instantiate that argument map with logical arguments. Once we have the logical representation of the arguments in an argument map, we can use automated reasoning to analyze the argumentation (e.g. check consistency of premises, check validity of claims, and check the labelling on each arc corresponds with thw logical arguments). We address this need by using classical logic for representing the explicit information in the text, and using default logic for representing the implicit information in the text. In order to investigate our proposal, we consider some specific options for instantiation.</p>
  </details>
</details>
<details>
  <summary>23. <b>【2408.08640】Math-PUMA: Progressive Upward Multimodal Alignment to Enhance Mathematical Reasoning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08640">https://arxiv.org/abs/2408.08640</a></p>
  <p><b>作者</b>：Wenwen Zhuang,Xin Huang,Xiantao Zhang,Jin Zeng</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Multimodal Large Language, Large Language, natural scene images, Multimodal Large</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal Large Language Models (MLLMs) excel in solving text-based mathematical problems, but they struggle with mathematical diagrams since they are primarily trained on natural scene images. For humans, visual aids generally enhance problem-solving, but MLLMs perform worse as information shifts from textual to visual modality. This decline is mainly due to their shortcomings in aligning images and text. To tackle aforementioned challenges, we propose Math-PUMA, a methodology focused on Progressive Upward Multimodal Alignment. This approach is designed to improve the mathematical reasoning skills of MLLMs through a three-stage training process, with the second stage being the critical alignment stage. We first enhance the language model's mathematical reasoning capabilities with extensive set of textual mathematical problems. We then construct a multimodal dataset with varying degrees of textual and visual information, creating data pairs by presenting each problem in at least two forms. By leveraging the Kullback-Leibler (KL) divergence of next-token prediction distributions to align visual and textual modalities, consistent problem-solving abilities are ensured. Finally, we utilize multimodal instruction tuning for MLLMs with high-quality multimodal data. Experimental results on multiple mathematical reasoning benchmarks demonstrate that the MLLMs trained with Math-PUMA surpass most open-source MLLMs. Our approach effectively narrows the performance gap for problems presented in different modalities.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2408.08632】A Survey on Benchmarks of Multimodal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08632">https://arxiv.org/abs/2408.08632</a></p>
  <p><b>作者</b>：Jian Li,Weiheng Lu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Multimodal Large Language, Large Language Models, visual question answering, Multimodal Large, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of \textbf{180 benchmarks} and evaluation for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to better support the development of MLLMs. For more details, please visit our GitHub repository: this https URL.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2408.08631】Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08631">https://arxiv.org/abs/2408.08631</a></p>
  <p><b>作者</b>：Junseok Kim,Nakyeong Yang,Kyomin Jung</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Recent studies demonstrate, Recent studies, LLM, LLM evaluator, Recent</p>
  <p><b>备注</b>： 13 pages, 4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent studies demonstrate that prompting an appropriate role-playing persona to an LLM improves its reasoning capability. However, assigning a proper persona is difficult since an LLM's performance is extremely sensitive to assigned prompts; therefore, personas sometimes hinder LLMs and degrade their reasoning capabilities. In this paper, we propose a novel framework, Jekyll \ Hyde, which ensembles the results of role-playing and neutral prompts to eradicate performance degradation via unilateral use of role-playing prompted LLM and enhance the robustness of an LLM's reasoning ability. Specifically, Jekyll \ Hyde collects two potential solutions from both role-playing and neutral prompts and selects a better solution after cross-checking via an LLM evaluator. However, LLM-based evaluators tend to be affected by the order of those potential solutions within the prompt when selecting the proper solution; thus, we also propose a robust LLM evaluator to mitigate the position bias. The experimental analysis demonstrates that role-playing prompts distract LLMs and degrade their reasoning abilities in 4 out of 12 datasets, even when using GPT-4. In addition, we reveal that Jekyll \ Hyde improves reasoning capabilities by selecting better choices among the potential solutions on twelve widely-used reasoning datasets. We further show that our proposed LLM evaluator outperforms other baselines, proving the LLMs' position bias is successfully mitigated.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2408.08624】RealMedQA: A pilot biomedical question answering dataset containing realistic clinical questions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08624">https://arxiv.org/abs/2408.08624</a></p>
  <p><b>作者</b>：Gregory Kell,Angus Roberts,Serge Umansky,Yuti Khare,Najma Ahmed,Nikhil Patel,Chloe Simela,Jack Coumbe,Julian Rozario,Ryan-Rhys Griffiths,Iain J. Marshall</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Clinical question answering, question answering systems, clinicians with relevant, relevant and timely, answering systems</p>
  <p><b>备注</b>： Accepted at AMIA Annual Symposium 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Clinical question answering systems have the potential to provide clinicians with relevant and timely answers to their questions. Nonetheless, despite the advances that have been made, adoption of these systems in clinical settings has been slow. One issue is a lack of question-answering datasets which reflect the real-world needs of health professionals. In this work, we present RealMedQA, a dataset of realistic clinical questions generated by humans and an LLM. We describe the process for generating and verifying the QA pairs and assess several QA models on BioASQ and RealMedQA to assess the relative difficulty of matching answers to questions. We show that the LLM is more cost-efficient for generating "ideal" QA pairs. Additionally, we achieve a lower lexical similarity between questions and answers than BioASQ which provides an additional challenge to the top two QA models, as per the results. We release our code and our dataset publicly to encourage further research.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2408.08590】A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08590">https://arxiv.org/abs/2408.08590</a></p>
  <p><b>作者</b>：Geonhee Kim,Marco Valentino,André Freitas</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：auto-regressive Language Models, exploit superficial patterns, Recent studies, auto-regressive Language, systematic reasoning principles</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent studies on logical reasoning in auto-regressive Language Models (LMs) have sparked a debate on whether such models can learn systematic reasoning principles during pre-training or merely exploit superficial patterns in the training data. This paper presents a mechanistic interpretation of syllogistic reasoning in LMs to further enhance our understanding of internal dynamics. Specifically, we present a methodology for circuit discovery aimed at disentangling content-independent reasoning mechanisms from world knowledge acquired during pre-training. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic reasoning, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes and model sizes, finding that the identified circuit is sufficient and necessary for all the schemes on which the model achieves high downstream accuracy ($\geq$ 60\%). Overall, our findings suggest that LMs indeed learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalisable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2408.08566】Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of Biomedical Research Articles</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08566">https://arxiv.org/abs/2408.08566</a></p>
  <p><b>作者</b>：Tomas Goldsack,Carolina Scarton,Matthew Shardlow,Chenghua Lin</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Biomedical Research Articles, Workshop at ACL, Lay Summarisation, Summarisation of Biomedical, BioLaySumm shared task</p>
  <p><b>备注</b>： Published in: Proceedings of the 23rd Workshop on Biomedical Natural Language Processing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper presents the setup and results of the second edition of the BioLaySumm shared task on the Lay Summarisation of Biomedical Research Articles, hosted at the BioNLP Workshop at ACL 2024. In this task edition, we aim to build on the first edition's success by further increasing research interest in this important task and encouraging participants to explore novel approaches that will help advance the state-of-the-art. Encouragingly, we found research interest in the task to be high, with this edition of the task attracting a total of 53 participating teams, a significant increase in engagement from the previous edition. Overall, our results show that a broad range of innovative approaches were adopted by task participants, with a predictable shift towards the use of Large Language Models (LLMs).</p>
  </details>
</details>
<details>
  <summary>29. <b>【2408.08564】Collaborative Cross-modal Fusion with Large Language Model for Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08564">https://arxiv.org/abs/2408.08564</a></p>
  <p><b>作者</b>：Zhongzhou Liu,Hao Zhang,Kuicai Dong,Yuan Fang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：conventional collaborative filtering, large language models, leveraging semantic knowledge, success of conventional, exhibit limitations</p>
  <p><b>备注</b>： 10 pages, 4 figures, accepted by CIKM 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite the success of conventional collaborative filtering (CF) approaches for recommendation systems, they exhibit limitations in leveraging semantic knowledge within the textual attributes of users and items. Recent focus on the application of large language models for recommendation (LLM4Rec) has highlighted their capability for effective semantic knowledge capture. However, these methods often overlook the collaborative signals in user behaviors. Some simply instruct-tune a language model, while others directly inject the embeddings of a CF-based model, lacking a synergistic fusion of different modalities. To address these issues, we propose a framework of Collaborative Cross-modal Fusion with Large Language Models, termed CCF-LLM, for recommendation. In this framework, we translate the user-item interactions into a hybrid prompt to encode both semantic knowledge and collaborative signals, and then employ an attentive cross-modal fusion strategy to effectively fuse latent embeddings of both modalities. Extensive experiments demonstrate that CCF-LLM outperforms existing methods by effectively utilizing semantic and collaborative signals in the LLM4Rec context.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2408.08551】Integrating Multi-view Analysis: Multi-view Mixture-of-Expert for Textual Personality Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08551">https://arxiv.org/abs/2408.08551</a></p>
  <p><b>作者</b>：Haohao Zhu,Xiaokun Zhang,Junyu Lu,Liang Yang,Hongfei Lin</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Textual personality detection, identify personality traits, personality detection, Textual personality, personality detection aims</p>
  <p><b>备注</b>： Accepted by NLPCC 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Textual personality detection aims to identify personality traits by analyzing user-generated content. To achieve this effectively, it is essential to thoroughly examine user-generated content from various perspectives. However, previous studies have struggled with automatically extracting and effectively integrating information from multiple perspectives, thereby limiting their performance on personality detection. To address these challenges, we propose the Multi-view Mixture-of-Experts Model for Textual Personality Detection (MvP). MvP introduces a Multi-view Mixture-of-Experts (MoE) network to automatically analyze user posts from various perspectives. Additionally, it employs User Consistency Regularization to mitigate conflicts among different perspectives and learn a multi-view generic user representation. The model's training is optimized via a multi-task joint learning strategy that balances supervised personality detection with self-supervised user consistency constraints. Experimental results on two widely-used personality detection datasets demonstrate the effectiveness of the MvP model and the benefits of automatically analyzing user posts from diverse perspectives for textual personality detection.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2408.08545】SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08545">https://arxiv.org/abs/2408.08545</a></p>
  <p><b>作者</b>：Kaushal Kumar Maurya,KV Aditya Srivatsa,Ekaterina Kochmar</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：gained increased popularity, increased popularity due, Large language models, gained increased, increased popularity</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have gained increased popularity due to their remarkable success across various tasks, which has led to the active development of a large set of diverse LLMs. However, individual LLMs have limitations when applied to complex tasks because of such factors as training biases, model sizes, and the datasets used. A promising approach is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. Towards this goal, we introduce a novel LLM selection algorithm called SelectLLM. This algorithm directs input queries to the most suitable subset of LLMs from a large pool, ensuring they collectively provide the correct response efficiently. SelectLLM uses a multi-label classifier, utilizing the classifier's predictions and confidence scores to design optimal policies for selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings show that the proposed model outperforms individual LLMs and achieves competitive performance compared to similarly sized, computationally expensive top-performing LLM subsets. Specifically, with a similarly sized top-performing LLM subset, we achieve a significant reduction in latency on two standard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower latency for MMLU. Additionally, we conduct comprehensive analyses and ablation studies, which validate the robustness of the proposed model.</p>
  </details>
</details>
<details>
  <summary>32. <b>【2408.08541】Where is the signal in tokenization space?</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08541">https://arxiv.org/abs/2408.08541</a></p>
  <p><b>作者</b>：Renato Lui Geh,Honghua Zhang,Kareem Ahmed,Benjie Wang,Guy Van den Broeck</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, so-called canonical token, canonical token sequences, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) are typically shipped with tokenizers that deterministically encode text into so-called canonical token sequences, to which the LLMs assign probability values. One common assumption is that the probability of a piece of text is the probability of its canonical token sequence. However, the tokenization of a string is not unique: e.g., the Llama2 tokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same text. In this paper, we study non-canonical tokenizations. We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations. We then show how the marginal is, in most cases, indistinguishable from the canonical probability. Surprisingly, we then empirically demonstrate the existence of a significant amount of signal hidden within tokenization space. Notably, by simply aggregating the probabilities of non-canonical tokenizations, we achieve improvements across a range of LLM evaluation benchmarks for a variety of architectures, including transformers and state space models.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2408.08535】CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08535">https://arxiv.org/abs/2408.08535</a></p>
  <p><b>作者</b>：Rong-Ching Chang,Jiawei Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, provide contextually rich, Graph-Retrieval Augmented Generation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems, their effectiveness is often hindered by a lack of integration with entity relationships and community structures, limiting their ability to provide contextually rich and accurate information retrieval for fact-checking. We introduce CommunityKG-RAG (Community Knowledge Graph-Retrieval Augmented Generation), a novel zero-shot framework that integrates community structures within Knowledge Graphs (KGs) with RAG systems to enhance the fact-checking process. Capable of adapting to new domains and queries without additional training, CommunityKG-RAG utilizes the multi-hop nature of community structures within KGs to significantly improve the accuracy and relevance of information retrieval. Our experimental results demonstrate that CommunityKG-RAG outperforms traditional methods, representing a significant advancement in fact-checking by offering a robust, scalable, and efficient solution.</p>
  </details>
</details>
<details>
  <summary>34. <b>【2408.08521】MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08521">https://arxiv.org/abs/2408.08521</a></p>
  <p><b>作者</b>：Zhengyuan Zhu,Daniel Lee,Hong Zhang,Sai Sree Harsha,Loic Feujio,Akash Maharaj,Yunyao Li</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：demonstrated impressive performance, Recent advancements, retrieval-augmented generation, advancements in retrieval-augmented, demonstrated impressive</p>
  <p><b>备注</b>： Preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in retrieval-augmented generation (RAG) have demonstrated impressive performance in the question-answering (QA) task. However, most previous works predominantly focus on text-based answers. While some studies address multimodal data, they still fall short in generating comprehensive multimodal answers, particularly for explaining concepts or providing step-by-step tutorials on how to accomplish specific goals. This capability is especially valuable for applications such as enterprise chatbots and settings such as customer service and educational systems, where the answers are sourced from multimodal data. In this paper, we introduce a simple and effective framework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR enhances text-based answers by retrieving relevant multimodal data and refining the responses to create coherent multimodal answers. This framework can be easily extended to support multimodal answers in enterprise chatbots with minimal modifications. Human evaluation results indicate that multimodal answers generated by MuRAR are more useful and readable compared to plain text answers.</p>
  </details>
</details>
<details>
  <summary>35. <b>【2408.08506】Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08506">https://arxiv.org/abs/2408.08506</a></p>
  <p><b>作者</b>：Huang Lei,Jiaming Guo,Guanhua He,Xishan Zhang,Rui Zhang,Shaohui Peng,Shaoli Liu,Tianshi Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Generating long-term texts, Generating long-term, long-term texts, artificial intelligence, Generating</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generating long-term texts such as novels using artificial intelligence has always been a challenge. A common approach is to use large language models (LLMs) to construct a hierarchical framework that first plans and then writes. Despite the fact that the generated novels reach a sufficient length, they exhibit poor logical coherence and appeal in their plots and deficiencies in character and event depiction, ultimately compromising the overall narrative quality. In this paper, we propose a method named Extracting Excelsior and Expanding. Ex3 initially extracts structure information from raw novel data. By combining this structure information with the novel data, an instruction-following dataset is meticulously crafted. This dataset is then utilized to fine-tune the LLM, aiming for excelsior generation performance. In the final stage, a tree-like expansion method is deployed to facilitate the generation of arbitrarily long novels. Evaluation against previous methods showcases Ex3's ability to produce higher-quality long-form novels.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2408.08459】JPEG-LM: LLMs as Image Generators with Canonical Codec Representations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08459">https://arxiv.org/abs/2408.08459</a></p>
  <p><b>作者</b>：Xiaochuang Han,Marjan Ghazvininejad,Pang Wei Koh,Yulia Tsvetkov</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：potentially easy integration, Recent work, generality and potentially, potentially easy, easy integration</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent work in image and video generation has been adopting the autoregressive LLM architecture due to its generality and potentially easy integration into multi-modal systems. The crux of applying autoregressive training in language generation to visual generation is discretization -- representing continuous data like images and videos as discrete tokens. Common methods of discretizing images and videos include modeling raw pixel values, which are prohibitively lengthy, or vector quantization, which requires convoluted pre-hoc training. In this work, we propose to directly model images and videos as compressed files saved on computers via canonical codecs (e.g., JPEG, AVC/H.264). Using the default Llama architecture without any vision-specific modifications, we pretrain JPEG-LM from scratch to generate images (and AVC-LM to generate videos as a proof of concept), by directly outputting compressed file bytes in JPEG and AVC formats. Evaluation of image generation shows that this simple and straightforward approach is more effective than pixel-based modeling and sophisticated vector quantization baselines (on which our method yields a 31% reduction in FID). Our analysis shows that JPEG-LM has an especial advantage over vector quantization models in generating long-tail visual elements. Overall, we show that using canonical codec representations can help lower the barriers between language generation and visual generation, facilitating future research on multi-modal language/image/video LLMs.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2408.08444】W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08444">https://arxiv.org/abs/2408.08444</a></p>
  <p><b>作者</b>：Jinming Nian,Zhiyuan Peng,Qifan Wang,Yi Fang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, open-domain question answering, factual answers relying, answers relying solely</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In knowledge-intensive tasks such as open-domain question answering (OpenQA), Large Language Models (LLMs) often struggle to generate factual answers relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create weakly labeled data for training dense retrievers. Specifically, we rerank the top-$K$ passages retrieved via BM25 by assessing the probability that LLMs will generate the correct answer based on the question and each passage. The highest-ranking passages are then used as positive training examples for dense retrieval. Our comprehensive experiments across four publicly available OpenQA datasets demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2408.08411】Rater Cohesion and Quality from a Vicarious Perspective</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08411">https://arxiv.org/abs/2408.08411</a></p>
  <p><b>作者</b>：Deepak Pandita,Tharindu Cyril Weerasooriya,Sujan Dutta,Sarah K. Luger,Tharindu Ranasinghe,Ashiqur R. KhudaBukhsh,Marcos Zampieri,Christopher M. Homan</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Human feedback, content moderation, sentiment analysis, feedback is essential, essential for building</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Human feedback is essential for building human-centered AI systems across domains where disagreement is prevalent, such as AI safety, content moderation, or sentiment analysis. Many disagreements, particularly in politically charged settings, arise because raters have opposing values or beliefs. Vicarious annotation is a method for breaking down disagreement by asking raters how they think others would annotate the data. In this paper, we explore the use of vicarious annotation with analytical methods for moderating rater disagreement. We employ rater cohesion metrics to study the potential influence of political affiliations and demographic backgrounds on raters' perceptions of offense. Additionally, we utilize CrowdTruth's rater quality metrics, which consider the demographics of the raters, to score the raters and their annotations. We study how the rater quality metrics influence the in-group and cross-group rater cohesion across the personal and vicarious levels.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2408.08400】Zero-Shot Learning and Key Points Are All You Need for Automated Fact-Checking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08400">https://arxiv.org/abs/2408.08400</a></p>
  <p><b>作者</b>：Mohammad Ghiasvand Mohammadkhani,Ali Ghiasvand Mohammadkhani,Hamid Beigy</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：determining the accurate, accurate status, proposed claim, vast amount, Natural Language Processing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Automated fact-checking is an important task because determining the accurate status of a proposed claim within the vast amount of information available online is a critical challenge. This challenge requires robust evaluation to prevent the spread of false information. Modern large language models (LLMs) have demonstrated high capability in performing a diverse range of Natural Language Processing (NLP) tasks. By utilizing proper prompting strategies, their versatility due to their understanding of large context sizes and zero-shot learning ability enables them to simulate human problem-solving intuition and move towards being an alternative to humans for solving problems. In this work, we introduce a straightforward framework based on Zero-Shot Learning and Key Points (ZSL-KeP) for automated fact-checking, which despite its simplicity, performed well on the AVeriTeC shared task dataset by robustly improving the baseline and achieving 10th place.</p>
  </details>
</details>
<details>
  <summary>40. <b>【2408.08396】Level Up Your Tutorials: VLMs for Game Tutorials Quality Assessment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08396">https://arxiv.org/abs/2408.08396</a></p>
  <p><b>作者</b>：Daniele Rege Cambrin,Gabriele Scaffidi Militone,Luca Colomba,Giovanni Malnati,Daniele Apiletti,Paolo Garza</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：complex core mechanics, Designing effective game, smooth learning curve, Designing effective, core mechanics</p>
  <p><b>备注</b>： Accepted at ECCV 2024 CV2 Workshop</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Designing effective game tutorials is crucial for a smooth learning curve for new players, especially in games with many rules and complex core mechanics. Evaluating the effectiveness of these tutorials usually requires multiple iterations with testers who have no prior knowledge of the game. Recent Vision-Language Models (VLMs) have demonstrated significant capabilities in understanding and interpreting visual content. VLMs can analyze images, provide detailed insights, and answer questions about their content. They can recognize objects, actions, and contexts in visual data, making them valuable tools for various applications, including automated game testing. In this work, we propose an automated game-testing solution to evaluate the quality of game tutorials. Our approach leverages VLMs to analyze frames from video game tutorials, answer relevant questions to simulate human perception, and provide feedback. This feedback is compared with expected results to identify confusing or problematic scenes and highlight potential errors for developers. In addition, we publish complete tutorial videos and annotated frames from different game versions used in our tests. This solution reduces the need for extensive manual testing, especially by speeding up and simplifying the initial development stages of the tutorial to improve the final game experience.</p>
  </details>
</details>
<details>
  <summary>41. <b>【2408.08379】owards Realistic Synthetic User-Generated Content: A Scaffolding Approach to Generating Online Discussions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08379">https://arxiv.org/abs/2408.08379</a></p>
  <p><b>作者</b>：Krisztian Balog,John Palowitch,Barbara Ikica,Filip Radlinski,Hamidreza Alvari,Mehdi Manshadi</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：modern machine learning, synthetic data represents, highly private, machine learning, offering a solution</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The emergence of synthetic data represents a pivotal shift in modern machine learning, offering a solution to satisfy the need for large volumes of data in domains where real data is scarce, highly private, or difficult to obtain. We investigate the feasibility of creating realistic, large-scale synthetic datasets of user-generated content, noting that such content is increasingly prevalent and a source of frequently sought information. Large language models (LLMs) offer a starting point for generating synthetic social media discussion threads, due to their ability to produce diverse responses that typify online interactions. However, as we demonstrate, straightforward application of LLMs yields limited success in capturing the complex structure of online discussions, and standard prompting mechanisms lack sufficient control. We therefore propose a multi-step generation process, predicated on the idea of creating compact representations of discussion threads, referred to as scaffolds. Our framework is generic yet adaptable to the unique characteristics of specific social media platforms. We demonstrate its feasibility using data from two distinct online discussion platforms. To address the fundamental challenge of ensuring the representativeness and realism of synthetic data, we propose a portfolio of evaluation measures to compare various instantiations of our framework.</p>
  </details>
</details>
<details>
  <summary>42. <b>【2408.08374】Evaluating Text Classification Robustness to Part-of-Speech Adversarial Examples</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08374">https://arxiv.org/abs/2408.08374</a></p>
  <p><b>作者</b>：Anahita Samadi,Allison Sullivan</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：machine learning systems, safety critical applications, machine learning, text-based adversarial, learning systems</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As machine learning systems become more widely used, especially for safety critical applications, there is a growing need to ensure that these systems behave as intended, even in the face of adversarial examples. Adversarial examples are inputs that are designed to trick the decision making process, and are intended to be imperceptible to humans. However, for text-based classification systems, changes to the input, a string of text, are always perceptible. Therefore, text-based adversarial examples instead focus on trying to preserve semantics. Unfortunately, recent work has shown this goal is often not met. To improve the quality of text-based adversarial examples, we need to know what elements of the input text are worth focusing on. To address this, in this paper, we explore what parts of speech have the highest impact of text-based classifiers. Our experiments highlight a distinct bias in CNN algorithms against certain parts of speech tokens within review datasets. This finding underscores a critical vulnerability in the linguistic processing capabilities of CNNs.</p>
  </details>
</details>
<details>
  <summary>43. <b>【2408.08335】Plan with Code: Comparing approaches for robust NL to DSL generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08335">https://arxiv.org/abs/2408.08335</a></p>
  <p><b>作者</b>：Nastaran Bassamzadeh,Chhaya Methani</p>
  <p><b>类目</b>：oftware Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：code, Natural Language, Domain Specific Languages, function, generated via Natural</p>
  <p><b>备注</b>： 9 pages, 1 figure, 5 tables. arXiv admin note: substantial text overlap with [arXiv:2407.02742](https://arxiv.org/abs/2407.02742) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Planning in code is considered a more reliable approach for many orchestration tasks. This is because code is more tractable than steps generated via Natural Language and make it easy to support more complex sequences by abstracting deterministic logic into functions. It also allows spotting issues with incorrect function names with the help of parsing checks that can be run on code. Progress in Code Generation methodologies, however, remains limited to general-purpose languages like C, C++, and Python. LLMs continue to face challenges with custom function names in Domain Specific Languages or DSLs, leading to higher hallucination rates and syntax errors. This is more common for custom function names, that are typically part of the plan. Moreover, keeping LLMs up-to-date with newer function names is an issue. This poses a challenge for scenarios like task planning over a large number of APIs, since the plan is represented as a DSL having custom API names. In this paper, we focus on workflow automation in RPA (Robotic Process Automation) domain as a special case of task planning. We present optimizations for using Retrieval Augmented Generation (or RAG) with LLMs for DSL generation along with an ablation study comparing these strategies with a fine-tuned model. Our results showed that the fine-tuned model scored the best on code similarity metric. However, with our optimizations, RAG approach is able to match the quality for in-domain API names in the test set. Additionally, it offers significant advantage for out-of-domain or unseen API names, outperforming Fine-Tuned model on similarity metric by 7 pts.</p>
  </details>
</details>
<details>
  <summary>44. <b>【2408.08333】CodeMirage: Hallucinations in Code Generated by Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08333">https://arxiv.org/abs/2408.08333</a></p>
  <p><b>作者</b>：Vibhor Agarwal,Yulong Pei,Salwa Alamir,Xiaomo Liu</p>
  <p><b>类目</b>：oftware Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, shown promising potentials, Language Models, code</p>
  <p><b>备注</b>： Accepted at AutoMates @ IJCAI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset for code hallucinations. The benchmark contains 1,137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets - HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.</p>
  </details>
</details>
<details>
  <summary>45. <b>【2408.08729】ConcateNet: Dialogue Separation Using Local And Global Feature Concatenation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08729">https://arxiv.org/abs/2408.08729</a></p>
  <p><b>作者</b>：Mhd Modar Halimeh,Matteo Torcoli,Emanuël Habets</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Sound (cs.SD)</p>
  <p><b>关键词</b>：separation involves isolating, Dialogue separation involves, involves isolating, Dialogue separation, separation involves</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Dialogue separation involves isolating a dialogue signal from a mixture, such as a movie or a TV program. This can be a necessary step to enable dialogue enhancement for broadcast-related applications. In this paper, ConcateNet for dialogue separation is proposed, which is based on a novel approach for processing local and global features aimed at better generalization for out-of-domain signals. ConcateNet is trained using a noise reduction-focused, publicly available dataset and evaluated using three datasets: two noise reduction-focused datasets (in-domain), which show competitive performance for ConcateNet, and a broadcast-focused dataset (out-of-domain), which verifies the better generalization performance for the proposed architecture compared to considered state-of-the-art noise-reduction methods.</p>
  </details>
</details>
<h1>信息检索</h1>
<details>
  <summary>1. <b>【2408.08821】EasyRec: Simple yet Effective Language Models for Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08821">https://arxiv.org/abs/2408.08821</a></p>
  <p><b>作者</b>：Xubin Ren,Chao Huang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Deep neural networks, user-item interaction data, Deep neural, neural networks, powerful technique</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deep neural networks have become a powerful technique for learning representations from user-item interaction data in collaborative filtering (CF) for recommender systems. However, many existing methods heavily rely on unique user and item IDs, which limits their ability to perform well in practical zero-shot learning scenarios where sufficient training data may be unavailable. Inspired by the success of language models (LMs) and their strong generalization capabilities, a crucial question arises: How can we harness the potential of language models to empower recommender systems and elevate its generalization capabilities to new heights? In this study, we propose EasyRec - an effective and easy-to-use approach that seamlessly integrates text-based semantic understanding with collaborative signals. EasyRec employs a text-behavior alignment framework, which combines contrastive learning with collaborative language model tuning, to ensure a strong alignment between the text-enhanced semantic space and the collaborative behavior information. Extensive empirical evaluations across diverse real-world datasets demonstrate the superior performance of EasyRec compared to state-of-the-art alternative models, particularly in the challenging text-based zero-shot recommendation scenarios. Furthermore, the study highlights the potential of seamlessly integrating EasyRec as a plug-and-play component into text-enhanced collaborative filtering frameworks, thereby empowering existing recommender systems to elevate their recommendation performance and adapt to the evolving user preferences in dynamic environments. For better result reproducibility of our EasyRec framework, the model implementation details, source code, and datasets are available at the link: this https URL.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2408.08713】Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08713">https://arxiv.org/abs/2408.08713</a></p>
  <p><b>作者</b>：Yunxiao Shi,Wujiang Wu,Mingyu Jin,Haimin Zhang,Qiang Wu,Yongfeng Zhang,Min Xu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：click-through rate, crucial for click-through, high-order explicit interactions, modeling high-order, interactions</p>
  <p><b>备注</b>： KarSein for CTR</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Modeling feature interactions is crucial for click-through rate (CTR) prediction, particularly when it comes to high-order explicit interactions. Traditional methods struggle with this task because they often predefine a maximum interaction order, which relies heavily on prior knowledge and can limit the model's effectiveness. Additionally, modeling high-order interactions typically leads to increased computational costs. Therefore, the challenge lies in adaptively modeling high-order feature interactions while maintaining efficiency. To address this issue, we introduce Kolmogorov-Arnold Represented Sparse Efficient Interaction Network (KarSein), designed to optimize both predictive accuracy and computational efficiency. We firstly identify limitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR and then introduce KarSein to overcome these issues. It features a novel architecture that reduces the computational costs of KAN and supports embedding vectors as feature inputs. Additionally, KarSein employs guided symbolic regression to address the challenge of KAN in spontaneously learning multiplicative relationships. Extensive experiments demonstrate KarSein's superior performance, achieving significant predictive accuracy with minimal computational overhead. Furthermore, KarSein maintains strong global explainability while enabling the removal of redundant features, resulting in a sparse network structure. These advantages also position KarSein as a promising method for efficient inference.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2408.08709】Multimodal Relational Triple Extraction with Query-based Entity Object Transformer</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08709">https://arxiv.org/abs/2408.08709</a></p>
  <p><b>作者</b>：Lei Hei,Ning An,Tingjing Liao,Qi Ma,Jiaqi Wang,Feiliang Ren</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：realistic knowledge graphs, Multimodal Relation Extraction, knowledge graphs, crucial for constructing, constructing flexible</p>
  <p><b>备注</b>： 15 pages, 7 figures, preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal Relation Extraction is crucial for constructing flexible and realistic knowledge graphs. Recent studies focus on extracting the relation type with entity pairs present in different modalities, such as one entity in the text and another in the image. However, existing approaches require entities and objects given beforehand, which is costly and impractical. To address the limitation, we propose a novel task, Multimodal Entity-Object Relational Triple Extraction, which aims to extract all triples (entity span, relation, object region) from image-text pairs. To facilitate this study, we modified a multimodal relation extraction dataset MORE, which includes 21 relation types, to create a new dataset containing 20,264 triples, averaging 5.75 triples per image-text pair. Moreover, we propose QEOT, a query-based model with a selective attention mechanism, to dynamically explore the interaction and fusion of textual and visual information. In particular, the proposed method can simultaneously accomplish entity extraction, relation classification, and object detection with a set of queries. Our method is suitable for downstream applications and reduces error accumulation due to the pipeline-style approaches. Extensive experimental results demonstrate that our proposed method outperforms the existing baselines by 8.06% and achieves state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2408.08686】SC-Rec: Enhancing Generative Retrieval with Self-Consistent Reranking for~Sequential Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08686">https://arxiv.org/abs/2408.08686</a></p>
  <p><b>作者</b>：Tongyoung Kim,Soojin Yoon,Seongku Kang,Jinyoung Yeo,Dongha Lee</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：advanced language understanding, advanced language, language understanding, generation capabilities, recommendation systems due</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Language Models (LMs) are increasingly employed in recommendation systems due to their advanced language understanding and generation capabilities. Recent recommender systems based on generative retrieval have leveraged the inferential abilities of LMs to directly generate the index tokens of the next item, based on item sequences within the user's interaction history. Previous studies have mostly focused on item indices based solely on textual semantic or collaborative information. However, although the standalone effectiveness of these aspects has been demonstrated, the integration of this information has remained unexplored. Our in-depth analysis finds that there is a significant difference in the knowledge captured by the model from heterogeneous item indices and diverse input prompts, which can have a high potential for complementarity. In this paper, we propose SC-Rec, a unified recommender system that learns diverse preference knowledge from two distinct item indices and multiple prompt templates. Furthermore, SC-Rec adopts a novel reranking strategy that aggregates a set of ranking results, inferred based on different indices and prompts, to achieve the self-consistency of the model. Our empirical evaluation on three real-world datasets demonstrates that SC-Rec considerably outperforms the state-of-the-art methods for sequential recommendation, effectively incorporating complementary knowledge from varied outputs of the model.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2408.08585】OptDist: Learning Optimal Distribution for Customer Lifetime Value Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08585">https://arxiv.org/abs/2408.08585</a></p>
  <p><b>作者</b>：Yunpeng Weng,Xing Tang,Zhenhao Xu,Fuyuan Lyu,Dugang Liu,Zexu Sun,Xiuqiang He</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：CLTV, Customer Lifetime, distribution, critical task, business applications</p>
  <p><b>备注</b>： CIKM 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Customer Lifetime Value (CLTV) prediction is a critical task in business applications. Accurately predicting CLTV is challenging in real-world business scenarios, as the distribution of CLTV is complex and mutable. Firstly, there is a large number of users without any consumption consisting of a long-tailed part that is too complex to fit. Secondly, the small set of high-value users spent orders of magnitude more than a typical user leading to a wide range of the CLTV distribution which is hard to capture in a single distribution. Existing approaches for CLTV estimation either assume a prior probability distribution and fit a single group of distribution-related parameters for all samples, or directly learn from the posterior distribution with manually predefined buckets in a heuristic manner. However, all these methods fail to handle complex and mutable distributions. In this paper, we propose a novel optimal distribution selection model OptDist for CLTV prediction, which utilizes an adaptive optimal sub-distribution selection mechanism to improve the accuracy of complex distribution modeling. Specifically, OptDist trains several candidate sub-distribution networks in the distribution learning module (DLM) for modeling the probability distribution of CLTV. Then, a distribution selection module (DSM) is proposed to select the sub-distribution for each sample, thus making the selection automatically and adaptively. Besides, we design an alignment mechanism that connects both modules, which effectively guides the optimization. We conduct extensive experiments on both two public and one private dataset to verify that OptDist outperforms state-of-the-art baselines. Furthermore, OptDist has been deployed on a large-scale financial platform for customer acquisition marketing campaigns and the online experiments also demonstrate the effectiveness of OptDist.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2408.08564】Collaborative Cross-modal Fusion with Large Language Model for Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08564">https://arxiv.org/abs/2408.08564</a></p>
  <p><b>作者</b>：Zhongzhou Liu,Hao Zhang,Kuicai Dong,Yuan Fang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：conventional collaborative filtering, large language models, leveraging semantic knowledge, success of conventional, exhibit limitations</p>
  <p><b>备注</b>： 10 pages, 4 figures, accepted by CIKM 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite the success of conventional collaborative filtering (CF) approaches for recommendation systems, they exhibit limitations in leveraging semantic knowledge within the textual attributes of users and items. Recent focus on the application of large language models for recommendation (LLM4Rec) has highlighted their capability for effective semantic knowledge capture. However, these methods often overlook the collaborative signals in user behaviors. Some simply instruct-tune a language model, while others directly inject the embeddings of a CF-based model, lacking a synergistic fusion of different modalities. To address these issues, we propose a framework of Collaborative Cross-modal Fusion with Large Language Models, termed CCF-LLM, for recommendation. In this framework, we translate the user-item interactions into a hybrid prompt to encode both semantic knowledge and collaborative signals, and then employ an attentive cross-modal fusion strategy to effectively fuse latent embeddings of both modalities. Extensive experiments demonstrate that CCF-LLM outperforms existing methods by effectively utilizing semantic and collaborative signals in the LLM4Rec context.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2408.08538】Don't Click the Bait: Title Debiasing News Recommendation via Cross-Field Contrastive Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08538">https://arxiv.org/abs/2408.08538</a></p>
  <p><b>作者</b>：Yijie Shu,Xiaokun Zhang,Youlin Wu,Bo Xu,Liang Yang,Hongfei Lin</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：access content, content of interest, vast amount, recommendation emerges, Cross-field Contrastive learning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:News recommendation emerges as a primary means for users to access content of interest from the vast amount of news. The title clickbait extensively exists in news domain and increases the difficulty for news recommendation to offer satisfactory services for users. Fortunately, we find that news abstract, as a critical field of news, aligns cohesively with the news authenticity. To this end, we propose a Title Debiasing News Recommendation with Cross-field Contrastive learning (TDNR-C2) to overcome the title bias by incorporating news abstract. Specifically, a multi-field knowledge extraction module is devised to extract multi-view knowledge about news from various fields. Afterwards, we present a cross-field contrastive learning module to conduct bias removal via contrasting learned knowledge from title and abstract fileds. Experimental results on a real-world dataset demonstrate the superiority of the proposed TDNR-C2 over existing state-of-the-art methods. Further analysis also indicates the significance of news abstract for title debiasing.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2408.08521】MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08521">https://arxiv.org/abs/2408.08521</a></p>
  <p><b>作者</b>：Zhengyuan Zhu,Daniel Lee,Hong Zhang,Sai Sree Harsha,Loic Feujio,Akash Maharaj,Yunyao Li</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：demonstrated impressive performance, Recent advancements, retrieval-augmented generation, advancements in retrieval-augmented, demonstrated impressive</p>
  <p><b>备注</b>： Preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in retrieval-augmented generation (RAG) have demonstrated impressive performance in the question-answering (QA) task. However, most previous works predominantly focus on text-based answers. While some studies address multimodal data, they still fall short in generating comprehensive multimodal answers, particularly for explaining concepts or providing step-by-step tutorials on how to accomplish specific goals. This capability is especially valuable for applications such as enterprise chatbots and settings such as customer service and educational systems, where the answers are sourced from multimodal data. In this paper, we introduce a simple and effective framework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR enhances text-based answers by retrieving relevant multimodal data and refining the responses to create coherent multimodal answers. This framework can be easily extended to support multimodal answers in enterprise chatbots with minimal modifications. Human evaluation results indicate that multimodal answers generated by MuRAR are more useful and readable compared to plain text answers.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2408.08444】W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08444">https://arxiv.org/abs/2408.08444</a></p>
  <p><b>作者</b>：Jinming Nian,Zhiyuan Peng,Qifan Wang,Yi Fang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, open-domain question answering, factual answers relying, answers relying solely</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In knowledge-intensive tasks such as open-domain question answering (OpenQA), Large Language Models (LLMs) often struggle to generate factual answers relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create weakly labeled data for training dense retrievers. Specifically, we rerank the top-$K$ passages retrieved via BM25 by assessing the probability that LLMs will generate the correct answer based on the question and each passage. The highest-ranking passages are then used as positive training examples for dense retrieval. Our comprehensive experiments across four publicly available OpenQA datasets demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2408.08379】owards Realistic Synthetic User-Generated Content: A Scaffolding Approach to Generating Online Discussions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08379">https://arxiv.org/abs/2408.08379</a></p>
  <p><b>作者</b>：Krisztian Balog,John Palowitch,Barbara Ikica,Filip Radlinski,Hamidreza Alvari,Mehdi Manshadi</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：modern machine learning, synthetic data represents, highly private, machine learning, offering a solution</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The emergence of synthetic data represents a pivotal shift in modern machine learning, offering a solution to satisfy the need for large volumes of data in domains where real data is scarce, highly private, or difficult to obtain. We investigate the feasibility of creating realistic, large-scale synthetic datasets of user-generated content, noting that such content is increasingly prevalent and a source of frequently sought information. Large language models (LLMs) offer a starting point for generating synthetic social media discussion threads, due to their ability to produce diverse responses that typify online interactions. However, as we demonstrate, straightforward application of LLMs yields limited success in capturing the complex structure of online discussions, and standard prompting mechanisms lack sufficient control. We therefore propose a multi-step generation process, predicated on the idea of creating compact representations of discussion threads, referred to as scaffolds. Our framework is generic yet adaptable to the unique characteristics of specific social media platforms. We demonstrate its feasibility using data from two distinct online discussion platforms. To address the fundamental challenge of ensuring the representativeness and realism of synthetic data, we propose a portfolio of evaluation measures to compare various instantiations of our framework.</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>【2408.08872】xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08872">https://arxiv.org/abs/2408.08872</a></p>
  <p><b>作者</b>：Le Xue,Manli Shu,Anas Awadalla,Jun Wang,An Yan,Senthil Purushwalkam,Honglu Zhou,Viraj Prabhu,Yutong Dai,Michael S Ryoo,Shrikant Kendre,Jieyu Zhang,Can Qin,Shu Zhang,Chia-Chih Chen,Ning Yu,Juntao Tan,Tulika Manoj Awalgaonkar,Shelby Heinecke,Huan Wang,Yejin Choi,Ludwig Schmidt,Zeyuan Chen,Silvio Savarese,Juan Carlos Niebles,Caiming Xiong,Ran Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：developing Large Multimodal, Large Multimodal Models, Large Multimodal, developing Large, Multimodal Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This report introduces xGen-MM (also known as BLIP-3), a framework for developing Large Multimodal Models (LMMs). The framework comprises meticulously curated datasets, a training recipe, model architectures, and a resulting suite of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen initiative on foundation AI models. Our models undergo rigorous evaluation across a range of tasks, including both single and multi-image benchmarks. Our pre-trained base model exhibits strong in-context learning capabilities and the instruction-tuned model demonstrates competitive performance among open-source LMMs with similar model sizes. In addition, we introduce a safety-tuned model with DPO, aiming to mitigate harmful behaviors such as hallucinations and improve safety. We open-source our models, curated large-scale datasets, and our fine-tuning codebase to facilitate further advancements in LMM research. Associated resources will be available on our project page above.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2408.08870】SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08870">https://arxiv.org/abs/2408.08870</a></p>
  <p><b>作者</b>：Xinyu Xiong,Zihuang Wu,Shuangyi Tan,Wenxue Li,Feilong Tang,Ying Chen,Siying Li,Jie Ma,Guanbin Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Image segmentation plays, plays an important, important role, vision understanding, vision foundation models</p>
  <p><b>备注</b>： Technical Report</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image segmentation plays an important role in vision understanding. Recently, the emerging vision foundation models continuously achieved superior performance on various tasks. Following such success, in this paper, we prove that the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shaped segmentation models. We propose a simple but effective framework, termed SAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts the Hiera backbone of SAM2 as the encoder, while the decoder uses the classic U-shaped design. Additionally, adapters are inserted into the encoder to allow parameter-efficient fine-tuning. Preliminary experiments on various downstream tasks, such as camouflaged object detection, salient object detection, marine animal segmentation, mirror detection, and polyp segmentation, demonstrate that our SAM2-UNet can simply beat existing specialized state-of-the-art methods without bells and whistles. Project page: \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2408.08855】DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08855">https://arxiv.org/abs/2408.08855</a></p>
  <p><b>作者</b>：Eman Ali,Sathira Silva,Muhammad Haris Khan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：shown remarkable potential, Vision-language models, shown remarkable, remarkable potential, Vision-language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Vision-language models (VLMs), e.g., CLIP, have shown remarkable potential in zero-shot image classification. However, adapting these models to new domains remains challenging, especially in unsupervised settings where labelled data is unavailable. Recent research has proposed pseudo-labelling approaches to adapt CLIP in an unsupervised manner using unlabelled target data. Nonetheless, these methods struggle due to noisy pseudo-labels resulting from the misalignment between CLIP's visual and textual representations. This study introduces DPA, an unsupervised domain adaptation method for VLMs. DPA introduces the concept of dual prototypes, acting as distinct classifiers, along with the convex combination of their outputs, thereby leading to accurate pseudo-label construction. Next, it ranks pseudo-labels to facilitate robust self-training, particularly during early training. Finally, it addresses visual-textual misalignment by aligning textual prototypes with image prototypes to further improve the adaptation performance. Experiments on 13 downstream vision tasks demonstrate that DPA significantly outperforms zero-shot CLIP and the state-of-the-art unsupervised adaptation baselines.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2408.08827】RGBT Tracking via All-layer Multimodal Interactions with Progressive Fusion Mamba</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08827">https://arxiv.org/abs/2408.08827</a></p>
  <p><b>作者</b>：Andong Lu,Wanyu Wang,Chenglong Li,Jin Tang,Bin Luo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：RGBT tracking, fusion Mamba, robust RGBT tracking, Difference-based Fusion Mamba, Existing RGBT tracking</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing RGBT tracking methods often design various interaction models to perform cross-modal fusion of each layer, but can not execute the feature interactions among all layers, which plays a critical role in robust multimodal representation, due to large computational burden. To address this issue, this paper presents a novel All-layer multimodal Interaction Network, named AINet, which performs efficient and effective feature interactions of all modalities and layers in a progressive fusion Mamba, for robust RGBT tracking. Even though modality features in different layers are known to contain different cues, it is always challenging to build multimodal interactions in each layer due to struggling in balancing interaction capabilities and efficiency. Meanwhile, considering that the feature discrepancy between RGB and thermal modalities reflects their complementary information to some extent, we design a Difference-based Fusion Mamba (DFM) to achieve enhanced fusion of different modalities with linear complexity. When interacting with features from all layers, a huge number of token sequences (3840 tokens in this work) are involved and the computational burden is thus large. To handle this problem, we design an Order-dynamic Fusion Mamba (OFM) to execute efficient and effective feature interactions of all layers by dynamically adjusting the scan order of different layers in Mamba. Extensive experiments on four public RGBT tracking datasets show that AINet achieves leading performance against existing state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2408.08822】PFDiff: Training-free Acceleration of Diffusion Models through the Gradient Guidance of Past and Future</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08822">https://arxiv.org/abs/2408.08822</a></p>
  <p><b>作者</b>：Guangyi Wang,Yuren Cai,Lijiang Li,Wei Peng,Songzhi Su</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：shown remarkable potential, Diffusion Probabilistic Models, ODE solvers, fast ODE solvers, Diffusion Probabilistic</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion Probabilistic Models (DPMs) have shown remarkable potential in image generation, but their sampling efficiency is hindered by the need for numerous denoising steps. Most existing solutions accelerate the sampling process by proposing fast ODE solvers. However, the inevitable discretization errors of the ODE solvers are significantly magnified when the number of function evaluations (NFE) is fewer. In this work, we propose PFDiff, a novel training-free and orthogonal timestep-skipping strategy, which enables existing fast ODE solvers to operate with fewer NFE. Based on two key observations: a significant similarity in the model's outputs at time step size that is not excessively large during the denoising process of existing ODE solvers, and a high resemblance between the denoising process and SGD. PFDiff, by employing gradient replacement from past time steps and foresight updates inspired by Nesterov momentum, rapidly updates intermediate states, thereby reducing unnecessary NFE while correcting for discretization errors inherent in first-order ODE solvers. Experimental results demonstrate that PFDiff exhibits flexible applicability across various pre-trained DPMs, particularly excelling in conditional DPMs and surpassing previous state-of-the-art training-free methods. For instance, using DDIM as a baseline, we achieved 16.46 FID (4 NFE) compared to 138.81 FID with DDIM on ImageNet 64x64 with classifier guidance, and 13.06 FID (10 NFE) on Stable Diffusion with 7.5 guidance scale.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2408.08813】Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08813">https://arxiv.org/abs/2408.08813</a></p>
  <p><b>作者</b>：Lin Zhao,Xiao Chen,Eric Z. Chen,Yikang Liu,Terrence Chen,Shanhui Sun</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Medical image segmentation, presents significant challenges, Medical image, few-shot medical image, image segmentation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Medical image segmentation is crucial for clinical decision-making, but the scarcity of annotated data presents significant challenges. Few-shot segmentation (FSS) methods show promise but often require retraining on the target domain and struggle to generalize across different modalities. Similarly, adapting foundation models like the Segment Anything Model (SAM) for medical imaging has limitations, including the need for finetuning and domain-specific adaptation. To address these issues, we propose a novel method that adapts DINOv2 and Segment Anything Model 2 (SAM 2) for retrieval-augmented few-shot medical image segmentation. Our approach uses DINOv2's feature as query to retrieve similar samples from limited annotated data, which are then encoded as memories and stored in memory bank. With the memory attention mechanism of SAM 2, the model leverages these memories as conditions to generate accurate segmentation of the target image. We evaluated our framework on three medical image segmentation tasks, demonstrating superior performance and generalizability across various modalities without the need for any retraining or finetuning. Overall, this method offers a practical and effective solution for few-shot medical image segmentation and holds significant potential as a valuable annotation tool in clinical applications.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2408.08802】PriorMapNet: Enhancing Online Vectorized HD Map Construction with Priors</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08802">https://arxiv.org/abs/2408.08802</a></p>
  <p><b>作者</b>：Rongxuan Wang,Xin Lu,Xiaoyang Liu,Xiaoyi Zou,Tongyi Cao,Ying Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Online vectorized High-Definition, autonomous driving, crucial for subsequent, Online vectorized, map construction</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Online vectorized High-Definition (HD) map construction is crucial for subsequent prediction and planning tasks in autonomous driving. Following MapTR paradigm, recent works have made noteworthy achievements. However, reference points are randomly initialized in mainstream methods, leading to unstable matching between predictions and ground truth. To address this issue, we introduce PriorMapNet to enhance online vectorized HD map construction with priors. We propose the PPS-Decoder, which provides reference points with position and structure priors. Fitted from the map elements in the dataset, prior reference points lower the learning difficulty and achieve stable matching. Furthermore, we propose the PF-Encoder to enhance the image-to-BEV transformation with BEV feature priors. Besides, we propose the DMD cross-attention, which decouples cross-attention along multi-scale and multi-sample respectively to achieve efficiency. Our proposed PriorMapNet achieves state-of-the-art performance in the online vectorized HD map construction task on nuScenes and Argoverse2 datasets. The code will be released publicly soon.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2408.08793】Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08793">https://arxiv.org/abs/2408.08793</a></p>
  <p><b>作者</b>：Simone Ricci,Niccolò Biondi,Federico Pernici,Alberto Del Bimbo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Visual retrieval systems, retrieval systems face, systems face significant, face significant challenges, Visual retrieval</p>
  <p><b>备注</b>： Accepted at BEW2024 Workshop at ECCV2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Visual retrieval systems face significant challenges when updating models with improved representations due to misalignment between the old and new representations. The costly and resource-intensive backfilling process involves recalculating feature vectors for images in the gallery set whenever a new model is introduced. To address this, prior research has explored backward-compatible training methods that enable direct comparisons between new and old representations without backfilling. Despite these advancements, achieving a balance between backward compatibility and the performance of independently trained models remains an open problem. In this paper, we address it by expanding the representation space with additional dimensions and learning an orthogonal transformation to achieve compatibility with old models and, at the same time, integrate new information. This transformation preserves the original feature space's geometry, ensuring that our model aligns with previous versions while also learning new data. Our Orthogonal Compatible Aligned (OCA) approach eliminates the need for re-indexing during model updates and ensures that features can be compared directly across different model updates without additional mapping functions. Experimental results on CIFAR-100 and ImageNet-1k demonstrate that our method not only maintains compatibility with previous models but also achieves state-of-the-art accuracy, outperforming several existing methods.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2408.08766】VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08766">https://arxiv.org/abs/2408.08766</a></p>
  <p><b>作者</b>：Albert Gassol Puigjaner,Edoardo Mello Rella,Erik Sandström,Ajad Chhatkuli,Luc Van Gool</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：shown surprising accuracy, neural radiance fields, neural radiance, shown surprising, surprising accuracy</p>
  <p><b>备注</b>： 15 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Implicit surfaces via neural radiance fields (NeRF) have shown surprising accuracy in surface reconstruction. Despite their success in reconstructing richly textured surfaces, existing methods struggle with planar regions with weak textures, which account for the majority of indoor scenes. In this paper, we address indoor dense surface reconstruction by revisiting key aspects of NeRF in order to use the recently proposed Vector Field (VF) as the implicit representation. VF is defined by the unit vector directed to the nearest surface point. It therefore flips direction at the surface and equals to the explicit surface normals. Except for this flip, VF remains constant along planar surfaces and provides a strong inductive bias in representing planar surfaces. Concretely, we develop a novel density-VF relationship and a training scheme that allows us to learn VF via volume rendering By doing this, VF-NeRF can model large planar surfaces and sharp corners accurately. We show that, when depth cues are available, our method further improves and achieves state-of-the-art results in reconstructing indoor scenes and rendering novel views. We extensively evaluate VF-NeRF on indoor datasets and run ablations of its components.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2408.08753】PCP-MAE: Learning to Predict Centers for Point Masked Autoencoders</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08753">https://arxiv.org/abs/2408.08753</a></p>
  <p><b>作者</b>：Xiangdong Zhang,Shaofeng Zhang,Junchi Yan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：point cloud self-supervised, cloud self-supervised learning, point cloud, Point Masked AutoEncoders, masked parts</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Masked autoencoder has been widely explored in point cloud self-supervised learning, whereby the point cloud is generally divided into visible and masked parts. These methods typically include an encoder accepting visible patches (normalized) and corresponding patch centers (position) as input, with the decoder accepting the output of the encoder and the centers (position) of the masked parts to reconstruct each point in the masked patches. Then, the pre-trained encoders are used for downstream tasks. In this paper, we show a motivating empirical result that when directly feeding the centers of masked patches to the decoder without information from the encoder, it still reconstructs well. In other words, the centers of patches are important and the reconstruction objective does not necessarily rely on representations of the encoder, thus preventing the encoder from learning semantic representations. Based on this key observation, we propose a simple yet effective method, i.e., learning to Predict Centers for Point Masked AutoEncoders (PCP-MAE) which guides the model to learn to predict the significant centers and use the predicted centers to replace the directly provided centers. Specifically, we propose a Predicting Center Module (PCM) that shares parameters with the original encoder with extra cross-attention to predict centers. Our method is of high pre-training efficiency compared to other alternatives and achieves great improvement over Point-MAE, particularly outperforming it by 5.50%, 6.03%, and 5.17% on three variants of ScanObjectNN. The code will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2408.08751】Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08751">https://arxiv.org/abs/2408.08751</a></p>
  <p><b>作者</b>：Sanchayan Vivekananthan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：Generative Adversarial Networks, Variational Autoencoders, generative modelling frameworks, Adversarial Networks, major generative modelling</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper examines three major generative modelling frameworks: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable Diffusion models. VAEs are effective at learning latent representations but frequently yield blurry results. GANs can generate realistic images but face issues such as mode collapse. Stable Diffusion models, while producing high-quality images with strong semantic coherence, are demanding in terms of computational resources. Additionally, the paper explores how incorporating Grounding DINO and Grounded SAM with Stable Diffusion improves image accuracy by utilising sophisticated segmentation and inpainting techniques. The analysis guides on selecting suitable models for various applications and highlights areas for further research.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2408.08736】ask-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image Super-Resolution</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08736">https://arxiv.org/abs/2408.08736</a></p>
  <p><b>作者</b>：Tianyi Xu,Yiji Zhou,Xiaotao Hu,Kai Zhang,Anran Zhang,Xingye Qiu,Jun Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：aims to learn, ASSR, learn a single, single model, arbitrary magnifying scales</p>
  <p><b>备注</b>： ECAI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Arbitrary-scale super-resolution (ASSR) aims to learn a single model for image super-resolution at arbitrary magnifying scales. Existing ASSR networks typically comprise an off-the-shelf scale-agnostic feature extractor and an arbitrary scale upsampler. These feature extractors often use fixed network architectures to address different ASSR inference tasks, each of which is characterized by an input image and an upsampling scale. However, this overlooks the difficulty variance of super-resolution on different inference scenarios, where simple images or small SR scales could be resolved with less computational effort than difficult images or large SR scales. To tackle this difficulty variability, in this paper, we propose a Task-Aware Dynamic Transformer (TADT) as an input-adaptive feature extractor for efficient image ASSR. Our TADT consists of a multi-scale feature extraction backbone built upon groups of Multi-Scale Transformer Blocks (MSTBs) and a Task-Aware Routing Controller (TARC). The TARC predicts the inference paths within feature extraction backbone, specifically selecting MSTBs based on the input images and SR scales. The prediction of inference path is guided by a new loss function to trade-off the SR accuracy and efficiency. Experiments demonstrate that, when working with three popular arbitrary-scale upsamplers, our TADT achieves state-of-the-art ASSR performance when compared with mainstream feature extractors, but with relatively fewer computational costs. The code will be publicly released.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2408.08723】Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08723">https://arxiv.org/abs/2408.08723</a></p>
  <p><b>作者</b>：Wei Sun,Xiaosong Zhang,Fang Wan,Yanzhao Zhou,Yuan Li,Qixiang Ye,Jianbin Jiao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：View Synthesis, variable operating conditions, promoting rapid response, rapid response capabilities, pre-processed camera poses</p>
  <p><b>备注</b>： arXiv admin note: text overlap with [arXiv:2312.07504](https://arxiv.org/abs/2312.07504) by other authors</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed camera poses--referred to as SfM-free methods--is crucial for promoting rapid response capabilities and enhancing robustness against variable operating conditions. Recent SfM-free methods have integrated pose optimization, designing end-to-end frameworks for joint camera pose estimation and NVS. However, most existing works rely on per-pixel image loss functions, such as L2 loss. In SfM-free methods, inaccurate initial poses lead to misalignment issue, which, under the constraints of per-pixel image loss functions, results in excessive gradients, causing unstable optimization and poor convergence for NVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussian splatting for NVS. We use correspondences between the target and the rendered result to achieve better pixel alignment, facilitating the optimization of relative poses between frames. We then apply the learned poses to optimize the entire scene. Each 2D screen-space pixel is associated with its corresponding 3D Gaussians through approximated surface rendering to facilitate gradient back propagation. Experimental results underline the superior performance and time efficiency of the proposed approach compared to the state-of-the-art baselines.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2408.08708】Decoupling Feature Representations of Ego and Other Modalities for Incomplete Multi-modal Brain Tumor Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08708">https://arxiv.org/abs/2408.08708</a></p>
  <p><b>作者</b>：Kaixiang Yang,Wenqi Shan,Xudong Li,Xuan Wang,Xikai Yang,Xi Wang,Pheng-Ann Heng,Qiang Li,Zhiwei Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：magnetic resonance imaging, significantly degrade performance, segmentation typically involves, modalities significantly degrade, resonance imaging</p>
  <p><b>备注</b>： 8 pages, 4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-modal brain tumor segmentation typically involves four magnetic resonance imaging (MRI) modalities, while incomplete modalities significantly degrade performance. Existing solutions employ explicit or implicit modality adaptation, aligning features across modalities or learning a fused feature robust to modality incompleteness. They share a common goal of encouraging each modality to express both itself and the others. However, the two expression abilities are entangled as a whole in a seamless feature space, resulting in prohibitive learning burdens. In this paper, we propose DeMoSeg to enhance the modality adaptation by Decoupling the task of representing the ego and other Modalities for robust incomplete multi-modal Segmentation. The decoupling is super lightweight by simply using two convolutions to map each modality onto four feature sub-spaces. The first sub-space expresses itself (Self-feature), while the remaining sub-spaces substitute for other modalities (Mutual-features). The Self- and Mutual-features interactively guide each other through a carefully-designed Channel-wised Sparse Self-Attention (CSSA). After that, a Radiologist-mimic Cross-modality expression Relationships (RCR) is introduced to have available modalities provide Self-feature and also `lend' their Mutual-features to compensate for the absent ones by exploiting the clinical prior knowledge. The benchmark results on BraTS2020, BraTS2018 and BraTS2015 verify the DeMoSeg's superiority thanks to the alleviated modality adaptation difficulty. Concretely, for BraTS2020, DeMoSeg increases Dice by at least 0.92%, 2.95% and 4.95% on whole tumor, tumor core and enhanced tumor regions, respectively, compared to other state-of-the-arts. Codes are at this https URL</p>
  </details>
</details>
<details>
  <summary>15. <b>【2408.08704】Beyond the Hype: A dispassionate look at vision-language models in medical scenario</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08704">https://arxiv.org/abs/2408.08704</a></p>
  <p><b>作者</b>：Yang Nan,Huichi Zhou,Xiaodan Xing,Guang Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Vision-Language Models, garnering significant attention, Recent advancements, Vision-Language Models, demonstrated remarkable capabilities</p>
  <p><b>备注</b>： 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across diverse tasks, garnering significant attention in AI communities. However, their performance and reliability in specialized domains such as medicine remain insufficiently assessed. In particular, most assessments over-concentrate in evaluating VLMs based on simple Visual Question Answering (VQA) on multi-modality data, while ignoring the in-depth characteristic of LVLMs. In this study, we introduce RadVUQA, a novel Radiological Visual Understanding and Question Answering benchmark, to comprehensively evaluate existing LVLMs. RadVUQA mainly validates LVLMs across five dimensions: 1) Anatomical understanding, assessing the models' ability to visually identify biological structures; 2) Multimodal comprehension, which involves the capability of interpreting linguistic and visual instructions to produce desired outcomes; 3) Quantitative and spatial reasoning, evaluating the models' spatial awareness and proficiency in combining quantitative analysis with visual and linguistic information; 4) Physiological knowledge, measuring the models' capability to comprehend functions and mechanisms of organs and systems; and 5) Robustness, which assesses the models' capabilities against unharmonised and synthetic data. The results indicate that both generalized LVLMs and medical-specific LVLMs have critical deficiencies with weak multimodal comprehension and quantitative reasoning capabilities. Our findings reveal the large gap between existing LVLMs and clinicians, highlighting the urgent need for more robust and intelligent LVLMs. The code and dataset will be available after the acceptance of this paper.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2408.08703】sCA: On the Semantic Consistency Alignment via Conditional Transport for Compositional Zero-Shot Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08703">https://arxiv.org/abs/2408.08703</a></p>
  <p><b>作者</b>：Miaoge Li,Jingcai Guo,Richard Yi Da Xu,Dongsheng Wang,Xiaofeng Cao,Song Guo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Compositional Zero-Shot Learning, aims to recognize, leveraging the shared, Compositional Zero-Shot, Trisets Consistency Alignment</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Compositional Zero-Shot Learning (CZSL) aims to recognize novel \textit{state-object} compositions by leveraging the shared knowledge of their primitive components. Despite considerable progress, effectively calibrating the bias between semantically similar multimodal representations, as well as generalizing pre-trained knowledge to novel compositional contexts, remains an enduring challenge. In this paper, our interest is to revisit the conditional transport (CT) theory and its homology to the visual-semantics interaction in CZSL and further, propose a novel Trisets Consistency Alignment framework (dubbed TsCA) that well-addresses these issues. Concretely, we utilize three distinct yet semantically homologous sets, i.e., patches, primitives, and compositions, to construct pairwise CT costs to minimize their semantic discrepancies. To further ensure the consistency transfer within these sets, we implement a cycle-consistency constraint that refines the learning by guaranteeing the feature consistency of the self-mapping during transport flow, regardless of modality. Moreover, we extend the CT plans to an open-world setting, which enables the model to effectively filter out unfeasible pairs, thereby speeding up the inference as well as increasing the accuracy. Extensive experiments are conducted to verify the effectiveness of the proposed method.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2408.08700】HyCoT: Hyperspectral Compression Transformer with an Efficient Training Strategy</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08700">https://arxiv.org/abs/2408.08700</a></p>
  <p><b>作者</b>：Martin Hermann Paul Fuchs,Behnood Rasti,Begüm Demir</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：attracted significant interest, recently attracted significant, learning-based hyperspectral image, significant interest, development of learning-based</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The development of learning-based hyperspectral image (HSI) compression models has recently attracted significant interest. Existing models predominantly utilize convolutional filters, which capture only local dependencies. Furthermore, they often incur high training costs and exhibit substantial computational complexity. To address these limitations, in this paper we propose Hyperspectral Compression Transformer (HyCoT) that is a transformer-based autoencoder for pixelwise HSI compression. Additionally, we introduce an efficient training strategy to accelerate the training process. Experimental results on the HySpecNet-11k dataset demonstrate that HyCoT surpasses the state-of-the-art across various compression ratios by over 1 dB with significantly reduced computational requirements. Our code and pre-trained weights are publicly available at this https URL .</p>
  </details>
</details>
<details>
  <summary>18. <b>【2408.08682】LLM-PCGC: Large Language Model-based Point Cloud Geometry Compression</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08682">https://arxiv.org/abs/2408.08682</a></p>
  <p><b>作者</b>：Yuqi Ye,Wei Gao</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：point cloud, point cloud geometry, point cloud compression, robust context model, context model consistent</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The key to effective point cloud compression is to obtain a robust context model consistent with complex 3D data structures. Recently, the advancement of large language models (LLMs) has highlighted their capabilities not only as powerful generators for in-context learning and generation but also as effective compressors. These dual attributes of LLMs make them particularly well-suited to meet the demands of data compression. Therefore, this paper explores the potential of using LLM for compression tasks, focusing on lossless point cloud geometry compression (PCGC) experiments. However, applying LLM directly to PCGC tasks presents some significant challenges, i.e., LLM does not understand the structure of the point cloud well, and it is a difficult task to fill the gap between text and point cloud through text description, especially for large complicated and small shapeless point clouds. To address these problems, we introduce a novel architecture, namely the Large Language Model-based Point Cloud Geometry Compression (LLM-PCGC) method, using LLM to compress point cloud geometry information without any text description or aligning operation. By utilizing different adaptation techniques for cross-modality representation alignment and semantic consistency, including clustering, K-tree, token mapping invariance, and Low Rank Adaptation (LoRA), the proposed method can translate LLM to a compressor/generator for point cloud. To the best of our knowledge, this is the first structure to employ LLM as a compressor for point cloud data. Experiments demonstrate that the LLM-PCGC outperforms the other existing methods significantly, by achieving -40.213% bit rate reduction compared to the reference software of MPEG Geometry-based Point Cloud Compression (G-PCC) standard, and by achieving -2.267% bit rate reduction compared to the state-of-the-art learning-based method.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2408.08671】owards Physical World Backdoor Attacks against Skeleton Action Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08671">https://arxiv.org/abs/2408.08671</a></p>
  <p><b>作者</b>：Qichen Zheng,Yi Yu,Siyuan Yang,Jun Liu,Kwok-Yan Lam,Alex Kot</p>
  <p><b>类目</b>：Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：attracted significant interest, Skeleton Action Recognition, human skeletal structure, Action Recognition, skeletal structure</p>
  <p><b>备注</b>： Accepted by ECCV 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Skeleton Action Recognition (SAR) has attracted significant interest for its efficient representation of the human skeletal structure. Despite its advancements, recent studies have raised security concerns in SAR models, particularly their vulnerability to adversarial attacks. However, such strategies are limited to digital scenarios and ineffective in physical attacks, limiting their real-world applicability. To investigate the vulnerabilities of SAR in the physical world, we introduce the Physical Skeleton Backdoor Attacks (PSBA), the first exploration of physical backdoor attacks against SAR. Considering the practicalities of physical execution, we introduce a novel trigger implantation method that integrates infrequent and imperceivable actions as triggers into the original skeleton data. By incorporating a minimal amount of this manipulated data into the training set, PSBA enables the system misclassify any skeleton sequences into the target class when the trigger action is present. We examine the resilience of PSBA in both poisoned and clean-label scenarios, demonstrating its efficacy across a range of datasets, poisoning ratios, and model architectures. Additionally, we introduce a trigger-enhancing strategy to strengthen attack performance in the clean label setting. The robustness of PSBA is tested against three distinct backdoor defenses, and the stealthiness of PSBA is evaluated using two quantitative metrics. Furthermore, by employing a Kinect V2 camera, we compile a dataset of human actions from the real world to mimic physical attack situations, with our findings confirming the effectiveness of our proposed attacks. Our project website can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2408.08670】Adaptive Layer Selection for Efficient Vision Transformer Fine-Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08670">https://arxiv.org/abs/2408.08670</a></p>
  <p><b>作者</b>：Alessio Devoto,Federico Alvetreti,Jary Pomponi,Paolo Di Lorenzo,Pasquale Minervini,Simone Scardapane</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Vision Transformers, foundation models based, foundation models, Recently, fine-tuning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, foundation models based on Vision Transformers (ViTs) have become widely available. However, their fine-tuning process is highly resource-intensive, and it hinders their adoption in several edge or low-energy applications. To this end, in this paper we introduce an efficient fine-tuning method for ViTs called $\textbf{ALaST}$ ($\textit{Adaptive Layer Selection Fine-Tuning for Vision Transformers}$) to speed up the fine-tuning process while reducing computational cost, memory load, and training time. Our approach is based on the observation that not all layers are equally critical during fine-tuning, and their importance varies depending on the current mini-batch. Therefore, at each fine-tuning step, we adaptively estimate the importance of all layers and we assign what we call ``compute budgets'' accordingly. Layers that were allocated lower budgets are either trained with a reduced number of input tokens or kept frozen. Freezing a layer reduces the computational cost and memory usage by preventing updates to its weights, while discarding tokens removes redundant data, speeding up processing and reducing memory requirements. We show that this adaptive compute allocation enables a nearly-optimal schedule for distributing computational resources across layers, resulting in substantial reductions in training time (up to 1.5x), FLOPs (up to 2x), and memory load (up to 2x) compared to traditional full fine-tuning approaches. Additionally, it can be successfully combined with other parameter-efficient fine-tuning methods, such as LoRA.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2408.08665】QMambaBSR: Burst Image Super-Resolution with Query State Space Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08665">https://arxiv.org/abs/2408.08665</a></p>
  <p><b>作者</b>：Xin Di,Long Peng,Peizhe Xia,Wenbo Li,Renjing Pei,Yang Cao,Yang Wang,Zheng-Jun Zha</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：reconstruct high-resolution images, burst low-resolution frames, Burst super-resolution aims, multiple burst low-resolution, aims to reconstruct</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Burst super-resolution aims to reconstruct high-resolution images with higher quality and richer details by fusing the sub-pixel information from multiple burst low-resolution frames. In BusrtSR, the key challenge lies in extracting the base frame's content complementary sub-pixel details while simultaneously suppressing high-frequency noise disturbance. Existing methods attempt to extract sub-pixels by modeling inter-frame relationships frame by frame while overlooking the mutual correlations among multi-current frames and neglecting the intra-frame interactions, leading to inaccurate and noisy sub-pixels for base frame super-resolution. Further, existing methods mainly employ static upsampling with fixed parameters to improve spatial resolution for all scenes, failing to perceive the sub-pixel distribution difference across multiple frames and cannot balance the fusion weights of different frames, resulting in over-smoothed details and artifacts. To address these limitations, we introduce a novel Query Mamba Burst Super-Resolution (QMambaBSR) network, which incorporates a Query State Space Model (QSSM) and Adaptive Up-sampling module (AdaUp). Specifically, based on the observation that sub-pixels have consistent spatial distribution while random noise is inconsistently distributed, a novel QSSM is proposed to efficiently extract sub-pixels through inter-frame querying and intra-frame scanning while mitigating noise interference in a single step. Moreover, AdaUp is designed to dynamically adjust the upsampling kernel based on the spatial distribution of multi-frame sub-pixel information in the different burst scenes, thereby facilitating the reconstruction of the spatial arrangement of high-resolution details. Extensive experiments on four popular synthetic and real-world benchmarks demonstrate that our method achieves a new state-of-the-art performance.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2408.08645】Extracting polygonal footprints in off-nadir images with Segment Anything Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08645">https://arxiv.org/abs/2408.08645</a></p>
  <p><b>作者</b>：Kai Li,Jingbo Chen,Yupeng Deng,Yu Meng,Diyou Liu,Junxian Ma,Chenhao Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Building Footprint Extraction, off-nadir aerial images, Footprint Extraction, off-nadir aerial, aerial images</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Building Footprint Extraction (BFE) in off-nadir aerial images often relies on roof segmentation and roof-to-footprint offset prediction, then drugging roof-to-footprint via the offset. However, the results from this multi-stage inference are not applicable in data production, because of the low quality of masks given by prediction. To solve this problem, we proposed OBMv2 in this paper, which supports both end-to-end and promptable polygonal footprint prediction. Different from OBM, OBMv2 using a newly proposed Self Offset Attention (SOFA) to bridge the performance gap on bungalow and skyscraper, which realized a real end-to-end footprint polygon prediction without postprocessing. %, such as Non-Maximum Suppression (NMS) and Distance NMS (DNMS). % To fully use information contained in roof masks, building masks and offsets, we proposed a Multi-level Information SyStem (MISS) for footprint prediction, with which OBMv2 can predict footprints even with insufficient predictions. Additionally, to squeeze information from the same model, we were inspired by Retrieval-Augmented Generation (RAG) in Nature Language Processing and proposed "RAG in BFE" problem. To verify the effectiveness of the proposed method, experiments were conducted on open datasets BONAI and OmniCity-view3. A generalization test was also conducted on Huizhou test set. The code will be available at \url{this https URL}.</p>
  </details>
</details>
<details>
  <summary>23. <b>【2408.08633】Historical Printed Ornaments: Dataset and Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08633">https://arxiv.org/abs/2408.08633</a></p>
  <p><b>作者</b>：Sayan Kumar Chaki,Zeynep Sonat Baltaci,Elliot Vincent,Remi Emonet,Fabienne Vial-Bonacci,Christelle Bahier-Porte,Mathieu Aubry,Thierry Fournel</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：unsupervised computer vision, modern unsupervised computer, historical printed ornaments, computer vision, paper aims</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper aims to develop the study of historical printed ornaments with modern unsupervised computer vision. We highlight three complex tasks that are of critical interest to book historians: clustering, element discovery, and unsupervised change localization. For each of these tasks, we introduce an evaluation benchmark, and we adapt and evaluate state-of-the-art models. Our Rey's Ornaments dataset is designed to be a representative example of a set of ornaments historians would be interested in. It focuses on an XVIIIth century bookseller, Marc-Michel Rey, providing a consistent set of ornaments with a wide diversity and representative challenges. Our results highlight the limitations of state-of-the-art models when faced with real data and show simple baselines such as k-means or congealing can outperform more sophisticated approaches on such data. Our dataset and code can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2408.08632】A Survey on Benchmarks of Multimodal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08632">https://arxiv.org/abs/2408.08632</a></p>
  <p><b>作者</b>：Jian Li,Weiheng Lu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Multimodal Large Language, Large Language Models, visual question answering, Multimodal Large, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of \textbf{180 benchmarks} and evaluation for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to better support the development of MLLMs. For more details, please visit our GitHub repository: this https URL.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2408.08623】SketchRef: A Benchmark Dataset and Evaluation Metrics for Automated Sketch Synthesis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08623">https://arxiv.org/abs/2408.08623</a></p>
  <p><b>作者</b>：Xingyue Lin,Xingjian Hu,Shuai Peng,Jianhua Zhu,Liangcai Gao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：powerful artistic technique, capture essential visual, essential visual information, increasingly gaining attention, image synthesis field</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Sketch, a powerful artistic technique to capture essential visual information about real-world objects, is increasingly gaining attention in the image synthesis field. However, evaluating the quality of synthesized sketches presents unique unsolved challenges. Current evaluation methods for sketch synthesis are inadequate due to the lack of a unified benchmark dataset, over-reliance on classification accuracy for recognizability, and unfair evaluation of sketches with different levels of simplification. To address these issues, we introduce SketchRef, a benchmark dataset comprising 4 categories of reference photos--animals, human faces, human bodies, and common objects--alongside novel evaluation metrics. Considering that classification accuracy is insufficient to measure the structural consistency between a sketch and its reference photo, we propose the mean Object Keypoint Similarity (mOKS) metric, utilizing pose estimation to assess structure-level recognizability. To ensure fair evaluation sketches with different simplification levels, we propose a recognizability calculation method constrained by simplicity. We also collect 8K responses from art enthusiasts, validating the effectiveness of our proposed evaluation methods. We hope this work can provide a comprehensive evaluation of sketch synthesis algorithms, thereby aligning their performance more closely with human understanding.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2408.08610】Generative Dataset Distillation Based on Diffusion Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08610">https://arxiv.org/abs/2408.08610</a></p>
  <p><b>作者</b>：Duo Su,Junjie Hou,Guang Li,Ren Togo,Rui Song,Takahiro Ogawa,Miki Haseyama</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Dataset Distillation Challenge, generative dataset distillation, dataset distillation method, Dataset Distillation, generative</p>
  <p><b>备注</b>： The Third Place Winner in Generative Track of the ECCV 2024 DD Challenge</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper presents our method for the generative track of The First Dataset Distillation Challenge at ECCV 2024. Since the diffusion model has become the mainstay of generative models because of its high-quality generative effects, we focus on distillation methods based on the diffusion model. Considering that the track can only generate a fixed number of images in 10 minutes using a generative model for CIFAR-100 and Tiny-ImageNet datasets, we need to use a generative model that can generate images at high speed. In this study, we proposed a novel generative dataset distillation method based on Stable Diffusion. Specifically, we use the SDXL-Turbo model which can generate images at high speed and quality. Compared to other diffusion models that can only generate images per class (IPC) = 1, our method can achieve an IPC = 10 for Tiny-ImageNet and an IPC = 20 for CIFAR-100, respectively. Additionally, to generate high-quality distilled datasets for CIFAR-100 and Tiny-ImageNet, we use the class information as text prompts and post data augmentation for the SDXL-Turbo model. Experimental results show the effectiveness of the proposed method, and we achieved third place in the generative track of the ECCV 2024 DD Challenge. Codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2408.08604】Bi-Directional Deep Contextual Video Compression</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08604">https://arxiv.org/abs/2408.08604</a></p>
  <p><b>作者</b>：Xihua Sheng,Li Li,Dong Liu,Shiqi Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：made remarkable process, concentrated on P-frame, P-frame coding, deep B-frame coding, recent years</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deep video compression has made remarkable process in recent years, with the majority of advancements concentrated on P-frame coding. Although efforts to enhance B-frame coding are ongoing, their compression performance is still far behind that of traditional bi-directional video codecs. In this paper, we introduce a bi-directional deep contextual video compression scheme tailored for B-frames, termed DCVC-B, to improve the compression performance of deep B-frame coding. Our scheme mainly has three key innovations. First, we develop a bi-directional motion difference context propagation method for effective motion difference coding, which significantly reduces the bit cost of bi-directional motions. Second, we propose a bi-directional contextual compression model and a corresponding bi-directional temporal entropy model, to make better use of the multi-scale temporal contexts. Third, we propose a hierarchical quality structure-based training strategy, leading to an effective bit allocation across large groups of pictures (GOP). Experimental results show that our DCVC-B achieves an average reduction of 26.6% in BD-Rate compared to the reference software for H.265/HEVC under random access conditions. Remarkably, it surpasses the performance of the H.266/VVC reference software on certain test datasets under the same configuration.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2408.08601】Learning A Low-Level Vision Generalist via Visual Task Prompt</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08601">https://arxiv.org/abs/2408.08601</a></p>
  <p><b>作者</b>：Xiangyu Chen,Yihao Liu,Yuandong Pu,Wenlong Zhang,Jiantao Zhou,Yu Qiao,Chao Dong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：holds significant research, Building a unified, tasks holds significant, holds significant, significant research</p>
  <p><b>备注</b>： Accepted to ACMMM24</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Building a unified model for general low-level vision tasks holds significant research and practical value. Current methods encounter several critical issues. Multi-task restoration approaches can address multiple degradation-to-clean restoration tasks, while their applicability to tasks with different target domains (e.g., image stylization) is limited. Methods like PromptGIP can handle multiple input-target domains but rely on the Masked Autoencoder (MAE) paradigm. Consequently, they are tied to the ViT architecture, resulting in suboptimal image reconstruction quality. In addition, these methods are sensitive to prompt image content and often struggle with low-frequency information processing. In this paper, we propose a Visual task Prompt-based Image Processing (VPIP) framework to overcome these challenges. VPIP employs visual task prompts to manage tasks with different input-target domains and allows flexible selection of backbone network suitable for general tasks. Besides, a new prompt cross-attention is introduced to facilitate interaction between the input and prompt information. Based on the VPIP framework, we train a low-level vision generalist model, namely GenLV, on 30 diverse tasks. Experimental results show that GenLV can successfully address a variety of low-level tasks, significantly outperforming existing methods both quantitatively and qualitatively. Codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>29. <b>【2408.08600】MM-UNet: A Mixed MLP Architecture for Improved Ophthalmic Image Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08600">https://arxiv.org/abs/2408.08600</a></p>
  <p><b>作者</b>：Zunjie Xiao,Xiaoqing Zhang,Risa Higashita,Jiang Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：ocular disease diagnosis, disease diagnosis, critical foundation, foundation for ocular, ocular disease</p>
  <p><b>备注</b>： OMIA2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Ophthalmic image segmentation serves as a critical foundation for ocular disease diagnosis. Although fully convolutional neural networks (CNNs) are commonly employed for segmentation, they are constrained by inductive biases and face challenges in establishing long-range dependencies. Transformer-based models address these limitations but introduce substantial computational overhead. Recently, a simple yet efficient Multilayer Perceptron (MLP) architecture was proposed for image classification, achieving competitive performance relative to advanced transformers. However, its effectiveness for ophthalmic image segmentation remains unexplored. In this paper, we introduce MM-UNet, an efficient Mixed MLP model tailored for ophthalmic image segmentation. Within MM-UNet, we propose a multi-scale MLP (MMLP) module that facilitates the interaction of features at various depths through a grouping strategy, enabling simultaneous capture of global and local information. We conducted extensive experiments on both a private anterior segment optical coherence tomography (AS-OCT) image dataset and a public fundus image dataset. The results demonstrated the superiority of our MM-UNet model in comparison to state-of-the-art deep segmentation networks.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2408.08591】Zero-Shot Dual-Path Integration Framework for Open-Vocabulary 3D Instance Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08591">https://arxiv.org/abs/2408.08591</a></p>
  <p><b>作者</b>：Tri Ton,Ji Woo Hong,SooHwan Eom,Jun Yeop Shim,Junyeong Kim,Chang D. Yoo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：transcends traditional closed-vocabulary, traditional closed-vocabulary methods, segmentation transcends traditional, real-world scenarios, transcends traditional</p>
  <p><b>备注</b>： OpenSUN 3D: 2nd Workshop on Open-Vocabulary 3D Scene Understanding (CVPR 2024)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Open-vocabulary 3D instance segmentation transcends traditional closed-vocabulary methods by enabling the identification of both previously seen and unseen objects in real-world scenarios. It leverages a dual-modality approach, utilizing both 3D point clouds and 2D multi-view images to generate class-agnostic object mask proposals. Previous efforts predominantly focused on enhancing 3D mask proposal models; consequently, the information that could come from 2D association to 3D was not fully exploited. This bias towards 3D data, while effective for familiar indoor objects, limits the system's adaptability to new and varied object types, where 2D models offer greater utility. Addressing this gap, we introduce Zero-Shot Dual-Path Integration Framework that equally values the contributions of both 3D and 2D modalities. Our framework comprises three components: 3D pathway, 2D pathway, and Dual-Path Integration. 3D pathway generates spatially accurate class-agnostic mask proposals of common indoor objects from 3D point cloud data using a pre-trained 3D model, while 2D pathway utilizes pre-trained open-vocabulary instance segmentation model to identify a diverse array of object proposals from multi-view RGB-D images. In Dual-Path Integration, our Conditional Integration process, which operates in two stages, filters and merges the proposals from both pathways adaptively. This process harmonizes output proposals to enhance segmentation capabilities. Our framework, utilizing pre-trained models in a zero-shot manner, is model-agnostic and demonstrates superior performance on both seen and unseen data, as evidenced by comprehensive evaluations on the ScanNet200 and qualitative results on ARKitScenes datasets.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2408.08584】S-RAF: A Simulation-Based Robustness Assessment Framework for Responsible Autonomous Driving</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08584">https://arxiv.org/abs/2408.08584</a></p>
  <p><b>作者</b>：Daniel Omeiza,Pratik Somaiya,Jo-Ann Pattinson,Carolyn Ten-Holter,Jack Stilgoe,Marina Jirotka,Lars Kunze</p>
  <p><b>类目</b>：Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：technology advances, artificial intelligence, Robustness Assessment Framework, AI-driven systems, Simulation-Based Robustness Assessment</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As artificial intelligence (AI) technology advances, ensuring the robustness and safety of AI-driven systems has become paramount. However, varying perceptions of robustness among AI developers create misaligned evaluation metrics, complicating the assessment and certification of safety-critical and complex AI systems such as autonomous driving (AD) agents. To address this challenge, we introduce Simulation-Based Robustness Assessment Framework (S-RAF) for autonomous driving. S-RAF leverages the CARLA Driving simulator to rigorously assess AD agents across diverse conditions, including faulty sensors, environmental changes, and complex traffic situations. By quantifying robustness and its relationship with other safety-critical factors, such as carbon emissions, S-RAF aids developers and stakeholders in building safe and responsible driving agents, and streamlining safety certification processes. Furthermore, S-RAF offers significant advantages, such as reduced testing costs, and the ability to explore edge cases that may be unsafe to test in the real world. The code for this framework is available here: this https URL</p>
  </details>
</details>
<details>
  <summary>32. <b>【2408.08578】AMER: Tree-Aware Transformer for Handwritten Mathematical Expression Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08578">https://arxiv.org/abs/2408.08578</a></p>
  <p><b>作者</b>：Jianhua Zhu,Wenqi Zhao,Yu Li,Xingjian Hu,Liangcai Gao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Mathematical Expression Recognition, Handwritten Mathematical Expression, Expression Recognition, Mathematical Expression, office automation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Handwritten Mathematical Expression Recognition (HMER) has extensive applications in automated grading and office automation. However, existing sequence-based decoding methods, which directly predict $\LaTeX$ sequences, struggle to understand and model the inherent tree structure of $\LaTeX$ and often fail to ensure syntactic correctness in the decoded results. To address these challenges, we propose a novel model named TAMER (Tree-Aware Transformer) for handwritten mathematical expression recognition. TAMER introduces an innovative Tree-aware Module while maintaining the flexibility and efficient training of Transformer. TAMER combines the advantages of both sequence decoding and tree decoding models by jointly optimizing sequence prediction and tree structure prediction tasks, which enhances the model's understanding and generalization of complex mathematical expression structures. During inference, TAMER employs a Tree Structure Prediction Scoring Mechanism to improve the structural validity of the generated $\LaTeX$ sequences. Experimental results on CROHME datasets demonstrate that TAMER outperforms traditional sequence decoding and tree decoding models, especially in handling complex mathematical structures, achieving state-of-the-art (SOTA) performance.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2408.08576】uning a SAM-Based Model with Multi-Cognitive Visual Adapter to Remote Sensing Instance Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08576">https://arxiv.org/abs/2408.08576</a></p>
  <p><b>作者</b>：Linghao Zheng,Xinyang Pu,Feng Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：natural scene image, scene image segmentation, demonstrates exceptional generalization, remote sensing images, foundational model designed</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The Segment Anything Model (SAM), a foundational model designed for promptable segmentation tasks, demonstrates exceptional generalization capabilities, making it highly promising for natural scene image segmentation. However, SAM's lack of pretraining on massive remote sensing images and its interactive structure limit its automatic mask prediction capabilities. In this paper, a Multi-Cognitive SAM-Based Instance Segmentation Model (MC-SAM SEG) is introduced to employ SAM on remote sensing domain. The SAM-Mona encoder utilizing the Multi-cognitive Visual Adapter (Mona) is conducted to facilitate SAM's transfer learning in remote sensing applications. The proposed method named MC-SAM SEG extracts high-quality features by fine-tuning the SAM-Mona encoder along with a feature aggregator. Subsequently, a pixel decoder and transformer decoder are designed for prompt-free mask generation and instance classification. The comprehensive experiments are conducted on the HRSID and WHU datasets for instance segmentation tasks on Synthetic Aperture Radar (SAR) images and optical remote sensing images respectively. The evaluation results indicate the proposed method surpasses other deep learning algorithms and verify its effectiveness and generalization.</p>
  </details>
</details>
<details>
  <summary>34. <b>【2408.08575】 Codec What Worth Compressing: Semantically Disentangled Image Coding for Machine with LMMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08575">https://arxiv.org/abs/2408.08575</a></p>
  <p><b>作者</b>：Jinming Liu,Yuntao Wei,Junyan Lin,Shengyang Zhao,Heming Sun,Zhibo Chen,Wenjun Zeng,Xin Jin</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Large Multimodal Models, Multimodal Models, paradigm to achieve, Large Multimodal, cleverly leveraging</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present a new image compression paradigm to achieve ``intelligently coding for machine'' by cleverly leveraging the common sense of Large Multimodal Models (LMMs). We are motivated by the evidence that large language/multimodal models are powerful general-purpose semantics predictors for understanding the real world. Different from traditional image compression typically optimized for human eyes, the image coding for machines (ICM) framework we focus on requires the compressed bitstream to more comply with different downstream intelligent analysis tasks. To this end, we employ LMM to \textcolor{red}{tell codec what to compress}: 1) first utilize the powerful semantic understanding capability of LMMs w.r.t object grounding, identification, and importance ranking via prompts, to disentangle image content before compression, 2) and then based on these semantic priors we accordingly encode and transmit objects of the image in order with a structured bitstream. In this way, diverse vision benchmarks including image classification, object detection, instance segmentation, etc., can be well supported with such a semantically structured bitstream. We dub our method ``\textit{SDComp}'' for ``\textit{S}emantically \textit{D}isentangled \textit{Comp}ression'', and compare it with state-of-the-art codecs on a wide variety of different vision tasks. SDComp codec leads to more flexible reconstruction results, promised decoded visual quality, and a more generic/satisfactory intelligent task-supporting ability.</p>
  </details>
</details>
<details>
  <summary>35. <b>【2408.08570】EraW-Net: Enhance-Refine-Align W-Net for Scene-Associated Driver Attention Estimation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08570">https://arxiv.org/abs/2408.08570</a></p>
  <p><b>作者</b>：Jun Zhou,Chunsheng Liu,Faliang Chang,Wenqian Wang,Penghui Hao,Yiming Huang,Zhiqiang Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：requires comprehensive consideration, Associating driver attention, cross-domain perception problem, driver status tracking, hard cross-domain perception</p>
  <p><b>备注</b>： 13pages, 9 figures,</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Associating driver attention with driving scene across two fields of views (FOVs) is a hard cross-domain perception problem, which requires comprehensive consideration of cross-view mapping, dynamic driving scene analysis, and driver status tracking. Previous methods typically focus on a single view or map attention to the scene via estimated gaze, failing to exploit the implicit connection between them. Moreover, simple fusion modules are insufficient for modeling the complex relationships between the two views, making information integration challenging. To address these issues, we propose a novel method for end-to-end scene-associated driver attention estimation, called EraW-Net. This method enhances the most discriminative dynamic cues, refines feature representations, and facilitates semantically aligned cross-domain integration through a W-shaped architecture, termed W-Net. Specifically, a Dynamic Adaptive Filter Module (DAF-Module) is proposed to address the challenges of frequently changing driving environments by extracting vital regions. It suppresses the indiscriminately recorded dynamics and highlights crucial ones by innovative joint frequency-spatial analysis, enhancing the model's ability to parse complex dynamics. Additionally, to track driver states during non-fixed facial poses, we propose a Global Context Sharing Module (GCS-Module) to construct refined feature representations by capturing hierarchical features that adapt to various scales of head and eye movements. Finally, W-Net achieves systematic cross-view information integration through its "Encoding-Independent Partial Decoding-Fusion Decoding" structure, addressing semantic misalignment in heterogeneous data integration. Experiments demonstrate that the proposed method robustly and accurately estimates the mapping of driver attention in scene on large public datasets.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2408.08568】Unsupervised Non-Rigid Point Cloud Matching through Large Vision Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08568">https://arxiv.org/abs/2408.08568</a></p>
  <p><b>作者</b>：Zhangquan Chen,Puhua Jiang,Ruqi Huang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：trained purely, correspondence annotation, extended naturally, point cloud matching, point cloud</p>
  <p><b>备注</b>： 12 pages, 4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we propose a novel learning-based framework for non-rigid point cloud matching, which can be trained purely on point clouds without any correspondence annotation but also be extended naturally to partial-to-full matching. Our key insight is to incorporate semantic features derived from large vision models (LVMs) to geometry-based shape feature learning. Our framework effectively leverages the structural information contained in the semantic features to address ambiguities arise from self-similarities among local geometries. Furthermore, our framework also enjoys the strong generalizability and robustness regarding partial observations of LVMs, leading to improvements in the regarding point cloud matching tasks. In order to achieve the above, we propose a pixel-to-point feature aggregation module, a local and global attention network as well as a geometrical similarity loss function. Experimental results show that our method achieves state-of-the-art results in matching non-rigid point clouds in both near-isometric and heterogeneous shape collection as well as more realistic partial and noisy data.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2408.08567】S$^3$Attention: Improving Long Sequence Attention with Smoothed Skeleton Sketching</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08567">https://arxiv.org/abs/2408.08567</a></p>
  <p><b>作者</b>：Xue Wang,Tian Zhou,Jianqing Zhu,Jialin Liu,Kun Yuan,Tao Yao,Wotao Yin,Rong Jin,HanQin Cai</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：Attention based models, Attention, based Attention structure, Attention based, vanilla Attention based</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Attention based models have achieved many remarkable breakthroughs in numerous applications. However, the quadratic complexity of Attention makes the vanilla Attention based models hard to apply to long sequence tasks. Various improved Attention structures are proposed to reduce the computation cost by inducing low rankness and approximating the whole sequence by sub-sequences. The most challenging part of those approaches is maintaining the proper balance between information preservation and computation reduction: the longer sub-sequences used, the better information is preserved, but at the price of introducing more noise and computational costs. In this paper, we propose a smoothed skeleton sketching based Attention structure, coined S$^3$Attention, which significantly improves upon the previous attempts to negotiate this trade-off. S$^3$Attention has two mechanisms to effectively minimize the impact of noise while keeping the linear complexity to the sequence length: a smoothing block to mix information over long sequences and a matrix sketching method that simultaneously selects columns and rows from the input matrix. We verify the effectiveness of S$^3$Attention both theoretically and empirically. Extensive studies over Long Range Arena (LRA) datasets and six time-series forecasting show that S$^3$Attention significantly outperforms both vanilla Attention and other state-of-the-art variants of Attention structures.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2408.08561】A New Chinese Landscape Paintings Generation Model based on Stable Diffusion using DreamBooth</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08561">https://arxiv.org/abs/2408.08561</a></p>
  <p><b>作者</b>：Yujia Gu,Xinyu Fang,Xueyuan Deng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Stable Diffusion Model, Chinese Landscape Paintings, Stable Diffusion, generating Chinese Landscape, Landscape Paintings</p>
  <p><b>备注</b>： accepted by AHPCAI</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study mainly introduces a method combining the Stable Diffusion Model (SDM) and Parameter-Efficient Fine-Tuning method for generating Chinese Landscape Paintings. This training process is accelerated by combining LoRA with pre-trained SDM and DreamBooth with pre-trained SDM, respectively. On the Chinese Landscape Paintings Internet dataset used in this paper, this study finds that SDM combined with DreamBooth exhibits superior performance, outperforming other models, including the generic pre-trained SDM and LoRA-based fine-tuning SDM. The SDM combined with DreamBooth achieves a FID of 12.75 on the dataset and outperforms all other models in terms of expert evaluation, highlighting the model's versatility in the field of Chinese Landscape Paintings given the unique identifier, high fidelity and high quality. This study illustrates the potential of specialised fine-tuning method to improve the performance of SDM on domain-specific tasks, particularly in the domain of Landscape Paintings.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2408.08560】A training regime to learn unified representations from complementary breast imaging modalities</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08560">https://arxiv.org/abs/2408.08560</a></p>
  <p><b>作者</b>：Umang Sharma,Jungkyu Park,Laura Heacock,Sumit Chopra,Krzysztof Geras</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Full Field Digital, Field Digital Mammograms, Digital Breast Tomosynthesis, Digital Mammograms, Full Field</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Full Field Digital Mammograms (FFDMs) and Digital Breast Tomosynthesis (DBT) are the two most widely used imaging modalities for breast cancer screening. Although DBT has increased cancer detection compared to FFDM, its widespread adoption in clinical practice has been slowed by increased interpretation times and a perceived decrease in the conspicuity of specific lesion types. Specifically, the non-inferiority of DBT for microcalcifications remains under debate. Due to concerns about the decrease in visual acuity, combined DBT-FFDM acquisitions remain popular, leading to overall increased exam times and radiation dosage. Enabling DBT to provide diagnostic information present in both FFDM and DBT would reduce reliance on FFDM, resulting in a reduction in both quantities. We propose a machine learning methodology that learns high-level representations leveraging the complementary diagnostic signal from both DBT and FFDM. Experiments on a large-scale data set validate our claims and show that our representations enable more accurate breast lesion detection than any DBT- or FFDM-based model.</p>
  </details>
</details>
<details>
  <summary>40. <b>【2408.08555】Detection and tracking of MAVs using a LiDAR with rosette scanning pattern</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08555">https://arxiv.org/abs/2408.08555</a></p>
  <p><b>作者</b>：Sándor Gazdag,Tom Möller,Tamás Filep,Anita Keszler,András L. Majdik</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：commercial Micro Aerial, usage of commercial, increased drastically, Micro Aerial Vehicles, Micro Aerial</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The usage of commercial Micro Aerial Vehicles (MAVs) has increased drastically during the last decade. While the added value of MAVs to society is apparent, their growing use is also coming with increasing risks like violating public airspace at airports or committing privacy violations. To mitigate these issues it is becoming critical to develop solutions that incorporate the detection and tracking of MAVs with autonomous systems. This work presents a method for the detection and tracking of MAVs using a novel, low-cost rosette scanning LiDAR on a pan-tilt turret. Once the static background is captured, a particle filter is utilized to detect a possible target and track its position with a physical, programmable pan-tilt system. The tracking makes it possible to keep the MAV in the center, maximizing the density of 3D points measured on the target by the LiDAR sensor. The developed algorithm was evaluated within the indoor MIcro aerial vehicle and MOtion capture (MIMO) arena and has state-of-the-art tracking accuracy, stability, and fast re-detection time in case of tracking loss. Based on the outdoor tests, it was possible to significantly increase the detection distance and number of returned points compared to other similar methods using LiDAR.</p>
  </details>
</details>
<details>
  <summary>41. <b>【2408.08544】Scaling up Multimodal Pre-training for Sign Language Understanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08544">https://arxiv.org/abs/2408.08544</a></p>
  <p><b>作者</b>：Wengang Zhou,Weichao Zhao,Hezhen Hu,Zecheng Li,Houqiang Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：Sign language, Sign language serves, Sign, language, sign language understanding</p>
  <p><b>备注</b>： Sign language recognition; Sign language translation; Sign language retrieval</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Sign language serves as the primary meaning of communication for the deaf-mute community. Different from spoken language, it commonly conveys information by the collaboration of manual features, i.e., hand gestures and body movements, and non-manual features, i.e., facial expressions and mouth cues. To facilitate communication between the deaf-mute and hearing people, a series of sign language understanding (SLU) tasks have been studied in recent years, including isolated/continuous sign language recognition (ISLR/CSLR), gloss-free sign language translation (GF-SLT) and sign language retrieval (SL-RT). Sign language recognition and translation aims to understand the semantic meaning conveyed by sign languages from gloss-level and sentence-level, respectively. In contrast, SL-RT focuses on retrieving sign videos or corresponding texts from a closed-set under the query-by-example search paradigm. These tasks investigate sign language topics from diverse perspectives and raise challenges in learning effective representation of sign language videos. To advance the development of sign language understanding, exploring a generalized model that is applicable across various SLU tasks is a profound research direction.</p>
  </details>
</details>
<details>
  <summary>42. <b>【2408.08543】Language-Driven Interactive Shadow Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08543">https://arxiv.org/abs/2408.08543</a></p>
  <p><b>作者</b>：Hongqiu Wang,Wei Wang,Haipeng Zhou,Huihui Xu,Shaozhi Wu,Lei Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Traditional shadow detectors, Traditional shadow, RVSD, natural language prompts, detectors often identify</p>
  <p><b>备注</b>： ACM MM 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Traditional shadow detectors often identify all shadow regions of static images or video sequences. This work presents the Referring Video Shadow Detection (RVSD), which is an innovative task that rejuvenates the classic paradigm by facilitating the segmentation of particular shadows in videos based on descriptive natural language prompts. This novel RVSD not only achieves segmentation of arbitrary shadow areas of interest based on descriptions (flexibility) but also allows users to interact with visual content more directly and naturally by using natural language prompts (interactivity), paving the way for abundant applications ranging from advanced video editing to virtual reality experiences. To pioneer the RVSD research, we curated a well-annotated RVSD dataset, which encompasses 86 videos and a rich set of 15,011 paired textual descriptions with corresponding shadows. To the best of our knowledge, this dataset is the first one for addressing RVSD. Based on this dataset, we propose a Referring Shadow-Track Memory Network (RSM-Net) for addressing the RVSD task. In our RSM-Net, we devise a Twin-Track Synergistic Memory (TSM) to store intra-clip memory features and hierarchical inter-clip memory features, and then pass these memory features into a memory read module to refine features of the current video frame for referring shadow detection. We also develop a Mixed-Prior Shadow Attention (MSA) to utilize physical priors to obtain a coarse shadow map for learning more visual features by weighting it with the input video frame. Experimental results show that our RSM-Net achieves state-of-the-art performance for RVSD with a notable Overall IOU increase of 4.4\%. Our code and dataset are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>43. <b>【2408.08529】Privacy-Preserving Vision Transformer Using Images Encrypted with Restricted Random Permutation Matrices</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08529">https://arxiv.org/abs/2408.08529</a></p>
  <p><b>作者</b>：Kouki Horio,Kiyoshi Nishikawa,Hitoshi Kiya</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：fine-tuning vision transformers, privacy-preserving fine-tuning vision, vision transformers, privacy-preserving fine-tuning, fine-tuning vision</p>
  <p><b>备注</b>： 4 pages, 9 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose a novel method for privacy-preserving fine-tuning vision transformers (ViTs) with encrypted images. Conventional methods using encrypted images degrade model performance compared with that of using plain images due to the influence of image encryption. In contrast, the proposed encryption method using restricted random permutation matrices can provide a higher performance than the conventional ones.</p>
  </details>
</details>
<details>
  <summary>44. <b>【2408.08527】Focus on Focus: Focus-oriented Representation Learning and Multi-view Cross-modal Alignment for Glioma Grading</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08527">https://arxiv.org/abs/2408.08527</a></p>
  <p><b>作者</b>：Li Pan,Yupei Zhang,Qiushi Yang,Tan Li,Xiaohan Xing,Maximus C. F. Yeung,Zhen Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：achieved a promising, multimodal deep learning, Recently, molecular biomarkers, integrates histopathology slides</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, multimodal deep learning, which integrates histopathology slides and molecular biomarkers, has achieved a promising performance in glioma grading. Despite great progress, due to the intra-modality complexity and inter-modality heterogeneity, existing studies suffer from inadequate histopathology representation learning and inefficient molecular-pathology knowledge alignment. These two issues hinder existing methods to precisely interpret diagnostic molecular-pathology features, thereby limiting their grading performance. Moreover, the real-world applicability of existing multimodal approaches is significantly restricted as molecular biomarkers are not always available during clinical deployment. To address these problems, we introduce a novel Focus on Focus (FoF) framework with paired pathology-genomic training and applicable pathology-only inference, enhancing molecular-pathology representation effectively. Specifically, we propose a Focus-oriented Representation Learning (FRL) module to encourage the model to identify regions positively or negatively related to glioma grading and guide it to focus on the diagnostic areas with a consistency constraint. To effectively link the molecular biomarkers to morphological features, we propose a Multi-view Cross-modal Alignment (MCA) module that projects histopathology representations into molecular subspaces, aligning morphological features with corresponding molecular biomarker status by supervised contrastive learning. Experiments on the TCGA GBM-LGG dataset demonstrate that our FoF framework significantly improves the glioma grading. Remarkably, our FoF achieves superior performance using only histopathology slides compared to existing multimodal methods. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>45. <b>【2408.08524】GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion Prior and Parametric Light Source Optimization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08524">https://arxiv.org/abs/2408.08524</a></p>
  <p><b>作者</b>：Kang Du,Zhihao Liang,Zeyu Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：intuitive light editing, Gaussian Splatting, view synthesis, synthesis and intuitive, illumination decomposition</p>
  <p><b>备注</b>： 15 pages, 13 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present GS-ID, a novel framework for illumination decomposition on Gaussian Splatting, achieving photorealistic novel view synthesis and intuitive light editing. Illumination decomposition is an ill-posed problem facing three main challenges: 1) priors for geometry and material are often lacking; 2) complex illumination conditions involve multiple unknown light sources; and 3) calculating surface shading with numerous light sources is computationally expensive. To address these challenges, we first introduce intrinsic diffusion priors to estimate the attributes for physically based rendering. Then we divide the illumination into environmental and direct components for joint optimization. Last, we employ deferred rendering to reduce the computational load. Our framework uses a learnable environment map and Spherical Gaussians (SGs) to represent light sources parametrically, therefore enabling controllable and photorealistic relighting on Gaussian Splatting. Extensive experiments and applications demonstrate that GS-ID produces state-of-the-art illumination decomposition results while achieving better geometry reconstruction and rendering performance.</p>
  </details>
</details>
<details>
  <summary>46. <b>【2408.08518】Visual-Friendly Concept Protection via Selective Adversarial Perturbations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08518">https://arxiv.org/abs/2408.08518</a></p>
  <p><b>作者</b>：Xiaoyue Mi,Fan Tang,Juan Cao,Peng Li,Yang Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Personalized concept generation, tuning diffusion models, raises potential legal, images raises potential, Personalized concept</p>
  <p><b>备注</b>： Under Review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Personalized concept generation by tuning diffusion models with a few images raises potential legal and ethical concerns regarding privacy and intellectual property rights. Researchers attempt to prevent malicious personalization using adversarial perturbations. However, previous efforts have mainly focused on the effectiveness of protection while neglecting the visibility of perturbations. They utilize global adversarial perturbations, which introduce noticeable alterations to original images and significantly degrade visual quality. In this work, we propose the Visual-Friendly Concept Protection (VCPro) framework, which prioritizes the protection of key concepts chosen by the image owner through adversarial perturbations with lower perceptibility. To ensure these perturbations are as inconspicuous as possible, we introduce a relaxed optimization objective to identify the least perceptible yet effective adversarial perturbations, solved using the Lagrangian multiplier method. Qualitative and quantitative experiments validate that VCPro achieves a better trade-off between the visibility of perturbations and protection effectiveness, effectively prioritizing the protection of target concepts in images with less perceptible perturbations.</p>
  </details>
</details>
<details>
  <summary>47. <b>【2408.08502】Efficient Image-to-Image Diffusion Classifier for Adversarial Robustness</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08502">https://arxiv.org/abs/2408.08502</a></p>
  <p><b>作者</b>：Hefei Mei,Minjing Dong,Chang Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：superior defense capability, demonstrated great potential, achieve superior defense, superior defense, defense capability</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models (DMs) have demonstrated great potential in the field of adversarial robustness, where DM-based defense methods can achieve superior defense capability without adversarial training. However, they all require huge computational costs due to the usage of large-scale pre-trained DMs, making it difficult to conduct full evaluation under strong attacks and compare with traditional CNN-based methods. Simply reducing the network size and timesteps in DMs could significantly harm the image generation quality, which invalidates previous frameworks. To alleviate this issue, we redesign the diffusion framework from generating high-quality images to predicting distinguishable image labels. Specifically, we employ an image translation framework to learn many-to-one mapping from input samples to designed orthogonal image labels. Based on this framework, we introduce an efficient Image-to-Image diffusion classifier with a pruned U-Net structure and reduced diffusion timesteps. Besides the framework, we redesign the optimization objective of DMs to fit the target of image classification, where a new classification loss is incorporated in the DM-based image translation framework to distinguish the generated label from those of other classes. We conduct sufficient evaluations of the proposed classifier under various attacks on popular benchmarks. Extensive experiments show that our method achieves better adversarial robustness with fewer computational costs than DM-based and CNN-based methods. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>48. <b>【2408.08500】CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08500">https://arxiv.org/abs/2408.08500</a></p>
  <p><b>作者</b>：Shihan Peng,Hanyu Zhou,Hao Dong,Zhiwei Shi,Haoyue Liu,Yuxing Duan,Yi Chang,Luxin Yan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Conventional frame camera, Conventional frame, driving scene perception, multimodal fusion, multimodal</p>
  <p><b>备注</b>： This work has been submitted to the IEEE for possible publication</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Conventional frame camera is the mainstream sensor of the autonomous driving scene perception, while it is limited in adverse conditions, such as low light. Event camera with high dynamic range has been applied in assisting frame camera for the multimodal fusion, which relies heavily on the pixel-level spatial alignment between various modalities. Typically, existing multimodal datasets mainly place event and frame cameras in parallel and directly align them spatially via warping operation. However, this parallel strategy is less effective for multimodal fusion, since the large disparity exacerbates spatial misalignment due to the large event-frame baseline. We argue that baseline minimization can reduce alignment error between event and frame cameras. In this work, we introduce hybrid coaxial event-frame devices to build the multimodal system, and propose a coaxial stereo event camera (CoSEC) dataset for autonomous driving. As for the multimodal system, we first utilize the microcontroller to achieve time synchronization, and then spatially calibrate different sensors, where we perform intra- and inter-calibration of stereo coaxial devices. As for the multimodal dataset, we filter LiDAR point clouds to generate depth and optical flow labels using reference depth, which is further improved by fusing aligned event and frame data in nighttime conditions. With the help of the coaxial device, the proposed dataset can promote the all-day pixel-level multimodal fusion. Moreover, we also conduct experiments to demonstrate that the proposed dataset can improve the performance and generalization of the multimodal fusion.</p>
  </details>
</details>
<details>
  <summary>49. <b>【2408.08495】Achieving Complex Image Edits via Function Aggregation with Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08495">https://arxiv.org/abs/2408.08495</a></p>
  <p><b>作者</b>：Mohammadreza Samadi,Fred X. Han,Mohammad Salameh,Hao Wu,Fengyu Sun,Chunhua Zhou,Di Niu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：demonstrated strong performance, making them ideal, demonstrated strong, strong performance, performance in generative</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have demonstrated strong performance in generative tasks, making them ideal candidates for image editing. Recent studies highlight their ability to apply desired edits effectively by following textual instructions, yet two key challenges persist. First, these models struggle to apply multiple edits simultaneously, resulting in computational inefficiencies due to their reliance on sequential processing. Second, relying on textual prompts to determine the editing region can lead to unintended alterations in other parts of the image. In this work, we introduce FunEditor, an efficient diffusion model designed to learn atomic editing functions and perform complex edits by aggregating simpler functions. This approach enables complex editing tasks, such as object movement, by aggregating multiple functions and applying them simultaneously to specific areas. FunEditor is 5 to 24 times faster inference than existing methods on complex tasks like object movement. Our experiments demonstrate that FunEditor significantly outperforms recent baselines, including both inference-time optimization methods and fine-tuned models, across various metrics, such as image quality assessment (IQA) and object-background consistency.</p>
  </details>
</details>
<details>
  <summary>50. <b>【2408.08461】EXTOC: Text-driven Object-Centric Style Transfer</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08461">https://arxiv.org/abs/2408.08461</a></p>
  <p><b>作者</b>：Jihun Park,Jongmin Gim,Kyoungmin Lee,Seunghun Lee,Sunghoon Im</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：present Text-driven Object-Centric, Text-driven Object-Centric Style, present Text-driven, Text-driven Object-Centric, guides style transfer</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present Text-driven Object-Centric Style Transfer (TEXTOC), a novel method that guides style transfer at an object-centric level using textual inputs. The core of TEXTOC is our Patch-wise Co-Directional (PCD) loss, meticulously designed for precise object-centric transformations that are closely aligned with the input text. This loss combines a patch directional loss for text-guided style direction and a patch distribution consistency loss for even CLIP embedding distribution across object regions. It ensures a seamless and harmonious style transfer across object regions. Key to our method are the Text-Matched Patch Selection (TMPS) and Pre-fixed Region Selection (PRS) modules for identifying object locations via text, eliminating the need for segmentation masks. Lastly, we introduce an Adaptive Background Preservation (ABP) loss to maintain the original style and structural essence of the image's background. This loss is applied to dynamically identified background areas. Extensive experiments underline the effectiveness of our approach in creating visually coherent and textually aligned style transfers.</p>
  </details>
</details>
<details>
  <summary>51. <b>【2408.08459】JPEG-LM: LLMs as Image Generators with Canonical Codec Representations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08459">https://arxiv.org/abs/2408.08459</a></p>
  <p><b>作者</b>：Xiaochuang Han,Marjan Ghazvininejad,Pang Wei Koh,Yulia Tsvetkov</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：potentially easy integration, Recent work, generality and potentially, potentially easy, easy integration</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent work in image and video generation has been adopting the autoregressive LLM architecture due to its generality and potentially easy integration into multi-modal systems. The crux of applying autoregressive training in language generation to visual generation is discretization -- representing continuous data like images and videos as discrete tokens. Common methods of discretizing images and videos include modeling raw pixel values, which are prohibitively lengthy, or vector quantization, which requires convoluted pre-hoc training. In this work, we propose to directly model images and videos as compressed files saved on computers via canonical codecs (e.g., JPEG, AVC/H.264). Using the default Llama architecture without any vision-specific modifications, we pretrain JPEG-LM from scratch to generate images (and AVC-LM to generate videos as a proof of concept), by directly outputting compressed file bytes in JPEG and AVC formats. Evaluation of image generation shows that this simple and straightforward approach is more effective than pixel-based modeling and sophisticated vector quantization baselines (on which our method yields a 31% reduction in FID). Our analysis shows that JPEG-LM has an especial advantage over vector quantization models in generating long-tail visual elements. Overall, we show that using canonical codec representations can help lower the barriers between language generation and visual generation, facilitating future research on multi-modal language/image/video LLMs.</p>
  </details>
</details>
<details>
  <summary>52. <b>【2408.08454】Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08454">https://arxiv.org/abs/2408.08454</a></p>
  <p><b>作者</b>：Zohaib Khan,Muhammad Khaquan,Omer Tafveez,Agha Ali Raza</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：revolutionized deep learning, effectively captures contextual, captures contextual information, GQA, architecture has revolutionized</p>
  <p><b>备注</b>： 11 pages, 9 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The Transformer architecture has revolutionized deep learning through its Self-Attention mechanism, which effectively captures contextual information. However, the memory footprint of Self-Attention presents significant challenges for long-sequence tasks. Grouped Query Attention (GQA) addresses this issue by grouping queries and mean-pooling the corresponding key-value heads - reducing the number of overall parameters and memory requirements in a flexible manner without adversely compromising model accuracy. In this work, we introduce enhancements to GQA, focusing on two novel approaches that deviate from the static nature of grouping: Key-Distributed GQA (KDGQA) and Dynamic Key-Distributed GQA (DGQA), which leverage information from the norms of the key heads to inform query allocation. Specifically, KDGQA looks at the ratios of the norms of the key heads during each forward pass, while DGQA examines the ratios of the norms as they evolve through training. Additionally, we present Perturbed GQA (PGQA) as a case-study, which introduces variability in (static) group formation via subtracting noise from the attention maps. Our experiments with up-trained Vision Transformers, for Image Classification on datasets such as CIFAR-10, CIFAR-100, Food101, and Tiny ImageNet, demonstrate the promise of these variants in improving upon the original GQA through more informed and adaptive grouping mechanisms: specifically ViT-L experiences accuracy gains of up to 8% when utilizing DGQA in comparison to GQA and other variants. We further analyze the impact of the number of Key-Value Heads on performance, underscoring the importance of utilizing query-key affinities.</p>
  </details>
</details>
<details>
  <summary>53. <b>【2408.08447】SpectralEarth: Training Hyperspectral Foundation Models at Scale</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08447">https://arxiv.org/abs/2408.08447</a></p>
  <p><b>作者</b>：Nassim Ait Ali Braham,Conrad M Albrecht,Julien Mairal,Jocelyn Chanussot,Yi Wang,Xiao Xiang Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：remote sensing, multispectral imagery, triggered a paradigm, paradigm shift, shift in computer</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Foundation models have triggered a paradigm shift in computer vision and are increasingly being adopted in remote sensing, particularly for multispectral imagery. Yet, their potential in hyperspectral imaging (HSI) remains untapped due to the absence of comprehensive and globally representative hyperspectral datasets. To close this gap, we introduce SpectralEarth, a large-scale multi-temporal dataset designed to pretrain hyperspectral foundation models leveraging data from the Environmental Mapping and Analysis Program (EnMAP). SpectralEarth comprises 538,974 image patches covering 415,153 unique locations from more than 11,636 globally distributed EnMAP scenes spanning two years of archive. Additionally, 17.5% of these locations include multiple timestamps, enabling multi-temporal HSI analysis. Utilizing state-of-the-art self-supervised learning (SSL) algorithms, we pretrain a series of foundation models on SpectralEarth. We integrate a spectral adapter into classical vision backbones to accommodate the unique characteristics of HSI. In tandem, we construct four downstream datasets for land-cover and crop-type mapping, providing benchmarks for model evaluation. Experimental results support the versatility of our models, showcasing their generalizability across different tasks and sensors. We also highlight computational efficiency during model fine-tuning. The dataset, models, and source code will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>54. <b>【2408.08437】PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08437">https://arxiv.org/abs/2408.08437</a></p>
  <p><b>作者</b>：Kshitij Bhardwaj</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：replacing convolutional neural, convolutional neural networks, computer vision tasks, optimize vision transformers, Vision Transformers</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:While Vision Transformers (ViTs) are extremely effective at computer vision tasks and are replacing convolutional neural networks as the new state-of-the-art, they are complex and memory-intensive models. In order to effectively run these models on resource-constrained mobile/edge systems, there is a need to not only compress these models but also to optimize them and convert them into deployment-friendly formats. To this end, this paper presents a combined pruning and quantization tool, called PQV-Mobile, to optimize vision transformers for mobile applications. The tool is able to support different types of structured pruning based on magnitude importance, Taylor importance, and Hessian importance. It also supports quantization from FP32 to FP16 and int8, targeting different mobile hardware backends. We demonstrate the capabilities of our tool and show important latency-memory-accuracy trade-offs for different amounts of pruning and int8 quantization with Facebook Data Efficient Image Transformer (DeiT) models. Our results show that even pruning a DeiT model by 9.375% and quantizing it to int8 from FP32 followed by optimizing for mobile applications, we find a latency reduction by 7.18X with a small accuracy loss of 2.24%. The tool is open source.</p>
  </details>
</details>
<details>
  <summary>55. <b>【2408.08412】Penny-Wise and Pound-Foolish in Deepfake Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08412">https://arxiv.org/abs/2408.08412</a></p>
  <p><b>作者</b>：Yabin Wang,Zhiwu Huang,Su Zhou,Adam Prugel-Bennett,Xiaopeng Hong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：deepfake detection, deepfake, prompting the urgent, technologies has sparked, sparked serious concerns</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The diffusion of deepfake technologies has sparked serious concerns about its potential misuse across various domains, prompting the urgent need for robust detection methods. Despite advancement, many current approaches prioritize short-term gains at expense of long-term effectiveness. This paper critiques the overly specialized approach of fine-tuning pre-trained models solely with a penny-wise objective on a single deepfake dataset, while disregarding the pound-wise balance for generalization and knowledge retention. To address this "Penny-Wise and Pound-Foolish" issue, we propose a novel learning framework (PoundNet) for generalization of deepfake detection on a pre-trained vision-language model. PoundNet incorporates a learnable prompt design and a balanced objective to preserve broad knowledge from upstream tasks (object classification) while enhancing generalization for downstream tasks (deepfake detection). We train PoundNet on a standard single deepfake dataset, following common practice in the literature. We then evaluate its performance across 10 public large-scale deepfake datasets with 5 main evaluation metrics-forming the largest benchmark test set for assessing the generalization ability of deepfake detection models, to our knowledge. The comprehensive benchmark evaluation demonstrates the proposed PoundNet is significantly less "Penny-Wise and Pound-Foolish", achieving a remarkable improvement of 19% in deepfake detection performance compared to state-of-the-art methods, while maintaining a strong performance of 63% on object classification tasks, where other deepfake detection models tend to be ineffective. Code and data are open-sourced at this https URL.</p>
  </details>
</details>
<details>
  <summary>56. <b>【2408.08396】Level Up Your Tutorials: VLMs for Game Tutorials Quality Assessment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08396">https://arxiv.org/abs/2408.08396</a></p>
  <p><b>作者</b>：Daniele Rege Cambrin,Gabriele Scaffidi Militone,Luca Colomba,Giovanni Malnati,Daniele Apiletti,Paolo Garza</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：complex core mechanics, Designing effective game, smooth learning curve, Designing effective, core mechanics</p>
  <p><b>备注</b>： Accepted at ECCV 2024 CV2 Workshop</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Designing effective game tutorials is crucial for a smooth learning curve for new players, especially in games with many rules and complex core mechanics. Evaluating the effectiveness of these tutorials usually requires multiple iterations with testers who have no prior knowledge of the game. Recent Vision-Language Models (VLMs) have demonstrated significant capabilities in understanding and interpreting visual content. VLMs can analyze images, provide detailed insights, and answer questions about their content. They can recognize objects, actions, and contexts in visual data, making them valuable tools for various applications, including automated game testing. In this work, we propose an automated game-testing solution to evaluate the quality of game tutorials. Our approach leverages VLMs to analyze frames from video game tutorials, answer relevant questions to simulate human perception, and provide feedback. This feedback is compared with expected results to identify confusing or problematic scenes and highlight potential errors for developers. In addition, we publish complete tutorial videos and annotated frames from different game versions used in our tests. This solution reduces the need for extensive manual testing, especially by speeding up and simplifying the initial development stages of the tutorial to improve the final game experience.</p>
  </details>
</details>
<details>
  <summary>57. <b>【2408.08381】Pre-processing and Compression: Understanding Hidden Representation Refinement Across Imaging Domains via Intrinsic Dimension</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08381">https://arxiv.org/abs/2408.08381</a></p>
  <p><b>作者</b>：Nicholas Konz,Maciej A. Mazurowski</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：geometric properties, medical image models, recent years, generalization ability, important model behavior</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In recent years, there has been interest in how geometric properties such as intrinsic dimension (ID) of a neural network's hidden representations evolve through its layers, and how such properties are predictive of important model behavior such as generalization ability. However, evidence has begun to emerge that such behavior can change significantly depending on the domain of the network's training data, such as natural versus medical images. Here, we further this inquiry by exploring how the ID of a network's learned representations evolves through its layers, in essence, characterizing how the network successively refines the information content of input data to be used for predictions. Analyzing eleven natural and medical image datasets across six network architectures, we find that the shape of this ID evolution curve differs noticeably between natural and medical image models: medical image models peak in representation ID earlier in the network, implying a difference in the image features and their abstractness that are typically used for downstream tasks in these domains. Additionally, we discover a strong correlation of this peak representation ID with the ID of the data in its input space, implying that the intrinsic information content of a model's learned representations is guided by that of the data it was trained on. Overall, our findings emphasize notable discrepancies in network behavior between natural and non-natural imaging domains regarding hidden representation information content, and provide further insights into how a network's learned features are shaped by its training data.</p>
  </details>
</details>
<details>
  <summary>58. <b>【2408.08345】5%100%: Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08345">https://arxiv.org/abs/2408.08345</a></p>
  <p><b>作者</b>：Dongshuo Yin,Leiyi Hu,Bin Li,Youqun Zhang,Xue Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Pre-training fine-tuning, full fine-tuning, fine-tuning, transferring efficiency, visual</p>
  <p><b>备注</b>： arXiv admin note: substantial text overlap with [arXiv:2311.15010](https://arxiv.org/abs/2311.15010) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Pre-training  fine-tuning can enhance the transferring efficiency and performance in visual tasks. Recent delta-tuning methods provide more options for visual classification tasks. Despite their success, existing visual delta-tuning art fails to exceed the upper limit of full fine-tuning on challenging tasks like object detection and segmentation. To find a competitive alternative to full fine-tuning, we propose the Multi-cognitive Visual Adapter (Mona) tuning, a novel adapter-based tuning method. First, we introduce multiple vision-friendly filters into the adapter to enhance its ability to process visual signals, while previous methods mainly rely on language-friendly linear filters. Second, we add the scaled normalization layer in the adapter to regulate the distribution of input features for visual filters. To fully demonstrate the practicality and generality of Mona, we conduct experiments on multiple representative visual tasks, including instance segmentation on COCO, semantic segmentation on ADE20K, object detection on Pascal VOC, oriented object detection on DOTA/STAR, and image classification on three common datasets. Exciting results illustrate that Mona surpasses full fine-tuning on all these tasks, and is the only delta-tuning method outperforming full fine-tuning on the above various tasks. For example, Mona achieves 1% performance gain on the COCO dataset compared to full fine-tuning. Comprehensive results suggest that Mona-tuning is more suitable for retaining and utilizing the capabilities of pre-trained models than full fine-tuning. We will make the code publicly available.</p>
  </details>
</details>
<details>
  <summary>59. <b>【2408.08342】CT4D: Consistent Text-to-4D Generation with Animatable Meshes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08342">https://arxiv.org/abs/2408.08342</a></p>
  <p><b>作者</b>：Ce Chen,Shaoli Huang,Xuelin Chen,Guangyi Chen,Xiaoguang Han,Kun Zhang,Mingming Gong</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：image diffusion model, video diffusion model, diffusion model, image diffusion, video diffusion</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text-to-4D generation has recently been demonstrated viable by integrating a 2D image diffusion model with a video diffusion model. However, existing models tend to produce results with inconsistent motions and geometric structures over time. To this end, we present a novel framework, coined CT4D, which directly operates on animatable meshes for generating consistent 4D content from arbitrary user-supplied prompts. The primary challenges of our mesh-based framework involve stably generating a mesh with details that align with the text prompt while directly driving it and maintaining surface continuity. Our CT4D framework incorporates a unique Generate-Refine-Animate (GRA) algorithm to enhance the creation of text-aligned meshes. To improve surface continuity, we divide a mesh into several smaller regions and implement a uniform driving function within each area. Additionally, we constrain the animating stage with a rigidity regulation to ensure cross-region continuity. Our experimental results, both qualitative and quantitative, demonstrate that our CT4D framework surpasses existing text-to-4D techniques in maintaining interframe consistency and preserving global geometry. Furthermore, we showcase that this enhanced representation inherently possesses the capability for combinational 4D generation and texture editing.</p>
  </details>
</details>
<details>
  <summary>60. <b>【2408.08340】METR: Image Watermarking with Large Number of Unique Messages</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08340">https://arxiv.org/abs/2408.08340</a></p>
  <p><b>作者</b>：Alexander Varlamov,Daria Diatlova,Egor Spirin</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：led researchers, improving watermarking algorithms, focus on improving, METR, Diffusion Model</p>
  <p><b>备注</b>： 14 pages, 9 figures, code is available at [this https URL](https://github.com/deepvk/metr) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Improvements in diffusion models have boosted the quality of image generation, which has led researchers, companies, and creators to focus on improving watermarking algorithms. This provision would make it possible to clearly identify the creators of generative art. The main challenges that modern watermarking algorithms face have to do with their ability to withstand attacks and encrypt many unique messages, such as user IDs. In this paper, we present METR: Message Enhanced Tree-Ring, which is an approach that aims to address these challenges. METR is built on the Tree-Ring watermarking algorithm, a technique that makes it possible to encode multiple distinct messages without compromising attack resilience or image quality. This ensures the suitability of this watermarking algorithm for any Diffusion Model. In order to surpass the limitations on the quantity of encoded messages, we propose METR++, an enhanced version of METR. This approach, while limited to the Latent Diffusion Model architecture, is designed to inject a virtually unlimited number of unique messages. We demonstrate its robustness to attacks and ability to encrypt many unique messages while preserving image quality, which makes METR and METR++ hold great potential for practical applications in real-world settings. Our code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>61. <b>【2408.08336】Graph representations of 3D data for machine learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08336">https://arxiv.org/abs/2408.08336</a></p>
  <p><b>作者</b>：Tomasz Prytuła</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：machine learning algorithms, graphs and meshes, learning algorithms, give an overview, overview of combinatorial</p>
  <p><b>备注</b>： 14 pages, 11 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We give an overview of combinatorial methods to represent 3D data, such as graphs and meshes, from the viewpoint of their amenability to analysis using machine learning algorithms. We highlight pros and cons of various representations and we discuss some methods of generating/switching between the representations. We finally present two concrete applications in life science and industry. Despite its theoretical nature, our discussion is in general motivated by, and biased towards real-world challenges.</p>
  </details>
</details>
<details>
  <summary>62. <b>【2408.08332】urboEdit: Instant text-based image editing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08332">https://arxiv.org/abs/2408.08332</a></p>
  <p><b>作者</b>：Zongze Wu,Nicholas Kolkin,Jonathan Brandt,Richard Zhang,Eli Shechtman</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：precise image inversion, address the challenges, challenges of precise, image, input image</p>
  <p><b>备注</b>： Accepted to European Conference on Computer Vision (ECCV), 2024. Project page: [this https URL](https://betterze.github.io/TurboEdit/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We address the challenges of precise image inversion and disentangled image editing in the context of few-step diffusion models. We introduce an encoder based iterative inversion technique. The inversion network is conditioned on the input image and the reconstructed image from the previous step, allowing for correction of the next reconstruction towards the input image. We demonstrate that disentangled controls can be easily achieved in the few-step diffusion model by conditioning on an (automatically generated) detailed text prompt. To manipulate the inverted image, we freeze the noise maps and modify one attribute in the text prompt (either manually or via instruction based editing driven by an LLM), resulting in the generation of a new image similar to the input image with only one attribute changed. It can further control the editing strength and accept instructive text prompt. Our approach facilitates realistic text-guided image edits in real-time, requiring only 8 number of functional evaluations (NFEs) in inversion (one-time cost) and 4 NFEs per edit. Our method is not only fast, but also significantly outperforms state-of-the-art multi-step diffusion editing techniques.</p>
  </details>
</details>
<details>
  <summary>63. <b>【2408.08321】Can ChatGPT assist visually impaired people with micro-navigation?</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08321">https://arxiv.org/abs/2408.08321</a></p>
  <p><b>作者</b>：Junxian He,Shrinivas Pundlik,Gang Luo</p>
  <p><b>类目</b>：Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Micro-navigation poses challenges, poses challenges, challenges for blind, SPE, Objective</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Objective: Micro-navigation poses challenges for blind and visually impaired individuals. They often need to ask for sighted assistance. We explored the feasibility of utilizing ChatGPT as a virtual assistant to provide navigation directions. Methods: We created a test set of outdoor and indoor micro-navigation scenarios consisting of 113 scene images and their human-generated text descriptions. A total of 412 way-finding queries and their expected responses were compiled based on the scenarios. Not all queries are answerable based on the information available in the scene image. "I do not know"response was expected for unanswerable queries, which served as negative cases. High level orientation responses were expected, and step-by-step guidance was not required. ChatGPT 4o was evaluated based on sensitivity (SEN) and specificity (SPE) under different conditions. Results: The default ChatGPT 4o, with scene images as inputs, resulted in SEN and SPE values of 64.8% and 75.9%, respectively. Instruction on how to respond to unanswerable questions did not improve SEN substantially but SPE increased by around 14 percentage points. SEN and SPE both improved substantially, by about 17 and 16 percentage points on average respectively, when human written descriptions of the scenes were provided as input instead of images. Providing further prompt instructions to the assistants when the input was text description did not substantially change the SEN and SPE values. Conclusion: Current native ChatGPT 4o is still unable to provide correct micro-navigation guidance in some cases, probably because its scene understanding is not optimized for navigation purposes. If multi-modal chatbots could interpret scenes with a level of clarity comparable to humans, and also guided by appropriate prompts, they may have the potential to provide assistance to visually impaired for micro-navigation.</p>
  </details>
</details>
<details>
  <summary>64. <b>【2408.08315】Segment Anything for Videos: A Systematic Survey</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08315">https://arxiv.org/abs/2408.08315</a></p>
  <p><b>作者</b>：Chunhui Zhang,Yawen Cui,Weilin Lin,Guanjie Huang,Yan Rong,Li Liu,Shiguang Shan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：witnessed tremendous success, exploring task-agnostic visual, SAM, computer vision, task-agnostic visual foundation</p>
  <p><b>备注</b>：  [this https URL](https://github.com/983632847/SAM-for-Videos) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The recent wave of foundation models has witnessed tremendous success in computer vision (CV) and beyond, with the segment anything model (SAM) having sparked a passion for exploring task-agnostic visual foundation models. Empowered by its remarkable zero-shot generalization, SAM is currently challenging numerous traditional paradigms in CV, delivering extraordinary performance not only in various image segmentation and multi-modal segmentation (\eg, text-to-mask) tasks, but also in the video domain. Additionally, the latest released SAM 2 is once again sparking research enthusiasm in the realm of promptable visual segmentation for both images and videos. However, existing surveys mainly focus on SAM in various image processing tasks, a comprehensive and in-depth review in the video domain is notably absent. To address this gap, this work conducts a systematic review on SAM for videos in the era of foundation models. As the first to review the progress of SAM for videos, this work focuses on its applications to various tasks by discussing its recent advances, and innovation opportunities of developing foundation models on broad applications. We begin with a brief introduction to the background of SAM and video-related research domains. Subsequently, we present a systematic taxonomy that categorizes existing methods into three key areas: video understanding, video generation, and video editing, analyzing and summarizing their advantages and limitations. Furthermore, comparative results of SAM-based and current state-of-the-art methods on representative benchmarks, as well as insightful analysis are offered. Finally, we discuss the challenges faced by current research and envision several future research directions in the field of SAM for video and beyond.</p>
  </details>
</details>
<details>
  <summary>65. <b>【2408.08847】HistoGym: A Reinforcement Learning Environment for Histopathological Image Analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08847">https://arxiv.org/abs/2408.08847</a></p>
  <p><b>作者</b>：Zhi-Bo Liu,Xiaobo Pang,Jizhao Wang,Shuai Liu,Chen Li</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：decision-making process based, pathological research, critically important, pathological images, based on pathological</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In pathological research, education, and clinical practice, the decision-making process based on pathological images is critically important. This significance extends to digital pathology image analysis: its adequacy is demonstrated by the extensive information contained within tissue structures, which is essential for accurate cancer classification and grading. Additionally, its necessity is highlighted by the inherent requirement for interpretability in the conclusions generated by algorithms. For humans, determining tumor type and grade typically involves multi-scale analysis, which presents a significant challenge for AI algorithms. Traditional patch-based methods are inadequate for modeling such complex structures, as they fail to capture the intricate, multi-scale information inherent in whole slide images. Consequently, there is a pressing need for advanced AI techniques capable of efficiently and accurately replicating this complex analytical process. To address this issue, we introduce HistoGym, an open-source reinforcement learning environment for histopathological image analysis. Following OpenAI Gym APIs, HistoGym aims to foster whole slide image diagnosis by mimicking the real-life processes of doctors. Leveraging the pyramid feature of WSIs and the OpenSlide API, HistoGym provides a unified framework for various clinical tasks, including tumor detection and classification. We detail the observation, action, and reward specifications tailored for the histopathological image analysis domain and provide an open-source Python-based interface for both clinicians and researchers. To accommodate different clinical demands, we offer various scenarios for different organs and cancers, including both WSI-based and selected region-based scenarios, showcasing several noteworthy results.</p>
  </details>
</details>
<details>
  <summary>66. <b>【2408.08792】Assessing Generalization Capabilities of Malaria Diagnostic Models from Thin Blood Smears</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08792">https://arxiv.org/abs/2408.08792</a></p>
  <p><b>作者</b>：Louise Guillon,Soheib Biga,Axel Puyo,Grégoire Pasquier,Valentin Foucher,Yendoubé E. Kantchire,Stéphane E. Sossou,Ameyo M. Dorkenoo,Laurent Bonnardot,Marc Thellier,Laurence Lachaud,Renaud Piarroux</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)</p>
  <p><b>关键词</b>：global health challenge, accurate diagnostic methods, significant global health, health challenge, necessitating rapid</p>
  <p><b>备注</b>： MICCAI 2024 AMAI Workshop, Accepted for presentation, Submitted Manuscript Version, 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Malaria remains a significant global health challenge, necessitating rapid and accurate diagnostic methods. While computer-aided diagnosis (CAD) tools utilizing deep learning have shown promise, their generalization to diverse clinical settings remains poorly assessed. This study evaluates the generalization capabilities of a CAD model for malaria diagnosis from thin blood smear images across four sites. We explore strategies to enhance generalization, including fine-tuning and incremental learning. Our results demonstrate that incorporating site-specific data significantly improves model performance, paving the way for broader clinical application.</p>
  </details>
</details>
<details>
  <summary>67. <b>【2408.08790】A Disease-Specific Foundation Model Using Over 100K Fundus Images: Release and Validation for Abnormality and Multi-Disease Classification on Downstream Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08790">https://arxiv.org/abs/2408.08790</a></p>
  <p><b>作者</b>：Boa Jang,Youngbin Ahn,Eun Kyung Choe,Chang Ki Yoon,Hyuk Jin Choi,Young-Gon Kim</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Artificial intelligence applied, Artificial intelligence, offers significant potential, artificial intelligence models, generalized artificial intelligence</p>
  <p><b>备注</b>： 10 pages, 4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Artificial intelligence applied to retinal images offers significant potential for recognizing signs and symptoms of retinal conditions and expediting the diagnosis of eye diseases and systemic disorders. However, developing generalized artificial intelligence models for medical data often requires a large number of labeled images representing various disease signs, and most models are typically task-specific, focusing on major retinal diseases. In this study, we developed a Fundus-Specific Pretrained Model (Image+Fundus), a supervised artificial intelligence model trained to detect abnormalities in fundus images. A total of 57,803 images were used to develop this pretrained model, which achieved superior performance across various downstream tasks, indicating that our proposed model outperforms other general methods. Our Image+Fundus model offers a generalized approach to improve model performance while reducing the number of labeled datasets required. Additionally, it provides more disease-specific insights into fundus images, with visualizations generated by our model. These disease-specific foundation models are invaluable in enhancing the performance and efficiency of deep learning models in the field of fundus imaging.</p>
  </details>
</details>
<details>
  <summary>68. <b>【2408.08784】Multi-task Learning Approach for Intracranial Hemorrhage Prognosis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08784">https://arxiv.org/abs/2408.08784</a></p>
  <p><b>作者</b>：Miriam Cobo,Amaia Pérez del Barrio,Pablo Menéndez Fernández-Miranda,Pablo Sanz Bellón,Lara Lloret Iglesias,Wilson Silva</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：intracranial hemorrhage, complex interplay, interplay between imaging, imaging and tabular, Glasgow Coma Scale</p>
  <p><b>备注</b>： 16 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Prognosis after intracranial hemorrhage (ICH) is influenced by a complex interplay between imaging and tabular data. Rapid and reliable prognosis are crucial for effective patient stratification and informed treatment decision-making. In this study, we aim to enhance image-based prognosis by learning a robust feature representation shared between prognosis and the clinical and demographic variables most highly correlated with it. Our approach mimics clinical decision-making by reinforcing the model to learn valuable prognostic data embedded in the image. We propose a 3D multi-task image model to predict prognosis, Glasgow Coma Scale and age, improving accuracy and interpretability. Our method outperforms current state-of-the-art baseline image models, and demonstrates superior performance in ICH prognosis compared to four board-certified neuroradiologists using only CT scans as input. We further validate our model with interpretability saliency maps. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>69. <b>【2408.08747】MicroSSIM: Improved Structural Similarity for Comparing Microscopy Data</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08747">https://arxiv.org/abs/2408.08747</a></p>
  <p><b>作者</b>：Ashesh Ashesh,Joran Deschamps,Florian Jug</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：image biological structures, structures of interest, SSIM, biological structures, images</p>
  <p><b>备注</b>： Accepted at BIC workshop, ECCV 24</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Microscopy is routinely used to image biological structures of interest. Due to imaging constraints, acquired images are typically low-SNR and contain noise. Over the last few years, regression-based tasks like unsupervised denoising and splitting have found utility in working with such noisy micrographs. For evaluation, Structural Similarity (SSIM) is one of the most popular measures used in the field. For such tasks, the best evaluation would be when both low-SNR noisy images and corresponding high-SNR clean images are obtained directly from a microscope. However, due to the following three peculiar properties of the microscopy data, we observe that SSIM is not well suited to this data regime: (a) high-SNR micrographs have higher intensity pixels as compared to low SNR micrographs, (b) high-SNR micrographs have higher intensity pixels than found in natural images, images for which SSIM was developed, and (c) a digitally configurable offset is added by the detector present inside the microscope. We show that SSIM components behave unexpectedly when the prediction generated from low-SNR input is compared with the corresponding high-SNR data. We explain this behavior by introducing the phenomenon of saturation, where the value of SSIM components becomes less sensitive to (dis)similarity between the images. We introduce microSSIM, a variant of SSIM, which overcomes the above-discussed issues. We justify the soundness and utility of microSSIM using theoretical and empirical arguments and show the utility of microSSIM on two tasks: unsupervised denoising and joint image splitting with unsupervised denoising. Since our formulation can be applied to a broad family of SSIM-based measures, we also introduce MicroMS3IM, a microscopy-specific variation of MS-SSIM. The source code and python package is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>70. <b>【2408.08742】A lifted Bregman strategy for training unfolded proximal neural network Gaussian denoisers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08742">https://arxiv.org/abs/2408.08742</a></p>
  <p><b>作者</b>：Xiaoyu Wang,Martin Benning,Audrey Repetti</p>
  <p><b>类目</b>：Optimization and Control (math.OC); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：proximal neural networks, proximal optimization approaches, Unfolded proximal neural, form a family, combines deep learning</p>
  <p><b>备注</b>： 2024 IEEE International Workshop on Machine Learning for Signal Processing, Sept. 22--25, 2024, London, UK</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Unfolded proximal neural networks (PNNs) form a family of methods that combines deep learning and proximal optimization approaches. They consist in designing a neural network for a specific task by unrolling a proximal algorithm for a fixed number of iterations, where linearities can be learned from prior training procedure. PNNs have shown to be more robust than traditional deep learning approaches while reaching at least as good performances, in particular in computational imaging. However, training PNNs still depends on the efficiency of available training algorithms. In this work, we propose a lifted training formulation based on Bregman distances for unfolded PNNs. Leveraging the deterministic mini-batch block-coordinate forward-backward method, we design a bespoke computational strategy beyond traditional back-propagation methods for solving the resulting learning problem efficiently. We assess the behaviour of the proposed training approach for PNNs through numerical simulations on image denoising, considering a denoising PNN whose structure is based on dual proximal-gradient iterations.</p>
  </details>
</details>
<details>
  <summary>71. <b>【2408.08647】Modeling the Neonatal Brain Development Using Implicit Neural Representations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08647">https://arxiv.org/abs/2408.08647</a></p>
  <p><b>作者</b>：Florentin Bieder,Paul Friedrich,Hélène Corbaz,Alicia Durrer,Julia Wolleb,Philippe C. Cattin</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：brain undergoes rapid, undergoes rapid development, human brain undergoes, trimester of pregnancy, undergoes rapid</p>
  <p><b>备注</b>： Preprint, Accepted for PRIME MICCAI 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The human brain undergoes rapid development during the third trimester of pregnancy. In this work, we model the neonatal development of the infant brain in this age range. As a basis, we use MR images of preterm- and term-birth neonates from the developing human connectome project (dHCP). We propose a neural network, specifically an implicit neural representation (INR), to predict 2D- and 3D images of varying time points. In order to model a subject-specific development process, it is necessary to disentangle the age from the subjects' identity in the latent space of the INR. We propose two methods, Subject Specific Latent Vectors (SSL) and Stochastic Global Latent Augmentation (SGLA), enabling this disentanglement. We perform an analysis of the results and compare our proposed model to an age-conditioned denoising diffusion model as a baseline. We also show that our method can be applied in a memory-efficient way, which is especially important for 3D data.</p>
  </details>
</details>
<details>
  <summary>72. <b>【2408.08616】Reference-free Axial Super-resolution of 3D Microscopy Images using Implicit Neural Representation with a 2D Diffusion Prior</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08616">https://arxiv.org/abs/2408.08616</a></p>
  <p><b>作者</b>：Kyungryun Lee,Won-Ki Jeong</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：pose challenges due, Analysis and visualization, images pose challenges, demanding volumetric super-resolution, pose challenges</p>
  <p><b>备注</b>： MICCAI2024 accepted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Analysis and visualization of 3D microscopy images pose challenges due to anisotropic axial resolution, demanding volumetric super-resolution along the axial direction. While training a learning-based 3D super-resolution model seems to be a straightforward solution, it requires ground truth isotropic volumes and suffers from the curse of dimensionality. Therefore, existing methods utilize 2D neural networks to reconstruct each axial slice, eventually piecing together the entire volume. However, reconstructing each slice in the pixel domain fails to give consistent reconstruction in all directions leading to misalignment artifacts. In this work, we present a reconstruction framework based on implicit neural representation (INR), which allows 3D coherency even when optimized by independent axial slices in a batch-wise manner. Our method optimizes a continuous volumetric representation from low-resolution axial slices, using a 2D diffusion prior trained on high-resolution lateral slices without requiring isotropic volumes. Through experiments on real and synthetic anisotropic microscopy images, we demonstrate that our method surpasses other state-of-the-art reconstruction methods. The source code is available on GitHub: this https URL.</p>
  </details>
</details>
<details>
  <summary>73. <b>【2408.08489】DFT-Based Adversarial Attack Detection in MRI Brain Imaging: Enhancing Diagnostic Accuracy in Alzheimer's Case Studies</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08489">https://arxiv.org/abs/2408.08489</a></p>
  <p><b>作者</b>：Mohammad Hossein Najafi,Mohammad Morsali,Mohammadmahdi Vahediahmar,Saeed Bagheri Shouraki</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Recent advancements, adversarial attacks, healthcare systems, attacks, significantly propelled</p>
  <p><b>备注</b>： 10 pages, 4 figures, conference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in deep learning, particularly in medical imaging, have significantly propelled the progress of healthcare systems. However, examining the robustness of medical images against adversarial attacks is crucial due to their real-world applications and profound impact on individuals' health. These attacks can result in misclassifications in disease diagnosis, potentially leading to severe consequences. Numerous studies have explored both the implementation of adversarial attacks on medical images and the development of defense mechanisms against these threats, highlighting the vulnerabilities of deep neural networks to such adversarial activities. In this study, we investigate adversarial attacks on images associated with Alzheimer's disease and propose a defensive method to counteract these attacks. Specifically, we examine adversarial attacks that employ frequency domain transformations on Alzheimer's disease images, along with other well-known adversarial attacks. Our approach utilizes a convolutional neural network (CNN)-based autoencoder architecture in conjunction with the two-dimensional Fourier transform of images for detection purposes. The simulation results demonstrate that our detection and defense mechanism effectively mitigates several adversarial attacks, thereby enhancing the robustness of deep neural networks against such vulnerabilities.</p>
  </details>
</details>
<details>
  <summary>74. <b>【2408.08456】Efficient Data-Sketches and Fine-Tuning for Early Detection of Distributional Drift in Medical Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08456">https://arxiv.org/abs/2408.08456</a></p>
  <p><b>作者</b>：Yusen Wu,Hao Chen,Alex Pissinou Makki,Phuong Nguyen,Yelena Yesha</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：underlying data distribution, treatment decisions, Distributional drift, Distributional drift detection, detect distributional drift</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect diagnostic or treatment decisions. However, current methods have limitations in detecting drift; for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a vision transformer pre-trained model to extract relevant features using breast cancer images as an example, significantly enhancing model accuracy to 99.11\%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50\% increased to 100\%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1\% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.</p>
  </details>
</details>
<details>
  <summary>75. <b>【2408.08432】Predictive uncertainty estimation in deep learning for lung carcinoma classification in digital pathology under real dataset shifts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.08432">https://arxiv.org/abs/2408.08432</a></p>
  <p><b>作者</b>：Abdur R. Fayjie,Jutika Borah,Florencia Carbone,Jan Tack,Patrick Vandewalle</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：shown tremendous progress, predictive uncertainty, shown tremendous, tremendous progress, wide range</p>
  <p><b>备注</b>： 17 pages, 2 figures, 5 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deep learning has shown tremendous progress in a wide range of digital pathology and medical image classification tasks. Its integration into safe clinical decision-making support requires robust and reliable models. However, real-world data comes with diversities that often lie outside the intended source distribution. Moreover, when test samples are dramatically different, clinical decision-making is greatly affected. Quantifying predictive uncertainty in models is crucial for well-calibrated predictions and determining when (or not) to trust a model. Unfortunately, many works have overlooked the importance of predictive uncertainty estimation. This paper evaluates whether predictive uncertainty estimation adds robustness to deep learning-based diagnostic decision-making systems. We investigate the effect of various carcinoma distribution shift scenarios on predictive performance and calibration. We first systematically investigate three popular methods for improving predictive uncertainty: Monte Carlo dropout, deep ensemble, and few-shot learning on lung adenocarcinoma classification as a primary disease in whole slide images. Secondly, we compare the effectiveness of the methods in terms of performance and calibration under clinically relevant distribution shifts such as in-distribution shifts comprising primary disease sub-types and other characterization analysis data; out-of-distribution shifts comprising well-differentiated cases, different organ origin, and imaging modality shifts. While studies on uncertainty estimation exist, to our best knowledge, no rigorous large-scale benchmark compares predictive uncertainty estimation including these dataset shifts for lung carcinoma classification.</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2024/08/20/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2024/08/20/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html"><img class="next-cover" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">🎨 Stable Diffusion 提示词指南书</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">信息检索</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">计算机视觉</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/08/20/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-08-20)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2024-08-20)"/></a><div class="content"><a class="title" href="/2024/08/20/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2024-08-20)">Arxiv每日速递(2024-08-20)</a><time datetime="2024-08-20T00:58:32.990Z" title="发表于 2024-08-20 08:58:32">2024-08-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书"><img src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="🎨 Stable Diffusion 提示词指南书"/></a><div class="content"><a class="title" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书">🎨 Stable Diffusion 提示词指南书</a><time datetime="2024-02-03T06:57:45.000Z" title="发表于 2024-02-03 14:57:45">2024-02-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer语言模型的位置编码与长度外推"/></a><div class="content"><a class="title" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推">Transformer语言模型的位置编码与长度外推</a><time datetime="2023-10-22T14:55:45.000Z" title="发表于 2023-10-22 22:55:45">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prompt：大语言模型的执行指南"/></a><div class="content"><a class="title" href="/2023/09/06/Prompt%EF%BC%9A%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%89%A7%E8%A1%8C%E6%8C%87%E5%8D%97.html" title="Prompt：大语言模型的执行指南">Prompt：大语言模型的执行指南</a><time datetime="2023-09-06T14:45:45.000Z" title="发表于 2023-09-06 22:45:45">2023-09-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (8)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-10-22</span><a class="blog-slider__title" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">Transformer语言模型的位置编码与长度外推</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt=""><img width="48" height="48" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-02-03</span><a class="blog-slider__title" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">🎨 Stable Diffusion 提示词指南书</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>