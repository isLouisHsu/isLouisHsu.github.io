<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv每日速递(2025-03-05) | LOUIS' BLOG</title><meta name="author" content="徐耀彬"><meta name="copyright" content="徐耀彬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新1086篇论文，其中：  自然语言处理183篇 信息检索33篇 计算机视觉282篇  自然语言处理    1. 【2503.01844】Can (A)I Change Your Mind?   链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2503.01844">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv每日速递(2025-03-05)">
<meta property="og:url" content="http://louishsu.xyz/2025/03/05/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">
<meta property="og:site_name" content="LOUIS&#39; BLOG">
<meta property="og:description" content="本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。 统计 今日共更新1086篇论文，其中：  自然语言处理183篇 信息检索33篇 计算机视觉282篇  自然语言处理    1. 【2503.01844】Can (A)I Change Your Mind?   链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2503.01844">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png">
<meta property="article:published_time" content="2025-03-05T01:09:10.861Z">
<meta property="article:modified_time" content="2025-03-05T01:11:12.187Z">
<meta property="article:author" content="徐耀彬">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://louishsu.xyz/2025/03/05/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2025-03-05 09:11:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">LOUIS' BLOG</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fas fa-pen"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/isLouisHsu/blog/tree/master/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-brands fa-app-store"></i><span> 利器</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://code.visualstudio.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> VSCode：微软旗下的跨平台代码编辑软件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mobaxterm.mobatek.net/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> MobaXterm：超好用的全能远程终端</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/CopyTranslator/CopyTranslator"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> CopyTranslator：“复制即翻译”的外文辅助阅读翻译解决方案</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.zotero.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zotero：便于收集、组织、引用、共享的文献管理工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://zealdocs.org/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Zeal：离线文档浏览器，其灵感来自 OS X平台上的 Dash，目前支持 Window 和 Liunx，基于 QT5</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ditto-cp.sourceforge.io/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Ditto：强大的Windows剪贴板增强工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://wise-system-monitor.en.softonic.com/"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> Wise System Monitor：监控从系统到本地网络的所有运行情况</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/indiff/qttabbar"><i class="fa-fw fa-sharp fa-solid fa-star"></i><span> QtTabBar：在Windows资源管理器中使用多标签功能扩展工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/sentialx/multrin"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Multrin：“窗口合并”辅助小工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/Wox-launcher/Wox"><i class="fa-fw fa-sharp fa-solid fa-star-half-stroke"></i><span> Wox &amp; Everything：基于名称快速定位文件和文件夹的搜索工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/NickeManarin/ScreenToGif"><i class="fa-fw fa-regular fa-star"></i><span> ScreenToGif：快速录制屏幕指定区域并保存为动图文件</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://mathpix.com/"><i class="fa-fw fa-regular fa-star"></i><span> Mathpix Snipping：识别数学公式并转换成LaTeX</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="http://www.uderzo.it/main_products/space_sniffer/index.html"><i class="fa-fw fa-regular fa-star"></i><span> Space Sniffer：磁盘空间分析工具</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv每日速递(2025-03-05)<a class="post-edit-link" href="https://github.com/isLouisHsu/blog/tree/master/source_posts/Arxiv每日速递.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-05T01:09:10.861Z" title="发表于 2025-03-05 09:09:10">2025-03-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-05T01:11:12.187Z" title="更新于 2025-03-05 09:11:12">2025-03-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">95k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>564分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/03/05/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要展示每日从Arxiv论文网站获取的最新论文列表，以自然语言处理、信息检索、计算机视觉等类目进行划分。</p>
<h1>统计</h1>
<p>今日共更新<strong>1086</strong>篇论文，其中：</p>
<ul>
<li>自然语言处理<strong>183</strong>篇</li>
<li>信息检索<strong>33</strong>篇</li>
<li>计算机视觉<strong>282</strong>篇</li>
</ul>
<h1>自然语言处理</h1>
<details>
  <summary>1. <b>【2503.01844】Can (A)I Change Your Mind?</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01844">https://arxiv.org/abs/2503.01844</a></p>
  <p><b>作者</b>：Miriam Havin,Timna Wharton Kleinman,Moran Koren,Yaniv Dover,Ariel Goldstein</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language model, everyday life raises, life raises critical, raises critical cognitive, based conversational agents</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The increasing integration of large language model (LLM) based conversational agents into everyday life raises critical cognitive and social questions about their potential to influence human opinions. Although previous studies have shown that LLM-based agents can generate persuasive content, these typically involve controlled, English-language settings. Addressing this, our preregistered study explored LLM's persuasive capabilities in more ecological, unconstrained scenarios, examining both static (written paragraphs) and dynamic (conversations via Telegram) interaction types. Conducted entirely in Hebrew with 200 participants, the study assessed the persuasive effects of both LLM and human interlocutors on controversial civil policy topics. Results indicated that participants adopted LLM and human perspectives similarly, with significant opinion changes evident across all conditions, regardless of interlocutor type or interaction mode. Confidence levels increased significantly in most scenarios, except in static LLM interactions. These findings demonstrate LLM-based agents' robust persuasive capabilities across diverse sources and settings, highlighting their potential impact on shaping public opinions.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2503.01840】EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01840">https://arxiv.org/abs/2503.01840</a></p>
  <p><b>作者</b>：Yuhui Li,Fangyun Wei,Chao Zhang,Hongyang Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：modern LLMs makes, expensive and slow, speculative sampling, sequential nature, nature of modern</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2503.01839】Jailbreaking Safeguarded Text-to-Image Models via Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01839">https://arxiv.org/abs/2503.01839</a></p>
  <p><b>作者</b>：Zhengyuan Jiang,Yuepeng Hu,Yuchen Yang,Yinzhi Cao,Neil Zhenqiang Gong</p>
  <p><b>类目</b>：Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：pornographic images, generate harmful content, harmful content, safety guardrails, safety</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text-to-Image models may generate harmful content, such as pornographic images, particularly when unsafe prompts are submitted. To address this issue, safety filters are often added on top of text-to-image models, or the models themselves are aligned to reduce harmful outputs. However, these defenses remain vulnerable when an attacker strategically designs adversarial prompts to bypass these safety guardrails. In this work, we propose PromptTune, a method to jailbreak text-to-image models with safety guardrails using a fine-tuned large language model. Unlike other query-based jailbreak attacks that require repeated queries to the target model, our attack generates adversarial prompts efficiently after fine-tuning our AttackLLM. We evaluate our method on three datasets of unsafe prompts and against five safety guardrails. Our results demonstrate that our approach effectively bypasses safety guardrails, outperforms existing no-box attacks, and also facilitates other query-based attacks.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2503.01836】CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01836">https://arxiv.org/abs/2503.01836</a></p>
  <p><b>作者</b>：Yisen Li,Lingfeng Yang,Wenxuan Shen,Pan Zhou,Yao Wan,Weiwei Lin,Dongping Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models', Distilling advanced Large, advanced Large Language, Language Models' instruction-following, Large Language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLM wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CrowdSelect, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CrowdSelect, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction. Code are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2503.01832】Rotary Outliers and Rotary Offset Features in Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01832">https://arxiv.org/abs/2503.01832</a></p>
  <p><b>作者</b>：André Jonasson</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Transformer-based Large Language, Large Language Models, Transformer-based Large, Large Language, provide sequence position</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Transformer-based Large Language Models (LLMs) rely on positional encodings to provide sequence position information to their attention mechanism. Rotary Positional Encodings (RoPE), which encode relative position by rotating queries and keys, have become widely used in modern LLMs. We study the features and patterns that emerge in queries and keys when using rotary embeddings. Our analysis reveals consistent patterns within the same model across layers and attention heads and across different models and architectures. We present and apply analysis techniques and show how the queries and keys use RoPE to construct various attention patterns, including attention sinks. We find and analyze outliers across models in queries and keys and find that they are likely to be found in rotary features with partial cycles. We derive bounds that tell us what rotary frequencies are likely to be selected as outlier features and at what minimum angle the query-key rotary pairs in these features tend to be above and verify the bounds empirically with models of significant architectural differences.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2503.01830】From Language to Cognition: How LLMs Outgrow the Human Language Network</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01830">https://arxiv.org/abs/2503.01830</a></p>
  <p><b>作者</b>：Badr AlKhamissi,Greta Tuckute,Yingtian Tang,Taha Binhuraib,Antoine Bosselut,Martin Schrimpf</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：exhibit remarkable similarity, Large language models, Large language, brain alignment, exhibit remarkable</p>
  <p><b>备注</b>： Preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) exhibit remarkable similarity to neural activity in the human language network. However, the key properties of language shaping brain-like representations, and their evolution during training as a function of different tasks remain unclear. We here benchmark 34 training checkpoints spanning 300B tokens across 8 different model sizes to analyze how brain alignment relates to linguistic competence. Specifically, we find that brain alignment tracks the development of formal linguistic competence -- i.e., knowledge of linguistic rules -- more closely than functional linguistic competence. While functional competence, which involves world knowledge and reasoning, continues to develop throughout training, its relationship with brain alignment is weaker, suggesting that the human language network primarily encodes formal linguistic structure rather than broader cognitive functions. We further show that model size is not a reliable predictor of brain alignment when controlling for feature size and find that the correlation between next-word prediction, behavioral alignment and brain alignment fades once models surpass human language proficiency. Finally, using the largest set of rigorous neural language benchmarks to date, we show that language brain alignment benchmarks remain unsaturated, highlighting opportunities for improving future models. Taken together, our findings suggest that the human language network is best modeled by formal, rather than functional, aspects of language.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2503.01829】Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01829">https://arxiv.org/abs/2503.01829</a></p>
  <p><b>作者</b>：Nimet Beyza Bozdag,Shuhaib Mehri,Gokhan Tur,Dilek Hakkani-Tür</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, rival human-level persuasion, Language Models, rival human-level</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) demonstrate persuasive capabilities that rival human-level persuasion. While these capabilities can be used for social good, they also present risks of potential misuse. Moreover, LLMs' susceptibility to persuasion raises concerns about alignment with ethical principles. To study these dynamics, we introduce Persuade Me If You Can (PMIYC), an automated framework for evaluating persuasion through multi-agent interactions. Here, Persuader agents engage in multi-turn conversations with the Persuadee agents, allowing us to measure LLMs' persuasive effectiveness and their susceptibility to persuasion. We conduct comprehensive evaluations across diverse LLMs, ensuring each model is assessed against others in both subjective and misinformation contexts. We validate the efficacy of our framework through human evaluations and show alignment with prior work. PMIYC offers a scalable alternative to human annotation for studying persuasion in LLMs. Through PMIYC, we find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness, outperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50% greater resistance to persuasion for misinformation compared to Llama-3.3-70B. These findings provide empirical insights into the persuasive dynamics of LLMs and contribute to the development of safer AI systems.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2503.01820】RSQ: Learning from Important Tokens Leads to Better Quantized LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01820">https://arxiv.org/abs/2503.01820</a></p>
  <p><b>作者</b>：Yi-Lin Sung,Prateek Yadav,Jialu Li,Jaehong Yoon,Mohit Bansal</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：efficiently compressing large, expensive retraining, Layer-wise quantization, key technique, technique for efficiently</p>
  <p><b>备注</b>： Our code is available at [this https URL](https://github.com/ylsung/rsq) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Layer-wise quantization is a key technique for efficiently compressing large models without expensive retraining. Previous methods typically quantize the weights of each layer by "uniformly" optimizing the layer reconstruction loss across all output tokens. However, in this paper, we demonstrate that better-quantized models can be obtained by prioritizing learning from important tokens (e.g. which have large attention scores). Building on this finding, we propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations (orthogonal transformation) to the model to mitigate outliers (those with exceptionally large magnitude), (2) scales the token feature based on its importance, and (3) quantizes the model using the GPTQ framework with the second-order statistics computed by scaled tokens. To compute token importance, we explore both heuristic and dynamic strategies. Based on a thorough analysis of all approaches, we adopt attention concentration, which uses attention scores of each token as its importance, as the best approach. We demonstrate that RSQ consistently outperforms baseline methods across multiple downstream tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally, models quantized with RSQ achieve superior performance on long-context tasks, further highlighting its effectiveness. Lastly, RSQ demonstrates generalizability across various setups, including different model sizes, calibration datasets, bit precisions, and quantization methods.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2503.01819】Do GFlowNets Transfer? Case Study on the Game of 24/42</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01819">https://arxiv.org/abs/2503.01819</a></p>
  <p><b>作者</b>：Adesh Gupta,Abhinav Kumar,Mansi Gupta,Paras Chopra</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：single accurate responses, Generating diverse solutions, Generating diverse, limiting creativity, human-like reasoning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generating diverse solutions is key to human-like reasoning, yet autoregressive language models focus on single accurate responses, limiting creativity. GFlowNets optimize solution generation as a flow network, promising greater diversity. Our case study shows their limited zero-shot transferability by fine-tuning small and medium-sized large language models on the Game of 24 and testing them on the Game of 42 datasets. Results revealed that GFlowNets struggle to maintain solution diversity and accuracy, highlighting key limitations in their cross-task generalization and the need for future research in improved transfer learning capabilities.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2503.01814】LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01814">https://arxiv.org/abs/2503.01814</a></p>
  <p><b>作者</b>：Weizhi Zhang,Liangwei Yang,Wooseong Yang,Henry Peng Zou,Yuqing Liu,Ke Xu,Sourav Medya,Philip S. Yu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Collaborative filtering models, Collaborative filtering, capturing user-item interactions, demonstrated strong performance, demonstrated strong</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Collaborative filtering models, particularly graph-based approaches, have demonstrated strong performance in capturing user-item interactions for recommendation systems. However, they continue to struggle in cold-start and data-sparse scenarios. The emergence of large language models (LLMs) like GPT and LLaMA presents new possibilities for enhancing recommendation performance, especially in cold-start settings. Despite their promise, LLMs pose challenges related to scalability and efficiency due to their high computational demands and limited ability to model complex user-item relationships effectively. In this work, we introduce a novel perspective on leveraging LLMs for CF model initialization. Through experiments, we uncover an embedding collapse issue when scaling CF models to larger embedding dimensions. To effectively harness large-scale LLM embeddings, we propose innovative selective initialization strategies utilizing random, uniform, and variance-based index sampling. Our comprehensive evaluation on multiple real-world datasets demonstrates significant performance gains across various CF models while maintaining a lower computational cost compared to existing LLM-based recommendation approaches.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2503.01807】Large-Scale Data Selection for Instruction Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01807">https://arxiv.org/abs/2503.01807</a></p>
  <p><b>作者</b>：Hamish Ivison,Muru Zhang,Faeze Brahman,Pang Wei Koh,Pradeep Dasigi</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Selecting high-quality training, carefully curated datasets, high-quality training data, instruction-tuning language models, high-quality training</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at this https URL.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2503.01805】Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01805">https://arxiv.org/abs/2503.01805</a></p>
  <p><b>作者</b>：Gilad Yehudai,Clayton Sanford,Maya Bechler-Speicher,Orr Fischer,Ran Gilad-Bachrach,Amir Globerson</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：machine learning, revolutionized the field, field of machine, width, graph-based</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement a task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We support our theoretical results with empirical evaluations.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2503.01804】$\texttt{SEM-CTRL}$: Semantically Controlled Decoding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01804">https://arxiv.org/abs/2503.01804</a></p>
  <p><b>作者</b>：Mohammad Albinhassan,Pranava Madhyastha,Alessandra Russo</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Model, Large Language, Language Model, significant challenge, real-world deployment</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Ensuring both syntactic and semantic correctness in Large Language Model (LLM) outputs remains a significant challenge, despite being critical for real-world deployment. In this paper, we introduce $\texttt{SEM-CTRL}$, a unified approach that enforces rich context-sensitive constraints and task- and instance-specific semantics directly on an LLM decoder. Our approach integrates token-level MCTS, which is guided by specific syntactic and semantic constraints. The constraints over the desired outputs are expressed using Answer Set Grammars -- a logic-based formalism that generalizes context-sensitive grammars while incorporating background knowledge to represent task-specific semantics. We show that our approach guarantees correct completions for any off-the-shelf LLM without the need for fine-tuning. We evaluate $\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar synthesis, combinatorial reasoning, and planning. Our results demonstrate that $\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform larger variants and state-of-the-art reasoning models (e.g., o1-preview) while simultaneously guaranteeing solution correctness.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2503.01781】Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01781">https://arxiv.org/abs/2503.01781</a></p>
  <p><b>作者</b>：Meghana Rajeev,Rajkumar Ramamurthy,Prapti Trivedi,Vikas Yadav,Oluwanifemi Bamgbose,Sathwik Tejaswi Madhusudan,James Zou,Nazneen Rajani</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：systematically mislead models, output incorrect answers, introducing query-agnostic adversarial, irrelevant text, systematically mislead</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We investigate the robustness of reasoning models trained for step-by-step problem solving by introducing query-agnostic adversarial triggers - short, irrelevant text that, when appended to math problems, systematically mislead models to output incorrect answers without altering the problem's semantics. We propose CatAttack, an automated iterative attack pipeline for generating triggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully transfer them to more advanced reasoning target models like DeepSeek R1 and DeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the likelihood of the target model generating an incorrect answer. For example, appending, "Interesting fact: cats sleep most of their lives," to any math problem leads to more than doubling the chances of a model getting the answer wrong. Our findings highlight critical vulnerabilities in reasoning models, revealing that even state-of-the-art models remain susceptible to subtle adversarial inputs, raising security and reliability concerns. The CatAttack triggers dataset with model responses is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2503.01773】Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01773">https://arxiv.org/abs/2503.01773</a></p>
  <p><b>作者</b>：Shiqi Chen,Tongyao Zhu,Ruochen Zhou,Jinghan Zhang,Siyang Gao,Juan Carlos Niebles,Mor Geva,Junxian He,Jiajun Wu,Manling Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Vision Language, Vision Language Models, Large Vision, Vision Language, spatial reasoning tasks</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Vision Language Models (VLMs) have long struggled with spatial reasoning tasks. Surprisingly, even simple spatial reasoning tasks, such as recognizing "under" or "behind" relationships between only two objects, pose significant challenges for current VLMs. In this work, we study the spatial reasoning challenge from the lens of mechanistic interpretability, diving into the model's internal states to examine the interactions between image and text tokens. By tracing attention distribution over the image through out intermediate layers, we observe that successful spatial reasoning correlates strongly with the model's ability to align its attention distribution with actual object locations, particularly differing between familiar and unfamiliar spatial relationships. Motivated by these findings, we propose ADAPTVIS based on inference-time confidence scores to sharpen the attention on highly relevant regions when confident, while smoothing and broadening the attention window to consider a wider context when confidence is lower. This training-free decoding method shows significant improvement (e.g., up to a 50 absolute point improvement) on spatial reasoning benchmarks such as WhatsUp and VSR with negligible cost. We make code and data publicly available for research purposes at this https URL.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2503.01763】Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01763">https://arxiv.org/abs/2503.01763</a></p>
  <p><b>作者</b>：Zhengliang Shi,Yuhan Wang,Lingyong Yan,Pengjie Ren,Shuaiqiang Wang,Dawei Yin,Zhaochun Ren</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：augment large language, Tool learning aims, solving practical tasks, learning aims, aims to augment</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2503.01753】Boolean-aware Attention for Dense Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01753">https://arxiv.org/abs/2503.01753</a></p>
  <p><b>作者</b>：Quan Mai,Susan Gauch,Douglas Adams</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：present Boolean-aware attention, dynamically adjusts token, adjusts token focus, present Boolean-aware, token focus based</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present Boolean-aware attention, a novel attention mechanism that dynamically adjusts token focus based on Boolean operators (e.g., and, or, not). Our model employs specialized Boolean experts, each tailored to amplify or suppress attention for operator-specific contexts. A predefined gating mechanism activates the corresponding experts based on the detected Boolean type. Experiments on Boolean retrieval datasets demonstrate that integrating BoolAttn with BERT greatly enhances the model's capability to process Boolean queries.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2503.01751】SAKE: Steering Activations for Knowledge Editing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01751">https://arxiv.org/abs/2503.01751</a></p>
  <p><b>作者</b>：Marco Scialanga,Thibault Laugel,Vincent Grari,Marcin Detyniecki</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Langue Models, efficient manner arises, Large Langue, memorize real-world facts, Langue Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2503.01743】Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01743">https://arxiv.org/abs/2503.01743</a></p>
  <p><b>作者</b>：Abdelrahman Abouelenin,Atabak Ashfaq,Adam Atkinson,Hany Awadalla,Nguyen Bach,Jianmin Bao,Alon Benhaim,Martin Cai,Vishrav Chaudhary,Congcong Chen,Dong Chen,Dongdong Chen,Junkun Chen,Weizhu Chen,Yen-Chun Chen,Yi-ling Chen,Qi Dai,Xiyang Dai,Ruchao Fan,Mei Gao,Min Gao,Amit Garg,Abhishek Goswami,Junheng Hao,Amr Hendy,Yuxuan Hu,Xin Jin,Mahmoud Khademi,Dongwoo Kim,Young Jin Kim,Gina Lee,Jinyu Li,Yunsheng Li,Chen Liang,Xihui Lin,Zeqi Lin,Mengchen Liu,Yang Liu,Gilsinia Lopez,Chong Luo,Piyush Madan,Vadim Mazalov,Ali Mousavi,Anh Nguyen,Jing Pan,Daniel Perez-Becker,Jacob Platin,Thomas Portet,Kai Qiu,Bo Ren,Liliang Ren,Sambuddha Roy,Ning Shang,Yelong Shen,Saksham Singhal,Subhojit Som,Xia Song,Tetyana Sych,Praneetha Vaddamanu,Shuohang Wang,Yiming Wang,Zhenghao Wang,Haibin Wu,Haoran Xu,Weijian Xu,Yifan Yang,Ziyi Yang,Donghan Yu,Ishmam Zabir,Jianwen Zhang,Li Lyna Zhang,Yunan Zhang,Xiren Zhou</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：highly capable language, highly capable, capable language, synthetic data, math and coding</p>
  <p><b>备注</b>： 39 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2503.01742】Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01742">https://arxiv.org/abs/2503.01742</a></p>
  <p><b>作者</b>：Alberto Purpura,Sahil Wadhwa,Jesse Zymet,Akshay Gupta,Andy Luo,Melissa Kazemi Rad,Swapnil Shinde,Mohammad Shahed Sorower</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, presents significant privacy, Large Language, growth of Large</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have recently complemented these efforts with an offensive approach that involves red teaming, i.e., proactively attacking LLMs with the purpose of identifying their vulnerabilities. This paper provides a concise and practical overview of the LLM red teaming literature, structured so as to describe a multi-component system end-to-end. To motivate red teaming we survey the initial safety needs of some high-profile LLMs, and then dive into the different components of a red teaming system as well as software packages for implementing them. We cover various attack methods, strategies for attack-success evaluation, metrics for assessing experiment outcomes, as well as a host of other considerations. Our survey will be useful for any reader who wants to rapidly obtain a grasp of the major red teaming concepts for their own use in practical applications.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2503.01724】Syntactic Learnability of Echo State Neural Language Models at Scale</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01724">https://arxiv.org/abs/2503.01724</a></p>
  <p><b>作者</b>：Ryo Ueda,Tatsuki Kuribayashi,Shunsuke Kando,Kentaro Inui</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：minimum architectural complexity, exhibits reasonable language, Recurrent Neural Networks, Echo State Network, language learning capability</p>
  <p><b>备注</b>： 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:What is a neural model with minimum architectural complexity that exhibits reasonable language learning capability? To explore such a simple but sufficient neural language model, we revisit a basic reservoir computing (RC) model, Echo State Network (ESN), a restricted class of simple Recurrent Neural Networks. Our experiments showed that ESN with a large hidden state is comparable or superior to Transformer in grammaticality judgment tasks when trained with about 100M words, suggesting that architectures as complex as that of Transformer may not always be necessary for syntactic learning.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2503.01714】Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01714">https://arxiv.org/abs/2503.01714</a></p>
  <p><b>作者</b>：Chenxi Wang,Tianle Gu,Zhongyu Wei,Lang Gao,Zirui Song,Xiuying Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：efficiently comprehend scrambled, word form, comprehend scrambled words, form, word</p>
  <p><b>备注</b>： 14 pages, 10 figures, submitted to ACL Rolling Review, February 2025 cycle, see [this https URL](https://github.com/Aurora-cx/TypoLLM) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Human readers can efficiently comprehend scrambled words, a phenomenon known as Typoglycemia, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the underlying mechanisms remain unclear. To investigate this, we conduct controlled experiments to analyze the roles of word form and contextual information in semantic reconstruction and examine LLM attention patterns. Specifically, we first propose SemRecScore, a reliable metric to quantify the degree of semantic reconstruction, and validate its effectiveness. Using this metric, we study how word form and contextual information influence LLMs' semantic reconstruction ability, identifying word form as the core factor in this process. Furthermore, we analyze how LLMs utilize word form and find that they rely on specialized attention heads to extract and process word form information, with this mechanism remaining stable across varying levels of word scrambling. This distinction between LLMs' fixed attention patterns primarily focused on word form and human readers' adaptive strategy in balancing word form and contextual information provides insights into enhancing LLM performance by incorporating human-like, context-aware mechanisms.</p>
  </details>
</details>
<details>
  <summary>23. <b>【2503.01711】MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01711">https://arxiv.org/abs/2503.01711</a></p>
  <p><b>作者</b>：Weicong Qin,Yi Xu,Weijie Yu,Chenglei Shen,Ming He,Jianping Fan,Xiao Zhang,Jun Xu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：match users' preferences, aims to retrieve, retrieve and rank, rank items, items that match</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Personalized product search aims to retrieve and rank items that match users' preferences and search intent. Despite their effectiveness, existing approaches typically assume that users' query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2503.01700】Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01700">https://arxiv.org/abs/2503.01700</a></p>
  <p><b>作者</b>：Yongchao Chen,Yilun Hao,Yang Zhang,Chuchu Fan</p>
  <p><b>类目</b>：Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, shown great potentials, potentials of Large</p>
  <p><b>备注</b>： 7 pages, 7 figures, 3 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent works have shown great potentials of Large Language Models (LLMs) in robot task and motion planning (TAMP). Current LLM approaches generate text- or code-based reasoning chains with sub-goals and action plans. However, they do not fully leverage LLMs' symbolic computing and code generation capabilities. Many robot TAMP tasks involve complex optimization under multiple constraints, where pure textual reasoning is insufficient. While augmenting LLMs with predefined solvers and planners improves performance, it lacks generalization across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP capabilities by steering them to generate code as symbolic planners for optimization and constraint verification. Unlike prior work that uses code to interface with robot action modules, we steer LLMs to generate code as solvers, planners, and checkers for TAMP tasks requiring symbolic computing, while still leveraging textual reasoning to incorporate common sense. With a multi-round guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner improves success rates by average 24.1\% over best baseline methods across seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows strong effectiveness and generalizability across discrete and continuous environments, 2D/3D simulations and real-world settings, as well as single- and multi-robot tasks with diverse requirements. See our project website this https URL for prompts, videos, and code.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2503.01695】Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01695">https://arxiv.org/abs/2503.01695</a></p>
  <p><b>作者</b>：Kun Li,Tianhua Zhang,Yunxiang Li,Hongyin Luo,Abdalla Moustafa,Xixin Wu,James Glass,Helen Meng</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：long-form question answering, involving knowledge conflicts, developing trustworthy retrieval, trustworthy retrieval augmented, retrieval augmented generation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Improving context faithfulness in large language models is essential for developing trustworthy retrieval augmented generation systems and mitigating hallucinations, especially in long-form question answering (LFQA) tasks or scenarios involving knowledge conflicts. Existing methods either intervene LLMs only at inference without addressing their inherent limitations or overlook the potential for self-improvement. In this paper, we introduce GenDiE (Generate, Discriminate, Evolve), a novel self-evolving framework that enhances context faithfulness through fine-grained sentence-level optimization. GenDiE combines both generative and discriminative training, equipping LLMs with self-generation and self-scoring capabilities to facilitate iterative self-evolution. This supports both data construction for model alignment and score-guided search during inference. Furthermore, by treating each sentence in a response as an independent optimization unit, GenDiE effectively addresses the limitations of previous approaches that optimize at the holistic answer level, which may miss unfaithful details. Experiments on ASQA (in-domain LFQA) and ConFiQA (out-of-domain counterfactual QA) datasets demonstrate that GenDiE surpasses various baselines in both faithfulness and correctness, and exhibits robust performance for domain adaptation.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2503.01688】When an LLM is apprehensive about its answers -- and when its uncertainty is justified</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01688">https://arxiv.org/abs/2503.01688</a></p>
  <p><b>作者</b>：Petr Sychev,Andrey Goncharov,Daniil Vyazhev,Edvard Khalafyan,Alexey Zaytsev</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：evaluating Large Language, Large Language Models, Large Language, incorrect answers result, evaluating Large</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and $14$ topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is $0.73$. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is $0.55$. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2503.01675】Using (Not so) Large Language Models for Generating Simulation Models in a Formal DSL -- A Study on Reaction Networks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01675">https://arxiv.org/abs/2503.01675</a></p>
  <p><b>作者</b>：Justin N. Kreikemeyer,Miłosz Jankowski,Pia Wilsdorf,Adelinde M. Uhrmacher</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Formal languages, integral part, natural language, Formal, Model</p>
  <p><b>备注</b>： 18 pages, 5 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to 84.5% of cases. In addition, our small-scale user study demonstrates the model's practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2503.01672】Automated Annotation of Evolving Corpora for Augmenting Longitudinal Network Data: A Framework Integrating Large Language Models and Expert Knowledge</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01672">https://arxiv.org/abs/2503.01672</a></p>
  <p><b>作者</b>：Xiao Liu,Zirui Wu,Jiayi Li,Zhicheng Shao,Xun Pang,Yansong Feng</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Social and Information Networks (cs.SI)</p>
  <p><b>关键词</b>：Longitudinal network data, Longitudinal network, systems and processes, essential for analyzing, social systems</p>
  <p><b>备注</b>： Work in progress, presented at the 2025 Asian PolMeth Conference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Longitudinal network data are essential for analyzing political, economic, and social systems and processes. In political science, these datasets are often generated through human annotation or supervised machine learning applied to evolving corpora. However, as semantic contexts shift over time, inferring dynamic interaction types on emerging issues among a diverse set of entities poses significant challenges, particularly in maintaining timely and consistent annotations. This paper presents the Expert-Augmented LLM Annotation (EALA) approach, which leverages Large Language Models (LLMs) in combination with historically annotated data and expert-constructed codebooks to extrapolate and extend datasets into future periods. We evaluate the performance and reliability of EALA using a dataset of climate negotiations. Our findings demonstrate that EALA effectively predicts nuanced interactions between negotiation parties and captures the evolution of topics over time. At the same time, we identify several limitations inherent to LLM-based annotation, highlighting areas for further improvement. Given the wide availability of codebooks and annotated datasets, EALA holds substantial promise for advancing research in political science and beyond.</p>
  </details>
</details>
<details>
  <summary>29. <b>【2503.01670】Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the Lens of Summarization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01670">https://arxiv.org/abs/2503.01670</a></p>
  <p><b>作者</b>：Siya Qi,Rui Cao,Yulan He,Zheng Yuan</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：text quality evaluation, widely adopted approach, including hallucination evaluation, large language models, hallucination evaluation</p>
  <p><b>备注</b>： 8 pages, 5 figures for main body</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the rapid development of large language models (LLMs), LLM-as-a-judge has emerged as a widely adopted approach for text quality evaluation, including hallucination evaluation. While previous studies have focused exclusively on single-context evaluation (e.g., discourse faithfulness or world factuality), real-world hallucinations typically involve mixed contexts, which remains inadequately evaluated. In this study, we use summarization as a representative task to comprehensively evaluate LLMs' capability in detecting mixed-context hallucinations, specifically distinguishing between factual and non-factual hallucinations. Through extensive experiments across direct generation and retrieval-based models of varying scales, our main observations are: (1) LLMs' intrinsic knowledge introduces inherent biases in hallucination evaluation; (2) These biases particularly impact the detection of factual hallucinations, yielding a significant performance bottleneck; (3) The fundamental challenge lies in effective knowledge utilization, balancing between LLMs' intrinsic knowledge and external context for accurate mixed-context hallucination evaluation.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2503.01659】Detecting Stylistic Fingerprints of Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01659">https://arxiv.org/abs/2503.01659</a></p>
  <p><b>作者</b>：Yehonatan Bitton,Elad Bitton,Shai Nisan</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large language models, Large language, writing styles, consistent stylistic fingerprints, distinct and consistent</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have distinct and consistent stylistic fingerprints, even when prompted to write in different writing styles. Detecting these fingerprints is important for many reasons, among them protecting intellectual property, ensuring transparency regarding AI-generated content, and preventing the misuse of AI technologies. In this paper, we present a novel method to classify texts based on the stylistic fingerprints of the models that generated them. We introduce an LLM-detection ensemble that is composed of three classifiers with varied architectures and training data. This ensemble is trained to classify texts generated by four well-known LLM families: Claude, Gemini, Llama, and OpenAI. As this task is highly cost-sensitive and might have severe implications, we want to minimize false-positives and increase confidence. We consider a prediction as valid when all three classifiers in the ensemble unanimously agree on the output classification. Our ensemble is validated on a test set of texts generated by Claude, Gemini, Llama, and OpenAI models, and achieves extremely high precision (0.9988) and a very low false-positive rate (0.0004). Furthermore, we demonstrate the ensemble's ability to distinguish between texts generated by seen and unseen models. This reveals interesting stylistic relationships between models. This approach to stylistic analysis has implications for verifying the originality of AI-generated texts and tracking the origins of model training techniques.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2503.01635】he Emergence of Grammar through Reinforcement Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01635">https://arxiv.org/abs/2503.01635</a></p>
  <p><b>作者</b>：Stephen Wechsler,James W. Shearer,Katrin Erk</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Theory (cs.IT)</p>
  <p><b>关键词</b>：reinforcement learning theory, evolution of grammatical, grammatical systems, systems of syntactic, syntactic and semantic</p>
  <p><b>备注</b>： 49 pages, 8 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The evolution of grammatical systems of syntactic and semantic composition is modeled here with a novel application of reinforcement learning theory. To test the functionalist thesis that speakers' expressive purposes shape their language, we include within the model a probability distribution over different messages that could be expressed in a given context. The proposed learning and production algorithm then breaks down language learning into a sequence of simple steps, such that each step benefits from the message probabilities. The results are presented in the form of numerical simulations of language histories and analytic proofs. The potential for applying these mathematical models to the study of natural language is illustrated with two case studies from the history of English.</p>
  </details>
</details>
<details>
  <summary>32. <b>【2503.01625】Annotating and Inferring Compositional Structures in Numeral Systems Across Languages</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01625">https://arxiv.org/abs/2503.01625</a></p>
  <p><b>作者</b>：Arne Rubehn,Christoph Rzymski,Luca Ciucci,Kellen Parker van Dam,Alžběta Kučerová,Katja Bocklage,David Snee,Abishek Stephen,Johann-Mattis List</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：world languages vary, Numeral systems, code numeral systems, current shape, vary in fascinating</p>
  <p><b>备注</b>： Submitted to the 7th Workshop on Research in Computational Linguistic Typology and Multilingual NLP (SIGTYP)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Numeral systems across the world's languages vary in fascinating ways, both regarding their synchronic structure and the diachronic processes that determined how they evolved in their current shape. For a proper comparison of numeral systems across different languages, however, it is important to code them in a standardized form that allows for the comparison of basic properties. Here, we present a simple but effective coding scheme for numeral annotation, along with a workflow that helps to code numeral systems in a computer-assisted manner, providing sample data for numerals from 1 to 40 in 25 typologically diverse languages. We perform a thorough analysis of the sample, focusing on the systematic comparison between the underlying and the surface morphological structure. We further experiment with automated models for morpheme segmentation, where we find allomorphy as the major reason for segmentation errors. Finally, we show that subword tokenization algorithms are not viable for discovering morphemes in low-resource scenarios.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2503.01623】Lost in Moderation: How Commercial Content Moderation APIs Over- and Under-Moderate Group-Targeted Hate Speech and Linguistic Variations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01623">https://arxiv.org/abs/2503.01623</a></p>
  <p><b>作者</b>：David Hartmann,Amin Oueslati,Dimitri Staufer,Lena Pohlmann,Simon Munzert,Hendrik Heuer</p>
  <p><b>类目</b>：Human-Computer Interaction (cs.HC); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Commercial content moderation, combat online hate, content moderation APIs, marketed as scalable, scalable solutions</p>
  <p><b>备注</b>： This is the author's version of the paper accepted at CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Commercial content moderation APIs are marketed as scalable solutions to combat online hate speech. However, the reliance on these APIs risks both silencing legitimate speech, called over-moderation, and failing to protect online platforms from harmful speech, known as under-moderation. To assess such risks, this paper introduces a framework for auditing black-box NLP systems. Using the framework, we systematically evaluate five widely used commercial content moderation APIs. Analyzing five million queries based on four datasets, we find that APIs frequently rely on group identity terms, such as ``black'', to predict hate speech. While OpenAI's and Amazon's services perform slightly better, all providers under-moderate implicit hate speech, which uses codified messages, especially against LGBTQIA+ individuals. Simultaneously, they over-moderate counter-speech, reclaimed slurs and content related to Black, LGBTQIA+, Jewish, and Muslim people. We recommend that API providers offer better guidance on API implementation and threshold setting and more transparency on their APIs' limitations.
Warning: This paper contains offensive and hateful terms and concepts. We have chosen to reproduce these terms for reasons of transparency.
</p><p>Comments:<br>
This is the author’s version of the paper accepted at CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan</p>
<p>Subjects:</p>
<p>Human-Computer Interaction (cs.HC); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)</p>
<p>Cite as:<br>
arXiv:2503.01623 [cs.HC]</p>
<p>(or<br>
arXiv:2503.01623v1 [cs.HC] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.01623">https://doi.org/10.48550/arXiv.2503.01623</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)
</code></pre>
<p>Related DOI:</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.1145/3706598.3713998">https://doi.org/10.1145/3706598.3713998</a></p>
<p>Focus to learn more</p>
<pre><code>            DOI(s) linking to related resources&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>34. <b>【2503.01622】DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01622">https://arxiv.org/abs/2503.01622</a></p>
  <p><b>作者</b>：Eliya Habba,Ofir Arviv,Itay Itzhak,Yotam Perlitz,Elron Bandel,Leshem Choshen,Michal Shmueli-Scheuer,Gabriel Stanovsky</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Recent work found, answer enumerators, instruction wording, Recent work, type of delimiters</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation.
Browse the data, contribute, and more: this https URL
</p><p>Subjects:</p>
<p>Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)</p>
<p>Cite as:<br>
arXiv:2503.01622 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]</p>
<p>(or<br>
arXiv:2503.01622v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.01622">https://doi.org/10.48550/arXiv.2503.01622</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>35. <b>【2503.01619】Advancing vision-language models in front-end development via data synthesis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01619">https://arxiv.org/abs/2503.01619</a></p>
  <p><b>作者</b>：Tong Ge,Yashu Liu,Jieping Ye,Tianyi Li,Chao Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Modern front-end, presents distinctive challenges, presents distinctive, leveraging the unique, unique features</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Modern front-end (FE) development, especially when leveraging the unique features of frameworks like React and Vue, presents distinctive challenges. These include managing modular architectures, ensuring synchronization between data and visual outputs for declarative rendering, and adapting reusable components to various scenarios. Such complexities make it particularly difficult for state-of-the-art large vision-language models (VLMs) to generate accurate and functional code directly from design images. To address these challenges, we propose a reflective agentic workflow that synthesizes high-quality image-text data to capture the diverse characteristics of FE development. This workflow automates the extraction of self-contained\footnote{A \textbf{self-contained} code snippet is one that encapsulates all necessary logic, styling, and dependencies, ensuring it functions independently without requiring external imports or context.} code snippets from real-world projects, renders the corresponding visual outputs, and generates detailed descriptions that link design elements to functional code. To further expand the scope and utility of the synthesis, we introduce three data synthesis strategies: Evolution-based synthesis, which enables scalable and diverse dataset expansion; Waterfall-Model-based synthesis, which generates logically coherent code derived from system requirements; and Additive Development synthesis, which iteratively increases the complexity of human-authored components. We build a large vision-language model, Flame, trained on the synthesized datasets and demonstrate its effectiveness in generating React code via the $\text{pass}@k$ metric. Our results suggest that a code VLM trained to interpret images before code generation may achieve better performance.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2503.01611】In-context Learning vs. Instruction Tuning: The Case of Small and Multilingual Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01611">https://arxiv.org/abs/2503.01611</a></p>
  <p><b>作者</b>：David Ponce,Thierry Etchegoyhen</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：perform downstream tasks, Large Language Models, ability for Large, Large Language, downstream tasks</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Instruction following is a critical ability for Large Language Models to perform downstream tasks. The standard approach to instruction alignment has relied on a specific phase of model tuning over curated instruction datasets, optionally complemented with an alignment step over human preferences. Recent work has shown the potential of in-context learning (ICL) alternatives to guide base models towards instruction following. This type of approach is particularly relevant to extend instruction following across languages and models of varying sizes adapted to different types of usage. In this work we compare ICL and instruction fine-tuning in English, French and Spanish, on Small Language Models, and provide experimental results on applying Direct Preference Optimisation (DPO) over base models. Our results show that scenarios involving multilingual and smaller models result in downgraded ICL instruction following performance, only partially mitigated by DPO alignment. This study aims to further our understanding of current strengths and limitations of alternative methods for instruction following.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2503.01606】Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01606">https://arxiv.org/abs/2503.01606</a></p>
  <p><b>作者</b>：Zhanghao Hu,Hanqi Yan,Qingling Zhu,Zhenyi Shen,Yulan He,Lin Gui</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：domain question answering, recently pushed open, pushed open domain, open domain question, Large language models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2503.01586】EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and Joint Low-Rank Projection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01586">https://arxiv.org/abs/2503.01586</a></p>
  <p><b>作者</b>：Yuhao Zhou,Sirui Song,Boyang Liu,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Zhihao Zhang,Wei Li,Xuanjing Huang</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Rotary Position Embedding, Rotary Position, Position Embedding, capture multi-frequency information, capture multi-frequency</p>
  <p><b>备注</b>： 13 pages, 8 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Rotary Position Embedding (RoPE) enables each attention head to capture multi-frequency information along the sequence dimension and is widely applied in foundation models. However, the nonlinearity introduced by RoPE complicates optimization of the key state in the Key-Value (KV) cache for RoPE-based attention. Existing KV cache compression methods typically store key state before rotation and apply the transformation during decoding, introducing additional computational overhead. This paper introduces EliteKV, a flexible modification framework for RoPE-based models supporting variable KV cache compression ratios. EliteKV first identifies the intrinsic frequency preference of each head using RoPElite, selectively restoring linearity to certain dimensions of key within attention computation. Building on this, joint low-rank compression of key and value enables partial cache sharing. Experimental results show that with minimal uptraining on only $0.6\%$ of the original training data, RoPE-based models achieve a $75\%$ reduction in KV cache size while preserving performance within a negligible margin. Furthermore, EliteKV consistently performs well across models of different scales within the same family.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2503.01564】Attention Condensation via Sparsity Induced Regularized Training</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01564">https://arxiv.org/abs/2503.01564</a></p>
  <p><b>作者</b>：Eli Sason,Darya Frolova,Boris Nazarov,Felix Goldberd</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：context window expands, self-attention increasingly dominates, transformer inference time, Large Language Models, window expands</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As the context window expands, self-attention increasingly dominates the transformer's inference time. Therefore, accelerating attention computation while minimizing performance degradation is essential for the efficient deployment of Large Language Models (LLMs). In this study we extend a theoretical framework of attention sparsity in LLMs. A customized loss function is designed to enforce the sparsity by restricting the number of top elements in the attention matrix. We perform an initial set of evaluations with GPT-2 to show the effectiveness of our sparsification approach. The attention matrices of the models trained with the proposed loss are both sparse and effective in capturing relevant input dependencies. We now continue working to demonstrate the value of our approach on larger models and different architectures.</p>
  </details>
</details>
<details>
  <summary>40. <b>【2503.01553】Co-creation for Sign Language Processing and Machine Translation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01553">https://arxiv.org/abs/2503.01553</a></p>
  <p><b>作者</b>：Lisa Lepp,Dimitar Shterionov,Mirella De Sisto,Grzegorz Chrupała</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：language machine translation, Sign language machine, Sign language, complex task, machine translation</p>
  <p><b>备注</b>： Submitted to the MDPI special issue "Human and Machine Translation: Recent Trends and Foundations"</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Sign language machine translation (SLMT) -- the task of automatically translating between sign and spoken languages or between sign languages -- is a complex task within the field of NLP. Its multi-modal and non-linear nature require the joint efforts of sign language (SL) linguists, technical experts and SL users. Effective user involvement is a challenge that can be addressed through co-creation. Co-creation has been formally defined in many fields, e.g. business, marketing, educational and others, however in NLP and in particular in SLMT there is no formal, widely accepted definition. Starting from the inception and evolution of co-creation across various fields over time, we develop a relationship typology to address the collaboration between deaf, Hard of Hearing and hearing researchers and the co-creation with SL-users. We compare this new typology to the guiding principles of participatory design for NLP. We, then, assess 110 articles from the perspective of involvement of SL users and highlight the lack of involvement of the sign language community or users in decision-making processes required for effective co-creation. Finally, we derive formal guidelines for co-creation for SLMT which take the dynamic nature of co-creation throughout the life cycle of a research project into account.</p>
  </details>
</details>
<details>
  <summary>41. <b>【2503.01550】None of the Above, Less of the Right: Parallel Patterns between Humans and LLMs on Multi-Choice Questions Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01550">https://arxiv.org/abs/2503.01550</a></p>
  <p><b>作者</b>：Zhi Rui Tam,Cheng-Kuang Wu,Chieh-Yen Lin,Yun-Nung Chen</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：assess true knowledge, Multiple-choice exam questions, existing research suggests, Multiple-choice exam, Large Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multiple-choice exam questions with "None of the above" (NA) options have been extensively studied in educational testing, in which existing research suggests that they better assess true knowledge. However, their impact on Large Language Models (LLMs) evaluation remains underexplored. Through systematic experiments with 28 LLMs on the MMLU benchmark, we examine how NA options affect model performance and confidence calibration. Our analysis reveals that NA options, when used as the correct answer, lead to a consistent 30-50\% performance drop across models regardless of scale--suggesting that LLMs lack the meta-cognitive ability to systematically evaluate and reject all given options when none are correct. This degradation shows strong domain dependence, with minimal impact on mathematical reasoning (14.6\% drop) but severe effects on tasks requiring uncertainty handling like business ethics (48.1\% drop). Our results highlight important implications for benchmark design and raise questions about LLMs' ability to handle uncertainty in real-world applications.</p>
  </details>
</details>
<details>
  <summary>42. <b>【2503.01544】Compositional Reasoning with Transformers, RNNs, and Chain of Thought</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01544">https://arxiv.org/abs/2503.01544</a></p>
  <p><b>作者</b>：Gilad Yehudai,Noah Amsel,Joan Bruna</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Compositional Reasoning Questions, term Compositional Reasoning, Reasoning Questions, Compositional Reasoning, term Compositional</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We study and compare the expressive power of transformers, RNNs, and transformers with chain of thought tokens on a simple and natural class of problems we term Compositional Reasoning Questions (CRQ). This family captures problems like evaluating Boolean formulas and multi-step word problems. Assuming standard hardness assumptions from circuit complexity and communication complexity, we prove that none of these three architectures is capable of solving CRQs unless some hyperparameter (depth, embedding dimension, and number of chain of thought tokens, respectively) grows with the size of the input. We also provide a construction for each architecture that solves CRQs. For transformers, our construction uses depth that is logarithmic in the problem size. For RNNs, logarithmic embedding dimension is necessary and sufficient, so long as the inputs are provided in a certain order. (Otherwise, a linear dimension is necessary). For transformers with chain of thought, our construction uses $n$ CoT tokens. These results show that, while CRQs are inherently hard, there are several different ways for language models to overcome this hardness. Even for a single class of problems, each architecture has strengths and weaknesses, and none is strictly better than the others.</p>
  </details>
</details>
<details>
  <summary>43. <b>【2503.01542】Revisiting Large Language Model Pruning using Neuron Semantic Attribution</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01542">https://arxiv.org/abs/2503.01542</a></p>
  <p><b>作者</b>：Yizhuo Ding,Xinwei Sun,Yanwei Fu,Guosheng Hu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：large language models, Model pruning technique, accelerating large language, language models, computational requirements</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Model pruning technique is vital for accelerating large language models by reducing their size and computational requirements. However, the generalizability of existing pruning methods across diverse datasets and tasks remains unclear. Thus, we conduct extensive evaluations on 24 datasets and 4 tasks using popular pruning methods. Based on these evaluations, we find and then investigate that calibration set greatly affect the performance of pruning methods. In addition, we surprisingly find a significant performance drop of existing pruning methods in sentiment classification tasks. To understand the link between performance drop and pruned neurons, we propose Neuron Semantic Attribution, which learns to associate each neuron with specific semantics. This method first makes the unpruned neurons of LLMs explainable.</p>
  </details>
</details>
<details>
  <summary>44. <b>【2503.01539】Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01539">https://arxiv.org/abs/2503.01539</a></p>
  <p><b>作者</b>：Xi Chen,Shuo Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：language detection techniques, developing toxic language, toxic language detection, detection techniques, toxic language</p>
  <p><b>备注</b>： 12 pages, 4 figures, 2 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rapid development of large language models (LLMs) gives rise to ethical concerns about their performance, while opening new avenues for developing toxic language detection techniques. However, LLMs' unethical output and their capability of detecting toxicity have primarily been tested on language data that do not demand complex meaning inference, such as the biased associations of 'he' with programmer and 'she' with household. Nowadays toxic language adopts a much more creative range of implicit forms, thanks to advanced censorship. In this study, we collect authentic toxic interactions that evade online censorship and that are verified by human annotators as inference intensive. To evaluate and improve LLMs' reasoning of the authentic implicit toxic language, we propose a new prompting method, Pragmatic Inference Chain (PIC), drawn on interdisciplinary findings from cognitive science and linguistics. The PIC prompting significantly improves the success rate of GPT-4o, Llama-3.1-70B-Instruct, and DeepSeek-v2.5 in identifying implicit toxic language, compared to both direct prompting and Chain-of-Thought. In addition, it also facilitates the models to produce more explicit and coherent reasoning processes, hence can potentially be generalized to other inference-intensive tasks, e.g., understanding humour and metaphors.</p>
  </details>
</details>
<details>
  <summary>45. <b>【2503.01513】Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01513">https://arxiv.org/abs/2503.01513</a></p>
  <p><b>作者</b>：Katerina Korre,Dimitris Tsirmpas,Nikos Gkoumas,Emma Cabalé,Dionysis Kontarinis,Danai Myrtzani,Theodoros Evgeniou,Ion Androutsopoulos,John Pavlopoulos</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, potential of Large, Language Models, Large Language, Natural Language Processing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present a survey of methods for assessing and enhancing the quality of online discussions, focusing on the potential of Large Language Models (LLMs). While online discourses aim, at least in theory, to foster mutual understanding, they often devolve into harmful exchanges, such as hate speech, threatening social cohesion and democratic values. Recent advancements in LLMs enable facilitation agents that not only moderate content, but also actively improve the quality of interactions. Our survey synthesizes ideas from Natural Language Processing (NLP) and Social Sciences to provide (a) a new taxonomy on discussion quality evaluation, (b) an overview of intervention and facilitation strategies, along with a new taxonomy on conversation facilitation datasets, (c) an LLM-oriented roadmap of good practices and future research directions, from technological and societal perspectives.</p>
  </details>
</details>
<details>
  <summary>46. <b>【2503.01510】KoWit-24: A Richly Annotated Dataset of Wordplay in News Headlines</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01510">https://arxiv.org/abs/2503.01510</a></p>
  <p><b>作者</b>：Alexander Baranov,Anna Palatkina,Yulia Makovka,Pavel Braslavski</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Russian news headlines, wordplay, fine-grained annotation, Russian, Abstract</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present KoWit-24, a dataset with fine-grained annotation of wordplay in 2,700 Russian news headlines. KoWit-24 annotations include the presence of wordplay, its type, wordplay anchors, and words/phrases the wordplay refers to. Unlike the majority of existing humor collections of canned jokes, KoWit-24 provides wordplay contexts -- each headline is accompanied by the news lead and summary. The most common type of wordplay in the dataset is the transformation of collocations, idioms, and named entities -- the mechanism that has been underrepresented in previous humor datasets. Our experiments with five LLMs show that there is ample room for improvement in wordplay detection and interpretation tasks. The dataset and evaluation scripts are available at this https URL</p>
  </details>
</details>
<details>
  <summary>47. <b>【2503.01506】SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01506">https://arxiv.org/abs/2503.01506</a></p>
  <p><b>作者</b>：Xiangyu Xi,Deyang Kong,Jian Yang,Jiawei Yang,Zhengyu Chen,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, determines domain weights, pretraining data mixing, language models, typically follow</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing pretraining data mixing methods for large language models (LLMs) typically follow a domain-wise methodology, a top-down process that first determines domain weights and then performs uniform data sampling across each domain. However, these approaches neglect significant inter-domain overlaps and commonalities, failing to control the global diversity of the constructed training dataset. Further, uniform sampling within domains ignores fine-grained sample-specific features, potentially leading to suboptimal data distribution. To address these shortcomings, we propose a novel sample-wise data mixture approach based on a bottom-up paradigm. This method performs global cross-domain sampling by systematically evaluating the quality and diversity of each sample, thereby dynamically determining the optimal domain distribution. Comprehensive experiments across multiple downstream tasks and perplexity assessments demonstrate that SampleMix surpasses existing domain-based methods. Meanwhile, SampleMix requires 1.4x to 2.1x training steps to achieves the baselines' performance, highlighting the substantial potential of SampleMix to optimize pre-training data.</p>
  </details>
</details>
<details>
  <summary>48. <b>【2503.01496】Liger: Linearizing Large Language Models to Gated Recurrent Structures</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01496">https://arxiv.org/abs/2503.01496</a></p>
  <p><b>作者</b>：Disen Lan,Weigao Sun,Jiaxi Hu,Jusen Du,Yu Cheng</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：modeling offer linear-time, offer linear-time training, recurrent modeling offer, linear recurrent modeling, linear recurrent</p>
  <p><b>备注</b>： Technical report, 13 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\% of the Transformer-based LLM at 0.02\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>49. <b>【2503.01493】Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01493">https://arxiv.org/abs/2503.01493</a></p>
  <p><b>作者</b>：Fajri Koto,Rituraj Joshi,Nurdaulet Mukhituly,Yuxia Wang,Zhuohan Xie,Rahul Pal,Daniil Orel,Parvez Mullah,Diana Turmakhan,Maiya Goloburda,Mohammed Kamran,Samujjwal Ghosh,Bokang Jia,Jonibek Mansurov,Mukhammed Togmanov,Debopriyo Banerjee,Nurkhan Laiyk,Akhmed Sakip,Xudong Han,Ekaterina Kochmar,Alham Fikri Aji,Aaryamonvikram Singh,Alok Anil Jadhav,Satheesh Katipomu,Samta Kamboj,Monojit Choudhury,Gurpreet Gosal,Gokul Ramakrishnan,Biswajit Mishra,Sarath Chandran,Avraham Sheinin,Natalia Vassilieva,Neha Sengupta,Larry Murray,Preslav Nakov</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：generative large language, open generative large, large language model, generative large, large language</p>
  <p><b>备注</b>： Technical Report</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outperforming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. We release Sherkala-Chat (8B) as an open-weight instruction-tuned model and provide a detailed overview of its training, fine-tuning, safety alignment, and evaluation, aiming to advance research and support diverse real-world applications.</p>
  </details>
</details>
<details>
  <summary>50. <b>【2503.01490】Improving Retrospective Language Agents via Joint Policy Gradient Optimization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01490">https://arxiv.org/abs/2503.01490</a></p>
  <p><b>作者</b>：Xueyang Feng,Bo Lan,Quanyu Dai,Lei Wang,Jiakai Tang,Xu Chen,Zhenhua Dong,Ji-Rong Wen</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：recent research advancements, sparked great interest, creating autonomous agents, large language models, recent research</p>
  <p><b>备注</b>： NAACL2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In recent research advancements within the community, large language models (LLMs) have sparked great interest in creating autonomous agents. However, current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile, although fine-tuning methods significantly enhance the capabilities of smaller LLMs, the fine-tuned agents often lack the potential for self-reflection and self-improvement. To address these challenges, we introduce a novel agent framework named RetroAct, which is a framework that jointly optimizes both task-planning and self-reflective evolution capabilities in language agents. Specifically, we develop a two-stage joint optimization process that integrates imitation learning and reinforcement learning, and design an off-policy joint policy gradient optimization algorithm with imitation learning regularization to enhance the data efficiency and training stability in agent tasks. RetroAct significantly improves the performance of open-source models, reduces dependency on closed-source LLMs, and enables fine-tuned agents to learn and evolve continuously. We conduct extensive experiments across various testing environments, demonstrating RetroAct has substantial improvements in task performance and decision-making processes.</p>
  </details>
</details>
<details>
  <summary>51. <b>【2503.01478】SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01478">https://arxiv.org/abs/2503.01478</a></p>
  <p><b>作者</b>：Lu Dai,Yijie Xu,Jinhui Ye,Hao Liu,Hui Xiong</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, demonstrated improved generation, improved generation performance</p>
  <p><b>备注</b>： ICLR 2025 Spotlight</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose Semantic Perplexity (SePer), a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios.</p>
  </details>
</details>
<details>
  <summary>52. <b>【2503.01464】Rethinking Data: Towards Better Performing Domain-Specific Small Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01464">https://arxiv.org/abs/2503.01464</a></p>
  <p><b>作者</b>：Boris Nazarov,Darya Frolova,Yackov Lubarsky,Alexei Gaissinski,Pavel Kisilev</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models, Fine-tuning of Large, shown significant promise, Large Language, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Fine-tuning of Large Language Models (LLMs) for downstream tasks, performed on domain-specific data has shown significant promise. However, commercial use of such LLMs is limited by the high computational cost required for their deployment at scale. On the other hand, small Language Models (LMs) are much more cost effective but have subpar performance in a similar setup. This paper presents our approach to finetuning a small LM, that reaches high accuracy in multiple choice question answering task. We achieve this by improving data quality at each stage of the LM training pipeline. In particular, we start with data structuring resulting in extraction of compact, semantically meaningful text chunks used by a retriever. This allows more efficient knowledge digestion by the LM. Further, we improve the retrieved context by training a lightweight Chunk Re-Ranker (CRR) that generates more accurate relative relevance chunk scores. Finally, we improve the model generalization ability by merging the models fine-tuned with different parameters on different data subsets. We present detailed procedure descriptions, and corresponding experimental findings that show the improvements of each one of the proposed techniques.</p>
  </details>
</details>
<details>
  <summary>53. <b>【2503.01461】owards Widening The Distillation Bottleneck for Reasoning Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01461">https://arxiv.org/abs/2503.01461</a></p>
  <p><b>作者</b>：Huifeng Yin,Yu Zhao,Minghao Wu,Xuanfan Ni,Bo Zeng,Hao Wang,Tianqi Shi,Liangying Shao,Chenyang Lyu,Longyue Wang,Weihua Luo,Kaifu Zhang</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Reasoning Models, scaling test-time compute, shown remarkable reasoning, remarkable reasoning capabilities, Large Reasoning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning(SFT) and Reinforcement Learning(RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the construted data.</p>
  </details>
</details>
<details>
  <summary>54. <b>【2503.01457】Structural Deep Encoding for Table Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01457">https://arxiv.org/abs/2503.01457</a></p>
  <p><b>作者</b>：Raphaël Mouravieff,Benjamin Piwowarski,Sylvain Lamprier</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Transformers-based architectures excel, Transformers-based architectures, processing textual information, architectures excel, excel at processing</p>
  <p><b>备注</b>： 8 pages, 4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Although Transformers-based architectures excel at processing textual information, their naive adaptation for tabular data often involves flattening the table structure. This simplification can lead to the loss of essential inter-dependencies between rows, columns, and cells, while also posing scalability challenges for large tables. To address these issues, prior works have explored special tokens, structured embeddings, and sparse attention patterns. In this paper, we conduct a comprehensive analysis of tabular encoding techniques, which highlights the crucial role of attention sparsity in preserving structural information of tables. We also introduce a set of novel sparse attention mask designs for tabular data, that not only enhance computational efficiency but also preserve structural integrity, leading to better overall performance.</p>
  </details>
</details>
<details>
  <summary>55. <b>【2503.01453】AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01453">https://arxiv.org/abs/2503.01453</a></p>
  <p><b>作者</b>：Pankaj Choudhury,Yogesh Aggarwal,Prithwijit Guha,Sukumar Nandi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：real-world adoption remains, adoption remains constrained, Neural networks, networks have significantly, significantly advanced</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Neural networks have significantly advanced AI applications, yet their real-world adoption remains constrained by high computational demands, hardware limitations, and accessibility challenges. In image captioning, many state-of-the-art models have achieved impressive performances while relying on resource-intensive architectures. This made them impractical for deployment on resource-constrained devices. This limitation is particularly noticeable for applications involving low-resource languages. We demonstrate the case of image captioning in Assamese language, where lack of effective, scalable systems can restrict the accessibility of AI-based solutions for native Assamese speakers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy visual feature extractors like FasterRCNN with lightweight ShuffleNetv2x1.5. Additionally, Gated Recurrent Units (GRUs) are used as the caption decoder to further reduce computational demands and model parameters. Furthermore, the integration of bilinear attention enhances the model's overall performance. AC-Lite can operate on edge devices, thereby eliminating the need for computation on remote servers. The proposed AC-Lite model achieves 82.3 CIDEr score on the COCO-AC dataset with 1.098 GFLOPs and 25.65M parameters.</p>
  </details>
</details>
<details>
  <summary>56. <b>【2503.01424】From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01424">https://arxiv.org/abs/2503.01424</a></p>
  <p><b>作者</b>：Zekun Zhou,Xiaocheng Feng,Lei Huang,Xiachong Feng,Ziyun Song,Ruihan Chen,Liang Zhao,Weitao Ma,Yuxuan Gu,Baoxin Wang,Dayong Wu,Guoping Hu,Ting Liu,Bing Qin</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：demands substantial time, fundamental process driving, human civilization, demands substantial, substantial time</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Research is a fundamental process driving the advancement of human civilization, yet it demands substantial time and effort from researchers. In recent years, the rapid development of artificial intelligence (AI) technologies has inspired researchers to explore how AI can accelerate and enhance research. To monitor relevant advancements, this paper presents a systematic review of the progress in this domain. Specifically, we organize the relevant studies into three main categories: hypothesis formulation, hypothesis validation, and manuscript publication. Hypothesis formulation involves knowledge synthesis and hypothesis generation. Hypothesis validation includes the verification of scientific claims, theorem proving, and experiment validation. Manuscript publication encompasses manuscript writing and the peer review process. Furthermore, we identify and discuss the current challenges faced in these areas, as well as potential future directions for research. Finally, we also offer a comprehensive overview of existing benchmarks and tools across various domains that support the integration of AI into the research process. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>57. <b>【2503.01422】Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01422">https://arxiv.org/abs/2503.01422</a></p>
  <p><b>作者</b>：Yiming Wang,Pei Zhang,Siyuan Huang,Baosong Yang,Zhuosheng Zhang,Fei Huang,Rui Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：scaling improves large, improves large language, adding extra compute, Test-time scaling improves, improves large</p>
  <p><b>备注</b>： 23 pages, 14 figures, 8 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Test-time scaling improves large language model performance by adding extra compute during decoding. Best-of-N (BoN) sampling serves as a common scaling technique, broadening the search space for finding better solutions from the model distribution. However, traditional BoN requires N full generations, leading to high GPU memory overhead and time latency. Moreover, some methods depend on reward models, adding computational cost and limiting domain generalization.
In this paper, we propose Self-Truncation Best-of-N (ST-BoN), a novel decoding method that avoids fully generating all samplings and eliminates the need for reward models. ST-BoN introduces early sampling consistency to estimate the most promising sample, truncating suboptimal ones to free memory and accelerate inference. This pushes the sampling-efficient test-time scaling. Compared to traditional BoN, ST-BoN can reduce dynamic GPU memory overhead by over 90% and time latency by 50%, while achieving comparable or even better performance across reasoning and open-ended domains.
</p><p>Comments:<br>
23 pages, 14 figures, 8 tables</p>
<p>Subjects:</p>
<p>Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)</p>
<p>Cite as:<br>
arXiv:2503.01422 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]</p>
<p>(or<br>
arXiv:2503.01422v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.01422">https://doi.org/10.48550/arXiv.2503.01422</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>58. <b>【2503.01419】Parameter-Efficient Fine-Tuning of Large Language Models via Deconvolution in Subspace</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01419">https://arxiv.org/abs/2503.01419</a></p>
  <p><b>作者</b>：Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia,Dong-Hai Zhu,Xi-He Qiu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Artificial General Intelligence, achieving Artificial General, Large language model, General Intelligence, Artificial General</p>
  <p><b>备注</b>： COLING 2025 main conference</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language model (LLM) is considered a milestone towards achieving Artificial General Intelligence (AGI). With its advanced emergent capabilities, it adapt to a wide range of specific applications. Fine-tuning LLMs for various downstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA) is well-known for its parameter efficiency. It can reduce the number of parameters needed to fine-tune LLMs by several orders of magnitude. However, LoRA-based approaches encounter a significant limitation due to the bottleneck imposed by rank one decomposition. As the parameters count in LLMs increase, even rank one decomposition might surpass the number of parameters truly necessary for handling more downstream tasks. In this paper, we propose a new method for Parameter-Efficient Fine-Tuning (PEFT) via deconvolution in subspace, dubbed as DCFT. We innovatively use deconvolution to complete details and enhance knowledge in subspace incremental matrices, and dynamically control parameters by adjusting the kernel size, unconstrained by rank-one decomposition. Extensive experiments are conducted to validate the effectiveness of DCFT. Results show that compared to LoRA, DCFT achieve an 8$\times$ reduction in parameters, and still achieves highly impressive performance. Our code is available here: this https URL.</p>
  </details>
</details>
<details>
  <summary>59. <b>【2503.01386】Geo-Semantic-Parsing: AI-powered geoparsing by traversing semantic knowledge graphs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01386">https://arxiv.org/abs/2503.01386</a></p>
  <p><b>作者</b>：Leonardo Nizzoli,Marco Avvenuti,Maurizio Tesconi,Stefano Cresci</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Social and Information Networks (cs.SI)</p>
  <p><b>关键词</b>：Online social networks, social networks convey, networks convey rich, Online social, convey rich information</p>
  <p><b>备注</b>： Postprint of the article published in the Decision Support Systems journal. Please, cite accordingly</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Online social networks convey rich information about geospatial facets of reality. However in most cases, geographic information is not explicit and structured, thus preventing its exploitation in real-time applications. We address this limitation by introducing a novel geoparsing and geotagging technique called Geo-Semantic-Parsing (GSP). GSP identifies location references in free text and extracts the corresponding geographic coordinates. To reach this goal, we employ a semantic annotator to identify relevant portions of the input text and to link them to the corresponding entity in a knowledge graph. Then, we devise and experiment with several efficient strategies for traversing the knowledge graph, thus expanding the available set of information for the geoparsing task. Finally, we exploit all available information for learning a regression model that selects the best entity with which to geotag the input text. We evaluate GSP on a well-known reference dataset including almost 10k event-related tweets, achieving $F1=0.66$. We extensively compare our results with those of 2 baselines and 3 state-of-the-art geoparsing techniques, achieving the best performance. On the same dataset, competitors obtain $F1 \leq 0.55$. We conclude by providing in-depth analyses of our results, showing that the overall superior performance of GSP is mainly due to a large improvement in recall, with respect to existing techniques.</p>
  </details>
</details>
<details>
  <summary>60. <b>【2503.01385】Q-NL Verifier: Leveraging Synthetic Data for Robust Knowledge Graph Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01385">https://arxiv.org/abs/2503.01385</a></p>
  <p><b>作者</b>：Tim Schwabe,Louisa Siebel,Patrik Valach,Maribel Acosta</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Databases (cs.DB); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：aligning user questions, requires accurately aligning, accurately aligning user, Question answering, user questions</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Question answering (QA) requires accurately aligning user questions with structured queries, a process often limited by the scarcity of high-quality query-natural language (Q-NL) pairs. To overcome this, we present Q-NL Verifier, an approach to generating high-quality synthetic pairs of queries and NL translations. Our approach relies on large language models (LLMs) to generate semantically precise natural language paraphrases of structured queries. Building on these synthetic Q-NL pairs, we introduce a learned verifier component that automatically determines whether a generated paraphrase is semantically equivalent to the original query. Our experiments with the well-known LC-QuAD 2.0 benchmark show that Q-NL Verifier generalizes well to paraphrases from other models and even human-authored translations. Our approach strongly aligns with human judgments across varying query complexities and outperforms existing NLP metrics in assessing semantic correctness. We also integrate the verifier into QA pipelines, showing that verifier-filtered synthetic data has significantly higher quality in terms of translation correctness and enhances NL to Q translation accuracy. Lastly, we release an updated version of the LC-QuAD 2.0 benchmark containing our synthetic Q-NL pairs and verifier scores, offering a new resource for robust and scalable QA.</p>
  </details>
</details>
<details>
  <summary>61. <b>【2503.01372】SwiLTra-Bench: The Swiss Legal Translation Benchmark</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01372">https://arxiv.org/abs/2503.01372</a></p>
  <p><b>作者</b>：Joel Niklaus,Jakob Merane,Luka Nenadic,Sina Ahmadi,Yingqiang Gao,Cyrill A. H. Chevalley,Claude Humbel,Christophe Gösken,Lorenzo Tanzi,Thomas Lüthi,Stefan Palombo,Spencer Poff,Boling Yang,Nan Wu,Matthew Guillod,Robin Mamié,Daniel Brunner,Julio Pereyra,Niko Grupen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：uniquely important due, Switzerland legal translation, Switzerland legal, multilingual legal documentation, uniquely important</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.</p>
  </details>
</details>
<details>
  <summary>62. <b>【2503.01346】SRAG: Structured Retrieval-Augmented Generation for Multi-Entity Question Answering over Wikipedia Graph</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01346">https://arxiv.org/abs/2503.01346</a></p>
  <p><b>作者</b>：Teng Lin,Yizhang Zhu,Yuyu Luo,Nan Tang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：poses significant challenges, large language models, consolidate scattered information, Multi-entity question answering, poses significant</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-entity question answering (MEQA) poses significant challenges for large language models (LLMs), which often struggle to consolidate scattered information across multiple documents. An example question might be "What is the distribution of IEEE Fellows among various fields of study?", which requires retrieving information from diverse sources e.g., Wikipedia pages. The effectiveness of current retrieval-augmented generation (RAG) methods is limited by the LLMs' capacity to aggregate insights from numerous pages. To address this gap, this paper introduces a structured RAG (SRAG) framework that systematically organizes extracted entities into relational tables (e.g., tabulating entities with schema columns like "name" and "field of study") and then apply table-based reasoning techniques. Our approach decouples retrieval and reasoning, enabling LLMs to focus on structured data analysis rather than raw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA tasks demonstrate that SRAG significantly outperforms state-of-the-art long-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy. The results underscore the efficacy of structuring unstructured data to enhance LLMs' reasoning capabilities.</p>
  </details>
</details>
<details>
  <summary>63. <b>【2503.01345】Same Question, Different Words: A Latent Adversarial Framework for Prompt Robustness</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01345">https://arxiv.org/abs/2503.01345</a></p>
  <p><b>作者</b>：Tingchen Fu,Fazl Barez</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Insensitivity to semantically-preserving, large language models, semantically-preserving variations, crucial for reliable, reliable behavior</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Insensitivity to semantically-preserving variations of prompts (paraphrases) is crucial for reliable behavior and real-world deployment of large language models. However, language models exhibit significant performance degradation when faced with semantically equivalent but differently phrased prompts, and existing solutions either depend on trial-and-error prompt engineering or require computationally expensive inference-time algorithms. In this study, built on the key insight that worst-case prompts exhibit a drift in embedding space, we present Latent Adversarial Paraphrasing (LAP), a dual-loop adversarial framework: the inner loop trains a learnable perturbation to serve as a "latent continuous paraphrase" while preserving semantics through Lagrangian regulation, and the outer loop optimizes the language model parameters on these perturbations. We conduct extensive experiments to demonstrate the effectiveness of LAP across multiple LLM architectures on the RobustAlpaca benchmark with a 0.5%-4% absolution improvement on worst-case win-rate compared with vanilla supervised fine-tuning.</p>
  </details>
</details>
<details>
  <summary>64. <b>【2503.01332】Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01332">https://arxiv.org/abs/2503.01332</a></p>
  <p><b>作者</b>：Cheng-Kuang Wu,Zhi Rui Tam,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：reliable decision-making language, decision-making language agents, answer or refuse, refuse is crucial, crucial for safe</p>
  <p><b>备注</b>： preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Knowing when to answer or refuse is crucial for safe and reliable decision-making language agents. Although prior work has introduced refusal strategies to boost LMs' reliability, how these models adapt their decisions to different risk levels remains underexplored. We formalize the task of risk-aware decision-making, expose critical weaknesses in existing LMs, and propose skill-decomposition solutions to mitigate them. Our findings show that even cutting-edge LMs--both regular and reasoning models--still require explicit prompt chaining to handle the task effectively, revealing the challenges that must be overcome to achieve truly autonomous decision-making agents.</p>
  </details>
</details>
<details>
  <summary>65. <b>【2503.01330】WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01330">https://arxiv.org/abs/2503.01330</a></p>
  <p><b>作者</b>：Jian Yuan,Ziwei He,Haoli Bai,Jingwen Leng,Bo Jiang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, textit, reduce redundant computation, Large Language, Language Models</p>
  <p><b>备注</b>： Accepted by ICASSP 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) use key-value (KV) cache to reduce redundant computation in autoregressive generation. However, the KV cache size increases linearly during generation, leading to excessive memory usage, especially for long texts. Most KV cache compression methods evict the unimportant KV pairs to maintain a fixed cache size, which leads to the permanent loss of tokens during generation. However, singular value decomposition shows that \textit{values} do not exhibit a strong low-rank property as \textit{keys} do, suggesting that information is distributed more evenly across \textit{values}, in contrast to its more redundant distribution within \textit{keys}. Therefore, methods that evict both \textit{keys} and \textit{values} risk losing crucial information and compromise context integrity, ultimately degrading the output quality. To address this problem, we propose WeightedKV, a novel, training-free approach that discards the \textit{keys} of less important tokens, while merging their \textit{values} into neighboring tokens via a convex combination weighted by their average attention scores. In this way, the retained \textit{keys} serve as anchors that guide the generation process, while the merged \textit{values} provide a rich contextual backdrop. We assess our method on four widely used language modeling datasets, demonstrating superior performance compared to all baseline methods, particularly with a lower budget ratio.</p>
  </details>
</details>
<details>
  <summary>66. <b>【2503.01315】Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01315">https://arxiv.org/abs/2503.01315</a></p>
  <p><b>作者</b>：Linhai Zhang,Ziyang Gao,Deyu Zhou,Yulan He</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：mental health disorder, widespread mental health, health disorder, mental health, depression Detection</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Depression is a widespread mental health disorder, and clinical interviews are the gold standard for assessment. However, their reliance on scarce professionals highlights the need for automated detection. Current systems mainly employ black-box neural networks, which lack interpretability, which is crucial in mental health contexts. Some attempts to improve interpretability use post-hoc LLM generation but suffer from hallucination. To address these limitations, we propose RED, a Retrieval-augmented generation framework for Explainable depression Detection. RED retrieves evidence from clinical interview transcripts, providing explanations for predictions. Traditional query-based retrieval systems use a one-size-fits-all approach, which may not be optimal for depression detection, as user backgrounds and situations vary. We introduce a personalized query generation module that combines standard queries with user-specific background inferred by LLMs, tailoring retrieval to individual contexts. Additionally, to enhance LLM performance in social intelligence, we augment LLMs by retrieving relevant knowledge from a social intelligence datastore using an event-centric retriever. Experimental results on the real-world benchmark demonstrate RED's effectiveness compared to neural networks and LLM-based baselines.</p>
  </details>
</details>
<details>
  <summary>67. <b>【2503.01307】Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01307">https://arxiv.org/abs/2503.01307</a></p>
  <p><b>作者</b>：Kanishk Gandhi,Ayush Chakravarthy,Anikait Singh,Nathan Lile,Noah D. Goodman</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Test-time inference, complex challenges, inference has emerged, powerful paradigm, paradigm for enabling</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.</p>
  </details>
</details>
<details>
  <summary>68. <b>【2503.01303】PROPER: A Progressive Learning Framework for Personalized Large Language Models with Group-Level Adaptation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01303">https://arxiv.org/abs/2503.01303</a></p>
  <p><b>作者</b>：Linhai Zhang,Jialong Wu,Deyu Zhou,Yulan He</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Personalized large language, large language models, aim to tailor, Personalized large, large language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Personalized large language models (LLMs) aim to tailor their outputs to user preferences. Recent advances in parameter-efficient fine-tuning (PEFT) methods have highlighted the effectiveness of adapting population-level LLMs to personalized LLMs by fine-tuning user-specific parameters with user history. However, user data is typically sparse, making it challenging to adapt LLMs to specific user patterns. To address this challenge, we propose PROgressive PERsonalization (PROPER), a novel progressive learning framework inspired by meso-level theory in social science. PROPER bridges population-level and user-level models by grouping users based on preferences and adapting LLMs in stages. It combines a Mixture-of-Experts (MoE) structure with Low Ranked Adaptation (LoRA), using a user-aware router to assign users to appropriate groups automatically. Additionally, a LoRA-aware router is proposed to facilitate the integration of individual user LoRAs with group-level LoRAs. Experimental results show that PROPER significantly outperforms SOTA models across multiple tasks, demonstrating the effectiveness of our approach.</p>
  </details>
</details>
<details>
  <summary>69. <b>【2503.01302】Causal Tree Extraction from Medical Case Reports: A Novel Task for Experts-like Text Comprehension</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01302">https://arxiv.org/abs/2503.01302</a></p>
  <p><b>作者</b>：Sakiko Yahata,Zhen Wan,Fei Cheng,Sadao Kurohashi,Hisahiko Sato,Ryozo Nagai</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Extracting causal relationships, Extracting causal, diagnostic process, causal relationships, Causal Tree Extraction</p>
  <p><b>备注</b>： Work in progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Extracting causal relationships from a medical case report is essential for comprehending the case, particularly its diagnostic process. Since the diagnostic process is regarded as a bottom-up inference, causal relationships in cases naturally form a multi-layered tree structure. The existing tasks, such as medical relation extraction, are insufficient for capturing the causal relationships of an entire case, as they treat all relations equally without considering the hierarchical structure inherent in the diagnostic process. Thus, we propose a novel task, Causal Tree Extraction (CTE), which receives a case report and generates a causal tree with the primary disease as the root, providing an intuitive understanding of a case's diagnostic process. Subsequently, we construct a Japanese case report CTE dataset, J-Casemap, propose a generation-based CTE method that outperforms the baseline by 20.2 points in the human evaluation, and introduce evaluation metrics that reflect clinician preferences. Further experiments also show that J-Casemap enhances the performance of solving other medical tasks, such as question answering.</p>
  </details>
</details>
<details>
  <summary>70. <b>【2503.01275】Enhancing Non-English Capabilities of English-Centric Large Language Models through Deep Supervision Fine-Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01275">https://arxiv.org/abs/2503.01275</a></p>
  <p><b>作者</b>：Wenshuai Huo,Xiaocheng Feng,Yichong Huang,Chengpeng Fu,Baohang Li,Yangfan Ye,Zhirui Zhang,Dandan Tu,Duyu Tang,Yunfei Lu,Hui Wang,Bing Qin</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：demonstrated significant progress, multilingual language understanding, understanding and generation, demonstrated significant, significant progress</p>
  <p><b>备注</b>： Accepted at AAAI 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have demonstrated significant progress in multilingual language understanding and generation. However, due to the imbalance in training data, their capabilities in non-English languages are limited. Recent studies revealed the English-pivot multilingual mechanism of LLMs, where LLMs implicitly convert non-English queries into English ones at the bottom layers and adopt English for thinking at the middle layers. However, due to the absence of explicit supervision for cross-lingual alignment in the intermediate layers of LLMs, the internal representations during these stages may become inaccurate. In this work, we introduce a deep supervision fine-tuning method (DFT) that incorporates additional supervision in the internal layers of the model to guide its workflow. Specifically, we introduce two training objectives on different layers of LLMs: one at the bottom layers to constrain the conversion of the target language into English, and another at the middle layers to constrain reasoning in English. To effectively achieve the guiding purpose, we designed two types of supervision signals: logits and feature, which represent a stricter constraint and a relatively more relaxed guidance. Our method guides the model to not only consider the final generated result when processing non-English inputs but also ensure the accuracy of internal representations. We conducted extensive experiments on typical English-centric large models, LLaMA-2 and Gemma-2, and the results on multiple multilingual datasets show that our method significantly outperforms traditional fine-tuning methods.</p>
  </details>
</details>
<details>
  <summary>71. <b>【2503.01269】ChatGPT for President! Presupposed content in politicians versus GPT-generated texts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01269">https://arxiv.org/abs/2503.01269</a></p>
  <p><b>作者</b>：Davide Garassino,Nicola Brocca,Viviana Masia</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Computers and Society (cs.CY)</p>
  <p><b>关键词</b>：replicate linguistic strategies, capability to replicate, replicate linguistic, large language models, manipulative language generation</p>
  <p><b>备注</b>： 36 pages, 6 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study examines ChatGPT-4's capability to replicate linguistic strategies used in political discourse, focusing on its potential for manipulative language generation. As large language models become increasingly popular for text generation, concerns have grown regarding their role in spreading fake news and propaganda. This research compares real political speeches with those generated by ChatGPT, emphasizing presuppositions (a rhetorical device that subtly influences audiences by packaging some content as already known at the moment of utterance, thus swaying opinions without explicit argumentation). Using a corpus-based pragmatic analysis, this study assesses how well ChatGPT can mimic these persuasive strategies. The findings reveal that although ChatGPT-generated texts contain many manipulative presuppositions, key differences emerge in their frequency, form, and function compared with those of politicians. For instance, ChatGPT often relies on change-of-state verbs used in fixed phrases, whereas politicians use presupposition triggers in more varied and creative ways. Such differences, however, are challenging to detect with the naked eye, underscoring the potential risks posed by large language models in political and public this http URL a corpus-based pragmatic analysis, this study assesses how well ChatGPT can mimic these persuasive strategies. The findings reveal that although ChatGPT-generated texts contain many manipulative presuppositions, key differences emerge in their frequency, form, and function compared with those of politicians. For instance, ChatGPT often relies on change-of-state verbs used in fixed phrases, whereas politicians use presupposition triggers in more varied and creative ways. Such differences, however, are challenging to detect with the naked eye, underscoring the potential risks posed by large language models in political and public discourse.</p>
  </details>
</details>
<details>
  <summary>72. <b>【2503.01263】Generalizable Prompt Learning of CLIP: A Brief Overview</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01263">https://arxiv.org/abs/2503.01263</a></p>
  <p><b>作者</b>：Fangming Cui,Yonggang Zhang,Xuan Wang,Xule Wang,Liang Xiao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Existing vision-language models, Existing vision-language, showcased an impressive, impressive capability, capability to generalize</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing vision-language models (VLMs) such as CLIP have showcased an impressive capability to generalize well across various downstream tasks. These models leverage the synergy between visual and textual information, enabling them to understand and reason about the content present in images and text in a unified manner. This article provides a brief overview of CLIP based on few-shot prompt learning, including experimental data and technical characteristics of some methods. The purpose of this review is to provide a reference for researchers who have just started their research in generalizable prompting of CLIP through few-shot training for classification across 15 datasets and also to facilitate the integration of this field by researchers in other downstream tasks.</p>
  </details>
</details>
<details>
  <summary>73. <b>【2503.01235】Your Model is Overconfident, and Other Lies We Tell Ourselves</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01235">https://arxiv.org/abs/2503.01235</a></p>
  <p><b>作者</b>：Timothee Mickus,Aman Sinha,Raúl Vázquez</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：neural NLP models, inherent ambiguity, evaluating neural NLP, overlooked factor, neural NLP</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The difficulty intrinsic to a given example, rooted in its inherent ambiguity, is a key yet often overlooked factor in evaluating neural NLP models. We investigate the interplay and divergence among various metrics for assessing intrinsic difficulty, including annotator dissensus, training dynamics, and model confidence. Through a comprehensive analysis using 29 models on three datasets, we reveal that while correlations exist among these metrics, their relationships are neither linear nor monotonic. By disentangling these dimensions of uncertainty, we aim to refine our understanding of data complexity and its implications for evaluating and improving NLP models.</p>
  </details>
</details>
<details>
  <summary>74. <b>【2503.01233】PEO: Improving Bi-Factorial Preference Alignment with Post-Training Policy Extrapolation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01233">https://arxiv.org/abs/2503.01233</a></p>
  <p><b>作者</b>：Yuxuan Liu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, balancing conflicting objectives, helpfulness and harmlessness, large language, language models</p>
  <p><b>备注</b>： Technical report, work in progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The alignment of large language models with human values presents a critical challenge, particularly when balancing conflicting objectives like helpfulness and harmlessness. Existing approaches, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), face notable limitations: RLHF suffers from instability and inefficiency in multi-objective optimization, while DPO lacks mechanisms for dynamic trade-offs. To address these challenges, we propose Post-Training Extrapolation Optimization (PEO), a novel and efficient framework for bi-factorial alignment. PEO generates a family of Pareto-optimal policies in a single training pass by leveraging a three-phase pipeline: (1) aspect-specific learning, (2) generalist initialization via interpolation, and (3) post-training optimization via extrapolation. PEO enables dynamic adaptation to diverse user preferences at inference time without retraining. Our comprehensive experiments across multiple LLMs demonstrate that PEO achieves superior Pareto fronts compared to baselines, offering improved flexibility and computational efficiency. Theoretical analyses further highlight PEO's capacity to overcome optimization bottlenecks, paving the way for scalable, personalized alignment.</p>
  </details>
</details>
<details>
  <summary>75. <b>【2503.01222】Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01222">https://arxiv.org/abs/2503.01222</a></p>
  <p><b>作者</b>：Wenbin Wang,Yongcheng Jing,Liang Ding,Yingjie Wang,Li Shen,Yong Luo,Bo Du,Dacheng Tao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：multimodal large language, large language models, image perception remains, remains a key, multimodal large</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:High-resolution (HR) image perception remains a key challenge in multimodal large language models (MLLMs). To overcome the limitations of existing methods, this paper shifts away from prior dedicated heuristic approaches and revisits the most fundamental idea to HR perception by enhancing the long-context capability of MLLMs, driven by recent advances in long-context techniques like retrieval-augmented generation (RAG) for general LLMs. Towards this end, this paper presents the first study exploring the use of RAG to address HR perception challenges. Specifically, we propose Retrieval-Augmented Perception (RAP), a training-free framework that retrieves and fuses relevant image crops while preserving spatial context using the proposed Spatial-Awareness Layout. To accommodate different tasks, the proposed Retrieved-Exploration Search (RE-Search) dynamically selects the optimal number of crops based on model confidence and retrieval scores. Experimental results on HR benchmarks demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving a 43% improvement on $V^*$ Bench and 19% on HR-Bench.</p>
  </details>
</details>
<details>
  <summary>76. <b>【2503.01217】HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01217">https://arxiv.org/abs/2503.01217</a></p>
  <p><b>作者</b>：Sijin Sun,Ming Deng,Xinrui Yu,Liangbin Zhao</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Named Entity Recognition, Chinese Named Entity, Incorrect boundary division, complex semantic representation, Entity Recognition</p>
  <p><b>备注</b>： 18 pages, 10 figures, under Review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Incorrect boundary division, complex semantic representation, and differences in pronunciation and meaning often lead to errors in Chinese Named Entity Recognition(CNER). To address these issues, this paper proposes HREB-CRF framework: Hierarchical Reduced-bias EMA with CRF. The proposed method amplifies word boundaries and pools long text gradients through exponentially fixed-bias weighted average of local and global hierarchical attention. Experimental results on the MSRA, Resume, and Weibo datasets show excellent in F1, outperforming the baseline model by 1.1\%, 1.6\%, and 9.8\%. The significant improvement in F1 shows evidences of strong effectiveness and robustness of approach in CNER tasks.</p>
  </details>
</details>
<details>
  <summary>77. <b>【2503.01208】Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01208">https://arxiv.org/abs/2503.01208</a></p>
  <p><b>作者</b>：Tianjie Ju,Yi Hua,Hao Fei,Zhenyu Shao,Yubin Zheng,Haodong Zhao,Mong-Li Lee,Wynne Hsu,Zhuosheng Zhang,Gongshen Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Visual Question Answering, Multi-Modal Large Language, Language Models, Question Answering</p>
  <p><b>备注</b>： Working in progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>78. <b>【2503.01194】Cancer Type, Stage and Prognosis Assessment from Pathology Reports using LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01194">https://arxiv.org/abs/2503.01194</a></p>
  <p><b>作者</b>：Rachit Saluja,Jacob Rosenthal,Yoav Artzi,David J. Pisapia,Benjamin L. Liechty,Mert R. Sabuncu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：shown significant promise, Large Language Models, natural language processing, Large Language, language processing tasks</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have shown significant promise across various natural language processing tasks. However, their application in the field of pathology, particularly for extracting meaningful insights from unstructured medical texts such as pathology reports, remains underexplored and not well quantified. In this project, we leverage state-of-the-art language models, including the GPT family, Mistral models, and the open-source Llama models, to evaluate their performance in comprehensively analyzing pathology reports. Specifically, we assess their performance in cancer type identification, AJCC stage determination, and prognosis assessment, encompassing both information extraction and higher-order reasoning tasks. Based on a detailed analysis of their performance metrics in a zero-shot setting, we developed two instruction-tuned models: Path-llama3.1-8B and Path-GPT-4o-mini-FT. These models demonstrated superior performance in zero-shot cancer type identification, staging, and prognosis assessment compared to the other models evaluated.</p>
  </details>
</details>
<details>
  <summary>79. <b>【2503.01174】alking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01174">https://arxiv.org/abs/2503.01174</a></p>
  <p><b>作者</b>：Siddhant Arora,Zhiyun Lu,Chung-Cheng Chiu,Ruoming Pang,Shinji Watanabe</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：turn-taking events, audio foundation models, recent wave, perform turn-taking events, audio FMs</p>
  <p><b>备注</b>： Accepted at ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.</p>
  </details>
</details>
<details>
  <summary>80. <b>【2503.01163】Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01163">https://arxiv.org/abs/2503.01163</a></p>
  <p><b>作者</b>：Rin Ashizawa,Yoichi Hirose,Nozomu Yoshinari,Kento Uchida,Shinichi Shirakawa</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)</p>
  <p><b>关键词</b>：large language models, Prompt design strategies, Prompt design, Prompt, Prompt optimization</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Prompt optimization aims to search for effective prompts that enhance the performance of large language models (LLMs). Although existing prompt optimization methods have discovered effective prompts, they often differ from sophisticated prompts carefully designed by human experts. Prompt design strategies, representing best practices for improving prompt performance, can be key to improving prompt optimization. Recently, a method termed the Autonomous Prompt Engineering Toolbox (APET) has incorporated various prompt design strategies into the prompt optimization process. In APET, the LLM is needed to implicitly select and apply the appropriate strategies because prompt design strategies can have negative effects. This implicit selection may be suboptimal due to the limited optimization capabilities of LLMs. This paper introduces Optimizing Prompts with sTrategy Selection (OPTS), which implements explicit selection mechanisms for prompt design. We propose three mechanisms, including a Thompson sampling-based approach, and integrate them into EvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for two LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench Hard. Our results show that the selection of prompt design strategies improves the performance of EvoPrompt, and the Thompson sampling-based mechanism achieves the best overall results. Our experimental code is provided at this https URL .</p>
  </details>
</details>
<details>
  <summary>81. <b>【2503.01159】Large Language Models for Healthcare Text Classification: A Systematic Review</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01159">https://arxiv.org/abs/2503.01159</a></p>
  <p><b>作者</b>：Hajar Sakai,Sarah S. Lam</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Natural Language Processing, Large Language, Language Models, Language Processing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have fundamentally transformed approaches to Natural Language Processing (NLP) tasks across diverse domains. In healthcare, accurate and cost-efficient text classification is crucial, whether for clinical notes analysis, diagnosis coding, or any other task, and LLMs present promising potential. Text classification has always faced multiple challenges, including manual annotation for training, handling imbalanced data, and developing scalable approaches. With healthcare, additional challenges are added, particularly the critical need to preserve patients' data privacy and the complexity of the medical terminology. Numerous studies have been conducted to leverage LLMs for automated healthcare text classification and contrast the results with existing machine learning-based methods where embedding, annotation, and training are traditionally required. Existing systematic reviews about LLMs either do not specialize in text classification or do not focus on the healthcare domain. This research synthesizes and critically evaluates the current evidence found in the literature regarding the use of LLMs for text classification in a healthcare setting. Major databases (e.g., Google Scholar, Scopus, PubMed, Science Direct) and other resources were queried, which focused on the papers published between 2018 and 2024 within the framework of PRISMA guidelines, which resulted in 65 eligible research articles. These were categorized by text classification type (e.g., binary classification, multi-label classification), application (e.g., clinical decision support, public health and opinion analysis), methodology, type of healthcare text, and metrics used for evaluation and validation. This review reveals the existing gaps in the literature and suggests future research lines that can be investigated and explored.</p>
  </details>
</details>
<details>
  <summary>82. <b>【2503.01155】Nature-Inspired Population-Based Evolution of Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01155">https://arxiv.org/abs/2503.01155</a></p>
  <p><b>作者</b>：Yiqun Zhang,Peng Ye,Xiaocui Yang,Shi Feng,Shufei Zhang,Lei Bai,Wanli Ouyang,Shuyue Hu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：life on Earth, survival and growth, growth of life, population-based process, LLMs</p>
  <p><b>备注</b>： preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Evolution, the engine behind the survival and growth of life on Earth, operates through the population-based process of reproduction. Inspired by this principle, this paper formally defines a newly emerging problem -- the population-based evolution of large language models (LLMs) -- and introduces a novel framework. Starting with a population of parent LLMs, our framework enables the population to evolve through four key operations: (i) crossover, merging the weights of different parents to create offspring LLMs, (ii) mutation, introducing small, random changes to model weights to foster diversity, (iii) selection, prioritizing high-performing models, and (iv) succession, transferring the learned experience from parent to offspring LLMs. With only 200 samples per new task, the LLM population evolves rapidly to adapt to the task at hand, without any gradients. Experiments on 12 datasets show that our framework consistently outperforms existing multi-LLM merging and adaptation methods, achieving accuracy gains of up to 54.8% over the best LLM in the initial population. Moreover, our framework allows for the evolution of LLMs across multiple new tasks simultaneously, scaling effectively with populations of up to 40 LLMs, and even zero-shot generalization to unseen held-out tasks. We have open-sourced the code on GitHub and released the weights of 10 parent LLMs, fine-tuned from gemma-2-2b-it, on HuggingFace$, enabling reproduction of our proposed framework using just a single 4090 GPU with 24GB memory, without any performance degradation.</p>
  </details>
</details>
<details>
  <summary>83. <b>【2503.01151】ReaderLM-v2: Small Language Model for HTML to Markdown and JSON</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01151">https://arxiv.org/abs/2503.01151</a></p>
  <p><b>作者</b>：Feng Wang,Zesheng Shi,Bo Wang,Nan Wang,Han Xiao</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：billion parameter language, web content extraction, efficient web content, billion parameter, parameter language model</p>
  <p><b>备注</b>： 9 pages, 10-12 refs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present ReaderLM-v2, a compact 1.5 billion parameter language model designed for efficient web content extraction. Our model processes documents up to 512K tokens, transforming messy HTML into clean Markdown or JSON formats with high accuracy -- making it an ideal tool for grounding large language models. The model's effectiveness results from two key innovations: (1) a three-stage data synthesis pipeline that generates high quality, diverse training data by iteratively drafting, refining, and critiquing web content extraction; and (2) a unified training framework combining continuous pre-training with multi-objective optimization. Intensive evaluation demonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger models by 15-20\% on carefully curated benchmarks, particularly excelling at documents exceeding 100K tokens, while maintaining significantly lower computational requirements.</p>
  </details>
</details>
<details>
  <summary>84. <b>【2503.01150】MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01150">https://arxiv.org/abs/2503.01150</a></p>
  <p><b>作者</b>：Chen Zhang,Mingxu Tao,Zhiyuan Liao,Yansong Feng</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large language models, communities in China, excel in high-resource, Large language, struggle with low-resource</p>
  <p><b>备注</b>： Code and data available at [this https URL](https://github.com/luciusssss/MiLiC-Eval) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) excel in high-resource languages but struggle with low-resource languages (LRLs), particularly those spoken by minority communities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To systematically track the progress in these languages, we introduce MiLiC-Eval, a benchmark designed for minority languages in China, featuring 24K instances across 9 tasks. MiLiC-Eval focuses on underrepresented writing systems and provides a fine-grained assessment of linguistic and problem-solving skills. Our evaluation reveals that LLMs perform poorly on syntax-intensive tasks and multi-script languages. We further demonstrate how MiLiC-Eval can help advance LRL research in handling diverse writing systems and understanding the process of language adaptation.</p>
  </details>
</details>
<details>
  <summary>85. <b>【2503.01141】How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01141">https://arxiv.org/abs/2503.01141</a></p>
  <p><b>作者</b>：Ayeong Lee,Ethan Che,Tianyi Peng</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：enabling large language, large language models, solve complex reasoning, powerful technique, technique for enabling</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Chain-of-thought prompting has emerged as a powerful technique for enabling large language models (LLMs) to solve complex reasoning tasks. However, these reasoning chains can be verbose, raising concerns about efficiency. In response, recent works have sought to decrease response lengths through simple prompting strategies (e.g. 'be concise'). In this work, we conduct the first systematic study of the relationship between reasoning length and model performance across a diverse range of compression instructions (e.g. 'use 10 words or less' or 'remove all punctuation'). In doing so, we discover a universal tradeoff between reasoning length and accuracy that persists across even very distinct reasoning chains. We demonstrate that this tradeoff emerges from a sharp threshold behavior at the question level: each task has an intrinsic 'token complexity' - a minimal number of tokens required for successful problem-solving. We show how token complexity enables us to compute information-theoretic limits on the accuracy-compression tradeoff, and find that prompt-based compression strategies operate far from these theoretical limits. This suggests there may be significant room for improvement and our framework provides a benchmark to help researchers evaluate progress in reasoning efficiency. Our work also highlights the importance of adaptive compression -- giving shorter responses for easier questions -- and we show that token complexity is a useful tool for measuring this capability.</p>
  </details>
</details>
<details>
  <summary>86. <b>【2503.01131】Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01131">https://arxiv.org/abs/2503.01131</a></p>
  <p><b>作者</b>：Shivam Ratnakar,Abhiroop Talasila,Raghav Chamadiya,Nikhil Agarwal,Vinayak K Doifode</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, facts into Large, focusing on improving</p>
  <p><b>备注</b>： Presented at the Workshop on Preparing Good Data for Generative AI: Challenges and Approaches (Good-Data) in conjunction with AAAI 2025. The authors retain the copyright</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper presents an extensive examination of Parameter-Efficient Fine-Tuning (PEFT) for embedding domain specific facts into Large Language Models (LLMs), focusing on improving the fine-tuning process by categorizing question-answer (QA) pairs into Factual and Conceptual classes using a BERT-based classifier. Two distinct Llama-2 models are fine-tuned based on these classifications and evaluated using larger models like GPT-3.5 Turbo and Gemini. Our results indicate that models trained on conceptual datasets outperform those trained on factual datasets. Additionally, we compare the efficiency of two synthetic fine-tuning dataset generation techniques, D-RAG and D-Naive, with D-Naive demonstrating superior performance. Although PEFT has shown effectiveness, our research indicates that it may not be the most optimal method for embedding facts into LLMs. However, it has demonstrated exceptional performance in instruction-based tasks. Our findings are reinforced by a 1000-sample dataset in the data center domain, where the fine-tuned Llama-2 7B model significantly outperforms the baseline model in generating product recommendations. Our study highlights the importance of QA pair categorization and synthetic dataset generation techniques in enhancing the performance of LLMs in specific domains.</p>
  </details>
</details>
<details>
  <summary>87. <b>【2503.01098】SolBench: A Dataset and Benchmark for Evaluating Functional Correctness in Solidity Code Completion and Repair</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01098">https://arxiv.org/abs/2503.01098</a></p>
  <p><b>作者</b>：Zaoyu Chen,Haoran Qin,Nuo Chen,Xiangyu Zhao,Lei Xue,Xiapu Luo,Xiao-Ming Wu</p>
  <p><b>类目</b>：oftware Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：immutability post-deployment makes, post-deployment makes functional, functional correctness vital, functional correctness, programs on blockchains</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Smart contracts are crucial programs on blockchains, and their immutability post-deployment makes functional correctness vital. Despite progress in code completion models, benchmarks for Solidity, the primary smart contract language, are lacking. Existing metrics like BLEU do not adequately assess the functional correctness of generated smart contracts. To fill this gap, we introduce SolBench, a benchmark for evaluating the functional correctness of Solidity smart contracts generated by code completion models. SolBench includes 4,178 functions from 1,155 Ethereum-deployed contracts. Testing advanced models revealed challenges in generating correct code without context, as Solidity functions rely on context-defined variables and interfaces. To address this, we propose a Retrieval-Augmented Code Repair framework. In this framework, an executor verifies functional correctness, and if necessary, an LLM repairs the code using retrieved snippets informed by executor traces. We conduct a comprehensive evaluation of both closed-source and open-source LLMs across various model sizes and series to assess their performance in smart contract completion. The results show that code repair and retrieval techniques effectively enhance the correctness of smart contract completion while reducing computational costs.</p>
  </details>
</details>
<details>
  <summary>88. <b>【2503.01090】Precise Localization of Memories: A Fine-grained Neuron-level Knowledge Editing Technique for LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01090">https://arxiv.org/abs/2503.01090</a></p>
  <p><b>作者</b>：Haowen Pan,Xiaozhi Wang,Yixin Cao,Zenglin Shi,Xun Yang,Juanzi Li,Meng Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, update outdated information, information in Large</p>
  <p><b>备注</b>： ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Knowledge editing aims to update outdated information in Large Language Models (LLMs). A representative line of study is locate-then-edit methods, which typically employ causal tracing to identify the modules responsible for recalling factual knowledge about entities. However, we find these methods are often sensitive only to changes in the subject entity, leaving them less effective at adapting to changes in relations. This limitation results in poor editing locality, which can lead to the persistence of irrelevant or inaccurate facts, ultimately compromising the reliability of LLMs. We believe this issue arises from the insufficient precision of knowledge localization. To address this, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method that enhances editing locality without affecting overall success rates. By precisely identifying and modifying specific neurons within feed-forward networks, FiNE significantly improves knowledge localization and editing. Quantitative experiments demonstrate that FiNE efficiently achieves better overall performance compared to existing techniques, providing new insights into the localization and modification of knowledge within LLMs.</p>
  </details>
</details>
<details>
  <summary>89. <b>【2503.01082】Efficient or Powerful? Trade-offs Between Machine Learning and Deep Learning for Mental Illness Detection on Social Media</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01082">https://arxiv.org/abs/2503.01082</a></p>
  <p><b>作者</b>：Zhanyi Ding,Zhongyan Wang,Yeyubei Zhang,Yuchen Cao,Yunchong Liu,Xiaorui Shen,Yexin Tian,Jianglai Dai</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Social media platforms, mental health conditions, Social media, mental health trends, mental health</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Social media platforms provide valuable insights into mental health trends by capturing user-generated discussions on conditions such as depression, anxiety, and suicidal ideation. Machine learning (ML) and deep learning (DL) models have been increasingly applied to classify mental health conditions from textual data, but selecting the most effective model involves trade-offs in accuracy, interpretability, and computational efficiency. This study evaluates multiple ML models, including logistic regression, random forest, and LightGBM, alongside deep learning architectures such as ALBERT and Gated Recurrent Units (GRUs), for both binary and multi-class classification of mental health conditions. Our findings indicate that ML and DL models achieve comparable classification performance on medium-sized datasets, with ML models offering greater interpretability through variable importance scores, while DL models are more robust to complex linguistic patterns. Additionally, ML models require explicit feature engineering, whereas DL models learn hierarchical representations directly from text. Logistic regression provides the advantage of capturing both positive and negative associations between features and mental health conditions, whereas tree-based models prioritize decision-making power through split-based feature selection. This study offers empirical insights into the advantages and limitations of different modeling approaches and provides recommendations for selecting appropriate methods based on dataset size, interpretability needs, and computational constraints.</p>
  </details>
</details>
<details>
  <summary>90. <b>【2503.01066】Alchemist: Towards the Design of Efficient Online Continual Learning System</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01066">https://arxiv.org/abs/2503.01066</a></p>
  <p><b>作者</b>：Yuyang Huang,Yuhan Liu,Haryadi S. Gunawi,Beibin Li,Changho Hwang</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC)</p>
  <p><b>关键词</b>：leveraging user feedback, refine models incrementally, online continual learning, enhancing model performance, Continual learning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Continual learning has emerged as a promising solution to refine models incrementally by leveraging user feedback, thereby enhancing model performance in applications like code completion, personal assistants, and chat interfaces. In particular, online continual learning - iteratively training the model with small batches of user feedback - has demonstrated notable performance improvements. However, the existing practice of segregating training and serving processes forces the online trainer to recompute the intermediate results already done during serving. Such redundant computations can account for 30%-42% of total training time.
In this paper, we propose Alchemist, to the best of our knowledge, the first online continual learning system that efficiently reuses intermediate results computed during serving to reduce redundant computation with minimal impact on the serving latency or capacity. Alchemist introduces two key techniques: (1) minimal activations recording and saving during serving, where activations are recorded and saved only during the prefill phase to minimize overhead; and (2) offloading of serving activations, which dynamically manages GPU memory by freeing activations in the forward order, while reloading them in the backward order during the backward pass. Evaluations with the ShareGPT dataset show that compared with a separate training cluster, Alchemist significantly increases training throughput by up to 1.72x, reduces up to 47% memory usage during training, and supports up to 2x more training tokens - all while maintaining negligible impact on serving latency.
</p><p>Subjects:</p>
<p>Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Distributed, Parallel, and Cluster Computing (cs.DC)</p>
<p>Cite as:<br>
arXiv:2503.01066 [cs.LG]</p>
<p>(or<br>
arXiv:2503.01066v1 [cs.LG] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.01066">https://doi.org/10.48550/arXiv.2503.01066</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>91. <b>【2503.01064】Scientific Reasoning: Assessment of Multimodal Generative LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01064">https://arxiv.org/abs/2503.01064</a></p>
  <p><b>作者</b>：Florian Dreyer,Ekaterina Kolos,Daria Matiash</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Large language models, Large language, complex tasks, scientific domain, answer questions</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) can answer questions and reason about complex tasks, also from the scientific domain. We assess several multimodal LLMs (MLLMs) on ScienceQA and find that Gemini models show the highest accuracy with little context, and the highest textual similarity to human explanations with richer context. Adapter-tuning of smaller MLLMs did not lead to any reliable performance. Training from Gemini outputs consistently underperformed training from the original data.</p>
  </details>
</details>
<details>
  <summary>92. <b>【2503.01063】AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01063">https://arxiv.org/abs/2503.01063</a></p>
  <p><b>作者</b>：David Noever</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, paper investigates, investigates the potential, potential for large, develop private tonal</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance.</p>
  </details>
</details>
<details>
  <summary>93. <b>【2503.01045】Language-agnostic, automated assessment of listeners' speech recall using large language models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01045">https://arxiv.org/abs/2503.01045</a></p>
  <p><b>作者</b>：Björn Herrmann</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Speech-comprehension difficulties, older people, common among older, native English speakers, Speech-comprehension</p>
  <p><b>备注</b>： 37 pages, 5 figure, 1 table</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Speech-comprehension difficulties are common among older people. Standard speech tests do not fully capture such difficulties because the tests poorly resemble the context-rich, story-like nature of ongoing conversation and are typically available only in a country's dominant/official language (e.g., English), leading to inaccurate scores for native speakers of other languages. Assessments for naturalistic, story speech in multiple languages require accurate, time-efficient scoring. The current research leverages modern large language models (LLMs) in native English speakers and native speakers of 10 other languages to automate the generation of high-quality, spoken stories and scoring of speech recall in different languages. Participants listened to and freely recalled short stories (in quiet/clear and in babble noise) in their native language. LLM text-embeddings and LLM prompt engineering with semantic similarity analyses to score speech recall revealed sensitivity to known effects of temporal order, primacy/recency, and background noise, and high similarity of recall scores across languages. The work overcomes limitations associated with simple speech materials and testing of closed native-speaker groups because recall data of varying length and details can be mapped across languages with high accuracy. The full automation of speech generation and recall scoring provides an important step towards comprehension assessments of naturalistic speech with clinical applicability.</p>
  </details>
</details>
<details>
  <summary>94. <b>【2503.01033】Variance reduction in output from generative AI</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01033">https://arxiv.org/abs/2503.01033</a></p>
  <p><b>作者</b>：Yu Xie,Yueqi Xie</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：increasingly replace humans, Generative AI models, important tasks, increasingly replace, replace humans</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generative AI models, such as ChatGPT, will increasingly replace humans in producing output for a variety of important tasks. While much prior work has mostly focused on the improvement in the average performance of generative AI models relative to humans' performance, much less attention has been paid to the significant reduction of variance in output produced by generative AI models. In this Perspective, we demonstrate that generative AI models are inherently prone to the phenomenon of "regression toward the mean" whereby variance in output tends to shrink relative to that in real-world distributions. We discuss potential social implications of this phenomenon across three levels-societal, group, and individual-and two dimensions-material and non-material. Finally, we discuss interventions to mitigate negative effects, considering the roles of both service providers and users. Overall, this Perspective aims to raise awareness of the importance of output variance in generative AI and to foster collaborative efforts to meet the challenges posed by the reduction of variance in output generated by AI models.</p>
  </details>
</details>
<details>
  <summary>95. <b>【2503.01030】Language Models Predict Empathy Gaps Between Social In-groups and Out-groups</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01030">https://arxiv.org/abs/2503.01030</a></p>
  <p><b>作者</b>：Yu Hou,Hal Daumé III,Rachel Rudinger</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：psychology have demonstrated, demonstrated that people, motivated to extend, extend empathy, Studies of human</p>
  <p><b>备注</b>： NAACL 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is replicated by LLMs in an emotion intensity prediction task. In this task, the LLM is given a short description of an experience a person had that caused them to feel a particular emotion; the LLM is then prompted to predict the intensity of the emotion the person experienced on a numerical scale. By manipulating the group identities assigned to the LLM's persona (the "perceiver") and the person in the narrative (the "experiencer"), we measure how predicted emotion intensities differ between in-group and out-group settings. We observe that LLMs assign higher emotion intensity scores to in-group members than out-group members. This pattern holds across all three types of social groupings we tested: race/ethnicity, nationality, and religion. We perform an in-depth analysis on Llama-3.1-8B, the model which exhibited strongest intergroup bias among those tested.</p>
  </details>
</details>
<details>
  <summary>96. <b>【2503.01003】A Semantic Search Pipeline for Causality-driven Adhoc Information Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01003">https://arxiv.org/abs/2503.01003</a></p>
  <p><b>作者</b>：Dhairya Dalal,Sharmi Dev Gupta,Bentolhoda Binaei</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Causality-driven Adhoc Information, Adhoc Information Retrieval, Causality-driven Adhoc, Adhoc Information, CAIR shared task</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present a unsupervised semantic search pipeline for the Causality-driven Adhoc Information Retrieval (CAIR-2021) shared task. The CAIR shared task expands traditional information retrieval to support the retrieval of documents containing the likely causes of a query event. A successful system must be able to distinguish between topical documents and documents containing causal descriptions of events that are causally related to the query event. Our approach involves aggregating results from multiple query strategies over a semantic and lexical index. The proposed approach leads the CAIR-2021 leaderboard and outperformed both traditional IR and pure semantic embedding-based approaches.</p>
  </details>
</details>
<details>
  <summary>97. <b>【2503.00995】Evaluating Polish linguistic and cultural competency in large language models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00995">https://arxiv.org/abs/2503.00995</a></p>
  <p><b>作者</b>：Sławomir Dadas,Małgorzata Grębowiec,Michał Perełkiewicz,Rafał Poświata</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：address real-world problems, generating multilingual texts, Large language models, problems more effectively, Large language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) are becoming increasingly proficient in processing and generating multilingual texts, which allows them to address real-world problems more effectively. However, language understanding is a far more complex issue that goes beyond simple text analysis. It requires familiarity with cultural context, including references to everyday life, historical events, traditions, folklore, literature, and pop culture. A lack of such knowledge can lead to misinterpretations and subtle, hard-to-detect errors. To examine language models' knowledge of the Polish cultural context, we introduce the Polish linguistic and cultural competency benchmark, consisting of 600 manually crafted questions. The benchmark is divided into six categories: history, geography, culture  tradition, art  entertainment, grammar, and vocabulary. As part of our study, we conduct an extensive evaluation involving over 30 open-weight and commercial LLMs. Our experiments provide a new perspective on Polish competencies in language models, moving past traditional natural language processing tasks and general knowledge assessment.</p>
  </details>
</details>
<details>
  <summary>98. <b>【2503.00992】Evidence of conceptual mastery in the application of rules by Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00992">https://arxiv.org/abs/2503.00992</a></p>
  <p><b>作者</b>：José Luiz Nunes,Guilherme FCF Almeida,Brian Flanagan</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：leverage psychological methods, investigate LLMs' conceptual, LLMs' conceptual mastery, paper we leverage, leverage psychological</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper we leverage psychological methods to investigate LLMs' conceptual mastery in applying rules. We introduce a novel procedure to match the diversity of thought generated by LLMs to that observed in a human sample. We then conducted two experiments comparing rule-based decision-making in humans and LLMs. Study 1 found that all investigated LLMs replicated human patterns regardless of whether they are prompted with scenarios created before or after their training cut-off. Moreover, we found unanticipated differences between the two sets of scenarios among humans. Surprisingly, even these differences were replicated in LLM responses. Study 2 turned to a contextual feature of human rule application: under forced time delay, human samples rely more heavily on a rule's text than on other considerations such as a rule's purpose.. Our results revealed that some models (Gemini Pro and Claude 3) responded in a human-like manner to a prompt describing either forced delay or time pressure, while others (GPT-4o and Llama 3.2 90b) did not. We argue that the evidence gathered suggests that LLMs have mastery over the concept of rule, with implications for both legal decision making and philosophical inquiry.</p>
  </details>
</details>
<details>
  <summary>99. <b>【2503.00985】Enhancing Text Editing for Grammatical Error Correction: Arabic as a Case Study</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00985">https://arxiv.org/abs/2503.00985</a></p>
  <p><b>作者</b>：Bashar Alhafni,Nizar Habash</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：grammatical error correction, sequence tagging problem, frames grammatical error, editing frames grammatical, error correction</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text editing frames grammatical error correction (GEC) as a sequence tagging problem, where edit tags are assigned to input tokens, and applying these edits results in the corrected text. This approach has gained attention for its efficiency and interpretability. However, while extensively explored for English, text editing remains largely underexplored for morphologically rich languages like Arabic. In this paper, we introduce a text editing approach that derives edit tags directly from data, eliminating the need for language-specific edits. We demonstrate its effectiveness on Arabic, a diglossic and morphologically rich language, and investigate the impact of different edit representations on model performance. Our approach achieves SOTA results on two Arabic GEC benchmarks and performs on par with SOTA on two others. Additionally, our models are over six times faster than existing Arabic GEC systems, making our approach more practical for real-world applications. Finally, we explore ensemble models, demonstrating how combining different models leads to further performance improvements. We make our code, data, and pretrained models publicly available.</p>
  </details>
</details>
<details>
  <summary>100. <b>【2503.00979】Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses in LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00979">https://arxiv.org/abs/2503.00979</a></p>
  <p><b>作者</b>：Ravi Ghadia,Avinash Kumar,Gaurav Jain,Prashant Nair,Poulami Das</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Autoregressive Transformers rely, Autoregressive Transformers, Transformers rely, rely on Key-Value, caching to accelerate</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Autoregressive Transformers rely on Key-Value (KV) caching to accelerate inference. However, the linear growth of the KV cache with context length leads to excessive memory consumption and bandwidth constraints. This bottleneck is particularly problematic in real-time applications -- such as chatbots and interactive assistants -- where low latency and high memory efficiency are critical. Existing methods drop distant tokens or compress states in a lossy manner, sacrificing accuracy by discarding vital context or introducing bias.
We propose MorphKV, an inference-time technique that maintains a constant-sized KV cache while preserving accuracy. MorphKV balances long-range dependencies and local coherence during text generation. It eliminates early-token bias while retaining high-fidelity context by adaptively ranking tokens through correlation-aware selection. Unlike heuristic retention or lossy compression, MorphKV iteratively refines the KV cache via lightweight updates guided by attention patterns of recent tokens. This approach captures inter-token correlation with greater accuracy, crucial for tasks like content creation and code generation. Our studies on long-response tasks show 52.9$\%$ memory savings and 18.2$\%$ higher accuracy on average compared to state-of-the-art prior works, enabling efficient real-world deployment.
</p><p>Subjects:</p>
<p>Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)</p>
<p>Cite as:<br>
arXiv:2503.00979 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]</p>
<p>(or<br>
arXiv:2503.00979v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.00979">https://doi.org/10.48550/arXiv.2503.00979</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>101. <b>【2503.00958】Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00958">https://arxiv.org/abs/2503.00958</a></p>
  <p><b>作者</b>：Milad Alshomary,Nikhil Reddy Varimalla,Vishal Anand,Kathleen McKeown</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：linguistic representations learned, pre-trained transformer-based models, authorship attribution task, task that leverages, linguistic representations</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We propose a new approach for the authorship attribution task that leverages the various linguistic representations learned at different layers of pre-trained transformer-based models. We evaluate our approach on three datasets, comparing it to a state-of-the-art baseline in in-domain and out-of-domain scenarios. We found that utilizing various transformer layers improves the robustness of authorship attribution models when tested on out-of-domain data, resulting in new state-of-the-art results. Our analysis gives further insights into how our model's different layers get specialized in representing certain stylistic features that benefit the model when tested out of the domain.</p>
  </details>
</details>
<details>
  <summary>102. <b>【2503.00955】SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00955">https://arxiv.org/abs/2503.00955</a></p>
  <p><b>作者</b>：Nam V. Nguyen,Dien X. Tran,Thanh T. Tran,Anh T. Hoang,Tai V. Duong,Di T. Le,Phuc-Lu Le</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models, GPT and Gemini, Language Models, Large Language, exacerbated by Large</p>
  <p><b>备注</b>： 18 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\% strict accuracy on ISE-DSC01 and 80.82\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>103. <b>【2503.00912】HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00912">https://arxiv.org/abs/2503.00912</a></p>
  <p><b>作者</b>：Zhuohang Jiang,Pangjing Wu,Ziran Liang,Peter Q. Chen,Xu Yuan,Ye Jia,Jiancheng Tu,Chen Li,Peter H.F. Ng,Qing Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：answer multi-hop questions, large language models, multi-hop questions, Structure reasoning, large language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Structure reasoning is a fundamental capability of large language models (LLMs), enabling them to reason about structured commonsense and answer multi-hop questions. However, existing benchmarks for structure reasoning mainly focus on horizontal and coordinate structures (\emph{e.g.} graphs), overlooking the hierarchical relationships within them. Hierarchical structure reasoning is crucial for human cognition, particularly in memory organization and problem-solving. It also plays a key role in various real-world tasks, such as information extraction and decision-making. To address this gap, we propose HiBench, the first framework spanning from initial structure generation to final proficiency assessment, designed to benchmark the hierarchical reasoning capabilities of LLMs systematically. HiBench encompasses six representative scenarios, covering both fundamental and practical aspects, and consists of 30 tasks with varying hierarchical complexity, totaling 39,519 queries. To evaluate LLMs comprehensively, we develop five capability dimensions that depict different facets of hierarchical structure understanding. Through extensive evaluation of 20 LLMs from 10 model families, we reveal key insights into their capabilities and limitations: 1) existing LLMs show proficiency in basic hierarchical reasoning tasks; 2) they still struggle with more complex structures and implicit hierarchical representations, especially in structural modification and textual reasoning. Based on these findings, we create a small yet well-designed instruction dataset, which enhances LLMs' performance on HiBench by an average of 88.84\% (Llama-3.1-8B) and 31.38\% (Qwen2.5-7B) across all tasks. The HiBench dataset and toolkit are available here, this https URL, to encourage evaluation.</p>
  </details>
</details>
<details>
  <summary>104. <b>【2503.00907】Unveiling Biases while Embracing Sustainability: Assessing the Dual Challenges of Automatic Speech Recognition Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00907">https://arxiv.org/abs/2503.00907</a></p>
  <p><b>作者</b>：Ajinkya Kulkarni,Atharva Kulkarni,Miguel Couceiro,Isabel Trancoso</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Sound (cs.SD); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：Automatic Speech Recognition, Massively Multilingual Speech, Speech Recognition, Automatic Speech, Multilingual Speech</p>
  <p><b>备注</b>： Interspeech 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we present a bias and sustainability focused investigation of Automatic Speech Recognition (ASR) systems, namely Whisper and Massively Multilingual Speech (MMS), which have achieved state-of-the-art (SOTA) performances. Despite their improved performance in controlled settings, there remains a critical gap in understanding their efficacy and equity in real-world scenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well as their effect on downstream tasks. In addition, we examine the environmental impact of ASR systems, scrutinizing the use of large acoustic models on carbon emission and energy consumption. We also provide insights into our empirical analyses, offering a valuable contribution to the claims surrounding bias and sustainability in ASR systems.</p>
  </details>
</details>
<details>
  <summary>105. <b>【2503.00902】Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00902">https://arxiv.org/abs/2503.00902</a></p>
  <p><b>作者</b>：Liping Liu,Chunhong Zhang,Likang Wu,Chuang Zhao,Zheng Hu,Ming He,Jianping Fan</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Self-reflection for Large, gained significant attention, Large Language, Language Models</p>
  <p><b>备注</b>： 23 pages, 5 figures, accepted by NAACL2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Self-reflection for Large Language Models (LLMs) has gained significant attention. Existing approaches involve models iterating and improving their previous responses based on LLMs' internal reflection ability or external feedback. However, recent research has raised doubts about whether intrinsic self-correction without external feedback may even degrade performance. Based on our empirical evidence, we find that current static reflection methods may lead to redundant, drift, and stubborn issues. To mitigate this, we introduce Instruct-of-Reflection (IoRT), a novel and general reflection framework that leverages dynamic-meta instruction to enhance the iterative reflection capability of LLMs. Specifically, we propose the instructor driven by the meta-thoughts and self-consistency classifier, generates various instructions, including refresh, stop, and select, to guide the next reflection iteration. Our experiments demonstrate that IoRT achieves an average improvement of 10.1% over established baselines in mathematical and commonsense reasoning tasks, highlighting its efficacy and applicability.</p>
  </details>
</details>
<details>
  <summary>106. <b>【2503.00867】DUAL: Diversity and Uncertainty Active Learning for Text Summarization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00867">https://arxiv.org/abs/2503.00867</a></p>
  <p><b>作者</b>：Petros Stylianos Giouroukis,Alexios Gidiotis,Grigorios Tsoumakas</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, Active learning, recent years, rise of large, large language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the rise of large language models, neural text summarization has advanced significantly in recent years. However, even state-of-the-art models continue to rely heavily on high-quality human-annotated data for training and evaluation. Active learning is frequently used as an effective way to collect such datasets, especially when annotation resources are scarce. Active learning methods typically prioritize either uncertainty or diversity but have shown limited effectiveness in summarization, often being outperformed by random sampling. We present Diversity and Uncertainty Active Learning (DUAL), a novel algorithm that combines uncertainty and diversity to iteratively select and annotate samples that are both representative of the data distribution and challenging for the current model. DUAL addresses the selection of noisy samples in uncertainty-based methods and the limited exploration scope of diversity-based methods. Through extensive experiments with different summarization models and benchmark datasets, we demonstrate that DUAL consistently matches or outperforms the best performing strategies. Using visualizations and quantitative metrics, we provide valuable insights into the effectiveness and robustness of different active learning strategies, in an attempt to understand why these strategies haven't performed consistently in text summarization. Finally, we show that DUAL strikes a good balance between diversity and robustness.</p>
  </details>
</details>
<details>
  <summary>107. <b>【2503.00865】Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00865">https://arxiv.org/abs/2503.00865</a></p>
  <p><b>作者</b>：Yiran Zhao,Chaoqun Liu,Yue Deng,Jiahao Ying,Mahani Aljunied,Zhaodonghui Li,Lidong Bing,Hou Pong Chan,Yu Rong,Deli Zhao,Wenxuan Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large language models, LLMs remain scarce, natural language processing, revolutionized natural language, Large language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce $\texttt{Babel}$, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: $\texttt{Babel-9B}$, designed for efficient inference and fine-tuning, and $\texttt{Babel-83B}$, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.</p>
  </details>
</details>
<details>
  <summary>108. <b>【2503.00847】Argument Summarization and its Evaluation in the Era of Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00847">https://arxiv.org/abs/2503.00847</a></p>
  <p><b>作者</b>：Moritz Altemeyer,Steffen Eger,Johannes Daxenberger,Tim Altendorf,Philipp Cimiano,Benjamin Schiller</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Natural Language Generation, including Argument Summarization, Large Language, Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have revolutionized various Natural Language Generation (NLG) tasks, including Argument Summarization (ArgSum), a key subfield of Argument Mining (AM). This paper investigates the integration of state-of-the-art LLMs into ArgSum, including for its evaluation. In particular, we propose a novel prompt-based evaluation scheme, and validate it through a novel human benchmark dataset. Our work makes three main contributions: (i) the integration of LLMs into existing ArgSum frameworks, (ii) the development of a new LLM-based ArgSum system, benchmarked against prior methods, and (iii) the introduction of an advanced LLM-based evaluation scheme. We demonstrate that the use of LLMs substantially improves both the generation and evaluation of argument summaries, achieving state-of-the-art results and advancing the field of ArgSum.</p>
  </details>
</details>
<details>
  <summary>109. <b>【2503.00845】Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00845">https://arxiv.org/abs/2503.00845</a></p>
  <p><b>作者</b>：Miao Peng,Nuo Chen,Zongrui Suo,Jia Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, Language Models, advancements in Large, developing advanced reasoning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite significant advancements in Large Language Models (LLMs), developing advanced reasoning capabilities in LLMs remains a key challenge. Process Reward Models (PRMs) have demonstrated exceptional promise in enhancing reasoning by providing step-wise feedback, particularly in the context of mathematical reasoning. However, their application to broader reasoning domains remains understudied, largely due to the high costs associated with manually creating step-level supervision. In this work, we explore the potential of PRMs in graph reasoning problems - a domain that demands sophisticated multi-step reasoning and offers opportunities for automated step-level data generation using established graph algorithms. We introduce GraphSILO, the largest dataset for graph reasoning problems with fine-grained step-wise labels, built using automated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to generate detailed reasoning steps with step-wise labels. Building upon this dataset, we train GraphPRM, the first PRM designed for graph reasoning problems, and evaluate its effectiveness in two key settings: inference-time scaling and reinforcement learning via Direct Preference Optimization (DPO). Experimental results show that GraphPRM significantly improves LLM performance across 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and demonstrating transferability to new graph reasoning datasets and new reasoning domains like mathematical problem-solving. Notably, GraphPRM enhances LLM performance on GSM8K and Math500, underscoring the cross-domain applicability of graph-based reasoning rewards. Our findings highlight the potential of PRMs in advancing reasoning across diverse domains, paving the way for more versatile and effective LLMs.</p>
  </details>
</details>
<details>
  <summary>110. <b>【2503.00831】Waste Not, Want Not; Recycled Gumbel Noise Improves Consistency in Natural Language Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00831">https://arxiv.org/abs/2503.00831</a></p>
  <p><b>作者</b>：Damien de Mijolla,Hannan Saddiq,Kim Moore</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：language models, language model outputs, language models learn, language, practical utility</p>
  <p><b>备注</b>： Accepted to NAACL 2025 main conference; Presented at Neurips Safe Generative AI workshop</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Consistency in the output of language models is critical for their reliability and practical utility. Due to their training objective, language models learn to model the full space of possible continuations, leading to outputs that can vary significantly in style and content, even for similar or repeated inputs. To address this, we propose a novel decoding algorithm that enhances response consistency across different prompts with no degradation in response quality. By incorporating a latent variable into the next-token sampling process based on the Gumbel reparametrisation trick, our method outperforms standard sampling by up to 10% across semantic and stylistic consistency benchmarks. Additionally, our approach integrates seamlessly with existing sampling methods with negligible computational overhead, providing a practical solution for improving the reliability of language model outputs.</p>
  </details>
</details>
<details>
  <summary>111. <b>【2503.00808】Predictive Data Selection: The Data That Predicts Is the Data That Teaches</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00808">https://arxiv.org/abs/2503.00808</a></p>
  <p><b>作者</b>：Kashun Shum,Yuzhen Huang,Hongjian Zou,Ding Qi,Yixuan Liao,Xiaoxin Chen,Qian Liu,Junxian He</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Language model pretraining, data quality plays, Language model, extensive corpora, pivotal role</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Language model pretraining involves training on extensive corpora, where data quality plays a pivotal role. In this work, we aim to directly estimate the contribution of data during pretraining and select pretraining data in an efficient manner. Specifically, we draw inspiration from recent findings showing that compression efficiency (i.e., the normalized loss) of diverse models on certain text correlates strongly with their downstream performance, when the text domain aligns with the downstream benchmark (Huang et al., 2024). Building on this observation, we hypothesize that data on which model losses are predictive of downstream abilities also contribute effectively to learning. To leverage this insight, we introduce data selection based on data's Predictive strength (Preselect), a lightweight and efficient data selection method that requires training and deploying only a fastText-based scorer. Through comprehensive experiments with 1B and 3B parameter models, we demonstrate that models trained on 30B tokens selected with PreSelect surpasses the performance of a vanilla baseline trained on 300B tokens, achieving a 10x reduction in compute requirements. Furthermore, PreSelect significantly outperforms other competitive data selection baselines, such as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our trained data selection scorer along with the curated datasets at this https URL.</p>
  </details>
</details>
<details>
  <summary>112. <b>【2503.00784】DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00784">https://arxiv.org/abs/2503.00784</a></p>
  <p><b>作者</b>：Kai Lv,Honglin Guo,Qipeng Guo,Xipeng Qiu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：hinders inference speed, process significantly hinders, significantly hinders inference, autoregressive generation process, Large language models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>113. <b>【2503.00771】Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00771">https://arxiv.org/abs/2503.00771</a></p>
  <p><b>作者</b>：Yupu Hao,Pengfei Cao,Zhuoran Jin,Huanxuan Liao,Yubo Chen,Kang Liu,Jun Zhao</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：large language models, aligning large language, Personalized tool utilization, language models, utilization is essential</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents. Our Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>114. <b>【2503.00751】RAPID: Efficient Retrieval-Augmented Long Text Generation with Writing Planning and Information Discovery</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00751">https://arxiv.org/abs/2503.00751</a></p>
  <p><b>作者</b>：Hongchao Gu,Dexun Li,Kuicai Dong,Hao Zhang,Hang Lv,Hao Wang,Defu Lian,Yong Liu,Enhong Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, Generating knowledge-intensive, knowledge-intensive and comprehensive</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generating knowledge-intensive and comprehensive long texts, such as encyclopedia articles, remains significant challenges for Large Language Models. It requires not only the precise integration of facts but also the maintenance of thematic coherence throughout the article. Existing methods, such as direct generation and multi-agent discussion, often struggle with issues like hallucinations, topic incoherence, and significant latency. To address these challenges, we propose RAPID, an efficient retrieval-augmented long text generation framework. RAPID consists of three main modules: (1) Retrieval-augmented preliminary outline generation to reduce hallucinations, (2) Attribute-constrained search for efficient information discovery, (3) Plan-guided article generation for enhanced coherence. Extensive experiments on our newly compiled benchmark dataset, FreshWiki-2024, demonstrate that RAPID significantly outperforms state-of-the-art methods across a wide range of evaluation metrics (e.g. long-text generation, outline quality, latency, etc). Our work provides a robust and efficient solution to the challenges of automated long-text generation.</p>
  </details>
</details>
<details>
  <summary>115. <b>【2503.00724】Unmasking Digital Falsehoods: A Comparative Analysis of LLM-Based Misinformation Detection Strategies</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00724">https://arxiv.org/abs/2503.00724</a></p>
  <p><b>作者</b>：Tianyi Huang,Jingyuan Yi,Peiyang Yu,Xiaochuan Xu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：significant societal concerns, raised significant societal, necessitating robust detection, societal concerns, necessitating robust</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The proliferation of misinformation on social media has raised significant societal concerns, necessitating robust detection mechanisms. Large Language Models such as GPT-4 and LLaMA2 have been envisioned as possible tools for detecting misinformation based on their advanced natural language understanding and reasoning capabilities. This paper conducts a comparison of LLM-based approaches to detecting misinformation between text-based, multimodal, and agentic approaches. We evaluate the effectiveness of fine-tuned models, zero-shot learning, and systematic fact-checking mechanisms in detecting misinformation across different topic domains like public health, politics, and finance. We also discuss scalability, generalizability, and explainability of the models and recognize key challenges such as hallucination, adversarial attacks on misinformation, and computational resources. Our findings point towards the importance of hybrid approaches that pair structured verification protocols with adaptive learning techniques to enhance detection accuracy and explainability. The paper closes by suggesting potential avenues of future work, including real-time tracking of misinformation, federated learning, and cross-platform detection models.</p>
  </details>
</details>
<details>
  <summary>116. <b>【2503.00703】owards hyperparameter-free optimization with differential privacy</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00703">https://arxiv.org/abs/2503.00703</a></p>
  <p><b>作者</b>：Zhiqi Bu,Ruixuan Liu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Differential privacy, training deep learning, deep learning models, learning rate schedule, learning rate</p>
  <p><b>备注</b>： Accepted to ICLR 2025 spotlight</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparameter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performance on various language and vision tasks.</p>
  </details>
</details>
<details>
  <summary>117. <b>【2503.00691】How Diversely Can Language Models Solve Problems? Exploring the Algorithmic Diversity of Model-Generated Code</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00691">https://arxiv.org/abs/2503.00691</a></p>
  <p><b>作者</b>：Seonghyeon Lee,Heejae Chon,Joonwon Jang,Dongha Lee,Hwanjo Yu</p>
  <p><b>类目</b>：oftware Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：natural language requirements, exhibited impressive abilities, language requirements, natural language, code</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Language models (LMs) have exhibited impressive abilities in generating code from natural language requirements. In this work, we highlight the diversity of code generated by LMs as a critical criterion for evaluating their code generation capabilities. There is a lack of studies focused on assessing the diversity of generated code, which overlooks its importance in code LMs. Therefore, we propose a systematic approach to evaluate code diversity, introducing various metrics with inter-code similarity. Specifically, we introduce code clustering methods that leverages LMs' capabilities in code understanding and reasoning, resulting in a set of metrics that represent the number of algorithms in model-generated solutions. We extensively investigate the property of model-generated solutions by contrasting them with human-written ones and quantifying the impact of various factors on code diversity: model size, temperature, instruction tuning, and problem complexity. Our analysis demonstrates that model-generated solutions exhibit low algorithmic diversity, which was neglected by the research community. Moreover, we explore methods to increase code diversity by combining solutions from different models and increasing sampling temperatures. Our findings highlight that code diversity can be enhanced with the help of heterogeneous models and setting temperature beyond 1.0 that has not been fully explored due to the functional correctness degradation. To facilitate our research direction, we publicly share our code and datasets through open-source repositories.</p>
  </details>
</details>
<details>
  <summary>118. <b>【2503.00624】An evaluation of DeepSeek Models in Biomedical Natural Language Processing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00624">https://arxiv.org/abs/2503.00624</a></p>
  <p><b>作者</b>：Zaifu Zhan,Shuang Zhou,Huixue Zhou,Jiawen Deng,Yu Hou,Jeremy Yeung,Rui Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Plan to submit to AMIA 2025 Annual Symposium. 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>119. <b>【2503.00600】Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00600">https://arxiv.org/abs/2503.00600</a></p>
  <p><b>作者</b>：Alexander W. Lee,Justin Chan,Michael Fu,Nicolas Kim,Akshay Mehta,Deepti Raghavan,Ugur Cetintemel</p>
  <p><b>类目</b>：Databases (cs.DB); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>120. <b>【2503.00597】Zero-Shot Keyphrase Generation: Investigating Specialized Instructions and Multi-Sample Aggregation on Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00597">https://arxiv.org/abs/2503.00597</a></p>
  <p><b>作者</b>：Jayanth Mohan,Jishnu Ray Chowdhury,Tomas Malik,Cornelia Caragea</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：essential topical phrases, Large Language Models, essential topical, topical phrases, phrases that summarize</p>
  <p><b>备注</b>： Accepted at NAACL 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Keyphrases are the essential topical phrases that summarize a document. Keyphrase generation is a long-standing NLP task for automatically generating keyphrases for a given document. While the task has been comprehensively explored in the past via various models, only a few works perform some preliminary analysis of Large Language Models (LLMs) for the task. Given the impact of LLMs in the field of NLP, it is important to conduct a more thorough examination of their potential for keyphrase generation. In this paper, we attempt to meet this demand with our research agenda. Specifically, we focus on the zero-shot capabilities of open-source instruction-tuned LLMs (Phi-3, Llama-3) and the closed-source GPT-4o for this task. We systematically investigate the effect of providing task-relevant specialized instructions in the prompt. Moreover, we design task-specific counterparts to self-consistency-style strategies for LLMs and show significant benefits from our proposals over the baselines.</p>
  </details>
</details>
<details>
  <summary>121. <b>【2503.00596】BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00596">https://arxiv.org/abs/2503.00596</a></p>
  <p><b>作者</b>：Terry Tong,Fei Wang,Zhe Zhao,Muhao Chen</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Published to ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>122. <b>【2503.00572】LoR2C : Low-Rank Residual Connection Adaptation for Parameter-Efficient Fine-Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00572">https://arxiv.org/abs/2503.00572</a></p>
  <p><b>作者</b>：Jiancheng Zhao,Xingda Yu,Yuxiang Zhang,Zhen Yang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>123. <b>【2503.00566】Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00566">https://arxiv.org/abs/2503.00566</a></p>
  <p><b>作者</b>：Kyle Gao,Dening Lu,Liangzhi Li,Nan Chen,Hongjie He,Linlin Xu,Jonathan Li</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>124. <b>【2503.00564】oolDial: Multi-turn Dialogue Generation Method for Tool-Augmented Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00564">https://arxiv.org/abs/2503.00564</a></p>
  <p><b>作者</b>：Jeonghoon Shim,Gyuhyeon Seo,Cheongsu Lim,Yohan Jo</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：leverage external APIs, answer user queries, leverage external, Tool-Augmented Language Models, Language Models</p>
  <p><b>备注</b>： Accepted to ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Tool-Augmented Language Models (TALMs) leverage external APIs to answer user queries across various domains. However, existing benchmark datasets for TALM research often feature simplistic dialogues that do not reflect real-world scenarios, such as the need for models to ask clarifying questions or proactively call additional APIs when essential information is missing. To address these limitations, we construct and release ToolDial, a dataset comprising 11,111 multi-turn dialogues, with an average of 8.95 turns per dialogue, based on APIs from RapidAPI. ToolDial has two key characteristics. First, the dialogues incorporate 16 user and system actions (e.g., "Request", "Clarify", "Fail inform") to capture the rich dynamics of real-world interactions. Second, we simulate dialogues where the system requests necessary information from the user based on API documentation and seeks additional APIs if the user fails to provide the required information. To facilitate this process, we introduce a method for generating an API graph that represents input and output compatibility between APIs. Using ToolDial, we evaluate a suite of language models on their ability to predict correct actions and extract input parameter values for API calls from the dialogue history. Modern language models achieve accuracy scores below 70%, indicating substantial room for improvement. We release our dataset and code at this https URL.</p>
  </details>
</details>
<details>
  <summary>125. <b>【2503.00539】Distributionally Robust Reinforcement Learning with Human Feedback</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00539">https://arxiv.org/abs/2503.00539</a></p>
  <p><b>作者</b>：Debmalya Mandal,Paulius Sasnauskas,Goran Radanovic</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>126. <b>【2503.00501】Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00501">https://arxiv.org/abs/2503.00501</a></p>
  <p><b>作者</b>：Jia Chen,Qian Dong,Haitao Li,Xiaohui He,Yan Gao,Shaosheng Cao,Yi Wu,Ping Yang,Chen Xu,Yao Hu,Qingyao Ai,Yiqun Liu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：improve user experiences, User-generated content, integrating visual, visual and textual, featuring multimodal content</p>
  <p><b>备注</b>： 11 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\R. To address the growing need for developing better S\R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\%. In contrast to existing datasets, \textsf{Qilin} offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \ evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\R systems. We hope that \textsf{Qilin} will significantly contribute to the advancement of multimodal content platforms with S\R services in the future.</p>
  </details>
</details>
<details>
  <summary>127. <b>【2503.00491】utorial Proposal: Speculative Decoding for Efficient LLM Inference</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00491">https://arxiv.org/abs/2503.00491</a></p>
  <p><b>作者</b>：Heming Xia,Cunxiao Du,Yongqi Li,Qian Liu,Wenjie Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： COLING 2025 Tutorial. Our homepage: [this https URL](https://speculative-decoding.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>128. <b>【2503.00489】Embracing Diversity: A Multi-Perspective Approach with Soft Labels</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00489">https://arxiv.org/abs/2503.00489</a></p>
  <p><b>作者</b>：Benedetta Muscato,Praveen Bushipaka,Gizem Gezici,Lucia Passaro,Fosca Giannotti,Tommaso Cucinotta</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>129. <b>【2503.00464】Unstable Grounds for Beautiful Trees? Testing the Robustness of Concept Translations in the Compilation of Multilingual Wordlists</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00464">https://arxiv.org/abs/2503.00464</a></p>
  <p><b>作者</b>：David Snee,Luca Ciucci,Arne Rubehn,Kellen Parker van Dam,Johann-Mattis List</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Submitted to the 7th Workshop on Research in Computational Linguistic Typology and Multilingual NLP (SIGTYP)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>130. <b>【2503.00449】Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00449">https://arxiv.org/abs/2503.00449</a></p>
  <p><b>作者</b>：Yanyue Zhang,Yulan He,Deyu Zhou</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 17 pages, 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>131. <b>【2503.00444】Figurative Archive: an open dataset and web-based application for the study of metaphor</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00444">https://arxiv.org/abs/2503.00444</a></p>
  <p><b>作者</b>：Maddalena Bressler,Veronica Mangiaterra,Paolo Canal,Federico Frau,Fabrizio Luciani,Biagio Scalingi,Chiara Barattieri di San Pietro,Chiara Battaglini,Chiara Pompei,Fortunata Romeo,Luca Bischetti,Valentina Bambini</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>132. <b>【2503.00435】AILS-NTUA at SemEval-2025 Task 8: Language-to-Code prompting and Error Fixing for Tabular Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00435">https://arxiv.org/abs/2503.00435</a></p>
  <p><b>作者</b>：Andreas Evangelatos,Giorgos Filandrianos,Maria Lymperaiou,Athanasios Voulodimos,Giorgos Stamou</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>133. <b>【2503.00417】A Multi-Labeled Dataset for Indonesian Discourse: Examining Toxicity, Polarization, and Demographics Information</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00417">https://arxiv.org/abs/2503.00417</a></p>
  <p><b>作者</b>：Lucky Susanto,Musa Wijanarko,Prasetia Pratama,Zilu Tang,Fariz Akyas,Traci Hong,Ika Idris,Alham Aji,Derry Wijaya</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>134. <b>【2503.00401】Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with Query-Oriented Pivot Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00401">https://arxiv.org/abs/2503.00401</a></p>
  <p><b>作者</b>：Zongru Wu,Pengzhou Cheng,Zheng Wu,Tianjie Ju,Zhuosheng Zhang,Gongshen Liu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>135. <b>【2503.00367】Approaching the Limits to EFL Writing Enhancement with AI-generated Text and Diverse Learners</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00367">https://arxiv.org/abs/2503.00367</a></p>
  <p><b>作者</b>：David James Woo,Hengky Susanto,Chi Ho Yeung,Kai Guo,Yilin Huang</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>136. <b>【2503.00356】BERT-based model for Vietnamese Fact Verification Dataset</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00356">https://arxiv.org/abs/2503.00356</a></p>
  <p><b>作者</b>：Bao Tran,T. N. Khanh,Khang Nguyen Tuong,Thien Dang,Quang Nguyen,Nguyen T. Thinh,Vo T. Hung</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： accepted for Oral Presentation in CITA 2024 (The 13th Conference on Information Technology and Its Applications) and will be published in VOLUME 1 OF CITA 2024 (Volume of the Lecture Notes in Network and Systems, Springer)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>137. <b>【2503.00355】Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00355">https://arxiv.org/abs/2503.00355</a></p>
  <p><b>作者</b>：Tianyi Huang,Elsa Fan</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted Paper (Oral Presentation) in the Workshop on the Social Impact of AI: Research, Diversity and Inclusion Frameworks at AAAI 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>138. <b>【2503.00353】U-NIAH: Unified RAG and LLM Evaluation for Long Context Needle-In-A-Haystack</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00353">https://arxiv.org/abs/2503.00353</a></p>
  <p><b>作者</b>：Yunfan Gao,Yun Xiong,Wenlong Wu,Zijing Huang,Bohan Li,Haofen Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, Recent advancements, Retrieval-Augmented Generation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in Large Language Models (LLMs) have expanded their context windows to unprecedented lengths, sparking debates about the necessity of Retrieval-Augmented Generation (RAG). To address the fragmented evaluation paradigms and limited cases in existing Needle-in-a-Haystack (NIAH), this paper introduces U-NIAH, a unified framework that systematically compares LLMs and RAG methods in controlled long context settings. Our framework extends beyond traditional NIAH by incorporating multi-needle, long-needle, and needle-in-needle configurations, along with different retrieval settings, while leveraging the synthetic Starlight Academy dataset-a fictional magical universe-to eliminate biases from pre-trained knowledge. Through extensive experiments, we investigate three research questions: (1) performance trade-offs between LLMs and RAG, (2) error patterns in RAG, and (3) RAG's limitations in complex settings. Our findings show that RAG significantly enhances smaller LLMs by mitigating the "lost-in-the-middle" effect and improving robustness, achieving an 82.58% win-rate over LLMs. However, we observe that retrieval noise and reverse chunk ordering degrade performance, while surprisingly, advanced reasoning LLMs exhibit reduced RAG compatibility due to sensitivity to semantic distractors. We identify typical error patterns including omission due to noise, hallucination under high noise critical condition, and self-doubt behaviors. Our work not only highlights the complementary roles of RAG and LLMs, but also provides actionable insights for optimizing deployments. Code: this https URL.</p>
  </details>
</details>
<details>
  <summary>139. <b>【2503.00342】Hierarchical Multi-Stage BERT Fusion Framework with Dual Attention for Enhanced Cyberbullying Detection in Social Media</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00342">https://arxiv.org/abs/2503.00342</a></p>
  <p><b>作者</b>：Jiani Wang,Xiaochuan Xu,Peiyang Yu,Zeqiu Xu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>140. <b>【2503.00333】More of the Same: Persistent Representational Harms Under Increased Representation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00333">https://arxiv.org/abs/2503.00333</a></p>
  <p><b>作者</b>：Jennifer Mickel,Maria De-Arteaga,Leqi Liu,Kevin Tian</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 26 pages, 7 figures, 6 tables, pre-print</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>141. <b>【2503.00330】How Deep is Love in LLMs' Hearts? Exploring Semantic Size in Human-like Cognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00330">https://arxiv.org/abs/2503.00330</a></p>
  <p><b>作者</b>：Yao Yao,Yifei Yang,Xinbei Ma,Dongjie Yang,Zhuosheng Zhang,Zuchao Li,Hai Zhao</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>142. <b>【2503.00306】Unlocking Efficient, Scalable, and Continual Knowledge Editing with Basis-Level Representation Fine-Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00306">https://arxiv.org/abs/2503.00306</a></p>
  <p><b>作者</b>：Tianci Liu,Ruirui Li,Yunzhe Qi,Hui Liu,Xianfeng Tang,Tianqi Zheng,Qingyu Yin,Monica Xiao Cheng,Jun Huan,Haoyu Wang,Jing Gao</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>143. <b>【2503.00295】Robust Multi-Objective Preference Alignment with Online DPO</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00295">https://arxiv.org/abs/2503.00295</a></p>
  <p><b>作者</b>：Raghav Gupta,Ryan Sullivan,Yunxuan Li,Samrat Phatale,Abhinav Rastogi</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： AAAI 2025 - AI Alignment Track</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>144. <b>【2503.00278】NeuroLit Navigator: A Neurosymbolic Approach to Scholarly Article Searches for Systematic Reviews</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00278">https://arxiv.org/abs/2503.00278</a></p>
  <p><b>作者</b>：Vedant Khandelwal,Kaushik Roy,Valerie Lookingbill,Ritvik Garimella,Harshul Surana,Heather Heckman,Amit Sheth</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Large Language Models, personalized learning materials, introduction of Large, Language Models, Large Language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The introduction of Large Language Models (LLMs) has significantly impacted various fields, including education, for example, by enabling the creation of personalized learning materials. However, their use in Systematic Reviews (SRs) reveals limitations such as restricted access to specialized vocabularies, lack of domain-specific reasoning, and a tendency to generate inaccurate information. Existing SR tools often rely on traditional NLP methods and fail to address these issues adequately. To overcome these challenges, we developed the ``NeuroLit Navigator,'' a system that combines domain-specific LLMs with structured knowledge sources like Medical Subject Headings (MeSH) and the Unified Medical Language System (UMLS). This integration enhances query formulation, expands search vocabularies, and deepens search scopes, enabling more precise searches. Deployed in multiple universities and tested by over a dozen librarians, the NeuroLit Navigator has reduced the time required for initial literature searches by 90\%. Despite this efficiency, the initial set of articles retrieved can vary in relevance and quality. Nonetheless, the system has greatly improved the reproducibility of search results, demonstrating its potential to support librarians in the SR process.</p>
  </details>
</details>
<details>
  <summary>145. <b>【2503.00269】Reducing Large Language Model Safety Risks in Women's Health using Semantic Entropy</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00269">https://arxiv.org/abs/2503.00269</a></p>
  <p><b>作者</b>：Jahan C. Penny-Dimri,Magdalena Bachmann,William R. Cooke,Sam Mathewlynn,Samuel Dockree,John Tolladay,Jannik Kossen,Lin Li,Yarin Gal,Gabriel Davis Jones</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 15 pages, 6 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>146. <b>【2503.00258】Decoupling Content and Expression: Two-Dimensional Detection of AI-Generated Text</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00258">https://arxiv.org/abs/2503.00258</a></p>
  <p><b>作者</b>：Guangsheng Bao,Lihua Rong,Yanbin Zhao,Qiji Zhou,Yue Zhang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)</p>
  <p><b>关键词</b>：LLMs raises critical, raises critical requirements, wide usage, usage of LLMs, LLMs raises</p>
  <p><b>备注</b>： 8 pages, 8 tables, 8 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The wide usage of LLMs raises critical requirements on detecting AI participation in texts. Existing studies investigate these detections in scattered contexts, leaving a systematic and unified approach unexplored. In this paper, we present HART, a hierarchical framework of AI risk levels, each corresponding to a detection task. To address these tasks, we propose a novel 2D Detection Method, decoupling a text into content and language expression. Our findings show that content is resistant to surface-level changes, which can serve as a key feature for detection. Experiments demonstrate that 2D method significantly outperforms existing detectors, achieving an AUROC improvement from 0.705 to 0.849 for level-2 detection and from 0.807 to 0.886 for RAID. We release our data and code at this https URL.</p>
  </details>
</details>
<details>
  <summary>147. <b>【2503.00245】CoSMoEs: Compact Sparse Mixture of Experts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00245">https://arxiv.org/abs/2503.00245</a></p>
  <p><b>作者</b>：Patrick Huber,Akshat Shrivastava,Ernie Chang,Chinnadhurai Sankar,Ahmed Aly,Adithya Sagar</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 11 pages, 8 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>148. <b>【2503.00231】Jawaher: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00231">https://arxiv.org/abs/2503.00231</a></p>
  <p><b>作者</b>：Samar M. Magdy,Sang Yun Kwon,Fakhraddin Alwajih,Safaa Abdelfadil,Shady Shehata,Muhammad Abdul-Mageed</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Project GitHub page is accessible at: [this https URL](https://github.com/UBC-NLP/jawaher) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>149. <b>【2503.00224】À la recherche du sens perdu: your favourite LLM might have more to say than you can understand</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00224">https://arxiv.org/abs/2503.00224</a></p>
  <p><b>作者</b>：K. O. T. Erziev</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 22 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>150. <b>【2503.00209】Autoencoder-Based Framework to Capture Vocabulary Quality in NLP</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00209">https://arxiv.org/abs/2503.00209</a></p>
  <p><b>作者</b>：Vu Minh Hoang Dang,Rakesh M. Verma</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Extended version of "Vocabulary Quality in NLP Datasets: An Autoencoder-Based Framework Across Domains and Languages" in IDA 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>151. <b>【2503.00203】Llamarine: Open-source Maritime Industry-specific Large Language Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00203">https://arxiv.org/abs/2503.00203</a></p>
  <p><b>作者</b>：William Nguyen,An Phan,Konobu Kimura,Hitoshi Maeno,Mika Tanaka,Quynh Le,William Poucher,Christopher Nguyen</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Work in progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>152. <b>【2503.00187】Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00187">https://arxiv.org/abs/2503.00187</a></p>
  <p><b>作者</b>：Hanjiang Hu,Alexander Robey,Changliu Liu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 28 pages, 10 figures, 7 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>153. <b>【2503.00179】Zero-Shot and Efficient Clarification Need Prediction in Conversational Search</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00179">https://arxiv.org/abs/2503.00179</a></p>
  <p><b>作者</b>：Lili Lu,Chuan Meng,Federico Ravenda,Mohammad Aliannejadi,Fabio Crestani</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>154. <b>【2503.00172】A Survey of Uncertainty Estimation Methods on Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00172">https://arxiv.org/abs/2503.00172</a></p>
  <p><b>作者</b>：Zhiqiu Xia,Jinxuan Xu,Yuqian Zhang,Hang Liu</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>155. <b>【2503.00162】PreMind: Multi-Agent Video Understanding for Advanced Indexing of Presentation-style Videos</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00162">https://arxiv.org/abs/2503.00162</a></p>
  <p><b>作者</b>：Kangda Wei,Zhengyu Zhou,Bingqing Wang,Jun Araki,Lukas Lange,Ruihong Huang,Zhe Feng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>156. <b>【2503.00151】Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00151">https://arxiv.org/abs/2503.00151</a></p>
  <p><b>作者</b>：Fakhraddin Alwajih,Abdellah El Mekki,Samar Mohamed Magdy,Abdelrahim A. Elmadany,Omer Nacar,El Moatez Billah Nagoudi,Reem Abdel-Salam,Hanin Atwany,Youssef Nafea,Abdulfattah Mohammed Yahya,Rahaf Alhamouri,Hamzah A. Alsayadi,Hiba Zayed,Sara Shatnawi,Serry Sibaee,Yasir Ech-Chammakhy,Walid Al-Dhabyani,Marwa Mohamed Ali,Imen Jarraya,Ahmed Oumar El-Shangiti,Aisha Alraeesi,Mohammed Anwar Al-Ghrawi,Abdulrahman S. Al-Batati,Elgizouli Mohamed,Noha Taha Elgindi,Muhammed Saeed,Houdaifa Atou,Issam Ait Yahia,Abdelhak Bouayad,Mohammed Machrouh,Amal Makouar,Dania Alkawi,Mukhtar Mohamed,Safaa Taher Abdelfadil,Amine Ziad Ounnoughene,Rouabhia Anfel,Rwaa Assi,Ahmed Sorkatti,Mohamedou Cheikh Tourad,Anis Koubaa,Ismail Berrada,Mustafa Jarrar,Shady Shehata,Muhammad Abdul-Mageed</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： More information about our dataset is available at our project page: [this https URL](https://github.com/UBC-NLP/palm) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>157. <b>【2503.00137】SCORE: Systematic COnsistency and Robustness Evaluation for Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00137">https://arxiv.org/abs/2503.00137</a></p>
  <p><b>作者</b>：Grigor Nalbandyan,Rima Shahbazyan,Evelina Bakhturina</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>158. <b>【2503.00134】Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00134">https://arxiv.org/abs/2503.00134</a></p>
  <p><b>作者</b>：Zhongqi Yang,Amir Rahmani</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>159. <b>【2503.00128】AnnoCaseLaw: A Richly-Annotated Dataset For Benchmarking Explainable Legal Judgment Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00128">https://arxiv.org/abs/2503.00128</a></p>
  <p><b>作者</b>：Magnus Sesodia,Alina Petrova,John Armour,Thomas Lukasiewicz,Oana-Maria Camburu,Puneet K. Dokania,Philip Torr,Christian Schroeder de Witt</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>160. <b>【2503.00124】Evaluation of LLMs-based Hidden States as Author Representations for Psychological Human-Centered NLP Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00124">https://arxiv.org/abs/2503.00124</a></p>
  <p><b>作者</b>：Nikita Soni,Pranav Chitale,Khushboo Singh,Niranjan Balasubramanian,H. Andrew Schwartz</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： To appear in Findings of NAACL 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>161. <b>【2503.00093】Rethinking LLM Bias Probing Using Lessons from the Social Sciences</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00093">https://arxiv.org/abs/2503.00093</a></p>
  <p><b>作者</b>：Kirsten N. Morehouse,Siddharth Swaroop,Weiwei Pan</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>162. <b>【2503.00084】InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00084">https://arxiv.org/abs/2503.00084</a></p>
  <p><b>作者</b>：Chong Zhang,Yukun Ma,Qian Chen,Wen Wang,Shengkui Zhao,Zexu Pan,Hao Wang,Chongjia Ni,Trung Hieu Nguyen,Kun Zhou,Yidi Jiang,Chaohong Tan,Zhifu Gao,Zhihao Du,Bin Ma</p>
  <p><b>类目</b>：ound (cs.SD); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Work in progress. Correspondence regarding this technical report should be directed to { [this http URL](http://chong.zhang) , [this http URL](http://yukun.ma) }@alibaba [this http URL](http://-inc.com) . Online demo available on [this https URL](https://modelscope.cn/studios/iic/InspireMusic) and [this https URL](https://huggingface.co/spaces/FunAudioLLM/InspireMusic) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>163. <b>【2503.00071】I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00071">https://arxiv.org/abs/2503.00071</a></p>
  <p><b>作者</b>：Esam Ghaleb,Bulat Khaertdinov,Aslı Özyürek,Raquel Fernández</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>164. <b>【2503.00069】Societal Alignment Frameworks Can Improve LLM Alignment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00069">https://arxiv.org/abs/2503.00069</a></p>
  <p><b>作者</b>：Karolina Stańczak,Nicholas Meade,Mehar Bhatia,Hattie Zhou,Konstantin Böttinger,Jeremy Barnes,Jason Stanley,Jessica Montgomery,Richard Zemel,Nicolas Papernot,Nicolas Chapados,Denis Therien,Timothy P. Lillicrap,Ana Marasović,Sylvie Delacroix,Gillian K. Hadfield,Siva Reddy</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>165. <b>【2503.00057】Improved YOLOv12 with LLM-Generated Synthetic Data for Enhanced Apple Detection and Benchmarking Against YOLOv11 and YOLOv10</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00057">https://arxiv.org/abs/2503.00057</a></p>
  <p><b>作者</b>：Ranjan Sapkota,Manoj Karkee</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 10 pages, 5 Figures, 2 Tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>166. <b>【2503.00043】VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00043">https://arxiv.org/abs/2503.00043</a></p>
  <p><b>作者</b>：Nilay Yilmaz,Maitreya Patel,Yiran Lawrence Luo,Tejas Gokhale,Chitta Baral,Suren Jayasuriya,Yezhou Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted at ICLR 2025. Code and data: [this https URL](https://github.com/nlylmz/Voila) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>167. <b>【2503.00038】from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00038">https://arxiv.org/abs/2503.00038</a></p>
  <p><b>作者</b>：Yu Yan,Sheng Sun,Zenghao Duan,Teli Liu,Min Liu,Zhiyi Yin,Qi Li,Jiangyu Lei</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： arXiv admin note: substantial text overlap with [arXiv:2412.12145](https://arxiv.org/abs/2412.12145) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>168. <b>【2503.00037】Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00037">https://arxiv.org/abs/2503.00037</a></p>
  <p><b>作者</b>：Wei Zhao,Zhe Li,Yige Li,Jun Sun</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>169. <b>【2503.00035】Constraining Sequential Model Editing with Editing Anchor Compression</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00035">https://arxiv.org/abs/2503.00035</a></p>
  <p><b>作者</b>：Hao-Xiang Xu,Jun-Yu Ma,Zhen-Hua Ling,Ningyu Zhang,Jia-Chen Gu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>170. <b>【2503.00032】Detecting LLM-Generated Korean Text through Linguistic Feature Analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00032">https://arxiv.org/abs/2503.00032</a></p>
  <p><b>作者</b>：Shinwoo Park,Shubin Kim,Do-Kyung Kim,Yo-Sub Han</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>171. <b>【2503.00031】Efficient Test-Time Scaling via Self-Calibration</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00031">https://arxiv.org/abs/2503.00031</a></p>
  <p><b>作者</b>：Chengsong Huang,Langlin Huang,Jixuan Leng,Jiacheng Liu,Jiaxin Huang</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>172. <b>【2503.00025】Evaluating Large Language Models on the Spanish Medical Intern Resident (MIR) Examination 2024/2025:A Comparative Analysis of Clinical Reasoning and Knowledge Application</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00025">https://arxiv.org/abs/2503.00025</a></p>
  <p><b>作者</b>：Carlos Luengo Vera,Ignacio Ferro Picon,M. Teresa del Val Nunez,Jose Andres Gomez Gandia,Antonio de Lucas Ancillo,Victor Ramos Arroyo,Carlos Milan Figueredo</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 26 pages, 1 table, 7 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>173. <b>【2503.00024】Do Emotions Really Affect Argument Convincingness? A Dynamic Approach with LLM-based Manipulation Checks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00024">https://arxiv.org/abs/2503.00024</a></p>
  <p><b>作者</b>：Yanran Chen,Steffen Eger</p>
  <p><b>类目</b>：Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Initial submit</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>174. <b>【2503.00022】KVCrush: Key value cache size-reduction using similarity in head-behaviour</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00022">https://arxiv.org/abs/2503.00022</a></p>
  <p><b>作者</b>：Gopi Krishna Jha,Sameh Gobriel,Liubov Talamanova,Alexander Kozlov,Nilesh Jain</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>175. <b>【2503.00020】A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00020">https://arxiv.org/abs/2503.00020</a></p>
  <p><b>作者</b>：Rakeen Rouf,Trupti Bavalatti,Osama Ahmed,Dhaval Potdar,Faraz Jawed</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted for publication in IEEE Access, DOI: [https://doi.org/10.1109/ACCESS.2025.3539933](https://doi.org/10.1109/ACCESS.2025.3539933) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>176. <b>【2503.00018】Eeyore: Realistic Depression Simulation via Supervised and Preference Optimization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00018">https://arxiv.org/abs/2503.00018</a></p>
  <p><b>作者</b>：Siyang Liu,Bianca Brie,Wenda Li,Laura Biester,Andrew Lee,James Pennebaker,Rada Mihalcea</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>177. <b>【2502.20730】DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.20730">https://arxiv.org/abs/2502.20730</a></p>
  <p><b>作者</b>：Zhuoqun Li,Haiyang Yu,Xuanang Chen,Hongyu Lin,Yaojie Lu,Fei Huang,Xianpei Han,Yongbin Li,Le Sun</p>
  <p><b>类目</b>：Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>178. <b>【2502.18509】Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.18509">https://arxiv.org/abs/2502.18509</a></p>
  <p><b>作者</b>：Ivoline Ngong,Swanand Kadhe,Hao Wang,Keerthiram Murugesan,Justin D. Weisz,Amit Dhurandhar,Karthikeyan Natesan Ramamurthy</p>
  <p><b>类目</b>：Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 22 pages, 2 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>179. <b>【2410.20724】Simple Is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.20724">https://arxiv.org/abs/2410.20724</a></p>
  <p><b>作者</b>：Mufei Li,Siqi Miao,Pan Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by ICLR 2025; Code available at [this https URL](https://github.com/Graph-COM/SubgraphRAG) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>180. <b>【2503.00733】UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00733">https://arxiv.org/abs/2503.00733</a></p>
  <p><b>作者</b>：Alexander H. Liu,Sang-gil Lee,Chao-Han Huck Yang,Yuan Gong,Yu-Chiang Frank Wang,James R. Glass,Rafael Valle,Bryan Catanzaro</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Sound (cs.SD)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： ICLR 2025; demo page at [this https URL](https://alexander-h-liu.github.io/uniwav-demo.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>181. <b>【2503.00725】Causal Inference on Outcomes Learned from Text</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00725">https://arxiv.org/abs/2503.00725</a></p>
  <p><b>作者</b>：Iman Modarressi,Jann Spiess,Amar Venugopal</p>
  <p><b>类目</b>：Econometrics (econ.EM); Computation and Language (cs.CL); Machine Learning (cs.LG); Methodology (stat.ME)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>182. <b>【2503.00612】Statistical Mechanics of Semantic Compression</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00612">https://arxiv.org/abs/2503.00612</a></p>
  <p><b>作者</b>：Tankut Can</p>
  <p><b>类目</b>：Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Computation and Language (cs.CL); Neurons and Cognition (q-bio.NC)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 20 pages (including appendix), 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>183. <b>【2503.00493】LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00493">https://arxiv.org/abs/2503.00493</a></p>
  <p><b>作者</b>：Boyi Kang,Xinfa Zhu,Zihan Zhang,Zhen Ye,Mingshuai Liu,Ziqian Wang,Yike Zhu,Guobin Ma,Jun Chen,Longshuai Xiao,Chao Weng,Wei Xue,Lei Xie</p>
  <p><b>类目</b>：Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Sound (cs.SD)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 13 pages, 2 figures, 8 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<h1>信息检索</h1>
<details>
  <summary>1. <b>【2503.01814】LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01814">https://arxiv.org/abs/2503.01814</a></p>
  <p><b>作者</b>：Weizhi Zhang,Liangwei Yang,Wooseong Yang,Henry Peng Zou,Yuqing Liu,Ke Xu,Sourav Medya,Philip S. Yu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Collaborative filtering models, Collaborative filtering, capturing user-item interactions, demonstrated strong performance, demonstrated strong</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Collaborative filtering models, particularly graph-based approaches, have demonstrated strong performance in capturing user-item interactions for recommendation systems. However, they continue to struggle in cold-start and data-sparse scenarios. The emergence of large language models (LLMs) like GPT and LLaMA presents new possibilities for enhancing recommendation performance, especially in cold-start settings. Despite their promise, LLMs pose challenges related to scalability and efficiency due to their high computational demands and limited ability to model complex user-item relationships effectively. In this work, we introduce a novel perspective on leveraging LLMs for CF model initialization. Through experiments, we uncover an embedding collapse issue when scaling CF models to larger embedding dimensions. To effectively harness large-scale LLM embeddings, we propose innovative selective initialization strategies utilizing random, uniform, and variance-based index sampling. Our comprehensive evaluation on multiple real-world datasets demonstrates significant performance gains across various CF models while maintaining a lower computational cost compared to existing LLM-based recommendation approaches.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2503.01776】Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01776">https://arxiv.org/abs/2503.01776</a></p>
  <p><b>作者</b>：Tiansheng Wen,Yifei Wang,Zequn Zeng,Zhong Peng,Yudi Su,Xinyang Liu,Bo Chen,Hongwei Liu,Stefanie Jegelka,Chenyu You</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：large-scale systems rely, high-quality deep representations, generative modeling, large-scale systems, systems rely</p>
  <p><b>备注</b>： A novel sparse coding framework designed for learning adaptive representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>3. <b>【2503.01763】Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01763">https://arxiv.org/abs/2503.01763</a></p>
  <p><b>作者</b>：Zhengliang Shi,Yuhan Wang,Lingyong Yan,Pengjie Ren,Shuaiqiang Wang,Dawei Yin,Zhaochun Ren</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：augment large language, Tool learning aims, solving practical tasks, learning aims, aims to augment</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2503.01713】SAGE: A Framework of Precise Retrieval for RAG</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01713">https://arxiv.org/abs/2503.01713</a></p>
  <p><b>作者</b>：Jintao Zhang,Guoliang Li,Jinyang Su</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：demonstrated significant proficiency, Retrieval-augmented generation, Current RAG methods, conducting question-answering, RAG</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.
In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG.
</p><p>Subjects:</p>
<p>Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Databases (cs.DB); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)</p>
<p>Cite as:<br>
arXiv:2503.01713 [cs.LG]</p>
<p>(or<br>
arXiv:2503.01713v1 [cs.LG] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.01713">https://doi.org/10.48550/arXiv.2503.01713</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>5. <b>【2503.01711】MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01711">https://arxiv.org/abs/2503.01711</a></p>
  <p><b>作者</b>：Weicong Qin,Yi Xu,Weijie Yu,Chenglei Shen,Ming He,Jianping Fan,Xiao Zhang,Jun Xu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：match users' preferences, aims to retrieve, retrieve and rank, rank items, items that match</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Personalized product search aims to retrieve and rank items that match users' preferences and search intent. Despite their effectiveness, existing approaches typically assume that users' query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2503.01670】Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the Lens of Summarization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01670">https://arxiv.org/abs/2503.01670</a></p>
  <p><b>作者</b>：Siya Qi,Rui Cao,Yulan He,Zheng Yuan</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：text quality evaluation, widely adopted approach, including hallucination evaluation, large language models, hallucination evaluation</p>
  <p><b>备注</b>： 8 pages, 5 figures for main body</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the rapid development of large language models (LLMs), LLM-as-a-judge has emerged as a widely adopted approach for text quality evaluation, including hallucination evaluation. While previous studies have focused exclusively on single-context evaluation (e.g., discourse faithfulness or world factuality), real-world hallucinations typically involve mixed contexts, which remains inadequately evaluated. In this study, we use summarization as a representative task to comprehensively evaluate LLMs' capability in detecting mixed-context hallucinations, specifically distinguishing between factual and non-factual hallucinations. Through extensive experiments across direct generation and retrieval-based models of varying scales, our main observations are: (1) LLMs' intrinsic knowledge introduces inherent biases in hallucination evaluation; (2) These biases particularly impact the detection of factual hallucinations, yielding a significant performance bottleneck; (3) The fundamental challenge lies in effective knowledge utilization, balancing between LLMs' intrinsic knowledge and external context for accurate mixed-context hallucination evaluation.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2503.01658】CoPL: Collaborative Preference Learning for Personalizing LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01658">https://arxiv.org/abs/2503.01658</a></p>
  <p><b>作者</b>：Youngbin Choi,Seunghyuk Cho,Minjong Lee,MoonJeong Park,Yesong Ko,Jungseul Ok,Dongwoo Kim</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Personalizing large language, Personalizing large, large language models, existing methods struggle, Collaborative Preference Learning</p>
  <p><b>备注</b>： 13pages, 4 figures, 6tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Personalizing large language models (LLMs) is important for aligning outputs with diverse user preferences, yet existing methods struggle with flexibility and generalization. We propose CoPL (Collaborative Preference Learning), a graph-based collaborative filtering framework that models user-response relationships to enhance preference estimation, particularly in sparse annotation settings. By integrating a mixture of LoRA experts, CoPL efficiently fine-tunes LLMs while dynamically balancing shared and user-specific preferences. Additionally, an optimization-free adaptation strategy enables generalization to unseen users without fine-tuning. Experiments on UltraFeedback-P demonstrate that CoPL outperforms existing personalized reward models, effectively capturing both common and controversial preferences, making it a scalable solution for personalized LLM alignment.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2503.01469】Hierarchical Causal Transformer with Heterogeneous Information for Expandable Sequential Recommendation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01469">https://arxiv.org/abs/2503.01469</a></p>
  <p><b>作者</b>：Hao Deng,Haibo Xing,Kanefumi Matsuyama,Yulei Huang,Jinxin Hu,Hong Wen,Jia Xu,Zulong Chen,Yu Zhang,Xiaoyi Zeng,Jing Zhang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Sequential recommendation systems, demonstrated exceptional capabilities, capturing user behavior, Sequential recommendation, recommendation systems leveraging</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Sequential recommendation systems leveraging transformer architectures have demonstrated exceptional capabilities in capturing user behavior patterns. At the core of these systems lies the critical challenge of constructing effective item representations. Traditional approaches employ feature fusion through simple concatenation or basic neural architectures to create uniform representation sequences. However, these conventional methods fail to address the intrinsic diversity of item attributes, thereby constraining the transformer's capacity to discern fine-grained patterns and hindering model extensibility. Although recent research has begun incorporating user-related heterogeneous features into item sequences, the equally crucial item-side heterogeneous feature continue to be neglected. To bridge this methodological gap, we present HeterRec - an innovative framework featuring two novel components: the Heterogeneous Token Flattening Layer (HTFL) and Hierarchical Causal Transformer (HCT). HTFL pioneers a sophisticated tokenization mechanism that decomposes items into multi-dimensional token sets and structures them into heterogeneous sequences, enabling scalable performance enhancement through model expansion. The HCT architecture further enhances pattern discovery through token-level and item-level attention mechanisms. furthermore, we develop a Listwise Multi-step Prediction (LMP) objective function to optimize learning process. Rigorous validation, including real-world industrial platforms, confirms HeterRec's state-of-the-art performance in both effective and efficiency.</p>
  </details>
</details>
<details>
  <summary>9. <b>【2503.01442】Leveraging LLMs for Mental Health: Detection and Recommendations from Social Discussions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01442">https://arxiv.org/abs/2503.01442</a></p>
  <p><b>作者</b>：Vaishali Aggarwal,Sachin Thukral,Krushil Patel,Arnab Chatterjee</p>
  <p><b>类目</b>：ocial and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Natural Language Processing, social platforms captures, pre-trained NLP models, mental health, Textual data</p>
  <p><b>备注</b>： 5 pages, 4 figures, 3 tables, to be published in WI-IAT 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Textual data from social platforms captures various aspects of mental health through discussions around and across issues, while users reach out for help and others sympathize and offer support. We propose a comprehensive framework that leverages Natural Language Processing (NLP) and Generative AI techniques to identify and assess mental health disorders, detect their severity, and create recommendations for behavior change and therapeutic interventions based on users' posts on Reddit.
To classify the disorders, we use rule-based labeling methods as well as advanced pre-trained NLP models to extract nuanced semantic features from the data. We fine-tune domain-adapted and generic pre-trained NLP models based on predictions from specialized Large Language Models (LLMs) to improve classification accuracy. Our hybrid approach combines the generalization capabilities of pre-trained models with the domain-specific insights captured by LLMs, providing an improved understanding of mental health discourse. Our findings highlight the strengths and limitations of each model, offering valuable insights into their practical applicability.
This research potentially facilitates early detection and personalized care to aid practitioners and aims to facilitate timely interventions and improve overall well-being, thereby contributing to the broader field of mental health surveillance and digital health analytics.
</p><p>Comments:<br>
5 pages, 4 figures, 3 tables, to be published in WI-IAT 2024</p>
<p>Subjects:</p>
<p>Social and Information Networks (<a target="_blank" rel="noopener" href="http://cs.SI">cs.SI</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)</p>
<p>Cite as:<br>
arXiv:2503.01442 [<a target="_blank" rel="noopener" href="http://cs.SI">cs.SI</a>]</p>
<p>(or<br>
arXiv:2503.01442v1 [<a target="_blank" rel="noopener" href="http://cs.SI">cs.SI</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.01442">https://doi.org/10.48550/arXiv.2503.01442</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>10. <b>【2503.01362】Streaming Piano Transcription Based on Consistent Onset and Offset Decoding with Sustain Pedal Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01362">https://arxiv.org/abs/2503.01362</a></p>
  <p><b>作者</b>：Weixing Wei,Jiahao Zhao,Yulun Wu,Kazuyoshi Yoshii</p>
  <p><b>类目</b>：ound (cs.SD); Information Retrieval (cs.IR); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：piano transcription approach, offset events, paper describes, aims to sequentially, sequentially translate</p>
  <p><b>备注</b>： Accepted to ISMIR 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper describes a streaming audio-to-MIDI piano transcription approach that aims to sequentially translate a music signal into a sequence of note onset and offset events. The sequence-to-sequence nature of this task may call for the computationally-intensive transformer model for better performance, which has recently been used for offline transcription benchmarks and could be extended for streaming transcription with causal attention mechanisms. We assume that the performance limitation of this naive approach lies in the decoder. Although time-frequency features useful for onset detection are considerably different from those for offset detection, the single decoder is trained to output a mixed sequence of onset and offset events without guarantee of the correspondence between the onset and offset events of the same note. To overcome this limitation, we propose a streaming encoder-decoder model that uses a convolutional encoder aggregating local acoustic features, followed by an autoregressive Transformer decoder detecting a variable number of onset events and another decoder detecting the offset events for the active pitches with validation of the sustain pedal at each time frame. Experiments using the MAESTRO dataset showed that the proposed streaming method performed comparably with or even better than the state-of-the-art offline methods while significantly reducing the computational cost.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2503.01346】SRAG: Structured Retrieval-Augmented Generation for Multi-Entity Question Answering over Wikipedia Graph</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01346">https://arxiv.org/abs/2503.01346</a></p>
  <p><b>作者</b>：Teng Lin,Yizhang Zhu,Yuyu Luo,Nan Tang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：poses significant challenges, large language models, consolidate scattered information, Multi-entity question answering, poses significant</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-entity question answering (MEQA) poses significant challenges for large language models (LLMs), which often struggle to consolidate scattered information across multiple documents. An example question might be "What is the distribution of IEEE Fellows among various fields of study?", which requires retrieving information from diverse sources e.g., Wikipedia pages. The effectiveness of current retrieval-augmented generation (RAG) methods is limited by the LLMs' capacity to aggregate insights from numerous pages. To address this gap, this paper introduces a structured RAG (SRAG) framework that systematically organizes extracted entities into relational tables (e.g., tabulating entities with schema columns like "name" and "field of study") and then apply table-based reasoning techniques. Our approach decouples retrieval and reasoning, enabling LLMs to focus on structured data analysis rather than raw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA tasks demonstrate that SRAG significantly outperforms state-of-the-art long-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy. The results underscore the efficacy of structuring unstructured data to enhance LLMs' reasoning capabilities.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2503.01334】Composed Multi-modal Retrieval: A Survey of Approaches and Applications</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01334">https://arxiv.org/abs/2503.01334</a></p>
  <p><b>作者</b>：Kun Zhang,Jingyu Li,Zhe Li,Jingjing Zhang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：utilizing heterogeneous information, short video platforms, Composed Multi-modal Retrieval, social media, heterogeneous information</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the rapid growth of multi-modal data from social media, short video platforms, and e-commerce, content-based retrieval has become essential for efficiently searching and utilizing heterogeneous information. Over time, retrieval techniques have evolved from Unimodal Retrieval (UR) to Cross-modal Retrieval (CR) and, more recently, to Composed Multi-modal Retrieval (CMR). CMR enables users to retrieve images or videos by integrating a reference visual input with textual modifications, enhancing search flexibility and precision. This paper provides a comprehensive review of CMR, covering its fundamental challenges, technical advancements, and categorization into supervised, zero-shot, and semi-supervised learning paradigms. We discuss key research directions, including data augmentation, model architecture, and loss optimization in supervised CMR, as well as transformation frameworks and external knowledge integration in zero-shot CMR. Additionally, we highlight the application potential of CMR in composed image retrieval, video retrieval, and person retrieval, which have significant implications for e-commerce, online search, and public security. Given its ability to refine and personalize search experiences, CMR is poised to become a pivotal technology in next-generation retrieval systems. A curated list of related works and resources is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>13. <b>【2503.01305】HI-Series Algorithms A Hybrid of Substance Diffusion Algorithm and Collaborative Filtering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01305">https://arxiv.org/abs/2503.01305</a></p>
  <p><b>作者</b>：Yu Peng,Ya-Hui An</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：traditional collaborative filtering, exhibit complementary limitations, collaborative filtering, complementary limitations, face the challenge</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recommendation systems face the challenge of balancing accuracy and diversity, as traditional collaborative filtering (CF) and network-based diffusion algorithms exhibit complementary limitations. While item-based CF (ItemCF) enhances diversity through item similarity, it compromises accuracy. Conversely, mass diffusion (MD) algorithms prioritize accuracy by favoring popular items but lack diversity. To address this trade-off, we propose the HI-series algorithms, hybrid models integrating ItemCF with diffusion-based approaches (MD, HHP, BHC, BD) through a nonlinear combination controlled by parameter $\epsilon$. This hybridization leverages ItemCF's diversity and MD's accuracy, extending to advanced diffusion models (HI-HHP, HI-BHC, HI-BD) for enhanced performance. Experiments on MovieLens, Netflix, and RYM datasets demonstrate that HI-series algorithms significantly outperform their base counterparts. In sparse data ($20\%$ training), HI-MD achieves a $0.8\%$-$4.4\%$ improvement in F1-score over MD while maintaining higher diversity (Diversity@20: 459 vs. 396 on MovieLens). For dense data ($80\%$ training), HI-BD improves F1-score by $2.3\%$-$5.2\%$ compared to BD, with diversity gains up to $18.6\%$. Notably, hybrid models consistently enhance novelty in sparse settings and exhibit robust parameter adaptability. The results validate that strategic hybridization effectively breaks the accuracy-diversity trade-off, offering a flexible framework for optimizing recommendation systems across data sparsity levels.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2503.01194】Cancer Type, Stage and Prognosis Assessment from Pathology Reports using LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01194">https://arxiv.org/abs/2503.01194</a></p>
  <p><b>作者</b>：Rachit Saluja,Jacob Rosenthal,Yoav Artzi,David J. Pisapia,Benjamin L. Liechty,Mert R. Sabuncu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：shown significant promise, Large Language Models, natural language processing, Large Language, language processing tasks</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have shown significant promise across various natural language processing tasks. However, their application in the field of pathology, particularly for extracting meaningful insights from unstructured medical texts such as pathology reports, remains underexplored and not well quantified. In this project, we leverage state-of-the-art language models, including the GPT family, Mistral models, and the open-source Llama models, to evaluate their performance in comprehensively analyzing pathology reports. Specifically, we assess their performance in cancer type identification, AJCC stage determination, and prognosis assessment, encompassing both information extraction and higher-order reasoning tasks. Based on a detailed analysis of their performance metrics in a zero-shot setting, we developed two instruction-tuned models: Path-llama3.1-8B and Path-GPT-4o-mini-FT. These models demonstrated superior performance in zero-shot cancer type identification, staging, and prognosis assessment compared to the other models evaluated.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2503.01151】ReaderLM-v2: Small Language Model for HTML to Markdown and JSON</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01151">https://arxiv.org/abs/2503.01151</a></p>
  <p><b>作者</b>：Feng Wang,Zesheng Shi,Bo Wang,Nan Wang,Han Xiao</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：billion parameter language, web content extraction, efficient web content, billion parameter, parameter language model</p>
  <p><b>备注</b>： 9 pages, 10-12 refs</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present ReaderLM-v2, a compact 1.5 billion parameter language model designed for efficient web content extraction. Our model processes documents up to 512K tokens, transforming messy HTML into clean Markdown or JSON formats with high accuracy -- making it an ideal tool for grounding large language models. The model's effectiveness results from two key innovations: (1) a three-stage data synthesis pipeline that generates high quality, diverse training data by iteratively drafting, refining, and critiquing web content extraction; and (2) a unified training framework combining continuous pre-training with multi-objective optimization. Intensive evaluation demonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger models by 15-20\% on carefully curated benchmarks, particularly excelling at documents exceeding 100K tokens, while maintaining significantly lower computational requirements.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2503.01031】Can We Find the Code? An Empirical Study of Google Scholar's Code Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01031">https://arxiv.org/abs/2503.01031</a></p>
  <p><b>作者</b>：Shi-Shun Chen</p>
  <p><b>类目</b>：Digital Libraries (cs.DL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Google Scholar, valuable resources, Scholar, Google, code</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Academic codes associated with research papers are valuable resources for scholars. In specialized fields outside computer science, code availability is often limited, making effective code retrieval essential. Google Scholar is a crucial academic search tool. If a code published in the paper is not retrievable via Google Scholar, its accessibility and impact are significantly reduced. This study takes the term "accelerated degradation" combined with "reliability" as an example, and finds that, for papers published by Elsevier, only GitHub links included in abstracts are comprehensively retrieved by Google Scholar. When such links appear within the main body of a paper, even in the "Data Availability" section, they may be ignored and become unsearchable. These findings highlight the importance of strategically placing GitHub links in abstracts to enhance code discoverability on Google Scholar.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2503.01003】A Semantic Search Pipeline for Causality-driven Adhoc Information Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01003">https://arxiv.org/abs/2503.01003</a></p>
  <p><b>作者</b>：Dhairya Dalal,Sharmi Dev Gupta,Bentolhoda Binaei</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Causality-driven Adhoc Information, Adhoc Information Retrieval, Causality-driven Adhoc, Adhoc Information, CAIR shared task</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present a unsupervised semantic search pipeline for the Causality-driven Adhoc Information Retrieval (CAIR-2021) shared task. The CAIR shared task expands traditional information retrieval to support the retrieval of documents containing the likely causes of a query event. A successful system must be able to distinguish between topical documents and documents containing causal descriptions of events that are causally related to the query event. Our approach involves aggregating results from multiple query strategies over a semantic and lexical index. The proposed approach leads the CAIR-2021 leaderboard and outperformed both traditional IR and pure semantic embedding-based approaches.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2503.01001】owards An Efficient LLM Training Paradigm for CTR Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01001">https://arxiv.org/abs/2503.01001</a></p>
  <p><b>作者</b>：Allen Lin,Renqin Cai,Yun He,Hanchao Yu,Jing Qian,Rui Li,Qifan Wang,James Caverlee</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, demonstrated tremendous potential, Large Language, ranking-based recommendation system</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have demonstrated tremendous potential as the next-generation ranking-based recommendation system. Many recent works have shown that LLMs can significantly outperform conventional click-through-rate (CTR) prediction approaches. Despite such promising results, the computational inefficiency inherent in the current training paradigm makes it particularly challenging to train LLMs for ranking-based recommendation tasks on large datasets. To train LLMs for CTR prediction, most existing studies adopt the prevalent ''sliding-window'' paradigm. Given a sequence of $m$ user interactions, a unique training prompt is constructed for each interaction by designating it as the prediction target along with its preceding $n$ interactions serving as context. In turn, the sliding-window paradigm results in an overall complexity of $O(mn^2)$ that scales linearly with the length of user interactions. Consequently, a direct adoption to train LLMs with such strategy can result in prohibitively high training costs as the length of interactions grows. To alleviate the computational inefficiency, we propose a novel training paradigm, namely Dynamic Target Isolation (DTI), that structurally parallelizes the training of $k$ (where $k  1$) target interactions. Furthermore, we identify two major bottlenecks - hidden-state leakage and positional bias overfitting - that limit DTI to only scale up to a small value of $k$ (e.g., 5) then propose a computationally light solution to effectively tackle each. Through extensive experiments on three widely adopted public CTR datasets, we empirically show that DTI reduces training time by an average of $\textbf{92%}$ (e.g., from $70.5$ hrs to $5.31$ hrs), without compromising CTR prediction performance.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2503.00999】Federated Conversational Recommender System</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00999">https://arxiv.org/abs/2503.00999</a></p>
  <p><b>作者</b>：Allen Lin,Jianling Wang,Ziwei Zhu,James Caverlee</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Conversational Recommender Systems, personalized recommendation experiences, providing personalized recommendation, Recommender Systems, Conversational Recommender</p>
  <p><b>备注</b>： ECIR 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Conversational Recommender Systems (CRSs) have become increasingly popular as a powerful tool for providing personalized recommendation experiences. By directly engaging with users in a conversational manner to learn their current and fine-grained preferences, a CRS can quickly derive recommendations that are relevant and justifiable. However, existing conversational recommendation systems (CRSs) typically rely on a centralized training and deployment process, which involves collecting and storing explicitly-communicated user preferences in a centralized repository. These fine-grained user preferences are completely human-interpretable and can easily be used to infer sensitive information (e.g., financial status, political stands, and health information) about the user, if leaked or breached. To address the user privacy concerns in CRS, we first define a set of privacy protection guidelines for preserving user privacy under the conversational recommendation setting. Based on these guidelines, we propose a novel federated conversational recommendation framework that effectively reduces the risk of exposing user privacy by (i) de-centralizing both the historical interests estimation stage and the interactive preference elicitation stage and (ii) strictly bounding privacy leakage by enforcing user-level differential privacy with meticulously selected privacy budgets. Through extensive experiments, we show that the proposed framework not only satisfies these user privacy protection guidelines, but also enables the system to achieve competitive recommendation performance even when compared to the state-of-the-art non-private conversational recommendation approach.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2503.00863】Systematic Literature Review on Clinical Trial Eligibility Matching</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00863">https://arxiv.org/abs/2503.00863</a></p>
  <p><b>作者</b>：Muhammad Talha Sharif,Abdul Rehman</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：reliable study outcomes, participants meet precise, meet precise criteria, Natural Language Processing, Clinical trial eligibility</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Clinical trial eligibility matching is a critical yet often labor-intensive and error-prone step in medical research, as it ensures that participants meet precise criteria for safe and reliable study outcomes. Recent advances in Natural Language Processing (NLP) have shown promise in automating and improving this process by rapidly analyzing large volumes of unstructured clinical text and structured electronic health record (EHR) data. In this paper, we present a systematic overview of current NLP methodologies applied to clinical trial eligibility screening, focusing on data sources, annotation practices, machine learning approaches, and real-world implementation challenges. A comprehensive literature search (spanning Google Scholar, Mendeley, and PubMed from 2015 to 2024) yielded high-quality studies, each demonstrating the potential of techniques such as rule-based systems, named entity recognition, contextual embeddings, and ontology-based normalization to enhance patient matching accuracy. While results indicate substantial improvements in screening efficiency and precision, limitations persist regarding data completeness, annotation consistency, and model scalability across diverse clinical domains. The review highlights how explainable AI and standardized ontologies can bolster clinician trust and broaden adoption. Looking ahead, further research into advanced semantic and temporal representations, expanded data integration, and rigorous prospective evaluations is necessary to fully realize the transformative potential of NLP in clinical trial recruitment.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2503.00781】owards Efficient Educational Chatbots: Benchmarking RAG Frameworks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00781">https://arxiv.org/abs/2503.00781</a></p>
  <p><b>作者</b>：Umar Ali Khan,Ekram Khan,Fiza Khan,Athar Ali Moinuddin</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Large Language Models, Large Language, proven immensely beneficial, capturing vast amounts, Graduate Aptitude Test</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large Language Models (LLMs) have proven immensely beneficial in education by capturing vast amounts of literature-based information, allowing them to generate context without relying on external sources. In this paper, we propose a generative AI-powered GATE question-answering framework (GATE stands for Graduate Aptitude Test in Engineering) that leverages LLMs to explain GATE solutions and support students in their exam preparation. We conducted extensive benchmarking to select the optimal embedding model and LLM, evaluating our framework based on criteria such as latency, faithfulness, and relevance, with additional validation through human evaluation. Our chatbot integrates state-of-the-art embedding models and LLMs to deliver accurate, context-aware responses. Through rigorous experimentation, we identified configurations that balance performance and computational efficiency, ensuring a reliable chatbot to serve students' needs. Additionally, we discuss the challenges faced in data processing and modeling and implemented solutions. Our work explores the application of Retrieval-Augmented Generation (RAG) for GATE Q/A explanation tasks, and our findings demonstrate significant improvements in retrieval accuracy and response quality. This research offers practical insights for developing effective AI-driven educational tools while highlighting areas for future enhancement in usability and scalability.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2503.00674】OrdRankBen: A Novel Ranking Benchmark for Ordinal Relevance in NLP</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00674">https://arxiv.org/abs/2503.00674</a></p>
  <p><b>作者</b>：Yan Wang,Lingfei Qian,Xueqing Peng,Jimin Huang,Dongji Feng</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 6 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>23. <b>【2503.00619】PinLanding: Content-First Keyword Landing Page Generation via Multi-Modal AI for Web-Scale Discovery</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00619">https://arxiv.org/abs/2503.00619</a></p>
  <p><b>作者</b>：Faye Zhang,Jasmine Wan,Qianyu Cheng,Jinfeng Rao</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Pinterest hosting vast, Pinterest hosting, Online platforms, collections traditionally rely, hosting vast content</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Online platforms like Pinterest hosting vast content collections traditionally rely on manual curation or user-generated search logs to create keyword landing pages (KLPs) -- topic-centered collection pages that serve as entry points for content discovery. While manual curation ensures quality, it doesn't scale to millions of collections, and search log approaches result in limited topic coverage and imprecise content matching. In this paper, we present PinLanding, a novel content-first architecture that transforms the way platforms create topical collections. Instead of deriving topics from user behavior, our system employs a multi-stage pipeline combining vision-language model (VLM) for attribute extraction, large language model (LLM) for topic generation, and a CLIP-based dual-encoder architecture for precise content matching. Our model achieves 99.7% Recall@10 on Fashion200K benchmark, demonstrating strong attribute understanding capabilities. In production deployment for search engine optimization with 4.2 million shopping landing pages, the system achieves a 4X increase in topic coverage and 14.29% improvement in collection attribute precision over the traditional search log-based approach via human evaluation. The architecture can be generalized beyond search traffic to power various user experiences, including content discovery and recommendations, providing a scalable solution to transform unstructured content into curated topical collections across any content domain.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2503.00501】Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00501">https://arxiv.org/abs/2503.00501</a></p>
  <p><b>作者</b>：Jia Chen,Qian Dong,Haitao Li,Xiaohui He,Yan Gao,Shaosheng Cao,Yi Wu,Ping Yang,Chen Xu,Yao Hu,Qingyao Ai,Yiqun Liu</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：improve user experiences, User-generated content, integrating visual, visual and textual, featuring multimodal content</p>
  <p><b>备注</b>： 11 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\R. To address the growing need for developing better S\R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\%. In contrast to existing datasets, \textsf{Qilin} offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \ evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\R systems. We hope that \textsf{Qilin} will significantly contribute to the advancement of multimodal content platforms with S\R services in the future.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2503.00479】Bayesian Active Learning for Multi-Criteria Comparative Judgement in Educational Assessment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00479">https://arxiv.org/abs/2503.00479</a></p>
  <p><b>作者</b>：Andy Gray,Alma Rahat,Tom Crick,Stephen Lindsay</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Information Retrieval (cs.IR); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>26. <b>【2503.00353】U-NIAH: Unified RAG and LLM Evaluation for Long Context Needle-In-A-Haystack</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00353">https://arxiv.org/abs/2503.00353</a></p>
  <p><b>作者</b>：Yunfan Gao,Yun Xiong,Wenlong Wu,Zijing Huang,Bohan Li,Haofen Wang</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Large Language Models, Language Models, Large Language, Recent advancements, Retrieval-Augmented Generation</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in Large Language Models (LLMs) have expanded their context windows to unprecedented lengths, sparking debates about the necessity of Retrieval-Augmented Generation (RAG). To address the fragmented evaluation paradigms and limited cases in existing Needle-in-a-Haystack (NIAH), this paper introduces U-NIAH, a unified framework that systematically compares LLMs and RAG methods in controlled long context settings. Our framework extends beyond traditional NIAH by incorporating multi-needle, long-needle, and needle-in-needle configurations, along with different retrieval settings, while leveraging the synthetic Starlight Academy dataset-a fictional magical universe-to eliminate biases from pre-trained knowledge. Through extensive experiments, we investigate three research questions: (1) performance trade-offs between LLMs and RAG, (2) error patterns in RAG, and (3) RAG's limitations in complex settings. Our findings show that RAG significantly enhances smaller LLMs by mitigating the "lost-in-the-middle" effect and improving robustness, achieving an 82.58% win-rate over LLMs. However, we observe that retrieval noise and reverse chunk ordering degrade performance, while surprisingly, advanced reasoning LLMs exhibit reduced RAG compatibility due to sensitivity to semantic distractors. We identify typical error patterns including omission due to noise, hallucination under high noise critical condition, and self-doubt behaviors. Our work not only highlights the complementary roles of RAG and LLMs, but also provides actionable insights for optimizing deployments. Code: this https URL.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2503.00309】Pseudo-Knowledge Graph: Meta-Path Guided Retrieval and In-Graph Text for RAG-Equipped LLM</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00309">https://arxiv.org/abs/2503.00309</a></p>
  <p><b>作者</b>：Yuxin Yang,Haoyang Wu,Tao Wang,Jia Yang,Hao Ma,Guojie Luo</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>28. <b>【2503.00278】NeuroLit Navigator: A Neurosymbolic Approach to Scholarly Article Searches for Systematic Reviews</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00278">https://arxiv.org/abs/2503.00278</a></p>
  <p><b>作者</b>：Vedant Khandelwal,Kaushik Roy,Valerie Lookingbill,Ritvik Garimella,Harshul Surana,Heather Heckman,Amit Sheth</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Computation and Language (cs.CL); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：Large Language Models, personalized learning materials, introduction of Large, Language Models, Large Language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The introduction of Large Language Models (LLMs) has significantly impacted various fields, including education, for example, by enabling the creation of personalized learning materials. However, their use in Systematic Reviews (SRs) reveals limitations such as restricted access to specialized vocabularies, lack of domain-specific reasoning, and a tendency to generate inaccurate information. Existing SR tools often rely on traditional NLP methods and fail to address these issues adequately. To overcome these challenges, we developed the ``NeuroLit Navigator,'' a system that combines domain-specific LLMs with structured knowledge sources like Medical Subject Headings (MeSH) and the Unified Medical Language System (UMLS). This integration enhances query formulation, expands search vocabularies, and deepens search scopes, enabling more precise searches. Deployed in multiple universities and tested by over a dozen librarians, the NeuroLit Navigator has reduced the time required for initial literature searches by 90\%. Despite this efficiency, the initial set of articles retrieved can vary in relevance and quality. Nonetheless, the system has greatly improved the reproducibility of search results, demonstrating its potential to support librarians in the SR process.</p>
  </details>
</details>
<details>
  <summary>29. <b>【2503.00238】Passage Query Methods for Retrieval and Reranking in Conversational Agents</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00238">https://arxiv.org/abs/2503.00238</a></p>
  <p><b>作者</b>：Victor De Lima,Grace Hui Yang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 7 pages, 3 figures. In Proceedings of the Thirty-Third Text Retrieval Conference (TREC 2024), November 18-22, 2024, Rockville, MD, USA</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>30. <b>【2503.00223】DeepRetrieval: Powerful Query Generation for Information Retrieval with Reinforcement Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00223">https://arxiv.org/abs/2503.00223</a></p>
  <p><b>作者</b>：Pengcheng Jiang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Ongoing work</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>31. <b>【2503.00179】Zero-Shot and Efficient Clarification Need Prediction in Conversational Search</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00179">https://arxiv.org/abs/2503.00179</a></p>
  <p><b>作者</b>：Lili Lu,Chuan Meng,Federico Ravenda,Mohammad Aliannejadi,Fabio Crestani</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>32. <b>【2503.00072】Enhancing Collaborative Filtering-Based Course Recommendations by Exploiting Time-to-Event Information with Survival Analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00072">https://arxiv.org/abs/2503.00072</a></p>
  <p><b>作者</b>：Alireza Gharahighehi,Achilleas Ghinis,Michela Venturini,Frederik Cornillie,Celine Vens</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 19 pages, 1 figure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>33. <b>【2410.20724】Simple Is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.20724">https://arxiv.org/abs/2410.20724</a></p>
  <p><b>作者</b>：Mufei Li,Siqi Miao,Pan Li</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by ICLR 2025; Code available at [this https URL](https://github.com/Graph-COM/SubgraphRAG) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<h1>计算机视觉</h1>
<details>
  <summary>1. <b>【2503.01845】Denoising Functional Maps: Diffusion Models for Shape Correspondence</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01845">https://arxiv.org/abs/2503.01845</a></p>
  <p><b>作者</b>：Aleksei Zhuravlev,Zorah Lähner,Vladislav Golyanik</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：deformable shapes remains, Estimating correspondences, challenging problem, remains a challenging, Estimating</p>
  <p><b>备注</b>： Accepted at CVPR 2025; Project page: [this https URL](https://alekseizhuravlev.github.io/denoising-functional-maps/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Estimating correspondences between pairs of deformable shapes remains a challenging problem. Despite substantial progress, existing methods lack broad generalization capabilities and require category-specific training data. To address these limitations, we propose a fundamentally new approach to shape correspondence based on denoising diffusion models. In our method, a diffusion model learns to directly predict the functional map, a low-dimensional representation of a point-wise map between shapes. We use a large dataset of synthetic human meshes for training and employ two steps to reduce the number of functional maps that need to be learned. First, the maps refer to a template rather than shape pairs. Second, the functional map is defined in a basis of eigenvectors of the Laplacian, which is not unique due to sign ambiguity. Therefore, we introduce an unsupervised approach to select a specific basis by correcting the signs of eigenvectors based on surface features. Our approach achieves competitive performance on standard human datasets, meshes with anisotropic connectivity, non-isometric humanoid shapes, as well as animals compared to existing descriptor-based and large-scale shape deformation methods.</p>
  </details>
</details>
<details>
  <summary>2. <b>【2503.01839】Jailbreaking Safeguarded Text-to-Image Models via Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01839">https://arxiv.org/abs/2503.01839</a></p>
  <p><b>作者</b>：Zhengyuan Jiang,Yuepeng Hu,Yuchen Yang,Yinzhi Cao,Neil Zhenqiang Gong</p>
  <p><b>类目</b>：Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：pornographic images, generate harmful content, harmful content, safety guardrails, safety</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text-to-Image models may generate harmful content, such as pornographic images, particularly when unsafe prompts are submitted. To address this issue, safety filters are often added on top of text-to-image models, or the models themselves are aligned to reduce harmful outputs. However, these defenses remain vulnerable when an attacker strategically designs adversarial prompts to bypass these safety guardrails. In this work, we propose PromptTune, a method to jailbreak text-to-image models with safety guardrails using a fine-tuned large language model. Unlike other query-based jailbreak attacks that require repeated queries to the target model, our attack generates adversarial prompts efficiently after fine-tuning our AttackLLM. We evaluate our method on three datasets of unsafe prompts and against five safety guardrails. Our results demonstrate that our approach effectively bypasses safety guardrails, outperforms existing no-box attacks, and also facilitates other query-based attacks.</p>
  </details>
</details>
<details>
  <summary>3. <b>【2503.01837】Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01837">https://arxiv.org/abs/2503.01837</a></p>
  <p><b>作者</b>：Adrià López Escoriza,Nicklas Hansen,Stone Tao,Tongzhou Mu,Hao Su</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：expansive state-action space, robotic manipulation present, manipulation present significant, present significant challenges, designing dense reward</p>
  <p><b>备注</b>： Project page can be found at [this https URL](https://adrialopezescoriza.github.io/demo3/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Long-horizon tasks in robotic manipulation present significant challenges in reinforcement learning (RL) due to the difficulty of designing dense reward functions and effectively exploring the expansive state-action space. However, despite a lack of dense rewards, these tasks often have a multi-stage structure, which can be leveraged to decompose the overall objective into manageable subgoals. In this work, we propose DEMO3, a framework that exploits this structure for efficient learning from visual inputs. Specifically, our approach incorporates multi-stage dense reward learning, a bi-phasic training scheme, and world model learning into a carefully designed demonstration-augmented RL framework that strongly mitigates the challenge of exploration in long-horizon tasks. Our evaluations demonstrate that our method improves data-efficiency by an average of 40% and by 70% on particularly difficult tasks compared to state-of-the-art approaches. We validate this across 16 sparse-reward tasks spanning four domains, including challenging humanoid visual control tasks using as few as five demonstrations.</p>
  </details>
</details>
<details>
  <summary>4. <b>【2503.01835】Primus: Enforcing Attention Usage for 3D Medical Image Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01835">https://arxiv.org/abs/2503.01835</a></p>
  <p><b>作者</b>：Tassilo Wald,Saikat Roy,Fabian Isensee,Constantin Ulrich,Sebastian Ziegler,Dasha Trofimova,Raphael Stock,Michael Baumgartner,Gregor Köhler,Klaus Maier-Hein</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：dominating major benchmarks, achieved remarkable success, major benchmarks, achieved remarkable, remarkable success</p>
  <p><b>备注</b>： Preprint</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Transformers have achieved remarkable success across multiple fields, yet their impact on 3D medical image segmentation remains limited with convolutional networks still dominating major benchmarks. In this work, we a) analyze current Transformer-based segmentation models and identify critical shortcomings, particularly their over-reliance on convolutional blocks. Further, we demonstrate that in some architectures, performance is unaffected by the absence of the Transformer, thereby demonstrating their limited effectiveness. To address these challenges, we move away from hybrid architectures and b) introduce a fully Transformer-based segmentation architecture, termed Primus. Primus leverages high-resolution tokens, combined with advances in positional embeddings and block design, to maximally leverage its Transformer blocks. Through these adaptations Primus surpasses current Transformer-based methods and competes with state-of-the-art convolutional models on multiple public datasets. By doing so, we create the first pure Transformer architecture and take a significant step towards making Transformers state-of-the-art for 3D medical image segmentation.</p>
  </details>
</details>
<details>
  <summary>5. <b>【2503.01794】OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-Adjustment</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01794">https://arxiv.org/abs/2503.01794</a></p>
  <p><b>作者</b>：Junhyun Park,Chanyu Moon,Donghwan Lee,Kyungsu Kim,Minho Hwang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Contrastive Language-Image Pre-Training, Language-Image Pre-Training, reducing reliance, manual annotations, reliance on manual</p>
  <p><b>备注</b>： 10 pages, 3 figures, and 5 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shot classification in radiology, reducing reliance on manual annotations. However, conventional contrastive learning struggles with normal case detection due to its strict intra-sample alignment, which disrupts normal sample clustering and leads to high false positives (FPs) and false negatives (FNs). To address these issues, we propose OFF-CLIP, a contrastive learning refinement that improves normal detection by introducing an off-diagonal term loss to enhance normal sample clustering and applying sentence-level text filtering to mitigate FNs by removing misaligned normal statements from abnormal reports. OFF-CLIP can be applied to radiology CLIP models without requiring any architectural modifications. Experimental results show that OFF-CLIP significantly improves normal classification, achieving a 0.61 Area under the curve (AUC) increase on VinDr-CXR over CARZero, the state-of-the-art zero-shot classification baseline, while maintaining or improving abnormal classification performance. Additionally, OFF-CLIP enhances zero-shot grounding by improving pointing game accuracy, confirming better anomaly localization. These results demonstrate OFF-CLIP's effectiveness as a robust and efficient enhancement for medical vision-language models.</p>
  </details>
</details>
<details>
  <summary>6. <b>【2503.01785】Visual-RFT: Visual Reinforcement Fine-Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01785">https://arxiv.org/abs/2503.01785</a></p>
  <p><b>作者</b>：Ziyu Liu,Zeyi Sun,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：learns from feedback, data is scarce, Reinforcement Fine-Tuning, Reinforcement, Visual Reinforcement Fine-Tuning</p>
  <p><b>备注</b>： project page: [this https URL](https://github.com/Liuziyu77/Visual-RFT) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by $24.3\%$ over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by $21.9$ on COCO's two-shot setting and $15.4$ on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.</p>
  </details>
</details>
<details>
  <summary>7. <b>【2503.01783】vS-Graphs: Integrating Visual SLAM and Situational Graphs through Multi-level Scene Understanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01783">https://arxiv.org/abs/2503.01783</a></p>
  <p><b>作者</b>：Ali Tourani,Saad Ejaz,Hriday Bavle,David Morilla-Cabello,Jose Luis Sanchez-Lopez,Holger Voos</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Current Visual Simultaneous, Visual Simultaneous Localization, Current Visual, Simultaneous Localization, Visual Simultaneous</p>
  <p><b>备注</b>： 13 pages, 8 figures, 2 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Current Visual Simultaneous Localization and Mapping (VSLAM) systems often struggle to create maps that are both semantically rich and easily interpretable. While incorporating semantic scene knowledge aids in building richer maps with contextual associations among mapped objects, representing them in structured formats like scene graphs has not been widely addressed, encountering complex map comprehension and limited scalability. This paper introduces visual S-Graphs (vS-Graphs), a novel real-time VSLAM framework that integrates vision-based scene understanding with map reconstruction and comprehensible graph-based representation. The framework infers structural elements (i.e., rooms and corridors) from detected building components (i.e., walls and ground surfaces) and incorporates them into optimizable 3D scene graphs. This solution enhances the reconstructed map's semantic richness, comprehensibility, and localization accuracy. Extensive experiments on standard benchmarks and real-world datasets demonstrate that vS-Graphs outperforms state-of-the-art VSLAM methods, reducing trajectory error by an average of 3.38% and up to 9.58% on real-world data. Furthermore, the proposed framework achieves environment-driven semantic entity detection accuracy comparable to precise LiDAR-based frameworks using only visual features. A web page containing more media and evaluation outcomes is available on this https URL.</p>
  </details>
</details>
<details>
  <summary>8. <b>【2503.01776】Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01776">https://arxiv.org/abs/2503.01776</a></p>
  <p><b>作者</b>：Tiansheng Wen,Yifei Wang,Zequn Zeng,Zhong Peng,Yudi Su,Xinyang Liu,Bo Chen,Hongwei Liu,Stefanie Jegelka,Chenyu You</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)</p>
  <p><b>关键词</b>：large-scale systems rely, high-quality deep representations, generative modeling, large-scale systems, systems rely</p>
  <p><b>备注</b>： A novel sparse coding framework designed for learning adaptive representation</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>9. <b>【2503.01774】Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01774">https://arxiv.org/abs/2503.01774</a></p>
  <p><b>作者</b>：Jay Zhangjie Wu,Yuxuan Zhang,Haithem Turki,Xuanchi Ren,Jun Gao,Mike Zheng Shou,Sanja Fidler,Zan Gojcic,Huan Ling</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Neural Radiance Fields, Gaussian Splatting, Radiance Fields, Splatting have revolutionized, novel-view synthesis task</p>
  <p><b>备注</b>： CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2$\times$ improvement in FID score over baselines while maintaining 3D consistency.</p>
  </details>
</details>
<details>
  <summary>10. <b>【2503.01768】SHADE-AD: An LLM-Based Framework for Synthesizing Activity Data of Alzheimer's Patients</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01768">https://arxiv.org/abs/2503.01768</a></p>
  <p><b>作者</b>：Heming Fu,Hongkai Chen,Shan Lin,Guoliang Xing</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Alzheimer Disease, increasingly critical global, global health concern, critical global health, Large Language Model</p>
  <p><b>备注</b>： 7 pages, 6 figures, ACM SenSys'25</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Alzheimer's Disease (AD) has become an increasingly critical global health concern, which necessitates effective monitoring solutions in smart health applications. However, the development of such solutions is significantly hindered by the scarcity of AD-specific activity datasets. To address this challenge, we propose SHADE-AD, a Large Language Model (LLM) framework for Synthesizing Human Activity Datasets Embedded with AD features. Leveraging both public datasets and our own collected data from 99 AD patients, SHADE-AD synthesizes human activity videos that specifically represent AD-related behaviors. By employing a three-stage training mechanism, it broadens the range of activities beyond those collected from limited deployment settings. We conducted comprehensive evaluations of the generated dataset, demonstrating significant improvements in downstream tasks such as Human Activity Recognition (HAR) detection, with enhancements of up to 79.69%. Detailed motion metrics between real and synthetic data show strong alignment, validating the realism and utility of the synthesized dataset. These results underscore SHADE-AD's potential to advance smart health applications by providing a cost-effective, privacy-preserving solution for AD monitoring.</p>
  </details>
</details>
<details>
  <summary>11. <b>【2503.01754】Enhancing Multi-hop Reasoning in Vision-Language Models via Self-Distillation with Multi-Prompt Ensembling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01754">https://arxiv.org/abs/2503.01754</a></p>
  <p><b>作者</b>：Guande Wu,Huan Song,Yawei Wang,Qiaojing Yan,Yijun Tian,Lin Lee Cheong,Panpan Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：advancement alongside large, alongside large language, large language models, rapid advancement alongside, Multi-modal large language</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-modal large language models have seen rapid advancement alongside large language models. However, while language models can effectively leverage chain-of-thought prompting for zero or few-shot learning, similar prompting strategies are less effective for multi-modal LLMs due to modality gaps and task complexity. To address this challenge, we explore two prompting approaches: a dual-query method that separates multi-modal input analysis and answer generation into two prompting steps, and an ensemble prompting method that combines multiple prompt variations to arrive at the final answer. Although these approaches enhance the model's reasoning capabilities without fine-tuning, they introduce significant inference overhead. Therefore, building on top of these two prompting techniques, we propose a self-distillation framework such that the model can improve itself without any annotated data. Our self-distillation framework learns representation intervention modules from the reasoning traces collected from ensembled dual-query prompts, in the form of hidden representations. The lightweight intervention modules operate in parallel with the frozen original model, which makes it possible to maintain computational efficiency while significantly improving model capability. We evaluate our method on five widely-used VQA benchmarks, demonstrating its effectiveness in performing multi-hop reasoning for complex tasks.</p>
  </details>
</details>
<details>
  <summary>12. <b>【2503.01739】VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01739">https://arxiv.org/abs/2503.01739</a></p>
  <p><b>作者</b>：Wenhao Wang,Yi Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：offering wide-ranging applications, dynamic visual content, generative models convert, models convert textual, convert textual prompts</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal ($0.29\%$) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over $1.09$ million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify $1,291$ user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about $1.09$ million video clips. Our experiments reveal that (1) current $16$ text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at this https URL under the CC BY 4.0 License.</p>
  </details>
</details>
<details>
  <summary>13. <b>【2503.01733】DISCOVER: Data-driven Identification of Sub-activities via Clustering and Visualization for Enhanced Activity Recognition in Smart Homes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01733">https://arxiv.org/abs/2503.01733</a></p>
  <p><b>作者</b>：Alexander Karpekov,Sonia Chernova,Thomas Plötz</p>
  <p><b>类目</b>：Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Human Activity Recognition, Activity Recognition, independent living, great potential, elder care</p>
  <p><b>备注</b>： v1: Initial submission. Under review at IMWUT</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Human Activity Recognition (HAR) using ambient sensors has great potential for practical applications, particularly in elder care and independent living. However, deploying HAR systems in real-world settings remains challenging due to the high cost of labeled data, the need for pre-segmented sensor streams, and the lack of flexibility in activity granularity. To address these limitations, we introduce DISCOVER, a method designed to discover fine-grained human sub-activities from unlabeled sensor data without relying on pre-segmentation. DISCOVER combines unsupervised feature extraction and clustering with a user-friendly visualization tool to streamline the labeling process. DISCOVER enables domain experts to efficiently annotate only a minimal set of representative cluster centroids, reducing the annotation workload to a small number of samples (0.05% of our dataset). We demonstrate DISCOVER's effectiveness through a re-annotation exercise on widely used HAR datasets, showing that it uncovers finer-grained activities and produces more nuanced annotations than traditional coarse labels. DISCOVER represents a step toward practical, deployable HAR systems that adapt to diverse real environments.</p>
  </details>
</details>
<details>
  <summary>14. <b>【2503.01725】HarmonySet: A Comprehensive Dataset for Understanding Video-Music Semantic Alignment and Temporal Synchronization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01725">https://arxiv.org/abs/2503.01725</a></p>
  <p><b>作者</b>：Zitang Zhou,Ke Mei,Yu Lu,Tianyi Wang,Fengyun Rao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：comprehensive dataset designed, advance video-music understanding, paper introduces HarmonySet, comprehensive dataset, dataset designed</p>
  <p><b>备注</b>： Accepted at CVPR 2025. Project page: [this https URL](https://harmonyset.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper introduces HarmonySet, a comprehensive dataset designed to advance video-music understanding. HarmonySet consists of 48,328 diverse video-music pairs, annotated with detailed information on rhythmic synchronization, emotional alignment, thematic coherence, and cultural relevance. We propose a multi-step human-machine collaborative framework for efficient annotation, combining human insights with machine-generated descriptions to identify key transitions and assess alignment across multiple dimensions. Additionally, we introduce a novel evaluation framework with tasks and metrics to assess the multi-dimensional alignment of video and music, including rhythm, emotion, theme, and cultural context. Our extensive experiments demonstrate that HarmonySet, along with the proposed evaluation framework, significantly improves the ability of multimodal models to capture and analyze the intricate relationships between video and music.</p>
  </details>
</details>
<details>
  <summary>15. <b>【2503.01715】KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01715">https://arxiv.org/abs/2503.01715</a></p>
  <p><b>作者</b>：Antoni Bigata,Michał Stypułkowski,Rodrigo Mira,Stella Bounareli,Konstantinos Vougioukas,Zoe Landgraf,Nikita Drobyshev,Maciej Zieba,Stavros Petridis,Maja Pantic</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Current audio-driven facial, methods achieve impressive, Current audio-driven, achieve impressive results, achieve impressive</p>
  <p><b>备注</b>： CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Current audio-driven facial animation methods achieve impressive results for short videos but suffer from error accumulation and identity drift when extended to longer durations. Existing methods attempt to mitigate this through external spatial control, increasing long-term consistency but compromising the naturalness of motion. We propose KeyFace, a novel two-stage diffusion-based framework, to address these issues. In the first stage, keyframes are generated at a low frame rate, conditioned on audio input and an identity frame, to capture essential facial expressions and movements over extended periods of time. In the second stage, an interpolation model fills in the gaps between keyframes, ensuring smooth transitions and temporal coherence. To further enhance realism, we incorporate continuous emotion representations and handle a wide range of non-speech vocalizations (NSVs), such as laughter and sighs. We also introduce two new evaluation metrics for assessing lip synchronization and NSV generation. Experimental results show that KeyFace outperforms state-of-the-art methods in generating natural, coherent facial animations over extended durations, successfully encompassing NSVs and continuous emotions.</p>
  </details>
</details>
<details>
  <summary>16. <b>【2503.01691】Open-Set Recognition of Novel Species in Biodiversity Monitoring</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01691">https://arxiv.org/abs/2503.01691</a></p>
  <p><b>作者</b>：Yuyan Chen,Nico Lang,B. Christian Schmidt,Aditya Jain,Yves Basset,Sara Beery,Maxim Larrivée,David Rolnick</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Machine learning, large-scale biodiversity monitoring, facilitate long-term, learning is increasingly, increasingly being applied</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Machine learning is increasingly being applied to facilitate long-term, large-scale biodiversity monitoring. With most species on Earth still undiscovered or poorly documented, species-recognition models are expected to encounter new species during deployment. We introduce Open-Insects, a fine-grained image recognition benchmark dataset for open-set recognition and out-of-distribution detection in biodiversity monitoring. Open-Insects makes it possible to evaluate algorithms for new species detection on several geographical open-set splits with varying difficulty. Furthermore, we present a test set recently collected in the wild with 59 species that are likely new to science. We evaluate a variety of open-set recognition algorithms, including post-hoc methods, training-time regularization, and training with auxiliary data, finding that the simple post-hoc approach of utilizing softmax scores remains a strong baseline. We also demonstrate how to leverage auxiliary data to improve the detection performance when the training dataset is limited. Our results provide timely insights to guide the development of computer vision methods for biodiversity monitoring and species discovery.</p>
  </details>
</details>
<details>
  <summary>17. <b>【2503.01667】oLo: A Two-Stage, Training-Free Layout-To-Image Generation Framework For High-Overlap Layouts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01667">https://arxiv.org/abs/2503.01667</a></p>
  <p><b>作者</b>：Linhao Huang,Jing Yu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：generating high-quality images, Recent training-free, demonstrated remarkable performance, demonstrated remarkable, generating high-quality</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent training-free layout-to-image diffusion models have demonstrated remarkable performance in generating high-quality images with controllable layouts. These models follow a one-stage framework: Encouraging the model to focus the attention map of each concept on its corresponding region by defining attention map-based losses. However, these models still struggle to accurately follow layouts with significant overlap, often leading to issues like attribute leakage and missing entities. In this paper, we propose ToLo, a two-stage, training-free layout-to-image generation framework for high-overlap layouts. Our framework consists of two stages: the aggregation stage and the separation stage, each with its own loss function based on the attention map. To provide a more effective evaluation, we partition the HRS dataset based on the Intersection over Union (IoU) of the input layouts, creating a new dataset for layout-to-image generation with varying levels of overlap. Through extensive experiments on this dataset, we demonstrate that ToLo significantly enhances the performance of existing methods when dealing with high-overlap layouts. Our code and dataset are available here: this https URL.</p>
  </details>
</details>
<details>
  <summary>18. <b>【2503.01661】MUSt3R: Multi-view Network for Stereo 3D Reconstruction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01661">https://arxiv.org/abs/2503.01661</a></p>
  <p><b>作者</b>：Yohann Cabon,Lucas Stoffl,Leonid Antsfeld,Gabriela Csurka,Boris Chidlovskii,Jerome Revaud,Vincent Leroy</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：geometric computer vision, arbitrary image collections, paradigm in geometric, geometric computer, computer vision</p>
  <p><b>备注</b>： Accepted at CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:DUSt3R introduced a novel paradigm in geometric computer vision by proposing a model that can provide dense and unconstrained Stereo 3D Reconstruction of arbitrary image collections with no prior information about camera calibration nor viewpoint poses. Under the hood, however, DUSt3R processes image pairs, regressing local 3D reconstructions that need to be aligned in a global coordinate system. The number of pairs, growing quadratically, is an inherent limitation that becomes especially concerning for robust and fast optimization in the case of large image collections. In this paper, we propose an extension of DUSt3R from pairs to multiple views, that addresses all aforementioned concerns. Indeed, we propose a Multi-view Network for Stereo 3D Reconstruction, or MUSt3R, that modifies the DUSt3R architecture by making it symmetric and extending it to directly predict 3D structure for all views in a common coordinate frame. Second, we entail the model with a multi-layer memory mechanism which allows to reduce the computational complexity and to scale the reconstruction to large collections, inferring thousands of 3D pointmaps at high frame-rates with limited added complexity. The framework is designed to perform 3D reconstruction both offline and online, and hence can be seamlessly applied to SfM and visual SLAM scenarios showing state-of-the-art performance on various 3D downstream tasks, including uncalibrated Visual Odometry, relative camera pose, scale and focal estimation, 3D reconstruction and multi-view depth estimation.</p>
  </details>
</details>
<details>
  <summary>19. <b>【2503.01655】Enhancing Object Detection Accuracy in Underwater Sonar Images through Deep Learning-based Denoising</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01655">https://arxiv.org/abs/2503.01655</a></p>
  <p><b>作者</b>：Ziyu Wang(1),Tao Xue(1),Yanbin Wang(1),Jingyuan Li(1),Haibin Zhang(1),Zhiqiang Xu(2),Gaofei Xu(3) ((1) Xidian University, (2) Jiangxi University of Science and Technology, (3) Institute of Deep-sea Science and Engineering)</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：images, object detection, Sonar, detection, sonar images</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Sonar image object detection is crucial for underwater robotics and other applications. However, various types of noise in sonar images can affect the accuracy of object detection. Denoising, as a critical preprocessing step, aims to remove noise while retaining useful information to improve detection accuracy. Although deep learning-based denoising algorithms perform well on optical images, their application to underwater sonar images remains underexplored. This paper systematically evaluates the effectiveness of several deep learning-based denoising algorithms, originally designed for optical images, in the context of underwater sonar image object detection. We apply nine trained denoising models to images from five open-source sonar datasets, each processing different types of noise. We then test the denoised images using four object detection algorithms. The results show that different denoising models have varying effects on detection performance. By combining the strengths of multiple denoising models, the detection results can be optimized, thus more effectively suppressing noise. Additionally, we adopt a multi-frame denoising technique, using different outputs generated by multiple denoising models as multiple frames of the same scene for further processing to enhance detection accuracy. This method, originally designed for optical images, leverages complementary noise-reduction effects. Experimental results show that denoised sonar images improve the performance of object detection algorithms compared to the original sonar images.</p>
  </details>
</details>
<details>
  <summary>20. <b>【2503.01654】A Shared Encoder Approach to Multimodal Representation Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01654">https://arxiv.org/abs/2503.01654</a></p>
  <p><b>作者</b>：Shuvendu Roy,Franklin Ogidi,Ali Etemad,Elham Dolatabadi,Arash Afkanpour</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：demonstrated remarkable potential, Multimodal representation learning, integrate diverse data, text and images, diverse data modalities</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal representation learning has demonstrated remarkable potential in enabling models to process and integrate diverse data modalities, such as text and images, for improved understanding and performance. While the medical domain can benefit significantly from this paradigm, the scarcity of paired multimodal data and reliance on proprietary or pretrained encoders pose significant challenges. In this work, we present a shared encoder framework for multimodal representation learning tailored to the medical domain. Our approach employs a single set of encoder parameters shared across modalities, augmented with learnable modality features. Empirical results demonstrate that our shared encoder idea achieves superior performance compared to separate modality-specific encoders, demonstrating improved generalization in data-constrained settings. Notably, the performance gains are more pronounced with fewer training examples, underscoring the efficiency of our shared encoder framework for real-world medical applications with limited data. Our code and experiment setup are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>21. <b>【2503.01653】Distilled Prompt Learning for Incomplete Multimodal Survival Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01653">https://arxiv.org/abs/2503.01653</a></p>
  <p><b>作者</b>：Yingxue Xu,Fengtao Zhou,Chenyu Zhao,Yihui Wang,Can Yang,Hao Chen</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：data including pathology, including pathology images, precise survival prediction, multimodal data including, multimodal survival models</p>
  <p><b>备注</b>： Accepted by CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The integration of multimodal data including pathology images and gene profiles is widely applied in precise survival prediction. Despite recent advances in multimodal survival models, collecting complete modalities for multimodal fusion still poses a significant challenge, hindering their application in clinical settings. Current approaches tackling incomplete modalities often fall short, as they typically compensate for only a limited part of the knowledge of missing modalities. To address this issue, we propose a Distilled Prompt Learning framework (DisPro) to utilize the strong robustness of Large Language Models (LLMs) to missing modalities, which employs two-stage prompting for compensation of comprehensive information for missing modalities. In the first stage, Unimodal Prompting (UniPro) distills the knowledge distribution of each modality, preparing for supplementing modality-specific knowledge of the missing modality in the subsequent stage. In the second stage, Multimodal Prompting (MultiPro) leverages available modalities as prompts for LLMs to infer the missing modality, which provides modality-common information. Simultaneously, the unimodal knowledge acquired in the first stage is injected into multimodal inference to compensate for the modality-specific knowledge of the missing modality. Extensive experiments covering various missing scenarios demonstrated the superiority of the proposed method. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>22. <b>【2503.01646】OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01646">https://arxiv.org/abs/2503.01646</a></p>
  <p><b>作者</b>：Dianyi Yang,Yu Gao,Xihan Wang,Yufeng Yue,Yi Yang,Mengyin Fu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Recent advancements, dense semantic SLAM, object-level scene understanding, Gaussian Voting Splatting, significantly improved</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in 3D Gaussian Splatting have significantly improved the efficiency and quality of dense semantic SLAM. However, previous methods are generally constrained by limited-category pre-trained classifiers and implicit semantic representation, which hinder their performance in open-set scenarios and restrict 3D object-level scene understanding. To address these issues, we propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian representation to perform dense semantic SLAM in open-set environments. Our system integrates explicit semantic labels derived from 2D foundational models into the 3D Gaussian framework, facilitating robust 3D object-level scene understanding. We introduce Gaussian Voting Splatting to enable fast 2D label map rendering and scene updating. Additionally, we propose a Confidence-based 2D Label Consensus method to ensure consistent labeling across multiple views. Furthermore, we employ a Segmentation Counter Pruning strategy to improve the accuracy of semantic scene representation. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method in scene understanding, tracking, and mapping, achieving 10 times faster semantic rendering and 2 times lower storage costs compared to existing methods. Project page: this https URL.</p>
  </details>
</details>
<details>
  <summary>23. <b>【2503.01645】DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01645">https://arxiv.org/abs/2503.01645</a></p>
  <p><b>作者</b>：Zhendong Wang,Jianmin Bao,Shuyang Gu,Dong Chen,Wengang Zhou,Houqiang Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：simple yet effective, synthesizing design images, textual descriptions, visual, task of synthesizing</p>
  <p><b>备注</b>： Accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation.</p>
  </details>
</details>
<details>
  <summary>24. <b>【2503.01633】SparseMamba-PCL: Scribble-Supervised Medical Image Segmentation via SAM-Guided Progressive Collaborative Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01633">https://arxiv.org/abs/2503.01633</a></p>
  <p><b>作者</b>：Luyi Qiu,Tristan Till,Xiaobao Guo,Adams Wai-Kin Kong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：complex anatomical structures, annotations significantly reduce, large medical datasets, Scribble annotations significantly, Sparse Mamba network</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Scribble annotations significantly reduce the cost and labor required for dense labeling in large medical datasets with complex anatomical structures. However, current scribble-supervised learning methods are limited in their ability to effectively propagate sparse annotation labels to dense segmentation masks and accurately segment object boundaries. To address these issues, we propose a Progressive Collaborative Learning framework that leverages novel algorithms and the Med-SAM foundation model to enhance information quality during training. (1) We enrich ground truth scribble segmentation labels through a new algorithm, propagating scribbles to estimate object boundaries. (2) We enhance feature representation by optimizing Med-SAM-guided training through the fusion of feature embeddings from Med-SAM and our proposed Sparse Mamba network. This enriched representation also facilitates the fine-tuning of the Med-SAM decoder with enriched scribbles. (3) For inference, we introduce a Sparse Mamba network, which is highly capable of capturing local and global dependencies by replacing the traditional sequential patch processing method with a skip-sampling procedure. Experiments on the ACDC, CHAOS, and MSCMRSeg datasets validate the effectiveness of our framework, outperforming nine state-of-the-art methods. Our code is available at \href{this https URL}{this http URL}.</p>
  </details>
</details>
<details>
  <summary>25. <b>【2503.01628】A General Purpose Spectral Foundational Model for Both Proximal and Remote Sensing Spectral Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01628">https://arxiv.org/abs/2503.01628</a></p>
  <p><b>作者</b>：William Michael Laprade,Jesper Cairo Westergaard,Svend Christensen,Mads Nielsen,Anders Bjorholm Dahl</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：wavelength and bandwidth, records the reflectance, specific wavelength, Spectral imaging, Spectral</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Spectral imaging data acquired via multispectral and hyperspectral cameras can have hundreds of channels, where each channel records the reflectance at a specific wavelength and bandwidth. Time and resource constraints limit our ability to collect large spectral datasets, making it difficult to build and train predictive models from scratch. In the RGB domain, we can often alleviate some of the limitations of smaller datasets by using pretrained foundational models as a starting point. However, most existing foundation models are pretrained on large datasets of 3-channel RGB images, severely limiting their effectiveness when used with spectral imaging data. The few spectral foundation models that do exist usually have one of two limitations: (1) they are built and trained only on remote sensing data limiting their application in proximal spectral imaging, (2) they utilize the more widely available multispectral imaging datasets with less than 15 channels restricting their use with hundred-channel hyperspectral images. To alleviate these issues, we propose a large-scale foundational model and dataset built upon the masked autoencoder architecture that takes advantage of spectral channel encoding, spatial-spectral masking and ImageNet pretraining for an adaptable and robust model for downstream spectral imaging tasks.</p>
  </details>
</details>
<details>
  <summary>26. <b>【2503.01619】Advancing vision-language models in front-end development via data synthesis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01619">https://arxiv.org/abs/2503.01619</a></p>
  <p><b>作者</b>：Tong Ge,Yashu Liu,Jieping Ye,Tianyi Li,Chao Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Modern front-end, presents distinctive challenges, presents distinctive, leveraging the unique, unique features</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Modern front-end (FE) development, especially when leveraging the unique features of frameworks like React and Vue, presents distinctive challenges. These include managing modular architectures, ensuring synchronization between data and visual outputs for declarative rendering, and adapting reusable components to various scenarios. Such complexities make it particularly difficult for state-of-the-art large vision-language models (VLMs) to generate accurate and functional code directly from design images. To address these challenges, we propose a reflective agentic workflow that synthesizes high-quality image-text data to capture the diverse characteristics of FE development. This workflow automates the extraction of self-contained\footnote{A \textbf{self-contained} code snippet is one that encapsulates all necessary logic, styling, and dependencies, ensuring it functions independently without requiring external imports or context.} code snippets from real-world projects, renders the corresponding visual outputs, and generates detailed descriptions that link design elements to functional code. To further expand the scope and utility of the synthesis, we introduce three data synthesis strategies: Evolution-based synthesis, which enables scalable and diverse dataset expansion; Waterfall-Model-based synthesis, which generates logically coherent code derived from system requirements; and Additive Development synthesis, which iteratively increases the complexity of human-authored components. We build a large vision-language model, Flame, trained on the synthesized datasets and demonstrate its effectiveness in generating React code via the $\text{pass}@k$ metric. Our results suggest that a code VLM trained to interpret images before code generation may achieve better performance.</p>
  </details>
</details>
<details>
  <summary>27. <b>【2503.01612】Robust Palm-Vein Recognition Using the MMD Filter: Improving SIFT-Based Feature Matching</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01612">https://arxiv.org/abs/2503.01612</a></p>
  <p><b>作者</b>：Kaveen Perera,Fouad Khelifi,Ammar Belatreche</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：palm vein images, palm vein, proposed MMD filter, fingers and thumb, hand posture</p>
  <p><b>备注</b>： Our previous work, presented at the 2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA) and published in IEEE Xplore. The code for the MMD filter is available at [this https URL](https://github.com/kaveenperera/MMD_filter) under Mozilla Public License Version 2.0</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:A major challenge with palm vein images is that slight movements of the fingers and thumb, or variations in hand posture, can stretch the skin in different areas and alter the vein patterns. This can result in an infinite number of variations in palm vein images for a given individual. This paper introduces a novel filtering technique for SIFT-based feature matching, known as the Mean and Median Distance (MMD) Filter. This method evaluates the differences in keypoint coordinates and computes the mean and median in each direction to eliminate incorrect matches. Experiments conducted on the 850nm subset of the CASIA dataset indicate that the proposed MMD filter effectively preserves correct points while reducing false positives detected by other filtering methods. A comparison with existing SIFT-based palm vein recognition systems demonstrates that the proposed MMD filter delivers outstanding performance, achieving lower Equal Error Rate (EER) values. This article presents an extended author's version based on our previous work, A Keypoint Filtering Method for SIFT based Palm-Vein Recognition.</p>
  </details>
</details>
<details>
  <summary>28. <b>【2503.01610】Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01610">https://arxiv.org/abs/2503.01610</a></p>
  <p><b>作者</b>：Chen Guo,Junxuan Li,Yash Kant,Yaser Sheikh,Shunsuke Saito,Chen Cao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：method to create, view points, human, create photorealistic, monocular</p>
  <p><b>备注</b>： Project page: [this https URL](https://moygcc.github.io/vid2avatar-pro/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present Vid2Avatar-Pro, a method to create photorealistic and animatable 3D human avatars from monocular in-the-wild videos. Building a high-quality avatar that supports animation with diverse poses from a monocular video is challenging because the observation of pose diversity and view points is inherently limited. The lack of pose variations typically leads to poor generalization to novel poses, and avatars can easily overfit to limited input view points, producing artifacts and distortions from other views. In this work, we address these limitations by leveraging a universal prior model (UPM) learned from a large corpus of multi-view clothed human performance capture data. We build our representation on top of expressive 3D Gaussians with canonical front and back maps shared across identities. Once the UPM is learned to accurately reproduce the large-scale multi-view human images, we fine-tune the model with an in-the-wild video via inverse rendering to obtain a personalized photorealistic human avatar that can be faithfully animated to novel human motions and rendered from novel views. The experiments show that our approach based on the learned universal prior sets a new state-of-the-art in monocular avatar reconstruction by substantially outperforming existing approaches relying only on heuristic regularization or a shape prior of minimally clothed bodies (e.g., SMPL) on publicly available datasets.</p>
  </details>
</details>
<details>
  <summary>29. <b>【2503.01605】A Leaf-Level Dataset for Soybean-Cotton Detection and Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01605">https://arxiv.org/abs/2503.01605</a></p>
  <p><b>作者</b>：Thiago H. Segreto,Juliano Negri,Paulo H. Polegato,João Manoel Herrera Pinheiro,Ricardo Godoy,Marcelo Becker</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：offering substantial economic, substantial economic returns, facing persistent challenges, countries' agricultural sectors, hamper sustainable management</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Soybean and cotton are major drivers of many countries' agricultural sectors, offering substantial economic returns but also facing persistent challenges from volunteer plants and weeds that hamper sustainable management. Effectively controlling volunteer plants and weeds demands advanced recognition strategies that can identify these amidst complex crop canopies. While deep learning methods have demonstrated promising results for leaf-level detection and segmentation, existing datasets often fail to capture the complexity of real-world agricultural fields. To address this, we collected 640 high-resolution images from a commercial farm spanning multiple growth stages, weed pressures, and lighting variations. Each image is annotated at the leaf-instance level, with 7,221 soybean and 5,190 cotton leaves labeled via bounding boxes and segmentation masks, capturing overlapping foliage, small leaf size, and morphological similarities. We validate this dataset using YOLOv11, demonstrating state-of-the-art performance in accurately identifying and segmenting overlapping foliage. Our publicly available dataset supports advanced applications such as selective herbicide spraying and pest monitoring and can foster more robust, data-driven strategies for soybean-cotton management.</p>
  </details>
</details>
<details>
  <summary>30. <b>【2503.01603】riple-Stream Deep Feature Selection with Metaheuristic Optimization and Machine Learning for Multi-Stage Hypertensive Retinopathy Diagnosis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01603">https://arxiv.org/abs/2503.01603</a></p>
  <p><b>作者</b>：Suleyman Burcin Suyun,Mustafa Yurdakul,Sakir Tasdemir,Serkan Bilic</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Cohen Kappa, severe eye disease, permanent vision loss, Hypertensive retinopathy, Cohen</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Hypertensive retinopathy (HR) is a severe eye disease that may cause permanent vision loss if not diagnosed early. Traditional diagnostic methods are time-consuming and subjective, highlighting the need for an automated, reliable system. Existing studies often use a single Deep Learning (DL) model, struggling to distinguish HR stages. This study introduces a three-stage approach to enhance HR diagnosis accuracy. Initially, 14 CNN models were tested, identifying DenseNet169, MobileNet, and ResNet152 as the most effective. DenseNet169 achieved 87.73% accuracy, 87.75% precision, 87.73% recall, 87.67% F1-score, and 0.8359 Cohen's Kappa. MobileNet followed with 86.40% accuracy, 86.60% precision, 86.40% recall, 86.31% F1-score, and 0.8180 Cohen's Kappa. ResNet152 ranked third with 85.87% accuracy, 86.01% precision, 85.87% recall, 85.83% F1-score, and 0.8188 Cohen's Kappa. In the second stage, deep features from these models were fused and classified using Machine Learning (ML) algorithms (SVM, RF, XGBoost). SVM (sigmoid kernel) performed best with 92.00% accuracy, 91.93% precision, 92.00% recall, 91.91% F1-score, and 0.8930 Cohen's Kappa. The third stage applied meta-heuristic optimization (GA, ABC, PSO, HHO) for feature selection. HHO yielded 94.66% accuracy, precision, and recall, 94.64% F1-score, and 0.9286 Cohen's Kappa. The proposed approach surpassed single CNN models and previous studies in HR diagnosis accuracy and generalization.</p>
  </details>
</details>
<details>
  <summary>31. <b>【2503.01601】Evaluating Stenosis Detection with Grounding DINO, YOLO, and DINO-DETR</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01601">https://arxiv.org/abs/2503.01601</a></p>
  <p><b>作者</b>：Muhammad Musab Ansari</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：managing cardiovascular diseases, Detecting stenosis, angiography is vital, vital for diagnosing, diagnosing and managing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Detecting stenosis in coronary angiography is vital for diagnosing and managing cardiovascular diseases. This study evaluates the performance of state-of-the-art object detection models on the ARCADE dataset using the MMDetection framework. The models are assessed using COCO evaluation metrics, including Intersection over Union (IoU), Average Precision (AP), and Average Recall (AR). Results indicate variations in detection accuracy across different models, attributed to differences in algorithmic design, transformer-based vs. convolutional architectures. Additionally, several challenges were encountered during implementation, such as compatibility issues between PyTorch, CUDA, and MMDetection, as well as dataset inconsistencies in ARCADE. The findings provide insights into model selection for stenosis detection and highlight areas for further improvement in deep learning-based coronary artery disease diagnosis.</p>
  </details>
</details>
<details>
  <summary>32. <b>【2503.01595】STAR: Stability-Inducing Weight Perturbation for Continual Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01595">https://arxiv.org/abs/2503.01595</a></p>
  <p><b>作者</b>：Masih Eskandar,Tooba Imtiaz,Davin Hill,Zifeng Wang,Jennifer Dy</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Humans can naturally, Continual learning, sequential manner, learning, Humans</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Humans can naturally learn new and varying tasks in a sequential manner. Continual learning is a class of learning algorithms that updates its learned model as it sees new data (on potentially new tasks) in a sequence. A key challenge in continual learning is that as the model is updated to learn new tasks, it becomes susceptible to catastrophic forgetting, where knowledge of previously learned tasks is lost. A popular approach to mitigate forgetting during continual learning is to maintain a small buffer of previously-seen samples and to replay them during training. However, this approach is limited by the small buffer size, and while forgetting is reduced, it is still present. In this paper, we propose a novel loss function, STAR, that exploits the worst-case parameter perturbation that reduces the KL-divergence of model predictions with that of its local parameter neighborhood to promote stability and alleviate forgetting. STAR can be combined with almost any existing rehearsal-based method as a plug-and-play component. We empirically show that STAR consistently improves the performance of existing methods by up to 15% across varying baselines and achieves superior or competitive accuracy to that of state-of-the-art methods aimed at improving rehearsal-based continual learning.</p>
  </details>
</details>
<details>
  <summary>33. <b>【2503.01582】Category-level Meta-learned NeRF Priors for Efficient Object Mapping</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01582">https://arxiv.org/abs/2503.01582</a></p>
  <p><b>作者</b>：Saad Ejaz,Hriday Bavle,Laura Ribeiro,Holger Voos,Jose Luis Sanchez-Lopez</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：priors enable efficient, category-level priors enable, enable efficient object, category-level priors, Prior-based Efficient Neural</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop). Recently, DeepSDF has predominantly been used as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency while enabling canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results on a low-end GPU highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21% lower Chamfer distance, demonstrating better reconstruction quality. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13% improvement averaged across all reconstruction metrics, a boost in rotation estimation accuracy, and comparable translation and size estimation performance, while being trained for 5x less time.</p>
  </details>
</details>
<details>
  <summary>34. <b>【2503.01576】MRI super-resolution reconstruction using efficient diffusion probabilistic model with residual shifting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01576">https://arxiv.org/abs/2503.01576</a></p>
  <p><b>作者</b>：Mojtaba Safari,Shansong Wang,Zach Eidex,Qiang Li,Erik H. Middlebrooks,David S. Yu,Xiaofeng Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)</p>
  <p><b>关键词</b>：residual error-shifting mechanism, preserving critical anatomical, accelerating MRI reconstruction, study introduces, error-shifting mechanism</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Objective:This study introduces a residual error-shifting mechanism that drastically reduces sampling steps while preserving critical anatomical details, thus accelerating MRI reconstruction. Approach:We propose a novel diffusion-based SR framework called Res-SRDiff, which integrates residual error shifting into the forward diffusion process. This enables efficient HR image reconstruction by aligning the degraded HR and LR this http URL evaluated Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional denoising diffusion probabilistic model with vision transformer backbone (TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), gradient magnitude similarity deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main results: Res-SRDiff significantly outperformed all comparative methods in terms of PSNR, SSIM, and GMSD across both datasets, with statistically significant improvements (p-values0.05). The model achieved high-fidelity image restoration with only four sampling steps, drastically reducing computational time to under one second per slice, which is substantially faster than conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses further demonstrated that Res-SRDiff effectively preserved fine anatomical details and lesion morphology in both brain and pelvic MRI images. Significance: Our findings show that Res-SRDiff is an efficient and accurate MRI SR method, markedly improving computational efficiency and image quality. Integrating residual error shifting into the diffusion process allows for rapid and robust HR image reconstruction, enhancing clinical MRI workflows and advancing medical imaging research. The source at:this https URL</p>
  </details>
</details>
<details>
  <summary>35. <b>【2503.01569】Meta Learning-Driven Iterative Refinement for Robust Anomaly Detection in Industrial Inspection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01569">https://arxiv.org/abs/2503.01569</a></p>
  <p><b>作者</b>：Muhammad Aqeel,Shakiba Sharifi,Marco Cristani,Francesco Setti</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：robust anomaly detection, anomaly detection models, handle noisy data, industrial inspection, Agnostic Meta Learning</p>
  <p><b>备注</b>： Accepted in the VISION workshop at ECCV 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study investigates the performance of robust anomaly detection models in industrial inspection, focusing particularly on their ability to handle noisy data. We propose to leverage the adaptation ability of meta learning approaches to identify and reject noisy training data to improve the learning process. In our model, we employ Model Agnostic Meta Learning (MAML) and an iterative refinement process through an Inter-Quartile Range rejection scheme to enhance their adaptability and robustness. This approach significantly improves the models capability to distinguish between normal and defective conditions. Our results of experiments conducted on well known MVTec and KSDD2 datasets demonstrate that the proposed method not only excels in environments with substantial noise but can also contribute in case of a clear training set, isolating those samples that are relatively out of distribution, thus offering significant improvements over traditional models.</p>
  </details>
</details>
<details>
  <summary>36. <b>【2503.01565】AutoLUT: LUT-Based Image Super-Resolution with Automatic Sampling and Adaptive Residual Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01565">https://arxiv.org/abs/2503.01565</a></p>
  <p><b>作者</b>：Yuheng Xu,Shijie Yang,Xin Liu,Jie Liu,Jie Tang,Gangshan Wu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：recent years, increasing popularity, popularity of Hi-DPI, Hi-DPI screens, screens has driven</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In recent years, the increasing popularity of Hi-DPI screens has driven a rising demand for high-resolution images. However, the limited computational power of edge devices poses a challenge in deploying complex super-resolution neural networks, highlighting the need for efficient methods. While prior works have made significant progress, they have not fully exploited pixel-level information. Moreover, their reliance on fixed sampling patterns limits both accuracy and the ability to capture fine details in low-resolution images. To address these challenges, we introduce two plug-and-play modules designed to capture and leverage pixel information effectively in Look-Up Table (LUT) based super-resolution networks. Our method introduces Automatic Sampling (AutoSample), a flexible LUT sampling approach where sampling weights are automatically learned during training to adapt to pixel variations and expand the receptive field without added inference cost. We also incorporate Adaptive Residual Learning (AdaRL) to enhance inter-layer connections, enabling detailed information flow and improving the network's ability to reconstruct fine details. Our method achieves significant performance improvements on both MuLUT and SPF-LUT while maintaining similar storage sizes. Specifically, for MuLUT, we achieve a PSNR improvement of approximately +0.20 dB improvement on average across five datasets. For SPF-LUT, with more than a 50% reduction in storage space and about a 2/3 reduction in inference time, our method still maintains performance comparable to the original. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>37. <b>【2503.01547】AI-Driven Relocation Tracking in Dynamic Kitchen Environments</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01547">https://arxiv.org/abs/2503.01547</a></p>
  <p><b>作者</b>：Arash Nasr Esfahani,Hamed Hosseini,Mehdi Tale Masouleh,Ahmad Kalhor,Hedieh Sajedi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：understand dynamic environments, daily life, smart homes, prevalent in daily, ability to understand</p>
  <p><b>备注</b>： Conference: 2024 14th International Conference on Computer and Knowledge Engineering (ICCKE) Publisher: IEEE</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:As smart homes become more prevalent in daily life, the ability to understand dynamic environments is essential which is increasingly dependent on AI systems. This study focuses on developing an intelligent algorithm which can navigate a robot through a kitchen, recognizing objects, and tracking their relocation. The kitchen was chosen as the testing ground due to its dynamic nature as objects are frequently moved, rearranged and replaced. Various techniques, such as SLAM feature-based tracking and deep learning-based object detection (e.g., Faster R-CNN), are commonly used for object tracking. Additionally, methods such as optical flow analysis and 3D reconstruction have also been used to track the relocation of objects. These approaches often face challenges when it comes to problems such as lighting variations and partial occlusions, where parts of the object are hidden in some frames but visible in others. The proposed method in this study leverages the YOLOv5 architecture, initialized with pre-trained weights and subsequently fine-tuned on a custom dataset. A novel method was developed, introducing a frame-scoring algorithm which calculates a score for each object based on its location and features within all frames. This scoring approach helps to identify changes by determining the best-associated frame for each object and comparing the results in each scene, overcoming limitations seen in other methods while maintaining simplicity in design. The experimental results demonstrate an accuracy of 97.72%, a precision of 95.83% and a recall of 96.84% for this algorithm, which highlights the efficacy of the model in detecting spatial changes.</p>
  </details>
</details>
<details>
  <summary>38. <b>【2503.01531】Diversity Covariance-Aware Prompt Learning for Vision-Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01531">https://arxiv.org/abs/2503.01531</a></p>
  <p><b>作者</b>：Songlin Dong,Zhengdong Zhou,Chenhao Ding,Xinyuan Gao,Alex Kot,Yihong Gong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：performance of visual-language, adapt to specific, specific applications, few-shot learning, enhance the performance</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Prompt tuning can further enhance the performance of visual-language models across various downstream tasks (e.g., few-shot learning), enabling them to better adapt to specific applications and needs. In this paper, we present a Diversity Covariance-Aware framework that learns distributional information from the data to enhance the few-shot ability of the prompt model. First, we propose a covariance-aware method that models the covariance relationships between visual features and uses anisotropic Mahalanobis distance, instead of the suboptimal cosine distance, to measure the similarity between two modalities. We rigorously derive and prove the validity of this modeling process. Then, we propose the diversity-aware method, which learns multiple diverse soft prompts to capture different attributes of categories and aligns them independently with visual modalities. This method achieves multi-centered covariance modeling, leading to more diverse decision boundaries. Extensive experiments on 11 datasets in various tasks demonstrate the effectiveness of our method.</p>
  </details>
</details>
<details>
  <summary>39. <b>【2503.01497】An Approach for Air Drawing Using Background Subtraction and Contour Extraction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01497">https://arxiv.org/abs/2503.01497</a></p>
  <p><b>作者</b>：Ramkrishna Acharya</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：image processing techniques, processing techniques, techniques to draw, screen by moving, moving fingers</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we propose a novel approach for air drawing that uses image processing techniques to draw on the screen by moving fingers in the air. This approach benefits a wide range of applications such as sign language, in-air drawing, and 'writing' in the air as a new way of input. The approach starts with preparing ROI (Region of Interest) background images by taking a running average in initial camera frames and later subtracting it from the live camera frames to get a binary mask image. We calculate the pointer's position as the top of the contour on the binary image. When drawing a circle on the canvas in that position, it simulates the drawing. Furthermore, we combine the pre-trained Tesseract model for OCR purposes. To address the false contours, we perform hand detection based on the haar cascade before performing the background subtraction. In an experimental setup, we achieved a latency of only 100ms in air drawing. The code used to this research are available in GitHub as this https URL</p>
  </details>
</details>
<details>
  <summary>40. <b>【2503.01463】MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01463">https://arxiv.org/abs/2503.01463</a></p>
  <p><b>作者</b>：Zhixiong Nan,Xianghong Li,Jifeng Dai,Tao Xiang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：cascaded decoder architecture, architecture commonly adopted, decoder architecture commonly, decoder architecture, cascaded decoder</p>
  <p><b>备注</b>： 14 pages,9 figures,accepted to CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Based on analyzing the character of cascaded decoder architecture commonly adopted in existing DETR-like models, this paper proposes a new decoder architecture. The cascaded decoder architecture constrains object queries to update in the cascaded direction, only enabling object queries to learn relatively-limited information from image features. However, the challenges for object detection in natural scenes (e.g., extremely-small, heavily-occluded, and confusingly mixed with the background) require an object detection model to fully utilize image features, which motivates us to propose a new decoder architecture with the parallel Multi-time Inquiries (MI) mechanism. MI enables object queries to learn more comprehensive information, and our MI based model, MI-DETR, outperforms all existing DETR-like models on COCO benchmark under different backbones and training epochs, achieving +2.3 AP and +0.6 AP improvements compared to the most representative model DINO and SOTA model Relation-DETR under ResNet-50 backbone. In addition, a series of diagnostic and visualization experiments demonstrate the effectiveness, rationality, and interpretability of MI.</p>
  </details>
</details>
<details>
  <summary>41. <b>【2503.01453】AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01453">https://arxiv.org/abs/2503.01453</a></p>
  <p><b>作者</b>：Pankaj Choudhury,Yogesh Aggarwal,Prithwijit Guha,Sukumar Nandi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：real-world adoption remains, adoption remains constrained, Neural networks, networks have significantly, significantly advanced</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Neural networks have significantly advanced AI applications, yet their real-world adoption remains constrained by high computational demands, hardware limitations, and accessibility challenges. In image captioning, many state-of-the-art models have achieved impressive performances while relying on resource-intensive architectures. This made them impractical for deployment on resource-constrained devices. This limitation is particularly noticeable for applications involving low-resource languages. We demonstrate the case of image captioning in Assamese language, where lack of effective, scalable systems can restrict the accessibility of AI-based solutions for native Assamese speakers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy visual feature extractors like FasterRCNN with lightweight ShuffleNetv2x1.5. Additionally, Gated Recurrent Units (GRUs) are used as the caption decoder to further reduce computational demands and model parameters. Furthermore, the integration of bilinear attention enhances the model's overall performance. AC-Lite can operate on edge devices, thereby eliminating the need for computation on remote servers. The proposed AC-Lite model achieves 82.3 CIDEr score on the COCO-AC dataset with 1.098 GFLOPs and 25.65M parameters.</p>
  </details>
</details>
<details>
  <summary>42. <b>【2503.01448】Generative Human Geometry Distribution</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01448">https://arxiv.org/abs/2503.01448</a></p>
  <p><b>作者</b>：Xiangjun Tang,Biao Zhang,Peter Wonka</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：fine clothing details, Realistic human geometry, Realistic human, clothing-pose interactions, geometry</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Realistic human geometry generation is an important yet challenging task, requiring both the preservation of fine clothing details and the accurate modeling of clothing-pose interactions. Geometry distributions, which can model the geometry of a single human as a distribution, provide a promising representation for high-fidelity synthesis. However, applying geometry distributions for human generation requires learning a dataset-level distribution over numerous individual geometry distributions. To address the resulting challenges, we propose a novel 3D human generative framework that, for the first time, models the distribution of human geometry distributions. Our framework operates in two stages: first, generating the human geometry distribution, and second, synthesizing high-fidelity humans by sampling from this distribution. We validate our method on two tasks: pose-conditioned 3D human generation and single-view-based novel pose generation. Experimental results demonstrate that our approach achieves the best quantitative results in terms of realism and geometric fidelity, outperforming state-of-the-art generative methods.</p>
  </details>
</details>
<details>
  <summary>43. <b>【2503.01436】Fall Detection from Indoor Videos using MediaPipe and Handcrafted Feature</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01436">https://arxiv.org/abs/2503.01436</a></p>
  <p><b>作者</b>：Fatima Ahmed,Parag Biswas,Abdur Rashid,Md. Khaliluzzaman</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：injuries and hospitalization, fatal injuries, fall detection, fall, detection</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Falls are a common cause of fatal injuries and hospitalization. However, having fall detection on person, in particular for senior citizens can prove to be critical. Presently,there are handheld, ambient detector and vision-based detection techniques being utilized for fall detection. However, the approaches have issues with accuracy and cost. In this regard, in this research, an approach is proposed to detect falls in indoor environments utilizing the handcrafted features extracted from human body skeleton. The human body skeleton is formed using MediaPipe framework. Results on UR Fall detection show the superiority of our model, capable of detecting falls correctly in a wide number of settings involving people belonging to different ages and genders. This proposed model using MediaPipe for fall classification in daily activities achieves significant accuracy compare to the present existing approaches.</p>
  </details>
</details>
<details>
  <summary>44. <b>【2503.01428】DLF: Extreme Image Compression with Dual-generative Latent Fusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01428">https://arxiv.org/abs/2503.01428</a></p>
  <p><b>作者</b>：Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Yuan Zhang,Yan Lu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：extreme image compression, achieved remarkable performance, studies in extreme, extreme image, image compression</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent studies in extreme image compression have achieved remarkable performance by compressing the tokens from generative tokenizers. However, these methods often prioritize clustering common semantics within the dataset, while overlooking the diverse details of individual objects. Consequently, this results in suboptimal reconstruction fidelity, especially at low bitrates. To address this issue, we introduce a Dual-generative Latent Fusion (DLF) paradigm. DLF decomposes the latent into semantic and detail elements, compressing them through two distinct branches. The semantic branch clusters high-level information into compact tokens, while the detail branch encodes perceptually critical details to enhance the overall fidelity. Additionally, we propose a cross-branch interactive design to reduce redundancy between the two branches, thereby minimizing the overall bit cost. Experimental results demonstrate the impressive reconstruction quality of DLF even below 0.01 bits per pixel (bpp). On the CLIC2020 test set, our method achieves bitrate savings of up to 27.93% on LPIPS and 53.55% on DISTS compared to MS-ILLM. Furthermore, DLF surpasses recent diffusion-based codecs in visual fidelity while maintaining a comparable level of generative realism. Code will be available later.</p>
  </details>
</details>
<details>
  <summary>45. <b>【2503.01425】MeshPad: Interactive Sketch Conditioned Artistic-designed Mesh Generation and Editing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01425">https://arxiv.org/abs/2503.01425</a></p>
  <p><b>作者</b>：Haoxuan Li,Ziya Erkoc,Lei Li,Daniele Sirigatti,Vladyslav Rozov,Angela Dai,Matthias Nießner</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：mesh, generative approach, sketch inputs, approach, Abstract</p>
  <p><b>备注</b>： Project page: [this https URL](https://derkleineli.github.io/meshpad/) Video: [this https URL](https://youtu.be/ql37mWf4pg8) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We introduce MeshPad, a generative approach that creates 3D meshes from sketch inputs. Building on recent advances in artistic-designed triangle mesh generation, our approach addresses the need for interactive artistic mesh creation. To this end, we focus on enabling consistent edits by decomposing editing into 'deletion' of regions of a mesh, followed by 'addition' of new mesh geometry. Both operations are invoked by simple user edits of a sketch image, facilitating an iterative content creation process and enabling the construction of complex 3D meshes. Our approach is based on a triangle sequence-based mesh representation, exploiting a large Transformer model for mesh triangle addition and deletion. In order to perform edits interactively, we introduce a vertex-aligned speculative prediction strategy on top of our additive mesh generator. This speculator predicts multiple output tokens corresponding to a vertex, thus significantly reducing the computational cost of inference and accelerating the editing process, making it possible to execute each editing step in only a few seconds. Comprehensive experiments demonstrate that MeshPad outperforms state-of-the-art sketch-conditioned mesh generation methods, achieving more than 22% mesh quality improvement in Chamfer distance, and being preferred by 90% of participants in perceptual evaluations.</p>
  </details>
</details>
<details>
  <summary>46. <b>【2503.01416】Learning to Generate Long-term Future Narrations Describing Activities of Daily Living</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01416">https://arxiv.org/abs/2503.01416</a></p>
  <p><b>作者</b>：Ramanathan Rajendiran,Debaditya Roy,Basura Fernando</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：smart home technology, Anticipating future events, Anticipating future, smart home, home technology</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Anticipating future events is crucial for various application domains such as healthcare, smart home technology, and surveillance. Narrative event descriptions provide context-rich information, enhancing a system's future planning and decision-making capabilities. We propose a novel task: $\textit{long-term future narration generation}$, which extends beyond traditional action anticipation by generating detailed narrations of future daily activities. We introduce a visual-language model, ViNa, specifically designed to address this challenging task. ViNa integrates long-term videos and corresponding narrations to generate a sequence of future narrations that predict subsequent events and actions over extended time horizons. ViNa extends existing multimodal models that perform only short-term predictions or describe observed videos by generating long-term future narrations for a broader range of daily activities. We also present a novel downstream application that leverages the generated narrations called future video retrieval to help users improve planning for a task by visualizing the future. We evaluate future narration generation on the largest egocentric dataset Ego4D.</p>
  </details>
</details>
<details>
  <summary>47. <b>【2503.01407】Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based Adversarial Purification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01407">https://arxiv.org/abs/2503.01407</a></p>
  <p><b>作者</b>：Gaozheng Pei,Shaojie Lyu,Gong Chen,Ke Ma,Qianqian Xu,Yingfei Sun,Qingming Huang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Existing diffusion-based purification, Existing diffusion-based, disrupt adversarial perturbations, aim to disrupt, recover clean</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing diffusion-based purification methods aim to disrupt adversarial perturbations by introducing a certain amount of noise through a forward diffusion process, followed by a reverse process to recover clean examples. However, this approach is fundamentally flawed: the uniform operation of the forward process across all pixels compromises normal pixels while attempting to combat adversarial perturbations, resulting in the target model producing incorrect predictions. Simply relying on low-intensity noise is insufficient for effective defense. To address this critical issue, we implement a heterogeneous purification strategy grounded in the interpretability of neural networks. Our method decisively applies higher-intensity noise to specific pixels that the target model focuses on while the remaining pixels are subjected to only low-intensity noise. This requirement motivates us to redesign the sampling process of the diffusion model, allowing for the effective removal of varying noise levels. Furthermore, to evaluate our method against strong adaptative attack, our proposed method sharply reduces time cost and memory usage through a single-step resampling. The empirical evidence from extensive experiments across three datasets demonstrates that our method outperforms most current adversarial training and purification techniques by a substantial margin.</p>
  </details>
</details>
<details>
  <summary>48. <b>【2503.01387】Blind Augmentation: Calibration-free Camera Distortion Model Estimation for Real-time Mixed-reality Consistency</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01387">https://arxiv.org/abs/2503.01387</a></p>
  <p><b>作者</b>：Siddhant Prakash,David R. Walton,Rafael K. dos Anjos,Anthony Steed,Tobias Ritschel</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：Real camera footage, Real camera, motion blur, depth of field, footage is subject</p>
  <p><b>备注</b>： To appear in IEEE Transactions on Visualization and Computer Graphics (IEEEVR 2025). Project page can be found at [this https URL](https://prakashsidd18.github.io/projects/blind_augmentation/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Real camera footage is subject to noise, motion blur (MB) and depth of field (DoF). In some applications these might be considered distortions to be removed, but in others it is important to model them because it would be ineffective, or interfere with an aesthetic choice, to simply remove them. In augmented reality applications where virtual content is composed into a live video feed, we can model noise, MB and DoF to make the virtual content visually consistent with the video. Existing methods for this typically suffer two main limitations. First, they require a camera calibration step to relate a known calibration target to the specific cameras response. Second, existing work require methods that can be (differentiably) tuned to the calibration, such as slow and specialized neural networks. We propose a method which estimates parameters for noise, MB and DoF instantly, which allows using off-the-shelf real-time simulation methods from e.g., a game engine in compositing augmented content. Our main idea is to unlock both features by showing how to use modern computer vision methods that can remove noise, MB and DoF from the video stream, essentially providing self-calibration. This allows to auto-tune any black-box real-time noise+MB+DoF method to deliver fast and high-fidelity augmentation consistency.</p>
  </details>
</details>
<details>
  <summary>49. <b>【2503.01370】Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01370">https://arxiv.org/abs/2503.01370</a></p>
  <p><b>作者</b>：Jiantao Lin,Xin Yang,Meixi Chen,Yingjie Xu,Dongyu Yan,Leyi Wu,Xinli Xu,Lie XU,Shunsi Zhang,Ying-Cong Chen</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：achieved great success, achieved great, great success, diffusion model, Diffusion</p>
  <p><b>备注</b>： The first three authors contributed equally to this work</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate ''3D Bundle Image'', a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently.</p>
  </details>
</details>
<details>
  <summary>50. <b>【2503.01347】Spatial Transcriptomics Analysis of Spatially Dense Gene Expression Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01347">https://arxiv.org/abs/2503.01347</a></p>
  <p><b>作者</b>：Ruikun Zhang,Yan Yang,Liyuan Pan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：gene expression, tissue molecular landscapes, expression, measures gene expression, gene</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Spatial transcriptomics (ST) measures gene expression at fine-grained spatial resolution, offering insights into tissue molecular landscapes. Previous methods for spatial gene expression prediction usually crop spots of interest from pathology tissue slide images, and learn a model that maps each spot to a single gene expression profile. However, it fundamentally loses spatial resolution of gene expression: 1) each spot often contains multiple cells with distinct gene expression; 2) spots are cropped at fixed resolutions, limiting the ability to predict gene expression at varying spatial scales. To address these limitations, this paper presents PixNet, a dense prediction network capable of predicting spatially resolved gene expression across spots of varying sizes and scales directly from pathology images. Different from previous methods that map individual spots to gene expression values, we generate a dense continuous gene expression map from the pathology image, and aggregate values within spots of interest to predict the gene expression. Our PixNet outperforms state-of-the-art methods on 3 common ST datasets, while showing superior performance in predicting gene expression across multiple spatial scales. The source code will be publicly available.</p>
  </details>
</details>
<details>
  <summary>51. <b>【2503.01342】UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01342">https://arxiv.org/abs/2503.01342</a></p>
  <p><b>作者</b>：Hao Tang,Chenwei Xie,Haiyang Wang,Xiaoyi Bao,Tingyu Weng,Pandeng Li,Yun Zheng,Liwei Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：achieved remarkable success, showcasing the potential, achieved remarkable, remarkable success, potential of unified</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \ours, a framework that \textbf{U}nifies \textbf{F}ine-grained visual perception tasks through an \textbf{O}pen-ended language interface. By transforming all perception targets into the language space, \ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models will be publicly available.</p>
  </details>
</details>
<details>
  <summary>52. <b>【2503.01339】Wavelet-Enhanced Desnowing: A Novel Single Image Restoration Approach for Traffic Surveillance under Adverse Weather Conditions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01339">https://arxiv.org/abs/2503.01339</a></p>
  <p><b>作者</b>：Zihan Shen,Yu Xuan,Qingyu Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：adverse weather conditions, weather conditions refers, improving visual quality, adverse weather, weather conditions</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image restoration under adverse weather conditions refers to the process of removing degradation caused by weather particles while improving visual quality. Most existing deweathering methods rely on increasing the network scale and data volume to achieve better performance which requires more expensive computing power. Also, many methods lack generalization for specific applications. In the traffic surveillance screener, the main challenges are snow removal and veil effect elimination. In this paper, we propose a wavelet-enhanced snow removal method that use a Dual-Tree Complex Wavelet Transform feature enhancement module and a dynamic convolution acceleration module to address snow degradation in surveillance images. We also use a residual learning restoration module to remove veil effects caused by rain, snow, and fog. The proposed architecture extracts and analyzes information from snow-covered regions, significantly improving snow removal performance. And the residual learning restoration module removes veiling effects in images, enhancing clarity and detail. Experiments show that it performs better than some popular desnowing methods. Our approach also demonstrates effectiveness and accuracy when applied to real traffic surveillance images.</p>
  </details>
</details>
<details>
  <summary>53. <b>【2503.01334】Composed Multi-modal Retrieval: A Survey of Approaches and Applications</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01334">https://arxiv.org/abs/2503.01334</a></p>
  <p><b>作者</b>：Kun Zhang,Jingyu Li,Zhe Li,Jingjing Zhang</p>
  <p><b>类目</b>：Information Retrieval (cs.IR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：utilizing heterogeneous information, short video platforms, Composed Multi-modal Retrieval, social media, heterogeneous information</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With the rapid growth of multi-modal data from social media, short video platforms, and e-commerce, content-based retrieval has become essential for efficiently searching and utilizing heterogeneous information. Over time, retrieval techniques have evolved from Unimodal Retrieval (UR) to Cross-modal Retrieval (CR) and, more recently, to Composed Multi-modal Retrieval (CMR). CMR enables users to retrieve images or videos by integrating a reference visual input with textual modifications, enhancing search flexibility and precision. This paper provides a comprehensive review of CMR, covering its fundamental challenges, technical advancements, and categorization into supervised, zero-shot, and semi-supervised learning paradigms. We discuss key research directions, including data augmentation, model architecture, and loss optimization in supervised CMR, as well as transformation frameworks and external knowledge integration in zero-shot CMR. Additionally, we highlight the application potential of CMR in composed image retrieval, video retrieval, and person retrieval, which have significant implications for e-commerce, online search, and public security. Given its ability to refine and personalize search experiences, CMR is poised to become a pivotal technology in next-generation retrieval systems. A curated list of related works and resources is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>54. <b>【2503.01333】Group Relative Policy Optimization for Image Captioning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01333">https://arxiv.org/abs/2503.01333</a></p>
  <p><b>作者</b>：Xu Liang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Image captioning tasks, complete model optimization, greedy decoding result, captioning tasks, SCST</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image captioning tasks usually use two-stage training to complete model optimization. The first stage uses cross-entropy as the loss function for optimization, and the second stage uses self-critical sequence training (SCST) for reinforcement learning optimization. However, the SCST algorithm has certain defects. SCST relies only on a single greedy decoding result as a baseline. If the model itself is not stable enough, the greedy decoding result may be relatively worst, which will lead to a high variance of advantage estimation, further leading to unstable policy updates. In addition, SCST only compares one sampling result with the greedy decoding result, and the generation diversity is limited, which may fall into a local optimum. In this paper, we propose using the latest Group Relative Policy Optimization (GRPO) reinforcement learning algorithm as an optimization solution for the second stage. GRPO generates multiple candidate captions for the input image and then continuously optimizes the model through intragroup comparison. By constraining the amplitude of policy updates and KL divergence, the stability of the model during training is greatly guaranteed. In addition, compared to SCST, which only samples one answer, GRPO samples and generates multiple answers. Multiple candidate answers in the group cover a wider solution space. Combined with KL divergence constraints, GRPO can improve diversity while ensuring model stability. The code for this article is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>55. <b>【2503.01323】CacheQuant: Comprehensively Accelerated Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01323">https://arxiv.org/abs/2503.01323</a></p>
  <p><b>作者</b>：Xuewen Liu,Zhikai Li,Qingyi Gu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：showcasing remarkable generative, remarkable generative capabilities, gradually gained prominence, image synthesis, showcasing remarkable</p>
  <p><b>备注</b>： CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have gradually gained prominence in the field of image synthesis, showcasing remarkable generative capabilities. Nevertheless, the slow inference and complex networks, resulting from redundancy at both temporal and structural levels, hinder their low-latency applications in real-world scenarios. Current acceleration methods for diffusion models focus separately on temporal and structural levels. However, independent optimization at each level to further push the acceleration limits results in significant performance degradation. On the other hand, integrating optimizations at both levels can compound the acceleration effects. Unfortunately, we find that the optimizations at these two levels are not entirely orthogonal. Performing separate optimizations and then simply integrating them results in unsatisfactory performance. To tackle this issue, we propose CacheQuant, a novel training-free paradigm that comprehensively accelerates diffusion models by jointly optimizing model caching and quantization techniques. Specifically, we employ a dynamic programming approach to determine the optimal cache schedule, in which the properties of caching and quantization are carefully considered to minimize errors. Additionally, we propose decoupled error correction to further mitigate the coupled and accumulated errors step by step. Experimental results show that CacheQuant achieves a 5.18 speedup and 4 compression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP score. Our code are open-sourced: this https URL .</p>
  </details>
</details>
<details>
  <summary>56. <b>【2503.01309】OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01309">https://arxiv.org/abs/2503.01309</a></p>
  <p><b>作者</b>：Yijie Tang,Jiazhao Zhang,Yuqing Lan,Yulan Guo,Dezun Dong,Chenyang Zhu,Kai Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：progressively reconstructed scene, embodied applications, progressively reconstructed, critical and challenging, challenging task</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Online 3D open-vocabulary segmentation of a progressively reconstructed scene is both a critical and challenging task for embodied applications. With the success of visual foundation models (VFMs) in the image domain, leveraging 2D priors to address 3D online segmentation has become a prominent research focus. Since segmentation results provided by 2D priors often require spatial consistency to be lifted into final 3D segmentation, an efficient method for identifying spatial overlap among 2D masks is essential - yet existing methods rarely achieve this in real time, mainly limiting its use to offline approaches. To address this, we propose an efficient method that lifts 2D masks generated by VFMs into a unified 3D instance using a hashing technique. By employing voxel hashing for efficient 3D scene querying, our approach reduces the time complexity of costly spatial overlap queries from $O(n^2)$ to $O(n)$. Accurate spatial associations further enable 3D merging of 2D masks through simple similarity-based filtering in a zero-shot manner, making our approach more robust to incomplete and noisy data. Evaluated on the ScanNet and SceneNN benchmarks, our approach achieves state-of-the-art performance in online, open-vocabulary 3D instance segmentation with leading efficiency.</p>
  </details>
</details>
<details>
  <summary>57. <b>【2503.01298】MINT: Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01298">https://arxiv.org/abs/2503.01298</a></p>
  <p><b>作者</b>：Yi Wang,Mushui Liu,Wanggui He,Longxiang Zhang,Ziwei Huang,Guanghao Zhang,Fangxun Shu,Zhong Tao,Dong She,Zhelun Yu,Haoyuan Li,Weilong Dai,Mingli Song,Jie Song,Hao Jiang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：demonstrated extraordinary performance, Unified generative models, Unified generative, demonstrated extraordinary, innovative unified generative</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Unified generative models have demonstrated extraordinary performance in both text and image generation. However, they tend to underperform when generating intricate images with various interwoven conditions, which is hard to solely rely on straightforward text-to-image generation. In response to this challenge, we introduce MINT, an innovative unified generative model, empowered with native multimodal chain of thought (MCoT) for enhanced image generation for the first time. Firstly, we design Mixture of Transformer Experts (MTXpert), an expert-parallel structure that effectively supports both natural language generation (NLG) and visual capabilities, while avoiding potential modality conflicts that could hinder the full potential of each modality. Building on this, we propose an innovative MCoT training paradigm, a step-by-step approach to multimodal thinking, reasoning, and reflection specifically designed to enhance image generation. This paradigm equips MINT with nuanced, element-wise decoupled alignment and a comprehensive understanding of textual and visual components. Furthermore, it fosters advanced multimodal reasoning and self-reflection, enabling the construction of images that are firmly grounded in the logical relationships between these elements. Notably, MINT has been validated to exhibit superior performance across multiple benchmarks for text-to-image (T2I) and image-to-text (I2T) tasks.</p>
  </details>
</details>
<details>
  <summary>58. <b>【2503.01294】Fine-Grained Controllable Apparel Showcase Image Generation via Garment-Centric Outpainting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01294">https://arxiv.org/abs/2503.01294</a></p>
  <p><b>作者</b>：Rong Zhang,Jingnan Wang,Zhiwen Zuo,Jianfeng Dong,Wei Li,Chi Wang,Weiwei Xu,Xun Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：latent diffusion model, showcase image generation, garment-centric outpainting, latent diffusion, controllable apparel showcase</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we propose a novel garment-centric outpainting (GCO) framework based on the latent diffusion model (LDM) for fine-grained controllable apparel showcase image generation. The proposed framework aims at customizing a fashion model wearing a given garment via text prompts and facial images. Different from existing methods, our framework takes a garment image segmented from a dressed mannequin or a person as the input, eliminating the need for learning cloth deformation and ensuring faithful preservation of garment details. The proposed framework consists of two stages. In the first stage, we introduce a garment-adaptive pose prediction model that generates diverse poses given the garment. Then, in the next stage, we generate apparel showcase images, conditioned on the garment and the predicted poses, along with specified text prompts and facial images. Notably, a multi-scale appearance customization module (MS-ACM) is designed to allow both overall and fine-grained text-based control over the generated model's appearance. Moreover, we leverage a lightweight feature fusion operation without introducing any extra encoders or modules to integrate multiple conditions, which is more efficient. Extensive experiments validate the superior performance of our framework compared to state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>59. <b>【2503.01292】PA-CLIP: Enhancing Zero-Shot Anomaly Detection through Pseudo-Anomaly Awareness</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01292">https://arxiv.org/abs/2503.01292</a></p>
  <p><b>作者</b>：Yurui Pan,Lidong Wang,Yuchao Chen,Wenbing Zhu,Bo Peng,Mingmin Chi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：amidst diverse anomalies, varying imaging conditions, imaging conditions remains, accurately identifying defects, identifying defects amidst</p>
  <p><b>备注</b>： 9 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In industrial anomaly detection (IAD), accurately identifying defects amidst diverse anomalies and under varying imaging conditions remains a significant challenge. Traditional approaches often struggle with high false-positive rates, frequently misclassifying normal shadows and surface deformations as defects, an issue that becomes particularly pronounced in products with complex and intricate surface features. To address these challenges, we introduce PA-CLIP, a zero-shot anomaly detection method that reduces background noise and enhances defect detection through a pseudo-anomaly-based framework. The proposed method integrates a multiscale feature aggregation strategy for capturing detailed global and local information, two memory banks for distinguishing background information, including normal patterns and pseudo-anomalies, from true anomaly features, and a decision-making module designed to minimize false positives caused by environmental variations while maintaining high defect sensitivity. Demonstrated on the MVTec AD and VisA datasets, PA-CLIP outperforms existing zero-shot methods, providing a robust solution for industrial defect detection.</p>
  </details>
</details>
<details>
  <summary>60. <b>【2503.01291】SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01291">https://arxiv.org/abs/2503.01291</a></p>
  <p><b>作者</b>：Peishan Cong,Ziyi Wang,Yuexin Ma,Xiangyu Yue</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：high-quality human interactive, applying human behaviors, Generating reasonable, human interactive motions, crucial for understanding</p>
  <p><b>备注</b>： accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generating reasonable and high-quality human interactive motions in a given dynamic environment is crucial for understanding, modeling, transferring, and applying human behaviors to both virtual and physical robots. In this paper, we introduce an effective method, SemGeoMo, for dynamic contextual human motion generation, which fully leverages the text-affordance-joint multi-level semantic and geometric guidance in the generation process, improving the semantic rationality and geometric correctness of generative motions. Our method achieves state-of-the-art performance on three datasets and demonstrates superior generalization capability for diverse interaction scenarios. The project page and code can be found at this https URL.</p>
  </details>
</details>
<details>
  <summary>61. <b>【2503.01288】Reconciling Stochastic and Deterministic Strategies for Zero-shot Image Restoration using Diffusion Model in Dual</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01288">https://arxiv.org/abs/2503.01288</a></p>
  <p><b>作者</b>：Chong Wang,Lanqing Guo,Zixuan Fu,Siyuan Yang,Hao Cheng,Alex C. Kot,Bihan Wen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：pre-trained diffusion model, diffusion model, implicit prior, Reconciling Diffusion Model, offer an iterative</p>
  <p><b>备注</b>： Accepted to CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Plug-and-play (PnP) methods offer an iterative strategy for solving image restoration (IR) problems in a zero-shot manner, using a learned \textit{discriminative denoiser} as the implicit prior. More recently, a sampling-based variant of this approach, which utilizes a pre-trained \textit{generative diffusion model}, has gained great popularity for solving IR problems through stochastic sampling. The IR results using PnP with a pre-trained diffusion model demonstrate distinct advantages compared to those using discriminative denoisers, \ie improved perceptual quality while sacrificing the data fidelity. The unsatisfactory results are due to the lack of integration of these strategies in the IR tasks. In this work, we propose a novel zero-shot IR scheme, dubbed Reconciling Diffusion Model in Dual (RDMD), which leverages only a \textbf{single} pre-trained diffusion model to construct \textbf{two} complementary regularizers. Specifically, the diffusion model in RDMD will iteratively perform deterministic denoising and stochastic sampling, aiming to achieve high-fidelity image restoration with appealing perceptual quality. RDMD also allows users to customize the distortion-perception tradeoff with a single hyperparameter, enhancing the adaptability of the restoration process in different practical scenarios. Extensive experiments on several IR tasks demonstrate that our proposed method could achieve superior results compared to existing approaches on both the FFHQ and ImageNet datasets.</p>
  </details>
</details>
<details>
  <summary>62. <b>【2503.01284】Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01284">https://arxiv.org/abs/2503.01284</a></p>
  <p><b>作者</b>：Md Abrar Jahin,Soudeep Shahriar,M. F. Mridha,Nilanjan Dey</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Convolutional Neural Networks, faces challenges due, visually similar symptoms, CNN-Graph Neural Network, conventional methods</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16\%$ accuracy, surpassing standalone CNNs ($\le95.04\%$) and traditional machine learning models ($\le77.05\%$). Ablation studies validate the sequential architecture's superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research.</p>
  </details>
</details>
<details>
  <summary>63. <b>【2503.01263】Generalizable Prompt Learning of CLIP: A Brief Overview</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01263">https://arxiv.org/abs/2503.01263</a></p>
  <p><b>作者</b>：Fangming Cui,Yonggang Zhang,Xuan Wang,Xule Wang,Liang Xiao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Existing vision-language models, Existing vision-language, showcased an impressive, impressive capability, capability to generalize</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing vision-language models (VLMs) such as CLIP have showcased an impressive capability to generalize well across various downstream tasks. These models leverage the synergy between visual and textual information, enabling them to understand and reason about the content present in images and text in a unified manner. This article provides a brief overview of CLIP based on few-shot prompt learning, including experimental data and technical characteristics of some methods. The purpose of this review is to provide a reference for researchers who have just started their research in generalizable prompting of CLIP through few-shot training for classification across 15 datasets and also to facilitate the integration of this field by researchers in other downstream tasks.</p>
  </details>
</details>
<details>
  <summary>64. <b>【2503.01262】Object-Aware Video Matting with Cross-Frame Guidance</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01262">https://arxiv.org/abs/2503.01262</a></p>
  <p><b>作者</b>：Huayu Zhang,Dongyue Wu,Yuanjie Shao,Nong Sang,Changxin Gao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：drawn increasing attention, human video matting, video matting due, video matting, drawn increasing</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recently, trimap-free methods have drawn increasing attention in human video matting due to their promising performance. Nevertheless, these methods still suffer from the lack of deterministic foreground-background cues, which impairs their ability to consistently identify and locate foreground targets over time and mine fine-grained details. In this paper, we present a trimap-free Object-Aware Video Matting (OAVM) framework, which can perceive different objects, enabling joint recognition of foreground objects and refinement of edge details. Specifically, we propose an Object-Guided Correction and Refinement (OGCR) module, which employs cross-frame guidance to aggregate object-level instance information into pixel-level detail features, thereby promoting their synergy. Furthermore, we design a Sequential Foreground Merging augmentation strategy to diversify sequential scenarios and enhance capacity of the network for object discrimination. Extensive experiments on recent widely used synthetic and real-world benchmarks demonstrate the state-of-the-art performance of our OAVM with only an initial coarse mask. The code and model will be available.</p>
  </details>
</details>
<details>
  <summary>65. <b>【2503.01261】owards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01261">https://arxiv.org/abs/2503.01261</a></p>
  <p><b>作者</b>：Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Junteng Zhao,Yunming Ye,Kola Ye,Yao He</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：discrete token sequence, token sequence, codebook, crucial technique, discrete token</p>
  <p><b>备注</b>： Accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level. In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning. However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.</p>
  </details>
</details>
<details>
  <summary>66. <b>【2503.01257】SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01257">https://arxiv.org/abs/2503.01257</a></p>
  <p><b>作者</b>：Xuan Zhu,Jijun Xiang,Xianqi Wang,Longliang Liu,Yu Wang,Hong Zhang,Fei Guo,Xin Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Lightweight direct, sensors are ideal, sensing on mobile, mobile devices, sparse dToF imaging</p>
  <p><b>备注</b>： Accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on mobile devices. However, due to the manufacturing constraints of compact devices and the inherent physical principles of imaging, dToF depth maps are sparse and noisy. In this paper, we propose a novel video depth completion method, called SVDC, by fusing the sparse dToF data with the corresponding RGB guidance. Our method employs a multi-frame fusion scheme to mitigate the spatial ambiguity resulting from the sparse dToF imaging. Misalignment between consecutive frames during multi-frame fusion could cause blending between object edges and the background, which results in a loss of detail. To address this, we introduce an adaptive frequency selective fusion (AFSF) module, which automatically selects convolution kernel sizes to fuse multi-frame features. Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to enhance features and generates an attention map as fusion weights. The AFSF ensures edge detail recovery while suppressing high-frequency noise in smooth regions. To further enhance temporal consistency, We propose a cross-window consistency loss to ensure consistent predictions across different windows, effectively reducing flickering. Our proposed SVDC achieves optimal accuracy and consistency on the TartanAir and Dynamic Replica datasets. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>67. <b>【2503.01254】Convex Hull-based Algebraic Constraint for Visual Quadric SLAM</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01254">https://arxiv.org/abs/2503.01254</a></p>
  <p><b>作者</b>：Xiaolong Yu,Junqiao Zhao,Shuangfu Song,Zhongyang Zhu,Zihan Yuan,Chen Ye,Tiantian Feng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：closed-form projection derivation, http URL, http URL constraint, http URL scrutinizing, quadric SLAM methods</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Using Quadrics as the object representation has the benefits of both generality and closed-form projection derivation between image and world spaces. Although numerous constraints have been proposed for dual quadric reconstruction, we found that many of them are imprecise and provide minimal improvements to this http URL scrutinizing the existing constraints, we introduce a concise yet more precise convex hull-based algebraic constraint for object landmarks, which is applied to object reconstruction, frontend pose estimation, and backend bundle this http URL constraint is designed to fully leverage precise semantic segmentation, effectively mitigating mismatches between complex-shaped object contours and dual this http URL on public datasets demonstrate that our approach is applicable to both monocular and RGB-D SLAM and achieves improved object mapping and localization than existing quadric SLAM methods. The implementation of our method is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>68. <b>【2503.01234】Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01234">https://arxiv.org/abs/2503.01234</a></p>
  <p><b>作者</b>：Sijin Sun,Ming Deng,Xingrui Yu,Xinyu Xi,Liangbin Zhao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：industrial quality assurance, existing methods struggle, complex defect states, Metal defect detection, Dynamic Gamma Correction</p>
  <p><b>备注</b>： 19 pages, 9 figures, under review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Metal defect detection is critical in industrial quality assurance, yet existing methods struggle with grayscale variations and complex defect states, limiting its robustness. To address these challenges, this paper proposes a Self-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced detection framework integrating a Dynamic Gamma Correction (GC) module to enhance grayscale representation and optimize feature extraction for precise defect reconstruction. A State-Space Search Management (SSM) architecture captures robust multi-scale features, effectively handling defects of varying shapes and scales. Focal Loss is employed to mitigate class imbalance and refine detection accuracy. Additionally, the CD5-DET dataset is introduced, specifically designed for port container maintenance, featuring significant grayscale variations and intricate defect patterns. Experimental results demonstrate that the proposed model achieves substantial improvements, with mAP@0.5 gains of 27.6\%, 6.6\%, and 2.6\% on the CD5-DET, NEU-DET, and GC10-DET datasets.</p>
  </details>
</details>
<details>
  <summary>69. <b>【2503.01222】Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01222">https://arxiv.org/abs/2503.01222</a></p>
  <p><b>作者</b>：Wenbin Wang,Yongcheng Jing,Liang Ding,Yingjie Wang,Li Shen,Yong Luo,Bo Du,Dacheng Tao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：multimodal large language, large language models, image perception remains, remains a key, multimodal large</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:High-resolution (HR) image perception remains a key challenge in multimodal large language models (MLLMs). To overcome the limitations of existing methods, this paper shifts away from prior dedicated heuristic approaches and revisits the most fundamental idea to HR perception by enhancing the long-context capability of MLLMs, driven by recent advances in long-context techniques like retrieval-augmented generation (RAG) for general LLMs. Towards this end, this paper presents the first study exploring the use of RAG to address HR perception challenges. Specifically, we propose Retrieval-Augmented Perception (RAP), a training-free framework that retrieves and fuses relevant image crops while preserving spatial context using the proposed Spatial-Awareness Layout. To accommodate different tasks, the proposed Retrieved-Exploration Search (RE-Search) dynamically selects the optimal number of crops based on model confidence and retrieval scores. Experimental results on HR benchmarks demonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving a 43% improvement on $V^*$ Bench and 19% on HR-Bench.</p>
  </details>
</details>
<details>
  <summary>70. <b>【2503.01220】ra-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01220">https://arxiv.org/abs/2503.01220</a></p>
  <p><b>作者</b>：Jiqing Wu,Ingrid Berg,Yawei Li,Ender Konukoglu,Viktor H. Koelzer</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：molecularly defined brain, defined brain structures, understanding complex brain, complex brain functions, modeling of molecularly</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Emerging tissue profiling technologies enable the construction of a comprehensive atlas of the mammalian brain with sub-cellular resolution and spatially resolved gene expression data. However, such tera-scale volumetric datasets present significant computational challenges in understanding complex brain functions within their native 3D spatial context. Here, we propose the novel generative approach $\textbf{Tera-MIND}$, which can simulate $\textbf{Tera}$-scale $\textbf{M}$ouse bra$\textbf{IN}s$ in 3D using a patch-based and boundary-aware $\textbf{D}$iffusion model. Taking spatial transcriptomic data as the conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D $gene$-$gene$ self-attention, we identify spatial molecular interactions for key transcriptomic pathways in the murine brain, exemplified by glutamatergic and dopaminergic neuronal systems. Importantly, these $in$-$silico$ biological findings are consistent and reproducible across three tera-scale virtual mouse brains. Therefore, Tera-MIND showcases a promising path toward efficient and generative simulations of whole organ systems for biomedical research. Project website: $\href{this http URL}{https}$</p>
  </details>
</details>
<details>
  <summary>71. <b>【2503.01214】One-Step Event-Driven High-Speed Autofocus</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01214">https://arxiv.org/abs/2503.01214</a></p>
  <p><b>作者</b>：Yuhan Bao,Shaohua Gao,Wenyong Li,Kaiwei Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Optics (physics.optics)</p>
  <p><b>关键词</b>：extreme scenes remains, significant challenge, High-speed autofocus, extreme scenes, scenes remains</p>
  <p><b>备注</b>： Main text: 9 pages, 6 figures. Supplementary Material: 4 pages, 3 figures. Accepted by CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:High-speed autofocus in extreme scenes remains a significant challenge. Traditional methods rely on repeated sampling around the focus position, resulting in ``focus hunting''. Event-driven methods have advanced focusing speed and improved performance in low-light conditions; however, current approaches still require at least one lengthy round of ``focus hunting'', involving the collection of a complete focus stack. We introduce the Event Laplacian Product (ELP) focus detection function, which combines event data with grayscale Laplacian information, redefining focus search as a detection task. This innovation enables the first one-step event-driven autofocus, cutting focusing time by up to two-thirds and reducing focusing error by 24 times on the DAVIS346 dataset and 22 times on the EVK4 dataset. Additionally, we present an autofocus pipeline tailored for event-only cameras, achieving accurate results across a range of challenging motion and lighting conditions. All datasets and code will be made publicly available.</p>
  </details>
</details>
<details>
  <summary>72. <b>【2503.01212】Understanding Dataset Distillation via Spectral Filtering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01212">https://arxiv.org/abs/2503.01212</a></p>
  <p><b>作者</b>：Deyu Bo,Songhua Liu,Xinchao Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：model training, promising approach, approach to compress, speed up model, compress datasets</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Dataset distillation (DD) has emerged as a promising approach to compress datasets and speed up model training. However, the underlying connections among various DD methods remain largely unexplored. In this paper, we introduce UniDD, a spectral filtering framework that unifies diverse DD objectives. UniDD interprets each DD objective as a specific filter function that affects the eigenvalues of the feature-feature correlation (FFC) matrix and modulates the frequency components of the feature-label correlation (FLC) matrix. In this way, UniDD reveals that the essence of DD fundamentally lies in matching frequency-specific features. Moreover, according to the filter behaviors, we classify existing methods into low-frequency matching and high-frequency matching, encoding global texture and local details, respectively. However, existing methods rely on fixed filter functions throughout distillation, which cannot capture the low- and high-frequency information simultaneously. To address this limitation, we further propose Curriculum Frequency Matching (CFM), which gradually adjusts the filter parameter to cover both low- and high-frequency information of the FFC and FLC matrices. Extensive experiments on small-scale datasets, such as CIFAR-10/100, and large-scale datasets, including ImageNet-1K, demonstrate the superior performance of CFM over existing baselines and validate the practicality of UniDD.</p>
  </details>
</details>
<details>
  <summary>73. <b>【2503.01210】Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01210">https://arxiv.org/abs/2503.01210</a></p>
  <p><b>作者</b>：Guanyao Wu,Haoyu Liu,Hongming Fu,Yichuan Peng,Jinyuan Liu,Xin Fan,Risheng Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Multi-modality image fusion, enhance scene understanding, integrating diverse modalities, visible image fusion, Multi-modality image</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-modality image fusion, particularly infrared and visible image fusion, plays a crucial role in integrating diverse modalities to enhance scene understanding. Early research primarily focused on visual quality, yet challenges remain in preserving fine details, making it difficult to adapt to subsequent tasks. Recent approaches have shifted towards task-specific design, but struggle to achieve the ``The Best of Both Worlds'' due to inconsistent optimization goals. To address these issues, we propose a novel method that leverages the semantic knowledge from the Segment Anything Model (SAM) to Grow the quality of fusion results and Establish downstream task adaptability, namely SAGE. Specifically, we design a Semantic Persistent Attention (SPA) Module that efficiently maintains source information via the persistent repository while extracting high-level semantic priors from SAM. More importantly, to eliminate the impractical dependence on SAM during inference, we introduce a bi-level optimization-driven distillation mechanism with triplet losses, which allow the student network to effectively extract knowledge at the feature, pixel, and contrastive semantic levels, thereby removing reliance on the cumbersome SAM model. Extensive experiments show that our method achieves a balance between high-quality visual results and downstream task adaptability while maintaining practical deployment efficiency.</p>
  </details>
</details>
<details>
  <summary>74. <b>【2503.01208】Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01208">https://arxiv.org/abs/2503.01208</a></p>
  <p><b>作者</b>：Tianjie Ju,Yi Hua,Hao Fei,Zhenyu Shao,Yubin Zheng,Haodong Zhao,Mong-Li Lee,Wynne Hsu,Zhuosheng Zhang,Gongshen Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：Large Language Models, Visual Question Answering, Multi-Modal Large Language, Language Models, Question Answering</p>
  <p><b>备注</b>： Working in progress</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>75. <b>【2503.01202】A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01202">https://arxiv.org/abs/2503.01202</a></p>
  <p><b>作者</b>：Jialei He,Zhihao Zhan,Zhituo Tu,Xiang Zhu,Jie Yuan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：Unmanned Aerial Vehicles, Aerial Vehicles, Unmanned Aerial, Inertial Measurement Unit, aerial mapping</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Rapid generation of large-scale orthoimages from Unmanned Aerial Vehicles (UAVs) has been a long-standing focus of research in the field of aerial mapping. A multi-sensor UAV system, integrating the Global Positioning System (GPS), Inertial Measurement Unit (IMU), 4D millimeter-wave radar and camera, can provide an effective solution to this problem. In this paper, we utilize multi-sensor data to overcome the limitations of conventional orthoimage generation methods in terms of temporal performance, system robustness, and geographic reference accuracy. A prior-pose-optimized feature matching method is introduced to enhance matching speed and accuracy, reducing the number of required features and providing precise references for the Structure from Motion (SfM) process. The proposed method exhibits robustness in low-texture scenes like farmlands, where feature matching is difficult. Experiments show that our approach achieves accurate feature matching orthoimage generation in a short time. The proposed drone system effectively aids in farmland detection and management.</p>
  </details>
</details>
<details>
  <summary>76. <b>【2503.01201】Parameter-free Video Segmentation for Vision and Language Understanding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01201">https://arxiv.org/abs/2503.01201</a></p>
  <p><b>作者</b>：Louis Mahon,Mirella Lapata</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：enable multimodal understanding, adapting language models, creative video content, handle video input, multimodal understanding</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The proliferation of creative video content has driven demand for adapting language models to handle video input and enable multimodal understanding. However, end-to-end models struggle to process long videos due to their size and complexity. An effective alternative is to divide them into smaller chunks to be processed separately, and this motivates a method for choosing where the chunk boundaries should be. In this paper, we propose an algorithm for segmenting videos into contiguous chunks, based on the minimum description length principle, coupled with a dynamic programming search. The algorithm is entirely parameter-free, given feature vectors, not requiring a set threshold or the number or size of chunks to be specified. We show empirically that the breakpoints it produces more accurately approximate scene boundaries in long videos, compared with existing methods for scene detection, even when such methods have access to the true number of scenes. We then showcase this algorithm in two tasks: long video summarization, and retrieval-augmented video question answering. In both cases, scene breaks produced by our algorithm lead to better downstream performance than existing methods for video segmentation.</p>
  </details>
</details>
<details>
  <summary>77. <b>【2503.01199】LiteGS: A High-Performance Modular Framework for Gaussian Splatting Training</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01199">https://arxiv.org/abs/2503.01199</a></p>
  <p><b>作者</b>：Kaimin Liao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：scenes in computer, graphics and vision, powerful technique, technique for reconstruction, computer graphics</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Gaussian splatting has emerged as a powerful technique for reconstruction of 3D scenes in computer graphics and vision. However, conventional implementations often suffer from inefficiencies, limited flexibility, and high computational overhead, which constrain their adaptability to diverse applications. In this paper, we present LiteGS,a high-performance and modular framework that enhances both the efficiency and usability of Gaussian splatting. LiteGS achieves a 3.4x speedup over the original 3DGS implementation while reducing GPU memory usage by approximately 30%. Its modular design decomposes the splatting process into multiple highly optimized operators, and it provides dual API support via a script-based interface and a CUDA-based interface. The script-based interface, in combination with autograd, enables rapid prototyping and straightforward customization of new ideas, while the CUDA-based interface delivers optimal training speeds for performance-critical applications. LiteGS retains the core algorithm of 3DGS, ensuring compatibility. Comprehensive experiments on the Mip-NeRF 360 dataset demonstrate that LiteGS accelerates training without compromising accuracy, making it an ideal solution for both rapid prototyping and production environments.</p>
  </details>
</details>
<details>
  <summary>78. <b>【2503.01195】PostHoc FREE Calibrating on Kolmogorov Arnold Networks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01195">https://arxiv.org/abs/2503.01195</a></p>
  <p><b>作者</b>：Wenhao Liang,Wei Emma Zhang,Lin Yue,Miao Xu,Olaf Maennel,Weitong Chen</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Kolmogorov Arnold representation, Kolmogorov Arnold Networks, Arnold representation theorem, Kolmogorov Arnold, locally adaptive function</p>
  <p><b>备注</b>： Under reviewing</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Kolmogorov Arnold Networks (KANs) are neural architectures inspired by the Kolmogorov Arnold representation theorem that leverage B Spline parameterizations for flexible, locally adaptive function approximation. Although KANs can capture complex nonlinearities beyond those modeled by standard MultiLayer Perceptrons (MLPs), they frequently exhibit miscalibrated confidence estimates manifesting as overconfidence in dense data regions and underconfidence in sparse areas. In this work, we systematically examine the impact of four critical hyperparameters including Layer Width, Grid Order, Shortcut Function, and Grid Range on the calibration of KANs. Furthermore, we introduce a novel TemperatureScaled Loss (TSL) that integrates a temperature parameter directly into the training objective, dynamically adjusting the predictive distribution during learning. Both theoretical analysis and extensive empirical evaluations on standard benchmarks demonstrate that TSL significantly reduces calibration errors, thereby improving the reliability of probabilistic predictions. Overall, our study provides actionable insights into the design of spline based neural networks and establishes TSL as a robust loss solution for enhancing calibration.</p>
  </details>
</details>
<details>
  <summary>79. <b>【2503.01193】Near-infrared Image Deblurring and Event Denoising with Synergistic Neuromorphic Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01193">https://arxiv.org/abs/2503.01193</a></p>
  <p><b>作者</b>：Chao Qu,Shuo Zhu,Yuhang Wang,Zongze Wu,Xiaoyu Chen,Edmund Y.Lam,Jing Han</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：extremely dark conditions, NIR images, near-infrared cameras make, recent years, partly driven</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The fields of imaging in the nighttime dynamic and other extremely dark conditions have seen impressive and transformative advancements in recent years, partly driven by the rise of novel sensing approaches, e.g., near-infrared (NIR) cameras with high sensitivity and event cameras with minimal blur. However, inappropriate exposure ratios of near-infrared cameras make them susceptible to distortion and blur. Event cameras are also highly sensitive to weak signals at night yet prone to interference, often generating substantial noise and significantly degrading observations and analysis. Herein, we develop a new framework for low-light imaging combined with NIR imaging and event-based techniques, named synergistic neuromorphic imaging, which can jointly achieve NIR image deblurring and event denoising. Harnessing cross-modal features of NIR images and visible events via spectral consistency and higher-order interaction, the NIR images and events are simultaneously fused, enhanced, and bootstrapped. Experiments on real and realistically simulated sequences demonstrate the effectiveness of our method and indicate better accuracy and robustness than other methods in practical scenarios. This study gives impetus to enhance both NIR images and events, which paves the way for high-fidelity low-light imaging and neuromorphic reasoning.</p>
  </details>
</details>
<details>
  <summary>80. <b>【2503.01190】Enhancing Retinal Vessel Segmentation Generalization via Layout-Aware Generative Modelling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01190">https://arxiv.org/abs/2503.01190</a></p>
  <p><b>作者</b>：Jonathan Fhima,Jan Van Eijgen,Lennert Beeckmans,Thomas Jacobs,Moti Freiman,Luis Filipe Nakayama,Ingeborg Stalmans,Chaim Baskin,Joachim A. Behar</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：medical segmentation models, limited annotated datasets, models is challenging, challenging due, due to limited</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generalization in medical segmentation models is challenging due to limited annotated datasets and imaging variability. To address this, we propose Retinal Layout-Aware Diffusion (RLAD), a novel diffusion-based framework for generating controllable layout-aware images. RLAD conditions image generation on multiple key layout components extracted from real images, ensuring high structural fidelity while enabling diversity in other components. Applied to retinal fundus imaging, we augmented the training datasets by synthesizing paired retinal images and vessel segmentations conditioned on extracted blood vessels from real images, while varying other layout components such as lesions and the optic disc. Experiments demonstrated that RLAD-generated data improved generalization in retinal vessel segmentation by up to 8.1%. Furthermore, we present REYIA, a comprehensive dataset comprising 586 manually segmented retinal images. To foster reproducibility and drive innovation, both our code and dataset will be made publicly accessible.</p>
  </details>
</details>
<details>
  <summary>81. <b>【2503.01187】DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01187">https://arxiv.org/abs/2503.01187</a></p>
  <p><b>作者</b>：Xingyuan Li,Zirui Wang,Yang Zou,Zhixin Chen,Jun Ma,Zhiying Jiang,Long Ma,Jinyuan Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：supportive modality due, challenging environments, Infrared, essential for autonomous, autonomous driving</p>
  <p><b>备注</b>： This paper was accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Infrared imaging is essential for autonomous driving and robotic operations as a supportive modality due to its reliable performance in challenging environments. Despite its popularity, the limitations of infrared cameras, such as low spatial resolution and complex degradations, consistently challenge imaging quality and subsequent visual tasks. Hence, infrared image super-resolution (IISR) has been developed to address this challenge. While recent developments in diffusion models have greatly advanced this field, current methods to solve it either ignore the unique modal characteristics of infrared imaging or overlook the machine perception requirements. To bridge these gaps, we propose DifIISR, an infrared image super-resolution diffusion model optimized for visual quality and perceptual performance. Our approach achieves task-based guidance for diffusion by injecting gradients derived from visual and perceptual priors into the noise during the reverse process. Specifically, we introduce an infrared thermal spectrum distribution regulation to preserve visual fidelity, ensuring that the reconstructed infrared images closely align with high-resolution images by matching their frequency components. Subsequently, we incorporate various visual foundational models as the perceptual guidance for downstream visual tasks, infusing generalizable perceptual features beneficial for detection and segmentation. As a result, our approach gains superior visual results while attaining State-Of-The-Art downstream task performance. Code is available at this https URL</p>
  </details>
</details>
<details>
  <summary>82. <b>【2503.01184】Language-Assisted Feature Transformation for Anomaly Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01184">https://arxiv.org/abs/2503.01184</a></p>
  <p><b>作者</b>：EungGu Yun,Heonjin Ha,Yeongwoo Nam,Bryan Dongik Lee</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：paper introduces LAFT, incorporate user knowledge, natural language, transformation method designed, paper introduces</p>
  <p><b>备注</b>： ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper introduces LAFT, a novel feature transformation method designed to incorporate user knowledge and preferences into anomaly detection using natural language. Accurately modeling the boundary of normality is crucial for distinguishing abnormal data, but this is often challenging due to limited data or the presence of nuisance attributes. While unsupervised methods that rely solely on data without user guidance are common, they may fail to detect anomalies of specific interest. To address this limitation, we propose Language-Assisted Feature Transformation (LAFT), which leverages the shared image-text embedding space of vision-language models to transform visual features according to user-defined requirements. Combined with anomaly detection methods, LAFT effectively aligns visual features with user preferences, allowing anomalies of interest to be detected. Extensive experiments on both toy and real-world datasets validate the effectiveness of our method.</p>
  </details>
</details>
<details>
  <summary>83. <b>【2503.01181】SAR-W-MixMAE: SAR Foundation Model Training Using Backscatter Power Weighting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01181">https://arxiv.org/abs/2503.01181</a></p>
  <p><b>作者</b>：Ali Caglayan,Nevrez Imamoglu,Toru Kouyama</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：Foundation model approaches, Synthetic Aperture Radar, satellite imagery, SAR, foundation models</p>
  <p><b>备注</b>： 5 pages, 1 figure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Foundation model approaches such as masked auto-encoders (MAE) or its variations are now being successfully applied to satellite imagery. Most of the ongoing technical validation of foundation models have been applied to optical images like RGB or multi-spectral images. Due to difficulty in semantic labeling to create datasets and higher noise content with respect to optical images, Synthetic Aperture Radar (SAR) data has not been explored a lot in the field for foundation models. Therefore, in this work as a pre-training approach, we explored masked auto-encoder, specifically MixMAE on Sentinel-1 SAR images and its impact on SAR image classification tasks. Moreover, we proposed to use the physical characteristic of SAR data for applying weighting parameter on the auto-encoder training loss (MSE) to reduce the effect of speckle noise and very high values on the SAR images. Proposed SAR intensity-based weighting of the reconstruction loss demonstrates promising results both on SAR pre-training and downstream tasks specifically on flood detection compared with the baseline model.</p>
  </details>
</details>
<details>
  <summary>84. <b>【2503.01175】HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01175">https://arxiv.org/abs/2503.01175</a></p>
  <p><b>作者</b>：Hongye Cheng,Tianyu Wang,Guangsi Shi,Zexing Zhao,Yanwei Fu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：crucial non-verbal cues, attracted increasing attention, enhance speech clarity, human communication, crucial non-verbal</p>
  <p><b>备注</b>： Accepted by CVPR 2025. See [this https URL](https://star-uu-wang.github.io/HOP/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Co-speech gestures are crucial non-verbal cues that enhance speech clarity and expressiveness in human communication, which have attracted increasing attention in multimodal research. While the existing methods have made strides in gesture accuracy, challenges remain in generating diverse and coherent gestures, as most approaches assume independence among multimodal inputs and lack explicit modeling of their interactions. In this work, we propose a novel multimodal learning method named HOP for co-speech gesture generation that captures the heterogeneous entanglement between gesture motion, audio rhythm, and text semantics, enabling the generation of coordinated gestures. By leveraging spatiotemporal graph modeling, we achieve the alignment of audio and action. Moreover, to enhance modality coherence, we build the audio-text semantic representation based on a reprogramming module, which is beneficial for cross-modality adaptation. Our approach enables the trimodal system to learn each other's features and represent them in the form of topological entanglement. Extensive experiments demonstrate that HOP achieves state-of-the-art performance, offering more natural and expressive co-speech gesture generation. More information, codes, and demos are available here: this https URL</p>
  </details>
</details>
<details>
  <summary>85. <b>【2503.01169】A Zero-Shot Learning Approach for Ephemeral Gully Detection from Remote Sensing using Vision Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01169">https://arxiv.org/abs/2503.01169</a></p>
  <p><b>作者</b>：Seyed Mohamad Ali Tousi,Ramy Farag,Jacket Demby's,Gbenga Omotara,John A. Lory,G. N. DeSouza</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：facilitate significant improvements, global agricultural systems, Ephemeral gullies, ephemeral gully detection, Visual Language Models</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Ephemeral gullies are a primary cause of soil erosion and their reliable, accurate, and early detection will facilitate significant improvements in the sustainability of global agricultural systems. In our view, prior research has not successfully addressed automated detection of ephemeral gullies from remotely sensed images, so for the first time, we present and evaluate three successful pipelines for ephemeral gully detection. Our pipelines utilize remotely sensed images, acquired from specific agricultural areas over a period of time. The pipelines were tested with various choices of Visual Language Models (VLMs), and they classified the images based on the presence of ephemeral gullies with accuracy higher than 70% and a F1-score close to 80% for positive gully detection. Additionally, we developed the first public dataset for ephemeral gully detection, labeled by a team of soil- and plant-science experts. To evaluate the proposed pipelines, we employed a variety of zero-shot classification methods based on State-of-the-Art (SOTA) open-source Vision-Language Models (VLMs). In addition to that, we compare the same pipelines with a transfer learning approach. Extensive experiments were conducted to validate the detection pipelines and to analyze the impact of hyperparameter changes in their performance. The experimental results demonstrate that the proposed zero-shot classification pipelines are highly effective in detecting ephemeral gullies in a scenario where classification datasets are scarce.</p>
  </details>
</details>
<details>
  <summary>86. <b>【2503.01167】Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01167">https://arxiv.org/abs/2503.01167</a></p>
  <p><b>作者</b>：Haoxin Li,Boyang Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：subtle variations, Subtle Variation Data, impressive advancements, due to limited, limited exposure</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite impressive advancements in various multimodal tasks, vision-language models (VLMs) still struggle with compositional understanding due to limited exposure to training samples that contain subtle variations within paired examples. With advances in multimodal generative models, a natural solution is to generate synthetic samples with subtle variations for training VLMs. However, generating and training on synthetic samples with subtle variations presents two challenges: difficulty in accurately creating precise variations and inconsistency in cross-modal alignment quality. To address these challenges, we propose SVD-GT (Subtle Variation Data Generation and Training), which integrates image feature injection into a text-to-image generative model to enhance the quality of synthetic variations and employs an adaptive margin loss to differentiate samples using adaptive margins, which help filter out potentially incorrect synthetic samples and focus the learning on informative hard samples. Evaluations on four compositional understanding benchmarks demonstrate that SVD-GT significantly improves the compositionality of VLMs, boosting the average accuracy of CLIP by over 8% across all benchmarks and outperforming state-of-the-art methods by 2% on three benchmarks.</p>
  </details>
</details>
<details>
  <summary>87. <b>【2503.01164】Med-LEGO: Editing and Adapting toward Generalist Medical Image Diagnosis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01164">https://arxiv.org/abs/2503.01164</a></p>
  <p><b>作者</b>：Yitao Zhu,Yuan Yin,Jiaming Li,Mengjie Xu,Zihao Zhao,Honglin Xiong,Sheng Wang,Qian Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：visual foundation models, computer-aided diagnosis, adoption of visual, common practice, practice in computer-aided</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The adoption of visual foundation models has become a common practice in computer-aided diagnosis (CAD). While these foundation models provide a viable solution for creating generalist medical AI, privacy concerns make it difficult to pre-train or continuously update such models across multiple domains and datasets, leading many studies to focus on specialist models. To address this challenge, we propose Med-LEGO, a training-free framework that enables the seamless integration or updating of a generalist CAD model by combining multiple specialist models, similar to assembling LEGO bricks. Med-LEGO enhances LoRA (low-rank adaptation) by incorporating singular value decomposition (SVD) to efficiently capture the domain expertise of each specialist model with minimal additional parameters. By combining these adapted weights through simple operations, Med-LEGO allows for the easy integration or modification of specific diagnostic capabilities without the need for original data or retraining. Finally, the combined model can be further adapted to new diagnostic tasks, making it a versatile generalist model. Our extensive experiments demonstrate that Med-LEGO outperforms existing methods in both cross-domain and in-domain medical tasks while using only 0.18% of full model parameters. These merged models show better convergence and generalization to new tasks, providing an effective path toward generalist medical AI.</p>
  </details>
</details>
<details>
  <summary>88. <b>【2503.01161】Split Gibbs Discrete Diffusion Posterior Sampling</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01161">https://arxiv.org/abs/2503.01161</a></p>
  <p><b>作者</b>：Wenda Chu,Yang Song,Yisong Yue</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：discrete diffusion models, diffusion models, discrete diffusion, posterior sampling, diffusion models remain</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We study the problem of posterior sampling in discrete-state spaces using discrete diffusion models. While posterior sampling methods for continuous diffusion models have achieved remarkable progress, analogous methods for discrete diffusion models remain challenging. In this work, we introduce a principled plug-and-play discrete diffusion posterior sampling algorithm based on split Gibbs sampling, which we call SG-DPS. Our algorithm enables reward-guided generation and solving inverse problems in discrete-state spaces. We demonstrate that SG-DPS converges to the true posterior distribution on synthetic benchmarks, and enjoys state-of-the-art posterior sampling performance on a range of benchmarks for discrete data, achieving up to 2x improved performance compared to existing baselines.</p>
  </details>
</details>
<details>
  <summary>89. <b>【2503.01158】EasyCraft: A Robust and Efficient Framework for Automatic Avatar Crafting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01158">https://arxiv.org/abs/2503.01158</a></p>
  <p><b>作者</b>：Suzhen Wang,Weijie Chen,Wei Zhang,Minda Zhao,Lincheng Li,Rongsheng Zhang,Zhipeng Hu,Xin Yu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：enhancing player engagement, enhancing player, player engagement, Character customization, crafting</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Character customization, or 'face crafting,' is a vital feature in role-playing games (RPGs), enhancing player engagement by enabling the creation of personalized avatars. Existing automated methods often struggle with generalizability across diverse game engines due to their reliance on the intermediate constraints of specific image domain and typically support only one type of input, either text or image. To overcome these challenges, we introduce EasyCraft, an innovative end-to-end feedforward framework that automates character crafting by uniquely supporting both text and image inputs. Our approach employs a translator capable of converting facial images of any style into crafting parameters. We first establish a unified feature distribution in the translator's image encoder through self-supervised learning on a large-scale dataset, enabling photos of any style to be embedded into a unified feature representation. Subsequently, we map this unified feature distribution to crafting parameters specific to a game engine, a process that can be easily adapted to most game engines and thus enhances EasyCraft's generalizability. By integrating text-to-image techniques with our translator, EasyCraft also facilitates precise, text-based character crafting. EasyCraft's ability to integrate diverse inputs significantly enhances the versatility and accuracy of avatar creation. Extensive experiments on two RPG games demonstrate the effectiveness of our method, achieving state-of-the-art results and facilitating adaptability across various avatar engines.</p>
  </details>
</details>
<details>
  <summary>90. <b>【2503.01144】One-shot In-context Part Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01144">https://arxiv.org/abs/2503.01144</a></p>
  <p><b>作者</b>：Zhenqi Dai,Ting Liu,Xingxing Zhang,Yunchao Wei,Yanning Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：leveraging visual foundation, visual foundation models, Part Segmentation, One-shot In-context Part, one-shot part segmentation</p>
  <p><b>备注</b>： 10 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we present the One-shot In-context Part Segmentation (OIParts) framework, designed to tackle the challenges of part segmentation by leveraging visual foundation models (VFMs). Existing training-based one-shot part segmentation methods that utilize VFMs encounter difficulties when faced with scenarios where the one-shot image and test image exhibit significant variance in appearance and perspective, or when the object in the test image is partially visible. We argue that training on the one-shot example often leads to overfitting, thereby compromising the model's generalization capability. Our framework offers a novel approach to part segmentation that is training-free, flexible, and data-efficient, requiring only a single in-context example for precise segmentation with superior generalization ability. By thoroughly exploring the complementary strengths of VFMs, specifically DINOv2 and Stable Diffusion, we introduce an adaptive channel selection approach by minimizing the intra-class distance for better exploiting these two features, thereby enhancing the discriminatory power of the extracted features for the fine-grained parts. We have achieved remarkable segmentation performance across diverse object categories. The OIParts framework not only eliminates the need for extensive labeled data but also demonstrates superior generalization ability. Through comprehensive experimentation on three benchmark datasets, we have demonstrated the superiority of our proposed method over existing part segmentation approaches in one-shot settings.</p>
  </details>
</details>
<details>
  <summary>91. <b>【2503.01136】Prior-guided Hierarchical Harmonization Network for Efficient Image Dehazing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01136">https://arxiv.org/abs/2503.01136</a></p>
  <p><b>作者</b>：Xiongfei Su,Siyuan Li,Yuning Cui,Miao Cao,Yulun Zhang,Zheng Chen,Zongliang Wu,Zedong Wang,Yuanlong Zhang,Xin Yuan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Bright Channel Prior, Dark Channel Prior, sharpness and textures, involves the enhancement, enhancement of degraded</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image dehazing is a crucial task that involves the enhancement of degraded images to recover their sharpness and textures. While vision Transformers have exhibited impressive results in diverse dehazing tasks, their quadratic complexity and lack of dehazing priors pose significant drawbacks for real-world applications.
In this paper, guided by triple priors, Bright Channel Prior (BCP), Dark Channel Prior (DCP), and Histogram Equalization (HE), we propose a \textit{P}rior-\textit{g}uided Hierarchical \textit{H}armonization Network (PGH$^2$Net) for image dehazing. PGH$^2$Net is built upon the UNet-like architecture with an efficient encoder and decoder, consisting of two module types: (1) Prior aggregation module that injects B/DCP and selects diverse contexts with gating attention. (2) Feature harmonization modules that subtract low-frequency components from spatial and channel aspects and learn more informative feature distributions to equalize the feature maps.
</p><p>Subjects:</p>
<p>Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)</p>
<p>Cite as:<br>
arXiv:2503.01136 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]</p>
<p>(or<br>
arXiv:2503.01136v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.01136">https://doi.org/10.48550/arXiv.2503.01136</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>92. <b>【2503.01130】AirRoom: Objects Matter in Room Reidentification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01130">https://arxiv.org/abs/2503.01130</a></p>
  <p><b>作者</b>：Runmao Yao,Yi Du,Zhuoqun Chen,Haoze Zheng,Chen Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Room reidentification, augmented reality, homecare robotics, challenging yet essential, essential task</p>
  <p><b>备注</b>： Paper accepted at CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Room reidentification (ReID) is a challenging yet essential task with numerous applications in fields such as augmented reality (AR) and homecare robotics. Existing visual place recognition (VPR) methods, which typically rely on global descriptors or aggregate local features, often struggle in cluttered indoor environments densely populated with man-made objects. These methods tend to overlook the crucial role of object-oriented information. To address this, we propose AirRoom, an object-aware pipeline that integrates multi-level object-oriented information-from global context to object patches, object segmentation, and keypoints-utilizing a coarse-to-fine retrieval approach. Extensive experiments on four newly constructed datasets-MPReID, HMReID, GibsonReID, and ReplicaReID-demonstrate that AirRoom outperforms state-of-the-art (SOTA) models across nearly all evaluation metrics, with improvements ranging from 6% to 80%. Moreover, AirRoom exhibits significant flexibility, allowing various modules within the pipeline to be substituted with different alternatives without compromising overall performance. It also shows robust and consistent performance under diverse viewpoint variations.</p>
  </details>
</details>
<details>
  <summary>93. <b>【2503.01124】ViKANformer: Embedding Kolmogorov Arnold Networks in Vision Transformers for Pattern-Based Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01124">https://arxiv.org/abs/2503.01124</a></p>
  <p><b>作者</b>：Shreyas S,Akshath M</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：significantly advanced image, advanced image classification, patch embeddings, Vision Transformers, significantly advanced</p>
  <p><b>备注</b>： This paper represents ongoing research and may be subject to revisions, refinements, and additional experiments in future updates</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Vision Transformers (ViTs) have significantly advanced image classification by applying self-attention on patch embeddings. However, the standard MLP blocks in each Transformer layer may not capture complex nonlinear dependencies optimally. In this paper, we propose ViKANformer, a Vision Transformer where we replace the MLP sub-layers with Kolmogorov-Arnold Network (KAN) expansions, including Vanilla KAN, Efficient-KAN, Fast-KAN, SineKAN, and FourierKAN, while also examining a Flash Attention variant. By leveraging the Kolmogorov-Arnold theorem, which guarantees that multivariate continuous functions can be expressed via sums of univariate continuous functions, we aim to boost representational power. Experimental results on MNIST demonstrate that SineKAN, Fast-KAN, and a well-tuned Vanilla KAN can achieve over 97% accuracy, albeit with increased training overhead. This trade-off highlights that KAN expansions may be beneficial if computational cost is acceptable. We detail the expansions, present training/test accuracy and F1/ROC metrics, and provide pseudocode and hyperparameters for reproducibility. Finally, we compare ViKANformer to a simple MLP and a small CNN baseline on MNIST, illustrating the efficiency of Transformer-based methods even on a small-scale dataset.</p>
  </details>
</details>
<details>
  <summary>94. <b>【2503.01122】ACCORD: Alleviating Concept Coupling through Dependence Regularization for Text-to-Image Diffusion Personalization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01122">https://arxiv.org/abs/2503.01122</a></p>
  <p><b>作者</b>：Shizhan Liu,Hao Zheng,Hang Yu,Jianguo Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：reference images, ability to customize, reference images leads, Image personalization, garnered attention</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image personalization has garnered attention for its ability to customize Text-to-Image generation using only a few reference images. However, a key challenge in image personalization is the issue of conceptual coupling, where the limited number of reference images leads the model to form unwanted associations between the personalization target and other concepts. Current methods attempt to tackle this issue indirectly, leading to a suboptimal balance between text control and personalization fidelity. In this paper, we take a direct approach to the concept coupling problem through statistical analysis, revealing that it stems from two distinct sources of dependence discrepancies. We therefore propose two complementary plug-and-play loss functions: Denoising Decouple Loss and Prior Decouple loss, each designed to minimize one type of dependence discrepancy. Extensive experiments demonstrate that our approach achieves a superior trade-off between text control and personalization fidelity.</p>
  </details>
</details>
<details>
  <summary>95. <b>【2503.01115】WeGen: A Unified Model for Interactive Multimodal Generation as We Chat</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01115">https://arxiv.org/abs/2503.01115</a></p>
  <p><b>作者</b>：Zhipeng Huang,Shaobin Zhuang,Canmiao Fu,Binxin Yang,Ying Zhang,Chong Sun,Zhizheng Zhang,Yali Wang,Chen Li,Zheng-Jun Zha</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Existing multimodal generative, generate imaginative outputs, Existing multimodal, generative models fall, models fall short</p>
  <p><b>备注</b>： CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing multimodal generative models fall short as qualified design copilots, as they often struggle to generate imaginative outputs once instructions are less detailed or lack the ability to maintain consistency with the provided references. In this work, we introduce WeGen, a model that unifies multimodal generation and understanding, and promotes their interplay in iterative generation. It can generate diverse results with high creativity for less detailed instructions. And it can progressively refine prior generation results or integrating specific contents from references following the instructions in its chat with users. During this process, it is capable of preserving consistency in the parts that the user is already satisfied with. To this end, we curate a large-scale dataset, extracted from Internet videos, containing rich object dynamics and auto-labeled dynamics descriptions by advanced foundation models to date. These two information are interleaved into a single sequence to enable WeGen to learn consistency-aware generation where the specified dynamics are generated while the consistency of unspecified content is preserved aligned with instructions. Besides, we introduce a prompt self-rewriting mechanism to enhance generation diversity. Extensive experiments demonstrate the effectiveness of unifying multimodal understanding and generation in WeGen and show it achieves state-of-the-art performance across various visual generation benchmarks. These also demonstrate the potential of WeGen as a user-friendly design copilot as desired. The code and models will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>96. <b>【2503.01114】Semi-Supervised 360 Layout Estimation with Panoramic Collaborative Perturbations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01114">https://arxiv.org/abs/2503.01114</a></p>
  <p><b>作者</b>：Junsong Zhang,Chunyu Lin,Zhijie Shen,Lang Nie,Kang Liao,Yao Zhao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：methods heavily relies, supervised layout estimation, heavily relies, existing supervised layout, estimation methods heavily</p>
  <p><b>备注</b>： 9 pages,4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The performance of existing supervised layout estimation methods heavily relies on the quality of data annotations. However, obtaining large-scale and high-quality datasets remains a laborious and time-consuming challenge. To solve this problem, semi-supervised approaches are introduced to relieve the demand for expensive data annotations by encouraging the consistent results of unlabeled data with different perturbations. However, existing solutions merely employ vanilla perturbations, ignoring the characteristics of panoramic layout estimation. In contrast, we propose a novel semi-supervised method named SemiLayout360, which incorporates the priors of the panoramic layout and distortion through collaborative perturbations. Specifically, we leverage the panoramic layout prior to enhance the model's focus on potential layout boundaries. Meanwhile, we introduce the panoramic distortion prior to strengthen distortion awareness. Furthermore, to prevent intense perturbations from hindering model convergence and ensure the effectiveness of prior-based perturbations, we divide and reorganize them as panoramic collaborative perturbations. Our experimental results on three mainstream benchmarks demonstrate that the proposed method offers significant advantages over existing state-of-the-art (SoTA) solutions.</p>
  </details>
</details>
<details>
  <summary>97. <b>【2503.01113】SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack Segmentation in Structures</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01113">https://arxiv.org/abs/2503.01113</a></p>
  <p><b>作者</b>：Hui Liu,Chen Jia,Fan Shi,Xu Cheng,Shengyong Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Vision Mamba Network, Pixel-level segmentation, scenarios remains, remains a considerable, Gated Bottleneck Convolution</p>
  <p><b>备注</b>： This paper has been accepted by CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Pixel-level segmentation of structural cracks across various scenarios remains a considerable challenge. Current methods encounter challenges in effectively modeling crack morphology and texture, facing challenges in balancing segmentation quality with low computational resource usage. To overcome these limitations, we propose a lightweight Structure-Aware Vision Mamba Network (SCSegamba), capable of generating high-quality pixel-level segmentation maps by leveraging both the morphological information and texture cues of crack pixels with minimal computational cost. Specifically, we developed a Structure-Aware Visual State Space module (SAVSS), which incorporates a lightweight Gated Bottleneck Convolution (GBC) and a Structure-Aware Scanning Strategy (SASS). The key insight of GBC lies in its effectiveness in modeling the morphological information of cracks, while the SASS enhances the perception of crack topology and texture by strengthening the continuity of semantic information between crack pixels. Experiments on crack benchmark datasets demonstrate that our method outperforms other state-of-the-art (SOTA) methods, achieving the highest performance with only 2.8M parameters. On the multi-scenario dataset, our method reached 0.8390 in F1 score and 0.8479 in mIoU. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>98. <b>【2503.01109】FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with Sparse and Dense Map Fusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01109">https://arxiv.org/abs/2503.01109</a></p>
  <p><b>作者</b>：Yansong Xu,Junlin Li,Wei Zhang,Siyu Chen,Shengyong Zhang,Yuquan Leng,Weijia Zhou</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)</p>
  <p><b>关键词</b>：advanced simultaneous localization, enabling real-time positioning, technology by enabling, splatting has advanced, advanced simultaneous</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:3D gaussian splatting has advanced simultaneous localization and mapping (SLAM) technology by enabling real-time positioning and the construction of high-fidelity maps. However, the uncertainty in gaussian position and initialization parameters introduces challenges, often requiring extensive iterative convergence and resulting in redundant or insufficient gaussian representations. To address this, we introduce a novel adaptive densification method based on Fourier frequency domain analysis to establish gaussian priors for rapid convergence. Additionally, we propose constructing independent and unified sparse and dense maps, where a sparse map supports efficient tracking via Generalized Iterative Closest Point (GICP) and a dense map creates high-fidelity visual representations. This is the first SLAM system leveraging frequency domain analysis to achieve high-quality gaussian mapping in real-time. Experimental results demonstrate an average frame rate of 36 FPS on Replica and TUM RGB-D datasets, achieving competitive accuracy in both localization and mapping.</p>
  </details>
</details>
<details>
  <summary>99. <b>【2503.01107】VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01107">https://arxiv.org/abs/2503.01107</a></p>
  <p><b>作者</b>：Juil Koo,Paul Guerrero,Chun-Hao Paul Huang,Duygu Ceylan,Minhyuk Sung</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：incomplete information, priors to perform, editing, edit object composition, object</p>
  <p><b>备注</b>： Project page: [this https URL](https://videohandles.github.io) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Generative methods for image and video editing use generative models as priors to perform edits despite incomplete information, such as changing the composition of 3D objects shown in a single image. Recent methods have shown promising composition editing results in the image setting, but in the video setting, editing methods have focused on editing object's appearance and motion, or camera motion, and as a result, methods to edit object composition in videos are still missing. We propose \name as a method for editing 3D object compositions in videos of static scenes with camera motion. Our approach allows editing the 3D position of a 3D object across all frames of a video in a temporally consistent manner. This is achieved by lifting intermediate features of a generative model to a 3D reconstruction that is shared between all frames, editing the reconstruction, and projecting the features on the edited reconstruction back to each frame. To the best of our knowledge, this is the first generative approach to edit object compositions in videos. Our approach is simple and training-free, while outperforming state-of-the-art image editing baselines.</p>
  </details>
</details>
<details>
  <summary>100. <b>【2503.01103】Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01103">https://arxiv.org/abs/2503.01103</a></p>
  <p><b>作者</b>：Kaiwen Zheng,Yongxin Chen,Huayu Chen,Guande He,Ming-Yu Liu,Jun Zhu,Qinsheng Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：achieved remarkable fidelity, objective inherently suffers, maximum likelihood estimation, Direct Discriminative Optimization, Direct Preference Optimization</p>
  <p><b>备注</b>： Project Page: [this https URL](https://research.nvidia.com/labs/dir/ddo/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256$\times$256.</p>
  </details>
</details>
<details>
  <summary>101. <b>【2503.01100】Fence Theorem: Preprocessing is Dual-Objective Semantic Structure Isolator in 3D Anomaly Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01100">https://arxiv.org/abs/2503.01100</a></p>
  <p><b>作者</b>：Hanzhe Liang,Jie Zhou,Xuanxin Chen,Jinbao Wang,Can Gao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：unified theoretical foundation, prominent but difficult, difficult due, due to lacking, lacking a unified</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:3D anomaly detection (AD) is prominent but difficult due to lacking a unified theoretical foundation for preprocessing design. We establish the Fence Theorem, formalizing preprocessing as a dual-objective semantic isolator: (1) mitigating cross-semantic interference to the greatest extent feasible and (2) confining anomaly judgments to aligned semantic spaces wherever viable, thereby establishing intra-semantic comparability. Any preprocessing approach achieves this goal through a two-stage process of Emantic-Division and Spatial-Constraints stage. Through systematic deconstruction, we theoretically and experimentally subsume existing preprocessing methods under this theorem via tripartite evidence: qualitative analyses, quantitative studies, and mathematical proofs. Guided by the Fence Theorem, we implement Patch3D, consisting of Patch-Cutting and Patch-Matching modules, to segment semantic spaces and consolidate similar ones while independently modeling normal features within each space. Experiments on Anomaly-ShapeNet and Real3D-AD with different settings demonstrate that progressively finer-grained semantic alignment in preprocessing directly enhances point-level AD accuracy, providing inverse validation of the theorem's causal logic.</p>
  </details>
</details>
<details>
  <summary>102. <b>【2503.01092】One-Shot Affordance Grounding of Deformable Objects in Egocentric Organizing Scenes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01092">https://arxiv.org/abs/2503.01092</a></p>
  <p><b>作者</b>：Wanjun Jia,Fan Yang,Mengfei Duan,Xianchi Chen,Yinxi Wang,Yiming Jiang,Wenrui Chen,Kailun Yang,Zhiyong Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：robotics presents significant, Deformable object manipulation, presents significant challenges, significant challenges due, Deformable Objects</p>
  <p><b>备注</b>： Source code and benchmark dataset will be publicly available at [this https URL](https://github.com/Dikay1/OS-AGDO) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deformable object manipulation in robotics presents significant challenges due to uncertainties in component properties, diverse configurations, visual interference, and ambiguous prompts. These factors complicate both perception and control tasks. To address these challenges, we propose a novel method for One-Shot Affordance Grounding of Deformable Objects (OS-AGDO) in egocentric organizing scenes, enabling robots to recognize previously unseen deformable objects with varying colors and shapes using minimal samples. Specifically, we first introduce the Deformable Object Semantic Enhancement Module (DefoSEM), which enhances hierarchical understanding of the internal structure and improves the ability to accurately identify local features, even under conditions of weak component information. Next, we propose the ORB-Enhanced Keypoint Fusion Module (OEKFM), which optimizes feature extraction of key components by leveraging geometric constraints and improves adaptability to diversity and visual interference. Additionally, we propose an instance-conditional prompt based on image data and task context, effectively mitigates the issue of region ambiguity caused by prompt words. To validate these methods, we construct a diverse real-world dataset, AGDDO15, which includes 15 common types of deformable objects and their associated organizational actions. Experimental results demonstrate that our approach significantly outperforms state-of-the-art methods, achieving improvements of 6.2%, 3.2%, and 2.9% in KLD, SIM, and NSS metrics, respectively, while exhibiting high generalization performance. Source code and benchmark dataset will be publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>103. <b>【2503.01087】Rashomon Sets for Prototypical-Part Networks: Editing Interpretable Models in Real-Time</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01087">https://arxiv.org/abs/2503.01087</a></p>
  <p><b>作者</b>：Jon Donnelly,Zhicheng Guo,Alina Jade Barnett,Hayden McTavish,Chaofan Chen,Cynthia Rudin</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Interpretability is critical, Interpretability, machine learning, ProtoPNets, model reasoning</p>
  <p><b>备注</b>： Accepted for publication in CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Interpretability is critical for machine learning models in high-stakes settings because it allows users to verify the model's reasoning. In computer vision, prototypical part models (ProtoPNets) have become the dominant model type to meet this need. Users can easily identify flaws in ProtoPNets, but fixing problems in a ProtoPNet requires slow, difficult retraining that is not guaranteed to resolve the issue. This problem is called the "interaction bottleneck." We solve the interaction bottleneck for ProtoPNets by simultaneously finding many equally good ProtoPNets (i.e., a draw from a "Rashomon set"). We show that our framework - called Proto-RSet - quickly produces many accurate, diverse ProtoPNets, allowing users to correct problems in real time while maintaining performance guarantees with respect to the training set. We demonstrate the utility of this method in two settings: 1) removing synthetic bias introduced to a bird identification model and 2) debugging a skin cancer identification model. This tool empowers non-machine-learning experts, such as clinicians or domain experts, to quickly refine and correct machine learning models without repeated retraining by machine learning experts.</p>
  </details>
</details>
<details>
  <summary>104. <b>【2503.01085】Identity documents recognition and detection using semantic segmentation with convolutional neural network</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01085">https://arxiv.org/abs/2503.01085</a></p>
  <p><b>作者</b>：Mykola Kozlenko,Volodymyr Sendetskyi,Oleksiy Simkiv,Nazar Savchenko,Andy Bosyi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Object recognition, Identity documents recognition, standard solutions, well-studied problems, developed set</p>
  <p><b>备注</b>： 9 pages, 8 figures. This paper was originally published in 2021 Workshop on Cybersecurity Providing in Information and Telecommunication Systems, in CEUR Workshop Proceedings, vol. 2923, available: [this https URL](https://ceur-ws.org/Vol-2923/paper25.pdf) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Object recognition and detection are well-studied problems with a developed set of almost standard solutions. Identity documents recognition, classification, detection, and localization are the tasks required in a number of applications, particularly, in physical access control security systems at critical infrastructure premises. In this paper, we propose the new original architecture of a model based on an artificial convolutional neural network and semantic segmentation approach for the recognition and detection of identity documents in images. The challenge with the processing of such images is the limited computational performance and the limited amount of memory when such an application is running on industrial oneboard microcomputer hardware. The aim of this research is to prove the feasibility of the proposed technique and to obtain quality metrics. The methodology of the research is to evaluate the deep learning detection model trained on the mobile identity document video dataset. The dataset contains five hundred video clips for fifty different identity document types. The numerical results from simulations are used to evaluate the quality metrics. We present the results as accuracy versus threshold of the intersection over union value. The paper reports an accuracy above 0.75 for the intersection over union (IoU) threshold value of 0.8. Besides, we assessed the size of the model and proved the feasibility of running the model on an industrial one-board microcomputer or smartphone hardware.</p>
  </details>
</details>
<details>
  <summary>105. <b>【2503.01074】OceanSim: A GPU-Accelerated Underwater Robot Perception Simulation Framework</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01074">https://arxiv.org/abs/2503.01074</a></p>
  <p><b>作者</b>：Jingyu Song,Haoyu Ma,Onur Bagoren,Advaith V. Sethuraman,Yiting Zhang,Katherine A. Skinner</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：underwater perception solutions, simulators offer support, robust underwater perception, building robust underwater, perception solutions</p>
  <p><b>备注</b>： 8 pages, 6 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Underwater simulators offer support for building robust underwater perception solutions. Significant work has recently been done to develop new simulators and to advance the performance of existing underwater simulators. Still, there remains room for improvement on physics-based underwater sensor modeling and rendering efficiency. In this paper, we propose OceanSim, a high-fidelity GPU-accelerated underwater simulator to address this research gap. We propose advanced physics-based rendering techniques to reduce the sim-to-real gap for underwater image simulation. We develop OceanSim to fully leverage the computing advantages of GPUs and achieve real-time imaging sonar rendering and fast synthetic data generation. We evaluate the capabilities and realism of OceanSim using real-world data to provide qualitative and quantitative results. The project page for OceanSim is this https URL.</p>
  </details>
</details>
<details>
  <summary>106. <b>【2503.01064】Scientific Reasoning: Assessment of Multimodal Generative LLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01064">https://arxiv.org/abs/2503.01064</a></p>
  <p><b>作者</b>：Florian Dreyer,Ekaterina Kolos,Daria Matiash</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Large language models, Large language, complex tasks, scientific domain, answer questions</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large language models (LLMs) can answer questions and reason about complex tasks, also from the scientific domain. We assess several multimodal LLMs (MLLMs) on ScienceQA and find that Gemini models show the highest accuracy with little context, and the highest textual similarity to human explanations with richer context. Adapter-tuning of smaller MLLMs did not lead to any reliable performance. Training from Gemini outputs consistently underperformed training from the original data.</p>
  </details>
</details>
<details>
  <summary>107. <b>【2503.01037】A Comparison of Object Detection and Phrase Grounding Models in Chest X-ray Abnormality Localization using Eye-tracking Data</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01037">https://arxiv.org/abs/2503.01037</a></p>
  <p><b>作者</b>：Elham Ghelichkhan,Tolga Tasdizen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：global health issues, dangerous global health, Chest diseases rank, health issues, diseases rank</p>
  <p><b>备注</b>： Accepted in 2025 IEEE International Symposium on Biomedical Imaging (ISBI 2025)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Chest diseases rank among the most prevalent and dangerous global health issues. Object detection and phrase grounding deep learning models interpret complex radiology data to assist healthcare professionals in diagnosis. Object detection locates abnormalities for classes, while phrase grounding locates abnormalities for textual descriptions. This paper investigates how text enhances abnormality localization in chest X-rays by comparing the performance and explainability of these two tasks. To establish an explainability baseline, we proposed an automatic pipeline to generate image regions for report sentences using radiologists' eye-tracking data. The better performance - mIoU = 0.36 vs. 0.20 - and explainability - Containment ratio 0.48 vs. 0.26 - of the phrase grounding model infers the effectiveness of text in enhancing chest X-ray abnormality localization.</p>
  </details>
</details>
<details>
  <summary>108. <b>【2503.01020】Delving into Out-of-Distribution Detection with Medical Vision-Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01020">https://arxiv.org/abs/2503.01020</a></p>
  <p><b>作者</b>：Lie Ju,Sijin Zhou,Yukun Zhou,Huimin Lu,Zhuoting Zhu,Pearse A. Keane,Zongyuan Ge</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：image classification tasks, zero-shot generalization capabilities, strong zero-shot generalization, Recent advances, demonstrate impressive performance</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advances in medical vision-language models (VLMs) demonstrate impressive performance in image classification tasks, driven by their strong zero-shot generalization capabilities. However, given the high variability and complexity inherent in medical imaging data, the ability of these models to detect out-of-distribution (OOD) data in this domain remains underexplored. In this work, we conduct the first systematic investigation into the OOD detection potential of medical VLMs. We evaluate state-of-the-art VLM-based OOD detection methods across a diverse set of medical VLMs, including both general and domain-specific purposes. To accurately reflect real-world challenges, we introduce a cross-modality evaluation pipeline for benchmarking full-spectrum OOD detection, rigorously assessing model robustness against both semantic shifts and covariate shifts. Furthermore, we propose a novel hierarchical prompt-based method that significantly enhances OOD detection performance. Extensive experiments are conducted to validate the effectiveness of our approach. The codes are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>109. <b>【2503.01019】MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01019">https://arxiv.org/abs/2503.01019</a></p>
  <p><b>作者</b>：Ziyang Zhang,Yang Yu,Yucheng Chen,Xulei Yang,Si Yong Yeo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：current approaches predominantly, approaches predominantly emphasize, predominantly emphasize feature, emphasize feature extraction, Vision-Language Pre-training</p>
  <p><b>备注</b>： To be pubilshed in CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content. This gap hinders the model's ability to synthesize coherent and novel visual representations from textual prompts, thereby reducing the effectiveness of multi-modal learning. In this work, we propose MedUnifier, a unified VLP framework tailored for medical data. MedUnifier seamlessly integrates text-grounded image generation capabilities with multi-modal learning strategies, including image-text contrastive alignment, image-text matching and image-grounded text generation. Unlike traditional methods that reply on continuous visual representations, our approach employs visual vector quantization, which not only facilitates a more cohesive learning strategy for cross-modal understanding but also enhances multi-modal generation quality by effectively leveraging discrete representations. Our framework's effectiveness is evidenced by the experiments on established benchmarks, including uni-modal tasks (supervised fine-tuning), cross-modal tasks (image-text retrieval and zero-shot image classification), and multi-modal tasks (medical report generation, image synthesis), where it achieves state-of-the-art performance across various tasks. MedUnifier also offers a highly adaptable tool for a wide range of language and vision tasks in healthcare, marking advancement toward the development of a generalizable AI model for medical applications.</p>
  </details>
</details>
<details>
  <summary>110. <b>【2503.01016】Generative Motion Infilling From Imprecisely Timed Keyframes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01016">https://arxiv.org/abs/2503.01016</a></p>
  <p><b>作者</b>：Purvi Goel,Haotian Zhang,C. Karen Liu,Kayvon Fatahalian</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：kinematic motion specification, standard representation, representation for kinematic, Keyframes, motion</p>
  <p><b>备注</b>： 10 pages, Eurographics 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Keyframes are a standard representation for kinematic motion specification. Recent learned motion-inbetweening methods use keyframes as a way to control generative motion models, and are trained to generate life-like motion that matches the exact poses and timings of input keyframes. However, the quality of generated motion may degrade if the timing of these constraints is not perfectly consistent with the desired motion. Unfortunately, correctly specifying keyframe timings is a tedious and challenging task in practice. Our goal is to create a system that synthesizes high-quality motion from keyframes, even if keyframes are imprecisely timed. We present a method that allows constraints to be retimed as part of the generation process. Specifically, we introduce a novel model architecture that explicitly outputs a time-warping function to correct mistimed keyframes, and spatial residuals that add pose details. We demonstrate how our method can automatically turn approximately timed keyframe constraints into diverse, realistic motions with plausible timing and detailed submovements.</p>
  </details>
</details>
<details>
  <summary>111. <b>【2503.00986】Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00986">https://arxiv.org/abs/2503.00986</a></p>
  <p><b>作者</b>：Baoqi Pei,Yifei Huang,Jilan Xu,Guo Chen,Yuping He,Lijin Yang,Yali Wang,Weidi Xie,Yu Qiao,Fei Wu,Limin Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：egocentric video understanding, video representation learning, video representation, role by nature, hands and objects</p>
  <p><b>备注</b>： Accepted as ICLR 2025 conference paper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature. However, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate dynamics between hands and objects. In this work, we aim to integrate the modeling of fine-grained hand-object dynamics into the video representation learning process. Since no suitable data is available, we introduce HOD, a novel pipeline employing a hand-object detector and a large language model to generate high-quality narrations with detailed descriptions of hand-object dynamics. To learn these fine-grained dynamics, we propose EgoVideo, a model with a new lightweight motion adapter to capture fine-grained hand-object motion information. Through our co-training strategy, EgoVideo effectively and efficiently leverages the fine-grained hand-object dynamics in the HOD data. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple egocentric downstream tasks, including improvements of 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and 16.3% in EGTEA classification in zero-shot settings. Furthermore, our model exhibits robust generalization capabilities in hand-object interaction and robot manipulation tasks. Code and data are available at this https URL.</p>
  </details>
</details>
<details>
  <summary>112. <b>【2503.00972】Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00972">https://arxiv.org/abs/2503.00972</a></p>
  <p><b>作者</b>：Wanwen Chen,Carson Studders,Jamie J.Y. Kwon,Emily H.T. Pang,Eitan Prisman,Septimiu E. Salcudean</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Point cloud registration, Point cloud, cloud registration methods, cloud registration, computer-aided interventions</p>
  <p><b>备注</b>： 10 pages, 3 figures, submitted to MICCAI 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Point cloud registration is important in computer-aided interventions (CAI). While learning-based point cloud registration methods have been developed, their clinical application is hampered by issues of generalizability and explainability. Therefore, classical point cloud registration methods, such as Iterative Closest Point (ICP), are still widely applied in CAI. ICP methods fail to consider that: (1) the points have well-defined semantic meaning, in that each point can be related to a specific anatomical label; (2) the deformation needs to follow biomechanical energy constraints. In this paper, we present a novel semantic ICP (sem-ICP) method that handles multiple point labels and uses linear elastic energy regularization. We use semantic labels to improve the robustness of the closest point matching and propose a new point cloud deformation representation to apply explicit biomechanical energy regularization. Our experiments on the Learn2reg abdominal MR-CT registration dataset and a trans-oral robotic surgery ultrasound-CT registration dataset show that our method improves the Hausdorff distance compared with other state-of-the-art ICP-based registration methods. We also perform a sensitivity study to show that our rigid initialization achieves better convergence with different initializations and visible ratios.</p>
  </details>
</details>
<details>
  <summary>113. <b>【2503.00962】Using Synthetic Images to Augment Small Medical Image Datasets</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00962">https://arxiv.org/abs/2503.00962</a></p>
  <p><b>作者</b>：Minh H. Vu,Lorenzo Tronchin,Tufve Nyholm,Tommy Löfstedt</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Recent years, medical imaging datasets, deep learning, medical imaging, years have witnessed</p>
  <p><b>备注</b>： 14 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent years have witnessed a growing academic and industrial interest in deep learning (DL) for medical imaging. To perform well, DL models require very large labeled datasets. However, most medical imaging datasets are small, with a limited number of annotated samples. The reason they are small is usually because delineating medical images is time-consuming and demanding for oncologists. There are various techniques that can be used to augment a dataset, for example, to apply affine transformations or elastic transformations to available images, or to add synthetic images generated by a Generative Adversarial Network (GAN). In this work, we have developed a novel conditional variant of a current GAN method, the StyleGAN2, to generate multi-modal high-resolution medical images with the purpose to augment small medical imaging datasets with these synthetic images. We use the synthetic and real images from six datasets to train models for the downstream task of semantic segmentation. The quality of the generated medical images and the effect of this augmentation on the segmentation performance were evaluated afterward. Finally, the results indicate that the downstream segmentation models did not benefit from the generated images. Further work and analyses are required to establish how this augmentation affects the segmentation performance.</p>
  </details>
</details>
<details>
  <summary>114. <b>【2503.00952】A Survey on Ordinal Regression: Applications, Advances and Prospects</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00952">https://arxiv.org/abs/2503.00952</a></p>
  <p><b>作者</b>：Jinhong Wang,Jintai Chen,Jian Liu,Dongqi Tang,Danny Z. Chen,Jian Wu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：classifying object instances, Ordinal regression, Ordinal regression refers, refers to classifying, classifying object</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Ordinal regression refers to classifying object instances into ordinal categories. Ordinal regression is crucial for applications in various areas like facial age estimation, image aesthetics assessment, and even cancer staging, due to its capability to utilize ordered information effectively. More importantly, it also enhances model interpretation by considering category order, aiding the understanding of data trends and causal relationships. Despite significant recent progress, challenges remain, and further investigation of ordinal regression techniques and applications is essential to guide future research. In this survey, we present a comprehensive examination of advances and applications of ordinal regression. By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different strategies and objectives: Continuous Space Discretization, Distribution Ordering Learning, and Ambiguous Instance Delving. This categorization enables a structured exploration of diverse insights in ordinal regression problems, providing a framework for a more comprehensive understanding and evaluation of this field and its related applications. To our best knowledge, this is the first systematic survey of ordinal regression, which lays a foundation for future research in this fundamental and generic domain.</p>
  </details>
</details>
<details>
  <summary>115. <b>【2503.00951】Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00951">https://arxiv.org/abs/2503.00951</a></p>
  <p><b>作者</b>：Xingzhuo Guo,Yu Zhang,Baixu Chen,Haoran Xu,Jianmin Wang,Mingsheng Long</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：generate realistic samples, progressively adding noise, powerful generative frameworks, Dynamical Diffusion, realistic samples</p>
  <p><b>备注</b>： ICLR 2025 Accepted</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies. Code is available at this repository: this https URL.</p>
  </details>
</details>
<details>
  <summary>116. <b>【2503.00948】Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00948">https://arxiv.org/abs/2503.00948</a></p>
  <p><b>作者</b>：Jie Tian,Xiaoye Qu,Zhenyi Lu,Wei Wei,Sichen Liu,Yu Cheng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：generation aims, aims to synthesize, motion, motion controllability, video clip</p>
  <p><b>备注</b>： Accepted by CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Image-to-Video (I2V) generation aims to synthesize a video clip according to a given image and condition (e.g., text). The key challenge of this task lies in simultaneously generating natural motions while preserving the original appearance of the images. However, current I2V diffusion models (I2V-DMs) often produce videos with limited motion degrees or exhibit uncontrollable motion that conflicts with the textual condition. To address these limitations, we propose a novel Extrapolating and Decoupling framework, which introduces model merging techniques to the I2V domain for the first time. Specifically, our framework consists of three separate stages: (1) Starting with a base I2V-DM, we explicitly inject the textual condition into the temporal module using a lightweight, learnable adapter and fine-tune the integrated model to improve motion controllability. (2) We introduce a training-free extrapolation strategy to amplify the dynamic range of the motion, effectively reversing the fine-tuning process to enhance the motion degree significantly. (3) With the above two-stage models excelling in motion controllability and degree, we decouple the relevant parameters associated with each type of motion ability and inject them into the base I2V-DM. Since the I2V-DM handles different levels of motion controllability and dynamics at various denoising time steps, we adjust the motion-aware parameters accordingly over time. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of our framework over existing methods.</p>
  </details>
</details>
<details>
  <summary>117. <b>【2503.00938】From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00938">https://arxiv.org/abs/2503.00938</a></p>
  <p><b>作者</b>：Chao Yuan,Guiwei Zhang,Changxiao Ma,Tianyi Zhang,Guanglin Niu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Person re-identification, extract accurate identity, Feature Centralization, URL Feature Centralization, aims to extract</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Person re-identification (ReID) aims to extract accurate identity representation features. However, during feature extraction, individual samples are inevitably affected by noise (background, occlusions, and model limitations). Considering that features from the same identity follow a normal distribution around identity centers after training, we propose a Training-Free Feature Centralization ReID framework (Pose2ID) by aggregating the same identity features to reduce individual noise and enhance the stability of identity representation, which preserves the feature's original distribution for following strategies such as re-ranking. Specifically, to obtain samples of the same identity, we introduce two components:Identity-Guided Pedestrian Generation: by leveraging identity features to guide the generation process, we obtain high-quality images with diverse poses, ensuring identity consistency even in complex scenarios such as infrared, and this http URL Feature Centralization: it explores each sample's potential positive samples from its neighborhood. Experiments demonstrate that our generative model exhibits strong generalization capabilities and maintains high identity consistency. With the Feature Centralization framework, we achieve impressive performance even with an ImageNet pre-trained model without ReID training, reaching mAP/Rank-1 of 52.81/78.92 on Market1501. Moreover, our method sets new state-of-the-art results across standard, cross-modality, and occluded ReID tasks, showcasing strong adaptability.</p>
  </details>
</details>
<details>
  <summary>118. <b>【2503.00936】IteRPrimE: Zero-shot Referring Image Segmentation with Iterative Grad-CAM Refinement and Primary Word Emphasis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00936">https://arxiv.org/abs/2503.00936</a></p>
  <p><b>作者</b>：Yuji Wang,Jingchen Ni,Yong Liu,Chun Yuan,Yansong Tang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Referring Image Segmentation, labor-intensive annotation process, Zero-shot Referring Image, Image Segmentation, Primary word Emphasis</p>
  <p><b>备注</b>： AAAI 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Zero-shot Referring Image Segmentation (RIS) identifies the instance mask that best aligns with a specified referring expression without training and fine-tuning, significantly reducing the labor-intensive annotation process. Despite achieving commendable results, previous CLIP-based models have a critical drawback: the models exhibit a notable reduction in their capacity to discern relative spatial relationships of objects. This is because they generate all possible masks on an image and evaluate each masked region for similarity to the given expression, often resulting in decreased sensitivity to direct positional clues in text inputs. Moreover, most methods have weak abilities to manage relationships between primary words and their contexts, causing confusion and reduced accuracy in identifying the correct target region. To address these challenges, we propose IteRPrimE (Iterative Grad-CAM Refinement and Primary word Emphasis), which leverages a saliency heatmap through Grad-CAM from a Vision-Language Pre-trained (VLP) model for image-text matching. An iterative Grad-CAM refinement strategy is introduced to progressively enhance the model's focus on the target region and overcome positional insensitivity, creating a self-correcting effect. Additionally, we design the Primary Word Emphasis module to help the model handle complex semantic relations, enhancing its ability to attend to the intended object. Extensive experiments conducted on the RefCOCO/+/g, and PhraseCut benchmarks demonstrate that IteRPrimE outperforms previous state-of-the-art zero-shot methods, particularly excelling in out-of-domain scenarios.</p>
  </details>
</details>
<details>
  <summary>119. <b>【2503.00932】Improving the Transferability of Adversarial Attacks by an Input Transpose</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00932">https://arxiv.org/abs/2503.00932</a></p>
  <p><b>作者</b>：Qing Wan,Shilong Deng,Xun Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：Deep neural networks, Deep neural, incorrect model predictions, subtle perturbations applied, neural networks</p>
  <p><b>备注</b>： 15 pages, 11 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deep neural networks (DNNs) are highly susceptible to adversarial examples--subtle perturbations applied to inputs that are often imperceptible to humans yet lead to incorrect model predictions. In black-box scenarios, however, existing adversarial examples exhibit limited transferability and struggle to effectively compromise multiple unseen DNN models. Previous strategies enhance the cross-model generalization of adversarial examples by introducing versatility into adversarial perturbations, thereby improving transferability. However, further refining perturbation versatility often demands intricate algorithm development and substantial computation consumption. In this work, we propose an input transpose method that requires almost no additional labor and computation costs but can significantly improve the transferability of existing adversarial strategies. Even without adding adversarial perturbations, our method demonstrates considerable effectiveness in cross-model attacks. Our exploration finds that on specific datasets, a mere $1^\circ$ left or right rotation might be sufficient for most adversarial examples to deceive unseen models. Our further analysis suggests that this transferability improvement triggered by rotating only $1^\circ$ may stem from visible pattern shifts in the DNN's low-level feature maps. Moreover, this transferability exhibits optimal angles that, when identified under unrestricted query conditions, could potentially yield even greater performance.</p>
  </details>
</details>
<details>
  <summary>120. <b>【2503.00928】Revisiting CAD Model Generation by Learning Raster Sketch</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00928">https://arxiv.org/abs/2503.00928</a></p>
  <p><b>作者</b>：Pu Li,Wenhao Zhang,Jianwei Guo,Jinglu Chen,Dong-Ming Yan</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：generating Computer-Aided Design, garnered increasing attention, Computer-Aided Design, deep generative networks, recent years</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The integration of deep generative networks into generating Computer-Aided Design (CAD) models has garnered increasing attention over recent years. Traditional methods often rely on discrete sequences of parametric line/curve segments to represent sketches. Differently, we introduce RECAD, a novel framework that generates Raster sketches and 3D Extrusions for CAD models. Representing sketches as raster images offers several advantages over discrete sequences: 1) it breaks the limitations on the types and numbers of lines/curves, providing enhanced geometric representation capabilities; 2) it enables interpolation within a continuous latent space; and 3) it allows for more intuitive user control over the output. Technically, RECAD employs two diffusion networks: the first network generates extrusion boxes conditioned on the number and types of extrusions, while the second network produces sketch images conditioned on these extrusion boxes. By combining these two networks, RECAD effectively generates sketch-and-extrude CAD models, offering a more robust and intuitive approach to CAD model generation. Experimental results indicate that RECAD achieves strong performance in unconditional generation, while also demonstrating effectiveness in conditional generation and output editing.</p>
  </details>
</details>
<details>
  <summary>121. <b>【2503.00925】Explainable Classifier for Malignant Lymphoma Subtyping via Cell Graph and Image Fusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00925">https://arxiv.org/abs/2503.00925</a></p>
  <p><b>作者</b>：Daiki Nishiyama,Hiroaki Miyoshi,Noriaki Hashimoto,Koichi Ohshima,Hidekata Hontani,Ichiro Takeuchi,Jun Sakuma</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：necessitating classification models, classification directly impacts, Malignant lymphoma subtype, directly impacts treatment, impacts treatment strategies</p>
  <p><b>备注</b>： 11 pages, 3 figure</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Malignant lymphoma subtype classification directly impacts treatment strategies and patient outcomes, necessitating classification models that achieve both high accuracy and sufficient explainability. This study proposes a novel explainable Multi-Instance Learning (MIL) framework that identifies subtype-specific Regions of Interest (ROIs) from Whole Slide Images (WSIs) while integrating cell distribution characteristics and image information. Our framework simultaneously addresses three objectives: (1) indicating appropriate ROIs for each subtype, (2) explaining the frequency and spatial distribution of characteristic cell types, and (3) achieving high-accuracy subtyping by leveraging both image and cell-distribution modalities. The proposed method fuses cell graph and image features extracted from each patch in the WSI using a Mixture-of-Experts (MoE) approach and classifies subtypes within an MIL framework. Experiments on a dataset of 1,233 WSIs demonstrate that our approach achieves state-of-the-art accuracy among ten comparative methods and provides region-level and cell-level explanations that align with a pathologist's perspectives.</p>
  </details>
</details>
<details>
  <summary>122. <b>【2503.00915】Multimodal Distillation-Driven Ensemble Learning for Long-Tailed Histopathology Whole Slide Images Analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00915">https://arxiv.org/abs/2503.00915</a></p>
  <p><b>作者</b>：Xitong Ling,Yifeng Ping,Jiawen Li,Jing Peng,Yuxuan Chen,Minxi Ouyang,Yizhi Wang,Yonghong He,Tian Guan,Xiaoping Liu,Lianghui Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Slide Image, enabling weakly supervised, Multiple Instance Learning, weakly supervised analysis, Instance Learning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multiple Instance Learning (MIL) plays a significant role in computational pathology, enabling weakly supervised analysis of Whole Slide Image (WSI) datasets. The field of WSI analysis is confronted with a severe long-tailed distribution problem, which significantly impacts the performance of classifiers. Long-tailed distributions lead to class imbalance, where some classes have sparse samples while others are abundant, making it difficult for classifiers to accurately identify minority class samples. To address this issue, we propose an ensemble learning method based on MIL, which employs expert decoders with shared aggregators and consistency constraints to learn diverse distributions and reduce the impact of class imbalance on classifier performance. Moreover, we introduce a multimodal distillation framework that leverages text encoders pre-trained on pathology-text pairs to distill knowledge and guide the MIL aggregator in capturing stronger semantic features relevant to class information. To ensure flexibility, we use learnable prompts to guide the distillation process of the pre-trained text encoder, avoiding limitations imposed by specific prompts. Our method, MDE-MIL, integrates multiple expert branches focusing on specific data distributions to address long-tailed issues. Consistency control ensures generalization across classes. Multimodal distillation enhances feature extraction. Experiments on Camelyon+-LT and PANDA-LT datasets show it outperforms state-of-the-art methods.</p>
  </details>
</details>
<details>
  <summary>123. <b>【2503.00905】DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00905">https://arxiv.org/abs/2503.00905</a></p>
  <p><b>作者</b>：Zhu Liu,Zijun Wang,Jinyuan Liu,Fanqi Meng,Long Ma,Risheng Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：complex degradations caused, unpredictable environmental factors, caused by hardware, hardware limitations, limitations and unpredictable</p>
  <p><b>备注</b>： The source code will be available at [this https URL](https://github.com/LiuZhu-CV/DEAL) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Thermal imaging is often compromised by dynamic, complex degradations caused by hardware limitations and unpredictable environmental factors. The scarcity of high-quality infrared data, coupled with the challenges of dynamic, intricate degradations, makes it difficult to recover details using existing methods. In this paper, we introduce thermal degradation simulation integrated into the training process via a mini-max optimization, by modeling these degraded factors as adversarial attacks on thermal images. The simulation is dynamic to maximize objective functions, thus capturing a broad spectrum of degraded data distributions. This approach enables training with limited data, thereby improving model this http URL, we introduce a dual-interaction network that combines the benefits of spiking neural networks with scale transformation to capture degraded features with sharp spike signal intensities. This architecture ensures compact model parameters while preserving efficient feature representation. Extensive experiments demonstrate that our method not only achieves superior visual quality under diverse single and composited degradation, but also delivers a significant reduction in processing when trained on only fifty clear images, outperforming existing techniques in efficiency and accuracy. The source code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>124. <b>【2503.00901】FunBench: Benchmarking Fundus Reading Skills of MLLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00901">https://arxiv.org/abs/2503.00901</a></p>
  <p><b>作者</b>：Qijie Wei,Kaiheng Qian,Xirong Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Multimodal Large Language, Large Language Models, Multimodal Large, shown significant potential, Large Language</p>
  <p><b>备注</b>： 7 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Multimodal Large Language Models (MLLMs) have shown significant potential in medical image analysis. However, their capabilities in interpreting fundus images, a critical skill for ophthalmology, remain under-evaluated. Existing benchmarks lack fine-grained task divisions and fail to provide modular analysis of its two key modules, i.e., large language model (LLM) and vision encoder (VE). This paper introduces FunBench, a novel visual question answering (VQA) benchmark designed to comprehensively evaluate MLLMs' fundus reading skills. FunBench features a hierarchical task organization across four levels (modality perception, anatomy perception, lesion analysis, and disease diagnosis). It also offers three targeted evaluation modes: linear-probe based VE evaluation, knowledge-prompted LLM evaluation, and holistic evaluation. Experiments on nine open-source MLLMs plus GPT-4o reveal significant deficiencies in fundus reading skills, particularly in basic tasks such as laterality recognition. The results highlight the limitations of current MLLMs and emphasize the need for domain-specific training and improved LLMs and VEs.</p>
  </details>
</details>
<details>
  <summary>125. <b>【2503.00897】A Simple and Effective Reinforcement Learning Method for Text-to-Image Diffusion Fine-tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00897">https://arxiv.org/abs/2503.00897</a></p>
  <p><b>作者</b>：Shashank Gupta,Chaitanya Ahuja,Tsung-Yu Lin,Sreya Dutta Roy,Harrie Oosterhuis,Maarten de Rijke,Satya Narayan Shukla</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Reinforcement learning, powerful approach, approach for aligning, PPO, policy optimization</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Reinforcement learning ( RL)-based fine-tuning has emerged as a powerful approach for aligning diffusion models with black-box objectives. Proximal policy optimization (PPO) is the most popular choice of method for policy optimization. While effective in terms of performance, PPO is highly sensitive to hyper-parameters and involves substantial computational overhead. REINFORCE, on the other hand, mitigates some computational complexities such as high memory overhead and sensitive hyper-parameter tuning, but has suboptimal performance due to high-variance and sample inefficiency. While the variance of the REINFORCE can be reduced by sampling multiple actions per input prompt and using a baseline correction term, it still suffers from sample inefficiency. To address these challenges, we systematically analyze the efficiency-effectiveness trade-off between REINFORCE and PPO, and propose leave-one-out PPO ( LOOP), a novel RL for diffusion fine-tuning method. LOOP combines variance reduction techniques from REINFORCE, such as sampling multiple actions per input prompt and a baseline correction term, with the robustness and sample efficiency of PPO via clipping and importance sampling. Our results demonstrate that LOOP effectively improves diffusion models on various black-box objectives, and achieves a better balance between computational efficiency and performance.</p>
  </details>
</details>
<details>
  <summary>126. <b>【2503.00890】Estimating Blood Pressure with a Camera: An Exploratory Study of Ambulatory Patients with Cardiovascular Disease</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00890">https://arxiv.org/abs/2503.00890</a></p>
  <p><b>作者</b>：Theodore Curran,Chengqian Ma,Xin Liu,Daniel McDuff,Girish Narayanswamy,George Stergiou,Shwetak Patel,Eugene Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：mortality worldwide, morbidity and mortality, rPPG, Hypertension, subjects</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Hypertension is a leading cause of morbidity and mortality worldwide. The ability to diagnose and treat hypertension in the ambulatory population is hindered by limited access and poor adherence to current methods of monitoring blood pressure (BP), specifically, cuff-based devices. Remote photoplethysmography (rPPG) evaluates an individual's pulse waveform through a standard camera without physical contact. Cameras are readily available to the majority of the global population via embedded technologies such as smartphones, thus rPPG is a scalable and promising non-invasive method of BP monitoring. The few studies investigating rPPG for BP measurement have excluded high-risk populations, including those with cardiovascular disease (CVD) or its risk factors, as well as subjects in active cardiac arrhythmia. The impact of arrhythmia, like atrial fibrillation, on the prediction of BP using rPPG is currently uncertain. We performed a study to better understand the relationship between rPPG and BP in a real-world sample of ambulatory patients from a cardiology clinic with established CVD or risk factors for CVD. We collected simultaneous rPPG, PPG, BP, ECG, and other vital signs data from 143 subjects while at rest, and used this data plus demographics to train a deep learning model to predict BP. We report that facial rPPG yields a signal that is comparable to finger PPG. Pulse wave analysis (PWA)-based BP estimates on this cohort performed comparably to studies on healthier subjects, and notably, the accuracy of BP prediction in subjects with atrial fibrillation was not inferior to subjects with normal sinus rhythm. In a binary classification task, the rPPG model identified subjects with systolic BP $\geq$ 130 mm Hg with a positive predictive value of 71% (baseline prevalence 48.3%), highlighting the potential of rPPG for hypertension monitoring.</p>
  </details>
</details>
<details>
  <summary>127. <b>【2503.00881】Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00881">https://arxiv.org/abs/2503.00881</a></p>
  <p><b>作者</b>：You Shen,Zhipeng Zhang,Xinyang Li,Yansong Qu,Yu Lin,Shengchuan Zhang,Liujuan Cao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：scenes from multiview, vision and graphics, multiview images, computer vision, Gaussian Splatting</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Representing 3D scenes from multiview images is a core challenge in computer vision and graphics, which requires both precise rendering and accurate reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention for its high-quality rendering and fast inference speed. Yet, due to the unstructured and irregular nature of Gaussian point clouds, ensuring accurate geometry reconstruction remains difficult. Existing methods primarily focus on geometry regularization, with common approaches including primitive-based and dual-model frameworks. However, the former suffers from inherent conflicts between rendering and reconstruction, while the latter is computationally and storage-intensive. To address these challenges, we propose CarGS, a unified model leveraging Contribution-adaptive regularization to achieve simultaneous, high-quality rendering and surface reconstruction. The essence of our framework is learning adaptive contribution for Gaussian primitives by squeezing the knowledge from geometry regularization into a compact MLP. Additionally, we introduce a geometry-guided densification strategy with clues from both normals and Signed Distance Fields (SDF) to improve the capability of capturing high-frequency details. Our design improves the mutual learning of the two tasks, meanwhile its unified structure does not require separate models as in dual-model based approaches, guaranteeing efficiency. Extensive experiments demonstrate the ability to achieve state-of-the-art (SOTA) results in both rendering fidelity and reconstruction accuracy while maintaining real-time speed and minimal storage size.</p>
  </details>
</details>
<details>
  <summary>128. <b>【2503.00861】Zero-Shot Head Swapping in Real-World Scenarios</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00861">https://arxiv.org/abs/2503.00861</a></p>
  <p><b>作者</b>：Sohyun Jeong,Taewoong Kang,Hyojin Jang,Jaegul Choo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：advanced head-swapping techniques, head-swapping techniques, integrating an entire, growing demand, demand in media</p>
  <p><b>备注</b>： CVPR'25</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:With growing demand in media and social networks for personalized images, the need for advanced head-swapping techniques, integrating an entire head from the head image with the body from the body image, has increased. However, traditional head swapping methods heavily rely on face-centered cropped data with primarily frontal facing views, which limits their effectiveness in real world applications. Additionally, their masking methods, designed to indicate regions requiring editing, are optimized for these types of dataset but struggle to achieve seamless blending in complex situations, such as when the original data includes features like long hair extending beyond the masked area. To overcome these limitations and enhance adaptability in diverse and complex scenarios, we propose a novel head swapping method, HID, that is robust to images including the full head and the upper body, and handles from frontal to side views, while automatically generating context aware masks. For automatic mask generation, we introduce the IOMask, which enables seamless blending of the head and body, effectively addressing integration challenges. We further introduce the hair injection module to capture hair details with greater precision. Our experiments demonstrate that the proposed approach achieves state-of-the-art performance in head swapping, providing visually consistent and realistic results across a wide range of challenging conditions.</p>
  </details>
</details>
<details>
  <summary>129. <b>【2503.00853】MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00853">https://arxiv.org/abs/2503.00853</a></p>
  <p><b>作者</b>：Rui Yi Yong,Samuel Picosson,Arnold Wiliem</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：visually sound reconstructions, fly-over perspective problem, work tackles, perspective problem, specific emphasis</p>
  <p><b>备注</b>： WACV Workshop 2025 - 3rd Workshop on Maritime Computer Vision (MaCVI2025)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This work tackles 3D scene reconstruction for a video fly-over perspective problem in the maritime domain, with a specific emphasis on geometrically and visually sound reconstructions. This will allow for downstream tasks such as segmentation, navigation, and localization. To our knowledge, there is no dataset available in this domain. As such, we propose a novel maritime 3D scene reconstruction benchmarking dataset, named as MTReD (Maritime Three-Dimensional Reconstruction Dataset). The MTReD comprises 19 fly-over videos curated from the Internet containing ships, islands, and coastlines. As the task is aimed towards geometrical consistency and visual completeness, the dataset uses two metrics: (1) Reprojection error; and (2) Perception based metrics. We find that existing perception-based metrics, such as Learned Perceptual Image Patch Similarity (LPIPS), do not appropriately measure the completeness of a reconstructed image. Thus, we propose a novel semantic similarity metric utilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity). We perform initial evaluation on two baselines: (1) Structured from Motion (SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We find that the reconstructed scenes by MASt3R have higher reprojection errors, but superior perception based metric scores. To this end, some pre-processing methods are explored, and we find a pre-processing method which improves both the reprojection error and perception-based score. We envisage our proposed MTReD to stimulate further research in these directions. The dataset and all the code will be made available in this https URL.</p>
  </details>
</details>
<details>
  <summary>130. <b>【2503.00848】PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00848">https://arxiv.org/abs/2503.00848</a></p>
  <p><b>作者</b>：BoCheng Li,WenJuan Zhang,Bing Zhang,YiLing Yao,YaNing Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：adaptive density control, Gaussian Splatting, achieves impressive results, Gaussian ellipsoid initialization, Gaussian ellipsoids</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:3D Gaussian Splatting (3D GS) achieves impressive results in novel view synthesis for small, single-object scenes through Gaussian ellipsoid initialization and adaptive density control. However, when applied to large-scale remote sensing scenes, 3D GS faces challenges: the point clouds generated by Structure-from-Motion (SfM) are often sparse, and the inherent smoothing behavior of 3D GS leads to over-reconstruction in high-frequency regions, where have detailed textures and color variations. This results in the generation of large, opaque Gaussian ellipsoids that cause gradient artifacts. Moreover, the simultaneous optimization of both geometry and texture may lead to densification of Gaussian ellipsoids at incorrect geometric locations, resulting in artifacts in other views. To address these issues, we propose PSRGS, a progressive optimization scheme based on spectral residual maps. Specifically, we create a spectral residual significance map to separate low-frequency and high-frequency regions. In the low-frequency region, we apply depth-aware and depth-smooth losses to initialize the scene geometry with low threshold. For the high-frequency region, we use gradient features with higher threshold to split and clone ellipsoids, refining the scene. The sampling rate is determined by feature responses and gradient loss. Finally, we introduce a pre-trained network that jointly computes perceptual loss from multiple views, ensuring accurate restoration of high-frequency details in both Gaussian ellipsoids geometry and color. We conduct experiments on multiple datasets to assess the effectiveness of our method, which demonstrates competitive rendering quality, especially in recovering texture details in high-frequency regions.</p>
  </details>
</details>
<details>
  <summary>131. <b>【2503.00838】Foundation Models Secretly Understand Neural Network Weights: Enhancing Hypernetwork Architectures with Foundation Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00838">https://arxiv.org/abs/2503.00838</a></p>
  <p><b>作者</b>：Jeffrey Gu,Serena Yeung-Levy</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Large pre-trained models, Large pre-trained, out-performing specialized models, foundation models, shown impressive performance</p>
  <p><b>备注</b>： ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Large pre-trained models, or foundation models, have shown impressive performance when adapted to a variety of downstream tasks, often out-performing specialized models. Hypernetworks, neural networks that generate some or all of the parameters of another neural network, have become an increasingly important technique for conditioning and generalizing implicit neural representations (INRs), which represent signals or objects such as audio or 3D shapes using a neural network. However, despite the potential benefits of incorporating foundation models in hypernetwork methods, this research direction has not been investigated, likely due to the dissimilarity of the weight generation task with other visual tasks. To address this gap, we (1) show how foundation models can improve hypernetworks with Transformer-based architectures, (2) provide an empirical analysis of the benefits of foundation models for hypernetworks through the lens of the generalizable INR task, showing that leveraging foundation models improves performance, generalizability, and data efficiency across a variety of algorithms and modalities. We also provide further analysis in examining the design space of foundation model-based hypernetworks, including examining the choice of foundation models, algorithms, and the effect of scaling foundation models.</p>
  </details>
</details>
<details>
  <summary>132. <b>【2503.00828】raining-Free Dataset Pruning for Instance Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00828">https://arxiv.org/abs/2503.00828</a></p>
  <p><b>作者</b>：Yalun Dai,Lingao Xiao,Ivor W. Tsang,Yang He</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：techniques primarily focus, pruning techniques primarily, dataset pruning techniques, limiting their applicability, classification tasks</p>
  <p><b>备注</b>： Accepted by ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Existing dataset pruning techniques primarily focus on classification tasks, limiting their applicability to more complex and practical tasks like instance segmentation. Instance segmentation presents three key challenges: pixel-level annotations, instance area variations, and class imbalances, which significantly complicate dataset pruning efforts. Directly adapting existing classification-based pruning methods proves ineffective due to their reliance on time-consuming model training process. To address this, we propose a novel Training-Free Dataset Pruning (TFDP) method for instance segmentation. Specifically, we leverage shape and class information from image annotations to design a Shape Complexity Score (SCS), refining it into a Scale-Invariant (SI-SCS) and Class-Balanced (CB-SCS) versions to address instance area variations and class imbalances, all without requiring model training. We achieve state-of-the-art results on VOC 2012, Cityscapes, and COCO datasets, generalizing well across CNN and Transformer architectures. Remarkably, our approach accelerates the pruning process by an average of 1349$\times$ on COCO compared to the adapted baselines. Source code is available at: this https URL</p>
  </details>
</details>
<details>
  <summary>133. <b>【2503.00823】ask-Agnostic Guided Feature Expansion for Class-Incremental Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00823">https://arxiv.org/abs/2503.00823</a></p>
  <p><b>作者</b>：Bowen Zheng,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：features, CIL, task-agnostic features, Guided Feature Expansion, diverse features</p>
  <p><b>备注</b>： Accepted to CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The ability to learn new concepts while preserve the learned knowledge is desirable for learning systems in Class-Incremental Learning (CIL). Recently, feature expansion of the model become a prevalent solution for CIL, where the old features are fixed during the training of the new task while new features are expanded for the new tasks. However, such task-specific features learned from the new task may collide with the old features, leading to misclassification between tasks. Therefore, the expanded model is often encouraged to capture diverse features from the new task, aiming to avoid such collision. However, the existing solution is largely restricted to the samples from the current task, because of the poor accessibility to previous samples. To promote the learning and transferring of diverse features across tasks, we propose a framework called Task-Agnostic Guided Feature Expansion (TagFex). Firstly, it captures task-agnostic features continually with a separate model, providing extra task-agnostic features for subsequent tasks. Secondly, to obtain useful features from the task-agnostic model for the current task, it aggregates the task-agnostic features with the task-specific feature using a merge attention. Then the aggregated feature is transferred back into the task-specific feature for inference, helping the task-specific model capture diverse features. Extensive experiments show the effectiveness and superiority of TagFex on various CIL settings. Code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>134. <b>【2503.00816】Random Walks in Self-supervised Learning for Triangular Meshes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00816">https://arxiv.org/abs/2503.00816</a></p>
  <p><b>作者</b>：Gal Yefet,Ayellet Tal</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：study addresses, addresses the challenge, challenge of self-supervised, mesh analysis, self-supervised learning</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This study addresses the challenge of self-supervised learning for 3D mesh analysis. It presents an new approach that uses random walks as a form of data augmentation to generate diverse representations of mesh surfaces. Furthermore, it employs a combination of contrastive and clustering losses. The contrastive learning framework maximizes similarity between augmented instances of the same mesh while minimizing similarity between different meshes. We integrate this with a clustering loss, enhancing class distinction across training epochs and mitigating training variance. Our model's effectiveness is evaluated using mean Average Precision (mAP) scores and a supervised SVM linear classifier on extracted features, demonstrating its potential for various downstream tasks such as object classification and shape retrieval.</p>
  </details>
</details>
<details>
  <summary>135. <b>【2503.00811】Evaluating and Predicting Distorted Human Body Parts for Generated Images</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00811">https://arxiv.org/abs/2503.00811</a></p>
  <p><b>作者</b>：Lu Ma,Kaibo Cao,Hao Liang,Jiaxin Lin,Zhuang Li,Yuhong Liu,Jihong Zhang,Wentao Zhang,Bin Cui</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：generating anatomically accurate, Recent advancements, figures remains challenging, high-quality image synthesis, Fréchet Inception Distance</p>
  <p><b>备注</b>： 8 pages, 6 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advancements in text-to-image (T2I) models enable high-quality image synthesis, yet generating anatomically accurate human figures remains challenging. AI-generated images frequently exhibit distortions such as proliferated limbs, missing fingers, deformed extremities, or fused body parts. Existing evaluation metrics like Inception Score (IS) and Fréchet Inception Distance (FID) lack the granularity to detect these distortions, while human preference-based metrics focus on abstract quality assessments rather than anatomical fidelity. To address this gap, we establish the first standards for identifying human body distortions in AI-generated images and introduce Distortion-5K, a comprehensive dataset comprising 4,700 annotated images of normal and malformed human figures across diverse styles and distortion types. Based on this dataset, we propose ViT-HD, a Vision Transformer-based model tailored for detecting human body distortions in AI-generated images, which outperforms state-of-the-art segmentation models and visual language models, achieving an F1 score of 0.899 and IoU of 0.831 on distortion localization. Additionally, we construct the Human Distortion Benchmark with 500 human-centric prompts to evaluate four popular T2I models using trained ViT-HD, revealing that nearly 50\% of generated images contain distortions. This work pioneers a systematic approach to evaluating anatomical accuracy in AI-generated humans, offering tools to advance the fidelity of T2I models and their real-world applicability. The Distortion-5K dataset, trained ViT-HD will soon be released in our GitHub repository: \href{this https URL}{this https URL}.</p>
  </details>
</details>
<details>
  <summary>136. <b>【2503.00807】GenAnalysis: Joint Shape Analysis by Learning Man-Made Shape Generators with Deformation Regularizations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00807">https://arxiv.org/abs/2503.00807</a></p>
  <p><b>作者</b>：Yuezhi Yang,Haitao Yang,Kiyohiro Nakayama,Xiangru Huang,Leonidas Guibas,Qixing Huang</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：shape generation framework, implicit shape generation, shape, present GenAnalysis, generation framework</p>
  <p><b>备注</b>： 21 pages, 25 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:We present GenAnalysis, an implicit shape generation framework that allows joint analysis of man-made shapes, including shape matching and joint shape segmentation. The key idea is to enforce an as-affine-as-possible (AAAP) deformation between synthetic shapes of the implicit generator that are close to each other in the latent space, which we achieve by designing a regularization loss. It allows us to understand the shape variation of each shape in the context of neighboring shapes and also offers structure-preserving interpolations between the input shapes. We show how to extract these shape variations by recovering piecewise affine vector fields in the tangent space of each shape. These vector fields provide single-shape segmentation cues. We then derive shape correspondences by iteratively propagating AAAP deformations across a sequence of intermediate shapes. These correspondences are then used to aggregate single-shape segmentation cues into consistent segmentations. We conduct experiments on the ShapeNet dataset to show superior performance in shape matching and joint shape segmentation over previous methods.</p>
  </details>
</details>
<details>
  <summary>137. <b>【2503.00804】DELST: Dual Entailment Learning for Hyperbolic Image-Gene Pretraining in Spatial Transcriptomics</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00804">https://arxiv.org/abs/2503.00804</a></p>
  <p><b>作者</b>：Xulin Chen,Junzhou Huang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：Spatial transcriptomics, tissue at individual, valuable resource, resource for multimodal, maps gene expression</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Spatial transcriptomics (ST) maps gene expression within tissue at individual spots, making it a valuable resource for multimodal representation learning. Additionally, ST inherently contains rich hierarchical information both across and within modalities. For instance, different spots exhibit varying numbers of nonzero gene expressions, corresponding to different levels of cellular activity and semantic hierarchies. However, existing methods rely on contrastive alignment of image-gene pairs, failing to accurately capture the intricate hierarchical relationships in ST data. Here, we propose DELST, the first framework to embed hyperbolic representations while modeling hierarchy for image-gene pretraining at two levels: (1) Cross-modal entailment learning, which establishes an order relationship between genes and images to enhance image representation generalization; (2) Intra-modal entailment learning, which encodes gene expression patterns as hierarchical relationships, guiding hierarchical learning across different samples at a global scale and integrating biological insights into single-modal representations. Extensive experiments on ST benchmarks annotated by pathologists demonstrate the effectiveness of our framework, achieving improved predictive performance compared to existing methods. Our code and models are available at: this https URL.</p>
  </details>
</details>
<details>
  <summary>138. <b>【2503.00803】HiMo: High-Speed Objects Motion Compensation in Point Clouds</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00803">https://arxiv.org/abs/2503.00803</a></p>
  <p><b>作者</b>：Qingwen Zhang,Ajinkya Khoche,Yi Yang,Li Ling,Sina Sharif Mansouri,Olov Andersson,Patric Jensfelt</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：point cloud distortion, LiDAR point clouds, cloud distortion, point cloud, LiDAR point</p>
  <p><b>备注</b>： 12 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:LiDAR point clouds often contain motion-induced distortions, degrading the accuracy of object appearances in the captured data. In this paper, we first characterize the underlying reasons for the point cloud distortion and show that this is present in public datasets. We find that this distortion is more pronounced in high-speed environments such as highways, as well as in multi-LiDAR configurations, a common setup for heavy vehicles. Previous work has dealt with point cloud distortion from the ego-motion but fails to consider distortion from the motion of other objects. We therefore introduce a novel undistortion pipeline, HiMo, that leverages scene flow estimation for object motion compensation, correcting the depiction of dynamic objects. We further propose an extension of a state-of-the-art self-supervised scene flow method. Due to the lack of well-established motion distortion metrics in the literature, we also propose two metrics for compensation performance evaluation: compensation accuracy at a point level and shape similarity on objects. To demonstrate the efficacy of our method, we conduct extensive experiments on the Argoverse 2 dataset and a new real-world dataset. Our new dataset is collected from heavy vehicles equipped with multi-LiDARs and on highways as opposed to mostly urban settings in the existing datasets. The source code, including all methods and the evaluation data, will be provided upon publication. See this https URL for more details.</p>
  </details>
</details>
<details>
  <summary>139. <b>【2503.00802】MFM-DA: Instance-Aware Adaptor and Hierarchical Alignment for Efficient Domain Adaptation in Medical Foundation Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00802">https://arxiv.org/abs/2503.00802</a></p>
  <p><b>作者</b>：Jia-Xuan Jiang,Wenhui Lei,Yifeng Wu,Hongtao Wu,Furong Li,Yining Xie,Xiaofan Zhang,Zhong Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Medical Foundation Models, demonstrated superior performance, Medical Foundation, trained on large-scale, large-scale datasets</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Medical Foundation Models (MFMs), trained on large-scale datasets, have demonstrated superior performance across various tasks. However, these models still struggle with domain gaps in practical applications. Specifically, even after fine-tuning on source-domain data, task-adapted foundation models often perform poorly in the target domain. To address this challenge, we propose a few-shot unsupervised domain adaptation (UDA) framework for MFMs, named MFM-DA, which only leverages a limited number of unlabeled target-domain images. Our approach begins by training a Denoising Diffusion Probabilistic Model (DDPM), which is then adapted to the target domain using a proposed dynamic instance-aware adaptor and a distribution direction loss, enabling the DDPM to translate source-domain images into the target domain style. The adapted images are subsequently processed through the MFM, where we introduce a designed channel-spatial alignment Low-Rank Adaptation (LoRA) to ensure effective feature alignment. Extensive experiments on optic cup and disc segmentation tasks demonstrate that MFM-DA outperforms state-of-the-art methods. Our work provides a practical solution to the domain gap issue in real-world MFM deployment. Code will be available at here.</p>
  </details>
</details>
<details>
  <summary>140. <b>【2503.00801】STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00801">https://arxiv.org/abs/2503.00801</a></p>
  <p><b>作者</b>：Zikuan Li,Honghua Chen,Yuecheng Wang,Sibo Wu,Mingqiang Wei,Jun Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Extracting geometric edges, Extracting geometric, unstructured point clouds, point clouds remains, significant challenge</p>
  <p><b>备注</b>： Accepted at CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Extracting geometric edges from unstructured point clouds remains a significant challenge, particularly in thin-walled structures that are commonly found in everyday objects. Traditional geometric methods and recent learning-based approaches frequently struggle with these structures, as both rely heavily on sufficient contextual information from local point neighborhoods. However, 3D measurement data of thin-walled structures often lack the accurate, dense, and regular neighborhood sampling required for reliable edge extraction, resulting in degraded performance.
In this work, we introduce STAR-Edge, a novel approach designed for detecting and refining edge points in thin-walled structures. Our method leverages a unique representation-the local spherical curve-to create structure-aware neighborhoods that emphasize co-planar points while reducing interference from close-by, non-co-planar surfaces. This representation is transformed into a rotation-invariant descriptor, which, combined with a lightweight multi-layer perceptron, enables robust edge point classification even in the presence of noise and sparse or irregular sampling. Besides, we also use the local spherical curve representation to estimate more precise normals and introduce an optimization function to project initially identified edge points exactly on the true edges. Experiments conducted on the ABC dataset and thin-walled structure-specific datasets demonstrate that STAR-Edge outperforms existing edge detection methods, showcasing better robustness under various challenging conditions.
</p><p>Comments:<br>
Accepted at CVPR 2025</p>
<p>Subjects:</p>
<p>Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)</p>
<p>Cite as:<br>
arXiv:2503.00801 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]</p>
<p>(or<br>
arXiv:2503.00801v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.00801">https://doi.org/10.48550/arXiv.2503.00801</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)&lt;/p&gt;
</code></pre>
  </details>
</details>
<details>
  <summary>141. <b>【2503.00796】A Simple and Efficient Baseline for Video Action Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00796">https://arxiv.org/abs/2503.00796</a></p>
  <p><b>作者</b>：Zhe Wang,Xulei Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：video action recognition, action recognition, recent years, video action, huge progress</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:There has been huge progress on video action recognition in recent years. However, many works focus on tweaking existing 2D backbones due to the reliance of ImageNet pretraining, which restrains the models from achieving higher efficiency for video recognition. In this work we introduce a simple and very efficient 3D convolutional neural network for video action recognition. The design of the building block consists of a channel-wise convolution, followed by a spatial group convolution, and finally a temporal convolution. We evaluate the performance and efficiency of our proposed network on several video action recognition datasets by directly training on the target dataset without relying on pertaining. On Something-Something-V1V2, Kinetics-400 and Multi-Moments in Time, our network can match or even surpass the performance of other models which are several times larger. On the fine-grained action recognition dataset FineGym, we beat the previous state-of-the-art accuracy achieved with 2-stream methods by more than 5% using only RGB input.</p>
  </details>
</details>
<details>
  <summary>142. <b>【2503.00793】Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00793">https://arxiv.org/abs/2503.00793</a></p>
  <p><b>作者</b>：Ukcheol Shin,Kyunghyun Lee,Jean Oh</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)</p>
  <p><b>关键词</b>：real world requires, Deploying depth estimation, world requires high-level, requires high-level robustness, Deploying depth</p>
  <p><b>备注</b>： Accepted at ICRA 2025, Github link: [this https URL](https://github.com/UkcheolShin/BridgeMultiSpectralDepth) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Deploying depth estimation networks in the real world requires high-level robustness against various adverse conditions to ensure safe and reliable autonomy. For this purpose, many autonomous vehicles employ multi-modal sensor systems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar. They mainly adopt two strategies to use multiple sensors: modality-wise and multi-modal fused inference. The former method is flexible but memory-inefficient, unreliable, and vulnerable. Multi-modal fusion can provide high-level reliability, yet it needs a specialized architecture. In this paper, we propose an effective solution, named align-and-fuse strategy, for the depth estimation from multi-spectral images. In the align stage, we align embedding spaces between multiple spectrum bands to learn shareable representation across multi-spectral images by minimizing contrastive loss of global and spatially aligned local features with geometry cue. After that, in the fuse stage, we train an attachable feature fusion module that can selectively aggregate the multi-spectral features for reliable and robust prediction results. Based on the proposed method, a single-depth network can achieve both spectral-invariant and multi-spectral fused depth estimation while preserving reliability, memory efficiency, and flexibility.</p>
  </details>
</details>
<details>
  <summary>143. <b>【2503.00783】CARIL: Confidence-Aware Regression in Imitation Learning for Autonomous Driving</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00783">https://arxiv.org/abs/2503.00783</a></p>
  <p><b>作者</b>：Elahe Delavari,Aws Khalil,Jaerock Kwon</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：control commands directly, demonstrated promising results, expert demonstrations, learning control commands, demonstrated promising</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:End-to-end vision-based imitation learning has demonstrated promising results in autonomous driving by learning control commands directly from expert demonstrations. However, traditional approaches rely on either regressionbased models, which provide precise control but lack confidence estimation, or classification-based models, which offer confidence scores but suffer from reduced precision due to discretization. This limitation makes it challenging to quantify the reliability of predicted actions and apply corrections when necessary. In this work, we introduce a dual-head neural network architecture that integrates both regression and classification heads to improve decision reliability in imitation learning. The regression head predicts continuous driving actions, while the classification head estimates confidence, enabling a correction mechanism that adjusts actions in low-confidence scenarios, enhancing driving stability. We evaluate our approach in a closed-loop setting within the CARLA simulator, demonstrating its ability to detect uncertain actions, estimate confidence, and apply real-time corrections. Experimental results show that our method reduces lane deviation and improves trajectory accuracy by up to 50%, outperforming conventional regression-only models. These findings highlight the potential of classification-guided confidence estimation in enhancing the robustness of vision-based imitation learning for autonomous driving. The source code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>144. <b>【2503.00782】Wavelet-Driven Masked Image Modeling: A Path to Efficient Visual Representation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00782">https://arxiv.org/abs/2503.00782</a></p>
  <p><b>作者</b>：Wenzhao Xiang,Chang Liu,Hongyang Yu,Xilin Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Masked Image Modeling, garnered significant attention, learn scalable visual, scalable visual representations, visual representations tailored</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Masked Image Modeling (MIM) has garnered significant attention in self-supervised learning, thanks to its impressive capacity to learn scalable visual representations tailored for downstream tasks. However, images inherently contain abundant redundant information, leading the pixel-based MIM reconstruction process to focus excessively on finer details such as textures, thus prolonging training times unnecessarily. Addressing this challenge requires a shift towards a compact representation of features during MIM reconstruction. Frequency domain analysis provides a promising avenue for achieving compact image feature representation. In contrast to the commonly used Fourier transform, wavelet transform not only offers frequency information but also preserves spatial characteristics and multi-level features of the image. Additionally, the multi-level decomposition process of wavelet transformation aligns well with the hierarchical architecture of modern neural networks. In this study, we leverage wavelet transform as a tool for efficient representation learning to expedite the training process of MIM. Specifically, we conduct multi-level decomposition of images using wavelet transform, utilizing wavelet coefficients from different levels to construct distinct reconstruction targets representing various frequencies and scales. These reconstruction targets are then integrated into the MIM process, with adjustable weights assigned to prioritize the most crucial information. Extensive experiments demonstrate that our method achieves comparable or superior performance across various downstream tasks while exhibiting higher training efficiency.</p>
  </details>
</details>
<details>
  <summary>145. <b>【2503.00780】Enhanced Multi-Class Classification of Gastrointestinal Endoscopic Images with Interpretable Deep Learning Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00780">https://arxiv.org/abs/2503.00780</a></p>
  <p><b>作者</b>：Astitva Kamble,Vani Bandodkar,Saakshi Dharmadhikary,Veena Anand,Pradyut Kumar Sanki,Mei X. Wu,Biswabandhu Jana</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：identifying GI-related disorders, Endoscopy serves, evaluating the gastrointestinal, tract and plays, GI-related disorders</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Endoscopy serves as an essential procedure for evaluating the gastrointestinal (GI) tract and plays a pivotal role in identifying GI-related disorders. Recent advancements in deep learning have demonstrated substantial progress in detecting abnormalities through intricate models and data augmentation this http URL research introduces a novel approach to enhance classification accuracy using 8,000 labeled endoscopic images from the Kvasir dataset, categorized into eight distinct classes. Leveraging EfficientNetB3 as the backbone, the proposed architecture eliminates reliance on data augmentation while preserving moderate model complexity. The model achieves a test accuracy of 94.25%, alongside precision and recall of 94.29% and 94.24% respectively. Furthermore, Local Interpretable Model-agnostic Explanation (LIME) saliency maps are employed to enhance interpretability by defining critical regions in the images that influenced model predictions. Overall, this work highlights the importance of AI in advancing medical imaging by combining high classification accuracy with interpretability.</p>
  </details>
</details>
<details>
  <summary>146. <b>【2503.00762】MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00762">https://arxiv.org/abs/2503.00762</a></p>
  <p><b>作者</b>：Fangming Shi,Jinzhen Liu,Xiangqian Meng,Yapeng Zhou,Hui Xiong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：Electrical Impedance Tomography, Impedance Tomography, Electrical Impedance, unsupervised learning mode, paper presents</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:This paper presents a multi-resolution reconstruction method for Electrical Impedance Tomography (EIT), referred to as MR-EIT, which is capable of operating in both supervised and unsupervised learning modes. MR-EIT integrates an ordered feature extraction module and an unordered coordinate feature expression module. The former achieves the mapping from voltage to two-dimensional conductivity features through pre-training, while the latter realizes multi-resolution reconstruction independent of the order and size of the input sequence by utilizing symmetric functions and local feature extraction mechanisms. In the data-driven mode, MR-EIT reconstructs high-resolution images from low-resolution data of finite element meshes through two stages of pre-training and joint training, and demonstrates excellent performance in simulation experiments. In the unsupervised learning mode, MR-EIT does not require pre-training data and performs iterative optimization solely based on measured voltages to rapidly achieve image reconstruction from low to high resolution. It shows robustness to noise and efficient super-resolution reconstruction capabilities in both simulation and real water tank experiments. Experimental results indicate that MR-EIT outperforms the comparison methods in terms of Structural Similarity (SSIM) and Relative Image Error (RIE), especially in the unsupervised learning mode, where it can significantly reduce the number of iterations and improve image reconstruction quality.</p>
  </details>
</details>
<details>
  <summary>147. <b>【2503.00761】RACE: A Self-Improving Framework for Robot Behavior Forecasting with Vision-Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00761">https://arxiv.org/abs/2503.00761</a></p>
  <p><b>作者</b>：Gokul Puthumanaillam,Paulo Padrao,Jose Fuentes,Pranay Thangeda,William E. Schafer,Jae Hyuk Song,Karan Jagdale,Leonardo Bobadilla,Melkior Ornik</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Multiagent Systems (cs.MA); Systems and Control (eess.SY)</p>
  <p><b>关键词</b>：Predicting the near-term, robotic scenarios, remains challenging, counterfactual exploration, Predicting</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Predicting the near-term behavior of a reactive agent is crucial in many robotic scenarios, yet remains challenging when observations of that agent are sparse or intermittent. Vision-Language Models (VLMs) offer a promising avenue by integrating textual domain knowledge with visual cues, but their one-shot predictions often miss important edge cases and unusual maneuvers. Our key insight is that iterative, counterfactual exploration--where a dedicated module probes each proposed behavior hypothesis, explicitly represented as a plausible trajectory, for overlooked possibilities--can significantly enhance VLM-based behavioral forecasting. We present TRACE (Tree-of-thought Reasoning And Counterfactual Exploration), an inference framework that couples tree-of-thought generation with domain-aware feedback to refine behavior hypotheses over multiple rounds. Concretely, a VLM first proposes candidate trajectories for the agent; a counterfactual critic then suggests edge-case variations consistent with partial observations, prompting the VLM to expand or adjust its hypotheses in the next iteration. This creates a self-improving cycle where the VLM progressively internalizes edge cases from previous rounds, systematically uncovering not only typical behaviors but also rare or borderline maneuvers, ultimately yielding more robust trajectory predictions from minimal sensor data. We validate TRACE on both ground-vehicle simulations and real-world marine autonomous surface vehicles. Experimental results show that our method consistently outperforms standard VLM-driven and purely model-based baselines, capturing a broader range of feasible agent behaviors despite sparse sensing. Evaluation videos and code are available at this http URL.</p>
  </details>
</details>
<details>
  <summary>148. <b>【2503.00748】Dynamic Gradient Sparsification Training for Few-Shot Fine-tuning of CT Lymph Node Segmentation Foundation Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00748">https://arxiv.org/abs/2503.00748</a></p>
  <p><b>作者</b>：Zihao Luo,Zijun Gao,Wenjun Liao,Shichuan Zhang,Guotai Wang,Xiangde Luo</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Accurate lymph node, Accurate lymph, lymph node, prognosis analysis, radiotherapy treatment</p>
  <p><b>备注</b>： 10 pages, 3 figures, 2 tables, and the lymph node segmentation foundation model code and pretrained model are available</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Accurate lymph node (LN) segmentation is critical in radiotherapy treatment and prognosis analysis, but is limited by the need for large annotated datasets. While deep learning-based segmentation foundation models show potential in developing high-performing models with fewer samples, their medical adaptation faces LN domain-specific prior deficiencies and inefficient few-shot fine-tuning for complex clinical practices, highlighting the necessity of an LN segmentation foundation model. In this work, we annotated 36,106 visible LNs from 3,346 publicly available head-and-neck CT scans to establish a robust LN segmentation model (nnUNetv2). Building on this, we propose Dynamic Gradient Sparsification Training (DGST), a few-shot fine-tuning approach that preserves foundational knowledge while dynamically updating the most critical parameters of the LN segmentation model with few annotations. We validate it on two publicly available LN segmentation datasets: SegRap2023 and LNQ2023. The results show that DGST outperforms existing few-shot fine-tuning methods, achieving satisfactory performance with limited labeled data. We release the dataset, models and all implementations to facilitate relevant research: this https URL.</p>
  </details>
</details>
<details>
  <summary>149. <b>【2503.00747】Unifying Light Field Perception with Field of Parallax</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00747">https://arxiv.org/abs/2503.00747</a></p>
  <p><b>作者</b>：Fei Teng,Buyin Deng,Boyuan Zheng,Kai Luo,Kunyu Peng,Jiaming Zhang,Kailun Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：Field of Parallax, spatial field, distills the common, provide flexible, flexible and consistent</p>
  <p><b>备注</b>： The source code will be made publicly available at [this https URL](https://github.com/warriordby/LFX) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Field of Parallax (FoP)}, a spatial field that distills the common features from different LF representations to provide flexible and consistent support for multi-task learning. FoP is built upon three core features--projection difference, adjacency divergence, and contextual consistency--which are essential for cross-task adaptability. To implement FoP, we design a two-step angular adapter: the first step captures angular-specific differences, while the second step consolidates contextual consistency to ensure robust representation. Leveraging the FoP-based representation, we introduce the LFX framework, the first to handle arbitrary LF representations seamlessly, unifying LF multi-task vision. We evaluated LFX across three different tasks, achieving new state-of-the-art results, compared with previous task-specific architectures: 84.74% in mIoU for semantic segmentation on UrbanLF, 0.84% in AP for object detection on PKU, and 0.030 in MAE and 0.026 in MAE for salient object detection on Duftv2 and PKU, respectively. The source code will be made publicly available at this https URL.</p>
  </details>
</details>
<details>
  <summary>150. <b>【2503.00746】DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00746">https://arxiv.org/abs/2503.00746</a></p>
  <p><b>作者</b>：Liao Shen,Tianqi Liu,Huiqiang Sun,Jiaqi Li,Zhiguo Cao,Wei Li,Chen Change Loy</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Gaussian Splatting, shown remarkable success, Recent advances, success in representing, generating high-quality</p>
  <p><b>备注</b>： CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable success in representing 3D scenes and generating high-quality, novel views in real-time. However, 3D-GS and its variants assume that input images are captured based on pinhole imaging and are fully in focus. This assumption limits their applicability, as real-world images often feature shallow depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-based imaging model based on geometric optics principles to control DoF effects. To ensure accurate scene geometry, we incorporate depth priors adjusted per scene, and we apply defocus-to-focus adaptation to minimize the gap in the circle of confusion. We also introduce a synthetic dataset to assess refocusing capabilities and the model's ability to learn precise lens parameters. Our framework is customizable and supports various interactive applications. Extensive experiments confirm the effectiveness of our method. Our project is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>151. <b>【2503.00744】Confounder-Aware Medical Data Selection for Fine-Tuning Pretrained Vision Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00744">https://arxiv.org/abs/2503.00744</a></p>
  <p><b>作者</b>：Anyang Ji,Qingbo Kang,Wei Xu,Changfan Wang,Kang Li,Qicheng Lao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：large-scale pre-trained vision, pre-trained vision foundation, vision foundation models, medical imaging field, confounding variables</p>
  <p><b>备注</b>： 5 pages, 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:The emergence of large-scale pre-trained vision foundation models has greatly advanced the medical imaging field through the pre-training and fine-tuning paradigm. However, selecting appropriate medical data for downstream fine-tuning remains a significant challenge considering its annotation cost, privacy concerns, and the detrimental effects of confounding variables. In this work, we present a confounder-aware medical data selection approach for medical dataset curation aiming to select minimal representative data by strategically mitigating the undesirable impact of confounding variables while preserving the natural distribution of the dataset. Our approach first identifies confounding variables within data and then develops a distance-based data selection strategy for confounder-aware sampling with a constrained budget in the data size. We validate the superiority of our approach through extensive experiments across diverse medical imaging modalities, highlighting its effectiveness in addressing the substantial impact of confounding variables and enhancing the fine-tuning efficiency in the medical imaging domain, compared to other data selection approaches.</p>
  </details>
</details>
<details>
  <summary>152. <b>【2503.00743】Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00743">https://arxiv.org/abs/2503.00743</a></p>
  <p><b>作者</b>：Dilxat Muhtar,Enzhuo Zhang,Zhenshi Li,Feng Gu,Yanglangxing He,Pengfeng Xiao,Xueliang Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：interpreting remote sensing, demonstrated great potential, language-guided semantic understanding, remote sensing, demonstrated great</p>
  <p><b>备注</b>： 21 pages, 7 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Vision-Language Models (VLMs) have demonstrated great potential in interpreting remote sensing (RS) images through language-guided semantic understanding. However, the effectiveness of these VLMs critically depends on high-quality image-text training data that captures rich semantic relationships between visual content and language descriptions. Unlike natural images, RS lacks large-scale interleaved image-text pairs from web data, making data collection challenging. While current approaches rely primarily on rule-based methods or flagship VLMs for data synthesis, a systematic framework for automated quality assessment of such synthetically generated RS visionlanguage data is notably absent. To fill this gap, we propose a novel score model trained on large-scale RS visionlanguage preference data for automated quality assessment. Our empirical results demonstrate that fine-tuning CLIP or advanced VLMs (e.g., Qwen2-VL) with the top 30% of data ranked by our score model achieves superior interpretation accuracy compared to both full-data fine-tuning and CLIP-score-based ranking approaches. Furthermore, we demonstrate applications of our scoring model for reinforcement learning (RL) training and best-of-N (BoN) testtime scaling, enabling significant improvements in VLM performance for RS tasks.</p>
  </details>
</details>
<details>
  <summary>153. <b>【2503.00740】FaceShot: Bring Any Character into Life</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00740">https://arxiv.org/abs/2503.00740</a></p>
  <p><b>作者</b>：Junyao Gao,Yanan Sun,Fei Shen,Xin Jiang,Zhening Xing,Kai Chen,Cairong Zhao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：animation framework designed, fine-tuning or retraining, framework designed, designed to bring, portrait animation framework</p>
  <p><b>备注</b>： Published as a conference paper at ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In this paper, we present FaceShot, a novel training-free portrait animation framework designed to bring any character into life from any driven video without fine-tuning or retraining. We achieve this by offering precise and robust reposed landmark sequences from an appearance-guided landmark matching module and a coordinate-based landmark retargeting module. Together, these components harness the robust semantic correspondences of latent diffusion models to produce facial motion sequence across a wide range of character types. After that, we input the landmark sequences into a pre-trained landmark-driven animation model to generate animated video. With this powerful generalization capability, FaceShot can significantly extend the application of portrait animation by breaking the limitation of realistic portrait landmark detection for any stylized character and driven video. Also, FaceShot is compatible with any landmark-driven animation model, significantly improving overall performance. Extensive experiments on our newly constructed character benchmark CharacBench confirm that FaceShot consistently surpasses state-of-the-art (SOTA) approaches across any character domain. More results are available at our project website this https URL.</p>
  </details>
</details>
<details>
  <summary>154. <b>【2503.00737】Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00737">https://arxiv.org/abs/2503.00737</a></p>
  <p><b>作者</b>：Jinjiang You,Hewei Wang,Yijie Li,Mingxiao Huo,Long Van Tran Ha,Mingyuan Ma,Jinfeng Xu,Puzhen Wu,Shubham Garg,Wei Pu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Calibrating large-scale camera, typically requires dedicated, Calibrating large-scale, time-intensive and typically, typically requires</p>
  <p><b>备注</b>： 8 pages</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Calibrating large-scale camera arrays, such as those in dome-based setups, is time-intensive and typically requires dedicated captures of known patterns. While extrinsics in such arrays are fixed due to the physical setup, intrinsics often vary across sessions due to factors like lens adjustments or temperature changes. In this paper, we propose a dense-feature-driven multi-frame calibration method that refines intrinsics directly from scene data, eliminating the necessity for additional calibration captures. Our approach enhances traditional Structure-from-Motion (SfM) pipelines by introducing an extrinsics regularization term to progressively align estimated extrinsics with ground-truth values, a dense feature reprojection term to reduce keypoint errors by minimizing reprojection loss in the feature space, and an intrinsics variance term for joint optimization across multiple frames. Experiments on the Multiface dataset show that our method achieves nearly the same precision as dedicated calibration processes, and significantly enhances intrinsics and 3D reconstruction accuracy. Fully compatible with existing SfM pipelines, our method provides an efficient and practical plug-and-play solution for large-scale camera setups. Our code is publicly available at: this https URL</p>
  </details>
</details>
<details>
  <summary>155. <b>【2503.00736】Shazam: Unifying Multiple Foundation Models for Advanced Computational Pathology</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00736">https://arxiv.org/abs/2503.00736</a></p>
  <p><b>作者</b>：Wenhui Lei,Anqi Li,Yusheng Tan,Hanyu Chen,Xiaofan Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：achieving strong performance, histopathology image datasets, Foundation Models, achieving strong, significantly advanced</p>
  <p><b>备注</b>： 9 pages, 2 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Foundation Models (FMs) in computational pathology (CPath) have significantly advanced the extraction of meaningful features from histopathology image datasets, achieving strong performance across various clinical tasks. Despite their impressive performance, these models often exhibit variability when applied to different tasks, prompting the need for a unified framework capable of consistently excelling across various applications. In this work, we propose Shazam, a novel framework designed to efficiently combine multiple CPath models. Unlike previous approaches that train a fixed-parameter FM, Shazam dynamically extracts and refines information from diverse FMs for each specific task. To ensure that each FM contributes effectively without dominance, a novel distillation strategy is applied, guiding the student model with features from all teacher models, which enhances its generalization ability. Experimental results on two pathology patch classification datasets demonstrate that Shazam outperforms existing CPath models and other fusion methods. Its lightweight, flexible design makes it a promising solution for improving CPath analysis in real-world settings. Code will be available at this https URL.</p>
  </details>
</details>
<details>
  <summary>156. <b>【2503.00731】LightEndoStereo: A Real-time Lightweight Stereo Matching Method for Endoscopy Images</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00731">https://arxiv.org/abs/2503.00731</a></p>
  <p><b>作者</b>：Yang Ding,Can Han,Sijia Du,Yaqi Wang,Dahong Qian</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：minimally invasive surgery, automated robotic minimally, robotic minimally invasive, invasive surgery, accurate depth</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Real-time acquisition of accurate depth of scene is essential for automated robotic minimally invasive surgery, and stereo matching with binocular endoscopy can generate such depth. However, existing algorithms struggle with ambiguous tissue boundaries and real-time performance in prevalent high-resolution endoscopic scenes. We propose LightEndoStereo, a lightweight real-time stereo matching method for endoscopic images. We introduce a 3D Mamba Coordinate Attention module to streamline the cost aggregation process by generating position-sensitive attention maps and capturing long-range dependencies across spatial dimensions using the Mamba block. Additionally, we introduce a High-Frequency Disparity Optimization module to refine disparity estimates at tissue boundaries by enhancing high-frequency information in the wavelet domain. Our method is evaluated on the SCARED and SERV-CT datasets, achieving state-of-the-art matching accuracy and a real-time inference speed of 42 FPS. The code is available at this https URL.</p>
  </details>
</details>
<details>
  <summary>157. <b>【2503.00726】Enhancing Monocular 3D Scene Completion with Diffusion Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00726">https://arxiv.org/abs/2503.00726</a></p>
  <p><b>作者</b>：Changlin Song,Jiaqi Wang,Liyun Zhu,He Weng</p>
  <p><b>类目</b>：Graphics (cs.GR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Gaussian Splatting techniques, virtual reality, autonomous driving, enabling machines, complex environments</p>
  <p><b>备注</b>： All authors had equal contribution</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:3D scene reconstruction is essential for applications in virtual reality, robotics, and autonomous driving, enabling machines to understand and interact with complex environments. Traditional 3D Gaussian Splatting techniques rely on images captured from multiple viewpoints to achieve optimal performance, but this dependence limits their use in scenarios where only a single image is available. In this work, we introduce FlashDreamer, a novel approach for reconstructing a complete 3D scene from a single image, significantly reducing the need for multi-view inputs. Our approach leverages a pre-trained vision-language model to generate descriptive prompts for the scene, guiding a diffusion model to produce images from various perspectives, which are then fused to form a cohesive 3D reconstruction. Extensive experiments show that our method effectively and robustly expands single-image inputs into a comprehensive 3D scene, extending monocular 3D reconstruction capabilities without further training. Our code is available this https URL.</p>
  </details>
</details>
<details>
  <summary>158. <b>【2503.00703】owards hyperparameter-free optimization with differential privacy</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00703">https://arxiv.org/abs/2503.00703</a></p>
  <p><b>作者</b>：Zhiqi Bu,Ruixuan Liu</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Differential privacy, training deep learning, deep learning models, learning rate schedule, learning rate</p>
  <p><b>备注</b>： Accepted to ICLR 2025 spotlight</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparameter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performance on various language and vision tasks.</p>
  </details>
</details>
<details>
  <summary>159. <b>【2503.00697】CREATE-FFPE: Cross-Resolution Compensated and Multi-Frequency Enhanced FS-to-FFPE Stain Transfer for Intraoperative IHC Images</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00697">https://arxiv.org/abs/2503.00697</a></p>
  <p><b>作者</b>：Yiyang Lin,Danling Jiang,Xinyu Liu,Yun Miao,Yixuan Yuan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：IHC images, determine the benignity, benignity or malignancy, observe IHC images, intraoperative IHC images</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:In the immunohistochemical (IHC) analysis during surgery, frozen-section (FS) images are used to determine the benignity or malignancy of the tumor. However, FS image faces problems such as image contamination and poor nuclear detail, which may disturb the pathologist's diagnosis. In contrast, formalin-fixed and paraffin-embedded (FFPE) image has a higher staining quality, but it requires quite a long time to prepare and thus is not feasible during surgery. To help pathologists observe IHC images with high quality in surgery, this paper proposes a Cross-REsolution compensATed and multi-frequency Enhanced FS-to-FFPE (CREATE-FFPE) stain transfer framework, which is the first FS-to-FFPE method for the intraoperative IHC images. To solve the slide contamination and poor nuclear detail mentioned above, we propose the cross-resolution compensation module (CRCM) and the wavelet detail guidance module (WDGM). Specifically, CRCM compensates for information loss due to contamination by providing more tissue information across multiple resolutions, while WDGM produces the desirable details in a wavelet way, and the details can be used to guide the stain transfer to be more precise. Experiments show our method can beat all the competing methods on our dataset. In addition, the FID has decreased by 44.4%, and KID*100 has decreased by 71.2% by adding the proposed CRCM and WDGM in ablation studies, and the performance of a downstream microsatellite instability prediction task with public dataset can be greatly improved by performing our FS-to-FFPE stain transfer.</p>
  </details>
</details>
<details>
  <summary>160. <b>【2503.00695】MoSFormer: Augmenting Temporal Context with Memory of Surgery for Surgical Phase Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00695">https://arxiv.org/abs/2503.00695</a></p>
  <p><b>作者</b>：Hao Ding,Xu Lian,Mathias Unberath</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Surgical phase recognition, downstream applications, phase recognition, Surgical phase, Surgical</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Surgical phase recognition from video enables various downstream applications. Transformer-based sliding window approaches have set the state-of-the-art by capturing rich spatial-temporal features. However, while transformers can theoretically handle arbitrary-length sequences, in practice they are limited by memory and compute constraints, resulting in fixed context windows that struggle with maintaining temporal consistency across lengthy surgical procedures. This often leads to fragmented predictions and limited procedure-level understanding. To address these challenges, we propose Memory of Surgery (MoS), a framework that enriches temporal modeling by incorporating both semantic interpretable long-term surgical history and short-term impressions. MoSFormer, our enhanced transformer architecture, integrates MoS using a carefully designed encoding and fusion mechanism. We further introduce step filtering to refine history representation and develop a memory caching pipeline to improve training and inference stability, mitigating shortcut learning and overfitting. MoSFormer demonstrates state-of-the-art performance on multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains 88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7 recall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level accuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1 score. Further studies confirms the individual and combined benefits of long-term and short-term memory components through ablation and counterfactual inference. Qualitative results shows improved temporal consistency. The augmented temporal context enables procedure-level understanding, paving the way for more comprehensive surgical video analysis.</p>
  </details>
</details>
<details>
  <summary>161. <b>【2503.00677】Advancing Prompt-Based Methods for Replay-Independent General Continual Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00677">https://arxiv.org/abs/2503.00677</a></p>
  <p><b>作者</b>：Zhiqi Kang,Liyuan Wang,Xingxing Zhang,Karteek Alahari</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：General continual learning, real-world continual learning, blurry task boundaries, describe real-world continual, continual learning</p>
  <p><b>备注</b>： ICLR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:General continual learning (GCL) is a broad concept to describe real-world continual learning (CL) problems, which are often characterized by online data streams without distinct transitions between tasks, i.e., blurry task boundaries. Such requirements result in poor initial performance, limited generalizability, and severe catastrophic forgetting, heavily impacting the effectiveness of mainstream GCL models trained from scratch. While the use of a frozen pretrained backbone with appropriate prompt tuning can partially address these challenges, such prompt-based methods remain suboptimal for CL of remaining tunable parameters on the fly. In this regard, we propose an innovative approach named MISA (Mask and Initial Session Adaption) to advance prompt-based methods in GCL. It includes a forgetting-aware initial session adaption that employs pretraining data to initialize prompt parameters and improve generalizability, as well as a non-parametric logit mask of the output layers to mitigate catastrophic forgetting. Empirical results demonstrate substantial performance gains of our approach compared to recent competitors, especially without a replay buffer (e.g., up to 18.39%, 22.06%, and 11.96% performance lead on CIFAR-100, Tiny-ImageNet, and ImageNet-R, respectively). Moreover, our approach features the plug-in nature for prompt-based methods, independence of replay, ease of implementation, and avoidance of CL-relevant hyperparameters, serving as a strong baseline for GCL research. Our source code is publicly available at this https URL</p>
  </details>
</details>
<details>
  <summary>162. <b>【2503.00675】Dur360BEV: A Real-world Single 360-degree Camera Dataset and Benchmark for Bird-Eye View Mapping in Autonomous Driving</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00675">https://arxiv.org/abs/2503.00675</a></p>
  <p><b>作者</b>：Wenke E,Chao Yuan,Li Li,Yixin Sun,Yona Falinie A. Gaus,Amir Atapour-Abarghouei,Toby P. Breckon</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>163. <b>【2503.00670】ransformer Based Self-Context Aware Prediction for Few-Shot Anomaly Detection in Videos</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00670">https://arxiv.org/abs/2503.00670</a></p>
  <p><b>作者</b>：Gargi V. Pillai,Ashish Verma,Debashis Sen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：video anomaly detection, Anomaly detection, approach video anomaly, challenging task, task as anomalies</p>
  <p><b>备注</b>： Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Anomaly detection in videos is a challenging task as anomalies in different videos are of different kinds. Therefore, a promising way to approach video anomaly detection is by learning the non-anomalous nature of the video at hand. To this end, we propose a one-class few-shot learning driven transformer based approach for anomaly detection in videos that is self-context aware. Features from the first few consecutive non-anomalous frames in a video are used to train the transformer in predicting the non-anomalous feature of the subsequent frame. This takes place under the attention of a self-context learned from the input features themselves. After the learning, given a few previous frames, the video-specific transformer is used to infer if a frame is anomalous or not by comparing the feature predicted by it with the actual. The effectiveness of the proposed method with respect to the state-of-the-art is demonstrated through qualitative and quantitative results on different standard datasets. We also study the positive effect of the self-context used in our approach.</p>
  </details>
</details>
<details>
  <summary>164. <b>【2503.00665】Development of an Unpaired Deep Neural Network for Synthesizing X-ray Fluoroscopic Images from Digitally Reconstructed Tomography in Image Guided Radiotherapy</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00665">https://arxiv.org/abs/2503.00665</a></p>
  <p><b>作者</b>：Chisako Hayashi,Shinichiro Mori,Yasukuni Mori,Lim Taehyeung,Hiroki Suyari,Hitoshi Ishikawa</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)</p>
  <p><b>关键词</b>：deep neural network, digitally reconstructed radiography, FPD images, lung cancer treatment, generating flat-panel detector</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Purpose The purpose of this study was to develop and evaluate a deep neural network (DNN) capable of generating flat-panel detector (FPD) images from digitally reconstructed radiography (DRR) images in lung cancer treatment, with the aim of improving clinical workflows in image-guided radiotherapy.
Methods A modified CycleGAN architecture was trained on paired DRR-FPD image data obtained from patients with lung tumors. The training dataset consisted of over 400 DRR-FPD image pairs, and the final model was evaluated on an independent set of 100 FPD images. Mean absolute error (MAE), peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and Kernel Inception Distance (KID) were used to quantify the similarity between synthetic and ground-truth FPD images. Computation time for generating synthetic images was also measured.
Results Despite some positional mismatches in the DRR-FPD pairs, the synthetic FPD images closely resembled the ground-truth FPD images. The proposed DNN achieved notable improvements over both input DRR images and a U-Net-based method in terms of MAE, PSNR, SSIM, and KID. The average image generation time was on the order of milliseconds per image, indicating its potential for real-time application. Qualitative evaluations showed that the DNN successfully reproduced image noise patterns akin to real FPD images, reducing the need for manual noise adjustments.
Conclusions The proposed DNN effectively converted DRR images into realistic FPD images for thoracic cases, offering a fast and practical method that could streamline patient setup verification and enhance overall clinical workflow. Future work should validate the model across different imaging systems and address remaining challenges in marker visualization, thereby fostering broader clinical adoption.
</p><p>Subjects:</p>
<p>Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Medical Physics (physics.med-ph)</p>
<p>Cite as:<br>
arXiv:2503.00665 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]</p>
<p>(or<br>
arXiv:2503.00665v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)</p>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2503.00665">https://doi.org/10.48550/arXiv.2503.00665</a></p>
<p>Focus to learn more</p>
<pre><code>              arXiv-issued DOI via DataCite (pending registration)
</code></pre>
<p>Submission history From: Shinichiro Mori [view email]       [v1]<br>
Sat, 1 Mar 2025 23:34:43 UTC (2,380 KB)</p><p></p>
  </details>
</details>
<details>
  <summary>165. <b>【2503.00643】Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00643">https://arxiv.org/abs/2503.00643</a></p>
  <p><b>作者</b>：Yante Li,Hanwen Qi,Haoyu Chen,Xinlian Liang,Guoying Zhao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 10 pages, 6 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>166. <b>【2503.00642】Self-supervision via Controlled Transformation and Unpaired Self-conditioning for Low-light Image Enhancement</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00642">https://arxiv.org/abs/2503.00642</a></p>
  <p><b>作者</b>：Aupendu Kar,Sobhan K. Dhara,Debashis Sen,Prabir K. Biswas</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：Real-world low-light images, imaging devices suffer, produce artifact-free outputs, Real-world low-light, low-light images captured</p>
  <p><b>备注</b>： Copyright 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Real-world low-light images captured by imaging devices suffer from poor visibility and require a domain-specific enhancement to produce artifact-free outputs that reveal details. In this paper, we propose an unpaired low-light image enhancement network leveraging novel controlled transformation-based self-supervision and unpaired self-conditioning strategies. The model determines the required degrees of enhancement at the input image pixels, which are learned from the unpaired low-lit and well-lit images without any direct supervision. The self-supervision is based on a controlled transformation of the input image and subsequent maintenance of its enhancement in spite of the transformation. The self-conditioning performs training of the model on unpaired images such that it does not enhance an already-enhanced image or a well-lit input image. The inherent noise in the input low-light images is handled by employing low gradient magnitude suppression in a detail-preserving manner. In addition, our noise handling is self-conditioned by preventing the denoising of noise-free well-lit images. The training based on low-light image enhancement-specific attributes allows our model to avoid paired supervision without compromising significantly in performance. While our proposed self-supervision aids consistent enhancement, our novel self-conditioning facilitates adequate enhancement. Extensive experiments on multiple standard datasets demonstrate that our model, in general, outperforms the state-of-the-art both quantitatively and subjectively. Ablation studies show the effectiveness of our self-supervision and self-conditioning strategies, and the related loss functions.</p>
  </details>
</details>
<details>
  <summary>167. <b>【2503.00641】How to Probe: Simple Yet Effective Techniques for Improving Post-hoc Explanations</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00641">https://arxiv.org/abs/2503.00641</a></p>
  <p><b>作者</b>：Siddhartha Gairola,Moritz Böhle,Francesco Locatello,Bernt Schiele</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：Deep Neural Networks, Deep Neural, Neural Networks, Post-hoc importance attribution, popular tool</p>
  <p><b>备注</b>： Accepted as poster at ICLR 2025. GitHub Link: [this https URL](https://github.com/sidgairo18/how-to-probe) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Post-hoc importance attribution methods are a popular tool for "explaining" Deep Neural Networks (DNNs) and are inherently based on the assumption that the explanations can be applied independently of how the models were trained. Contrarily, in this work we bring forward empirical evidence that challenges this very notion. Surprisingly, we discover a strong dependency on and demonstrate that the training details of a pre-trained model's classification layer (less than 10 percent of model parameters) play a crucial role, much more than the pre-training scheme itself. This is of high practical relevance: (1) as techniques for pre-training models are becoming increasingly diverse, understanding the interplay between these techniques and attribution methods is critical; (2) it sheds light on an important yet overlooked assumption of post-hoc attribution methods which can drastically impact model explanations and how they are interpreted eventually. With this finding we also present simple yet effective adjustments to the classification layers, that can significantly enhance the quality of model explanations. We validate our findings across several visual pre-training frameworks (fully-supervised, self-supervised, contrastive vision-language training) and analyse how they impact explanations for a wide range of attribution methods on a diverse set of evaluation metrics.</p>
  </details>
</details>
<details>
  <summary>168. <b>【2503.00625】Perceptual Visual Quality Assessment: Principles, Methods, and Future Directions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00625">https://arxiv.org/abs/2503.00625</a></p>
  <p><b>作者</b>：Wei Zhou,Hadi Amirpour,Christian Timmerer,Guangtao Zhai,Patrick Le Callet,Alan C. Bovik</p>
  <p><b>类目</b>：Multimedia (cs.MM); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： A tutorial and review</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>169. <b>【2503.00605】GenVDM: Generating Vector Displacement Maps From a Single Image</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00605">https://arxiv.org/abs/2503.00605</a></p>
  <p><b>作者</b>：Yuezhi Yang,Qimin Chen,Vladimir G. Kim,Siddhartha Chaudhuri,Qixing Huang,Zhiqin Chen</p>
  <p><b>类目</b>：Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： accepted to CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>170. <b>【2503.00591】AesthetiQ: Enhancing Graphic Layout Design via Aesthetic-Aware Preference Alignment of Multi-modal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00591">https://arxiv.org/abs/2503.00591</a></p>
  <p><b>作者</b>：Sohan Patnaik,Rishabh Jain,Balaji Krishnamurthy,Mausoom Sarkar</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted for publication in CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>171. <b>【2503.00548】Unbiased Video Scene Graph Generation via Visual and Semantic Dual Debiasing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00548">https://arxiv.org/abs/2503.00548</a></p>
  <p><b>作者</b>：Yanjun Li,Zhaoyang Li,Honghui Chen,Lizhi Xu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 17 pages, 8 figures, CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>172. <b>【2503.00545】RFWNet: A Lightweight Remote Sensing Object Detector Integrating Multi-Scale Receptive Fields and Foreground Focus Mechanism</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00545">https://arxiv.org/abs/2503.00545</a></p>
  <p><b>作者</b>：Yujie Lei,Wenjie Sun,Sen Jia,Qingquan Li,Jie Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>173. <b>【2503.00540】Streaming Video Question-Answering with In-context Video KV-Cache Retrieval</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00540">https://arxiv.org/abs/2503.00540</a></p>
  <p><b>作者</b>：Shangzhe Di,Zhelun Yu,Guanghao Zhang,Haoyuan Li,Tao Zhong,Hao Cheng,Bolin Li,Wanggui He,Fangxun Shu,Hao Jiang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted to ICLR 2025. Code: [this https URL](https://github.com/Becomebright/ReKV) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>174. <b>【2503.00531】GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00531">https://arxiv.org/abs/2503.00531</a></p>
  <p><b>作者</b>：Runyi Li,Xuanyu Zhang,Chuhan Tong,Zhipei Xu,Jian Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>175. <b>【2503.00528】Efficient Prompting for Continual Adaptation to Missing Modalities</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00528">https://arxiv.org/abs/2503.00528</a></p>
  <p><b>作者</b>：Zirun Guo,Shulei Wang,Wang Lin,Weicai Yan,Yangyang Wu,Tao Jin</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted to NAACL 2025 Main</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>176. <b>【2503.00521】2DMCG:2DMambawith Change Flow Guidance for Change Detection in Remote Sensing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00521">https://arxiv.org/abs/2503.00521</a></p>
  <p><b>作者</b>：JunYao Kaung,HongWei Ge</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>177. <b>【2503.00518】Explainable LiDAR 3D Point Cloud Segmentation and Clustering for Detecting Airplane-Generated Wind Turbulence</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00518">https://arxiv.org/abs/2503.00518</a></p>
  <p><b>作者</b>：Zhan Qu,Shuzhou Yuan,Michael Färber,Marius Brennfleck,Niklas Wartha,Anton Stephan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted at KDD 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>178. <b>【2503.00516】wo-stream Beats One-stream: Asymmetric Siamese Network for Efficient Visual Tracking</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00516">https://arxiv.org/abs/2503.00516</a></p>
  <p><b>作者</b>：Jiawen Zhu,Huayi Tang,Xin Chen,Xinying Wang,Dong Wang,Huchuan Lu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by AAAI2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>179. <b>【2503.00515】Class-Independent Increment: An Efficient Approach for Multi-label Class-Incremental Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00515">https://arxiv.org/abs/2503.00515</a></p>
  <p><b>作者</b>：Songlin Dong,Yuhang He,Zhengdong Zhou,Haoyu Luo,Xing Wei,Alex C. Kot,Yihong Gong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>180. <b>【2503.00513】Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00513">https://arxiv.org/abs/2503.00513</a></p>
  <p><b>作者</b>：Hanxun Yu,Wentong Li,Song Wang,Junbo Chen,Jianke Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： CVPR2025, Code Link: [this https URL](https://github.com/hanxunyu/Inst3D-LMM) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>181. <b>【2503.00495】owards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00495">https://arxiv.org/abs/2503.00495</a></p>
  <p><b>作者</b>：Xuanchen Li,Jianyu Wang,Yuhao Cheng,Yikun Zeng,Xingyu Ren,Wenhan Zhu,Weiming Zhao,Yichao Yan</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>182. <b>【2503.00477】SDW: A Tri-Stream Dynamic Weight Network for Cloth-Changing Person Re-Identification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00477">https://arxiv.org/abs/2503.00477</a></p>
  <p><b>作者</b>：Ruiqi He,Zihan Wang,Xiang Zhou</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>183. <b>【2503.00470】Rapid morphology characterization of two-dimensional TMDs and lateral heterostructures based on deep learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00470">https://arxiv.org/abs/2503.00470</a></p>
  <p><b>作者</b>：Junqi He,Yujie Zhang,Jialu Wang,Tao Wang,Pan Zhang,Chengjie Cai,Jinxing Yang,Xiao Lin,Xiaohui Yang</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Computer Vision and Pattern Recognition (cs.CV); Optics (physics.optics)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>184. <b>【2503.00467】Adaptive Rectangular Convolution for Remote Sensing Pansharpening</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00467">https://arxiv.org/abs/2503.00467</a></p>
  <p><b>作者</b>：Xueyang Wang,Zhixin Zheng,Jiandong Shao,Yule Duan,Liang-Jian Deng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 8 pages, 6 figures, Accepted by CVPR</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>185. <b>【2503.00466】Bring Your Own Grasp Generator: Leveraging Robot Grasp Generation for Prosthetic Grasping</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00466">https://arxiv.org/abs/2503.00466</a></p>
  <p><b>作者</b>：Giuseppe Stracquadanio,Federico Vasile,Elisa Maiettini,Nicolò Boccardo,Lorenzo Natale</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted to ICRA 2025. Project Website: [this https URL](https://hsp-iit.github.io/byogg/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>186. <b>【2503.00458】Using Machine Learning for move sequence visualization and generation in climbing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00458">https://arxiv.org/abs/2503.00458</a></p>
  <p><b>作者</b>：Thomas Rimbot,Martin Jaggi,Luis Barba</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>187. <b>【2503.00453】Deep Learning based approach to detect Customer Age, Gender and Expression in Surveillance Video</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00453">https://arxiv.org/abs/2503.00453</a></p>
  <p><b>作者</b>：Earnest Paul Ijjina,Goutham Kanahasabai,Aniruddha Srinivas Joshi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>188. <b>【2503.00452】Customer Analytics using Surveillance Video</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00452">https://arxiv.org/abs/2503.00452</a></p>
  <p><b>作者</b>：Earnest Paul Ijjina,Aniruddha Srinivas Joshi,Goutham Kanahasabai,Keerthi Priyanka P</p>
  <p><b>类目</b>：Computers and Society (cs.CY); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>189. <b>【2503.00450】Ranking pre-trained segmentation models for zero-shot transferability</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00450">https://arxiv.org/abs/2503.00450</a></p>
  <p><b>作者</b>：Joshua Talks,Anna Kreshuk</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 11 pages, 3 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>190. <b>【2503.00442】Detection of Customer Interested Garments in Surveillance Video using Computer Vision</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00442">https://arxiv.org/abs/2503.00442</a></p>
  <p><b>作者</b>：Earnest Paul Ijjina,Aniruddha Srinivas Joshi,Goutham Kanahasabai</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>191. <b>【2503.00441】Split Adaptation for Pre-trained Vision Transformers</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00441">https://arxiv.org/abs/2503.00441</a></p>
  <p><b>作者</b>：Lixu Wang,Bingqi Shang,Yi Li,Payal Mohapatra,Wei Dong,Xiao Wang,Qi Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： This paper has been accepted by CVPR 2025. The first two authors contributed equally</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>192. <b>【2503.00436】HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00436">https://arxiv.org/abs/2503.00436</a></p>
  <p><b>作者</b>：Maria Lymperaiou,Giorgos FIlandrianos,Angeliki Dimitriou,Athanasios Voulodimos,Giorgos Stamou</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>193. <b>【2503.00429】DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00429">https://arxiv.org/abs/2503.00429</a></p>
  <p><b>作者</b>：Jingyi Yang,Xun Lin,Zitong Yu,Liepiao Zhang,Xin Liu,Hui Li,Xiaochen Yuan,Xiaochun Cao</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 18 pages, 9 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>194. <b>【2503.00428】DashCop: Automated E-ticket Generation for Two-Wheeler Traffic Violations Using Dashcam Videos</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00428">https://arxiv.org/abs/2503.00428</a></p>
  <p><b>作者</b>：Deepti Rawat,Keshav Gupta,Aryamaan Basu Roy,Ravi Kiran Sarvadevabhatla</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>195. <b>【2503.00414】SGC-Net: Stratified Granular Comparison Network for Open-Vocabulary HOI Detection</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00414">https://arxiv.org/abs/2503.00414</a></p>
  <p><b>作者</b>：Xin Lin,Chong Shi,Zuopeng Yang,Haojin Tang,Zhili Zhou</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>196. <b>【2503.00413】CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00413">https://arxiv.org/abs/2503.00413</a></p>
  <p><b>作者</b>：Tianyu Huai,Jie Zhou,Xingjiao Wu,Qin Chen,Qingchun Bai,Ze Zhou,Liang He</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 10 pages,4 figures,accepted by CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>197. <b>【2503.00410】High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00410">https://arxiv.org/abs/2503.00410</a></p>
  <p><b>作者</b>：Zhaoyi Tian,Feifeng Wang,Shiwei Wang,Zihao Zhou,Yao Zhu,Liquan Shen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>198. <b>【2503.00401】Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with Query-Oriented Pivot Tasks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00401">https://arxiv.org/abs/2503.00401</a></p>
  <p><b>作者</b>：Zongru Wu,Pengzhou Cheng,Zheng Wu,Tianjie Ju,Zhuosheng Zhang,Gongshen Liu</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>199. <b>【2503.00400】Inteval Analysis for two spherical functions arising from robust Perspective-n-Lines problem</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00400">https://arxiv.org/abs/2503.00400</a></p>
  <p><b>作者</b>：Xiang Zheng,Haodong Jiang,Junfeng Wu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>200. <b>【2503.00399】aming Large Multimodal Agents for Ultra-low Bitrate Semantically Disentangled Image Compression</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00399">https://arxiv.org/abs/2503.00399</a></p>
  <p><b>作者</b>：Juan Song,Lijie Yang,Mingtao Feng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted to CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>201. <b>【2503.00397】Floorplan-SLAM: A Real-Time, High-Accuracy, and Long-Term Multi-Session Point-Plane SLAM for Efficient Floorplan Reconstruction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00397">https://arxiv.org/abs/2503.00397</a></p>
  <p><b>作者</b>：Haolin Wang,Zeren Lv,Hao Wei,Haijiang Zhu,Yihong Wu</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>202. <b>【2503.00389】BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00389">https://arxiv.org/abs/2503.00389</a></p>
  <p><b>作者</b>：Yuto Shibata,Yusuke Oumi,Go Irie,Akisato Kimura,Yoshimitsu Aoki,Mariko Isogawa</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>203. <b>【2503.00384】A Survey of Adversarial Defenses in Vision-based Systems: Categorization, Methods and Challenges</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00384">https://arxiv.org/abs/2503.00384</a></p>
  <p><b>作者</b>：Nandish Chattopadhyay,Abdul Basit,Bassem Ouni,Muhammad Shafique</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>204. <b>【2503.00382】EigenActor: Variant Body-Object Interaction Generation Evolved from Invariant Action Basis Reasoning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00382">https://arxiv.org/abs/2503.00382</a></p>
  <p><b>作者</b>：Xuehao Gao,Yang Yang,Shaoyi Du,Yang Wu,Yebin Liu,Guo-Jun Qi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>205. <b>【2503.00377】Adversarial Attacks on Event-Based Pedestrian Detectors: A Physical Approach</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00377">https://arxiv.org/abs/2503.00377</a></p>
  <p><b>作者</b>：Guixu Lin,Muyao Niu,Qingtian Zhu,Zhengwei Yin,Zhuoxiao Li,Shengfeng He,Yinqiang Zheng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by AAAI 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>206. <b>【2503.00376】Few-shot crack image classification using clip based on bayesian optimization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00376">https://arxiv.org/abs/2503.00376</a></p>
  <p><b>作者</b>：Yingchao Zhang,Cheng Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 5 pages, 5 figures, 3 tables, submit to the 1st International Workshop on Bayesian Approach in Civil Engineering (IWOBA 2025)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>207. <b>【2503.00374】MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00374">https://arxiv.org/abs/2503.00374</a></p>
  <p><b>作者</b>：Tianyi Wang,Jianan Fan,Dingxin Zhang,Dongnan Liu,Yong Xia,Heng Huang,Weidong Cai</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 10 pages, 5 figures, 3 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>208. <b>【2503.00371】Jointly Understand Your Command and Intention:Reciprocal Co-Evolution between Scene-Aware 3D Human Motion Synthesis and Analysis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00371">https://arxiv.org/abs/2503.00371</a></p>
  <p><b>作者</b>：Xuehao Gao,Yang Yang,Shaoyi Du,Guo-Jun Qi,Junwei Han</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>209. <b>【2503.00364】CFSum: A Transformer-Based Multi-Modal Video Summarization Framework With Coarse-Fine Fusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00364">https://arxiv.org/abs/2503.00364</a></p>
  <p><b>作者</b>：Yaowei Guo,Jiazheng Xing,Xiaojun Hou,Shuo Xin,Juntao Jiang,Demetri Terzopoulos,Chenfanfu Jiang,Yong Liu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>210. <b>【2503.00361】Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00361">https://arxiv.org/abs/2503.00361</a></p>
  <p><b>作者</b>：Wei Suo,Lijun Zhang,Mengyang Sun,Lin Yuanbo Wu,Peng Wang,Yanning Zhang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>211. <b>【2503.00359】Solving Instance Detection from an Open-World Perspective</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00359">https://arxiv.org/abs/2503.00359</a></p>
  <p><b>作者</b>：Qianqian Shen,Yunhan Zhao,Nahyun Kwon,Jeeeun Kim,Yanan Li,Shu Kong</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted at CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>212. <b>【2503.00357】CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00357">https://arxiv.org/abs/2503.00357</a></p>
  <p><b>作者</b>：Yu-Ting Zhan,Cheng-Yuan Ho,Hebi Yang,Yi-Hsin Chen,Jui Chiu Chiang,Yu-Lun Liu,Wen-Hsiao Peng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted for Publication in International Conference on Learning Representations (ICLR)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>213. <b>【2503.00348】SHAZAM: Self-Supervised Change Monitoring for Hazard Detection and Mapping</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00348">https://arxiv.org/abs/2503.00348</a></p>
  <p><b>作者</b>：Samuel Garske,Konrad Heidler,Bradley Evans,KC Wong,Xiao Xiang Zhu</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 20 pages, 9 figures, 3 tables, code available at: [this https URL](https://github.com/WiseGamgee/SHAZAM) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>214. <b>【2503.00329】ABC: Achieving Better Control of Multimodal Embeddings using VLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00329">https://arxiv.org/abs/2503.00329</a></p>
  <p><b>作者</b>：Benjamin Schneider,Florian Kerschbaum,Wenhu Chen</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>215. <b>【2503.00325】CADRef: Robust Out-of-Distribution Detection via Class-Aware Decoupled Relative Feature Leveraging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00325">https://arxiv.org/abs/2503.00325</a></p>
  <p><b>作者</b>：Zhiwei Ling,Yachen Chang,Hailiang Zhao,Xinkui Zhao,Kingsum Chow,Shuiguang Deng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： This paper has been accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>216. <b>【2503.00308】Abstract Rendering: Computing All Seen in Gaussian Splat Scenes</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00308">https://arxiv.org/abs/2503.00308</a></p>
  <p><b>作者</b>：Yangge Li,Chenxi Ji,Xiangru Zhong,Huan Zhang,Sayan Mitra</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>217. <b>【2503.00301】Differential Coding for Training-Free ANN-to-SNN Conversion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00301">https://arxiv.org/abs/2503.00301</a></p>
  <p><b>作者</b>：Zihan Huang,Wei Fang,Tong Bu,Peng Xue,Zecheng Hao,Wenxuan Liu,Yuanhong Tang,Zhaofei Yu,Tiejun Huang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>218. <b>【2503.00276】Learning to Animate Images from A Few Videos to Portray Delicate Human Actions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00276">https://arxiv.org/abs/2503.00276</a></p>
  <p><b>作者</b>：Haoxin Li,Yingchen Yu,Qilong Wu,Hanwang Zhang,Boyang Li,Song Bai</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>219. <b>【2503.00266】Flow Matching for Medical Image Synthesis: Bridging the Gap Between Speed and Quality</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00266">https://arxiv.org/abs/2503.00266</a></p>
  <p><b>作者</b>：Milad Yazdani,Yasamin Medghalchi,Pooria Ashrafian,Ilker Hacihaliloglu,Dena Shahriari</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>220. <b>【2503.00260】Seeing A 3D World in A Grain of Sand</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00260">https://arxiv.org/abs/2503.00260</a></p>
  <p><b>作者</b>：Yufan Zhang,Yu Ji,Yu Guo,Jinwei Ye</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>221. <b>【2503.00250】Solar Multimodal Transformer: Intraday Solar Irradiance Predictor using Public Cameras and Time Series</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00250">https://arxiv.org/abs/2503.00250</a></p>
  <p><b>作者</b>：Yanan Niu,Roy Sarkis,Demetri Psaltis,Mario Paolone,Christophe Moser,Luisa Lambertini</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： WACV2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>222. <b>【2503.00232】ransformers with Joint Tokens and Local-Global Attention for Efficient Human Pose Estimation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00232">https://arxiv.org/abs/2503.00232</a></p>
  <p><b>作者</b>：Kaleab A. Kinfu,René Vidal</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： This work has been submitted to the IEEE for possible publication</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>223. <b>【2503.00228】Seeing Eye to AI? Applying Deep-Feature-Based Similarity Metrics to Information Visualization</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00228">https://arxiv.org/abs/2503.00228</a></p>
  <p><b>作者</b>：Sheng Long,Angelos Chatzimparmpas,Emma Alexander,Matthew Kay,Jessica Hullman</p>
  <p><b>类目</b>：Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted at CHI 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>224. <b>【2503.00226】Spiking Transformer:Introducing Accurate Addition-Only Spiking Self-Attention for Transformer</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00226">https://arxiv.org/abs/2503.00226</a></p>
  <p><b>作者</b>：Yufei Guo,Xiaode Liu,Yuanpei Chen,Weihang Peng,Yuhan Zhang,Zhe Ma</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by CVPR2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>225. <b>【2503.00210】Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00210">https://arxiv.org/abs/2503.00210</a></p>
  <p><b>作者</b>：Wenrui Fan,L. M. Riza Rizky,Jiayang Zhang,Chen Chen,Haiping Lu,Kevin Teh,Dinesh Selvarajah,Shuo Zhou</p>
  <p><b>类目</b>：Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>226. <b>【2503.00202】MIDAS: Mixing Ambiguous Data with Soft Labels for Dynamic Facial Expression Recognition</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00202">https://arxiv.org/abs/2503.00202</a></p>
  <p><b>作者</b>：Ryosuke Kawamura,Hideaki Hayashi,Noriko Takemura,Hajime Nagahara</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted at WACV2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>227. <b>【2503.00200】Unified Video Action Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00200">https://arxiv.org/abs/2503.00200</a></p>
  <p><b>作者</b>：Shuang Li,Yihuai Gao,Dorsa Sadigh,Shuran Song</p>
  <p><b>类目</b>：Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Project website: [this https URL](https://unified-video-action-model.github.io/) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>228. <b>【2503.00196】PRISM: High-Resolution  Precise Counterfactual Medical Image Generation using Language-guided Stable Diffusion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00196">https://arxiv.org/abs/2503.00196</a></p>
  <p><b>作者</b>：Amar Kumar,Anita Kriz,Mohammad Havaei,Tal Arbel</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Under Review for MIDL 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>229. <b>【2503.00171】PaliGemma-CXR: A Multi-task Multimodal Model for TB Chest X-ray Interpretation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00171">https://arxiv.org/abs/2503.00171</a></p>
  <p><b>作者</b>：Denis Musinguzi,Andrew Katumba,Sudi Murindanyi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>230. <b>【2503.00168】SSL4EO-S12 v1.1: A Multimodal, Multiseasonal Dataset for Pretraining, Updated</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00168">https://arxiv.org/abs/2503.00168</a></p>
  <p><b>作者</b>：Benedikt Blumenstiel,Nassim Ait Ali Braham,Conrad M Albrecht,Stefano Maurogiovanni,Paolo Fraccaro</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>231. <b>【2503.00167】EVLoc: Event-based Visual Localization in LiDAR Maps via Event-Depth Registration</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00167">https://arxiv.org/abs/2503.00167</a></p>
  <p><b>作者</b>：Kuangyi Chen,Jun Zhang,Friedrich Fraundorfer</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>232. <b>【2503.00162】PreMind: Multi-Agent Video Understanding for Advanced Indexing of Presentation-style Videos</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00162">https://arxiv.org/abs/2503.00162</a></p>
  <p><b>作者</b>：Kangda Wei,Zhengyu Zhou,Bingqing Wang,Jun Araki,Lukas Lange,Ruihong Huang,Zhe Feng</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>233. <b>【2503.00147】Precise Event Spotting in Sports Videos: Solving Long-Range Dependency and Class Imbalance</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00147">https://arxiv.org/abs/2503.00147</a></p>
  <p><b>作者</b>：Sanchayan Santra,Vishal Chudasama,Pankaj Wasnik,Vineeth N. Balasubramanian</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted at CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>234. <b>【2503.00136】Conformal Risk Control for Semantic Uncertainty Quantification in Computed Tomography</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00136">https://arxiv.org/abs/2503.00136</a></p>
  <p><b>作者</b>：Jacopo Teneggi,J Webster Stayman,Jeremias Sulam</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>235. <b>【2503.00132】CNSv2: Probabilistic Correspondence Encoded Neural Image Servo</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00132">https://arxiv.org/abs/2503.00132</a></p>
  <p><b>作者</b>：Anzhe Chen,Hongxiang Yu,Shuxin Li,Yuxi Chen,Zhongxiang Zhou,Wentao Sun,Rong Xiong,Yue Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>236. <b>【2503.00122】Unmanned Aerial Vehicle (UAV)-Based Mapping of Iris Pseudacorus L. Invasion in Laguna del Sauce (Uruguay) Coast</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00122">https://arxiv.org/abs/2503.00122</a></p>
  <p><b>作者</b>：Alejo Silvarrey,Pablo Negri</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Emerging Technologies (cs.ET)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 14 pages, 10 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>237. <b>【2503.00086】Generalization of CNNs on Relational Reasoning with Bar Charts</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00086">https://arxiv.org/abs/2503.00086</a></p>
  <p><b>作者</b>：Zhenxing Cui,Lu Chen,Yunhai Wang,Daniel Haehn,Yong Wang,Hanspeter Pfister</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by TVCG. GitHub repository: [this https URL](https://github.com/Ideas-Laboratory/Graphical-Perception) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>238. <b>【2503.00073】Forecasting Whole-Brain Neuronal Activity from Volumetric Video</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00073">https://arxiv.org/abs/2503.00073</a></p>
  <p><b>作者</b>：Alexander Immer,Jan-Matthis Lueckmann,Alex Bo-Yuan Chen,Peter H. Li,Mariela D. Petkova,Nirmala A. Iyer,Aparna Dev,Gudrun Ihrke,Woohyun Park,Alyson Petruncio,Aubrey Weigel,Wyatt Korff,Florian Engert,Jeff W. Lichtman,Misha B. Ahrens,Viren Jain,Michał Januszewski</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>239. <b>【2503.00071】I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00071">https://arxiv.org/abs/2503.00071</a></p>
  <p><b>作者</b>：Esam Ghaleb,Bulat Khaertdinov,Aslı Özyürek,Raquel Fernández</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Multimedia (cs.MM)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>240. <b>【2503.00068】PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00068">https://arxiv.org/abs/2503.00068</a></p>
  <p><b>作者</b>：Ziyu Wu,Yufan Xiong,Mengting Niu,Fangting Xie,Quan Wan,Qijun Ying,Boyan Liu,Xiaohui Cai</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Emerging Technologies (cs.ET)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepeted by CVPR2025 (submitted version)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>241. <b>【2503.00063】NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00063">https://arxiv.org/abs/2503.00063</a></p>
  <p><b>作者</b>：Zezeng Li,Xiaoyu Du,Na Lei,Liming Chen,Weimin Wang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>242. <b>【2503.00060】SAC-ViT: Semantic-Aware Clustering Vision Transformer with Early Exit</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00060">https://arxiv.org/abs/2503.00060</a></p>
  <p><b>作者</b>：Youbing Hu,Yun Cheng,Anqi Lu,Dawei Wei,Zhijun Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>243. <b>【2503.00059】Investigating and Enhancing Vision-Audio Capability in Omnimodal Large Language Models</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00059">https://arxiv.org/abs/2503.00059</a></p>
  <p><b>作者</b>：Rui Hu,Delai Qiu,Shuyu Wei,Jiaming Zhang,Yining Wang,Shengping Liu,Jitao Sang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>244. <b>【2503.00058】African Gender Classification Using Clothing Identification Via Deep Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00058">https://arxiv.org/abs/2503.00058</a></p>
  <p><b>作者</b>：Samuel Ozechi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 3 Pages, 10 Figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>245. <b>【2503.00057】Improved YOLOv12 with LLM-Generated Synthetic Data for Enhanced Apple Detection and Benchmarking Against YOLOv11 and YOLOv10</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00057">https://arxiv.org/abs/2503.00057</a></p>
  <p><b>作者</b>：Ranjan Sapkota,Manoj Karkee</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 10 pages, 5 Figures, 2 Tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>246. <b>【2503.00054】Deciphering the complaint aspects: Towards an aspect-based complaint identification model with video complaint dataset in finance</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00054">https://arxiv.org/abs/2503.00054</a></p>
  <p><b>作者</b>：Sarmistha Das,Basha Mujavarsheik,R E Zera Lyngkhoi,Sriparna Saha,Alka Maurya</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>247. <b>【2503.00052】RURA-Net: A general disease diagnosis method based on Zero-Shot Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00052">https://arxiv.org/abs/2503.00052</a></p>
  <p><b>作者</b>：Yan Su,Qiulin Wu,Weizhen Li,Chengchang Pan,Honggang Qi</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 10 pages, 3 figures, 6 tables, submitted to The 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2025)</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>248. <b>【2503.00051】Correspondence-Free Pose Estimation with Patterns: A Unified Approach for Multi-Dimensional Vision</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00051">https://arxiv.org/abs/2503.00051</a></p>
  <p><b>作者</b>：Quan Quan,Dun Dai</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>249. <b>【2503.00049】Omni-SILA: Towards Omni-scene Driven Visual Sentiment Identifying, Locating and Attributing in Videos</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00049">https://arxiv.org/abs/2503.00049</a></p>
  <p><b>作者</b>：Jiamin Luo,Jingjing Wang,Junxiao Ma,Yujie Jin,Shoushan Li,Guodong Zhou</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>250. <b>【2503.00046】Leveraging Large Models for Evaluating Novel Content: A Case Study on Advertisement Creativity</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00046">https://arxiv.org/abs/2503.00046</a></p>
  <p><b>作者</b>：Zhaoyi Joey Hou,Adriana Kovashka,Xiang Lorraine Li</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>251. <b>【2503.00044】Advanced YOLO-based Real-time Power Line Detection for Vegetation Management</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00044">https://arxiv.org/abs/2503.00044</a></p>
  <p><b>作者</b>：Shuaiang Rong,Lina He,Salih Furkan Atici,Ahmet Enis Cetin</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 13 pages. Revised version submitted to IEEE Transaction on Power Delivery</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>252. <b>【2503.00043】VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00043">https://arxiv.org/abs/2503.00043</a></p>
  <p><b>作者</b>：Nilay Yilmaz,Maitreya Patel,Yiran Lawrence Luo,Tejas Gokhale,Chitta Baral,Suren Jayasuriya,Yezhou Yang</p>
  <p><b>类目</b>：Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted at ICLR 2025. Code and data: [this https URL](https://github.com/nlylmz/Voila) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>253. <b>【2503.00040】Memory-Free and Parallel Computation for Quantized Spiking Neural Networks</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00040">https://arxiv.org/abs/2503.00040</a></p>
  <p><b>作者</b>：Dehao Zhang,Shuai Wang,Yichen Xiao,Wenjie Wei,Yimeng Shan,Malu Zhang,Yang Yang</p>
  <p><b>类目</b>：Neural and Evolutionary Computing (cs.NE); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>254. <b>【2503.00037】Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00037">https://arxiv.org/abs/2503.00037</a></p>
  <p><b>作者</b>：Wei Zhao,Zhe Li,Yige Li,Jun Sun</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>255. <b>【2503.00020】A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00020">https://arxiv.org/abs/2503.00020</a></p>
  <p><b>作者</b>：Rakeen Rouf,Trupti Bavalatti,Osama Ahmed,Dhaval Potdar,Faraz Jawed</p>
  <p><b>类目</b>：Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted for publication in IEEE Access, DOI: [https://doi.org/10.1109/ACCESS.2025.3539933](https://doi.org/10.1109/ACCESS.2025.3539933) </p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>256. <b>【2503.01634】M-SCAN: A Multistage Framework for Lumbar Spinal Canal Stenosis Grading Using Multi-View Cross Attention</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01634">https://arxiv.org/abs/2503.01634</a></p>
  <p><b>作者</b>：Arnesh Batra,Arush Gumber,Anushk Kumar</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>257. <b>【2503.01592】An Efficient Approach to Detecting Lung Nodules Using Swin Transformer</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01592">https://arxiv.org/abs/2503.01592</a></p>
  <p><b>作者</b>：Saeed Shakuri,Alireza Rezvanian</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 19th Iranian Conference on Intelligent Systems (ICIS), IEEE, 2024</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>258. <b>【2503.01505】Lossy Neural Compression for Geospatial Analytics: A Review</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01505">https://arxiv.org/abs/2503.01505</a></p>
  <p><b>作者</b>：Carlos Gomes,Isabelle Wittmann,Damien Robert,Johannes Jakubik,Tim Reichelt,Michele Martone,Stefano Maurogiovanni,Rikard Vinge,Jonas Hurst,Erik Scheurer,Rocco Sedona,Thomas Brunschwiler,Stefan Kesselheim,Matej Batic,Philip Stier,Jan Dirk Wegner,Gabriele Cavallaro,Edzer Pebesma,Michael Marszalek,Miguel A Belenguer-Plomer,Kennedy Adriko,Paolo Fraccaro,Romeo Kienzler,Rania Briq,Sabrina Benassou,Michele Lazzarini,Conrad M Albrecht</p>
  <p><b>类目</b>：ignal Processing (eess.SP); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Geophysics (physics.geo-ph)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： self-consistent review paper</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>259. <b>【2503.01462】S-R2D2: a spherical extension of the R2D2 deep neural network series paradigm for wide-field radio-interferometric imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01462">https://arxiv.org/abs/2503.01462</a></p>
  <p><b>作者</b>：A. Tajja,A. Aghabiglou,E. Tolley,J-P. Kneib,J-P. Thiran,Y. Wiaux</p>
  <p><b>类目</b>：Instrumentation and Methods for Astrophysics (astro-ph.IM); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 16 pages, 13 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>260. <b>【2503.01400】Hyperspectral image segmentation with a machine learning model trained using quantum annealer</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01400">https://arxiv.org/abs/2503.01400</a></p>
  <p><b>作者</b>：Dawid Mazur,Tomasz Rybotycki,Piotr Gawron</p>
  <p><b>类目</b>：Quantum Physics (quant-ph); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 16 pages, 6 figures, 3 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>261. <b>【2503.01352】Diffusion-based Virtual Staining from Polarimetric Mueller Matrix Imaging</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01352">https://arxiv.org/abs/2503.01352</a></p>
  <p><b>作者</b>：Xiaoyu Zheng,Jing Wen,Jiaxin Zhuang,Yao Du,Jing Cong,Limei Guo,Chao He,Lin Luo,Hao Chen</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>262. <b>【2503.01306】From Claims to Evidence: A Unified Framework and Critical Analysis of CNN vs. Transformer vs. Mamba in Medical Image Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01306">https://arxiv.org/abs/2503.01306</a></p>
  <p><b>作者</b>：Pooya Mohammadi Kazaj,Giovanni Baj,Yazdan Salimi,Anselm W. Stark,Waldo Valenzuela,George CM. Siontis,Habib Zaidi,Mauricio Reyes,Christoph Graeni,Isaac Shiri</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>263. <b>【2503.01265】Interactive Gadolinium-Free MRI Synthesis: A Transformer with Localization Prompt Learning</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01265">https://arxiv.org/abs/2503.01265</a></p>
  <p><b>作者</b>：Linhao Li,Changhui Su,Yu Guo,Huimao Zhang,Dong Liang,Kun Shang</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>264. <b>【2503.01248】Automated Retinal Layer and Fluid Segmentation and Cross-sectional Analysis using Spectral Domain Optical Coherence Tomography Images for Diabetic Retinopathy</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01248">https://arxiv.org/abs/2503.01248</a></p>
  <p><b>作者</b>：S. Chen,D. Ma,M. Raviselvan,S. Sundaramoorthy,K. Popuri,M. J. Ju,M. V. Sarunic,D. Ratra,M. F. Beg</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Tissues and Organs (q-bio.TO)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 16 pages, 9 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>265. <b>【2503.01075】ackling Hallucination from Conditional Models for Medical Image Reconstruction with DynamicDPS</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.01075">https://arxiv.org/abs/2503.01075</a></p>
  <p><b>作者</b>：Seunghoi Kim,Henry F. J. Tregidgo,Matteo Figini,Chen Jin,Sarang Joshi,Daniel C. Alexander</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>266. <b>【2503.00945】Cross Modality Medical Image Synthesis for Improving Liver Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00945">https://arxiv.org/abs/2503.00945</a></p>
  <p><b>作者</b>：Muhammad Rafiq,Hazrat Ali,Ghulam Mujtaba,Zubair Shah,Shoaib Azmat</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Submitted to Computer Methods in Biomechanics and Biomedical Engineering: Imaging  Visualization</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>267. <b>【2503.00908】Patient-Level Anatomy Meets Scanning-Level Physics: Personalized Federated Low-Dose CT Denoising Empowered by Large Language Model</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00908">https://arxiv.org/abs/2503.00908</a></p>
  <p><b>作者</b>：Ziyuan Yang,Yingyu Chen,Zhiwen Wang,Hongming Shan,Yang Chen,Yi Zhang</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Accepted by CVPR 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>268. <b>【2503.00760】NCF: Neural Correspondence Field for Medical Image Registration</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00760">https://arxiv.org/abs/2503.00760</a></p>
  <p><b>作者</b>：Lei Zhou,Nimu Yuan,Katjana Ehrlich,Jinyi Qi</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>269. <b>【2503.00745】Geodesic Diffusion Models for Medical Image-to-Image Generation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00745">https://arxiv.org/abs/2503.00745</a></p>
  <p><b>作者</b>：Teng Zhang,Hongxu Jiang,Kuang Gong,Wei Shao</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>270. <b>【2503.00741】LesionDiffusion: Towards Text-controlled General Lesion Synthesis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00741">https://arxiv.org/abs/2503.00741</a></p>
  <p><b>作者</b>：Henrui Tian,Wenhui Lei,Linrui Dai,Hanyu Chen,Xiaofan Zhang</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 10 pages, 4 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>271. <b>【2503.00657】Artificially Generated Visual Scanpath Improves Multi-label Thoracic Disease Classification in Chest X-Ray Images</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00657">https://arxiv.org/abs/2503.00657</a></p>
  <p><b>作者</b>：Ashish Verma,Aupendu Kar,Krishnendu Ghosh,Sobhan Kanti Dhara,Debashis Sen,Prabir Kumar Biswas</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Copyright 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>272. <b>【2503.00586】Cross-Attention Fusion of MRI and Jacobian Maps for Alzheimer's Disease Diagnosis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00586">https://arxiv.org/abs/2503.00586</a></p>
  <p><b>作者</b>：Shijia Zhang,Xiyu Ding,Brian Caffo,Junyu Chen,Cindy Zhang,Hadi Kharrazi,Zheyu Wang</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： Submitted to MICCAI 2025</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>273. <b>【2503.00510】NeuroSymAD: A Neuro-Symbolic Framework for Interpretable Alzheimer's Disease Diagnosis</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00510">https://arxiv.org/abs/2503.00510</a></p>
  <p><b>作者</b>：Yexiao He,Ziyao Wang,Yuning Zhang,Tingting Dan,Tianlong Chen,Guorong Wu,Ang Li</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>274. <b>【2503.00503】BELE: Blur Equivalent Linearized Estimator</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00503">https://arxiv.org/abs/2503.00503</a></p>
  <p><b>作者</b>：Paolo Giannitrapani,Elio D. Di Claudio,Giovanni Jacovitti</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>275. <b>【2503.00366】AI-Augmented Thyroid Scintigraphy for Robust Classification</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00366">https://arxiv.org/abs/2503.00366</a></p>
  <p><b>作者</b>：Maziar Sabouri,Ghasem Hajianfar,Alireza Rafiei Sardouei,Milad Yazdani,Azin Asadzadeh,Soroush Bagheri,Mohsen Arabi,Seyed Rasoul Zakavi,Emran Askari,Atena Aghaee,Dena Shahriari,Habib Zaidi,Arman Rahmim</p>
  <p><b>类目</b>：Medical Physics (physics.med-ph); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>276. <b>【2503.00267】SegImgNet: Segmentation-Guided Dual-Branch Network for Retinal Disease Diagnoses</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00267">https://arxiv.org/abs/2503.00267</a></p>
  <p><b>作者</b>：Xinwei Luo,Songlin Zhao,Yun Zong,Yong Chen,Gui-shuang Ying,Lifang He</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>277. <b>【2503.00242】Boundary-Emphasized Weight Maps for Distal Airway Segmentation</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00242">https://arxiv.org/abs/2503.00242</a></p>
  <p><b>作者</b>：Ali Keshavarzi,Elsa Angelini</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：monitoring pulmonary diseases, Automated airway segmentation, Automated airway, pulmonary diseases, lung CT scans</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>Abstract:Automated airway segmentation from lung CT scans is vital for diagnosing and monitoring pulmonary diseases. Despite advancements, challenges like leakage, breakage, and class imbalance persist, particularly in capturing small airways and preserving topology. We propose the Boundary-Emphasized Loss (BEL), which enhances boundary preservation using a boundary-based weight map and an adaptive weight refinement strategy. Unlike centerline-based approaches, BEL prioritizes boundary voxels to reduce misclassification, improve topology, and enhance structural consistency, especially on distal airway branches. Evaluated on ATM22 and AIIB23, BEL outperforms baseline loss functions, achieving higher topology-related metrics and comparable overall-based measures. Qualitative results further highlight BEL's ability to capture fine anatomical details and reduce segmentation errors, particularly in small airways. These findings establish BEL as a promising solution for accurate and topology-enhancing airway segmentation in medical imaging.</p>
  </details>
</details>
<details>
  <summary>278. <b>【2503.00159】EXACT-CT: EXplainable Analysis for Crohn's and Tuberculosis using CT</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00159">https://arxiv.org/abs/2503.00159</a></p>
  <p><b>作者</b>：Shashwat Gupta,Sarthak Gupta,Akshan Agrawal,Mahim Naaz,Rajanikanth Yadav,Priyanka Bagade</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 8 figures, 5 tables</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>279. <b>【2503.00156】Neural Posterior Estimation for Cataloging Astronomical Images with Spatially Varying Backgrounds and Point Spread Functions</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00156">https://arxiv.org/abs/2503.00156</a></p>
  <p><b>作者</b>：Aakash Patel,Tianqing Zhang,Camille Avestruz,Jeffrey Regier, theLSST Dark Energy Science Collaboration</p>
  <p><b>类目</b>：Instrumentation and Methods for Astrophysics (astro-ph.IM); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Applications (stat.AP)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>280. <b>【2503.00047】PCE-GAN: A Generative Adversarial Network for Point Cloud Attribute Quality Enhancement based on Optimal Transport</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00047">https://arxiv.org/abs/2503.00047</a></p>
  <p><b>作者</b>：Tian Guo,Hui Yuan,Qi Liu,Honglei Su,Raouf Hamzaoui,Sam Kwong</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>281. <b>【2503.00042】An Analysis of Segment Anything 2</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00042">https://arxiv.org/abs/2503.00042</a></p>
  <p><b>作者</b>：Clayton Bromley,Alexander Moore,Amar Saini,Doug Poland,Carmen Carrano</p>
  <p><b>类目</b>：Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>： 19 pages, 30 figures</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
<details>
  <summary>282. <b>【2503.00027】Observability Investigation for Rotational Calibration of (Global-pose aided) VIO under Straight Line Motion</b></summary>
  <p><b>链接</b>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.00027">https://arxiv.org/abs/2503.00027</a></p>
  <p><b>作者</b>：Junlin Song,Antoine Richard,Miguel Olivares-Mendez</p>
  <p><b>类目</b>：ignal Processing (eess.SP); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)</p>
  <p><b>关键词</b>：</p>
  <p><b>备注</b>：</p>
  <details>
    <summary><b>点击查看摘要</b></summary>
    <p>None</p>
  </details>
</details>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">徐耀彬</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://louishsu.xyz/2025/03/05/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html">http://louishsu.xyz/2025/03/05/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://louishsu.xyz" target="_blank">LOUIS' BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/01/01/%E5%9B%BE%E8%A7%A3%EF%BC%9ALLM%E6%98%AF%E6%80%8E%E4%B9%88%E9%80%9A%E8%BF%87PPO%E4%BC%98%E5%8C%96%E5%81%8F%E5%A5%BD%E7%9A%84%EF%BC%9F.html"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">图解：LLM是怎么通过PPO优化偏好的？</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">徐耀彬</div><div class="author-info__description">💭这个人很懒，什么都没有留下</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/isLouisHsu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/isLouisHsu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:is.louishsu@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有问题可通过邮箱is.louishsu@foxmail.com联系，欢迎交流！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">信息检索</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">计算机视觉</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/05/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2025-03-05)"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/_posts/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92/wc.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv每日速递(2025-03-05)"/></a><div class="content"><a class="title" href="/2025/03/05/Arxiv%E6%AF%8F%E6%97%A5%E9%80%9F%E9%80%92.html" title="Arxiv每日速递(2025-03-05)">Arxiv每日速递(2025-03-05)</a><time datetime="2025-03-05T01:09:10.861Z" title="发表于 2025-03-05 09:09:10">2025-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/01/%E5%9B%BE%E8%A7%A3%EF%BC%9ALLM%E6%98%AF%E6%80%8E%E4%B9%88%E9%80%9A%E8%BF%87PPO%E4%BC%98%E5%8C%96%E5%81%8F%E5%A5%BD%E7%9A%84%EF%BC%9F.html" title="图解：LLM是怎么通过PPO优化偏好的？"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="图解：LLM是怎么通过PPO优化偏好的？"/></a><div class="content"><a class="title" href="/2025/01/01/%E5%9B%BE%E8%A7%A3%EF%BC%9ALLM%E6%98%AF%E6%80%8E%E4%B9%88%E9%80%9A%E8%BF%87PPO%E4%BC%98%E5%8C%96%E5%81%8F%E5%A5%BD%E7%9A%84%EF%BC%9F.html" title="图解：LLM是怎么通过PPO优化偏好的？">图解：LLM是怎么通过PPO优化偏好的？</a><time datetime="2025-01-01T10:59:45.000Z" title="发表于 2025-01-01 18:59:45">2025-01-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书"><img src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="🎨 Stable Diffusion 提示词指南书"/></a><div class="content"><a class="title" href="/2024/02/03/Stable%20Diffusion%20%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8C%87%E5%8D%97%E4%B9%A6.html" title="🎨 Stable Diffusion 提示词指南书">🎨 Stable Diffusion 提示词指南书</a><time datetime="2024-02-03T06:57:45.000Z" title="发表于 2024-02-03 14:57:45">2024-02-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer语言模型的位置编码与长度外推"/></a><div class="content"><a class="title" href="/2023/10/22/Transformer%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E4%B8%8E%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8.html" title="Transformer语言模型的位置编码与长度外推">Transformer语言模型的位置编码与长度外推</a><time datetime="2023-10-22T14:55:45.000Z" title="发表于 2023-10-22 22:55:45">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"><img src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度"/></a><div class="content"><a class="title" href="/2023/09/22/vLLM%EF%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E9%A1%B5%E7%BC%93%E5%AD%98%E5%92%8C%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E6%8F%90%E9%AB%98%E5%A4%A7%E6%A8%A1%E5%9E%8B2~4x%E6%8E%A8%E7%90%86%E9%80%9F%E5%BA%A6.html" title="vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><time datetime="2023-09-22T14:55:45.000Z" title="发表于 2023-09-22 22:55:45">2023-09-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 徐耀彬</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'blog-',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start -->
<script src="https://cdn.jsdelivr.net/npm/swiper@11.1.9/swiper-bundle.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.1.9/swiper-bundle.min.css">
<style>
    :root {
      --swiper-theme-color: var(--theme-color);
      --swiper-pagination-bottom: 0;
    }
    .swiper {
      padding-bottom: 32px;
      margin-bottom: 20px;
    }
    .swiper .swiper-slide .swiper-slide-img {
      display: block;
      width: 100%;
      object-fit: contain;
      background: var(--body-bg-color);
      margin: 0;
    }
  </style> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/机器学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 机器学习 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/竞赛相关/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 竞赛相关 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://louishsu.xyz/categories/阅读笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 阅读笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://louishsu.xyz/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt=""><img width="48" height="48" src="https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161037709574435991610377095138.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-05-19</span><a class="blog-slider__title" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/05/19/全球人工智能技术创新大赛【赛道一】：医学影像报告异常检测(三等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/cail2021.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-10-22</span><a class="blog-slider__title" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">中国法律智能技术评测(CAIL2021)：信息抽取(Rank2)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/10/22/中国法律智能技术评测(CAIL2021)：信息抽取(Rank2).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt=""><img width="48" height="48" src="https://cdn.kesci.com/upload/image/r7j60un866.png?imageView2/2/w/2500/h/2500" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-17</span><a class="blog-slider__title" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖)</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/17/2022全球人工智能技术创新大赛(GAIIC2022)：商品标题实体识别(二等奖).html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/01/01/图解：LLM是怎么通过PPO优化偏好的？.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww3.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-01-01</span><a class="blog-slider__title" href="2025/01/01/图解：LLM是怎么通过PPO优化偏好的？.html" alt="">图解：LLM是怎么通过PPO优化偏好的？</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2025/01/01/图解：LLM是怎么通过PPO优化偏好的？.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww5.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-22</span><a class="blog-slider__title" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/22/vLLM：利用分页缓存和张量并行提高大模型2~4x推理速度.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/city.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-10-22</span><a class="blog-slider__title" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">Transformer语言模型的位置编码与长度外推</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/10/22/Transformer语言模型的位置编码与长度外推.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt=""><img width="48" height="48" src="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-02-03</span><a class="blog-slider__title" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">🎨 Stable Diffusion 提示词指南书</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2024/02/03/Stable Diffusion 提示词指南书.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/ww6.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-09-06</span><a class="blog-slider__title" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">Prompt：大语言模型的执行指南</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/09/06/Prompt：大语言模型的执行指南.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/11/26/升级深度学习开发环境全攻略.html" alt=""><img width="48" height="48" src="https://cdn.jsdelivr.net/gh/isLouisHsu/resource@master/blog_resource/misc/cover/default.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-11-26</span><a class="blog-slider__title" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">升级深度学习开发环境全攻略</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/11/26/升级深度学习开发环境全攻略.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>