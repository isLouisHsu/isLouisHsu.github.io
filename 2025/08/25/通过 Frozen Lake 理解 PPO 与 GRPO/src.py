import os
import json
import copy
import time
import random
from typing import *
from tqdm import trange
from dataclasses import dataclass
from argparse import ArgumentParser

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter

import gymnasium as gym
from gymnasium.envs.toy_text.frozen_lake import generate_random_map


class ActorNet(nn.Module):

    def __init__(self, input_size: int, num_actions: int, feature_size: int = 128) -> None:
        super(ActorNet, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(in_channels=2, out_channels=feature_size, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            nn.Conv2d(in_channels=feature_size, out_channels=feature_size, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        
        h_out = w_out = input_size // 4
        conv_output_size = feature_size * h_out * w_out
        
        self.fc_layers = nn.Sequential(
            nn.Linear(conv_output_size, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, num_actions),
        )

        self.num_actions = num_actions

    def forward(self, state: torch.Tensor, action: torch.Tensor = None):
        x = self.feature_extractor(state)
        x = x.view(x.size(0), -1)
        logits = self.fc_layers(x)                  # (batch_size, num_actions)
        proba = F.softmax(logits, dim=-1)           # (batch_size, num_actions)

        if action is None:
            return proba, None
        
        # åœ¨è¿™é‡Œè®¡ç®—logproba
        log_proba = F.log_softmax(logits, dim=-1)   # (batch_size, num_actions)
        log_proba_selected = log_proba.gather(1, action.long().unsqueeze(1)).squeeze(1)  # (batch_size,)
        
        return proba, log_proba_selected


class CriticNet(nn.Module):

    def __init__(self, input_size: int, feature_size: int = 128) -> None:
        super(CriticNet, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(in_channels=2, out_channels=feature_size, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            nn.Conv2d(in_channels=feature_size, out_channels=feature_size, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        
        h_out = w_out = input_size // 4
        conv_output_size = feature_size * h_out * w_out
        
        self.fc_layers = nn.Sequential(
            nn.Linear(conv_output_size, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 1),
        )

    def forward(self, state: torch.Tensor):
        # è¾“å…¥å¼ é‡å½¢çŠ¶åº”è¯¥æ˜¯ 1 x input_size x input_size (C x H x W)
        x = self.feature_extractor(state)
        x = x.view(x.size(0), -1)  # å°†å¤šç»´ç‰¹å¾å›¾å±•å¹³ä¸ºä¸€ç»´å‘é‡
        return self.fc_layers(x)


class Utils():

    @staticmethod
    def set_seed(seed: int) -> None:
        """è®¾ç½® Python ç¯å¢ƒçš„æ‰€æœ‰å¸¸ç”¨éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚"""
        if seed is None: 
            return
        random.seed(seed)  # Python's built-in random module
        np.random.seed(seed)  # Numpy library
        os.environ['PYTHONHASHSEED'] = str(seed)  # Environment variable

        # TensorFlow 2.x
        # import tensorflow as tf
        # tf.random.set_seed(seed)

        # PyTorch - If you are using PyTorch, you would also need to set its seed
        import torch
        torch.manual_seed(seed)
        # if you are using CUDA:
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.

        # Other libraries might also have their own random number generators.

    @staticmethod
    def whiten_sequence(sequence: torch.Tensor, shift_mean: bool = True) -> torch.Tensor:
        # å¦‚æœæ€»å…ƒç´ æ•° <= 1ï¼Œstd å¿…ç„¶ä¸º 0ï¼Œç›´æ¥å¤„ç†
        if sequence.numel() <= 1:
            return sequence - sequence.mean() if shift_mean else sequence.clone()
        mean, std = sequence.mean(), sequence.std()
        # é¿å…å…¨é›¶æ–¹å·®å¯¼è‡´çˆ†ç‚¸
        if std.item() < 1e-8:
            return sequence - mean if shift_mean else sequence.clone()
        whiten = (sequence - mean) / (std + 1e-8)
        if not shift_mean:
            whiten += mean
        return whiten


class DataUtils():
    
    @staticmethod
    def get_env(size: int = 8, is_slippery: bool = True, render_mode: str = None) -> gym.Env:
        return gym.make(
            'FrozenLake-v1',
            desc=generate_random_map(size=size),
            is_slippery=is_slippery,
            render_mode=render_mode,
        )

    @staticmethod
    def build_static_grid(env: gym.Env) -> torch.Tensor:
        """ ä» env.unwrapped.desc æ„é€ é™æ€ç½‘æ ¼é€šé“ï¼ˆS/G/F/H -> 0/1/2/3ï¼‰ """
        mapping = {b'S': 0.0, b'G': 1.0, b'F': 2.0, b'H': 3.0}
        desc = env.unwrapped.desc  # np.ndarray of bytes, shape (H, W)
        H, W = desc.shape
        grid = torch.empty((H, W), dtype=torch.float32)
        for i in range(H):
            for j in range(W):
                grid[i, j] = mapping[desc[i, j]]
        return grid

    @staticmethod
    def make_state_tensor(static_grid: torch.Tensor, obs: int) -> torch.Tensor:
        """ æ ¹æ® obsï¼ˆç¦»æ•£ç´¢å¼•ï¼‰æ„é€ ä½ç½® one-hot é€šé“ï¼Œå¹¶ä¸é™æ€ç½‘æ ¼é€šé“å †å  """
        H, W = static_grid.shape
        pos = torch.zeros((H, W), dtype=torch.float32)
        pos[obs // W, obs % W] = 1.0
        return torch.stack([static_grid, pos], dim=0)  # (2, H, W)

    @staticmethod
    @torch.no_grad()
    def sample_action(actor_model: nn.Module, state: torch.Tensor) -> Tuple[int, float]:
        device = next(actor_model.parameters()).device
        state = state.unsqueeze(0).float().to(device)  # (1, 2, H, W)
        probas, _ = actor_model(state)
        dist = torch.distributions.Categorical(probas)
        action = dist.sample()
        action_log_proba = dist.log_prob(action)
        return int(action.item()), float(action_log_proba.item())

    @staticmethod
    @torch.no_grad()
    def sample_round(env: gym.Env, actor_model: nn.Module, render_mode: str = None) -> List[Dict[str, Any]]:
        sequence = []
        score = None
        obs, info = env.reset()

        static_grid = DataUtils.build_static_grid(env)
        state = DataUtils.make_state_tensor(static_grid, obs)

        while True:
            if render_mode in ("rgb_array", "human"):
                env.render()
                time.sleep(0.3)
            action, _ = DataUtils.sample_action(actor_model, state)
            obs, reward, terminated, truncated, info = env.step(action)
            next_state = DataUtils.make_state_tensor(static_grid, obs)

            sequence.append((state, action))
            state = next_state

            if terminated or truncated:
                sequence.append((state, None))
                score = float(reward)
                break

        states, actions = list(zip(*sequence))

        return dict(states=list(states), actions=list(actions), score=score)

    @staticmethod 
    @torch.no_grad()
    def sample_batch(actor_model: nn.Module, batch_size: int, group_size: int, **env_args) -> List[Dict[str, Any]]:
        actor_model.eval()
        examples = []
        for i in range(batch_size):
            env = DataUtils.get_env(**env_args)
            try:
                for i in range(group_size):
                    examples.append(DataUtils.sample_round(env, actor_model))
            finally:
                env.close()
        return examples


@dataclass
class Config():

    version: str = "v0"
    seed: int = 42
    frozen_lake_size: int = 4
    frozen_lake_slippery: bool = False
    num_actions: int = 4

    whiten_rewards: bool = False

    max_steps: int = 1000
    save_steps: int = 100
    batch_size: int = 32
    group_size: int = 8
    num_updates_per_batch: int = 1
    max_grad_norm: float = 0.5

    clip_epsilon: float = 0.2
    entropy_coef: float = 0.01

    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    output_dir: str = None

    def __post_init__(self):
        self.output_dir = os.path.join("./", self.version)
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"Saving to {self.output_dir}")


class Inferer():

    def __init__(self, config: Config, step_no: int, render_mode: str = "human") -> None:
        self.config = config
        self.step_no = step_no
        self.render_mode = render_mode

        # è¯»å–æ¨¡å‹
        save_dir = os.path.join(self.config.output_dir, f"checkpoint-{step_no:06d}")
        print(f"Loading model states from {save_dir}")
        self.actor_model = ActorNet(self.config.frozen_lake_size, self.config.num_actions).to(self.config.device)
        self.actor_model.load_state_dict(torch.load(os.path.join(save_dir, "actor.pt")))
        self.actor_model.eval()

    @torch.no_grad()
    def infer(self, ) -> None:
        # åˆå§‹åŒ–ç¯å¢ƒ
        env = DataUtils.get_env(
            self.config.frozen_lake_size,
            self.config.frozen_lake_slippery,
            render_mode=self.render_mode,
        )
        return DataUtils.sample_round(env, self.actor_model, render_mode=self.render_mode)


class Trainer():

    def __init__(self, config: Config) -> None:
        self.config = config
        self.writer = SummaryWriter(
            os.path.join(config.output_dir, "logs/")
        )

    @torch.no_grad()
    def prepare_inputs(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        raise NotImplementedError
    
    def update_model(self) -> None:
        raise NotImplementedError
    
    def save_model(self, step_no: int) -> str:
        raise NotImplementedError
    
    def train(self):
        for step_no in trange(self.config.max_steps, desc="Training...", total=self.config.max_steps):
            # é‡‡æ ·ä¸€æ‰¹æ•°æ®
            batch = DataUtils.sample_batch(
                self.actor_model, 
                self.config.batch_size, 
                self.config.group_size, 
                size=self.config.frozen_lake_size, 
                is_slippery=self.config.frozen_lake_slippery,
            )
            # å‡†å¤‡è¾“å…¥
            batch = self.prepare_inputs(batch)
            # æ›´æ–°æ¨¡å‹å‚æ•°
            metrics = self.update_model(batch)
            # æ‰“å°å‚æ•°
            print(json.dumps(metrics, ensure_ascii=False))
            for score_name, score_value in metrics.items():
                self.writer.add_scalar(score_name, score_value, step_no)  
            # ä¿å­˜æ¨¡å‹
            if step_no > 0 and step_no % self.config.save_steps == 0:
                model_path = self.save_model(step_no)
                print(f"Step [{step_no+1}/{self.config.max_steps}] model saved at {model_path}")


@dataclass
class PPOConfig(Config):

    actor_learning_rate: float = 1e-4
    critic_learning_rate: float = 3e-4

    gamma: float = 0.9
    lam: float = 0.95

    critic_loss_coef: float = 0.5


class PPOTrainer(Trainer):

    def __init__(self, config: PPOConfig) -> None:
        super().__init__(config)

        self.actor_model = ActorNet(config.frozen_lake_size, config.num_actions).to(config.device)
        self.critic_model = CriticNet(config.frozen_lake_size).to(config.device)
        self.reference_model = None         # é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºreferenceæ¨¡å‹ï¼Œä½†è¯¥å®éªŒæ— é¢„è®­ç»ƒæ¨¡å‹

        self.actor_optimizer = optim.Adam(self.actor_model.parameters(), lr=config.actor_learning_rate)
        self.critic_optimizer = optim.Adam(self.critic_model.parameters(), lr=config.critic_learning_rate)
    
    def compute_gae(self, step_level_rewards: torch.Tensor, values: torch.Tensor) -> torch.Tensor:
        sequence_length = step_level_rewards.size(0)
        lastgaelam = 0
        advantages_reversed = []
        for t in reversed(range(sequence_length)):  # ä¼˜åŠ¿å‡½æ•°ä¾èµ–äºæœªæ¥çš„å€¼ï¼Œæ‰€ä»¥ä»ç»ˆç‚¹å¾€å›æ¨
            next_value = values[t + 1] if t + 1 < sequence_length else 0.0              # æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæ²¡æœ‰åç»­çŠ¶æ€äº†ï¼Œç›¸å½“äºå‡è®¾ episode ç»“æŸï¼Œä»·å€¼ä¸º 0
            delta = step_level_rewards[t] + self.config.gamma * next_value - values[t]  # è®¡ç®—TDè¯¯å·®ï¼ˆTemporal Difference Errorï¼‰ï¼š
                                                                                        #   \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam       # é€’å½’è®¡ç®— GAE ä¼˜åŠ¿å€¼ï¼š
                                                                                        #   \A^{GAE}(s_t, a_t) = \delta_t + \gamma \lambda \delta_{t+1} + (\gamma \lambda) ** 2 \delta_{t+2} + ...
                                                                                        # å½“ Î» = 1ï¼Œæ¥è¿‘è’™ç‰¹å¡æ´›ä¼˜åŠ¿ï¼ˆå³å¤šæ­¥ï¼Œé«˜æ–¹å·®ä½åå·®ï¼‰ï¼›
                                                                                        # å½“ Î» = 0ï¼Œé€€åŒ–ä¸ºå•æ­¥TDè¯¯å·®ï¼ˆå³å•æ­¥ \delta_tï¼Œä½æ–¹å·®é«˜åå·®ï¼‰ï¼›
                                                                                        # å–ä¸­é—´å€¼ï¼Œå¹³è¡¡åå·®ä¸æ–¹å·®ã€‚
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1], axis=-1)                    # (sequence_length,)
        return advantages
        
    @torch.no_grad()
    def prepare_inputs(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        batch = copy.deepcopy(batch)
        for example_no in range(self.config.batch_size * self.config.group_size):
            example = batch[example_no]
            states: List[str] = example["states"]                                       # (sequence_length,)
            actions: List[int] = example["actions"]                                     # (sequence_length,)
            score: float = example["score"]
            sequence_length: int = len(states)

            # step 1. è®¡ç®—æ¯ä¸€æ­¥æ—¶ï¼Œé‡‡å–åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆaction_log_probsï¼‰å’ŒçŠ¶æ€ä»·å€¼ï¼ˆvaluesï¼‰
            encode_states = torch.stack(states, dim=0).float()                          # (sequence_length, channel, height, width)
            encode_actions = torch.tensor(actions[:-1], dtype=torch.int64)              # (sequence_length - 1,)
            _, action_log_probs = self.actor_model(encode_states[:-1], encode_actions)  # (sequence_length - 1,)
            values = self.critic_model(encode_states).squeeze(-1)                       # (sequence_length,)

            # step 2. è®¡ç®—æ­¥çº§å¥–åŠ±ï¼ˆstep_level_rewardsï¼‰ï¼Œå¦‚æœæœ‰å‚è€ƒæ¨¡å‹ï¼ˆreference_modelï¼‰ï¼Œè¿™é‡Œåº”è¯¥ï¼š
            # 1ï¼‰è®¡ç®—step-levelçš„KLæ•£åº¦ä½œä¸ºæ­¥çº§å¥–åŠ±ï¼›
            # 2ï¼‰æŠŠåºåˆ—çº§å¥–åŠ±åŠ åˆ°æœ€åä¸€æ­¥
            step_level_rewards = [0.0] * (sequence_length - 1) + [score]
            step_level_rewards = torch.tensor(step_level_rewards, dtype=torch.float32)  # (sequence_length,) [0.0, 0.0, 0.0, ..., 1.0]
            if self.config.whiten_rewards:
                step_level_rewards = Utils.whiten_sequence(step_level_rewards, shift_mean=False)

            # step 3. GAEï¼ˆGeneralized Advantage Estimationï¼‰ï¼Œè®¡ç®—æ¯ä¸€æ­¥çš„ä¼˜åŠ¿å€¼ï¼ˆadvantagesï¼‰å’Œå›æŠ¥ï¼ˆreturnsï¼‰
            advantages = self.compute_gae(step_level_rewards, values)                   # (sequence_length,)
            returns = advantages + values                                               # è®¡ç®—å›æŠ¥å€¼ï¼Œä½œä¸ºcritic modelçš„groundtruth
                                                                                        # å·²çŸ¥ï¼š
            advantages = Utils.whiten_sequence(advantages)

            example["action_log_probs"] = action_log_probs                              # (sequence_length - 1,)
            example["values"] = values                                                  # (sequence_length,)
            example["advantages"] = advantages                                          # (sequence_length,)
            example["returns"] = returns                                                # (sequence_length,)
        
        return batch

    def update_model(self, batch: List[Dict[str, Any]]) -> None:
        self.actor_model.train()
        log_actor_loss = 0.0
        log_critic_loss = 0.0
        # æ›´æ–°æ¨¡å‹å‚æ•°
        for epoch_no in range(self.config.num_updates_per_batch):
            # ä½¿ç”¨â€œæ­¥æ•°åŠ æƒâ€çš„ç´¯è®¡å™¨
            device = next(self.actor_model.parameters()).device
            total_actor_loss = torch.tensor(0.0, device=device)
            total_actor_steps = 0   # è®°å½•æ­¥æ•°ï¼Œé˜²æ­¢åºåˆ—é•¿åº¦å½±å“æ ·æœ¬æƒé‡
            total_critic_loss = torch.tensor(0.0, device=device)
            total_critic_steps = 0  # è®°å½•æ­¥æ•°ï¼Œé˜²æ­¢åºåˆ—é•¿åº¦å½±å“æ ·æœ¬æƒé‡

            for example_no in range(self.config.batch_size * self.config.group_size):
                example = batch[example_no]
                states: List[str] = example["states"]                                       # (sequence_length,)
                actions: List[int] = example["actions"]                                     # (sequence_length,)
                old_action_log_probs: torch.Tensor = example["action_log_probs"]            # (sequence_length - 1,)
                advantages: torch.Tensor = example["advantages"]                            # (sequence_length,)
                returns: torch.Tensor = example["returns"]                                  # (sequence_length,)

                # é‡æ–°å‰å‘
                encode_states = torch.stack(states, dim=0).float()                          # (sequence_length, channel, height, width)
                encode_actions = torch.tensor(actions[:-1], dtype=torch.int64)              # (sequence_length - 1,)
                probas, action_log_probs = self.actor_model(encode_states[:-1], encode_actions)  # (sequence_length - 1,)
                values = self.critic_model(encode_states).squeeze(-1)                       # (sequence_length,)

                # actorï¼šé€æ­¥æŸå¤±ï¼Œä¸åš mean
                ratio = torch.exp(action_log_probs - old_action_log_probs)                  # (sequence_length - 1,)
                step_actor_loss = - torch.min(
                    ratio * advantages[:-1],
                    torch.clamp(
                        ratio, 
                        1 - self.config.clip_epsilon, 
                        1 + self.config.clip_epsilon,
                    ) * advantages[:-1]
                )                                                                           # (sequence_length - 1,)

                # ç†µå¥–åŠ±ï¼Œæœ€å¤§åŒ–è¡ŒåŠ¨ç†µä»¥é¼“åŠ±æ¢ç´¢
                entropy = - (probas * torch.log(torch.clamp(probas, min=1e-8))).sum(dim=1)                                  # (sequence_length,)
                step_actor_loss = step_actor_loss - self.config.entropy_coef * entropy

                # criticï¼šé€æ­¥ MSEï¼Œä¸åš mean
                step_critic_loss = 0.5 * torch.square(values - returns)                     # (sequence_length,)

                # ç´¯åŠ æ€»å’Œä¸æœ‰æ•ˆæ­¥æ•°
                total_actor_loss += step_actor_loss.sum()
                total_actor_steps += step_actor_loss.numel()

                total_critic_loss += (self.config.critic_loss_coef * step_critic_loss).sum()
                total_critic_steps += step_critic_loss.numel()

                # å¦‚éœ€è®°å½•æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼Œä¸ç”¨äºæ¢¯åº¦ï¼‰
                example["actor_loss"] = step_actor_loss.mean().item()
                example["critic_loss"] = step_critic_loss.mean().item()

            # ç”¨â€œæ€»å’Œ / æ€»æ­¥æ•°â€å¾—åˆ° batch çº§æŸå¤±ï¼Œç¡®ä¿æ¯ä¸ªæ—¶é—´æ­¥æƒé‡ä¸€è‡´
            actor_loss = total_actor_loss / max(1, total_actor_steps)
            critic_loss = total_critic_loss / max(1, total_critic_steps)
            log_actor_loss += (actor_loss.item() / self.config.num_updates_per_batch)
            log_critic_loss += (critic_loss.item() / self.config.num_updates_per_batch)

            # æ›´æ–°actor
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            nn.utils.clip_grad_norm_(self.actor_model.parameters(), self.config.max_grad_norm)
            self.actor_optimizer.step()

            # æ›´æ–°critic
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            nn.utils.clip_grad_norm_(self.critic_model.parameters(), self.config.max_grad_norm)
            self.critic_optimizer.step()

        # æ‰“å°æŒ‡æ ‡ï¼ˆä¿æŒä¸å˜ï¼‰
        metrics = {
            "score/mean": torch.tensor([e["score"] for e in batch]).mean().item(),
            "score/max": torch.tensor([e["score"] for e in batch]).max().item(),
            "score/min": torch.tensor([e["score"] for e in batch]).min().item(),
            "actor_loss": log_actor_loss,
            "critic_loss": log_critic_loss,
        }
        return metrics

    def save_model(self, step_no: int) -> str:
        save_dir = os.path.join(self.config.output_dir, f"checkpoint-{step_no:06d}")
        os.makedirs(save_dir, exist_ok=True)
        torch.save(self.actor_model.state_dict(), os.path.join(save_dir, f"actor.pt"))
        torch.save(self.critic_model.state_dict(), os.path.join(save_dir, f"critic.pt"))
        return save_dir

@dataclass
class GRPOConfig(Config):

    actor_learning_rate: float = 1e-4


class GRPOTrainer(Trainer):

    def __init__(self, config: PPOConfig) -> None:
        super().__init__(config)

        self.actor_model = ActorNet(config.frozen_lake_size, config.num_actions).to(config.device)
        self.reference_model = None         # é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºreferenceæ¨¡å‹ï¼Œä½†è¯¥å®éªŒæ— é¢„è®­ç»ƒæ¨¡å‹

        self.actor_optimizer = optim.Adam(self.actor_model.parameters(), lr=config.actor_learning_rate)

    def compute_grpo(self, rewards: torch.Tensor) -> torch.Tensor:
        # å¦‚æœæ€»å…ƒç´ æ•° <= 1ï¼Œstd å¿…ç„¶ä¸º 0ï¼Œç›´æ¥å¤„ç†
        if rewards.numel() <= 1:
            return rewards - rewards.mean()
        mean, std = rewards.mean(), rewards.std()
        # é¿å…å…¨é›¶æ–¹å·®å¯¼è‡´çˆ†ç‚¸
        if std.item() < 1e-8:
            return rewards - mean
        return (rewards - mean) / (std + 1e-8)
        
    @torch.no_grad()
    def prepare_inputs(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        batch = copy.deepcopy(batch)
        device = self.config.device

        for group_no in range(self.config.batch_size):
            group_start = group_no * self.config.group_size
            group_end = group_start + self.config.group_size
            group = batch[group_start: group_end]

            # GRPOï¼ˆGroup Relative Policy Optimizationï¼‰: group relative advantage estimation
            grouped_rewards = torch.tensor([example["score"] for example in group]).float().to(device)
            grouped_advantages = self.compute_grpo(grouped_rewards)     # len = group_size

            for example_no in range(self.config.group_size):
                example = group[example_no]
                states: List[str] = example["states"]                                       # (sequence_length,)
                actions: List[int] = example["actions"]                                     # (sequence_length,)
                score: float = example["score"]
                sequence_length: int = len(states)

                # step 1. è®¡ç®—æ¯ä¸€æ­¥æ—¶ï¼Œé‡‡å–åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆaction_log_probsï¼‰å’ŒçŠ¶æ€ä»·å€¼ï¼ˆvaluesï¼‰
                encode_states = torch.stack(states, dim=0).float()                          # (sequence_length, channel, height, width)
                encode_actions = torch.tensor(actions[:-1], dtype=torch.int64)              # (sequence_length - 1,)
                _, action_log_probs = self.actor_model(encode_states[:-1], encode_actions)  # (sequence_length - 1,)

                # step 2. GRPOï¼ˆGroup Relative Policy Optimizationï¼‰: group relative advantage estimation
                # DeepSeekåŸæ–‡ï¼šOutcome supervision provides the normalized reward at the end of each output ğ‘œğ‘– and 
                #              sets the advantages ğ´Ë†ğ‘–,ğ‘¡ of all tokens in the output as the normalized reward
                advantages = grouped_advantages[example_no]                     # (1,)

                example["action_log_probs"] = action_log_probs.detach()         # (sequence_length,)
                example["advantages"] = advantages.detach()                     # (sequence_length,)
        
        return batch

    def update_model(self, batch: List[Dict[str, Any]]) -> None:
        self.actor_model.train()
        log_actor_loss = 0.0
        # æ›´æ–°æ¨¡å‹å‚æ•°
        for epoch_no in range(self.config.num_updates_per_batch):
            # ä½¿ç”¨â€œæ­¥æ•°åŠ æƒâ€çš„ç´¯è®¡å™¨
            device = next(self.actor_model.parameters()).device
            total_actor_loss = torch.tensor(0.0, device=device)
            total_actor_steps = 0   # è®°å½•æ­¥æ•°ï¼Œé˜²æ­¢åºåˆ—é•¿åº¦å½±å“æ ·æœ¬æƒé‡

            for example_no in range(self.config.batch_size * self.config.group_size):
                example = batch[example_no]
                states: List[str] = example["states"]                                       # (sequence_length,)
                actions: List[int] = example["actions"]                                     # (sequence_length,)
                old_action_log_probs: torch.Tensor = example["action_log_probs"]            # (sequence_length - 1,)
                advantages: torch.Tensor = example["advantages"]                            # (1,)

                # é‡æ–°å‰å‘
                encode_states = torch.stack(states, dim=0).float()                          # (sequence_length, channel, height, width)
                encode_actions = torch.tensor(actions[:-1], dtype=torch.int64)              # (sequence_length - 1,)
                probas, action_log_probs = self.actor_model(encode_states[:-1], encode_actions)  # (sequence_length - 1,)

                # actorï¼šé€æ­¥æŸå¤±ï¼Œä¸åš mean
                ratio = torch.exp(action_log_probs - old_action_log_probs)                  # (sequence_length - 1,)
                step_actor_loss = - torch.min(
                    ratio * advantages,
                    torch.clamp(
                        ratio, 
                        1 - self.config.clip_epsilon, 
                        1 + self.config.clip_epsilon,
                    ) * advantages
                )                                                                           # (sequence_length - 1,)

                # ç†µå¥–åŠ±ï¼Œæœ€å¤§åŒ–è¡ŒåŠ¨ç†µä»¥é¼“åŠ±æ¢ç´¢
                entropy = - (probas * torch.log(torch.clamp(probas, min=1e-8))).sum(dim=1)                                  # (sequence_length,)
                step_actor_loss = step_actor_loss - self.config.entropy_coef * entropy

                # ç´¯åŠ æ€»å’Œä¸æœ‰æ•ˆæ­¥æ•°
                total_actor_loss += step_actor_loss.sum()
                total_actor_steps += step_actor_loss.numel()

                # å¦‚éœ€è®°å½•æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼Œä¸ç”¨äºæ¢¯åº¦ï¼‰
                example["actor_loss"] = step_actor_loss.mean().item()

            # ç”¨â€œæ€»å’Œ / æ€»æ­¥æ•°â€å¾—åˆ° batch çº§æŸå¤±ï¼Œç¡®ä¿æ¯ä¸ªæ—¶é—´æ­¥æƒé‡ä¸€è‡´
            actor_loss = total_actor_loss / max(1, total_actor_steps)
            log_actor_loss += (actor_loss.item() / self.config.num_updates_per_batch)

            # æ›´æ–°actor
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            nn.utils.clip_grad_norm_(self.actor_model.parameters(), self.config.max_grad_norm)
            self.actor_optimizer.step()

        # æ‰“å°æŒ‡æ ‡ï¼ˆä¿æŒä¸å˜ï¼‰
        metrics = {
            "score/mean": torch.tensor([e["score"] for e in batch]).mean().item(),
            "score/max": torch.tensor([e["score"] for e in batch]).max().item(),
            "score/min": torch.tensor([e["score"] for e in batch]).min().item(),
            "actor_loss": log_actor_loss,
        }
        return metrics

    def save_model(self, step_no: int) -> str:
        save_dir = os.path.join(self.config.output_dir, f"checkpoint-{step_no:06d}")
        os.makedirs(save_dir, exist_ok=True)
        torch.save(self.actor_model.state_dict(), os.path.join(save_dir, f"actor.pt"))
        return save_dir


if __name__ == "__main__":
    parser = ArgumentParser(description="""
# æœ€ç®€å•çš„å®ç°ï¼Œæ²¡æœ‰è¿›è¡Œå¼‚æ­¥é‡‡æ ·ã€è®­ç»ƒ
# ä¸ºæ–¹ä¾¿ç†è§£ï¼Œæ²¡æœ‰é‡‡å–å…¨å‘é‡åŒ–çš„è®¡ç®—æ–¹å¼ï¼Œæ¯”å¦‚å›æŠ¥ï¼ˆreturnsï¼‰çš„è®¡ç®—ï¼Œä¹Ÿæ²¡æœ‰ç”¨åˆ°GPUåŠ é€Ÿ

# # ç¯å¢ƒè¯´æ˜ï¼šhttps://gymnasium.farama.org/environments/toy_text/frozen_lake/
# desc = generate_random_map(size=8)
# env = gym.make('FrozenLake-v1', desc=desc, is_slippery=True)

# RLè¿ç®—å‚è€ƒï¼š
# PPOï¼šhttps://github.com/huggingface/trl/blob/20cc58d7772ae660792c7b5249d8b817986a547d/trl/trainer/ppo_trainer.py#L448
# GRPOï¼šhttps://github.com/huggingface/trl/blob/9e5e60c9334d0d6d52498da4de68632148fceafb/trl/trainer/grpo_trainer.py#L1362
    """)
    parser.add_argument("--version", type=str, default="v0")
    parser.add_argument("--seed", type=int, default=42)

    parser.add_argument("--observation_size", type=int, default=4)
    parser.add_argument("--num_actions", type=int, default=4)
    parser.add_argument("--frozen_lake_size", type=int, default=4)

    parser.add_argument("--adv_estimator", type=str, choices=["ppo", "grpo"], default="ppo")
    parser.add_argument("--max_steps", type=int, default=1000, help="æ€»çš„è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--save_steps", type=int, default=100, help="æ¯éš”è‹¥å¹²æ­¥æ•°ä¿å­˜ä¸€æ¬¡æ¨¡å‹")
    parser.add_argument("--batch_size", type=int, default=32, help="æ¯ä¸ªstepä¸­çš„æ ·æœ¬æ•°é‡")
    parser.add_argument("--group_size", type=int, default=8,  help="æ¯ä¸ªæ ·æœ¬é‡‡æ ·çš„ä¸ªæ•°ï¼Œæ¯ä¸ªstepä¸­çš„æ€»æ ·æœ¬æ•°æ˜¯(batch_size * group_size)")
    parser.add_argument("--num_updates_per_batch", type=int, default=1, help="æ¯ä¸ªé‡‡æ ·çš„æ‰¹æ¬¡ç”¨äºè¿­ä»£æ¨¡å‹çš„è½®æ•°")
    parser.add_argument("--actor_learning_rate", type=float, default=1e-4, help="actoræ¨¡å‹å­¦ä¹ ç‡")
    parser.add_argument("--critic_learning_rate", type=float, default=3e-4, help="criticæ¨¡å‹å­¦ä¹ ç‡")
    parser.add_argument("--max_grad_norm", type=float, default=0.5)

    parser.add_argument("--whiten_rewards", action="store_true")
    parser.add_argument("--gamma", type=float, default=0.9)
    parser.add_argument("--lam", type=float, default=0.95)
    parser.add_argument("--clip_epsilon", type=float, default=0.2)

    parser.add_argument("--entropy_coef", type=float, default=0.0, help="ç†µå¥–åŠ±ç³»æ•°ï¼Œç”¨äºæœ€å¤§åŒ–è¡ŒåŠ¨ç†µä»¥é¼“åŠ±æ¢ç´¢")
    parser.add_argument("--critic_loss_coef", type=float, default=1.0, help="criticæ¨¡å‹çš„æƒé‡ç³»æ•°")

    args = parser.parse_args()

    Utils.set_seed(args.seed)

    if args.adv_estimator == "ppo":
        ppo_config = PPOConfig(
            version=args.version,
            seed=args.seed,
            frozen_lake_size=args.frozen_lake_size,
            num_actions=args.num_actions,
            max_steps=args.max_steps,
            batch_size=args.batch_size,
            group_size=args.group_size,
            num_updates_per_batch=args.num_updates_per_batch,
            actor_learning_rate=args.actor_learning_rate,
            critic_learning_rate=args.critic_learning_rate,
            max_grad_norm=args.max_grad_norm,
            whiten_rewards=args.whiten_rewards,
            gamma=args.gamma,
            lam=args.lam,
            clip_epsilon=args.clip_epsilon,
            entropy_coef=args.entropy_coef,
            critic_loss_coef=args.critic_loss_coef,
        )
        # inferer = Inferer(ppo_config, step_no=900)
        # for i in range(100):
        #     inferer.infer()
        # exit(0)

        trainer = PPOTrainer(ppo_config)
        trainer.train()

    elif args.adv_estimator == "grpo":
        grpo_config = GRPOConfig(
            version=args.version,
            seed=args.seed,
            frozen_lake_size=args.frozen_lake_size,
            num_actions=args.num_actions,
            max_steps=args.max_steps,
            save_steps=args.save_steps,
            batch_size=args.batch_size,
            group_size=args.group_size,
            num_updates_per_batch=args.num_updates_per_batch,
            actor_learning_rate=args.actor_learning_rate,
            max_grad_norm=args.max_grad_norm,
            whiten_rewards=args.whiten_rewards,
            clip_epsilon=args.clip_epsilon,
            entropy_coef=args.entropy_coef,
        )

        trainer = GRPOTrainer(grpo_config)
        trainer.train()

