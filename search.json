[{"title":"[REPRODUCTION]A Recipe for Training Neural Networks","url":"/2019/04/29/REPRODUCTION-A-Recipe-for-Training-Neural-Networks/","content":"\n**转载自[Andrej Karpathy blog](http://karpathy.github.io)**\n\n\n## The recipe\n\nIn light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of \"unverified\" complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you'd want to use a very small learning rate and guess and then evaluate the full test set after every iteration.\n\n#### 1. Become one with the data\n\nThe first step to training a neural net is to not touch any neural net code at all and instead begin by <u>thoroughly inspecting your data</u>. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we'll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels?\n\n> 1. data contained duplicate examples\n> 2. corrupted images & labels\n> 3. data imbalances and biases\n> 4. local features v.s. global context\n> 5. quantity & form of variation\n> 6. preprocess out some variation\n> 7. spatial position v.s. average pool\n> 8. detail v.s. downsample\n> 9. labels are noisy?\n\nIn addition, since the neural net is effectively a compressed/compiled version of your dataset, you'll be able to <u>look at your network (mis)predictions and understand where they might be coming from</u>. And if your network is giving you some prediction that doesn't seem consistent with what you've seen in the data, something is off.\n\nOnce you get a qualitative sense it is also a good idea to write some simple code to <u>search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis.</u> The outliers especially almost always uncover some bugs in data quality or preprocessing.\n\n#### 2. Set up the end-to-end training/evaluation skeleton + get dumb baselines\n\nNow that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. <u>Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments.</u> At this stage it is best to pick some simple model that you couldn't possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We'll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way.\n\nTips & tricks for this stage:\n\n- **fix random seed**. Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome. This removes a factor of variation and will help keep you sane.\n- **simplify**. Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage. Data augmentation is a regularization strategy that we may incorporate later, but for now it is just another opportunity to introduce some dumb bug.\n- **add significant digits to your eval**. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.\n- **verify loss @ init**. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure `-log(1/n_classes)` on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc.\n- **init well**. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate \"hockey stick\" loss curves where in the first few iteration your network is basically just learning the bias.\n- **human baseline**. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth.\n- **input-indepent baseline**. Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all?\n- **overfit one batch**. Overfit a single batch of only a few examples (e.g. as little as two). To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). I also like to visualize in the same plot both the label and the prediction and ensure that they end up aligning perfectly once we reach the minimum loss. If they do not, there is a bug somewhere and we cannot continue to the next stage.\n- **verify decreasing training loss**. At this stage you will hopefully be underfitting on your dataset because you're working with a toy model. Try to increase its capacity just a bit. Did your training loss go down as it should?\n- **visualize just before the net**. The unambiguously correct place to visualize your data is immediately before your `y_hat = model(x)` (or `sess.run` in tf). That is - you want to visualize *exactly* what goes into your network, decoding that raw tensor of data and labels into visualizations. This is the only \"source of truth\". I can't count the number of times this has saved me and revealed problems in data preprocessing and augmentation.\n- **visualize prediction dynamics**. I like to visualize model predictions on a fixed test batch during the course of training. The \"dynamics\" of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network \"struggle\" to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter.\n- **use backprop to chart dependencies**. Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I've come across a few times is that people get this wrong (e.g. they use `view` instead of `transpose/permute` somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss for some example **i** to be 1.0, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the **i-th** example. More generally, gradients give you information about what depends on what in your network, which can be useful for debugging.\n- **generalize a special case**. This is a bit more of a general coding tip but I've often seen people create bugs when they bite off more than they can chew, writing a relatively general functionality from scratch. I like to write a very specific function to what I'm doing right now, get that to work, and then generalize it later making sure that I get the same result. Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time.\n\n> 1. 固定随机种子，消除随机带来的误差\n> 2. 简单出发，先不使用数据集扩增\n> 3. 测试集不要画曲线，不然会疯的\n> 4. 评估起始损失值，`-log(1/n_classes)`，各类初始概率应大致相等；**思路一致**, $p\\approx$\n> 5. 初始化最后一层权重很重要；**没看懂，一般都是采用随机初始化方法？？**\n> 6. 从人类角度评估，多在意准确率而不是损失；**好吧**\n> 7. 先训练一个`baseline`，不用追求准确率，观察网络是否提取了想要的特征\n> 8. 反复训练同一个批次的数据，使网络过拟合，查看损失最低能到多少；**确定网络结构没有问题**\n> 9. 试着增加模型`capacity`，观察训练集损失是否下降，以确定合适的网络容量；**确定模型参数量**\n> 10. 输入网络前，可视化数据，查看数据是否正确；**确定输入数据没有问题**\n> 11. 可视化一些相同数据的输出，观察输出波动；**观察网络收敛情况**\n> 12. 用反向传播debug网络；**高级技能？技能点还不够**\n> 13. 全循环慢慢改成矢量化代码；**一些复杂的计算可参考**\n\n#### 3. Overfit\n\nAt this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.\n\nThe approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.\n\nA few tips & tricks for this stage:\n\n- **picking the model**. To reach a good training loss you'll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: **Don't be a hero**. I've seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don't be a hero and just copy paste a ResNet-50 for your first run. You're allowed to do something more custom later and beat this.\n- **adam is safe**. In the early stages of setting baselines I like to use Adam with a learning rate of [3e-4](https://twitter.com/karpathy/status/801621764144971776?lang=en). In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don't be a hero and follow whatever the most related papers do.)\n- **complexify only one at a time**. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you'd expect. Don't throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc.\n- **do not trust learning rate decay defaults**. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you're not training ImageNet then you almost certainly do not want this. If you're not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.\n\n> 1. 不要逞强，搭建各种奇奇怪怪的模型。先从相近任务效果良好的网络结构出发，慢慢改进再击败它；\n> 2. `Adam`对参数敏感性低，`SGD`往往效果更好；\n> 3. 慢慢提高输入数据的复杂性，如输入数据的特征数、图像尺寸；\n> 4. 根据自己的学习任务，调整学习率衰减参数；\n\n#### 4. Regularize\n\nIdeally, we are now at a place where we have a large model that is fitting at least the training set. Now it is time to regularize it and gain some validation accuracy by giving up some of the training accuracy. Some tips & tricks:\n\n- **get more data**. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I'm aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.\n- **data augment**. The next best thing to real data is half-fake data - try out more aggressive data augmentation.\n- **creative augmentation**. If half-fake data doesn't do it, fake data may also do something. People are finding creative ways of expanding datasets; For example, [domain randomization](https://openai.com/blog/learning-dexterity/), use of [simulation](http://vladlen.info/publications/playing-data-ground-truth-computer-games/), clever [hybrids](https://arxiv.org/abs/1708.01642) such as inserting (potentially simulated) data into scenes, or even GANs.\n- **pretrain**. It rarely ever hurts to use a pretrained network if you can, even if you have enough data.\n- **stick with supervised learning**. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).\n- **smaller input dimensionality**. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don't matter much try to input a smaller image.\n- **smaller model size**. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.\n- **decrease the batch size**. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale & offset \"wiggles\" your batch around more.\n- **drop**. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout [does not seem to play nice](https://arxiv.org/abs/1801.05134) with batch normalization.\n- **weight decay**. Increase the weight decay penalty.\n- **early stopping**. Stop training based on your measured validation loss to catch your model just as it's about to overfit.\n- **try a larger model**. I mention this last and only after early stopping but I've found a few times in the past that larger models will of course overfit much more eventually, but their \"early stopped\" performance can often be much better than that of smaller models.\n\nFinally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network's first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems.\n\n> 略微升高一点训练数据的损失，换取验证集损失的下降。\n> 1. 扩大数据集；**拼的就是算力和数据量**\n> 2. 数据集扩增；**来了**\n> 3. 使用一些生成的虚假数据；\n> 4. 预训练模型，即使有足够的数据量；\n> 5. 监督性学习比非监督好；\n> 6. 减少数据特征的冗余性，如删减特征、降低图像分辨率；**数据冗余容易过拟合**\n> 7. 减少模型参数容量；**过大可能过拟合**\n> 8. 增大批数据量；**梯度下降方向更准确**\n> 9. `Dropout`；**`BatchNorm`效果更好？**\n> 10. 权重衰减；**正则惩罚**\n> 11. `Early stopping`；**提前中断训练防止过拟合**\n> 12. 尝试更大的模型，有时候更大容易过拟合，但是提前中断训练效果会更好；\n> 13. 第一层网络的权值是否有可解释性；**WTF?**\n\n#### 5. Tune\n\nYou should now be \"in the loop\" with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step:\n\n- **random over grid search**. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is [best to use random search instead](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf). Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter **a** matters but changing **b** has no effect then you'd rather sample **a** more throughly than at a few fixed points multiple times.\n- **hyper-parameter optimization**. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding.\n\n> 1. 随机搜索超参数，对重要参数调整更多；\n> 2. 招募一个实习生帮助自己调参hhhhhh；\n\n#### 6. Squeeze out the juice\n\nOnce you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system:\n\n- **ensembles**. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can't afford the computation at test time look into distilling your ensemble into a network using [dark knowledge](https://arxiv.org/abs/1503.02531).\n- **leave it training**. I've often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (\"state of the art\").\n\n> 1. 集成方法；\n> 2. 坚信模型能收敛并能取得良好的效果，不要手贱中断他。\n\n#### Conclusion\n\nOnce you make it here you'll have all the ingredients for success: You have a deep understanding of the technology, the dataset and the problem, you've set up the entire training/evaluation infrastructure and achieved high confidence in its accuracy, and you've explored increasingly more complex models, gaining performance improvements in ways you've predicted each step of the way. You're now ready to read a lot of papers, try a large number of experiments, and get your SOTA results. Good luck!","categories":["Deep Learning"]},{"title":"在宇宙间不易被风吹散","url":"/2019/04/28/在宇宙间不易被风吹散/","content":"\n![1](/在宇宙间不易被风吹散/1.jpg)\n![2](/在宇宙间不易被风吹散/2.png)\n![3](在宇宙间不易被风吹散/3.jpg)\n\n# 前言\n喜欢冯唐的文集，用他自己文章里写的句子描述，“毫不掩饰的小说”，露骨但是真实，文字如锦如绣，文字间有墨香、有美人、有Kindle、有古玉和瓷器。\n\n> 你迷恋什么，什么就是你的障碍。有个笃定的核，在宇宙间不易被风吹散。\n\n# 精选\n1. 人是需要有点精神的，有点通灵的精神，否则很容易出溜成行尸走肉，任由人性中暗黑的一面驱使自己禽兽一样的肉身，在世间做一些腐朽不堪的事情。\n2. 一杆进洞，四下无人，人生悲惨莫过于此。\n3. 一日茶，一夜酒，一部毫不掩饰的小说，一次没有目的的见面，一群不谈正经事的朋友，用美好的器物消磨必定留不住的时间。\n4. 做一个人性的矿工，挖一挖，再挖一挖，看看下面的下面还有什么。\n5. 我觉得眼睛看到的一切似乎想要告诉我世界是什么但是又不明说到底是什么。\n6. 在纸书里，在啤酒里，在阳光里，在暖气里，宅着，屌着，无所事事，随梦所之。\n7. 我的、我的、我的、我的，一瞬间的我执爆棚，真好。\n8. 宇宙间大多数现象超越人类的知识范围，不可解释的例子比比皆是，比如人骨骼为啥是206块骨头，比如我爱你你为什么不爱我。\n9. 人生苦短，不如不管，继续任性。\n10. 时间在床边和鬓边一路小跑，有些事物在不知不觉中浅吟低唱，明生暗长。\n11. 人又不是黄金，怎么能让所有人都喜欢？任何事做到顶尖，都是政治，都会被人妒忌；即使是黄金，也会被某些人说成是臭狗屎。\n12. 既然死了的人都没睡醒过，活着时候睡觉就是很吃亏的一件事。\n13. 白白的，小小的，紧紧的，香香的，佛说第一次触摸最接近佛。——《初恋》\n14. 有时候，人会因为一两个微不足道的美好，安安渴望一个巨大的负面，比如因为像有机会用一下图案撩骚的Zippo打火机而渴望抽烟，比如因为一把好乳或者一头长发而舍不得一个三观凌乱的悍妇，比如因为一个火炉而期待北京一个漫长而寒冷的冬天。\n15. 脱离长期背在身上的人的羁绊，让身体里的禽兽和仙人在山林里和酒里渐渐增加比例，裸奔、裸泳，在池塘里带着猴子捞月亮，在山顶问神仙：人到底是个什么东西？\n16. 想起后半生最不靠谱的事儿，结论是：最靠谱的还是买个酒庄。\n17. 天大理比不过“我喜欢”。\n18. 涉及终极的事儿，听天，听命，让自己和身体尽人力，其他不必去想，多想无益，徒增烦恼。\n19. 全球化了，各国的建筑师都到处串了，各种时装杂志都到处发行了，各地的楼宇和姑娘越来越像，像到面目模糊，天下一城。\n20. 最后一个能想到的原因，是随身佩带之后，无时无刻不提醒自己一些必须珍惜的事物和必须坚守的品质。君子无辜，玉不去身，时刻提醒自己，不要吃喝嫖赌抽、坑蒙拐骗偷。\n21. 科技的快速进步让很多人变得过时，也让很多器物变得多余。\n22. 我会老到有一天，不需要手表告诉我，时间是如何自己消失，也不需要靠名牌手表告诉周围人类我的品味、格调、富裕程度和牛逼等级。我会根据四季里的光线的变化，大致推断现在是几点了，根据肠胃的叫声决定是否该去街口的小馆儿了。\n23. 男人要有些士的精神，有所不为，有所必为，活着不是唯一的追求和最终的底线。\n24. 女人一头长发，骑匹大马，很迷人，非常迷人，而且，她是来救你的，就无比迷人。无论她要带你去哪，你都不要拒绝，先上马，然后闭嘴，什么都不要问。\n25. 买件立领风衣，浓个眉大个眼，一直走，不要往两边看，还能再混几十年。\n26. 家庭太复杂，涉及太多硬件和软件、生理和心理、现在和未来，一篇文章不容易讲透。\n27. 上天下地，背山面海，每天看看不一样的云，想想昨晚的梦，和自己聊一会天，日子容易丰盛起来。\n28. 朋友们就散住在附近几个街区，不用提前约，菜香升起时，几个电话就能聚起几个人，酒量不同，酒品相近，术业不同，三观接近。菜一般，就多喝点酒；酒不好，就再多喝点，很快就能高兴起来。\n29. 一生中，除了做自己喜欢的事儿，剩下最重要的就是和相看两不厌的人待在一起。\n30. 不和这个世界争，也不和别人争，更不要和自己争。争的结果可能是一时牛逼，也可能是心脑血管意外，后者造成的持续影响大很多。\n31. 尽管没去过南极，但是也见过了风雨，俗事已经懒得分析，不如一起一边慢跑，一边咒骂彼此生活中奇葩一样摇曳的傻逼。\n32. 世界这么多凶狠，他人心里那么多地狱，内心没有一点混蛋，如何走的下去？\n33. 其实我们跟鱼、植物甚至草履虫有很多相近的地方，人或如草木，人可以甚至应该偶尔禽兽。\n37. 如果没有真的存在，所谓的善只能是伪善，所谓的美也只能是妄美。\n38. 因为人是要死的，所以要常常叨念冯唐说的九字箴言：不着急，不害怕，不要脸。\n39. 钱超过一定数目就不是用来个人消费的了，个人能温饱就好。多处的个人欲望需要靠修行来消灭，而不能靠多花钱来满足。\n40. 有帽子是一种相，没帽子也是一种相。内心不必太执着于无帽子的相，也不必太执着于有帽子的相。有帽子，无帽子，都需要亲尝，皆为玩耍。\n41. 既然戴帽子是相，投射到不同人的心识里就是不同的相，何必强求赞美？何必强调一致？何必消除噪音？\n42. “宇戴王冠，必承其重。不要低头，王冠会掉。不要哭泣，有人会笑。”这个态度也太励志、太谋权，放松，戴戴耍耍，不留神，王冠掉了，掉就掉了，掉了就索性长发飘飘。\n43. 能做到实事求是地自恋其实是自信和自尊。任何领域做到最好之后，人只能相信自己的判断，只能自恋。……。从这个意义上讲，自恋不应该是被诟病的对象，不能实事求是的傻逼才应该是被诟病的对象。……。实事求是地修炼，实事求是地恋他和自恋，让别人闹心去吧。\n44. 矮子更爱居高临下，傻子更容易认为自己充满道理。\n45. 非让矮子明白自己是矮子，非让傻子明白自己是傻子，也是很耗神费时的事儿。对付世间闹心的事儿，只需要搞清楚两件事，一件是“关我屁事”，另一件是“关你屁事”。\n46. 小孩在天地间疯跑，不知道名利为何物，学习基本常识，食蔬饮水，应付无聊的课程，傻愣愣地杀无聊的时间，骂所有看不上的人“傻逼”。本身近佛，不需要佛。\n47. 有多少似乎过不起的事儿过不去一年？有多少看上去的大事最后真是大事？名片上印不下的名头，敌不过左图且书、右琴与壶，抵不过不得不褪去时一颗好心脏、一个好女生。\n48. 这样简单下去，再简单下去，脑子没弯儿了，手脚有劲儿了，山顶慢慢低于脚面了，拉萨就在眼前了。你我竟然像山、云、湖水和星空一样，一直在老去，一直在变化，一直没问题。再简单下去，在这样下去，你我都是佛了。\n49. 我常年劳碌，尽管热爱妇女，但没时间，无法让任何妇女满意。情伤之后，“得不到”，“留不住”，“无可奈何，奈何奈何”，唯一疗伤的方式就是拿伤口当笔头，写几行诗，血干了，诗出了，心里放下了。\n50. 如果去一座荒岛，没电，没电视，没电脑，一片蛮荒。我想了想，如果只能带一个活物。我就带一个和我能聊很多天的女人；如果只能带一本书，我就带一本《唐诗三百首》。","categories":["冯唐"]},{"title":"LDA","url":"/2019/04/22/LDA/","content":"\n# 前言\n在[PCA](https://louishsu.xyz/2018/10/22/PCA/)中，介绍了无监督降维方法，主成分分析(Principal Components Analysis)。从投影后数据方差最大的角度出发，选取主轴。下面介绍一种有监督的降维方法，线性判别分析(Linear Discriminant Analysis)。\n\n# 原理\n设有$n$维样本集$D=\\{x^{(1)}, ..., x^{(i)}, ..., x^{(m)}\\}$，所属类别数目为$C$，现求其第一投影轴$u_1$，即\n$$\n\\tilde{x}^{(i)}_1 = u_1^Tx^{(i)}\n$$\n\n现希望投影后，**类内距离越小越好，类间距离越大越好**，定义衡量指标\n1. 类内离差阵\n    $$\n    S_W = \\sum_{j=1}^C \\frac{m_j}{m} \\left[ \\frac{1}{m_j} \\sum_{i=1}^{m_j} (x^{(i)} - \\mu^{(j)}) (x^{(i)} - \\mu^{(j)})^T \\right] \\tag{1}\n    $$\n\n    即\n    $$\n    S_W = \\frac{1}{m} \\sum_{j=1}^C \\sum_{i=1}^{m_j} (x^{(i)} - \\mu^{(j)}) (x^{(i)} - \\mu^{(j)})^T \\tag{1}\n    $$\n\n2. 类间离差阵\n    $$\n    S_B = \\sum_{j=1}^C \\frac{m_j}{m} (\\mu^{(j)} - \\mu) (\\mu^{(j)} - \\mu)^T \\tag{2}\n    $$\n\n则投影到第一主轴$u_1$后数据的类内离差阵和类间离差阵为\n$$\n\\tilde{S_W} = \\sum_{j=1}^C \\frac{m_j}{m} \\left[ \\frac{1}{m_j} \\sum_{i=1}^{m_j} \n(\\tilde{x}^{(i)}_1 - \\tilde{\\mu}^{(j)}_1) (\\tilde{x}^{(i)}_1 - \\tilde{\\mu}^{(j)}_1)^T \\right]\n$$\n\n$$\n\\tilde{S_B} = \\sum_{j=1}^C \\frac{m_j}{m} \n(\\tilde{\\mu}^{(j)}_1 - \\tilde{\\mu}_1) (\\tilde{\\mu}^{(j)}_1 - \\tilde{\\mu}_1)^T\n$$\n\n其中\n$$\n\\tilde{x}^{(i)}_1 = u_1^T x^{(i)}\n$$\n\n$$\n\\tilde{\\mu}^{(j)}_1 = u_1^T \\mu^{(j)}\n$$\n\n$$\n\\tilde{\\mu}_1 = u_1^T \\mu\n$$\n\n带入后得到\n$$\n\\tilde{S_W} = \\frac{1}{m} \\sum_{j=1}^C \\sum_{i=1}^{m_j} \n(u_1^T x^{(i)} - u_1^T \\mu^{(j)}) (u_1^T x^{(i)} - u_1^T \\mu^{(j)})^T\n$$\n\n$$\n= \\frac{1}{m} \\sum_{j=1}^C \\sum_{i=1}^{m_j} \nu_1^T(x^{(i)} - \\mu^{(j)}) (x^{(i)} - \\mu^{(j)})^T u_1\n$$\n\n$$\n= u_1^T S_W u_1\n$$\n\n同理\n$$\n\\tilde{S_B} = u_1^T S_B u_1\n$$\n\n定义优化目标\n$$\nJ = \\min \\left\\{ \\frac{\\tilde{S_W}}{\\tilde{S_B}} \\right\\}\n= \\min \\left\\{ \\frac{u_1^T S_W u_1}{u_1^T S_B u_1} \\right\\} \\tag{3}\n$$\n\n取\n$$\nL(u_1) = \\frac{u_1^T S_W u_1}{u_1^T S_B u_1} \\tag{4}\n$$\n\n则其极值为\n$$\n\\frac{∂L}{∂u_1} = \\frac{2(u_1^T S_B u_1) S_W u_1 - 2(u_1^T S_W u_1) S_B u_1}{(u_1^T S_B u_1)^2} = 0\n$$\n\n得到\n$$\n(u_1^T S_B u_1) S_W u_1 = (u_1^T S_W u_1) S_B u_1\n$$\n\n令$\\lambda_1 = \\frac{u_1^T S_B u_1}{u_1^T S_W u_1}$，有\n$$\nS_B u_1 = \\lambda_1 S_W u_1 \\tag{5}\n$$\n\n当$m$较大时，$S_W$一般非奇异，故\n$$\nS_W^{-1} S_B u_1 = \\lambda_1 u_1 \\tag{6}\n$$\n\n即$\\{\\lambda_1, u_1\\}$为矩阵$S_W^{-1} S_B$的特征对。\n\n更进一步，对于而分类问题，我们有\n$$\nS_B = \\sum_{j=1}^C \\frac{m_j}{m} (\\mu^{(j)} - \\mu) (\\mu^{(j)} - \\mu)^T\n$$\n\n$$\n= \\frac{m_1}{m} (\\mu^{(1)} - \\mu) (\\mu^{(1)} - \\mu)^T + \\frac{m_2}{m} (\\mu^{(2)} - \\mu) (\\mu^{(2)} - \\mu)^T\n$$\n\n$$\n= \\frac{m_1}{m} \\mu^{(1)} \\mu^{(1)T} + \\frac{m_2}{m} \\mu^{(2)} \\mu^{(2)T} - 2 \\mu \\left( \\frac{m_1}{m} \\mu^{(1)T} + \\frac{m_2}{m} \\mu^{(2)T} \\right) + \\mu \\mu^T\n$$\n\n其中\n$$\n\\frac{m_1}{m} \\mu^{(1)} + \\frac{m_2}{m} \\mu^{(2)} = \\mu \\tag{*1}\n$$\n\n所以$(7)$带入$S_B$化简得\n$$\nS_B = \\frac{m_1}{m} \\mu^{(1)} \\mu^{(1)T} + \\frac{m_2}{m} \\mu^{(2)} \\mu^{(2)T} - \\mu \\mu^T\n$$\n\n$$\n= \\frac{m_1}{m} \\mu^{(1)} \\mu^{(1)T} + \\frac{m_2}{m} \\mu^{(2)} \\mu^{(2)T} - (\\frac{m_1}{m} \\mu^{(1)} + \\frac{m_2}{m} \\mu^{(2)}) (\\frac{m_1}{m} \\mu^{(1)} + \\frac{m_2}{m} \\mu^{(2)})^T\n$$\n\n$$\n= \\frac{m_1}{m} \\left(1-\\frac{m_1}{m}\\right) \\mu^{(1)} \\mu^{(1)T} + \\frac{m_2}{m} \\left(1-\\frac{m_2}{m}\\right) \\mu^{(2)} \\mu^{(2)T} - \\frac{m_1}{m} \\frac{m_2}{m} \\mu^{(2)} \\mu^{(1)T} - \\frac{m_1}{m} \\frac{m_2}{m} \\mu^{(1)} \\mu^{(2)T}\n$$\n\n$$\n= \\left( \\frac{m_1}{m} \\mu^{(1)} - \\frac{m_2}{m} \\mu^{(2)} \\right) \\left( \\frac{m_1}{m} \\mu^{(1)} - \\frac{m_2}{m} \\mu^{(2)} \\right)^T\n$$\n\n$S_B$带入式$(*)$，得\n$$\n左边 = S_W^{-1} \\left( \\frac{m_1}{m} \\mu^{(1)} - \\frac{m_2}{m} \\mu^{(2)} \\right) \\left( \\frac{m_1}{m} \\mu^{(1)} - \\frac{m_2}{m} \\mu^{(2)} \\right)^T u_1\n$$\n\n其中$\\left( \\frac{m_1}{m} \\mu^{(1)} - \\frac{m_2}{m} \\mu^{(2)} \\right)^T u_1$为常数，记作$\\alpha$，所以\n$$\nu_1 = \\frac{\\alpha}{\\lambda_1} S_W^{-1} \\left( \\frac{m_1}{m} \\mu^{(1)} - \\frac{m_2}{m} \\mu^{(2)} \\right) \\tag{*2}\n$$\n\n其中常数$\\frac{\\alpha}{\\lambda_1}$不影响投影结果，如取$1$，则得到\n$$\nu_1 = S_W^{-1} \\left( \\frac{m_1}{m} \\mu^{(1)} - \\frac{m_2}{m} \\mu^{(2)} \\right)\n$$\n\n同理，可求得第二、三主成分轴\n\n# 计算步骤\n给定数据集矩阵\n$$\nX = \\left[ \\begin{matrix}\n    ... \\\\\n    x^{(i)T} \\\\\n    ... \n\\end{matrix} \\right]\n$$\n\n其中$x^{(i)} = \\left[ x^{(i)}_1, ..., x^{(i)}_j, ... x^{(i)}_n \\right]^T$\n\n1. 计算类内离差阵$S_W$与类间离差阵$S_B$；\n2. 计算矩阵$S_W^{-1}S_B$的特征对$(\\lambda_i, u_i)$；\n3. 按特征值从大到小排序，选取最大的特征值作为第一主轴；\n4. 将数据投影到主轴上；\n\n# 应用\nLDA可用于分类，以二分类为例，在求取主轴$u_1$后，将各类中心投影到主轴上，即\n$$\n\\begin{cases}\n    \\tilde{\\mu}^{(1)}_1 = u_1^T \\mu^{(1)} \\\\\n    \\tilde{\\mu}^{(2)}_1 = u_1^T \\mu^{(2)}\n\\end{cases}\n$$\n\n选取阈值，如\n$$\n\\tilde{x}_1 = \\frac{\\tilde{\\mu}^{(1)}_1 + \\tilde{\\mu}^{(2)}_1}{2}\n$$\n\n则预测时，判决方程为\n$$\n\\begin{cases}\n    u_1^T x^{(i)} < \\tilde{x}_1 \\Rightarrow x^{(i)} \\in \\omega_1 \\\\\n    u_1^T x^{(i)} > \\tilde{x}_1 \\Rightarrow x^{(i)} \\in \\omega_2\n\\end{cases}\n$$\n\n# 代码\n\n详情查看[Github](https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/lda.py)\n\n<details>\n<summary>展开</summary>\n<pre><code>\nclass LDA(object):\n    \"\"\" 先行判别分析\n\n    Attributes:\n        n_components:   {int} 主成分个数\n        axis:           {ndarray(n_features, n_component)}\n    Notes:\n        S_W = \\frac{1}{m} \\sum_{j=1}^C \\sum_{i=1}^m_j (x^{(i)} - \\mu^{(j)}) (x^{(i)} - \\mu^{(j)})^T\n        S_B = \\sum_{j=1}^C \\frac{m_j}{m} (\\mu^{(j)} - \\mu) (\\mu^{(j)} - \\mu)^T\n    Example:\n\n    \"\"\"\n\n    def __init__(self, n_components=-1):\n        self.n_components = n_components\n        self.axis = None\n\n    def fit(self, X, y, prop=0.99):\n        \"\"\" train the model\n        Params:\n            X:      {ndarray(n_samples, n_features)}\n            y:      {ndarray(n_samples)}\n            prop:   {float}:  在[0, 1]范围内，表示取特征值之和占所有特征值的比重\n        Notes:\n            - `prop`参数仅在`n_components=-1`时生效\n        \"\"\"\n        labels = list(set(list(y)))\n        n_class = len(labels)\n        n_samples, n_feats = X.shape\n\n        ## 计算 S_W, S_B\n        S_W = np.zeros(shape=(n_feats, n_feats))\n        S_B = np.zeros(shape=(n_feats, n_feats))\n        mean_ = np.mean(X, axis=0)\n        for i_class in range(n_class):\n            X_ = X[y==labels[i_class]]\n            weight = X_.shape[0] / n_samples\n            means_ = np.mean(X_, axis=0)\n            S_W += ((X_ - means_).T).dot(X_ - means_) / X_.shape[0] * weight\n            S_B += (means_ - mean_).dot((means_ - mean_).T) * weight\n\n        ## 计算特征对\n        eigVal, eigVec = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n        order = np.argsort(eigVal)[::-1]\n        eigVal = eigVal[order]\n        eigVec = eigVec.T[order].T\n\n        ## 选取主轴\n        if self.n_components == -1:\n            sumOfEigVal = np.sum(eigVal)\n            sum_tmp = 0\n            for k in range(eigVal.shape[0]):\n                sum_tmp += eigVal[k]\n                if sum_tmp > prop * sumOfEigVal:\n                    self.n_components = k + 1\n                    break\n        self.axis = eigVec[:, :self.n_components]\n\n    def transform(self, X):\n        \"\"\"\n        Params:\n            X:  {ndarray(n_samples, n_features)}\n        Returns:\n            X:  {ndarray(n_samples, n_components)}\n        \"\"\"\n        X = X.dot(self.axis)\n        return X\n    \n    def fit_transform(self, X, y, prop=0.99):\n        \"\"\"\n        Params:\n            X:  {ndarray(n_samples, n_features)}\n        Returns:\n            X:  {ndarray(n_samples, n_components)}\n        \"\"\"\n        self.fit(X, y, prop=prop)\n        X = self.transform(X)\n        return X\n    \n    def transform_inv(self, X):\n        \"\"\"\n        Params:\n            X:  {ndarray(n_samples, n_components)}\n        Returns:\n            X:  {ndarray(n_samples, n_features)}\n        \"\"\"\n        X = X.dot(self.axis.T)\n        return X\n</code></pre>\n</details>\n\n与PCA投影结果对比如下\n- PCA\n    ![PCA](/LDA/PCA.png)\n- LDA\n    ![LDA](/LDA/LDA.png)\n\n# Reference\n1. [机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)","categories":["Machine Learning"]},{"title":"N-dim Array PCA","url":"/2019/04/18/n-dim-Array-PCA/","content":"\n# 前言\n在[PCA](https://louishsu.xyz/2018/10/22/PCA/)中介绍了1维数据的主成分分析，那么对于多维数据，如何进行处理呢？有一种做法是，将单份的样本数据展开为1维向量，再进行PCA，例如著名的[Eigenface](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E8%84%B8)，如图所示\n\n![Eigenfaces](/n-dim-Array-PCA/Eigenfaces.png)\n\n这有一个缺点是，忽略了多维数据的空间信息，且计算时，若展开后维度过大，协方差矩阵的求逆过程非常耗时间，以下介绍多维主成分分析。\n\n# 原理\n对于单维$n$维度数据$X = \\left[\\begin{matrix} x^{(1)} \\\\ x^{(2)} \\\\ ... \\\\ x^{(N)} \\end{matrix}\\right]$，我们有\n$$\nX' = XU\n$$\n\n其中，$x^{(i)}$为$n$维行向量，组成样本矩阵$X_{N×n}$，$U_{n×n_1}$为投影矩阵，$X'_{N×n_1}$为降维后的样本矩阵。\n\n对于多(n)维样本张量数据$X_{N×n_{d_1}×n_{d_2}×...×n_{d_n}}$，指定各维度降维顺序，在$d_i$维度上，我们将张量在该维度上展开为$1$维向量\n$$\nX_{N_{d_i}×n_{d_i}}\n$$\n\n其中\n$$\nN_{d_i} = \\prod_{j=0,j≠i}^n n_{d_j}\n$$\n\n然后在$d_i$维度上进行降维，即\n$$\nX'_{N_{d_i}×n'_{d_i}} = X_{N_{d_i}×n_{d_i}} U^{d_i}\n$$\n\n其中$U^{d_i}$表示$d_i$维度的投影矩阵，其大小为$n_{d_i}×n'_{d_i}$，然后，将该张量其余维度恢复，得到\n$$\nX^{d_i}_{N×n_{d_1}×..×n'_{d_i}×...×n_{d_n}}\n$$\n\n如此循环，在各维度进行降维，得到张量\n$$\nX^{d_1, ..., d_n}_{N×n'_{d_1}×..×n'_{d_i}×...×n'_{d_n}}\n$$\n\n**注意，不同的降维顺序得到的参数会存在差异。**\n\n以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$，其降维过程表示如图\n- 在$H$维度上\n\n    ![d1](/n-dim-Array-PCA/d1.JPG)\n\n- 在$W$维度上\n\n    ![d2](/n-dim-Array-PCA/d2.JPG)\n    \n- 在$C$维度上\n\n    ![d3](/n-dim-Array-PCA/d3.JPG)\n\n# 代码实现\n\n同样的，以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$\n1. 在$H$维度上\n   1. 将该张量转置，得到$X^{T_{d_1d_3}}(C, W, H)$\n   2. 将其展开为$X_{flatten}(N_H×H)$，其中$N_H=C×W$\n   3. 降维得到$X'_{flatten}(N_H×H')$\n   4. 重新将另外两维恢复，得到$X^{T_{d_1d_3}'}(C, W, H')$\n   5. 将张量转置，得到$X^{d_1}(H', W, C)$\n\n2. 在$W$维度上\n   1. 将$X^{d_1}(H', W, C)$转置，得到$X^{T_{d_2d_3}}(H', C, W)$\n   2. 将其展开为$X_{flatten}(N_W×W)$，其中$N_W=H'×C$\n   3. 降维得到$X'_{flatten}(N_W×W')$\n   4. 重新将另外两维恢复，得到$X^{T_{d_2d_3}'}(H', C, W')$\n   5. 将张量转置，得到$X^{d_1d_2}(H', W', C)$\n\n3. 在$C$维度上\n   1. 将$X^{d_1d_2}(H', W', C)$展开为$X_{flatten}(N_C×C)$，其中$N_C=H'×W'$\n   2. 降维得到$X'_{flatten}(N_C×C')$\n   3. 重新将另外两维恢复，得到$X^{d_1d_2d_3}(H', W', C')$\n\n\n详细查看[GitHub](https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/tensor_pca.py)，其核心代码如下\n\n```\ndef transform(self, X):\n    \"\"\"\n    Params:\n        X: {ndarray(n_samples, d0, d1, d2, ..., dn-1)} n-dim array\n    Returns:\n        X: {ndarray(n_samples, d0, d1, d2, ..., dn-1)} n-dim array\n    \"\"\"\n\n    assert self.n_dims == len(X.shape) - 1, 'please check input dimension! '\n    idx = [i for i in range(len(X.shape))]   # index of dimensions\n\n    for i_dim in range(self.n_dims):\n            \n        ## transpose tensor\n        idx[-1], idx[i_dim + 1] = idx[i_dim + 1], idx[-1]\n        X = X.transpose(idx)\n        shape = list(X.shape)\n\n        # 1-dim pca\n        X = X.reshape((-1, shape[-1]))\n        X = self.decomposers[i_dim].transform(X)\n\n        ## transpose tensor\n        X = X.reshape(shape[:-1]+[X.shape[-1]])\n        X = X.transpose(idx)\n        idx[-1], idx[i_dim + 1] = idx[i_dim + 1], idx[-1]\n        \n    return X\n```\n\n# 实验显示\n将2维数据降维，指定通道维度数目从$3$降至$1$，得到实验结果如下\n- 原始数据\n    ![origin_data](/n-dim-Array-PCA/origin_data.png)\n\n- 降维后数据\n    ![transform](/n-dim-Array-PCA/transform.png)\n    \n- 重建数据\n    ![inv_transform](/n-dim-Array-PCA/inv_transform.png)\n\n# Reference\n1. [特征脸 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E8%84%B8)","categories":["Machine Learning"]},{"title":"Python字符串格式化","url":"/2019/02/19/Python字符串格式化/","content":"\n# 前言\nPython操作字符串行云流水，当然也支持格式化字符串。\n\n# 通过格式符\n```\nprint(\"我叫%s, 今年%d岁\" % ('Louis Hsu', 18))\n```\n\n或者使用字典进行值传递\n```\nprint(\"我叫%(name), 今年%(age)岁\" % {'name': 'Louis Hsu', 'age': 18})\n```\n\n**typecode**\n| 格式符 | 含义 |\n| ------ | ------ |\n| %s | 字符串 (采用str()的显示) | \n| %r | 字符串 (采用repr()的显示) | \n| %c | 单个字符 | \n| %b | 二进制整数 | \n| %d | 十进制整数 | \n| %i | 十进制整数 | \n| %o | 八进制整数 | \n| %x | 十六进制整数 | \n| %e | 指数 (基底写为e) | \n| %E | 指数 (基底写为E) | \n| %f | 浮点数 | \n| %F | 浮点数，与上相同 | \n| %g | 指数(e)\u0010或浮点数 (根据显示长度) | \n| %G | 指数(E)或浮点数 (根据显示长度) | \n| %% | 字符\"%\" | \n\n**高阶**\n```\n% [flags] [width].[precision] typecode\n- flags:        '+'(右对齐), '-'(左对齐), ' '(左侧填充一个空格，与负数对齐), '0'(用0填充)\n- width:        显示宽度\n- precision:    小数精度位数，可使用'*'进行动态代入\n- typecode:     格式符\n```\n\n例如\n```\nprint('pi is %+2.2f' % (3.1415926)) -> pi is +3.14\nprint('pi is %-2.2f' % (3.1415926)) -> pi is 3.14\nprint('pi is % 2.2f' % (3.1415926)) -> pi is  3.14\nprint('pi is %02.2f' % (3.1415926)) -> pi is 3.14\n\n# 同\nprint('pi is %+*.*f' % (2, 2, 3.1415926))\nprint('pi is %-*.*f' % (2, 2, 3.1415926))\nprint('pi is % *.*f' % (2, 2, 3.1415926))\nprint('pi is %0*.*f' % (2, 2, 3.1415926))\n```\n\n# 通过format\n## 位置传递\n1. 使用位置参数\n    ```\n    >>> li = ['hoho',18]\n    >>> 'my name is {} ,age {}'.format('hoho',18)\n    'my name is hoho ,age 18'\n    >>> 'my name is {1} ,age {0}'.format(10,'hoho')\n    'my name is hoho ,age 10'\n    >>> 'my name is {1} ,age {0} {1}'.format(10,'hoho')\n    'my name is hoho ,age 10 hoho'\n    >>> 'my name is {} ,age {}'.format(*li)\n    'my name is hoho ,age 18'\n    ```\n\n2. 使用关键字参数\n    ```\n    >>> hash = {'name':'hoho','age':18}\n    >>> 'my name is {name},age is {age}'.format(name='hoho',age=19)\n    'my name is hoho,age is 19'\n    >>> 'my name is {name},age is {age}'.format(**hash)\n    'my name is hoho,age is 18'\n    ```\n    \n## 格式限定\n基本格式如下\n```\n{[:pad][align][sign][typecode]}\n- :pad :    填充字符，空白位用该字符填充\n- align:    '^', '<', '>' 分别表示 '居中', '左对齐', '右对齐'(默认)，后面加宽度\n- sign :    '+', '-' , ' ' 分别表示 '正', '负', '正数前加空格'\n- typecode: 'b', 'd', 'o', 'x', 'f', ',', '%', 'e' 分别表示 '二进制', '十进制', '八进制', '十六进制', '浮点数', '逗号分隔', '百分比格式',  '指数记法'\n```\n\n# Reference\n> python之字符串格式化(format) - benric - 博客园 https://www.cnblogs.com/benric/p/4965224.html\n> Python format 格式化函数 | 菜鸟教程 http://www.runoob.com/python/att-string-format.html","categories":["Python"]},{"title":"Python命令行参数解析","url":"/2019/02/18/Python命令行参数解析/","content":"\n# 前言\n\n# C/C++的参数传递\n我们知道C/C++主函数形式如下\n```\nint main(int argc,char * argv[],char * envp[])\n{\n    // do something\n    return 0;\n}\n```\n其中各参数含义如下\n- `argc`：`argument count`，表示参数数量\n- `argv`：`argument value`，表示参数值\n    最后一个元素存放了一个NULL的指针\n- `envp`：系统环境变量\n    最后一个元素存放了一个NULL的指针\n\n例如\n```\n#include<stdio.h>\n\nint main(int argc,char * argv[],char * envp[])\n{\n    printf(\"argc is %d \\n\", argc);\n \n    int i;\n    for (i=0; i<argc; i++)\n    {\n        printf(\"arcv[%d] is %s\\n\", i, argv[i]);\n    }\n\n    for (i=0; envp[i]!=NULL; i++)\n    {\n        printf(\"envp[%d] is %s\\n\", i, envp[i]);\n    }\n\t\n\treturn 0;\n}\n```\n\n测试平台为Windows10，执行编译和运行操作，结果如下:\n```\n> gcc main.c -o main.exe\n> ./main.exe param1 param2 param3 param4\nargc is 5\narcv[0] is C:\\OneDrive\\▒ĵ▒\\Louis' Blog\\source\\_drafts\\Python▒▒▒▒▒в▒▒▒▒▒▒▒\\test.exe\narcv[1] is param1\narcv[2] is param2\narcv[3] is param3\narcv[4] is param4\nenvp[0] is ACLOCAL_PATH=C:\\MyApplications\\Git\\mingw64\\share\\aclocal;C:\\MyApplica                                                                                                                tions\\Git\\usr\\share\\aclocal\nenvp[1] is ALLUSERSPROFILE=C:\\ProgramData\n...(省略)\nenvp[71] is WINDIR=C:\\WINDOWS\nenvp[72] is _=./main.exe\n```\n\n# Python的参数传递\n可以使用sys模块得到命令行参数，主函数文件`main.py`如下\n```\nimport sys\n\nif __name__ == '__main__':\n    print(sys.argv)\n```\n\n执行\n```\n> python main.py param1 param2 pram3\n['main.py', 'param1', 'param2', 'pram3']\n```\n\n# getopt\n```\nimport sys\nimport getopt\nimport argparse\n\nif __name__ == '__main__':\n    argv = sys.argv\n    \n    if len(argv) == 1:\n        print(\n            \"\"\"\n            Usage: python main.py [option]\n            -h or --help:    显示帮助信息\n            -v or --version: 显示版本\n            -i or --input:   指定输入文件路径\n            -o or --output:  指定输出文件路径\n\n            \"\"\"\n        )\n\n    try:\n        opts, args = getopt.getopt(args=argv[1:],\n                                   shortopts='hvi:o:',\n                                   longopts=['help', 'version', 'input=', 'output=']\n                                )\n    except getopt.GetoptError:\n        print(\"argv error,please input\")\n        sys.exit(1)\n\n    for cmd, arg in opts:\n\n        if cmd in ['-h', '--help']:\n            print(\"help info\")\n            sys.exit(0)\n        elif cmd in ['-v', '--version']:\n            print(\"main 1.0\")\n            sys.exit(0)\n\n        if cmd in ['-i', '--input']:\n            input = arg\n        if cmd in ['-o', '--output']:\n            output = arg\n```\n\n\n说明\n- `args=sys.argv[1:]`\n    传入的参数，除去`sys.argv[0]`，即主函数文件路径\n\n- `shortopts='hvi:o:'`\n    字符串，支持形如`-h`的选项\n    - 若无需指定参数，形如`c`；\n    - 若必须指定参数，则需为`c:`；\n\n- `longopts=['help', 'version', 'input=', 'output=']`\n    字符串列表，可选参数，是否支持形如`--help`的选项\n    - 若无需指定参数，形如`cmd`；\n    - 若必须指定参数，则需为`cmd=`；\n\n执行\n```\n> python main.py\n\n            Usage: python main.py [option]\n            -h or --help:    显示帮助信息\n            -v or --version: 显示版本\n            -i or --input:   指定输入文件路径\n            -o or --output:  指定输出文件路径\n\n\n> python main.py -h\nhelp info\n\n> python main.py -v\nmain 1.0\n\n> python main.py -i\nargv error,please input\n\n> python main.py -i a.txt -o b.txt\n\n```\n\n# argsparse\n```\nimport argparse\n\ndef show_args(args):\n    if args.opencv:\n        print(\"opencv is used \")\n    else:\n        print(\"opencv is not used \")\n\n    print(args.steps)\n    print(args.file)\n    print(args.data)\n    \n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description=\"learn to use `argparse`\")\n\n    # 标志位\n    parser.add_argument('--opencv', '-cv', action='store_true', help='use opencv if set ')\n    # 必需参数\n    parser.add_argument('--steps', '-s', required=True, type=int, help='number of steps')\n    # 默认参数\n    parser.add_argument('--file', '-f', default='a.txt')\n    # 候选参数\n    parser.add_argument('--data', '-d', choices=['data1', 'data2'])\n\n    \n    args = parser.parse_args()\n    show_args(args)\n\n```\n\n说明\n- 帮助信息\n    参数`help`，用于显示在`-h`帮助信息中\n- 标志位参数\n    参数`action='store_true'`，即保存该参数为`True`\n- 必需参数\n    置位`required`，即运行该程序必须带上该参数，否则报错\n- 默认参数\n    参数`default`填写默认参数\n- 候选参数\n    参数`choices`填写候选参数列表\n\n运行\n```\n# 显示帮助信息\n> python main.py -h\nusage: main.py [-h] [--opencv] --steps STEPS [--file FILE]\n               [--data {data1,data2}]\n\nlearn to use `argparse`\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --opencv, -cv         use opencv if set\n  --steps STEPS, -s STEPS\n                        number of steps\n  --file FILE, -f FILE\n  --data {data1,data2}, -d {data1,data2}\n\n\n# 测试必须参数\n> python main.py\nusage: main.py [-h] [--opencv] --steps STEPS [--file FILE]\n               [--data {data1,data2}]\nmain.py: error: the following arguments are required: --steps/-s\n\n> python main.py -s 100\nopencv is not used\n100\na.txt\nNone\n\n# 测试标志位参数\n> python main.py -s 100 -cv\nopencv is used\n100\na.txt\nNone\n\n# 测试默认参数\n> python main.py -s 100 -f b.txt\nopencv is not used\n100\nb.txt\nNone\n\n# 测试可选参数\n> python main.py -s 100 -d data1\nopencv is not used\n100\na.txt\ndata1\n\n> python main.py -s 100 -d data0\nusage: main.py [-h] [--opencv] --steps STEPS [--file FILE]\n               [--data {data1,data2}]\nmain.py: error: argument --data/-d: invalid choice: 'data0' (choose from 'data1', 'data2')\n\n```\n\n# Reference\n> Python命令行参数解析：getopt和argparse - 死胖子的博客 - CSDN博客 https://blog.csdn.net/lanzheng_1113/article/details/77574446\n> Python模块之命令行参数解析 - 每天进步一点点！！！ - 博客园 https://www.cnblogs.com/madsnotes/articles/5687079.html\n> Python解析命令行读取参数 -- argparse模块 - Arkenstone - 博客园 https://www.cnblogs.com/arkenstone/p/6250782.html","categories":["Python"]},{"title":"Python生成词云图","url":"/2019/02/17/Python生成词云图/","content":"\n# 前言\n一个没什么用的小技能\n\n# 模块\n> wordcloud · PyPI https://pypi.org/project/wordcloud/\n\n安装该模块\n```\n> pip install wordcloud\n```\n\n主要用到的为\n```\nwordcloud.WordCloud(font_path=None, width=400, height=200, margin=2,\n                ranks_only=None, prefer_horizontal=.9, mask=None, scale=1,\n                color_func=None, max_words=200, min_font_size=4,\n                stopwords=None, random_state=None, background_color='black',\n                max_font_size=None, font_step=1, mode=\"RGB\",\n                relative_scaling='auto', regexp=None, collocations=True,\n                colormap=None, normalize_plurals=True, contour_width=0,\n                contour_color='black', repeat=False)\n```\n\n# 使用例程\n```\nimport os\nimport cv2\nimport numpy as np\nfrom wordcloud import WordCloud\n\nfont = 'C:/Windows/Fonts/SIMYOU.TTF'    # 幼圆\nstring = 'LouisHsu 单键 小叔叔 想静静 95后 傲娇 skrrrrrrr 大猫座 佛了 要秃 嘤嘤嘤 真香'\n\nmask = cv2.imread('./mask.jpg', cv2.IMREAD_GRAYSCALE)\nthresh, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n\nwc = WordCloud(\n        font_path=font, \n        background_color='white',\n        color_func=lambda *args, **kwargs: (0,0,0),\n        mask=mask,\n        max_words=500,\n        min_font_size=4,\n        max_font_size=None,\n        contour_width=1,\n        repeat=True                     # 允许词重复\n    )\nwc.generate_from_text(string)\nwc.to_file('./wc.jpg')                  #保存图片\n```\n\n输入原图为\n![mask](/Python生成词云图/mask.jpg)\n\n生成图像\n![wc](/Python生成词云图/wc.jpg)\n\n# Reference\n> python WordCloud 简单实例 - 博客 - CSDN博客 https://blog.csdn.net/cy776719526/article/details/80171790\n","categories":["Python"]},{"title":"makefile简单教程","url":"/2019/01/05/makefile简单教程/","content":"\n# 前言\n\n# 准备源文件\n新建目录`demo/`，其结构如下\n```\n\\-- demo\n    \\-- bin             # 二进制文件，即可执行文件\n    \\-- include         # 头文件`.h`\n        |-- hello.h\n    \\-- obj             # 目标文件`.o`\n        |-- *.o\n    \\-- src             # 源文件`.c`\n        |-- hello.c\n        |-- test.c\n```\n\n编辑`hello.h`\n```\n#ifndef __HELLO_H\n#define __HELLO_H\n\n#include <stdio.h>\n\nvoid __hello();\n\n#endif\n```\n\n编辑`hello.c`\n```\n#include \"hello.h\"\n\nvoid __hello()\n{\n\tprintf(\"Hello world!\\n\");\n}\n```\n\n编辑`test.h`\n```\n#include \"hello.h\"\n\nint main()\n{\n\t__hello();\n\treturn 0;\n}\n```\n\n\n# 命令行编译\n```\n$ ls\nhello.c  hello.h  makefile  README.md  test.c\n$ gcc test.c hello.c -o test\n$ ls\nhello.c  hello.h  makefile  README.md  test  test.c\n$ ./test\nHello world!\n```\n\n但是这样编译会每次都重新编译整个工程，时间比较长，所以可以先生成`.o`文件，当`test.c`代码改动后，重新生成`test.o`即可，`hello.o`不用重新编译\n```\n$ ls\nhello.c  hello.h  makefile  README.md  test.c\n$ gcc test.c hello.c -c\n$ ls\nhello.c  hello.h  hello.o  makefile  README.md  test.c  test.o\n$ gcc test.o hello.o -o test\n$ ls\nhello.c  hello.h  hello.o  makefile  README.md  test  test.c  test.o\n$ ./test\nHello world!\n```\n\n# Makefile\n## 格式\n```\n<target>: <dependencies>\n\t<command>\t\t# [TAB]<command>\n```\n\n例如\n```\ntest: test.c\n\tgcc test.c -o test\n```\n\n## \n编辑`Makefile`\n```\n# directories & target name\nDIR_INC = ./include  \nDIR_SRC = ./src\nDIR_OBJ = ./obj\nDIR_BIN = ./bin\nTARGET  = test\n\n# compile macro  \nCC      = gcc\nCFLAGS  = -g -Wall -I${DIR_INC}\t\t                 # `-g`表示调试选项，`-Wall`表示编译后显示所有警告\n\n# load source files\nSRC = $(wildcard ${DIR_SRC}/*.c)                         # 匹配目录中所有的`.c`文件\n\n# build target\nOBJ = $(patsubst %.c, ${DIR_OBJ}/%.o, ${notdir ${SRC}})  # 由`SRC`字符串内容，指定生成`.o`文件的名称与目录\nBIN = ${DIR_BIN}/${TARGET}                               # 指定可执行文件名称与目录\n\n# build\n${BIN}: ${OBJ}\n\t$(CC) $(OBJ) -o $@                               # 即 `$ gcc ./obj/*.o -o ./bin/test`\n${DIR_OBJ}/%.o: ${DIR_SRC}/%.c\n\t$(CC) $(CFLAGS) -c $< -o $@                      # 即 `$ gcc ./src/*.c -g -Wall -I./include -c ./obj/*.o`\n\n# clean\n.PHONY: clean                                            # 伪目标\nclean:\n\tfind ${DIR_OBJ} -name *.o -exec rm -rf {} \\;\n```\n\n执行\n```\n$ make\ngcc -g -Wall -I./include  \t\t -c src/hello.c -o obj/hello.o\ngcc -g -Wall -I./include  \t\t -c src/test.c -o obj/test.o\ngcc  ./obj/hello.o  ./obj/test.o  -o bin/test\n$ ls obj/\nhello.o  test.o\n$ ls bin/\ntest\n$ ./bin/test \nHello world!\n$ make clean\nfind ./obj -name *.o -exec rm -rf {} \\;\n$ ls obj/\n$ ls bin/\ntest\n```\n\n## 解释\n1.  符号 `$@`, `$^`, `$<`，`$?`\n\t- `$@`: 表示目标文件\n\t- `$^`: 表示所有的依赖文件\n\t- `$<`: 表示第一个依赖文件\n\t- `$?`: 表示比目标还要新的依赖文件列表\n2. `wildcard`，`notdir`，`patsubst`\n\t- `wildcard`\t: 扩展通配符\n\t\t`SOURCES = $(wildcard *.c)`: 产生一个所有以 ’.c’ 结尾的文件的列表，然后存入变量 SOURCES 里。 \n\t- `notdir`\t: 去除路径，可以在使用`wildcard`函数后，再配合使用`notdir`函数只得到文件名（不含路径）。\n\t- `patsubst`\t: 替换通配符，需要３个参数，第一个是个需要匹配的式样，第二个表示用什么来替换他，第三个是个需要被处理的由空格分隔的字列。\n\t\t`OBJS = $(patsubst %.c,%.o,$(SOURCES))`\n\t\t\t- 将处理所有在 SOURCES 字列中的字（一列文件名），如果他的 结尾是 `.c` ，就用 `.o` 把 `.c`取代\n\t\t\t- 这里的 % 符号将匹配一个或多个字符，而他每次所匹配的字串叫做一个‘柄’(stem) \n\t\t\t- 在第二个参数里， %被解读成用第一参数所匹配的那个柄。\n3. `-I`，`-L`，`-l`\n\t- `-I`: 将指定目录作为第一个寻找头文件的目录\n\t- `-L`: 将指定目录作为第一个寻找库文件的目录\n\t- `-l`: 在库文件路径中寻找`.so`动态库文件（如果gcc编译选项中加入了`-static`表示寻找`.a`静态库文件）\n4. `.PHONY`后面的`target`表示的也是一个伪造的`target`, 而不是真实存在的文件`target`，注意`Makefile`的`target`默认是文件。\n\n## 关于三个函数的使用\n```\nDIR_INC = ./include\nDIR_SRC = ./src\nDIR_OBJ = ./obj\n\nSRC = $(wildcard ${DIR_SRC}/*.c)\t\t# 指定编译当前目录下所有`.c`文件，全路径`./src/*.c`\nDIR = $(notdir ${SRC})\t\t\t\t# 去除路径名，只留下文件名`*.c`\nOBJ = $(patsubst %.c, ${DIR_OBJ}/%.o, ${DIR})\t# 将`DIR`中匹配到的`%.c`，替换为`${DIR_OBJ}/%.o`\n\nALL:\n\t@echo $(SRC)\n\t@echo $(DIR)\n\t@echo $(OBJ)\n```\n\n执行\n```\n$ make\n./src/hello.c ./src/test.c\nhello.c test.c\n./obj/hello.o ./obj/test.o\n```\n\n> 注：若`./src`目录下还有子目录`./src/inc`，则\n> ```\n> SRC = $(wildcard ${DIR_SRC}/*.c) $(wildcard ${DIR_SRC}/inc/*.c)\n> ```\n\n\n# Reference\n> Makefile 使用总结 - wang_yb - 博客园 https://www.cnblogs.com/wang_yb/p/3990952.html\n> ","categories":["Linux"]},{"title":"Cmake编译库文件","url":"/2019/01/05/Cmake编译库文件/","content":"\n# 前言\n库文件即源代码的二进制文件，我们通常把一些公用函数制作成函数库，供其它程序使用。函数库分为静态库和动态库两种。静态库在程序编译时会被连接到目标代码中，程序运行时将不再需要该静态库；动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此在程序运行时还需要动态库存在。\n\n# 编译\n以DarkNet为例，我们将其源代码编译成`.a`静态库文件。\n1. 下载源码\n    > YOLO: Real-Time Object Detection https://pjreddie.com/darknet/yolo/\n\n    ```\n    $ git clone https://github.com/pjreddie/darknet\n    ```\n\n2. 整理文件\n    我们将`include/`与`src/`目录复制到新建文件夹`darknet/`。\n\n3. 在`darknet/`目录下编写`CmakeLists.txt`文件，内容如下\n    ```\n    CMAKE_MINIMUM_REQUIRED(VERSION 2.8)\t                                # cmake需要的最小版本号\n    PROJECT(darknet)\t\t\t                                        # 项目名\n\n    MESSAGE(STATUS \n        \"----------------------------------------------------------------------\"\n    )\n    MESSAGE(STATUS \n        \"project name:      \" ${PROJECT_NAME}           # cmake默认参数\n    )\n    MESSAGE(STATUS \n        \"source directory:  \" ${PROJECT_SOURCE_DIR}     # cmake默认参数\n    )\n    MESSAGE(STATUS \n        \"binary directory:  \" ${PROJECT_BINARY_DIR}     # cmake默认参数\n    )\n    MESSAGE(STATUS \n        \"----------------------------------------------------------------------\"\n    )\n\n\n    # ----------------------------------------------------------------------------------\n    INCLUDE_DIRECTORIES(                                # 头文件目录\n        ${PROJECT_SOURCE_DIR}/include\n        ${PROJECT_SOURCE_DIR}/src\n    )                                                   \n    AUX_SOURCE_DIRECTORY(                               # 源文件\n        ${PROJECT_SOURCE_DIR}/src \n        lib_srcfile\n    )                                                   \n    SET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)  # 设置保存`.a`的目录\n\n    # ----------------------------------------------------------------------------------\n    ADD_LIBRARY(                                        # 生成库文件，可选择`.a`或`.so`\n        ${PROJECT_NAME}\n        STATIC                                          # `.a`\n        # SHARED                                        # `.so`\n        ${lib_srcfile}\n    ) \n\n    # ----------------------------------------------------------------------------------\n    SET_TARGET_PROPERTIES(\n        ${PROJECT_NAME}\n        PROPERTIES\n        LINKER_LANGUAGE C\n    )\n    ```\n\n4. 执行命令\n    我们在`darknet/`目录打开终端\n    ```\n    $ mkdir build\n    $ cd build\n    $ cmake ..\n    -- The C compiler identification is GNU 5.4.0\n    -- The CXX compiler identification is GNU 5.4.0\n    -- Check for working C compiler: /usr/bin/cc\n    -- Check for working C compiler: /usr/bin/cc -- works\n    -- Detecting C compiler ABI info\n    -- Detecting C compiler ABI info - done\n    -- Detecting C compile features\n    -- Detecting C compile features - done\n    -- Check for working CXX compiler: /usr/bin/c++\n    -- Check for working CXX compiler: /usr/bin/c++ -- works\n    -- Detecting CXX compiler ABI info\n    -- Detecting CXX compiler ABI info - done\n    -- Detecting CXX compile features\n    -- Detecting CXX compile features - done\n    -- Configuring done\n    -- Generating done\n    -- Build files have been written to: /home/louishsu/Work/Codes/makefile/test/build\n    louishsu@ThinkPad-T440s:~/Work/Codes/makefile/test/build$ make\n    Scanning dependencies of target Test\n    [ 50%] Building C object CMakeFiles/Test.dir/src/test.c.o\n    [100%] Linking C executable Test\n    [100%] Built target Test\n    louishsu@ThinkPad-T440s:~/Work/Codes/makefile/test/build$ cd ../../darknet/build/\n    louishsu@ThinkPad-T440s:~/Work/Codes/makefile/darknet/build$ rm -rf *\n    louishsu@ThinkPad-T440s:~/Work/Codes/makefile/darknet/build$ cmake ..\n    -- The C compiler identification is GNU 5.4.0\n    -- The CXX compiler identification is GNU 5.4.0\n    -- Check for working C compiler: /usr/bin/cc\n    -- Check for working C compiler: /usr/bin/cc -- works\n    -- Detecting C compiler ABI info\n    -- Detecting C compiler ABI info - done\n    -- Detecting C compile features\n    -- Detecting C compile features - done\n    -- Check for working CXX compiler: /usr/bin/c++\n    -- Check for working CXX compiler: /usr/bin/c++ -- works\n    -- Detecting CXX compiler ABI info\n    -- Detecting CXX compiler ABI info - done\n    -- Detecting CXX compile features\n    -- Detecting CXX compile features - done\n    -- ----------------------------------------------------------------------\n    -- project name:      darknet\n    -- source directory:  /home/louishsu/Work/Codes/makefile/darknet\n    -- binary directory:  /home/louishsu/Work/Codes/makefile/darknet/build\n    -- ----------------------------------------------------------------------\n    -- Configuring done\n    -- Generating done\n    -- Build files have been written to: /home/louishsu/Work/Codes/makefile/darknet/build\n    $ make\n    Scanning dependencies of target darknet\n    [  2%] Building C object CMakeFiles/darknet.dir/src/gemm.c.o\n    [  4%] Building C object CMakeFiles/darknet.dir/src/reorg_layer.c.o\n    [  6%] Building C object CMakeFiles/darknet.dir/src/logistic_layer.c.o\n    [  8%] Building C object CMakeFiles/darknet.dir/src/activation_layer.c.o\n    [ 10%] Building C object CMakeFiles/darknet.dir/src/network.c.o\n    [ 12%] Building C object CMakeFiles/darknet.dir/src/region_layer.c.o\n    [ 14%] Building C object CMakeFiles/darknet.dir/src/compare.c.o\n    [ 16%] Building C object CMakeFiles/darknet.dir/src/yolo_layer.c.o\n    [ 18%] Building C object CMakeFiles/darknet.dir/src/data.c.o\n    [ 20%] Building C object CMakeFiles/darknet.dir/src/route_layer.c.o\n    [ 22%] Building C object CMakeFiles/darknet.dir/src/detection_layer.c.o\n    [ 25%] Building C object CMakeFiles/darknet.dir/src/list.c.o\n    [ 27%] Building C object CMakeFiles/darknet.dir/src/option_list.c.o\n    [ 29%] Building C object CMakeFiles/darknet.dir/src/activations.c.o\n    [ 31%] Building C object CMakeFiles/darknet.dir/src/maxpool_layer.c.o\n    [ 33%] Building C object CMakeFiles/darknet.dir/src/dropout_layer.c.o\n    [ 35%] Building C object CMakeFiles/darknet.dir/src/cost_layer.c.o\n    [ 37%] Building C object CMakeFiles/darknet.dir/src/crop_layer.c.o\n    [ 39%] Building C object CMakeFiles/darknet.dir/src/convolutional_layer.c.o\n    [ 41%] Building C object CMakeFiles/darknet.dir/src/softmax_layer.c.o\n    [ 43%] Building C object CMakeFiles/darknet.dir/src/parser.c.o\n    [ 45%] Building C object CMakeFiles/darknet.dir/src/utils.c.o\n    [ 47%] Building C object CMakeFiles/darknet.dir/src/box.c.o\n    [ 50%] Building C object CMakeFiles/darknet.dir/src/batchnorm_layer.c.o\n    [ 52%] Building C object CMakeFiles/darknet.dir/src/iseg_layer.c.o\n    [ 54%] Building C object CMakeFiles/darknet.dir/src/lstm_layer.c.o\n    [ 56%] Building C object CMakeFiles/darknet.dir/src/connected_layer.c.o\n    [ 58%] Building C object CMakeFiles/darknet.dir/src/deconvolutional_layer.c.o\n    [ 60%] Building C object CMakeFiles/darknet.dir/src/image.c.o\n    [ 62%] Building C object CMakeFiles/darknet.dir/src/im2col.c.o\n    [ 64%] Building C object CMakeFiles/darknet.dir/src/col2im.c.o\n    [ 66%] Building C object CMakeFiles/darknet.dir/src/l2norm_layer.c.o\n    [ 68%] Building C object CMakeFiles/darknet.dir/src/demo.c.o\n    [ 70%] Building C object CMakeFiles/darknet.dir/src/cuda.c.o\n    [ 72%] Building C object CMakeFiles/darknet.dir/src/matrix.c.o\n    [ 75%] Building C object CMakeFiles/darknet.dir/src/upsample_layer.c.o\n    [ 77%] Building C object CMakeFiles/darknet.dir/src/rnn_layer.c.o\n    [ 79%] Building C object CMakeFiles/darknet.dir/src/tree.c.o\n    [ 81%] Building C object CMakeFiles/darknet.dir/src/gru_layer.c.o\n    [ 83%] Building C object CMakeFiles/darknet.dir/src/crnn_layer.c.o\n    [ 85%] Building C object CMakeFiles/darknet.dir/src/normalization_layer.c.o\n    [ 87%] Building C object CMakeFiles/darknet.dir/src/layer.c.o\n    [ 89%] Building C object CMakeFiles/darknet.dir/src/shortcut_layer.c.o\n    [ 91%] Building C object CMakeFiles/darknet.dir/src/avgpool_layer.c.o\n    [ 93%] Building C object CMakeFiles/darknet.dir/src/local_layer.c.o\n    [ 95%] Building CXX object CMakeFiles/darknet.dir/src/image_opencv.cpp.o\n    [ 97%] Building C object CMakeFiles/darknet.dir/src/blas.c.o\n    [100%] Linking C static library lib/libdarknet.a\n    [100%] Built target darknet\n    ```\n\n在`darknet/build/lib`目录下即可找到`libdarknet.a`库文件，当前目录结构如下\n```\n\\-- darknet\n    \\-- include\n        |-- darknet.h\n    \\-- src\n        |-- *.h\n        |-- *.c\n    \\--build\n        \\-- lib\n            |-- libdarknet.a(.so)\n    |-- CMakeLists.txt\n```\n\n# 调用库函数\n为测试该库函数是否编译成功，编写测试代码，新建目录`/test/`，其文件结构为\n```\n\\-- test\n    \\-- include\n        |-- test.h\n    \\-- src\n        |-- test.c\n    \\-- build\n        |-- test\n    |-- CMakeLists.txt\n```\n\n头文件`/include/test.h`内容为\n```\n#ifndef TEST_H\n#define TEST_H\n\n#include \"darknet.h\"\n\n#endif\n```\n\n源文件`/src/test.c`内容如下\n```\n#include \"test.h\"\n\nint main()\n{\n\tprintf(\"Hello! Darknet!\\n\");\n\tmatrix M = make_matrix(4, 5);\n\tprintf(\"The size of matrix M is %ld bytes\\n\", sizeof(M));\n\treturn 0;\n}\n```\n\n编译文件`CMakeLists.txt`内容如下\n```\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)\t                        # cmake需要的最小版本号\nPROJECT(Test)\t\t\t                               \t# 项目名\n\n# ----------------------------------------------------------------------------------\nSET(DARKNET ../darknet)\nINCLUDE_DIRECTORIES(                                            # 头文件目录\n\t${DARKNET}/include\n\t${DARKNET}/src\n)          \t\t\t\t\t\t\t\nLINK_DIRECTORIES(                                               # 库文件目录\n    ${DARKNET}/build/lib\n)                  \t\n\n# ----------------------------------------------------------------------------------\nINCLUDE_DIRECTORIES(./include)                                \t# 当前项目头文件目录\nAUX_SOURCE_DIRECTORY(./src SRC_FILES)                          \t# 当前项目源文件目录\n\n# ----------------------------------------------------------------------------------\nADD_EXECUTABLE(${PROJECT_NAME} ${SRC_FILES})                 \t# 添加可执行文件\nTARGET_LINK_LIBRARIES(\t\t\t\t\t\t# 引用库\n\t${PROJECT_NAME}\n\tdarknet\t\t\t\t\t\t\t# darknet\n\tm\t\t\t\t\t\t\t# 数学函数库\n)              \t\t\t\t\t\t\t\n```\n\n执行指令\n```\n$ mkdir build\n$ cd build\n$ cmake ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/louishsu/Work/Codes/makefile/test/build\n$ make\nScanning dependencies of target Test\n[ 50%] Building C object CMakeFiles/Test.dir/src/test.c.o\n[100%] Linking C executable Test\n[100%] Built target Test\n```\n\n运行可执行文件\n```\n$ ./Test\nHello! Darknet!\nThe size of matrix M is 16 bytes\n```\n\n查看结构体`matrix`定义\n```\ntypedef struct matrix{\n    int rows, cols;\n    float **vals;\n} matrix;\n```\n`int`占`32bit`，`float* `占`64bit`，故\n```\n(32bit * 2 + 64bit) / 8 = 16byte\n```\n运行成功。\n\n# Reference\n> 静态库和动态库的优缺点 - 默默淡然 - 博客园 https://www.cnblogs.com/liangxiaofeng/p/3228145.html","categories":["Linux"]},{"title":"Ubuntu编译安装Tensorflow","url":"/2019/01/04/Ubuntu编译安装Tensorflow/","content":"# 非常重要\n如果中途出现错误，`xxxx`文件找不到，不要怀疑！就是大天朝的网络问题！推荐科学上网！\n\n# 安装CUDA与CUDNN\n首先查看显卡是否支持`CUDA`加速，输入\n```\n$ nvidia-smi\n```\n\n![nvidia-smi](/Ubuntu编译安装Tensorflow/nvidia-smi.png)\n\n\n在`Ubuntu16.04 LTS`下，推荐安装`CUDA9.0`和`CUDNN 7`。\n\n![cuda_cudnn1](/Ubuntu编译安装Tensorflow/cuda_cudnn1.png)\n\n- CUDA\n    > CUDA Toolkit 9.0 Downloads | NVIDIA Developer https://developer.nvidia.com/cuda-90-download-archive\n\n    ![cuda](/Ubuntu编译安装Tensorflow/cuda.png)\n    \n    下载`.run`版本，安装方法如下\n    ```\n    $ sudo chmod +x cuda_9.0.176_384.81_linux.run \n    $ sudo sh ./cuda_9.0.176_384.81_linux.run\n    ```\n\n    服务条款很长。。。。\n\n\n- CUDNN\n    > NVIDIA cuDNN | NVIDIA Developer https://developer.nvidia.com/cudnn\n\n    ![cudnn1](/Ubuntu编译安装Tensorflow/cudnn1.png)\n\n    ![cudnn2](/Ubuntu编译安装Tensorflow/cudnn2.png)\n\n    ```\n    $ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz\n    $ sudo cp cuda/include/cudnn.h /usr/local/cuda/include\n    $ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\n    $ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n    ```\n\n    安装后进行验证\n    ```\n    $ cp -r /usr/src/cudnn_samples_v7/ $HOME\n    $ cd  $HOME/cudnn_samples_v7/mnistCUDNN\n    $ make clean && make\n    $ ./mnistCUDNN\n    ```\n\n    ![cuda_cudnn_verifying](/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying.png)\n\n    ![cuda_cudnn_verifying2](/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying2.png)\n\n# 编译Tensorflow(CPU version)\n由于训练代码使用`Python`实现，故`C++`版本的`Tensorflow`不使用`GPU`，仅实现预测代码即可。\n\n## bazel\n> Installing Bazel on Ubuntu - Bazel https://docs.bazel.build/versions/master/install-ubuntu.html\n> 一定要用源码安装！！！\n\ndownload the Bazel binary installer named `bazel-<version>-installer-linux-x86_64.sh` from the [Bazel releases page on GitHub](https://github.com/bazelbuild/bazel/releases).\n    \n```\n$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python\n$ chmod +x bazel-<version>-installer-linux-x86_64.sh\n$ ./bazel-<version>-installer-linux-x86_64.sh --user\n$ sudo nano ~/.bashrc # export PATH=\"$PATH:$HOME/bin\"\n$ source ~/.bashrc \n$ bazel version\n```\n\n![bazel](/Ubuntu编译安装Tensorflow/bazel.png)\n\n## 编译CPU版本的CPU\n查看`java`版本\n``` \n$ java -version\nopenjdk version \"1.8.0_191\"\nOpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)\nOpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)\n```\n\n## 安装依赖软件包环境\n```\n$ sudo apt install python3-dev\n$ pip3 install six\n$ pip3 install numpy\n$ pip3 instal wheel\n```\n\n## 下载`Tensorflow`源码\n```\n$ git clone https://github.com/tensorflow/tensorflow\n```\n\n## 编译与安装\n```\n$ cd tensorflow\n$ ./configure \n```\n\n配置选项如下\n```\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\nINFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5\nYou have bazel 0.20.0 installed.\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\n\n\nFound possible Python library paths:\n  /usr/local/lib/python3.5/dist-packages\n  /usr/lib/python3/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\n\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\nNo XLA JIT support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\nNo OpenCL SYCL support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\nNo ROCm support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\nNo CUDA support will be enabled for TensorFlow.\n\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\nClang will not be downloaded.\n\nDo you wish to build TensorFlow with MPI support? [y/N]: n\nNo MPI support will be enabled for TensorFlow.\n\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \n\n\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\nNot configuring the WORKSPACE for Android builds.\n\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\n\t--config=mkl         \t# Build with MKL support.\n\t--config=monolithic  \t# Config for mostly static monolithic build.\n\t--config=gdr         \t# Build with GDR support.\n\t--config=verbs       \t# Build with libverbs support.\n\t--config=ngraph      \t# Build with Intel nGraph support.\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\nPreconfigured Bazel build configs to DISABLE default on features:\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\n\t--config=nogcp       \t# Disable GCP support.\n\t--config=nohdfs      \t# Disable HDFS support.\n\t--config=noignite    \t# Disable Apacha Ignite support.\n\t--config=nokafka     \t# Disable Apache Kafka support.\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\nConfiguration finished\n```\n\n使用`bazel`编译\n```\n$ bazel build --config=opt //tensorflow:libtensorflow_cc.so\n```\n\n出现错误\n> TF failing to build on Bazel CI · Issue #19464 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/19464\n> Failure to build TF 1.12 from source - multiple definitions in grpc · Issue #23402 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197\n> Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583\n> Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5\n\n解决方法\n- 方法1\n    ![tools_bazel.rc](/Ubuntu编译安装Tensorflow/tools_bazel.rc.png)\n- 方法2\n    将`tools/bazel.rc`中内容粘到`.tf_configure.bazelrc`中，每次重新配置后需要重新粘贴一次。\n\n- 源码安装`protobuf3.6.0`\n    > https://github.com/protocolbuffers/protobuf\n    ```\n    ./autogen.sh\n    ./configure\n    make\n    make install\n    ```\n\n\n\n\n- 下载其他文件\n    ```\n    $ ./tensorflow/contrib/makefile/download_dependencies.sh\n    mkdir /tmp/eigen\n    \n    ```\n\n    - 值得注意，`download_dependencies.sh`中下载依赖包时，需要用到`curl`，但是默认方式安装\n        ```\n        $ sudo apt install curl\n        ```\n            \n        > 现在是2018/12/19/02:48，被这个问题折腾了3个小时。\n\n\n    时不支持`https`协议，故需要安装`OpenSSL`，并源码安装，详细资料见[curl提示不支持https协议解决方法 - 标配的小号 - 博客园](https://www.cnblogs.com/biaopei/p/8669810.html)\n\n    - 执行`./autogen.sh`时，发生错误`autoreconf: not found`，则安装\n        ```\n        $ sudo apt install autoconf aotomake libtool\n        $ sudo apt install libffi-dev\n        ```\n- 源码安装`Eigen`\n    ```\n    cd tensorflow/contrib/makefile/Downloads/eigen\n    mkdir build\n    cd build\n    cmake\n    make install\n    ```\n\n\n# 调用C++版本的Tensorflow\n创建文件目录如下\n```\n|-- tf_test\n    |-- build\n    |-- main.cpp\n    |-- CMakeLists.txt\n```\n\n`main.cpp`文件内容如下\n```\n#include \"tensorflow/cc/client/client_session.h\"\n#include \"tensorflow/cc/ops/standard_ops.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n\nint main() \n{\n    using namespace tensorflow;\n    using namespace tensorflow::ops;\n    Scope root = Scope::NewRootScope();\n    // Matrix A = [3 2; -1 0]\n    auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f}});\n    // Vector b = [3 5]\n    auto b = Const(root, { {3.f, 5.f}});\n    // v = Ab^T\n    auto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\n    std::vector<Tensor> outputs;\n    ClientSession session(root);\n    // Run and fetch v\n    TF_CHECK_OK(session.Run({v}, &outputs));\n    // Expect outputs[0] == [19; -3]\n    LOG(INFO) << outputs[0].matrix<float>();\n    return 0;\n}\n```\n\n`CMakeLists.txt`内容如下\n```\ncmake_minimum_required (VERSION 2.8.8)\nproject (tf_example)\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -g -std=c++11 -W\")\n\nset(EIGEN_DIR \t\t/usr/local/include/eigen3)\nset(PROTOBUF_DIR\t/usr/local/include/google/protobuf)\nset(TENSORFLOW_DIR \t/home/louishsu/install/tensorflow-1.12.0)\n\ninclude_directories(\n\t${EIGEN_DIR}\n\t${PROTOBUF_DIR}\n   \t${TENSORFLOW_DIR}\n\t${TENSORFLOW_DIR}/bazel-genfiles\n\t${TENSORFLOW_DIR}/tensorflow/contrib/makefile/downloads/absl\n)\nlink_directories(\n\t/usr/local/lib\n\n)\n\nadd_executable(\n\ttf_test\n\tmain.cpp\n)\n\ntarget_link_libraries(\n\ttf_test\n\ttensorflow_cc\n\ttensorflow_framework\n)\n\n```\n\n```\n$ mkdir build && cd build\n$ cmake .. && make\n$ ./tf_test\n```\n\n# install tensorflow-gpu for python\n可使用`pip`指令安装，推荐下载安装包，\n> tensorflow · PyPI https://pypi.org/project/tensorflow/\n\n![tensorflow_for_python](/Ubuntu编译安装Tensorflow/tensorflow_for_python.png)\n\n```\n$ cd ~/Downloads\n$ pip3 --default-timeout=1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user\n```\n\n安装后进行验证\n```\n$ python3\nPython 3.5.2 (default, Nov 12 2018, 13:43:14) \n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n>>> sess = tf.Session()\n2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \nname: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758\npciBusID: 0000:04:00.0\ntotalMemory: 983.44MiB freeMemory: 177.19MiB\n2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n2018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -> physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)\n>>> a = tf.Variable([233])\n>>> init = tf.initialize_all_variables()\nWARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\nInstructions for updating:\nUse `tf.global_variables_initializer` instead.\n>>> sess.run(init)\n>>> sess.run(a)\narray([233], dtype=int32)\n\n>>> sess.close()\n```\n\n注意，如果异常中断程序，显存不会被释放，需要自行`kill`\n```\n$ nvidia-smi\n```\n\n获得`PID`序号，使用指令结束进程\n```\n$ kill -9 pid\n```\n\n# Reference\n> TensorFlow C++动态库编译 - 简书 https://www.jianshu.com/p/d46596558640\n> Tensorflow C++ 从训练到部署(1)：环境搭建 | 技术刘 http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","tags":["Linux"],"categories":["Linux","Deep Learning"]},{"title":"Ubuntu编译安装OpenCV","url":"/2019/01/04/Ubuntu编译安装OpenCV/","content":"\n# 下载源码\n> OpenCV library https://opencv.org/\n\n# 编译安装\n## 依赖软件包\n```\n$ sudo apt install cmake\n$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev\n```\n\n## 编译\n```\n$ unzip opencv-3.4.4.zip\n$ cd opencv-3.4.4\n$ mkdir build && cd build\n$ cmake ..\n$ make -j4\n```\n\n## 安装\n```\n$ sudo make install\n$ sudo nano /etc/ld.so.conf.d/opencv.conf # add `/usr/local/lib`\n$ sudo ldconfig \n```\n\n## 验证\n`OpenCV`自带验证程序，在`opencv-3.4.4/samples/cpp/example_cmake`中可以找到\n\n```\n$ cd opencv-3.4.4/samples/cpp/example_cmake\n$ cmake .\n$ make\n$ ./opencv_example\n```\n\n如果没问题，可以看到你的大脸了~\n\n\n# Reference\n> Ubuntu16.04安装openCV3.4.4 - 辣屁小心的学习笔记 - CSDN博客 https://blog.csdn.net/weixin_39992397/article/details/84345197","tags":["Linux"],"categories":["Linux"]},{"title":"Python读写配置文件","url":"/2019/01/04/Python读写配置文件/","content":"\n在深度学习中，有许多运行参数需要指定，有几种方法可以解决\n- 定义`.py`文件存储变量\n- 定义命名元组`collections.namedtuple()`\n- 创建`.config`，`.ini`等配置文件\n\nPython 读取写入配置文件很方便，使用内置模块`configparser`即可\n\n# 读出\n首先创建文件`test.config`或`test.ini`，写入如下内容\n```\n[db]\ndb_port = 3306\ndb_user = root\ndb_host = 127.0.0.1\ndb_pass = test\n\n[concurrent]\nprocessor = 20\nthread = 10\n```\n\n读取操作如下\n```\n>>> import os\n>>> import configparser\n>>> \n>>> configfile = \"./test.config\"\n>>> inifile = \"./test.ini\"\n>>> \n>>> cf = configparser.ConfigParser()\n>>> cf.read(configfile)                     # 读取文件内容\n>>> \n>>> sections = cf.sections()                # 所有的section，以列表的形式返回\n>>> sections\n['db', 'concurrent']\n>>> \n>>> options = cf.options('db')              # 该section的所有option\n>>> options\n['db_port', 'db_user', 'db_host', 'db_pass']\n>>> \n>>> items = cf.items('db')                  # 该section的所有键值对\n>>> items\n[('db_port', '3306'), ('db_user', 'root'), ('db_host', '127.0.0.1'), ('db_pass', 'test')]\n>>> \n>>> db_user = cf.get('db', 'db_user')       # section中option的值，返回为string类型\n>>> db_user\n'root'\n>>> \n>>> db_port = cf.getint('db', 'db_port')    # 得到section中option的值，返回为int类型\n>>>                                         # 类似的还有getboolean()与getfloat()\n>>> db_port\n3306\n```\n\n# 写入\n```\n>>> import os\n>>> import configparser\n>>> \n>>> cf = configparser.ConfigParser()\n>>> cf.add_section('test1')                 # 新增section\n>>> \n>>> cf.set(\"test\", \"count\", 1)              # 新增option：错误示范\nTraceback (most recent call last):\n  File \"<pyshell#7>\", line 1, in <module>\n    cf.set(\"test\", \"count\", 1)\n  File \"C:\\MyApplications\\Python3\\lib\\configparser.py\", line 1192, in set\n    self._validate_value_types(option=option, value=value)\n  File \"C:\\MyApplications\\Python3\\lib\\configparser.py\", line 1177, in _validate_value_types\n    raise TypeError(\"option values must be strings\")\nTypeError: option values must be strings\n>>> \n>>> cf.set(\"test\", \"count\", '1')            # 新增option\n>>> \n>>> cf.set(\"test1\", \"opt1\", 'ok')           # 新增option\n>>> cf.remove_option(\"test1\", \"opt1\")       # 删除option\nTrue\n>>> \n>>> cf.add_section('test2')                 # 新增section\n>>> cf.remove_section('test2')              # 删除section\nTrue\n>>> \n>>> with open(\"./test_wr.config\", 'w+') as f:\n        cf.write(f)                         # 写入文件test_wr.config\n\n\t\n>>> \n```\n\n现在目录已创建文件`test_wr.config`，打开可以看到\n```\n[test1]\ncount = 1\n```","categories":["Python"]},{"title":"Python更新安装的包","url":"/2019/01/04/Python更新安装的包/","content":"\n`pip`不提供升级全部已安装模块的方法，以下指令可查看更新信息\n```\n$ pip list --outdate\n```\n\n得到输出信息如下\n```\nPackage           Version   Latest     Type\n----------------- --------- ---------- -----\nabsl-py           0.3.0     0.6.1      sdist\nautopep8          1.3.5     1.4.2      sdist\nbleach            2.1.4     3.0.2      wheel\ncertifi           2018.8.24 2018.10.15 wheel\ndask              0.20.0    0.20.1     wheel\ngrpcio            1.14.1    1.16.0     wheel\nipykernel         5.0.0     5.1.0      wheel\nipython           7.0.1     7.1.1      wheel\njedi              0.12.1    0.13.1     wheel\njupyter-console   5.2.0     6.0.0      wheel\nMarkdown          2.6.11    3.0.1      wheel\nMarkupSafe        1.0       1.1.0      wheel\nmatplotlib        2.2.2     3.0.2      wheel\nmistune           0.8.3     0.8.4      wheel\nnumpy             1.14.5    1.15.4     wheel\nopencv-python     3.4.2.17  3.4.3.18   wheel\nPillow            5.2.0     5.3.0      wheel\nprometheus-client 0.3.1     0.4.2      sdist\npyparsing         2.2.0     2.3.0      wheel\npython-dateutil   2.7.3     2.7.5      wheel\npytz              2018.5    2018.7     wheel\nurllib3           1.23      1.24.1     wheel\n```\n\n以下提供一键升级的方法，可能比较久hhhh\n```\nfrom pip._internal.utils.misc import get_installed_distributions\nfrom subprocess import call\n \nfor dist in get_installed_distributions():\n    modulename = dist.project_name\n    print('start processing module ' + modulename)\n    call(\"pip install --upgrade \" + modulename, shell=True)\n    print('module ' + modulename + 'done!')\n```","categories":["Python"]},{"title":"Python记录日志","url":"/2019/01/04/Python记录日志/","content":"\n# 前言\n日志可以用来记录应用程序的状态、错误和信息消息，也经常作为调试程序的工具。\n`Python`提供了一个标准的日志接口，就是`logging`模块。日志级别有`DEBUG`、`INFO`、`WARNING`、`ERROR`、`CRITICAL`五种。\n\n[logging — Logging facility for Python — Python 3.7.1 documentation](https://docs.python.org/3/library/logging.html)\n\n# 使用方法\n## `logger`对象\n```\n>>> import logging\n>>> logger = logging.getLogger(__name__)\n>>> logger\n<Logger __main__ (WARNING)>\n```\n\n## 日志级别\n可输出五种不同的日志级别，分别为有`DEBUG`、`INFO`、`WARNING`、`ERROR`、`CRITICAL`\n```\n>>> logger.debug('test log')\n>>> logger.info('test log')\n>>> logger.warning('test log')\ntest log\n>>> logger.error('test log')\ntest log\n>>> logger.critical('test log')\ntest log\n```\n\n可以看到只有`WARNING`及以上级别日志被输出，这是由于默认的日志级别是`WARNING` ，所以低于此级别的日志不会记录。\n\n## 基础配置\n```\nlogging.basicConfig(**kwarg)\n```\n\n`**kwarg`中部分参数如下\n- `format`\n    ```\n    %(levelname)：日志级别的名字格式\n    %(levelno)s：日志级别的数字表示\n    %(name)s：日志名字\n    %(funcName)s：函数名字\n    %(asctime)：日志时间，可以使用datefmt去定义时间格式，如上图。\n    %(pathname)：脚本的绝对路径\n    %(filename)：脚本的名字\n    %(module)：模块的名字\n    %(thread)：thread id\n    %(threadName)：线程的名字\n    ```\n- `datefmt`\n    ```\n    '%Y-%m-%d %H:%M:%S'\n    ```\n- `level`\n    默认为`ERROR `\n    ```\n    logging.DEBUG\n    logging.INFO\n    logging.WARNING\n    logging.ERROR\n    logging.CRITICAL\n    ```\n\n例如\n```\n>>> # 未输出debug\n>>> logger = logging.getLogger()\n>>> logger.debug('test log')\n>>> \n>>> # 修改配置\n>>> log_format = '%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'\n>>> log_datefmt = '%Y-%m-%d %H:%M:%S'\n>>> log_level = logging.DEBUG\n>>> logging.basicConfig(format=log_format, \n                        datefmt=log_datefmt, \n                        level=log_level)\n>>> \n>>> # 输出debug\n>>> logger = logging.getLogger()\n>>> logger.debug('test log')\n<pyshell#8> [2018-11-13 11:59:52] [DEBUG] test log\n```\n\n## 输出到日志文件\n保存代码为文件`log_test.py`\n```\nimport logging\n\nlog_format = '%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'\nlog_datefmt = '%Y-%m-%d %H:%M:%S'\nlog_level = logging.DEBUG\nlog_filename = './test.log'\nlog_filemode = 'a'  # 也可以为'w', 'w+'等\n\nlogging.basicConfig(format=log_format,\n                    datefmt=log_datefmt, \n                    level=log_level,\n                    filename=log_filename, \n                    filemode=log_filemode)\n\nlogger = logging.getLogger(__name__)\nlogger.debug('test log')\nlogger.info('test log')\nlogger.warning('test log')\nlogger.error('test log')\nlogger.critical('test log')\n\n```\n\n运行完毕，打开`log_test.log`文件可以看到\n```\nlog_test.py [2018-11-13 12:11:04] [DEBUG] test log\nlog_test.py [2018-11-13 12:11:04] [INFO] test log\nlog_test.py [2018-11-13 12:11:04] [WARNING] test log\nlog_test.py [2018-11-13 12:11:04] [ERROR] test log\nlog_test.py [2018-11-13 12:11:04] [CRITICAL] test log\n```\n","categories":["Python"]},{"title":"Hexo+Github博客搭建","url":"/2019/01/04/Github-Hexo博客搭建/","content":"\n# 前言\n那么问题来了，现有的博客还是现有的这篇文章呢？\n\n# 软件安装\n安装[node.js](https://nodejs.org/en/), [git](https://git-scm.com/), [hexo](https://hexo.io/zh-cn/)\n\n# 博客搭建\n## 初始化\n推荐使用`git`命令窗口，执行如下指令\n```\n$ mkdir Blog\n$ cd Blog\n$ hexo init\nINFO  Cloning hexo-starter to ~\\Desktop\\Blog\nCloning into 'C:\\Users\\LouisHsu\\Desktop\\Blog'...\nremote: Enumerating objects: 68, done.\nremote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68\nUnpacking objects: 100% (68/68), done.\nSubmodule 'themes/landscape' (https://github.com/hexojs/hexo-theme-landscape.git) registered for path 'themes/landscape'\nCloning into 'C:/Users/LouisHsu/Desktop/Blog/themes/landscape'...\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866\nReceiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.\nResolving deltas: 100% (459/459), done.\nSubmodule path 'themes/landscape': checked out '73a23c51f8487cfcd7c6deec96ccc7543960d350'\n\u001b[32mINFO \u001b[39m Install dependencies\nnpm WARN deprecated titlecase@1.1.2: no longer maintained\nnpm WARN deprecated postinstall-build@5.0.3: postinstall-build's behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.\n\n> nunjucks@3.1.6 postinstall C:\\Users\\LouisHsu\\Desktop\\Blog\\node_modules\\nunjucks\n> node postinstall-build.js src\n\nnpm notice created a lockfile as package-lock.json. You should commit this file.\nnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\\fsevents):\nnpm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"win32\",\"arch\":\"x64\"})\n\nadded 422 packages from 501 contributors and audited 4700 packages in 59.195s\nfound 0 vulnerabilities\n\nINFO  Start blogging with Hexo!\n```\n\n生成目录结构如下\n```\n\\-- scaffolds\n\\-- source\n    \\-- _posts\n\\-- themes\n|-- _config.yml\n|-- package.json\n```\n\n继续\n```\n$ npm install\nnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\\fsevents):\nnpm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"win32\",\"arch\":\"x64\"})\n\naudited 4700 packages in 5.99s\nfound 0 vulnerabilities\n```\n\n现在该目录执行指令，开启`hexo`服务器\n```\n$ hexo s\nINFO  Start processing\nINFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.\n```\n![hexo_server](/Github-Hexo博客搭建/hexo_server.png)\n\n## 生成目录和标签\n```\n$ hexo n page about\n$ hexo n page archives\n$ hexo n page categories\n$ hexo n page tags\n```\n修改`/source/tags/index.md`，其他同理\n```\n01| ---\n02| title: tags\n03| date: 2019-01-04 17:34:15\n04| ---\n\n->\n\n01| ---\n02| title: tags\n03| date: 2019-01-04 17:34:15\n04| type: \"tags\"\n05| comments: false\n06| ---\n```\n\n## 关联`Github`\n在`Github`新建一个仓库，命名为`username.github.io`，例如`isLouisHsu.github.io`，新建时勾选`Initialize this repository with a README`，因为这个仓库必须不能为空。\n![github_io](/Github-Hexo博客搭建/github_io.png)\n\n打开博客目录下的`_config.yml`配置文件，定位到最后的`deploy`选项，修改如下\n```\ndeploy:\n    type: git\n    repository: git@github.com:isLouisHsu/isLouisHsu.github.io.git\n    branch: master\n```\n\n安装插件\n```\n$ npm install hexo-deployer-git --save\n```\n\n现在就可以将该目录内容推送到`Github`新建的仓库中了\n```\n$ hexo d\n```\n\n## 使用个人域名\n1. 在`source`目录下新建文件`CNAME`，输入解析后的个人域名\n2. 在`Github`主页修改域名\n\n# 备份博客\n> 没。没什么用\n> 我。我不备份了\n> 可以新建一个仓库专门保存文件试试\n\n现在博客的源文件仅保存在`PC`上， 我们对它们进行备份，并将仓库作为博客文件夹\n1. 在仓库新建分支`hexo`，设置为默认分支\n    ![create_branch_hexo](/Github-Hexo博客搭建/create_branch_hexo.png)\n    ![change_branch_hexo](/Github-Hexo博客搭建/change_branch_hexo.png)\n2. 将仓库克隆至本地\n    ```\n    $ git clone https://github.com/isLouisHsu/isLouisHsu.github.io.git\n    ```\n3. 克隆文件\n    将之前的Hexo文件夹中的\n    ```\n    scffolds/\n    source/\n    themes/\n    .gitignore\n    _config.yml\n    package.json\n    ```\n    复制到克隆下来的仓库文件夹`isLouisHsu.github.io`\n    ![backup_blog](/Github-Hexo博客搭建/backup_blog.png)\n4. 安装包\n    ```\n    $ npm install\n    $ npm install hexo --save\n    $ npm install hexo-deployer-git --save\n    ```\n\n    备份博客使用以下指令\n    ```\n    $ git add .\n    $ git commit -m \"backup\"\n    $ git push origin hexo\n    ```\n\n5. 部署博客指令\n    ```\n    $ hexo g -d\n    ```\n\n6. `单键`提交\n    编写脚本`commit.bat`，双击即可\n    ```\n    git add .\n    git commit -m 'backup'\n    git push origin hexo\n    hexo g -d\n    ```\n\n# 使用方法\n- 目录结构\n    - `public`  生成的网站文件，发布的站点文件。\n    - `source`  资源文件夹，用于存放内容。\n    - `tag`     标签文件夹。\n    - `archive` 归档文件夹。\n    - `category`分类文件夹。\n    - `downloads/code include code`文件夹。\n    - `:lang i18n_dir` 国际化文件夹。\n    - `_config.yml` 配置文件\n  \n- 指令\n    ```\n    $ hexo help\n    Usage: hexo <command>\n\n    Commands:\n        clean     Remove generated files and cache.\n        config    Get or set configurations.\n        deploy    Deploy your website.\n        generate  Generate static files.\n        help      Get help on a command.\n        init      Create a new Hexo folder.\n        list      List the information of the site\n        migrate   Migrate your site from other system to Hexo.\n        new       Create a new post.\n        publish   Moves a draft post from _drafts to _posts folder.\n        render    Render files with renderer plugins.\n        server    Start the server.\n        version   Display version information.\n\n    Global Options:\n        --config  Specify config file instead of using _config.yml\n        --cwd     Specify the CWD\n        --debug   Display all verbose messages in the terminal\n        --draft   Display draft posts\n        --safe    Disable all plugins and scripts\n        --silent  Hide output on console\n\n    For more help, you can use 'hexo help [command]' for the detailed information or you can check the docs: http://hexo.io/docs/\n    ```\n\n<!-- # 修改主题 -->\n    \n\n# 拓展功能支持\n## 插入图片\n```\n$ npm install hexo-asset-image --save\n```\n修改文件`_config.yml`\n```\npost_asset_folder: true\n```\n在执行`$ hexo n [layout] <title>`时会生成同名文件夹，把图片放在这个文件夹内，在`.md`文件中插入图片\n```\n![image_name](/title/image_name.png)\n```\n\n\n## 搜索功能\n```\n$ npm install hexo-generator-searchdb --save\n$ npm install hexo-generator-search --save\n```\n站点配置文件`_config.yml`中添加\n```\nsearch:\n  path: search.xml\n  field: post\n  format: html\n  limit: 10000\n```\n修改主题配置文件`/themes/xxx/_config.yml`\n```\nlocal_search:\n  enable: true\n```\n\n## 带过滤功能的首页插件\n在首页只显示指定分类下面的文章列表。\n```\n$ npm install hexo-generator-index2 --save\n$ npm uninstall hexo-generator-index --save\n```\n修改`_config.yml`\n```\nindex_generator:\n  per_page: 10\n  order_by: -date\n  include:\n    - category Web  # 只包含Web分类下的文章\n  exclude:\n    - tag Hexo      # 不包含标签为Hexo的文章\n```\n\n## 数学公式支持\n`hexo`默认的渲染引擎是`marked`，但是`marked`不支持`mathjax`。`kramed`是在`marked`的基础上进行修改。\n```\n$ npm uninstall hexo-math --save              # 停止使用 hexo-math\n$ npm install hexo-renderer-mathjax --save    # 安装hexo-renderer-mathjax包：\n$ npm uninstall hexo-renderer-marked --save   # 卸载原来的渲染引擎\n$ npm install hexo-renderer-kramed --save     # 安装新的渲染引擎\n```\n修改`/node_modules/kramed/lib/rules/inline.js`\n```\n11| escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_>])/,\n...\n20| em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n\n->\n\n11| escape: /^\\\\([`*\\[\\]()#$+\\-.!_>])/,\n...\n20| em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n```\n修改`/node_modules/hexo-renderer-kramed/lib/renderer.js`\n```\n64| // Change inline math rule\n65| function formatText(text) {\n66|   // Fit kramed's rule: $$ + \\1 + $$\n67|   return text.replace(/`\\$(.*?)\\$`/g, '$$$$$1$$$$');\n68| }\n\n->\n\n64| // Change inline math rule\n65| function formatText(text) {\n66|   // Fit kramed's rule: $$ + \\1 + $$\n67|   // return text.replace(/`\\$(.*?)\\$`/g, '$$$$$1$$$$');\n68|   return text;\n69| }\n```\n在主题中开启`mathjax`开关，例如`next`主题中\n```\n# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n在文章中\n```\n---\ntitle: title.md\ndate: 2019-01-04 12:47:37\ncategories:\ntags:\nmathjax: true\ntop:\n---\n```\n\n测试\n$$\nA = \\left[\\begin{matrix}\n    a_{11} & a_{12} \\\\\n    a_{21} & a_{22}\n\\end{matrix}\\right]\n$$\n\n# Reference\n> 基于hexo+github搭建一个独立博客 - 牧云云 - 博客园 https://www.cnblogs.com/MuYunyun/p/5927491.html\n> hexo+github pages轻松搭博客(1) | ex2tron's Blog http://ex2tron.wang/hexo-blog-with-github-pages-1/\n> hexo下LaTeX无法显示的解决方案 - crazy_scott的博客 - CSDN博客 https://blog.csdn.net/crazy_scott/article/details/79293576\n> 在Hexo中渲染MathJax数学公式 - 简书 https://www.jianshu.com/p/7ab21c7f0674\n> 怎么去备份你的Hexo博客 - 简书 https://www.jianshu.com/p/baab04284923\n> Hexo中添加本地图片 - 蜕变C - 博客园 https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&utm_medium=referral\n> hexo 搜索功能 - 阿甘的博客 - CSDN博客 https://blog.csdn.net/ganzhilin520/article/details/79047983","categories":["Others"]},{"title":"scikit-learn: 处理特征","url":"/2018/11/24/scikit-learn-处理特征/","content":"\n# 数据预处理(preprocessing)\n## Module\n```\n>>> import sklearn.preprocessing as preprocessing\n>>> dir(preprocessing)\n['Binarizer', 'CategoricalEncoder', 'FunctionTransformer', 'Imputer', 'KBinsDiscretizer', \n'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MaxAbsScaler', 'MinMaxScaler', \n'MultiLabelBinarizer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PolynomialFeatures', \n'PowerTransformer', 'QuantileTransformer', 'RobustScaler', 'StandardScaler', \n'__all__', '__builtins__', '__cached__', '__doc__', '__file__', \n'__loader__', '__name__', '__package__', '__path__', '__spec__', \n'_discretization', '_encoders', '_function_transformer', \n'add_dummy_feature', 'base', 'binarize', 'data', \n'imputation', 'label', 'label_binarize', 'maxabs_scale', \n'minmax_scale', 'normalize', 'power_transform', \n'quantile_transform', 'robust_scale', 'scale']\n```\n\n# 特征抽取(feature extraction)\n## Module\n```\n>>> import sklearn.feature_extraction as feature_extraction\n>>> dir(feature_extraction)\n['DictVectorizer', 'FeatureHasher', \n'__all__', '__builtins__', '__cached__', '__doc__', '__file__', \n'__loader__', '__name__', '__package__', '__path__', '__spec__', \n'_hashing', 'dict_vectorizer', 'grid_to_graph', 'hashing', \n'image', 'img_to_graph', 'stop_words', 'text']\n```\n\n# 特征选择(feature selection)\n当数据预处理完成后，我们需要选择有意义的特征，将其输入到模型中训练，主要从两个方面考虑\n- **特征是否发散**\n    若一个特征不发散，其方差接近$0$，则表示该特征在各个样本上没有差别，对于样本的区分没什么用；\n- **特征与目标的相关性**\n    与目标`(target)`相关性高的特征，应当优先选择。\n\n\n特征选择的方法可以根据特征选择的形式分为$3$种\n- **过滤法`(Filter)`**\n    按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n- **包装法`(Wrapper)`**\n    根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。\n- **嵌入法`(Embedded)`**\n    先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于`Filter`方法，但是是通过训练来确定特征的优劣。\n\n## Filter\n### 移除低方差\n> `Removing features with low variance`\n\n即移除那些方差较小的特征，当特征的取值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。\n\n现实中这种方法作用不大，可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。\n\n```\n# class sklearn.feature_selection.VarianceThreshold(threshold=0.0)\n```\n\n调用例程如下\n```\n>>> X = [\n            [0, 0, 1], \n            [0, 1, 0], \n            [1, 0, 0], \n            [0, 1, 1], \n            [0, 1, 0], \n            [0, 1, 1],\n    ]\n>>> feature_selection.VarianceThreshold(threshold=(.8 * (1 - .8))).fit_transform(X) \narray([[0, 1],\n       [1, 0],\n       [0, 0],\n       [1, 1],\n       [1, 0],\n       [1, 1]])\n>>> # 选出了第2, 3列特征\n```\n\n\n### 单变量特征选择\n> `Univariate feature selection`\n\n分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。\n\n指标适用情况：\n1. 对于分类问题(`target`离散)\n    卡方检验，f_classif, mutual_info_classif，互信息\n2. 对于回归问题(`target`连续)\n\n> 注：分类与回归在一定程度上可以互相转换，\n\n## Wrapper\n\n## Embedded\n\n## Module\n```\n>>> import sklearn.feature_selection as feature_selection\n>>> dir(feature_selection)\n['GenericUnivariateSelect', 'RFE', 'RFECV', \n'SelectFdr', 'SelectFpr', 'SelectFromModel', 'SelectFwe', \n'SelectKBest', 'SelectPercentile', 'VarianceThreshold', \n'__all__', '__builtins__', '__cached__', '__doc__', '__file__', \n'__loader__', '__name__', '__package__', '__path__', '__spec__', \n'base', 'chi2', 'f_classif', 'f_oneway', \n'f_regression', 'from_model', 'mutual_info_', 'mutual_info_classif', \n'mutual_info_regression', 'rfe', 'univariate_selection', 'variance_threshold']\n```\n\n# 参考博客(reference)\n> 使用sklearn优雅地进行数据挖掘 - jasonfreak - 博客园 http://www.cnblogs.com/jasonfreak/p/5448462.html\n> 特征选择 (feature_selection) - 会飞的蝸牛 - 博客园 https://www.cnblogs.com/stevenlk/p/6543628.html\n\n","categories":["Machine Learning"]},{"title":"Metrics","url":"/2018/11/21/Metrics/","content":"\n# 回归(regression)评估指标\n## 解释方差(Explained Variance)\n$$\nEV(\\hat{y}, y)\n= 1 - \\frac{Var(y-\\hat{y})}{Var(y)}\n$$\n\n解释方差越接近$1$表示回归效果越好。\n\n## 平均绝对误差(Mean Absolute Error - MAE)\n$$\nMAE(\\hat{y}, y) \n= E(||\\hat{y} - y||_1)\n= \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} |\\hat{y}^{(i)} - y^{(i)}|\n$$\n\n$MAE$越小表示回归效果越好。\n\n## 平均平方误差(Mean Squared Error - MSE)\n在[线性回归](https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/)一节，使用的损失函数即$MSE$\n\n$$\nMSE(\\hat{y}, y) \n= E(||\\hat{y} - y||_2^2)\n= \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} (\\hat{y}^{(i)} - y^{(i)})^2\n$$\n\n其中$y$与$\\hat{y}$均为$1$维向量，$MSE$越小表示回归效果越好。\n\n其含义比较直观，即偏差的平方和。也可以从最小化方差的角度解释，定义误差向量\n$$\ne = \\hat{y} - y\n$$\n\n我们假定其期望为$0$，即\n$$\nE(e) = 0　或　\\overline{e} = 0\n$$\n\n那么误差的方差为\n$$\nVar(e) = E[(e - \\overline{e})^T (e - \\overline{e})] = E(||e||_2^2)\n$$\n\n也即$MSE$。\n\n## 均方根误差(Root Mean Squared Error - RMSE)\n$$\nRMSE(\\hat{y}, y) \n= \\sqrt{\\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} (\\hat{y}^{(i)} - y^{(i)})^2}\n$$\n\n实质与$MSE$是一样的。只不过用于数据更好的描述，使计算得损失的值较小。$RMSE$越小表示回归效果越好。\n\n## 均方对数误差(Mean Squard Logarithmic Error - MSLE)\n$$\nMSLE(\\hat{y}, y) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} \\left[\\log (1+y^{(i)}) - \\log (1+\\hat{y}^{(i)})\\right]^2\n$$\n\n通常用于输出指数增长的模型，如，人口统计，商品的平均销售量，以及一段时间内的平均销售量等。注意，由对数性质，这一指标对过小的预测的惩罚大于预测过大的预测的惩罚。\n\n## 中值绝对误差(Median Absolute Error - MedAE)\n$$\nMedAE(\\hat{y}, y) = median(|y - \\hat{y}|)\n$$\n\n## R决定系数(R2)\n又称拟合优度，提供了一个衡量未来样本有多好的预测模型。最佳可能的分数是$1.0$，它可以是负的(因为模型可以任意恶化)。一个常数模型总是预测$y$的期望值，而不考虑输入特性，则得到$R^2$分数为$0.0$。\n$$\nR^2(\\hat{y}, y) = 1 - \\frac{\\sum_{i=1}^{n_{samples}} (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^{n_{samples}} (y^{(i)} - \\overline{y})^2}\n$$\n\n其中\n$$\n\\overline{y} = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} y^{(i)}\n$$\n\n# 分类(classification)评估指标\n先作如下定义\n![terminology_and_derivations_1](/Metrics/terminology_and_derivations_1.png)\n![terminology_and_derivations_2](/Metrics/terminology_and_derivations_2.png)\n![terminology_and_derivations_3](/Metrics/terminology_and_derivations_3.png)\n\n![metrics_classification2](/Metrics/metrics_classification2.png)\n\n\n## 准确率(Accuracy)\n$$\nAccuracy(y, \\hat{y})\n= \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} 1(y^{(i)}=\\hat{y}^{(i)})\n$$\n\n也即\n$$\nAccuracy\n= \\frac{TN+TP}{TN+TP+FN+FP}\n$$\n\n精度只是简单地计算出比例，但是没有对不同类别进行区分。因为不同类别错误代价可能不同。例如：判断这个病人是不是病危，如果不是病危错误判断为病危，那只是损失一点医务人员的时间和精力，如果是把病危的人判断为非病危状态，那损失的就是一条人命。他们之间存在重要性差异，这时候就不能用精度。对于样本不均衡的情况，也不是用精度来衡量。例如：有A类1000个，B类5个，如果我把这1005个样本都预测成A类，正确率=1000/1005=99.5%。\n\n## 精确率(Precision)与召回率(Recall)\n- 精确率`(Precision)`\n    即预测正样本中，实际为正样本的百分比，度量了分类器不会将真正的负样本错误地分为正样本的能力。\n    $$\n    Precision = \\frac{TP}{TP+FP}\n    $$\n\n- 召回率`(Recall)`\n    又称查全率，即实际正样本中，被预测为正样本的百分比，度量了分类器找到所有正样本的能力。\n    $$\n    Recall = \\frac{TP}{TP + FN}\n    $$\n![precision_recall](/Metrics/precision_recall.png)\n\n## F度量\n> [F1 score - Wikipedia](https://en.wikipedia.org/wiki/F1_score)\n\n- $F_1$\n    为精确率`(Precision)`与召回率`(Recall)`的调和均值`(harmonic mean)`。\n    $$\n    \\frac{1}{F_1} \n    = \\frac{1}{2} (\\frac{1}{Precision} + \\frac{1}{Recall})\n    $$\n\n    也即\n    $$\n    F_1 = 2 · \\frac{Precision·Recall}{Precision + Recall}\n    $$\n\n- $F_{\\beta}$\n    在$F_1$度量的基础上增加权值$\\beta$，$\\beta$越大，$Recall$的权重越大，否则$Precision$的权重越大。\n    $$\n    \\frac{1}{F_{\\beta}} = \\frac{1}{1+\\beta^2} \\frac{1}{Precision} + \\frac{\\beta^2}{1+\\beta^2}\\frac{1}{Recall}\n    $$\n\n    也即\n    $$\n    F_{\\beta} = (1+\\beta^2)·\\frac{Precision·Recall}{(\\beta^2·Precision) + Recall}\n    $$\n\n## 混淆矩阵\n`Confusion matrix`，也被称作错误矩阵`(Error matrix)`，是一个特别的表。无监督学习中，通常称作匹配矩阵`(Matching matrix)`。每一列表达了分类器对样本的类别预测，每一行表达了样本所属的真实类别。\n\n\n例如我们有$27$个待分类样本，将其划分为`Cat`，`Dog`，`Rabbit`，讲实际标签与预测标签数目统计后填入混淆矩阵。\n\n![confusion_matrix](/Metrics/confusion_matrix.png)\n\n例如实际上有$8$个样本为`Cat`，而该分类器将其中$3$个划分为`Dog`，将$2$个为`Dog`的样本划分为`Cat`。我们可以根据上述混淆矩阵得出结论，该分类器对`Dog`和`Cat`分类能力较弱，而对`Rabbit`分类能力较强。而且正确预测的样本数目都在对角线上，很容易直观地检查表中的预测错误。\n\n以下为`scikit-learn`中混淆矩阵的`API`\n```\n>>> from sklearn.metrics import confusion_matrix\n>>> \n>>> y_true = [2, 0, 2, 2, 0, 1]\n>>> y_pred = [0, 0, 2, 2, 0, 2]\n>>> confusion_matrix(y_true, y_pred)\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n>>> \n>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n>>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n>>> \n>>> # In the binary case, we can extract true positives, etc as follows:\n>>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n>>> (tn, fp, fn, tp)\n(0, 2, 1, 1)\n\n```\n\n## ROC曲线\n`Receiver Operating Characteristic`，是根据一系列不同的二分类方式(分界值或决定阈)，以召回率(真正率`TPR`、灵敏度)为纵坐标，`fall-out`(假正率`FPR`、$1$-特异度)为横坐标绘制的曲线。\n- `true positive rate - TPR`\n    所有阳性样本中有多少正确的阳性结果。\n    $$\n    TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}\n    $$\n\n- `false positive rate - FPR`\n    所有阴性样本中有多少不正确的阳性结果。\n    $$\n    FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}\n    $$\n\n### ROC space\n![ROC_space](模型的评估指标/ROC_space.png)\n\n- 分别以`FPR`与`TPR`作为横纵轴(又称灵敏度-$1$特异度曲线`sensitivity vs (1 − specificity) plot`)；\n- 每次预测结果或混淆矩阵的实例代表了`ROC`空间中的一个点；\n    例如上图中$A, B, C, C'$是以下表数据计算得到的点。\n    ![ROC_space_samples](/Metrics/ROC_space_samples.png)\n- 在`ROC`空间中最左上方的点$(0, 1)$称作完美分类器`(perfect classification)`；\n- 随机分类器的结果分布在`ROC space`对角线$(0, 0)-(1, 1)$上，当实验次数足够多，其分区趋向$(0.5, 0.5)$;\n- 对角线以上的点代表好的分类结果(比随机的好)；线下的点代表坏的结果(比随机的差)；\n- 注意，持续不良分类器的输出可以简单地反转以获得一个好的分类器，反转后的分类器与原分类器在平面上关于对角线对称，例如点$C'$。\n\n### ROC曲线的绘制\n若训练集样本中，正样本与负样本以正态分布的形式分布在样本平面上，如下图，左峰为负样本，右峰为正样本，存在部分重叠(不然就不用搞这么多分类算法了)。\n\n![ROC_curves.svg](/Metrics/ROC_curves.svg.png)\n\n若假设正样本概率密度为$f_1(x)$，负样本的概率密度为$f_0(x)$，给定阈值$T$，则右\n$$\nTPR(T) = \\int_T^{\\infty} f_1(x) dx\n$$\n\n$$\nFPR(T) = \\int_T^{\\infty} f_0(x) dx\n$$\n\n选取不同的阈值划分分类器输出，就能得到`ROC`曲线。\n\n在基于有限样本作`ROC`图时，可以看到曲线每次都是一个“爬坡”，遇到正例往上爬一格$(1/m+)$，错了往右爬一格$(1/m-)$，显然往上爬对于算法性能来说是最好的。\n![ROC_curves_up_right](/Metrics/ROC_curves_up_right.png)\n\n\n### Area Under the Curve - AUC\n`ROC`曲线下的面积`AUC`物理意义为，任取一对正负样本，正样本的预测值大于负样本的预测值的概率。\n\n$$\nA = \\int_{-\\infty}^{\\infty} TPR(T) dFPR(T)\n$$\n\n$$\n= \\int_{-\\infty}^{\\infty} \n\\int_{-\\infty}^{\\infty}\nI(T'> T)\nf_1(T') f_0(T)\ndT' dT\n$$\n\n$$\n= P(X_1 > X_0)\n$$\n\n同样的，在有限个样本下，其面积用累加的方法计算(梯形面积)\n\n![ROC_curves_AUC](/Metrics/ROC_curves_AUC.png)\n\n$$\nAUC = \\sum_{i=1}^{m-1} \\frac{1}{2} (y_{i+1} + y_i)(x_{i+1} - x_i)\n$$\n\n- $AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n- $0.5 < AUC < 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。\n- $AUC = 0.5$，跟随机猜测一样（例：丢铜板），模型没有预测价值。\n- $AUC < 0.5$，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。\n\n### sklearn\n以下为`scikit-learn`中混淆矩阵的`ROC`曲线`API`。\n```\n>>> import numpy as np\n>>> from sklearn import metrics\n>>> \n>>> y = np.array([1, 1, 2, 2])\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> \n>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n>>> fpr\narray([ 0. ,  0.5,  0.5,  1. ])\n>>> tpr\narray([ 0.5,  0.5,  1. ,  1. ])\n>>> thresholds\narray([ 0.8 ,  0.4 ,  0.35,  0.1 ])\n>>> \n>>> metrics.auc(fpr, tpr)\n0.75\n```\n\n# 聚类(clustering)评估指标\n> - [AI（005） - 笔记 - 聚类性能评估（Clustering Evaluation） - DarkRabbit的专栏 - CSDN博客 ](https://blog.csdn.net/darkrabbit/article/details/80378597)\n> - [Wikipedia, the free encyclopedia](https://en.wikipedia.org/wiki/Main_Page)\n\n\n## 说明\n聚类性能比较好，就是聚类结果簇内相似度`(intra-cluster similarity)`高，而簇间相似度`(inter-cluster similarity)`低，即同一簇的样本尽可能的相似，不同簇的样本尽可能不同。\n\n聚类性能的评估（度量）分为两大类：\n- 外部评估`(external evaluation)`：将结果与某个参考模型`(reference model)`进行比较；\n- 内部评估`(internal evaluation)`：直接考虑聚类结果而不利用任何参考模型。\n\n将$n_{samples}$个样本$\\{x^{(1)}, ..., x^{(n_{samples})}\\}$用待评估聚类算法划分为$K$个类$\\{X_1, ..., X_K\\}$，假定参考模型将其划分为$L$类$\\{Y_1, ..., Y_L\\}$，将样本两辆匹配\n$$\n\\begin{cases}\n    a = |SS| &  SS = \\{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \\in X_k; x^{(i)}, x^{(j)} \\in Y_l\\} \\\\\n    b = |SD| &  SD = \\{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \\in X_k; x^{(i)} \\in Y_{l1}, x^{(j)} \\in Y_{l2}\\} \\\\\n    c = |DS| &  DS = \\{(x^{(i)}, x^{(j)}) | x^{(i)} \\in X_{k1}, x^{(j)} \\in X_{k2}; x^{(i)}, x^{(j)} \\in Y_l\\} \\\\\n    d = |DD| &  DD = \\{(x^{(i)}, x^{(j)}) | x^{(i)} \\in X_{k1}, x^{(j)} \\in X_{k2}; x^{(i)} \\in Y_{l1}, x^{(j)} \\in Y_{l2}\\}\n\\end{cases}\n$$\n\n其中$k = 1, ..., K; l = 1, ..., L$\n\n$$\na + b + c + d\n=   \\left(\n        \\begin{matrix}\n            n \\\\ 2\n        \\end{matrix}\n    \\right)\n= \\frac{n(n-1)}{2}\n$$\n\n> - $SS$包含两种划分中均属于同一类的样本对；\n> - $SD$包含用待评估聚类算法划分中属于同一类，而在参考模型中属于不同类的样本对；\n> - $DS$包含用待评估聚类算法划分中属于不同类，而在参考模型中属于同一类的样本对；\n> - $DD$包含两种划分中均不属于同一类的样本对。\n\n\n## 常用外部评估(external evaluation)\n### Rand Index(RI)\n> [Rand index - Wikipedia](https://en.wikipedia.org/wiki/Rand_index)\n\n$$\nRI = \\frac{a+d}{a + b + c + d} = \\frac{a+d}{\\left(\\begin{matrix} n \\\\ 2 \\end{matrix}\\right)}\n$$\n\n显然，结果值在$[0,1]$之间，且值越大越好。\n- 当为$0$时，两个聚类无重叠；\n- 当为$1$时，两个聚类完全重叠。\n\n### Adjust Rand Index(ARI)\n让$RI$有了修正机会`(corrected-for-chance)`，在取值上从$[0,1]$变成$[-1, 1]$\n\n对于$X$与$Y$的重叠可以用一个列联表`(contingency table)`表示，记作$[n_{ij}]$，$n_{ij} = |X_i \\bigcap Y_j|$\n![ARI](/聚类/ARI.svg)\n\n则定义$ARI$如下\n![ARI_Def](/聚类/ARI_Def.svg)\n\n### 互信息与调整互信息(Adjusted Mutual Information - AMI)\n\n> 关于互信息可查看[熵]()一节说明。\n\n$X_i$类别的概率定义为\n$$\nP(k) = \\frac{|X_k|}{N}\n$$\n\n则划分结果的熵定义为\n$$\nH(X) = - \\sum_k P(k) \\log P(k)\n$$\n\n类似的\n$$\nP'(l) = \\frac{|Y_l|}{N}\n$$\n\n$$\nH(Y) = - \\sum_j P'(l) \\log P'(l)\n$$\n\n另外\n$$\nP(k, l) = \\frac{|X_k, Y_l|}{N}\n$$\n\n那么两种划分的互信息定义为\n$$\nMI(X, Y) = \\sum_{k, l} P(k, l) \\log \\frac{P(k, l)}{P(k) P'(l)}\n$$\n\n\n和$ARI$一样，我们对它进行调整。\n$$\nE[MI(X, Y)] \n= \\sum_k \\sum_l \\sum_{n_{kl} = \\max\\{1, a_k + b_l - N\\}}^{\\min \\{a_k, b_l\\}}\n\\frac{n_{kl}}{N}\n\\log \\left( \\frac{N·n_{kl}}{a_k b_l} \\right) ×\n$$\n\n$$\n\\frac\n{a_k!b_l!(N-a_k)!(N-b_l)!}\n{N!n_{kl}!(a_k-n_{kl})!(b_l-n_{kl})!(N-a_k-b_l+n_{kl})!}\n$$\n\n最终$AMI$表达式为\n$$\nAMI(X, Y) = \\frac{MI(X, Y) - E[MI(X, Y)]}{\\max \\{H(X), H(Y)\\} - E[MI(X, Y)]}\n$$\n\n### 同质性(Homogeneity)与完整性(Completeness)\n这两个类似分类种的的准确率`(accuracy)`与召回率`(recall)`。\n- 同质性`(Homogeneity)`\n    即一个簇仅包含一个类别的样本\n    $$\n    H = 1 - \\frac{H(X|Y)}{H(X)}\n    $$\n\n    其中$H(X|Y)$为条件熵\n    $$\n    H(X|Y) = \\sum_k \\sum_l P(X_k, Y_l) \\log \\frac{P(Y_l)}{P(X_k, Y_l)}\n    = \\sum_k \\sum_l \\frac{n_{kl}}{N} \\log \\frac{n_{kl}}{N}\n    $$\n\n- 完整性`(Completeness)`\n    同类别样本被归类到相同簇中\n    $$\n    C = 1 - \\frac{H(Y|X)}{H(Y)}\n    $$\n\n- $V-measure$\n    `Homogeneity`和`Completeness`的调和平均\n    $$\n    V = \\frac{1}{\\frac{1}{2} \\left(\\frac{1}{H} + \\frac{1}{C}\\right)} = \\frac{2HC}{H + C}\n    $$\n\n### Fowlkes-Mallows index(FMI)\n成对精度和召回率的几何均值\n\n> [Fowlkes–Mallows index - Wikipedia](https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index)\n\n定义\n- $TP$ as the number of points that are present in the same cluster in both $A_1$ and $A_2$.\n- $FP$ as the number of points that are present in the same cluster in $A_1$ but not in $A_2$.\n- $FN$ as the number of points that are present in the same cluster in $A_2$ but not in $A_1$.\n- $TN$ as the number of points that are in different clusters in both $A_1$ and $A_2$.\n\n则\n$$\nTP + FP + TN + FN = \\frac{n(n-1)}{2}\n$$\n\n定义\n$$\nFMI = \\sqrt{\\frac{TP}{TP + FP} · \\frac{TP}{TP + FN}}\n$$\n\n### 杰卡德系数(Jaccard Coefficient - JC)\n\n> [Jaccard index - Wikipedia](https://en.wikipedia.org/wiki/Jaccard_index)\n\n\n给定两个具有$n$个元素的集合$A, B$，定义\n- $M_{11}$ represents the total number of attributes where $A$ and $B$ both have a value of $1$.\n- $M_{01}$ represents the total number of attributes where the attribute of $A$ is $0$ and the attribute of $B$ is $1$.\n- $M_{10}$ represents the total number of attributes where the attribute of $A$ is $1$ and the attribute of $B$ is $0$.\n- $M_{00}$ represents the total number of attributes where $A$ and $B$ both have a value of $0$.\n\n则有\n$$\nM_{11} + M_{01} + M_{10} + M_{00} = n\n$$\n\n- `Jaccard`相似度系数\n    $$\n    J = \\frac{M_{11}}{M_{01} + M_{10} + M_{11}}\n    $$\n\n    > 也即$J=\\frac{A \\cap B}{A \\cup B}$\n\n- `Jaccard`距离\n    $$\n    D_J = 1 - J\n    $$\n\n## 常用内部评估(internal evaluation)\n### 轮廓系数(Silhouette coefficient)\n又称侧影法，适用于实际类别信息未知的情况，对其中一个样本点$x^{(i)}$，记\n- $a(i)$：到本簇其他样本点的距离的平均值\n- $b(i)$：该点到其他各个簇的样本点的平均距离的最小值\n\n定义轮廓系数\n$$\nS(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n$$\n\n或者\n$$\nS(i) = \\begin{cases}\n    1 - \\frac{a(i)}{b(i)} & a(i) < b(i) \\\\\n    0 & a(i) = b(i) \\\\\n    \\frac{b(i)}{a(i)} - 1 & a(i) > b(i)\n\\end{cases}\n$$\n\n其含义如下\n- 当$a(i) \\ll b(i)$时，无限接近于$1$，则意味着聚类合适；\n- 当$a(i) \\gg b(i)$时，无限接近于$-1$，则意味着把样本i聚类到相邻簇中更合适；\n- 当$a(i)\\approxeq b(i)$时，无限接近于$0$，则意味着样本在两个簇交集处。\n\n一般再对各个点的轮廓系数求均值\n$$\n\\overline{S} = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} S(i)\n$$\n\n- 当$\\overline{S} > 0.5$，表示聚类合适；\n- 当$\\overline{S} < 0.2$，表示表明数据不存在聚类特征\n\n### Calinski-Harabaz(CH)\n也适用于实际类别信息未知的情况，以$K$分类为例\n- 类内散度$W$\n    $$\n    W(K) = \\sum_k \\sum_{C(j)=k} ||x_j - \\overline{x_k}||^2\n    $$\n\n- 类间散度$B$\n    $$\n    B(K) = \\sum_k a_k ||\\overline{x_k} - \\overline{x}||^2\n    $$\n\n- $CH$\n    $$\n    CH(K) = \\frac{B(K)(N-K)}{W(K)(K-1)}\n    $$\n\n### Davies-Bouldin Index(DBI)\n定义\n- $c_k$：簇$C_k$的中心点\n- $\\sigma_k$：簇$C_k$中所有元素到$c_k$的距离的均值\n- $d(c_i, c_j)$：簇中心$c_i$与$c_j$之间的距离\n\n则\n$$\nDBI = \\frac{1}{K} \\sum_{i=1}^K \\max_{j \\neq i} \\left( \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)} \\right)\n$$\n\n$DBI$越小越好\n\n### Dunn index(DI)\n定义\n- $d(i,j)$：两类簇的距离，定义方法多样，例如两类簇中心的距离；\n- $d'(k)$：簇$C_k$的类内距离，同样的，可定义多种，例如簇$C_k$中任意两点距离的最大值。\n\n则\n$$\nDI = \\frac{\\min_{1 \\leq i < j \\leq K} d(i, j)}{\\max_{1 \\leq k \\leq K} d'(k)}\n$$\n\n# sklearn中的评价指标\n> [3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.19.0 documentation - ApacheCN](http://sklearn.apachecn.org/en/0.19.0/modules/model_evaluation.html)\n\n![sklearn_metrics](/Metrics/sklearn_metrics.png)\n\n```\n>>> from sklearn import metrics\n>>> dir(metrics)\n['SCORERS', '__all__', '__builtins__', '__cached__', '__doc__', \n'__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__',\n 'accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', \n 'auc', 'average_precision_score', 'balanced_accuracy_score', \n 'base', 'brier_score_loss', 'calinski_harabaz_score', 'check_scoring', \n 'classification', 'classification_report', 'cluster', 'cohen_kappa_score', \n 'completeness_score', 'confusion_matrix', 'consensus_score', \n 'coverage_error', 'davies_bouldin_score', 'euclidean_distances', \n 'explained_variance_score', 'f1_score', 'fbeta_score', \n 'fowlkes_mallows_score', 'get_scorer', 'hamming_loss', 'hinge_loss', \n 'homogeneity_completeness_v_measure', 'homogeneity_score', \n 'jaccard_similarity_score', 'label_ranking_average_precision_score', \n 'label_ranking_loss', 'log_loss', 'make_scorer', 'matthews_corrcoef', \n 'mean_absolute_error', 'mean_squared_error', 'mean_squared_log_error', \n 'median_absolute_error', 'mutual_info_score', \n 'normalized_mutual_info_score', 'pairwise', 'pairwise_distances', \n 'pairwise_distances_argmin', 'pairwise_distances_argmin_min', \n 'pairwise_distances_chunked', 'pairwise_fast', 'pairwise_kernels', \n 'precision_recall_curve', 'precision_recall_fscore_support', \n 'precision_score', 'r2_score', 'ranking', 'recall_score', 'regression', \n 'roc_auc_score', 'roc_curve', 'scorer', 'silhouette_samples', \n 'silhouette_score', \n 'v_measure_score', \n 'zero_one_loss']\n```","categories":["Machine Learning"]},{"title":"Entropy","url":"/2018/11/21/Entropy/","content":"\n# 信息量\n概率$p$是对确定性的度量，那么信息量就是对不确定性的度量，公式定义为\n$$\nI(x) = - \\log p(x) \\tag{1}\n$$\n\n信息量也被称为随机变量$x$的自信息`(self-information)`\n\n> 底数为$2$时，单位为`bit`，底数为$e$时，单位为`nat`\n\n![信息量](/Entropy/信息量.png)\n\n# 信息熵\n信息熵`(information entropy)`定义为\n$$\nH(X) = - \\sum_{x} p(x) \\log p(x) \\tag{2}\n$$\n\n可看作**信息量的期望**,在$0-1$分布的信息熵为\n$$H(p) = - p \\log p - (1 - p) \\log (1 - p)$$\n\n图像如下，可见在$p=0.5$时，熵最大。\n![entropy_of_01](/Entropy/entropy_of_01.png)\n\n\n\n> 函数$y=x \\log x$的图像\n> ![xlogx](/Entropy/xlogx.png)\n> 有\n> $$\n> \\lim_{x \\rightarrow 0} y = \\lim_{x \\rightarrow 1} y = 0\n> $$ \n\n\n# 联合熵\n根据信息熵的定义，推广到多维随机变量，就得到联合熵的定义式，以$2$维随机变量为例\n$$\nH(X, Y) = - \\sum_{x, y} p(x, y) \\log p(x, y) \\tag{3}\n$$\n\n可推广至多维。\n\n<!-- 机器学习笔记十：各种熵总结 - 谢小小XH - CSDN博客 https://blog.csdn.net/xierhacker/article/details/53463567 -->\n\n\n# 交叉熵\n现在有关于样本集的两个概率分布$p(x)$和$q(x)$，其中$p(x)$为真实分布，$q(x)$非真实分布。\n\n如果用真实分布$p(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:\n$$\nH(p) = - \\sum_x p(x) \\log p(x)\n$$\n\n如果用非真实分布$q(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:\n$$\nH(p, q) = - \\sum_x p(x) \\log q(x) \\tag{4}\n$$\n\n注意\n$$\nH(p, q) - H(p)\n= \\sum_x p(x) \\log \\frac{p(x)}{q(x)}\n= D_{KL}(p||q)\n$$\n\n当用非真实分布$q(x)$得到的平均码长比真实分布$p(x)$得到的平均码长多出的比特数就是相对熵。我们希望通过最小化相对熵$D_{KL}(p||q)$使$q(x)$尽量趋近$p(x)$，即\n$$\nq(x) = \\arg \\min_{q(x)} D_{KL} (p||q)\n$$\n\n而$H(p)$是样本集的熵，为固定的值，故\n$$\nq(x) = \\arg \\min_{q(x)} H(p, q)\n$$\n\n即等价于最小化交叉熵。\n\n# 条件熵\n条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。定义为在给定$X$下$Y$的条件概率分布的熵对$X$的期望，即\n$$\nH(Y|X) \n= E_{p(x)} H(Y|X=x)\n= \\sum_x p(x) H(Y|X=x) \\tag{5}\n$$\n\n其中\n$$\nH(Y|X=x) = - \\sum_y p(y|x) \\log p(y|x)\n$$\n\n故\n$$\nH(Y|X) = \\sum_x p(x) \\left[- \\sum_y p(y|x) \\log p(y|x)\\right]\n$$\n\n$$\n= - \\sum_x \\sum_y p(x, y) \\log p(y|x) \n$$\n\n即\n$$\nH(Y|X) = - \\sum_{x, y} p(x, y) \\log p(y|x) \\tag{6}\n$$\n\n实际上，条件熵满足\n$$\nH(Y|X) = H(X, Y) - H(X) \\tag{7}\n$$\n\n> 证明：\n> 已知\n> $$\n> H(X, Y) = - \\sum_{x, y} p(x, y) \\log p(x, y)\n> $$\n> \n> $$\n> H(X) = - \\sum_{x} p(x) \\log p(x)\n> $$\n> \n> 则\n> $$\n> H(X, Y) - H(X)\n> $$\n> \n> $$ \n> = - \\sum_{x, y} p(x, y) \\log p(x, y) + \\sum_{x} p(x) \\log p(x)\n> $$\n> \n> $$\n> = - \\sum_{x, y} p(x, y) \\log p(x, y) + \\sum_{x, y} p(x, y) \\log p(x)\n> $$\n> \n> $$\n> = \\sum_{x, y} p(x, y) \\log \\frac{p(x)}{p(x, y)}\n> $$\n> \n> $$\n> = \\sum_{x, y} p(x, y) \\log p(y|x)\n> $$\n> \n> $$\n> = H(Y|X)\n> $$\n\n\n# 相对熵\n相对熵`(relative entropy)`，又称`KL`散度`(Kullback–Leibler divergence)`。可以用来衡量两个概率分布之间的差异，就是求$p(x)$与$q(x)$之间的对数差在 pp 上的期望值。\n$$\nD_{KL} (p||q) \n= E_{p(x)} \\log \\frac{p(x)}{q(x)}\n= \\sum_x p(x) \\log \\frac{p(x)}{q(x)} \\tag{8}\n$$\n\n注意\n- 相对熵不具有对称性，即\n    $$\n    D_{KL} (p||q) \\neq D_{KL} (q||p)\n    $$\n\n- $D_{KL} (p||q) \\geq 0$\n    > 证明：\n    > $$\n    > D_{KL} (p||q) = \\sum_x p(x) \\log \\frac{p(x)}{q(x)} = - \\sum_x p(x) \\log \\frac{q(x)}{p(x)}\n    > $$\n    > \n    > 由`Jensen inequality`\n    > $$\n    > \\sum_x p(x) \\log \\frac{q(x)}{p(x)}\n    > \\leq \\log \\sum_x p(x) \\frac{q(x)}{p(x)}\n    > = \\log \\sum_x q(x)\n    > $$\n    > \n    > 所以\n    > $$\n    > D_{KL} (p||q) \\geq - \\log \\sum_x q(x)\n    > $$\n    > \n    > 而$0 \\leq q(x) \\leq 1$，故\n    > $$\n    > D_{KL} (p||q) \\geq 0\n    > $$\n","categories":["Machine Learning","Deep Learing"]},{"title":"Non-parameter Estimation","url":"/2018/11/19/Non-parameter-Estimation/","content":"\n# 前言\n若参数估计时我们不知道样本的分布形式，那么就无法确定需要估计的概率密度函数，无法用[最大似然估计、贝叶斯估计等参数估计方法](https://louishsu.xyz/2018/10/22/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/)，应该用非参数估计方法。\n\n需要知道的是，作为非参数方法的共同问题是对样本数量需求较大，只要样本数目足够大众可以保证收敛于任何复杂的位置密度，但是计算量和存储量都比较大。当样本数很少时，如果能够对密度函数有先验认识，则参数估计能取得更好的估计效果。\n\n# 基本原理\n若有$M$个样本$x^{(1)}, ..., x^{(M)}$，依概率密度函数$p(x)$独立同分布抽样得到。\n\n一个样本$x$落在区域$R$中的概率$P$可表示为\n$$\nP = \\int_R p(x) dx \\tag{1}\n$$\n\n我们通过计算$P$来估计概率密度$p(x)$。\n\n\n$K$个样本落入区域$R$的概率$P_K$为二项分布，即$K \\sim B(M, P)$\n$$\nP_K = \\left(\\begin{matrix} M\\\\K \\end{matrix}\\right) P^K (1-P)^{M-K} \\tag{2}\n$$\n\n则$K$的期望与方差分别为\n$$\nE(K) = MP;　D(K) = MP(1-P)\n$$\n\n样本个数$M$越多，$D(K)$越大，即$K$在期望附近的波峰越明显，因此样本足够多时，用$K/M$作为$P$的一个估计非常准确，即\n$$\nP \\approx \\frac{K}{M} \\tag{3}\n$$\n\n若我们假设$p(x)$是连续的，且区域$R$足够小，记其体积为$V$，那么有\n$$\nP = \\int_R p(x)dx \\approx p(x) V \\tag{4}\n$$\n\n所以根据$(3)(4)$，得到\n$$\np(x) \\approx \\frac{K/M}{V} \\tag{*}\n$$\n\n但是我们获得的其实为平滑后的概率密度函数\n$$\n\\frac{P}{V} = \\frac{\\int_R p(x)dx}{\\int_R dx}\n$$\n\n我们希望其尽可能地趋近$p(x)$，那么必须要求$V \\rightarrow 0$，但是这样就可能不包含任何样本，那么$p(x)\\approx 0$，这样估计的结果毫无意义。\n\n所以在实际中，一般构造多个包含样本$x$的区域$R_1, ..., R_i, ..., R_n$，第$i$个区域使用$i$个样本，记$V_i$为$R_i$的体积，$M_i$为落在$R_i$中的样本个数，则对$p(x)$第$i$次估计$p_i(x)$表示为\n$$\np_i(x) \\approx \\frac{M_i / M}{V_i} \\tag{5}\n$$\n\n若要求$p_i(x)$收敛到$p(x)$，则必须满足\n- $\\lim_{i\\rightarrow \\infty} V_i = 0$\n- $\\lim_{i\\rightarrow \\infty} M_i = 0$\n- $\\lim_{i\\rightarrow \\infty} \\frac{M_i}{M} = 0$\n\n\n# 直方图法\n记不记得小学时的直方图统计，直方图方法的思想就是这样，以$1$维样本为例，我们将$x$的取值范围平均等分为$K$个区间，统计每个区间内样本的个数，由此计算区间的概率密度。\n\n## 原理\n若共有$N$维样本$M$组，在每个维度上$K$等分，就有$K^N$个小空间，每个小空间的体积$V_i$可以定义为\n$$\nV_i = \\prod_{n=1}^N d_n,　i=1,...,K^N\n$$\n\n其中\n$$\nd_n = \\frac{\\max x_n - \\min x_n}{K}\n$$\n\n假设样本落到各个小空间的概率相同，若第$i$个小空间包含$M_i$个样本，则该空间的概率密度$\\hat{p_i}$为\n$$\n\\hat{p_i} = \\frac{M_i / M}{V_i} \\tag{6}\n$$\n\n估计的效果与小区间的大小密切相连，如果区域选择过大，会导致最终估计出来的概率密度函数非常粗糙；如果区域的选择过小，可能会导致有些区域内根本没有样本或者样本非常少，这样会导致估计出来的概率密度函数很不连续。\n\n## 代码\n[@Github: Non-parametric Estmation](https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py)\n\n我们可以用`matplotlib.pyplot.hist()`或`numpy.histogram()`实现\n- `matplotlib`\n    ```\n    n, bins, patches = plt.hist(arr, bins=10, normed=0, facecolor='black', edgecolor='black',alpha=1，histtype='bar')\n    ```\n    - `Args`\n        参数很多，选几个常用的讲解\n        - arr: 需要计算直方图的一维数组\n        - bins: 直方图的柱数，可选项，默认为10\n        - normed: 是否将得到的直方图向量归一化。默认为0\n        - facecolor: 直方图颜色\n        - edgecolor: 直方图边框颜色\n        - alpha: 透明度\n        - histtype: 直方图类型，‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’\n    - `Returns`\n        - n: 直方图向量，是否归一化由参数normed设定\n        - bins: 返回各个bin的区间范围\n        - patches: 返回每个bin里面包含的数据，是一个list\n\n- `numpy`\n    ```\n    hist, bin_edges = histogram(a, bins=10, range=None, normed=None, weights=None, density=None)\n    ```\n\n```\ndef histEstimate(X, n_bins, showfig=False):\n    \"\"\" 直方图密度估计\n    Args:\n        n_bins: {int} 直方图的条数\n    Returns:\n        hist: {ndarray(n_bins,)}\n    \"\"\"\n    n, bins, patches = plt.hist(X, bins=n_bins, normed=1, facecolor='lightblue', edgecolor='white')\n    if showfig: plt.show()\n    return n, bins, patches\n```\n\n`matplotlib`直方图显示如下\n![hist_matplotlib](/Non-parameter-Estimation/hist_matplotlib.png)\n\n拟合各中心点显示如下\n![hist_ploy](/Non-parameter-Estimation/hist_ploy.png)\n\n# $K_n$近邻估计法\n随着样本数的增加，区域的体积应该尽可能小，同时又必须保证区域内有充分多的样本，但是每个区域的样本数有必须是总样本数的很小的一部分，而不是与直方图估计那样体积不变。\n\n那么我们想，能否根据样本的分布调整分区大小呢，$K$近邻估计法就是一种采用可变大小区间的密度估计方法。\n## 原理\n根据总样本确定参数$K_n$，在求样本$x$处的密度估计$\\hat{p}(x)$时，调整区域体积$V(x)$，直到区域内恰好落入$K_n$个样本，估计公式为\n$$\n\\hat{p}(x) = \\frac{K_n/M}{V(x)} \\tag{7}\n$$\n\n一般指定超参数$a$，取\n$$\nK_n = a × \\sqrt{M} \\tag{8}\n$$\n\n> $$\n> \\hat{p}(x) = \\frac{a × \\sqrt{M} /M}{V(x)} = \\frac{K_n'/M}{V'(x)}\n> $$\n> \n> 其中$K_n' = a,V'(x) = V(x)×\\frac{1}{\\sqrt{M}}$\n\n\n在样本密度比较高的区域的体积就会比较小，而在密度低的区域的体积则会自动增大，这样就能够较好的兼顾在高密度区域估计的分辨率和在低密度区域估计的连续性。\n\n# Parzen窗法\n又称核密度估计。\n\n## 原理\n我们暂时假设待估计点$x$的附近区间$R$为一个$N$维的**超立方体**，用$h$表示边的长度，那么\n$$\nV_i = h^N\n$$\n\n即\n![Parzen_window](/Non-parameter-Estimation/Parzen_window.jpg)\n定义窗函数$\\varphi(·)$，表示落入以$x$为中心的超立方体的区域的点\n$$\n\\varphi \\left(\\frac{x_i-x}{h}\\right) \n= \\begin{cases}\n    1 & \\frac{|x_{in}-x_n|}{h} \\leq \\frac{1}{2},　n=1,...,N \\\\\n    0 & otherwise\n\\end{cases} \\tag{9}\n$$\n\n> $$\\frac{|x_{in}-x_n|}{h} \\leq \\frac{1}{2}　即　(x_i-x)_n \\leq \\frac{h}{2}$$\n> \n> 这里的$h$起到单位化的作用，便于推广\n\n\n那么落入以$x$为中心的**超立方体**的区域的点的个数为\n$$\nM_i = \\sum_{i=1}^M \\varphi \\left(\\frac{x_i-x}{h}\\right) \\tag{10}\n$$\n\n代入$p(x) \\approx \\frac{M_i/M}{V_i}$，我们得到\n$$\np(x) \n\\approx \\frac{\\sum_{i=1}^M \\varphi \\left(\\frac{x_i-x}{h}\\right)/M}{V_i}\n= \\frac{1}{M} \\sum_{i=1}^M \\frac{1}{V_i} \\varphi \\left(\\frac{x_i-x}{h}\\right) \\tag{11}\n$$\n\n我们定义核函数(或称“窗函数”)\n$$\n\\kappa(z) = \\frac{1}{V_i} \\varphi(z) \\tag{12}\n$$\n\n核函数反应了一个观测样本$x_i$对在$x$处的概率密度估计的贡献，与样本$x_i$和$x$的距离有关。而概率密度估计就是在这一点上把所有观测样本的贡献进行平均\n$$\np(x) \n\\approx \\frac{1}{M} \\sum_{i=1}^M \\kappa\\left(\\frac{x_i-x}{h}\\right) \\tag{13}\n$$\n\n## 核函数\n核函数应满足概率密度的要求，即\n$$\n\\kappa(z) \\geq 0　\\And　\\int \\kappa(z)dz = 1 \n$$\n\n通常有以下几种核函数\n- 均匀核\n    $$\n    \\kappa(z)\n    = \\begin{cases}\n        1 & |z_n| \\leq \\frac{1}{2},　n=1,...,N \\\\\n        0 & otherwise\n    \\end{cases}\n    $$\n\n- 高斯核(正态核)\n    高斯核是将窗放大到整个空间，各个观测样本$x_i$对待观测点$x$的加权和(越远权值越小)。\n    $$\n    \\kappa(z)\n    = \\frac{1}{(2\\pi)^{N/2}|\\Sigma|^{1/2}}\n    \\exp \\left(-\\frac{1}{2} (z - \\mu)^T \\Sigma^{-1} (z - \\mu)\\right)\n    $$\n\n- 超球窗\n    $$\n    \\kappa(z)\n    = \\begin{cases}\n        V^{-1} & ||z|| \\leq 1 \\\\\n        0 & otherwise\n    \\end{cases}\n    $$\n\n    > $z=\\frac{x_i-x}{h}$，故$||z||\\leq 1$即$||x_i-x||^2\\leq h^2$\n    > 此时$h$表示超球体的半径\n\n## sklearn\n[sklearn.neighbors.KernelDensity — scikit-learn 0.19.0 documentation - ApacheCN](http://sklearn.apachecn.org/en/0.19.0/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity)\n```\n>>> from sklearn.neighbors import KernelDensity\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\n>>> kde.score_samples(X)\narray([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,\n       -0.41076071])\n>>> kde.sample(10)\narray([[ 1.80042291,  1.1030739 ],\n       [ 0.87299669,  1.0762352 ],\n       [-2.40180586, -1.19554374],\n       [-1.97985919, -1.19361193],\n       [-2.95866231, -2.1972637 ],\n       [-1.12739556, -0.80851063],\n       [ 1.03756706,  1.24855099],\n       [ 1.21729703,  1.02345815],\n       [-2.11816867, -1.0486257 ],\n       [-1.04875537, -0.89928711]])\n```\n\n## 代码\n具体代码见\n[@Github: Non-parametric Estmation](https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py)\n\n定义核函数如下\n```\n# 高斯核\ngaussian = lambda z: np.exp(-0.5*(np.linalg.norm(z)**2)) / np.sqrt(2*np.pi)\n# 均匀核\nsquare = lambda z: 1 if (np.linalg.norm(z) <= 0.5) else 0\n```\n\n密度估计函数如下，需要对连续范围内的各个点，即$x \\in [min(X), max(X)]$进行估计获得`p`，作图显示$x-p$即可\n```\ndef parzenEstimate(X, kernel, h, n_num=50):\n    \"\"\" 核参数估计\n    Args:\n        X: {ndarray(n_samples,)}\n        kernel: {function} 可调用的核函数\n        h: {float} 核函数的参数\n    Returns:\n        p: {ndarray(n_num,)}\n    Notes:\n        - 一维，故`V_i = h`\n        - p(x) = \\frac{1}{M} \\sum_{i=1}^M \\kappa \\left( \\frac{x_i - x}{h} \\right)\n    \"\"\"\n    x = np.linspace(np.min(X), np.max(X), num=n_num)\n    p = np.zeros(shape=(x.shape[0],))\n    z = lambda x, x_i, h: (x - x_i) / h\n    V_i = h; n_samples = X.shape[0]\n    for idx in range(x.shape[0]):\n        for i in range(X.shape[0]):\n            p[idx] += kernel(z(x[idx], X[i], h)) / V_i\n        p[idx] /= n_samples\n    return p\n```\n\n### 均匀核\n- $h=0.5$\n    ![01_h_0.5](/Non-parameter-Estimation/01_h_0.5.png)\n\n- $h=0.8$\n    ![01_h_0.8](/Non-parameter-Estimation/01_h_0.8.png)\n\n- $h=1.0$\n    ![01_h_1.0](/Non-parameter-Estimation/01_h_1.0.png)\n\n- $h=2.0$\n    ![01_h_2.0](/Non-parameter-Estimation/01_h_2.0.png)\n\n\n\n### 高斯核\n- $h=0.5$\n    ![gaussian_h_0.5](/Non-parameter-Estimation/gaussian_h_0.5.png)\n\n- $h=0.8$\n    ![gaussian_h_0.8](/Non-parameter-Estimation/gaussian_h_0.8.png)\n\n- $h=1.0$\n    ![gaussian_h_1.0](/Non-parameter-Estimation/gaussian_h_1.0.png)\n\n- $h=2.0$\n    ![gaussian_h_2.0](/Non-parameter-Estimation/gaussian_h_2.0.png)\n","categories":["Machine Learning"]},{"title":"Parameter Estimation","url":"/2018/11/19/Parameter-Estimation/","content":"\n[贝叶斯学派与频率学派有何不同？ - 任坤的回答 - 知乎](https://www.zhihu.com/question/20587681/answer/17435552)\n\n# 引言\n参数估计`(parameter estimation)`，统计推断的一种。根据从总体中抽取的随机样书．来估计总体分布中未知参数的过程。主要介绍最大似然估计`(MLE: Maximum Likelihood Estimation)`，最大后验概率估计`(MAP: Maximum A Posteriori Estimation)`，贝叶斯估计`(Bayesian Estimation)`。\n\n> 解释一下“似然函数”和“后验概率”，在[贝叶斯决策](https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/)一节，给出定义如下\n> $$ P(c_k|x)=\\frac{p(x|c_k)P(c_k)}{p(x)} $$\n> \n>上式中$ k=1,...,K  $，各部分定义如下\n> $P(c_k|x)$——`后验概率(posteriori probability)`\n> $P(c_k)$——`先验概率(priori probability)`\n> $p(x|c_k)$——$c_k$关于$x$的`似然函数(likelihood)`\n> $p(x)$——`证据因子(evidence)`\n\n\n# 引例\n以最经典的掷硬币实验为例，假设有一枚硬币，投掷一次出现正面记$\"1\"$，投掷$10$次的实验结果如下\n$$\n\\{ 0， 1， 1， 1， 1， 0， 1， 1， 1，0 \\}\n$$\n\n记硬币投掷结果为随机变量$X$，且$ x \\in \\{0, 1\\}$，硬币投掷一次服从二项分布，估计二项分布的参数$\\theta$\n\n# 最大似然估计(MLE)\n\n## 似然函数\n> [Likelihood function - Wikipedia](https://en.wikipedia.org/wiki/Likelihood_function#Definition)\n\n- 离散型\n  $$\n  L(x | \\theta) = p_{\\theta}(x)=P_{\\theta}(X = x)\n  $$\n\n- 连续型\n  $$\n  L(x | \\theta) = f_{\\theta}(x)\n  $$\n\n> 很多人能讲出一大堆哲学理论来阐明这一对区别。\n> 但我觉得，从工程师角度来讲，这样理解就够了:\n> 频率 $vs$ 贝叶斯 = $P(X; w)$ $vs$ $P(X|w)$ 或 $P(X,w)$\n> \n> 作者：许铁-巡洋舰科技\n> 链接：https://www.zhihu.com/question/20587681/answer/122348889\n> 来源：知乎\n> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\n## 模型\n有数据集$D = \\{x_1, x_2, ..., x_N\\}$，按$c$个类别分成$\\{D_1, D_2, ..., D_C\\}$，各个类别服从的概率分布密度函数模型已给出，估计参数$\\hat{\\Theta} = \\{\\hat{\\theta}_{c_1}, \\hat{\\theta}_{c_2}, ..., \\hat{\\theta}_{c_C}\\} $\n\n**假定**\n- 类别间独立，且各自服从概率分布密度函数为$p(x|c_j)$\n- 各类别的概率密度$p(x|c_j)$以参数$\\theta_{c_j}$确定，即$p(x|c_j; \\theta_{c_j})$\n\n故似然函数为\n$$\nL(D | \\Theta) = P(x_1, x_2, ..., x_N | \\Theta) = \\prod_{i=1}^N p(x_i | \\theta_{x_i \\in c_j})\n$$\n\n> 理解为，在参数$\\Theta$为何值的条件下，实验结果出现数据集$D$的概率最大\n\n求取其极大值对应的参数即可\n\n- 一般取对数似然函数\n$$\\log L(D | \\Theta) = \\sum_{i=1}^N \\log p(x_i | \\theta_{x_i \\in c_j}) $$\n- 极大值即对应梯度为$\\vec{0}$的位置，即\n$$ \n∇_\\Theta  \\log L(D | \\Theta) = \\vec{0}\n\\Rightarrow\n\\hat{\\Theta}\n$$\n\n> Some comments about ML\n> - ML estimation is usually simpler than alternative methods. \n> - Has good convergence properties as the number of training samples increases. \n> - If the model chosen for p(x|θ) is correct, and independence assumptions among variables are true, ML will give very good results.\n> - If the model is wrong, ML will give poor results.\n> <div style=\"text-align: right\"> —— Zhao Haitao. Maximum Likelihood and Bayes Estimation </div>\n\n## 例：正态分布的最大似然估计\n数据集(单类别)服从高斯分布$N(\\mu, \\sigma^2)$时的的最大似然估计\n$$\nP(x_i | \\mu, \\sigma^2) \n= \\frac{1}{\\sqrt{2\\pi} \\sigma } \ne^ {-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}\n$$\n\n$$\nL(D | \\mu, \\sigma^2) \n= \\prod_{i=1}^N \n\\frac{1}{\\sqrt{2\\pi} \\sigma } \ne^ {-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}\n=\\left( \\frac{1}{\\sqrt{2\\pi} \\sigma } \\right)^N \n\\prod_{i=1}^N \ne^ {-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}\n$$\n\n取对数似然\n$$\n\\log L(D | \\mu, \\sigma^2) \n= - \\frac{N}{2} \\log(2\\pi \\sigma^2) - \n\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2\n$$\n\n### 1. 参数$\\mu$的估计\n$$\n\\frac{∂}{∂\\mu} \nL(D | \\mu, \\sigma^2) \n= \\frac{1}{\\sigma^2} (\\sum_{i=1}^N x_i - N\\mu)\n= 0\n$$\n\n$$\n\\Rightarrow\n\\hat{\\mu}\n= \\frac{1}{N}\n\\sum_{i=1}^N x_i\n$$\n\n### 2. 参数$\\sigma^2$的估计\n$$\n\\frac{∂}{∂\\sigma^2} \n\\log L(D | \\mu, \\sigma^2) \n= - \\frac{N}{2\\sigma^2} +\n\\frac{1}{2\\sigma^4} \\sum_{i=1}^N (x_i - \\mu)^2\n= 0\n$$\n\n$$\n\\Rightarrow\n\\hat{\\sigma^2} \n= \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2\n$$\n\n> 参数$\\hat{\\mu}, \\hat{\\sigma}^2$的值与样本均值和样本方差相等\n\n# 最大后验概率估计(MAP)\n<!-- > [高斯混合模型(GMM)与最大期望算法(EM)]() -->\n\n## 模型\n最大似然估计是求参数$\\theta$, 使似然函数$P(D | \\theta)$最大，最大后验概率估计则是求$\\theta$使$P(\\theta | D)$最大\n> 理解为，在已出现的实验样本$D$上，参数$\\theta$取何值的概率最大\n\n且注意到\n$$\nP(\\theta | D) = \\frac{P(D | \\theta)P(\\theta)}{P(D)}\n$$\n\n故$MAP$不仅仅使似然函数$P(D | \\theta)$最大，而且使$P(\\theta)$最大，即\n$$\n\\theta = argmax L(\\theta | D)\n$$\n\n$$\nL(\\theta | D) = P(\\theta) P(D | \\theta)\n= P(\\theta) \\prod_{i=1}^N p(x_i | \\theta)\n$$\n\n> 比$ML$多了一项$P(\\theta)$\n\n- 取对数后\n$$\n\\log L(\\theta | D) = \\sum_{i=1}^N \\log p(x_i | \\theta) + \\log P(\\theta)\n$$\n\n- 求取极大值\n$$\n∇_\\theta L(\\theta | D) = 0\n\\Rightarrow\n\\hat{\\theta}\n$$\n\n> $MAP$和$MLE$的区别：\n> $MAP$允许我们把先验知识加入到估计模型中，这在**样本很少**的时候是很有用的，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如`beta`分布的$\\alpha, \\beta$，我们还可以调节把估计的结果“拉”向先验的幅度，$\\alpha, \\beta$越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。\n> [极大似然估计，最大后验概率估计(MAP)，贝叶斯估计 - 李鑫o_O - CSDN博客](https://blog.csdn.net/hustlx/article/details/51144710)\n\n## 例：正态分布的最大后验概率估计\n数据集(单类别)服从高斯分布$N(\\mu, \\sigma^2)$时的最大后验概率估计\n$$\np(x_i | \\mu, \\sigma^2) \n= \\frac{1}{\\sqrt{2\\pi} \\sigma } \ne^ {-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}\n$$\n\n> $$\n\\log p(x_i | \\mu, \\sigma^2)\n= - \\frac{1}{2} \\log(2\\pi \\sigma^2) - \n\\frac{1}{2\\sigma^2} (x_i - \\mu)^2\n$$\n\n### 1. 参数$\\mu$的估计\n给定先验条件：$\\mu$服从正态分布$N(\\mu_0, \\sigma_{\\mu_0}^2)$，即\n$$\np(\\mu) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{\\mu_0}} \ne^ {-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_{\\mu_0}^2}}\n$$\n\n> $$\n\\log p(\\mu)\n= - \\frac{1}{2} \\log(2\\pi \\sigma_{\\mu_0}^2) - \n\\frac{1}{2\\sigma_{\\mu_0}^2} (\\mu - \\mu_0)^2\n$$\n\n则\n$$\n\\log L(\\mu, \\sigma^2 | D)\n= - \\frac{N}{2} \\log(2\\pi \\sigma^2) - \n\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2 - \n\\frac{1}{2} \\log(2\\pi \\sigma_{\\mu_0}^2) - \n\\frac{1}{2\\sigma_{\\mu_0}^2} (\\mu - \\mu_0)^2\n$$\n\n则\n$$\n\\frac{∂}{∂\\mu} \\log L(\\mu, \\sigma^2 | D)\n= \\frac{1}{\\sigma^2} \\sum_{i=0}^N (x_i - \\mu) - \\frac{1}{\\sigma_{\\mu_0}^2} (\\mu - \\mu_0)\n= 0\n$$\n\n$$\n\\Rightarrow\n\\hat{\\mu}\n= \\frac{\\mu_0 \\sigma^2 + \\sigma_{\\mu_0}^2 \\sum_{i=0}^N x_i}\n{\\sigma^2 + N \\sigma_{\\mu_0}^2}\n= \\frac{\\mu_0 + \\frac{\\sigma_{\\mu_0}^2}{\\sigma^2} \\sum_{i=0}^N x_i}\n{1 + \\frac{\\sigma_{\\mu_0}^2}{\\sigma^2} N }\n$$\n\n### 2. 参数$\\sigma^2$的估计\n给定先验条件：$\\sigma^2$服从正态分布$N(\\sigma_0^2, \\sigma_{\\sigma_0^2}^2)$，即\n$$\np(\\sigma^2) = \\frac{1}{\\sqrt{2\\pi} \\sigma_{\\sigma_0^2}} e^ {-\\frac{(\\sigma^2- \\sigma_0^2)^2}{2 \\sigma_{\\sigma_0^2} ^2}}\n$$\n\n> $$\n\\log p(\\sigma^2) \n= - \\frac{1}{2} \\log(2\\pi \\sigma_{\\sigma_0}^2) - \n\\frac{1}{2\\sigma_{\\sigma_0}^2} (\\sigma - \\sigma_0)^2\n$$\n\n则\n$$\n\\log L(\\mu, \\sigma^2 | D)\n= - \\frac{N}{2} \\log(2\\pi \\sigma^2) - \n\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2 - \\frac{1}{2} \\log(2\\pi \\sigma_{\\sigma_0}^2) - \n\\frac{1}{2\\sigma_{\\sigma_0}^2} (\\sigma - \\sigma_0)^2\n$$\n\n则\n$$\n\\frac{∂}{∂\\sigma^2} \\log L(\\mu, \\sigma^2 | D)\n= - \\frac{N}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^N (x_i - \\mu)^2 - \n\\frac{1}{2\\sigma_{\\sigma_0}^2} \n\\frac{\\sigma - \\sigma_0}{\\sigma}\n\\Rightarrow\n\\hat{\\sigma^2}(略)\n$$\n\n> $$\n\\frac{∂}{∂\\sigma^2}(\\sigma - \\sigma_0)^2\n= 2(\\sigma - \\sigma_0)\n\\frac{∂}{∂\\sigma^2} (\\sigma - \\sigma_0)\n= \\frac{\\sigma - \\sigma_0}{\\sigma}\n$$\n\n# 贝叶斯估计\n\n## 模型\n<!-- \n贝叶斯公式：\n$$\nP(c_i|x) = \\frac{p(x|c_i)p(c_i)}{\\sum_j p(x|c_j)p(c_j)}\n$$\n\n在给定数据集$D=\\{x_1, x_2, ..., x_N\\}$的情况下，可从数据中估计先验概率和似然函数，即\n$$\nP(c_i|x, D) = \\frac{p(x|c_i, D)p(c_i, D)}{\\sum_j p(x|c_j, D)p(c_j, D)}\n$$\n\n**假定**\n- 先验概率密度函数为$p(c_i)$已知\n- 抽样结果几乎与真实分布一致，即$ p(c_i, D) \\approx p(c_i) $\n\n则\n$$\nP(c_i|x, D) = \\frac{p(x|c_i, D)p(c_i)}{\\sum_j p(x|c_j, D)p(c_j)}\n$$\n\n只需从样本中，估计每个类别的似然函数$p(x|c_i, D)$即可\n\n>-----------------------------------------------\n\n现考虑**单个类别**中抽取的数据集$D$，如何估计该类别的似然函数$p(x | \\theta)$参数$\\theta$呢？\n\n若概率密度函数为$p(x | \\theta)$，记从数据集中估计得到的似然函数为$p(x | D)$，有\n$$\np(x | \\theta) \\approx p(x | D)\n$$\n\n> $p(x | D)$ would be the estimate of $p(x | \\theta)$ given $D$\n\n且\n$$\np(x | D) \n= \\int p(x, \\theta | D) d \\theta\n= \\int p(x | \\theta, D) p(\\theta | D) d \\theta\n$$\n\n> Links $p(x | D)$ with $p(θ | D)$\n\n其中\n- $\np(x | \\theta, D) \\approx p(x | \\theta) \n$\n\n- $\np(\\theta | D) \n= \\frac {P(D | \\theta)p(\\theta)} {P(D)}\n= \\frac{p(\\theta) \\prod_{i=1}^N p(x_i | \\theta)}{P(D)}\n$\n\n故\n$$\np(x | D) \n= \\int p(x | \\theta) p(\\theta | D) d \\theta\n$$\n\n总的来说 \n-->\n\n$$\np(\\theta | D) \n= \\frac {P(D | \\theta)p(\\theta)} {P(D)}\n= a · p(\\theta) \\prod_{i=1}^N p(x_i | \\theta)\n$$\n\n其中$a$是使\n$$\n\\int p(\\theta | D)  = 1\n$$\n\n利用“质心公式”求解贝叶斯的点估计\n$$\nθ_{Bayes} = \\int θ·p(θ|D) d θ\n$$\n\n## 例：正态分布的贝叶斯估计\n数据集(单类别)服从高斯分布$N(\\mu, \\sigma^2)$时的贝叶斯估计\n$$\np(x_i | \\mu, \\sigma^2) \n= \\frac{1}{\\sqrt{2\\pi} \\sigma } \ne^ {-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}\n$$\n\n### 参数$\\mu$的估计\n给定先验条件：$\\mu$服从正态分布$N(\\mu_0, \\sigma_{\\mu_0}^2)$，即\n$$\np(\\mu) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{\\mu_0}} \ne^ {-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_{\\mu_0}^2}}\n$$\n\n则\n$$\nP(\\mu | D) \n= a · p(\\mu) \\prod_{i=1}^N p(x_i | \\mu)\n= a · \n\\frac{1}{\\sqrt{2\\pi}\\sigma_{\\mu_0}} \ne^ {-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_{\\mu_0}^2}}\n\\prod_{i=1}^N \n\\frac{1}{\\sqrt{2\\pi} \\sigma } \ne^ {-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}\n$$\n\n$$\n= a · \n\\left( \\frac{1}{\\sqrt{2\\pi}} \\right)^{N + 1}\n\\frac{1}{\\sigma_{\\mu_0} \\sigma^N}\ne^ {\n-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_{\\mu_0}^2} \n-\\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}\n}\n$$\n\n易证\n> <p align=\"right\">我已经想到了一个绝妙的证明,但是这台电脑的硬盘太小了,写不下。</p>\n<!-- > <p align=\"right\">诶嘿，显然成立，2333333</p> -->\n\n\n$$\np(\\mu | D) = \\frac{1}{\\sqrt{2\\pi}\\sigma_N} \ne^ {-\\frac{(\\mu - \\mu_N)^2}{2\\sigma_N^2}}\n$$\n\n其中\n$$\n\\mu_N = \n\\frac{N \\sigma_0^2}\n{N \\sigma_0^2 + \\sigma^2}\n\\frac{1}{N} \\sum_{i=1}^N x_i\n+\\frac{\\sigma^2}{N \\sigma_0^2 + \\sigma^2}\n\\mu_0\n$$\n\n$$\n\\sigma_N^2 = \n\\frac{\\sigma_0^2 \\sigma^2}\n{N \\sigma_0^2 + \\sigma^2}\n$$\n\n> **与$MLE$，$MAP$的区别**\n> \n> - 相比较$MLE$与$MAP$的点估计，贝叶斯估计得到的结果是参数$\\theta$的密度函数$p(\\theta | D)$\n> - 最大后验概率估计为求取对应最大后验概率的点\n> $$ \\theta = argmax_\\theta p(\\theta | D) $$\n> \n> - 贝叶斯估计为求取整个取值范围的概率密度$p(\\theta | D)$，既然如此，必有\n> $$\\int p(\\theta | D) d\\theta = 1$$\n\n\n\n> [统计学习方法学习笔记（一）--极大似然估计与贝叶斯估计原理及区别 - YJ-20 - 博客园](https://www.cnblogs.com/zjh225901/p/7495505.html)\n> $$\n> p(\\theta | D) \n> = \\frac\n> {p(\\theta) \\prod_{i=1}^N p(x_i | \\theta)}\n> {\\int_\\theta p(\\theta) \\prod_{i=1}^N p(x_i | \\theta) d\\theta}\n> $$\n> \n> 由于$\\theta$是满足一定概率分布的变量，所以在计算的时候需要将考虑所有$\\theta$取值的情况，在计算过程中不可避免地高复杂度。所以计算时并不把所有地后验概率$p(\\theta | D)$都找出来，而是采用类似于极大似然估计地思想，来极大化后验概率，得到这种有效的叫做$MAP$\n\n\n# 引例的求解\n已知硬币投掷结果服从$Bernoulli$分布\n<table>\n  <tr>\n    <th>X</th>\n    <th>0</th>\n    <th>1</th>\n  </tr>\n  <tr>\n    <td>P</td>\n    <td>1-θ</td>\n    <td>θ</td>\n  </tr>\n</table>\n\n或者\n$$\nP(X_i) = \\theta ^{X_i} (1 - \\theta) ^{1 - X_i}\n$$\n\n## 最大似然估计\n实验结果中正面出现$7$次，反面出现$3$次，似然函数为\n$$\nL(\\theta) \n= \\prod_{i=1}^{10} \\theta ^{X_i} (1 - \\theta) ^{1 - X_i} \n= \\theta ^7 (1 - \\theta) ^3\n$$\n\n取对数似然函数并求极大值\n$$\n\\log L(\\theta) = 7 \\log \\theta + 3 \\log (1 - \\theta)\n$$\n\n令\n$$\n\\frac{∂}{∂ \\theta} \\log L(\\theta)\n= \\frac{7}{\\theta} - \\frac{3}{1-\\theta} = 0\n$$\n\n解得\n$$\n\\theta = 0.7\n$$\n\n即硬币服从$B(1, 0.7)$的概率分布\n> 做出$L(\\theta)$图像验证，如下\n> ![最大似然估计](/Parameter-Estimation/最大似然估计.png)\n\n\n## 最大后验概率估计\n给定先验条件\n$$\n\\theta \\thicksim N(\\theta_0, \\sigma_{\\theta_0}^2)\n$$\n\n则最大化\n$$\nL(\\theta | D) = \\theta ^7 (1 - \\theta) ^3 · \\frac{1}{\\sqrt{2\\pi}\\sigma_{\\theta_0}} e^ {-\\frac{(\\theta - \\theta_0)^2}{2\\sigma_{\\theta_0}^2}}\n$$\n\n取对数\n$$\n\\log L(\\theta | D)\n= 7 \\log \\theta + 3 \\log (1 - \\theta) - \n\\frac{1}{2} \\log(2\\pi \\sigma_{\\theta_0}^2) - \n\\frac{1}{2\\sigma_{\\theta_0}^2} (\\theta - \\theta_0)^2\n$$\n\n求取极大值点\n$$\n\\frac{∂}{∂\\theta} \\log L(\\theta | D) \n= \\frac {7}{\\theta} - \\frac{3}{1-\\theta} - \n\\frac{\\theta - \\theta_0}{\\sigma_{\\theta_0}^2} = 0\n$$\n\n得到\n$$\n\\theta^3 - (\\theta_0 + 1) \\theta^2 + (\\theta_0 - 10\\sigma_{\\theta_0}^2) \\theta + 7\\sigma_{\\theta_0}^2 = 0\n$$\n\n以下为选取不同先验条件时的$L(\\theta | D)$图像，用于对比\n> - 第一张图为极大似然估计$L(D|\\theta)$\n> - 第二张图为先验概率密度函数$P(\\theta)$\n> - 第三张图为最大后验概率估计$L(\\theta | D)$，$\\hat{\\theta}$由查表法求解\n> 代码见[仓库](https://github.com/isLouisHsu/isLouisHsu.github.io/tree/Hexo/source/_posts//参数估计的几种方法/temp.py)\n\n- $\\theta_0 = 0.3, \\sigma_{\\theta_0} = 0.1$ $\\Rightarrow$ $\\hat{\\theta} = 0.42$\n![对比图](/Parameter-Estimation/MAP_theta0_0.3&#32;sigma0_0.1.png)\n\n- $\\theta_0 = 0.5, \\sigma_{\\theta_0} = 0.1$ $\\Rightarrow$ $\\hat{\\theta} = 0.56$\n![对比图](/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.1.png)\n\n- $\\theta_0 = 0.7, \\sigma_{\\theta_0} = 0.1$ $\\Rightarrow$ $\\hat{\\theta} = 0.70$\n![对比图](/Parameter-Estimation/MAP_theta0_0.7&#32;sigma0_0.1.png)\n\n- $\\theta_0 = 0.5, \\sigma_{\\theta_0} = 0.01$ $\\Rightarrow$ $\\hat{\\theta} = 0.50$\n![对比图](/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.01.png)\n\n- $\\theta_0 = 0.5, \\sigma_{\\theta_0} = 1.0$ $\\Rightarrow$ $\\hat{\\theta} = 0.70$\n![对比图](/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_1.0.png)\n\n> 结论\n> - 由图$1, 2, 3$，可以看到当$\\theta_0$偏移$0.7$时，$MAP$结果也相应偏移；\n> - 由图$2, 4, 5$，可以看到当$\\sigma_{\\theta_0}^2$越小，即越确定先验概率分布时，$MAP$结果也越趋向于先验概率分布。\n\n\n## 贝叶斯估计\n先验条件为正态分布\n$$\n\\theta \\thicksim N(\\theta_0, \\sigma_{\\theta_0}^2)\n$$\n\n$$\np(\\theta | D)\n= a · p(\\theta) \\prod_{i=1}^N p(x_i | \\theta)\n= a \n· \\frac{1}{\\sqrt{2\\pi}\\sigma_{\\theta_0}} e^ {-\\frac{(\\theta - \\theta_0)^2}{2\\sigma_{\\theta_0}^2}} \n· \\theta ^7 (1 - \\theta) ^3 \n$$\n\n> 参数$a$使用`scipy.integrate.quad`求解\n\n选取不同先验条件时的$L(\\theta | D)$图像，用于对比\n\n- $\\theta_0 = 0.3, \\sigma_{\\theta_0} = 0.1$ \n![对比图](/Parameter-Estimation/BE_theta0_0.3&#32;sigma0_0.1.png)\n\n- $\\theta_0 = 0.5, \\sigma_{\\theta_0} = 0.1$ \n![对比图](/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.1.png)\n\n- $\\theta_0 = 0.7, \\sigma_{\\theta_0} = 0.1$ \n![对比图](/Parameter-Estimation/BE_theta0_0.7&#32;sigma0_0.1.png)\n\n- $\\theta_0 = 0.5, \\sigma_{\\theta_0} = 0.01$\n![对比图](/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.01.png)\n\n- $\\theta_0 = 0.5, \\sigma_{\\theta_0} = 1.0$ \n![对比图](/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_1.0.png)\n","categories":["Machine Learning"]},{"title":"Clustering","url":"/2018/11/16/Clustering/","content":"\n# 前言\n这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。\n\n# 基础知识\n## 距离度量方法\n机器学习中距离度量方法有很多，以下简单介绍几种。\n> [机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客](https://blog.csdn.net/taotiezhengfeng/article/details/80492128)\n> [算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离......） - 啊哦123的博客 - CSDN博客 ](https://blog.csdn.net/u014782458/article/details/58180885)\n\n\n定义两个$n$维向量\n$$\nx = [x_1, x_2, ..., x_n]^T\n$$\n\n$$\ny = [y_1, y_2, ..., y_n]^T\n$$\n\n- 曼哈顿距离`(Manhattan Distance)`\n    $$\n    d = || x - y ||_1 = \\sum_i |x_i - y_i|\n    $$\n\n- 欧氏距离`(Euclidean Distance)`\n    $$\n    d = || x - y ||_2 = \\sqrt{\\sum_i (x_i - y_i)^2}\n    $$\n\n- 闽可夫斯基距离`(Minkowski Distance)`\n    $$\n    d = || x - y ||_p = \\left(\\sum_i | x_i - y_i |^{p} \\right)^{\\frac{1}{p}}\n    $$\n\n    当$p$取$1$时为曼哈顿距离，取$2$时为欧式距离。\n\n- 余弦距离`(Cosine)`\n    $$\n    d = \\frac{x^T y}{||x||_2 ||y||_2} = \\frac{\\sum_i x_i y_i}{\\sqrt{\\sum_i x_i^2} \\sqrt{\\sum_i y_i^2}}\n    $$\n    \n    > 突然想到为什么向量的夹角余弦是怎么来的，高中学习一直背的公式，现在给一下证明。\n    > 证明：向量的夹角公式\n    > ![cosine_distance](/Clustering/cosine_distance.png)\n    > \n    > 从余弦定理(余弦定理用几何即可)出发，有\n    > $$\n    > \\cos \\theta = \\frac{a^2+b^2-c^2}{2ab}\n    > $$\n    > \n    > 其中\n    > $$\n    > ||\\vec{a}|| = \\sqrt{x_1^2 + y_1^2}\n    > $$\n    >\n    > $$\n    > ||\\vec{b}|| = \\sqrt{x_2^2 + y_2^2}\n    > $$\n    > \n    > $$\n    > ||\\vec{c}|| = \\sqrt{(x_1 - x_2)^2 + (x_2 - y_2)^2}\n    > $$\n    > \n    > 故\n    > $$\n    > \\cos \\theta = \\frac\n    {(\\sqrt{x_1^2 + y_1^2})^2 + (\\sqrt{x_2^2 + y_2^2})^2 - (\\sqrt{(x_1 - x_2)^2 + (x_2 - y_2))^2}}\n    {2 \\sqrt{x_1^2 + y_1^2} \\sqrt{x_2^2 + y_2^2}}\n    > $$\n    > \n    > $$\n    > = \\frac\n    {x_1 x_2 + y_1 y_2}\n    {\\sqrt{x_1^2 + y_1^2} \\sqrt{x_2^2 + y_2^2}}\n    = \\frac{a^T b}{||a||·||b||}\n    > $$\n\n## hard vs. soft clustering\n- 硬聚类`(hard clustering)`\n    计算的是一个硬分配`(hard ssignment)`过程,即每个样本仅仅属于一个簇。\n- 软聚类`(soft clustering)`\n    分配过程是软的，即一个样本的分配结果是在所有簇上的一个分布，在软分配结果中，一个样本可能对多个簇都具有隶属度。\n\n## 聚类方法的分类\n- 划分方法\n    `K-means`，`K-medoids`，`GMM`等。\n- 层次方法\n    `AGNES`，`DIANA`，`BIRCH`，`CURE`和`CURE-NS`等。\n- 基于密度的方法\n    `DBSCAN`，`OPTICS`，`DENCLUE`等。\n- 其他\n    如`STING`等。\n\n# 常用聚类方法\n## K均值(K-means)\n是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一，通常用于寻找次优解，再通过其他算法(如`GMM`)寻找更优的聚类结果。\n\n### 原理\n给定$N$维数据集\n$$\nX = [x^{(1)}, x^{(2)}, ..., x^{(M)}]\n$$\n\n指定类别数$K$与初始中心点$\\mu^{(0)}$，将样本划分到中心点距离其最近的簇中，再根据本次划分更新各簇的中心$\\mu^{(t)}$，如此迭代直至得到最好的聚类结果。预测测试样本时，将其划分到中心点距其最近的簇，也可通过`KNN`等方法。\n\n一般使用欧式距离度量样本到各中心点的距离，也可选择余弦距离等，这也是`K-means`算法的关键\n$$\nD(x^{(i)}, \\mu_k) = || x^{(i)} - \\mu_k ||_2^2\n$$\n\n定义损失函数为\n$$\nJ(\\Omega) = \\sum_i \\sum_k r^{(i)}_k D(x^{(i)}, \\mu_k)\n$$\n\n其中\n$$\nr^{(i)}_k = \\begin{cases}\n    1 & x^{(i)} \\in C_k \\\\\n    0 & otherwise\n\\end{cases}\n$$\n\n或表示为\n$$\nr^{(i)} = [0, ..., 1_k, ..., 0]^T\n$$\n\n在迭代过程中，损失函数的值不断下降，优化目标为\n$$\n\\min J(\\Omega)\n$$\n\n\n\n### 计算步骤\n1. 随机选取$K$个中心点；\n2. 遍历所有数据，计算每个点到各中心点的距离；\n3. 将每个数据划分到最近的中心点中；\n4. 计算每个聚类的平均值，作为新的中心点；\n5. 重复步骤2-步骤4，直到这k个中线点不再变化(收敛)，或执行了足够多的迭代；\n\n`K-means`更新迭代过程如下图\n![kmeans_example](/Clustering/kmeans_example.gif)\n### 缺点与部分解决方法\n- 局部最优\n- 初值敏感\n    初始点的选择会影响`K-means`聚类的结果，即可能会陷入局部最优解，如下图\n    ![k_means_init](/Clustering/k_means_init.png)\n    可通过如下方法解决\n    - 多次选择初始点运行`K-means`算法，选择最优的作为输出结果；\n    - `K-means++`\n- 需要定义`mean`，对于标称型`(categorical)`数据不适用\n- 需要给定聚类簇数目$K$\n    这里给出一种选择簇数目的方法，选择多个$K$值进行聚类，计算代价函数，做成折线图后如下，可以看到在$K=3$处损失值的变化率出现较大变化，则可选择簇的数目为$3$。\n    ![k_means_choose_K](/Clustering/k_means_choose_K.png)\n- 噪声数据干扰大\n- 对于非凸集`(non-convex)`数据无能为力\n    谱聚类可解决非凸集数据的聚类问题。\n\n### 改进\n- `K-means++`\n    改进初始点选择方法，第$1$个中心点随机选择；之后的初始中心点根据前面选择的中心点决定，若已选取$n$个初始聚类中心$(0<n<K)$，选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。\n- `ISODATA`\n    思想：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别.\n- `Kernel K-means`\n    参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类。\n\n### 类似的算法\n与`K-means`类似的算法有很多，例如\n- `K-medoids`\n    `K-means`的取值范围可以是连续空间中的任意值，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。`K-medoids`的取值是数据样本范围中的样本，且可应用在非数值型数据样本上。\n- `k-medians`\n    $K$中值，选择中位数更新各簇的中心点。\n- `K-centers`\n    [混合类型数据的K-Centers聚类算法/The K-Centers Clustering Algorithm for Categorical and Mixe](https://www.bjdxs.com/xueshu/28151.html)\n\n### 代码\n[@Github: K-Means](https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-1-kmeans/KMeans.py)\n```\nclass KMeans():\n    def __init__(self, n_cluster, mode):\n        self.n_cluster = n_cluster  # 簇的个数\n        self.mode = mode            # 距离度量方式\n        self.centroids = None       # 簇的中心\n        self.loss = float('inf')    # 优化目标值\n        plt.ion()\n    def fit(self, X, max_iter=5, min_move=0.1, display=False):\n        def initializeCentroids():\n            '''\n            选择初始点\n            '''\n            centroid = np.zeros(shape=(self.n_cluster, X.shape[1])) # 保存选出的点\n            pointIdx = []                                           # 保存已选出的点的索引\n            for n in range(self.n_cluster):\n                idx = np.random.randint(0, X.shape[0])              # 随机选择一个点\n                while idx in pointIdx:                              # 若该点已选出，则丢弃重新选择\n                    idx = np.random.randint(0, X.shape[0])\n                pointIdx.append(idx)\n                centroid[n] = X[idx]\n            return centroid\n        def dist2Centroids(x, centroids, mode):\n            '''\n            返回向量x到k个中心点的距离值\n            '''\n            d = np.zeros(shape=(self.n_cluster,))\n            for n in range(self.n_cluster):\n                d[n] = mathFunc.distance(x, centroids[n], mode)\n            return d\n        def nearestInfo(centroids, mode):\n            '''\n            每个点最近的簇中心索引、距离\n            '''\n            ctIdx = -np.ones(shape=(X.shape[0],), dtype=np.int8)    # 每个点最近的簇中心索引，初始化为-1，可作为异常条件\n            ctDist = np.ones(shape=(X.shape[0],), dtype=np.float)   # 每个点到最近簇中心的距离\n            for i in range(X.shape[0]):\n                dists = dist2Centroids(X[i], centroids, mode)\n                if mode == 'Euclidean': ctIdx[i] = np.argmin(dists)\n                elif mode == 'Cosine':  ctIdx[i] = np.argmax(dists)\n                ctDist[i] = dists[ctIdx[i]]             # 保存最相似的距离度量，用于计算loss\n            return ctIdx, ctDist\n        def updateCentroids(ctIdx):\n            '''\n            更新簇中心\n            '''\n            centroids = np.zeros(shape=(self.n_cluster, X.shape[1]))\n            for n in range(self.n_cluster):\n                X_ = X[ctIdx == n]                      # 筛选出离簇中心Cn最近的样本点\n                centroids[n] = np.mean(X_, axis=0)      # 根据筛选出的样本点更新中心值\n            return centroids\n        def loss(dist):\n            return np.mean(dist**2)\n        # -----------------------------------------\n        loss_min = float('inf')                         # 最优分类时的损失值，最小\n        n_iter = 0     \n        while n_iter < max_iter:                        # 每次迭代选择不同的初始点\n            n_iter += 1; isDone = False                 # 表示本次迭代是否已收敛\n            centroids_tmp = initializeCentroids()       # 选择本次迭代的初始点\n            loss_last = float('inf')                    # 本次迭代中，中心点更新前的损失值\n            n_update = 0                                # 本次迭代的更新次数计数\n            while not isDone:\n                n_update += 1\n                ctIdx, ctDist = nearestInfo(centroids_tmp, mode=self.mode)\n                centroids_tmp = updateCentroids(ctIdx)  # 更新簇中心\n                # --- 可视化 ---\n                if (display==True) and (X.shape[1] == 2):\n                    plt.ion()\n                    plt.figure(n_iter); plt.cla()\n                    plt.scatter(X[:, 0], X[:, 1], c=ctIdx)\n                    plt.scatter(centroids_tmp[:, 0], centroids_tmp[:, 1], c='r')\n                    plt.pause(0.5)\n                # -------------\n                loss_now = loss(ctDist); moved = np.abs(loss_last - loss_now)\n                if moved < min_move:                    # 若移动过小，则本次迭代收敛\n                    isDone = True\n                    print('第%d次迭代结束，中心点更新%d次' % (n_iter, n_update))\n                else: loss_last = loss_now\n            if loss_now < loss_min:\n                self.centroids = centroids_tmp          # 保存损失最小的模型(最优)\n                loss_min = loss_now\n                # print('聚类结果已更新')\n        self.loss = loss_min\n        print('=========== 迭代结束 ===========')\n    def predict(self, X):\n        '''\n        各个样本的最近簇中心索引\n        '''\n        labels = -np.ones(shape=(X.shape[0],), dtype=np.int)    # 初始化为-1，可用作异常条件\n        for i in range(X.shape[0]):\n            dists_i = np.zeros(shape=(self.n_cluster,))         # 保存X[i]到中心点Cn的距离\n            for n in range(self.n_cluster):\n                dists_i[n] = mathFunc.distance(X[i], self.centroids[n], mode=self.mode)\n            if self.mode == 'Euclidean':\n                labels[i] = np.argmin(dists_i)\n            elif self.mode == 'Cosine':\n                labels[i] = np.argmax(dists_i)\n        return labels\n```\n\n簇数的选择代码如下\n```\ndef chooseBestK(X, start, stop, step=1, mode='Euclidean'):\n    Ks = np.arange(start, stop + 1, step, dtype=np.int) # 待选择的K\n    Losses = np.zeros(shape=Ks.shape)                   # 保存不同K值时的最小损失值\n    for k in range(1, Ks.shape[0] + 1):                 # 对于不同的K，训练模型，计算损失\n        print('K = %d', k)\n        estimator = KMeans(n_cluster=k, mode=mode)\n        estimator.fit(X, max_iter=10, min_move=0.01, display=False)\n        Losses[k - 1] = estimator.loss\n    plt.ioff()\n    plt.figure(); plt.xlabel('n_clusters'); plt.ylabel('loss')\n    plt.plot(Ks, Losses)                                # 做出loss-K曲线\n    plt.show()\n```\n\n## 均值漂移(Meanshift)\n本质是一个迭代的过程，能够在一组数据的密度分布中寻找到局部极值，比较稳定，而且是无参密度估计(不需要事先知道样本数据的概率密度分布函数，完全依靠对样本点的计算)，而且在采样充分的情况下，一定会收敛，即可以对服从任意分布的数据进行密度估计。\n\n### 原理\n有一个滑动窗口的思想，即利用当前中心点一定范围内(通常为球域)的点迭代更新中心点，重复移动窗口，直到满足收敛条件。简单的说，`Meanshift`就是沿着密度上升的方向寻找同属一个簇的数据点。\n\n定义点$x_0$的$\\epsilon$球域如下\n$$\nS_h(x_0) = \\{ x | (x - x_0)^T (x - x_0) \\leq \\epsilon \\}\n$$\n\n若有$n$个点$(x_1, ..., x_n)$落在中心点$ptCentroid$的邻域内，其分布如图\n![DBSCAN4](/Clustering/DBSCAN4.jpg)\n\n则偏移向量计算方式为\n$$\nvecShift = \\frac{1}{n} \\sum_{i=1}^n (x_i - ptCentroid)\n$$\n\n中心点更新公式为\n$$\nptCentroid := ptCentroid + vecShift\n$$\n\n> 展开后可发现，其更新公式即\n> $$\n> vecShift\n> = \\frac{1}{n} \\sum_{i=1}^n x_i - ptCentroid\n> $$\n> \n> $$\n> ptCentroid :=  \\frac{1}{n} \\sum_{i=1}^n x_i\n> $$\n\n\n![DBSCAN3](/Clustering/DBSCAN3.jpg)\n\n一个滑动窗口的动态更新过程如下图\n![meanshift_example1](/Clustering/meanshift_example1.gif)\n初始化多个滑动窗口进行`MeanShift`算法，其更新过程如下，其中每个黑点代表滑动窗口的质心，每个灰点代表一个数据点\n![meanshift_example2](Clustering/meanshift_example2.gif)\n\n### 高斯权重\n基本思想是，距离当前中心点近的向量对更新结果权重大，而远的权重小，可减小远点的干扰，如下图，$vecShift_2$为高斯权重下的偏移向量\n![DBSCAN5](/Clustering/DBSCAN5.jpg)\n\n其偏移向量计算方式为\n$$\nvecShift = \\frac{1}{n} \\sum_{i=1}^n w_i · (x_i - ptCentroid)\n$$\n\n$$\nw_i = \\frac{\\kappa(x_i - ptCentroid)}{\\sum_j \\kappa(x_j - ptCentroid)}\n$$\n\n其中\n$$\n\\kappa(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{||z||^2}{2\\sigma^2} \\right)\n$$\n\n中心点更新公式仍然为\n$$\nptCentroid := ptCentroid + vecShift\n$$\n\n> 展开也可得到\n> $$\n> ptCentroid := \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_j w_j}\n> $$\n\n### 计算步骤\n对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, ..., x^{(M)})$，指定邻域参数$\\epsilon_0$，终止条件参数$\\epsilon_1$，簇合并参数$\\epsilon_2$，并指定样本距离度量方式，目标为将其划分为$K$个簇。\n1. 初始化：\n   - 在样本集中随机选择$K_0(K_0 \\gg K)$个样本作为初始中心点，以邻域大小为$\\epsilon_0$建立滑动窗口；\n   - 各个样本初始化一个标记向量，用于记录被各类别访问的次数；\n2. 以单个滑动窗口分析，记其中心点为$ptCentroid$，找到滑动窗口内的所有点，记作集合$M$，认为这些点属于该滑动窗口所属的簇类别，同时，这些点被该簇访问的次数$+1$；\n3. 以$ptCentroid$为中心，计算其到集合$M$中各个元素的向量，以这些向量计算得到偏移向量$vecShift$；\n4. 更新中心点：$ptCentroid = ptCentroid + vecShift$，即滑动窗口沿着$vecShift$方向移动，距离为$||vecShift||$；\n5. 重复步骤$2-4$，直到$||vecShift||<\\epsilon_1$，保存当前中心点；\n6. 如果收敛时当前簇$ptCentroid$与其它已经存在的簇的中心的距离小于阈值$\\epsilon_2$，那么这两个簇合并。否则，把当前簇作为新的簇类，增加$1$类；\n7. 重复迭代直到所有的点都被标记访问；\n8. 根据每个样本被各簇的访问频率，取访问频率最大的那个簇类别作为当前点集的所属类。\n\n> 即不同类型的滑窗沿着密度上升的方向进行移动，对各样本点进行标记，最后将样本划分为标记最多的类别；当两类非常接近时，合并为一类。\n\n\n### 代码\n[@Github: MeanShift](https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p71_meanshift.py)\n\n先定义了窗格对象\n```\nclass SlidingWindow():\n    \"\"\"\n    Attributes:\n        centroid: {ndarray(n_features,)}\n        epsilon: {float} 滑动窗格大小，为半径的平方\n        sigma: {float} 高斯核函数的参数\n        label: {int} 该窗格的标记\n        X: {ndarray(n_samples, n_features)}\n        containIdx: {ndarray(n_contain,)} 窗格内包含点的索引\n    \"\"\"\n    def __init__(self, centroid, epsilon, sigma, label, X):\n        self.centroid = centroid\n        self.epsilon = epsilon\n        self.sigma = sigma\n        self.label = label\n        self.containIdx = self.updateContain(X)\n    def k(self, z):\n        \"\"\" 高斯核函数\n        Args:\n            z: {ndarray(n_features,)}\n        Notes:\n            - \\kappa(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{||z||^2}{2\\sigma^2} \\right)\n        \"\"\"\n        norm = np.linalg.norm(z)\n        return np.exp(- 0.5 * (norm / self.sigma)**2) / np.sqrt(2*np.pi)\n    def step(self, X):\n        \"\"\" 更新滑动窗格的中心点和所包含点\n        Returns: {float}\n        \"\"\"\n        dshift = self.shift(X)\n        self.containIdx = self.updateContain(X)\n        return dshift\n    def shift(self, X):\n        \"\"\" 移动窗格\n        Args:\n            vecShift: {ndarray(n_features,)}\n        Returns:\n            dshift: {float} 移动的距离\n        \"\"\"\n        (n_samples, n_features) = X.shape\n        n_contain = self.containIdx.shape[0]\n        contain_weighted_sum = np.zeros(shape=(n_features, ))\n        weight_sum = 0\n        # 按包含的点进行移动\n        for i_contain in range(n_contain):\n            vector = X[self.containIdx[i_contain]] - self.centroid\n            weight = self.k(vector)\n            contain_weighted_sum += weight*X[self.containIdx[i_contain]]\n            weight_sum += weight\n        centroid = contain_weighted_sum / weight_sum\t \n        # 计算移动的距离   \n        dshift = np.linalg.norm(self.centroid - centroid)\n        self.centroid = centroid\n        return dshift\n    def updateContain(self, X):\n        \"\"\" 更新窗格内的点索引\n        Args:\n            X: {ndarray(n_samples, n_features)}\n        Notes:\n            - 用欧式距离作为度量\n        \"\"\"\n        d = lambda x_i, x_j: np.linalg.norm(x_i - x_j)\n        n_samples = X.shape[0]\n        containIdx = np.array([], dtype='int')\n        for i_samples in range(n_samples):\n            if d(X[i_samples], self.centroid) < self.epsilon:\n                containIdx = np.r_[containIdx, i_samples]\n        return containIdx\n```\n\n聚类算法如下\n```\nclass MeanShift():\n    \"\"\"\n    Attributes:\n        n_clusters: {int} 划分簇的个数\n        n_windows: {int} 滑动窗格的个数\n        epsilon: {float} 滑动窗格的大小\n        sigma: {float} {float} 高斯核参数\n        thresh: {float} 若两个窗格中心距离小于thresh，则合并两类簇\n        min_move: {float} 终止条件\n        windows: {list[class SlidingWindow()]}\n    Note:\n        - 假设所有点均被窗格划过\n    \"\"\"\n    \n    def __init__(self, n_clusters, n_windows=-1, epsilon=0.5, sigma=2, thresh=1e-2, min_move=1e-3):\n        self.n_clusters = n_clusters\n        self.n_windows = 5*n_clusters if (n_windows == -1) else n_windows\n        self.epsilon = epsilon\n        self.sigma = sigma\n        self.thresh = thresh\n        self.min_move = min_move\n        self.windows = []\n        self.centroids = None\n    def fit(self, X):\n        (n_samples, n_features) = X.shape\n        # 创建窗格\n        for i_windows in range(self.n_windows):\n            idx = np.random.randint(n_samples)\n            window = SlidingWindow(X[idx], self.epsilon,\n                            self.sigma, i_windows, X)\n            # 将各窗格包含的点标记\n            n_contain = window.containIdx.shape[0]\n            self.windows.append(window)\n\n        dshift = float('inf')   # 初始化为无穷大\n        plt.figure(); plt.ion()\n        while dshift > self.min_move:\n            # ------ 做图显示 ------\n            plt.cla()\n            plt.scatter(X[:, 0], X[:, 1], c='b')\n            for i_windows in range(self.n_windows):\n                centroid = self.windows[i_windows].centroid\n                plt.scatter(centroid[0], centroid[1], c='r')\n            plt.pause(0.5)\n            # ---------------------\n            dshift = self.step(X)\n        plt.ioff()\n        \n        # 合并窗格\n        dists = np.zeros(shape=(self.n_windows, self.n_windows))\n        for i_windows in range(self.n_windows):\n            for j_windows in range(i_windows):\n                centroid_i = self.windows[i_windows].centroid\n                centroid_j = self.windows[j_windows].centroid\n                dists[i_windows, j_windows] = np.linalg.norm(centroid_i-centroid_j)\n                dists[j_windows, i_windows] = dists[i_windows, j_windows]\n        \n        # 获得距离相近索引\n        index = np.where(dists<self.thresh)\n        # 用于标记类别\n        winlabel = np.zeros(shape=(self.n_windows,), dtype='int')\n        label = 1; winlabel[0] = label\n        for i_windows in range(self.n_windows):\n            idx_row = index[0][i_windows]\n            idx_col = index[1][i_windows]\n            # 若其中一个点被标记，则将令一个点并入该类\n            if winlabel[idx_row]!=0:\n                winlabel[idx_col] = winlabel[idx_row]\n            elif winlabel[idx_col]!=0:\n                winlabel[idx_row] = winlabel[idx_col]\n            # 否则新创建类别\n            else:\n                label += 1\n                winlabel[idx_row] = label\n                winlabel[idx_col] = label\n        \n        # 将标签一样的窗格合并\n        labels = list(set(winlabel))                            # 去重后的标签\n        n_labels = len(labels)                                  # 标签种类数\n        self.centroids = np.zeros(shape=(n_labels, n_features)) # 记录最终聚类中心\n        for i_labels in range(n_labels):\n            cnt = 0\n            for i_windows in range(self.n_windows):\n                if winlabel[i_windows] == labels[i_labels]:\n                    self.centroids[i_labels] += self.windows[i_windows].centroid\n                    cnt += 1\n            self.centroids[i_labels] /= cnt\t                    # 取同类窗格中心点的均值\n        return self.centroids\n\n    def step(self, X):\n        \"\"\" update all sliding windows\n        Returns:\n            dshift: \\sum_i^{n_windows} dshift_{i}\n        \"\"\" \n        dshift = 0\n        for i_windows in range(self.n_windows):\n            dshift += self.windows[i_windows].step(X)\n            # label the points\n            n_contain = self.windows[i_windows].containIdx.shape[0]\n        return dshift\n    def predict(self, X):\n        \"\"\" 简单的用近邻的方法求\n        \"\"\"\n        (n_samples, n_features) = X.shape\n        dists = np.zeros(shape=(n_samples, self.n_clusters))\n        for i_samples in range(n_samples):\n            for i_clusters in range(self.n_clusters):\n                dists[i_samples, i_clusters] = np.linalg.norm(X[i_samples]-self.centroids[i_clusters])\n        return np.argmin(dists, axis=1)\n```\n\n## 谱聚类(Spectral Clustering)\n谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用，比起传统的`K-Means`算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多。\n### 原理\n> [谱聚类（spectral clustering）原理总结 - 刘建平Pinard - 博客园 ](https://www.cnblogs.com/pinard/p/6221564.html)\n\n\n#### 无向权重图\n我们用点的集合$V$和边的集合$E$描述一个图，即$G(V, E)$，其中$V$即数据集中的点\n$$\nV = [v_1, v_2, ..., v_n]\n$$\n\n而点$v_i, v_j$间连接权值$w_{ij}$组成邻接矩阵$W$，由于为无向图，故满足$w_{ij}=w_{ji}$\n$$\nW = \\left[\n    \\begin{matrix}\n        w_{11} & ... & w_{1n} \\\\\n        ... & ... & ... \\\\\n        w_{n1} & ... & w_{nn} \\\\\n    \\end{matrix}\n    \\right]\n$$\n\n对于图中的任意一个点$v_i$，定义其度$d_i$为\n$$\nd_i = \\sum_{j=1}^n w_{ij}\n$$\n\n则我们可以得到一个度矩阵$D=diag(d_1, ..., d_n)$\n$$\nD = \\left[\n        \\begin{matrix}\n            d_1 & & \\\\\n            & ... & \\\\\n             & & d_n\\\\\n        \\end{matrix}\n    \\right]\n$$\n\n除此之外，对于$V$中子集$V_{sub} \\subset V$，定义子集$V_{sub}$点的个数为\n$$\n|V_{sub}| := n_{sub}\n$$\n\n另外，定义该子集中点的度之和为\n$$\nvol(V_{sub}) = \\sum_{i \\in V_{sub}} d_i\n$$\n\n#### 相似矩阵\n上面讲到的邻接矩阵$W$可以指定权值，但对于数据量庞大的数据集，这显然不是一个$wise$的选择。我们可以用相似矩阵$S$来获得邻接矩阵$W$，基本思想是，距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高。\n\n构建邻接矩阵$W$的方法有三类：$\\epsilon$-邻近法，$K$邻近法和全连接法，定义距离\n$$\nd_{ij} = ||x^{(i)} - x^{(j)}||_2^2\n$$\n\n- $\\epsilon$-邻近法\n    设置距离阈值$\\epsilon$，用欧式距离度量两点的距离$d_{ij}$，然后通过下式确定邻接权值$w_{ij}$\n    $$\n    w_{ij} = \\begin{cases}\n        0 & d_{ij} > \\epsilon \\\\\n        \\epsilon & otherwise\n    \\end{cases}\n    $$\n\n    > 两点间的权重要不就是$\\epsilon$，要不就是0，距离远近度量很不精确，因此在实际应用中，我们很少使用$\\epsilon$-邻近法。\n\n- $K$邻近法\n    - 第一种\n        只要一个点在另一个点的$K$近邻中，则保留$d_{ij}$\n        $$\n        w_{ij} = \\begin{cases}\n            \\exp \\left( -\\frac{d_{ij}}{2\\sigma^2} \\right) & x^{(i)} \\in KNN(x^{(j)})　or　x^{(j)} \\in KNN(x^{(i)}) \\\\\n            0 & otherwise\n        \\end{cases}\n        $$\n\n    - 第二种\n        互为$K$近邻时保留$d_{ij}$\n        $$\n        w_{ij} = \\begin{cases}\n            \\exp \\left( -\\frac{d_{ij}}{2\\sigma^2} \\right) & x^{(i)} \\in KNN(x^{(j)})　and　x^{(j)} \\in KNN(x^{(i)}) \\\\\n            0 & otherwise\n        \\end{cases}\n        $$\n\n- 全连接法\n    可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和`Sigmoid`核函数。最常用的是高斯核函数`RBF`，此时相似矩阵和邻接矩阵相同\n    $$\n    w_{ij} = \\exp \\left( -\\frac{d_{ij}}{2\\sigma^2} \\right)\n    $$\n\n#### 拉普拉斯矩阵(Graph Laplacians)\n定义\n$$\nL = D - W\n$$\n\n正则化的拉普拉斯矩阵为\n$$\nL = D^{-1} (D - W)\n$$\n\n具有的性质如下\n1. $L^T = L$\n2. 其特征值均为实数，即$\\lambda_i \\in \\mathbb{R}$\n3. 正定性：$\\lambda_i \\geq 0$\n4. 对于任意向量$x$，都有\n   $$\n   x^T L x = \\frac{1}{2} \\sum_{i,j} w_{ij} (x_i - x_j)^2\n   $$\n   \n    > 证明：\n    > $$\n    > x^T L x = x^T D x - x^T W x = \\sum_i d_i > x_i^2 - \\sum_{ij} w_{ij} x_i x_j\n    > $$\n    > \n    > $$\n    > = \\frac{1}{2} \\left[ \\sum_i d_i x_i^2 - > 2\\sum_{ij} w_{ij} x_i x_j + \\sum_j d_j x_j^2 \\right]\n    > $$\n    >\n    > 其中$ d_i = \\sum_j w_{ij} $，所以\n    > \n    > $$\n    > x^T L x = \\frac{1}{2} \\sum_{i,j} w_{ij} (x_i - x_j)^2\n    > $$\n\n#### 无向图的切图\n##### cut\n我们希望把一张无向图$G(V, E)$按一定方法切成多个子图，各个子图间无连接，每个子图的点集为$V_1, ..., V_K$，满足\n- $\\bigcup_{k=1}^K V_k = V$\n- $V_i \\cap V_j = \\emptyset$\n\n定义两个子图点集合$A, B$之间的切图权重为\n$$\nW(A, B) = \\sum_{i \\in A, j \\in B} w_{ij}\n$$\n\n> 共有$n_A × n_B$个权值作累加\n\n那么对于$K$个子图点的集合$V_1, ..., V_K$，定义切图为\n$$\ncut(V_1, ..., V_K) = \\frac{1}{2} \\sum_{i=1}^K cut(V_i, \\overline{V_i})\n$$\n\n$$\ncut(V_i, \\overline{V_i}) = W (V_i, \\overline{V_i})\n$$\n\n其中$\\overline{V_i}$表示$V_i$的补集，或者\n$$\n\\overline{V_i} = \\bigcup_{k \\neq i} V_k\n$$\n\n通过最小化$cut(V_1, ..., V_K)$使子图内权重和大，而子图间权重和小。但是这种方法存在问题，如下图\n![cut](/Clustering/cut.jpg)\n\n选择一个权重最小的边缘的**点**，比如$C$和$H$之间进行$cut$，这样可以最小化$cut(V_1, ..., V_K)$，但是却不是最优的切图。\n\n为解决上述问题，需要对每个子图的规模做出限定，以下介绍两种切图方式。\n\n##### Ratio Cut\n不仅考虑最小化$cut(V_1, ..., V_K)$，而且最大化每个子图的点个数，即\n$$\nRatioCut(V_1, ..., V_K) = \\frac{1}{2} \\sum_k \\frac{cut(V_i, \\overline{V_i})}{|V_k|}\n$$\n\n$$\ncut(V_i, \\overline{V_i}) = W (V_i, \\overline{V_i})\n$$\n\n> - $W(V_k, \\overline{V_k}) = \\sum_{i \\in V_k, j \\in \\overline{V_k}} w_{ij}$\n> - $|V_k| = n_k$\n\n\n如果按照遍历的方法求解，由前面分析，$W(V_k, \\overline{V_k})$需计算$n_{V_k} × n_{\\overline{V_k}}$次累加，计算量庞大，那么如何求解呢？\n\n定义指示向量$h_k$，其构成矩阵$H$\n$$\nH = [ h_1, ..., h_k, ..., h_K]\n$$\n\n其中\n$$\nh_k = \\left[h_{k1}, h_{k2}, , ..., h_{kM} \\right]^T\n$$\n\n$$\nh_{ki} = \\begin{cases}\n    \\frac{1}{\\sqrt{|V_k|}} & x^{(i)}\\in V_k \\\\\n    0 & otherwise\n\\end{cases}\n$$\n\n> $h_k$为单位向量，且两两正交\n> $$\n> h_i^T h_j = \n>       \\begin{cases}\n>           \\sum_{|V_i|} \\frac{1}{|V_i|} = |V_i| · \\frac{1}{|V_i|} = 1 & i = j \\\\\n>           0 & i \\neq j\n>       \\end{cases}\n> $$\n\n\n那么由拉式矩阵性质$4$\n$$\nh_k^T L h_k = \\frac{1}{2} \\sum_{i,j} w_{ij} (h_{ki} - h_{kj})^2\n$$\n\n$$\n= \\frac{1}{2} \n    [\n        \\sum_{i \\in V_k, j \\in V_k} w_{ij} (h_{ki} - h_{kj})^2 + \n        \\sum_{i \\notin V_k, j \\in V_k} w_{ij} (h_{ki} - h_{kj})^2 + \n$$\n\n$$\n        \\sum_{i \\in V_k, j \\notin V_k} w_{ij} (h_{ki} - h_{kj})^2 + \n        \\sum_{i \\notin V_k, j \\notin V_k} w_{ij} (h_{ki} - h_{kj})^2\n    ]\n$$\n\n$$\n= \\frac{1}{2}\n    [\n        \\sum_{i \\in V_k, j \\in V_k} w_{ij} (\\frac{1}{\\sqrt{|V_k|}} - \\frac{1}{\\sqrt{|V_k|}})^2 + \n        \\sum_{i \\notin V_k, j \\in V_k} w_{ij} (0 - \\frac{1}{\\sqrt{|V_k|}})^2 +\n$$\n\n$$\n        \\sum_{i \\in V_k, j \\notin V_k} w_{ij} (\\frac{1}{\\sqrt{|V_k|}} - 0)^2 +\n        \\sum_{i \\notin V_k, j \\notin V_k} w_{ij} (0 - 0)^2\n    ]\n$$\n\n$$\n= \\frac{1}{2}\n    [\n        \\sum_{i \\notin V_k, j \\in V_k} w_{ij} \\frac{1}{|V_k|} +\n        \\sum_{i \\in V_k, j \\notin V_k} w_{ij} \\frac{1}{|V_k|}\n    ]\n$$\n\n> $$cut(V_i, \\overline{V_i}) = W (V_i, \\overline{V_i}) = \\sum_{i \\in V_k, j \\in \\overline{V_k}} w_{ij}$$\n\n\n$$\nh_k^T L h_k = \\frac{1}{2} [\\frac{1}{|V_k|} cut(V_k, \\overline{V_k}) + \\frac{1}{|V_k|} cut(V_k, \\overline{V_k})]\n$$\n\n$$\n= \\frac{1}{|V_k|} cut(V_k, \\overline{V_k})\n$$\n\n推到这里就能理解为什么要定义$h_k$了\n$$\nRatioCut(V_1, ..., V_K)\n= \\frac{1}{2} \\sum_k h_k^T L h_k\n$$\n\n并且\n$$\nh_k^T L h_k = tr(H^T L H)\n$$\n\n> $$\n> H^T L H \n> = \\left[\n>       \\begin{matrix}\n>           — & h_1^T & — \\\\\n>            & ... &  \\\\\n>           — & h_K^T & — \\\\\n>       \\end{matrix}\n> \\right]\n> L\n> \\left[  \n>         \\begin{matrix}\n>             | & & | \\\\\n>             h_1 & ... & h_K \\\\\n>             | & & |\n>         \\end{matrix}\n>     \\right]\n> $$\n> \n> $$\n> = \\left[\n>       \\begin{matrix}\n>           h_1^T L h_1 & ... & h_1^T L h_K \\\\\n>           ... & ... & ... \\\\\n>           h_K^T L h_K & ... & h_K^T L h_K \\\\\n>       \\end{matrix}\n> \\right]\n> $$\n\n\n所以最终优化目标为\n$$\n\\min_H tr(H^T L H)\n$$\n\n$$\ns.t.　H^T H = I\n$$\n\n> $$\n> H^T H = \\left[\n>       \\begin{matrix}\n>           h_1^T h_1 & ... & h_1^T h_K \\\\\n>           ... & ... & ... \\\\\n>           h_K^T h_K & ... & h_K^T h_K \\\\\n>       \\end{matrix}\n> \\right]\n> $$\n\n\n而矩阵的正交相似变换$A = P \\Lambda P^{-1}$满足\n$$\ntr(A) = tr(\\Lambda) = \\sum_i \\lambda_i\n$$\n\n故\n$$\ntr(H^T L H) = tr(L) = \\sum_{i=1}^M \\lambda_i\n$$\n\n$\\lambda_i$为矩阵$L$的特征值。\n\n我们再进行维度规约，将维度从$M$降到$k_1$，即找到$k_1$个最小的特征值之和。\n\n##### N Cut\n推导过程与`RatioCut`完全一致，只是将分母$|V_i|$换成$vol(V_i)$\n$$\nNCut(V_1, ..., V_K) = \\frac{1}{2} \\sum_k \\frac{cut(V_i, \\overline{V_i})}{vol(V_i)}\n$$\n\n$$\ncut(V_i, \\overline{V_i}) = W (V_i, \\overline{V_i})\n$$\n\n> $$vol(V_{sub}) = \\sum_{i \\in V_{sub}} d_i$$\n\n \n### 计算步骤\n对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, ..., x^{(M)})$，将其划分为$K$类$(C_1, ..., C_K)$\n1. 根据输入的相似矩阵的生成方式构建样本的相似矩阵$S_{M×M}$；\n2. 根据相似矩阵$S$构建邻接矩阵$W_{M×M}$；\n3. 构建度矩阵$D_{M×M}$；\n4. 计算拉普拉斯矩阵$L_{M×M}$，可进行规范化$ L := D^{-1}L $；\n5. 对$L$进行特征值分解`(EVD)`，得到特征对$ (\\lambda_i, \\alpha_i), i=1,...,M $；\n6. 指定超参数$K_1$，选取$K_1$个最小特征值对应的特征向量组成矩阵$F_{M×K_1}$，并将其按行标准化；\n7. 以$F$的行向量作为新的样本数($k_1$维，这里也有降维操作)进行聚类，划分为$K$类，可使用`K-means`；\n8. 聚类结果即为输出结果\n\n注意是$K_1$个最小特征值对应的特征向量，别问我为什么知道。。。\n\n### 代码\n[@Github: Spectral Clustering](https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p86_spectral_clustering.py)\n```\nclass SpectralClustering():\n    \"\"\"\n    Attributes:\n        k: {int}, k < n_samples\n        sigma: {float}\n    Notes:\n        Steps:\n            - similarity matrix [W_{n×n}]\n            - diagonal matrix [D_{n×n}] is defined as\n                    D_{ii} = \\begin{cases}\n                                \\sum_j W_{ij} & i \\neq j \\\\\n                                0 & i = j\n                            \\end{cases}\n            - Laplacian matrix [L_{n×n}], Laplacian matrix is defined as\n                    L = D - W or L = D^{-1} (D - W)\n            - EVD: L \\alpha_i = \\lambda_i \\alpha_i\n            - takes the eigenvector corresponding to the largest eigenvalue as\n                    B_{n×k} = [\\beta_1, \\beta_2, ..., \\beta_k]\n            - apply K-Means to the row vectors of matrix B\n    \"\"\"\n    def __init__(self, k, n_clusters=2, sigma=1.0):\n        self.kmeans = KMeans(n_clusters=n_clusters)\n        self.k = k\n        self.sigma = sigma\n    def predict(self, X):\n        n_samples = X.shape[0]\n        # step 1\n        kernelGaussian = lambda z, sigma: np.exp(-0.5 * np.square(z/sigma))\n        W = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(i):\n                W[i, j] = kernelGaussian(np.linalg.norm(X[i] - X[j]), self.sigma)\n                W[j, i] = W[i, j]\n        # step 2\n        D = np.diag(np.sum(W, axis=1))\n        # step 3\n        L = D - W\n        L = np.linalg.inv(D).dot(L)\n        # step 4\n        eigval, eigvec = np.linalg.eig(L)\n        # step 5\n        order = np.argsort(eigval)\n        eigvec = eigvec[:, order]\n        beta = eigvec[:, :self.k]\n        # step 6\n        self.kmeans.fit(beta)\n        return self.kmeans.labels_\n```\n\n![spectral_clustering](/Clustering/spectral_clustering.png)\n\n## DBSCAN\n`DBSCAN(Density-Based Spatial Clustering of Applications with Noise)`，具有噪声的基于密度的聚类方法。假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的。\n> [DBSCAN密度聚类算法 - 刘建平Pinard - 博客园](https://www.cnblogs.com/pinard/p/6208966.html)\n\n### 原理\n先介绍几个关于密度的概念\n- $\\epsilon$-邻域\n    对于样本$x^{(i)}$，其$\\epsilon$-邻域包含样本集中与$x^{(i)}$距离不大于$\\epsilon$的子样本集，其样本个数记作$|N_{\\epsilon}(x^{(i)})|$。\n    $$\n    N_{\\epsilon}(x^{(i)}) = \\{ x^{(j)} | d_{ij} \\leq \\epsilon \\}\n    $$\n\n- 核心对象\n    对于任一样本$x^{(i)}$，若其$\\epsilon$-邻域$N_{\\epsilon}(x^{(i)})$至少包含$minPts$个样本，则该样本为核心对象。如图，选择若选取$\\epsilon=5$，则红点均为核心对象\n- 密度直达\n    若样本$x^{(j)} \\in N_{\\epsilon}(x^{(i)})$，且$x^{(i)}$为核心对象，则称$x^{(j)}$由$x^{(i)}$密度直达。不满足对称性，即反之不一定成立，除非$x^{(j)}$也为核心对象。如图，$x^{(8)}$可由$x^{(6)}$密度直达，而反之$x^{(6)}$不可由$x^{(8)}$密度直达，因为$x^{(8)}$不为核心对象。\n- 密度可达\n    若存在样本序列$p_1, p_2, ..., p_T$，满足$p_1 = x^{(i)}, p_T = x^{(j)}$，且$p_{t+1}$可由$p_t$密度直达，也就是说$p_1, p_2, ..., p_{T-1}$均为核心对象，则称$x^{(j)}$由$x^{(i)}$密度可达。也不满足对称性。如图，$x^{(4)}$可由$x^{(1)}$密度可达，而$x^{(2)}$不可由$x^{(4)}$密度可达，因为$x^{(4)}$不为核心对象。\n- 密度相连\n    存在核心对象$x^{(k)}$，使得$x^{(i)}$与$x^{(j)}$均由$x^{(k)}$密度可达，则称$x^{(i)}$与$x^{(j)}$密度相连。注意密度相连满足对称性。如图，$x^{(8)}$与$x^{(4)}$均可由$x^{(1)}$密度可达，则$x^{(8)}$与$x^{(4)}$密度相连。\n\n![DBSCAN1](/Clustering/DBSCAN1.jpg)\n\n### 计算思想\n`DBSCAN`的聚类思想是，由**密度可达关系**导出的最大密度相连的样本集合，即为我们最终聚类的一个簇，这个簇里可能只有一个核心对象，也可能有多个核心对象，若有多个，则簇里的任意一个核心对象的$\\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。\n\n另外，考虑以下三个问题\n- 噪音点\n    一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，这些样本点标记为噪音点，`with Noise`就是这个意思。\n- 距离的度量\n    一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和`KNN`算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用`KDTree`或者球树来快速的搜索最近邻。\n- 类别重复时的判别\n    某些样本可能到两个核心对象的距离都小于$\\epsilon$，但是这两个核心对象如下图所示，不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？\n    ![DBSCAN2](/Clustering/DBSCAN2.jpg)\n    一般来说，此时`DBSCAN`采用**先来后到**，先进行聚类的类别簇会标记这个样本为它的类别。也就是说`BDSCAN`不是完全稳定的算法。\n\n### 算法步骤\n对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, ..., x^{(M)})$，指定邻域参数$(\\epsilon, minPts)$与样本距离度量方式，将其划分为$K$类。\n1. 检测数据库中尚未检查过的对象$p$，如果$p$未被处理(归为某个簇或者标记为噪声)，则检查其邻域：\n   - 若包含的对象数不小于$minPts$，建立新簇$C$，将其中的所有点加入候选集$N$；\n2. 对候选集$N$中所有尚未被处理的对象$q$，检查其邻域：\n   - 若至少包含$minPts$个对象，则将这些对象加入$N$；\n   - 如果$q$未归入任何一个簇，则将$q$加入$C$；\n3. 重复步骤$2$，继续检查$N$中未处理的对象，直到当前候选集$N$为空；\n4. 重复步骤$1$-$3$，直到所有对象都归入了某个簇或标记为噪声。\n\n## 高斯混合模型(GMM)\n详情查看[EM算法 & GMM模型](https://louishsu.xyz/2018/11/12/EM%E7%AE%97%E6%B3%95-GMM%E6%A8%A1%E5%9E%8B/)。\n\n## 层次聚类(Hierarchical Clustering)\n层次聚类更多的是一种思想，而不是方法，通过从下往上不断合并簇，或者从上往下不断分离簇形成嵌套的簇。例如上面讲到的`DBSCAN`最后簇的合并就有这种思想。\n\n层次的类通过“树状图”来表示，如下\n![层次聚类](/Clustering/层次聚类.png)\n\n主要的思想或方法有两种\n- 自底向上的凝聚方法`(agglomerative hierarchical clustering)`\n    如`AGNES`。\n- 自上向下的分裂方法`(divisive hierarchical clustering)`\n    如`DIANA`。\n\n## 图团体检测(Graph Community Detection) \n略\n\n","categories":["Machine Learning"]},{"title":"EM & GMM","url":"/2018/11/12/EM-GMM/","content":"\n# EM算法\n`Expectation Maximization Algorithm`，是 Dempster, Laind, Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 `MLE` 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据。\n\n## 引例：先挖个坑\n给出李航《统计学习方法》的三硬币模型例子，假设有$3$枚硬币$A, B, C$，各自出现正面的概率分别为$\\pi, p, q$，先进行如下实验：先投掷硬币$A$，若结果为正面，则选择硬币$B$投掷一次，否则选择$C$，记录投掷结果如下\n$$\n1, 1, 0, 1, 0, 0, 1, 0, 1, 1\n$$\n\n只能观测到实验结果，而投掷过程未知，即硬币$A$的投掷结果未知，现欲估计三枚硬币的参数$\\pi, p, q$。\n\n**解**：根据题意可以得到三个随机变量$X_1, X_2, X_3$的概率分布如下\n$$\nP(X_1) = \\pi ^ {X_1} (1 - \\pi) ^ {1 - X_1}\n$$\n\n$$\nP(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2}\n$$\n\n$$\nP(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}\n$$\n\n定义随机变量$X$表示观测结果为正面，由全概率公式可以得到\n$$\nP(X) \n= P(X|X_1)P(X_1) + P(X|\\overline{X_1})P(\\overline{X_1})\n= \\pi p + (1 - \\pi) q\n$$\n\n$$\nP(\\overline{X}) = P(\\overline{X}|X_1)P(X_1) + P(\\overline{X}|\\overline{X_1})P(\\overline{X_1})\n= \\pi (1 - p) + (1 - \\pi) (1 - q)\n$$\n\n即\n$$\nP(X) = [\\pi p + (1 - \\pi) q] ^ {X} [\\pi (1 - p) + (1 - \\pi) (1 - q)] ^ {1 - X}\n$$\n\n利用最大似然估计，有\n$$\n\\log L(D | \\theta) = 6 \\log [\\pi p + (1 - \\pi) q] + 4 \\log [\\pi (1 - p) + (1 - \\pi) (1 - q)]\n$$\n\n至此，我们一定能想到通过求似然函数极值来求解参数\n$$\n\\frac{∂ }{∂ \\pi} \\log L = 0 \\Rightarrow 5 \\pi (p - q) + 5q - 3 = 0\n$$\n\n$$\n\\frac{∂ }{∂ p} \\log L = 0 \\Rightarrow 5 \\pi (p - q) + 5q - 3 = 0\n$$\n\n$$\n\\frac{∂ }{∂ q} \\log L = 0 \\Rightarrow 5 \\pi (p - q) + 5q - 3 = 0\n$$\n\n但是好像出了问题，并不能求解，所以我们引入`EM算法`迭代求解。\n\n## 推导\n以$x^{(i)}$表示训练数据，$w_k$表示类别，设当前迭代参数为$\\theta^{(t)}$，则下一次迭代应有\n$$\n\\theta^{(t+1)} = \\arg \\max \\sum_i \\log P(x^{(i)}|\\theta) \\tag{1}\n$$\n\n由边缘概率公式\n$$\n\\sum_i \\log P(x^{(i)}|\\theta)\n= \\sum_i \\log \\sum_k P(x^{(i)}, w_k^{(i)}|\\theta) \\tag{2}\n$$\n\n> $P(x^{(i)}, w_k^{(i)}|\\theta) = P(x^{(i)} | w_k^{(i)}, \\theta) P(w_k^{(i)}|x^{(i)}, \\theta)$\n> 至此已得出引例中的表达式，其中$P(w_k^{(i)}|x^{(i)}, \\theta)$与$P(x^{(i)} | w_k^{(i)}, \\theta)$均未知，而通过求极值不能解得参数。\n\n我们引入迭代参数$\\theta^{(t)}$，即第$t$次迭代时的参数$\\theta$，该参数为已知变量\n$$\n\\sum_i \\log P(x^{(i)}|\\theta)\n= \\sum_i \\log \\sum_k P(x^{(i)}, w_k^{(i)}|\\theta) \n\\frac{P(w_k^{(i)} | \\theta^{(t)})}\n{P(w_k^{(i)} | \\theta^{(t)})}\n$$\n\n> $P(w_k^{(i)}|\\theta^{(t)})$表示样本$x^{(i)}$类别为$w_k^{(i)}$的概率，注意上标。\n\n引入`Jensen不等式`：\n> For a real convex function $\\varphi$, numbers $x_1, ..., x_n$ in its domain, and positive weights $a_i$, Jensen's inequality can be stated as:\n> $$\n> \\varphi\\left(\\frac{\\sum a_i x_i}{\\sum a_i}\\right)\n> \\leq \\frac{\\sum a_i \\varphi(x_i)}{\\sum a_i}\n> $$\n> \n> and the inquality is reversed if $\\varphi$ is concave, which is\n> $$\n> \\varphi\\left(\\frac{\\sum a_i x_i}{\\sum a_i}\\right)\n> \\geq \\frac{\\sum a_i \\varphi(x_i)}{\\sum a_i}\n> $$\n> \n> Equality holds if and only if $x_1 = ... = x_n$ or $\\varphi$ is linear.\n\n\n$\\log(·)$为凹函数`(concave)`，且满足\n$$\n\\sum_k P(w_k^{(i)} | \\theta^{(t)}) = 1\n$$\n\n所以有\n$$\n\\sum_i \\log P(x^{(i)}|\\theta)\n= \\sum_i \\log \\sum_k P(x^{(i)}, w_k^{(i)}|\\theta) \n\\frac{P(w_k^{(i)}|\\theta^{(t)})}\n{P(w_k^{(i)}|\\theta^{(t)})}\n$$\n\n$$\n\\geq \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log \\frac{P(x^{(i)}, w_k^{(i)}|\\theta)}{P(w_k^{(i)}|\\theta^{(t)})} \\tag{3}\n$$\n\n\n此时我们得到似然函数$\\sum_i \\log P(x^{(i)}|\\theta)$的一个下界，但必须保证这个下界是紧的，也就是至少有点能使等号成立\n> 由`Jensen不等式`，当且仅当$　P(x^{(i)}, w_k^{(i)}|\\theta)=C　$时取等号\n\n\n定义\n$$\nL(\\theta|\\theta^{(t)})\n= \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log \\frac{P(x^{(i)}, w_k^{(i)}|\\theta)}{P(w_k^{(i)}|\\theta^{(t)})}\n$$\n\n$$\n= \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log P(x^{(i)}, w_k^{(i)}|\\theta) - \nP(w_k^{(i)}|\\theta^{(t)}) \n\\log P(w_k^{(i)}|\\theta^{(t)})\n$$\n\n其中第一项即期望\n$$\nE_w\\left[\n    \\log P(X, w|\\theta) | X, \\theta^{(t)}\n\\right] \n= \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \n\\log P(x^{(i)}, w_k^{(i)}|\\theta) \\tag{4}\n$$\n\n第二项为$P(w | X, \\theta^{(t)})$的信息熵\n$$\nH[P(w | X, \\theta^{(t)})] \n= - \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log P(w_k^{(i)}|\\theta^{(t)}) \\tag{5}\n$$\n\n即\n$$\nL(\\theta|\\theta^{(t)})\n= E_w\\left[\n    \\log P(X, w|\\theta) | X, \\theta^{(t)}\n\\right] +\nH[P(w | X, \\theta^{(t)})] \\tag{E-step}\n$$\n\n> 注意到$H[P(w | X, \\theta^{(t)})]$项为常数，故也可设\n> $$ \n> Q(\\theta|\\theta^{(t)}) = \n> E_w\\left[\n> \\log P(X, w|\\theta) | X, \\theta^{(t)} \\right]\n> $$\n\n代回$(1)$，得到优化目标\n$$\n\\theta^{(t+1)} \n= \\arg \\max L(\\theta|\\theta^{(t)}) \\tag{M-step}\n$$\n\n我们需要不断最大化$L(\\theta | \\theta^{(t)})$来不断优化，这就是所谓的`EM算法`，`E-step`是指求出期望，`M-step`是指迭代更新参数\n![EM算法图解](/EM-GMM/EM算法图解.png)\n\n伪代码如下\n```\nAccording to prior knowledge set \n    $\\theta$\nRepeat until convergence{\n    E-step: The expectation of hidden variables\n    M-step: Finding the maximum of likelihood function\n}\n```\n\n实际上，从边缘概率与条件概率入手\n$$\n\\sum_i \\log P(x^{(i)}|\\theta)\n= \\sum_i \\log \\sum_k P(x^{(i)}, w_k^{(i)}|\\theta)\n$$\n\n$$\n= \\sum_i \\log \\sum_k P(x^{(i)} | w_k^{(i)}, \\theta) P(w_k^{(i)} | \\theta)\n$$\n\n$$\n\\geq \\sum_i \\sum_k P(w_k^{(i)} | \\theta) \\log P(x^{(i)} | w_k^{(i)}, \\theta) \\tag{Jensen inequality}\n$$\n\n$$\n= \\sum_i \\sum_k P(w_k^{(i)} | \\theta) \\log \\frac{P(x^{(i)}, w_k^{(i)}|\\theta)}{P(w_k^{(i)}|\\theta)}\n$$\n\n\n而由$(3)$，引入迭代变量可以得到\n$$\n\\sum_i \\log P(x^{(i)}|\\theta)\n\\geq L(\\theta|\\theta^{(t)})\n$$\n\n其中\n$$\nL(\\theta|\\theta^{(t)})\n= \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log \\frac{P(x^{(i)}, w_k^{(i)}|\\theta)}{P(w_k^{(i)}|\\theta^{(t)})}\n$$\n\n则\n$$\n\\sum_i \\log P(x^{(i)}|\\theta) - L(\\theta|\\theta^{(t)})\n$$\n\n$$\n= \\sum_i \\sum_k P(w_k^{(i)} | \\theta) \\log \\frac{P(x^{(i)}, w_k^{(i)}|\\theta)}{P(w_k^{(i)}|\\theta)} -\n\\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log \\frac{P(x^{(i)}, w_k^{(i)}|\\theta)}{P(w_k^{(i)}|\\theta^{(t)})}\n$$\n\n$$\n= \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log \\frac{P(w_k^{(i)}|\\theta^{(t)})}{P(w_k^{(i)}|\\theta)}\n$$\n\n而由`KL散度( Kullback–Leibler divergence)`(又称`相对熵(relative entropy)`)定义\n\n> $$\n> D(P||Q) = \\sum P(x) \\log \\frac{P(x)}{Q(x)}\n> $$\n\n可知\n$$\n\\sum_i \\log P(x^{(i)}|\\theta) - L(\\theta|\\theta^{(t)}) \n= D\\left[ P(w_k^{(i)}|\\theta^{(t)}) || P(w_k^{(i)}|\\theta) \\right]\n$$\n\n即迭代的$P(w_k^{(i)}|\\theta^{(t)})$与真实的$P(w_k^{(i)}|\\theta)$之间的相对熵！\n\n> 这里关于`K-L散度`的困扰了$N$久，终于搞出来了。\n\n<!-- 另外，附上证明一则\n> 证明对数似然函数$\\sum_i \\log P(x^{(i)} | \\theta)$满足\n> $$\n> \\sum_i \\log P(x^{(i)} | \\theta^{(t+1)}) \n> \\geq \\sum_i \\log P(x^{(i)} | \\theta^{(t)})\n> $$\n> \n> 证明：由$(M-step)$\n> $$\\sum_i \\log P(x^{(i)} | \\theta^{(t+1)})\n> = \\max L(\\theta | \\theta^{(t)})$$\n> \n> 而$\\theta^{(t+1)}为函数L(\\theta|\\theta^{(t)})极值点$，所以\n> $$\\max L(\\theta | \\theta^{(t)})\n> \\geq L(\\theta | \\theta^{(t)})$$\n> \n> 其中\n> $$\n> L(\\theta | \\theta^{(t)})\n> = \\sum_i \\log P(x^{(i)} | \\theta^{(t)})\n> $$\n> \n> 故\n> $$\n> \\sum_i \\log P(x^{(i)} | \\theta^{(t+1)}) \n> \\geq \\sum_i \\log P(x^{(i)} | \\theta^{(t)})\n> $$ -->\n\n\n\n## 引例的求解\n> $Q(\\theta|\\theta^{(t)}) = \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log P(x^{(i)}, w_k^{(i)}|\\theta)$\n\n此题中\n$$\nP(w_k|\\pi) = \\pi^{w_k}(1-\\pi)^{1-w_k}\n$$\n\n$$\nP(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}}\n$$\n\n$$\nP(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}}\n$$\n\n- $E-step$\n    $$\n    Q(\\pi, p, q | \\pi^{(t)}, p^{(t)}, q^{(t)})\n    = \\sum_i \\sum_k P(w_k^{(i)}|\\pi^{(t)}, p^{(t)}, q^{(t)}) \n    \\log P(x^{(i)}, w_k^{(i)} | \\pi, p, q)\n    $$\n\n    先求$P(w_k^{(i)}|\\pi^{(t)}, p^{(t)}, q^{(t)})$，即第一次投掷结果为$w_k$的概率\n    $$\n    P(w_k^{(i)}|\\pi^{(t)}, p^{(t)}, q^{(t)})\n    = \\frac\n    {\\left[\\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\\right]^{w_k}\n    \\left[(1-\\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\\right]^{1-w_k}}\n    {\\sum_j \\left[\\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\\right]^{w_j}\n    \\left[(1-\\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\\right]^{1-w_j}}\n    $$\n\n    即\n    $$\n    \\begin{cases}\n        P(w_1^{(i)}|\\pi^{(t)}, p^{(t)}, q^{(t)}) = \\frac\n            {\\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}}\n            {\\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + \n            (1-\\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\\\\n        P(w_2^{(i)}|\\pi^{(t)}, p^{(t)}, q^{(t)}) = \\frac\n            {(1-\\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}\n            {\\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + \n            (1-\\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}\n    \\end{cases}\n    $$\n\n    记\n    $$\\mu_1^{(i)} = P(w_1^{(i)}|\\pi^{(t)}, p^{(t)}, q^{(t)})$$\n\n    $$\\mu_2^{(i)} = 1 - \\mu_1^{(i)}$$\n    \n    > 注意$w^{(i)}_k$上标`^{(i)}`\n\n    再求$P(x^{(i)}, w_k^{(i)} | \\pi, p, q)$，已知\n    $$\n    P(x^{(i)}, w_k^{(i)} | \\pi, p, q)\n    = P(x^{(i)} | w_k^{(i)}, \\pi, p, q)\n    P(w_k^{(i)} | \\pi, p, q)\n    $$\n    \n    所以\n    $$\n    P(x^{(i)}, w_k^{(i)} | \\pi, p,q)\n    = \\left[\\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\\right]^{w_k}\n    \\left[(1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\\right]^{1-w_k}\n    $$\n\n    综上\n    $$\n    Q(\\pi, p, q | \\pi^{(t)}, p^{(t)}, q^{(t)})\n    = \\sum_i \\sum_{k=1}^2 \\mu^{(i)}_k \\left[\\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\\right]^{w_k}\n    \\left[(1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\\right]^{1-w_k}\n    $$\n\n    $$\n    = \\sum_i \\mu_1^{(i)} \\log \\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \\mu_1^{(i)}) \\log (1-\\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}\n    $$\n\n- $M-step$\n    - $\\frac{∂Q}{∂\\pi} = 0$\n        $$\n        \\frac{∂Q}{∂\\pi}\n        = \\sum_i \\mu_1^{(i)} \n        \\frac\n        {p^{x^{(i)}}(1-p)^{1-x^{(i)}}}\n        {\\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} + \n        (1 - \\mu_1^{(i)})\n        \\frac\n        {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}}\n        {(1-\\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}}\n        $$\n\n        $$\n        = \\sum_i \\frac{\\mu_1^{(i)}}{\\pi} + \\frac{\\mu_1^{(i)} - 1}{1 - \\pi}\n        = \\sum_i \\frac{\\mu_1^{(i)} - \\pi}{\\pi(1 - \\pi)}\n        = \\frac{\\sum_i \\mu_1^{(i)} - n\\pi}{\\pi(1 - \\pi)} = 0\n        $$\n\n        $$\n        \\Rightarrow \\pi^{(t+1)} = \\frac{1}{n} \\sum_i \\mu_1^{(i)}\n        $$\n        \n    - $\\frac{∂Q}{∂p} = 0$\n        $$\n        \\frac{∂Q}{∂p}\n        = \\sum_i \\mu_1^{(i)} \n        \\left[\n            \\frac{x^{(i)}}{p} - \\frac{1 - x^{(i)}}{1 - p}\n        \\right]\n        $$\n\n        $$\n        = \\frac{1}{p(1 - p)} \n        \\sum_i \\mu_1^{(i)} (x^{(i)} - p)\n        = \\frac{1}{p(1 - p)}\n        \\left[\n            \\sum_i \\mu_1^{(i)} x^{(i)} - p \\sum_i \\mu_1^{(i)}\n        \\right] = 0\n        $$\n\n        $$\n        \\Rightarrow p^{(t+1)} = \\frac{\\sum_i \\mu_1^{(i)} x^{(i)}}{\\sum_i \\mu_1^{(i)}}\n        $$\n\n    - $\\frac{∂Q}{∂q} = 0$\n        $$\n        \\frac{∂Q}{∂q}\n        = \\sum_i (1 - \\mu_1^{(i)}) \n        \\left[\n            \\frac{x^{(i)}}{q} - \\frac{1 - x^{(i)}}{1 - q}\n        \\right]\n        $$\n\n        $$\n        = \\frac{1}{q(1 - q)}\n        \\sum_i (1 - \\mu_1^{(i)}) \n        (x^{(i)} - q)\n        $$\n\n        $$\n        = \\frac{1}{q(1 - q)}\n        \\left[\n            \\sum_i (1 - \\mu_1^{(i)}) x^{(i)} -\n            q \\sum_i (1 - \\mu_1^{(i)})\n        \\right] = 0\n        $$\n\n        $$\n        \\Rightarrow q^{(t+1)} = \\frac{\\sum_i (1 - \\mu_1^{(i)}) x^{(i)}}{\\sum_i (1 - \\mu_1^{(i)})}\n        $$\n\n多次迭代即可求解，终止条件可设置为\n$$\n|| \\theta^{(t+1)} - \\theta^{(t)} || < \\epsilon\n$$\n\n或\n$$\n||Q(\\theta^{(t+1)} | \\theta^{(t)}) - Q(\\theta^{(t)} |\\theta^{(t)})|| < \\epsilon\n$$\n\n# GMM模型\n`Gaussian Mixture Model`，是一种无监督学习算法，常用于聚类。当聚类问题中各个类别的尺寸不同、聚类间有相关关系的时候，往往使用`GMM`更合适。对一个样本来说，`GMM`得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被成为软聚类。一般说来， 任意形状的概率分布都可以用多个高斯分布函数去近似，因而，`GMM`的应用也比较广泛。\n\n高斯混合模型，指具有如下形式的概率分布模型：\n$$\nP(x|\\mu_k, \\Sigma_k)\n= \\sum_{k=1}^K \\pi_k N(x|\\mu_k, \\Sigma_k)\n$$\n\n其中\n- $\\pi_k(0 \\leq \\pi_k \\leq 1)$是系数，且$\\sum_k \\pi_k = 1$\n- $N(x|\\mu_k, \\Sigma_k)$为高斯密度函数\n\n$$\nN(x|\\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma_k|^{1/2}} \n\\exp \\left[\n    -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\n\\right]\n$$\n\n> - 即多个高斯分布叠加出来的玩意；\n> - 现在我们需要求取系数$\\pi_k$及高斯模型的参数$(\\mu_k, \\Sigma_k)$；\n> - 与`K-Means`等聚类方法区别是，`GMM`求出的是连续的分布模型，可计算出“归属于”哪一类的概率。\n\n## 推导\n$$\n\\log P(X|\\pi, \\mu, \\Sigma)\n= \\sum_i \\log \\sum_k \\pi_k N(x|\\mu_k, \\Sigma_k)\n$$\n\n$$\ns.t.　\\sum_k \\pi_k = 1\n$$\n\n### 暴力求解\n以$1$维高斯分布为例\n$$\nN(x|\\mu_k, \\sigma_k^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} e^{-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}}\n$$\n\n构造拉格朗日`(Lagrange)`函数\n$$\nL(\\pi, \\mu, \\sigma^2) \n= \\sum_i \\log \\sum_k \\pi_k N(x|\\mu_k, \\sigma_k^2) + \\lambda \\left(\\sum_k \\pi_k - 1 \\right) \\tag{5}\n$$\n\n$$\n\\begin{cases}\n    \\frac{∂}{∂\\pi_k} L(\\pi, \\mu, \\sigma^2) \n        = \\sum_i\n        \\frac{N(x^{(i)}|\\mu_k, \\sigma_k^2)}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\sigma_j^2)} + \\lambda \\\\\n    \\frac{∂}{∂\\mu_k} L(\\pi, \\mu, \\sigma^2)\n        = \\sum_i\n        \\frac{\\pi_k}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\sigma_j^2)} \\frac{∂}{∂\\mu_k}N(x^{(i)}|\\mu_k, \\sigma_k^2) \\\\\n    \\frac{∂}{∂\\sigma_k^2} L(\\pi, \\mu, \\sigma^2)\n        = \\sum_i\n        \\frac{\\pi_k}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\sigma_j^2)} \\frac{∂}{∂\\sigma_k^2}N(x^{(i)}|\\mu_k, \\sigma_k^2)\n\\end{cases} \\tag{6}\n$$\n\n其中\n$$\n\\frac{∂}{∂\\mu_k} N(x|\\mu_k, \\sigma_k^2)\n= \\frac{1}{\\sqrt{2\\pi}\\sigma_k} e^{-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}} \\frac{x-\\mu_k}{\\sigma_k^2}\n= N(x|\\mu_k, \\sigma_k^2) · \\frac{x-\\mu_k}{\\sigma_k^2}\n$$\n\n$$\n\\frac{∂}{∂\\sigma_k^2} N(x|\\mu_k, \\sigma_k^2)\n= \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}} \\frac{∂}{∂\\sigma_k^2} \\left(\\frac{1}{\\sigma_k}\\right) + \n\\frac{1}{\\sqrt{2\\pi}\\sigma_k} e^{-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}} \\left(-\\frac{(x - \\mu_k)^2}{2}\\right) \\frac{∂}{∂\\sigma_k^2} \\left(\\frac{1}{\\sigma_k^2}\\right)\n$$\n\n> $\\frac{∂}{∂\\sigma_k^2} \\left(\\frac{1}{\\sigma_k}\\right) = - \\frac{\\sigma_k^{-3}}{2};　\\frac{∂}{∂\\sigma_k^2} \\left(\\frac{1}{\\sigma_k^2}\\right) = - \\frac{1}{\\sigma_k^4}$\n\n\n$$\n= \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}} \\left(- \\frac{\\sigma_k^{-3}}{2}\\right) + \n\\frac{1}{\\sqrt{2\\pi}\\sigma_k} e^{-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}} \\left(-\\frac{(x - \\mu_k)^2}{2}\\right) \\left(- \\frac{1}{\\sigma_k^4}\\right)\n$$\n\n$$\n= N(x|\\mu_k, \\sigma_k^2) \\left[\n    \\frac{(x - \\mu_k)^2}{\\sigma_k^2} - 1\n\\right] \\frac{1}{2 \\sigma_k^2}\n$$\n\n代回$(6)$可以得到\n$$\n\\begin{cases}\n    \\frac{∂}{∂\\pi_k} L(\\pi, \\mu, \\sigma^2)\n        = \\sum_i\n        \\frac{N(x^{(i)}|\\mu_k, \\sigma_k^2)}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\sigma_j^2)} + \\lambda \\\\\n    \\frac{∂}{∂\\mu_k} L(\\pi, \\mu, \\sigma^2)\n        = \\sum_i\n        \\frac{\\pi_k N(x^{(i)}|\\mu_k, \\sigma_k^2)}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\sigma_j^2)} \\frac{x^{(i)}-\\mu_k}{\\sigma_k^2} \\\\\n    \\frac{∂}{∂\\sigma_k^2} L(\\pi, \\mu, \\sigma^2)\n        = \\sum_i\n        \\frac{\\pi_k N(x^{(i)}|\\mu_k, \\sigma_k^2)}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\sigma_j^2)} \\left[\n    \\frac{(x^{(i)} - \\mu_k)^2}{\\sigma_k^2} - 1\n\\right] \\frac{1}{2 \\sigma_k^2}\n\\end{cases} \\tag{7}\n$$\n\n令\n$$\n\\gamma^{(i)}_k = \\frac{\\pi_k N(x^{(i)}|\\mu_k, \\sigma_k^2)}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\sigma_j^2)} \\tag{8}\n$$\n\n> 通俗理解：$\\gamma^{(i)}_k$表示样本$x^{(i)}$中来自类别$w_k$的“贡献百分比”\n\n- 令$\\frac{∂}{∂\\mu_k} \\log P(X|\\pi, \\mu, \\sigma^2) = 0$，整理得到\n    $$\n    \\sum_i \\gamma^{(i)}_k (x^{(i)} - \\mu_k) = 0\n    \\Rightarrow \n    \\mu_k = \\frac{\\sum_i \\gamma^{(i)}_k x^{(i)}}{\\sum_i \\gamma^{(i)}_k}\n    $$\n\n- 令$\\frac{∂}{∂\\sigma_k^2} \\log P(X|\\pi, \\mu, \\sigma^2) = 0$，整理得到\n    $$\n    \\sum_i\n        \\gamma^{(i)}_k \n        \\left[\n            \\frac{(x^{(i)} - \\mu_k)^2}{\\sigma_k^2} - 1\n        \\right] = 0\n    \\Rightarrow\n    \\sigma_k^2 = \\frac{\\sum_i \\gamma^{(i)}_k (x^{(i)} - \\mu_k)^2}{\\sum_i \\gamma^{(i)}_k}\n    $$\n\n- 对于$\\frac{∂}{∂\\pi_k} \\log P(X|\\pi, \\mu, \\sigma^2) = 0$，需要做一点处理\n    两边同乘$\\pi_k$，得到\n    $$\n    \\sum_i \\gamma^{(i)}_k = - \\lambda \\pi_k \\tag{9}\n    $$\n\n    然后两边对$k$作累加\n    $$\n    \\sum_k \\sum_i \\gamma^{(i)}_k = - \\lambda \\sum_k \\pi_k\n    $$\n    \n    > $\\sum_k \\sum_i \\gamma^{(i)}_k = \\sum_i \\sum_k \\gamma^{(i)}_k = N,　\\sum_k \\pi_k = 1$\n\n    $$\n    N = - \\lambda　或　\\lambda = -N \\tag{10}\n    $$\n\n    代回$(9)$，得到\n    $$\n    \\pi_k = \\frac{\\sum_i \\gamma^{(i)}_k}{N}\n    $$\n\n综上，我们得到$4$个用于迭代的计算式，将其推广至多维即\n$$\n\\gamma^{(i)}_k = \\frac{\\pi_k N(x^{(i)}|\\mu_k, \\Sigma_k)}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\Sigma_j)}\n$$\n\n$$\n\\mu_k = \\frac{\\sum_i \\gamma^{(i)}_k x^{(i)}}{\\sum_i \\gamma^{(i)}_k}\n$$\n\n$$\n\\Sigma_k = \\frac{\\sum_i \\gamma^{(i)}_k (x^{(i)} - \\mu_k) (x^{(i)} - \\mu_k)^T}{\\sum_i \\gamma^{(i)}_k}\n$$\n\n$$\n\\pi_k = \\frac{\\sum_i \\gamma^{(i)}_k}{N}\n$$\n\n\n### 用`EM算法`求解\n> $Q(\\theta|\\theta^{(t)}) = \\sum_i \\sum_k P(w_k^{(i)}|\\theta^{(t)}) \\log P(x^{(i)}, w_k^{(i)}|\\theta)$\n\n\n$$\nQ(\\mu_k, \\Sigma_k|\\mu_k^{(t)}, \\Sigma_k^{(t)}) \n= \\sum_i \\sum_k \nP(w_k^{(i)}|\\mu_k^{(t)}, \\Sigma_k^{(t)})\n\\log P(x^{(i)}, w_k^{(i)}|\\mu_k, \\Sigma_k)\n$$\n\n\n- $ M-step $\n    $$\n    P(w_k^{(i)}|\\mu_k^{(t)}, \\Sigma_k^{(t)})\n    = \\frac{\\pi_k N(x^{(i)}|\\mu_k, \\Sigma_k)}{\\sum_j \\pi_j N(x^{(i)}|\\mu_j, \\Sigma_j)}\n    = \\gamma^{(i)}_k\n    $$\n    \n    $$\n    P(x^{(i)}, w_k^{(i)}|\\mu_k, \\Sigma_k)\n    = P(x^{(i)} | w_k^{(i)}, \\mu_k, \\Sigma_k) \n    P(w_k^{(i)}|\\mu_k, \\Sigma_k)\n    = \\pi_k N(x^{(i)}|\\mu_k, \\Sigma_k)\n    $$\n\n    故\n    $$\n    Q(\\mu_k, \\Sigma_k|\\mu_k^{(t)}, \\Sigma_k^{(t)}) \n    = \\sum_i \\sum_k \\gamma^{(i)}_k \\log \\pi_k N(x^{(i)}|\\mu_k, \\Sigma_k)\n    $$\n\n    通过求解极值可得到与$\\underline{暴力求解}$一样的等式，即\n    $$\n    \\gamma^{(i)(t)}_k = \\frac{\\pi^{(t)}_k N(x^{(i)}|\\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_j \\pi_j^{(t)} N(x^{(i)}|\\mu_j^{(t)}, \\Sigma_j^{(t)})}\n    $$\n    \n    $$\n    \\mu_k^{(t+1)} = \\frac{\\sum_i \\gamma^{(i)(t)}_k x^{(i)}}{\\sum_i \\gamma^{(i)(t)}_k}\n    $$\n\n    $$\n    \\Sigma_k^{(t+1)} = \\frac{\\sum_i \\gamma^{(i)(t)}_k (x^{(i)} - \\mu_k) (x^{(i)} - \\mu_k)^T}{\\sum_i \\gamma^{(i)(t)}_k}\n    $$\n\n    $$\n    \\pi_k^{(t+1)} = \\frac{\\sum_i \\gamma^{(i)(t)}_k}{N}\n    $$\n    \n    伪代码为\n    ```\n    According to prior knowledge set\n        \\pi^{(t)}(n_clusters,)\n        \\mu^{(t)}(n_clusters, n_features)\n        \\Sigma^{(t)}(n_clusters, n_features, n_features)\n    Repeat until convergence{\n        # E-step: calculate \\gamma^{(t)}\n            \\gamma(n_samples, n_clusters)\n        # M-step: update \\pi, \\mu, \\Sigma\n            \\pi^{(t+1)}(n_clusters,)\n            \\mu^{(t+1)}(n_clusters, n_features)\n            \\Sigma^{(t+1)}(n_clusters, n_features, n_features)\n    }\n    ```\n\n    > 初始点的选择可以随机选择，也可使用`K-Means`\n\n    `GMM`算法收敛过程如下\n    ![gmm](/EM-GMM/gmm.gif)\n\n## 代码\n[@Github: GMM](https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p82_gmm.py)\n```\nclass GMM():\n    \"\"\" Gaussian Mixture Model\n    Attributes:\n        n_clusters {int}\n        prior {ndarray(n_clusters,)}\n        mu {ndarray(n_clusters, n_features)}\n        sigma {ndarray(n_clusters, n_features, n_features)}\n    \"\"\"\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n        self.prior = None\n        self.mu = None\n        self.sigma = None\n    def fit(self, X, delta=0.01):\n        \"\"\"\n        Args:\n            X {ndarray(n_samples, n_features)}\n            delta {float}\n        Notes:\n            - Initialize with k-means\n        \"\"\"\n        (n_samples, n_features) = X.shape\n\n        # initialize with k-means\n        clf = KMeans(n_clusters=self.n_clusters)\n        clf.fit(X)\n        self.mu = clf.cluster_centers_ \n        self.prior = np.zeros(self.n_clusters)\n        self.sigma = np.zeros((self.n_clusters, n_features, n_features))\n        for k in range(self.n_clusters):\n            X_ = X[clf.labels_==k]\n            self.prior[k] = X_.shape[0] / X_.shape[0]\n            self.sigma[k] = np.cov(X_.T)\n        \n        while True:\n            mu_ = self.mu.copy()\n            # E-step: updata gamma\n            gamma = np.zeros((n_samples, self.n_clusters))\n            for i in range(n_samples):\n                for k in range(self.n_clusters):\n                    denominator = 0\n                    for j in range(self.n_clusters):\n                        post = self.prior[k] *\\\n                                    multiGaussian(X[i], self.mu[j], self.sigma[j])\n                        denominator += post\n                        if j==k: numerator = post\n                    gamma[i, k] = numerator/denominator\n            # M-step: updata prior, mu, sigma\n            for k in range(self.n_clusters):\n                sum1 = 0\n                sum2 = 0\n                sum3 = 0\n                for i in range(n_samples):\n                    sum1 += gamma[i, k]\n                    sum2 += gamma[i, k] * X[i]\n                    x_ = np.reshape(X[i] - self.mu[k], (n_features, 1))\n                    sum3 += gamma[i, k] * x_.dot(x_.T)\n                self.prior[k]  = sum1 / n_samples\n                self.mu[k]     = sum2 / sum1\n                self.sigma[k]  = sum3 / sum1\n            # to stop\n            mu_delta = 0\n            for k in range(self.n_clusters):\n                mu_delta += nl.norm(self.mu[k] - mu_[k])\n            print(mu_delta)\n            if mu_delta < delta: break\n        return self.prior, self.mu, self.sigma\n    def predict_proba(self, X):\n        \"\"\"\n        Args:\n            X {ndarray(n_samples, n_features)}\n        Returns:\n            y_pred_proba {ndarray(n_samples, n_clusters)}\n        \"\"\"\n        (n_samples, n_features) = X.shape\n        y_pred_proba = np.zeros((n_samples, self.n_clusters))\n        for i in range(n_samples):\n            for k in range(self.n_clusters):\n                y_pred_proba[i, k] = self.prior[k] *\\\n                                multiGaussian(X[i], self.mu[k], self.sigma[k])\n        return y_pred_proba\n    def predict(self, X):\n        \"\"\"\n        Args:\n            X {ndarray(n_samples, n_features)}\n        Returns:\n            y_pred_proba {ndarray(n_samples,)}\n        \"\"\"\n        y_pred_proba = self.predict_proba(X)\n        return np.argmax(y_pred_proba, axis=1)\n```","categories":["Machine Learning"]},{"title":"Data Augmentation","url":"/2018/11/02/Data-Augmentation/","content":"\n> “有时候不是由于算法好赢了。而是由于拥有很多其它的数据才赢了。”\n\n# 数据集扩增\n在深度学习中,很多训练数据意味着能够用更深的网络，训练出更好的模型。既然这样，收集很多其它的数据不即可啦？假设能够收集很多其它能够用的数据当然好，比如[ImageNet](http://www.image-net.org/)上图像数据量已达到$1400$万张，可是非常多时候，收集很多其它的数据意味着须要耗费很多其它的人力物力，这就需要使用一定的方法扩增数据集。\n\n# 图像扩增\n大部分借助`OpenCV`库，这里推荐一位学长的博客，整理了大量的`OpenCV`使用方法.\n> [Ex2tron's Blog](http://ex2tron.wang/)\n\n`TensorFlow`也提供相应图像处理方法\n[Module: tf.image | TensorFlow ](https://tensorflow.google.cn/api_docs/python/tf/image)\n\n需要注意的是，扩增过程中，需注意图像数据类型，可以将数据归一化到$(0, 1)$间再进行处理\n\n## 翻转\n```\ndef flip(image):\n    \"\"\"\n    Parameters:\n        image {ndarray(H, W, C)}\n    \"\"\"\n    rand_var = np.random.random()\n    image = image[:, ::-1, :] if rand_var > 0.5 else image\n    return image\n```\n\n## 旋转\n```\ndef rotate(image, degree):\n    \"\"\"\n    Parameters:\n        image {ndarray(H, W, C)}\n        degree {float}\n    \"\"\"\n    (h, w) = image.shape[:2]\n    center = (w // 2, h // 2)\n    random_angel = np.random.randint(-degree, degree)\n    M = cv2.getRotationMatrix2D(center, random_angel, 1.0)\n    image = cv2.warpAffine(image, M, (w, h))\n    return image\n```\n\n## 噪声\n可手动实现，如椒盐噪声代码如下\n```\ndef saltnoise(image, salt=0.0):\n    \"\"\" add salt & pepper and gaussian noise\n    Parameters:\n        image {ndarray(H, W, C)}\n        salt {float(0, 1)} number of salt pixel = salt*h*w\n    Notes:\n        TODO: gaussain noise\n    \"\"\"\n    (h, w) = image.shape[:2]\n    n_salt = int(salt * h * w)\n    for n in range(n_salt):\n        hr = np.random.randint(0, h)\n        wr = np.random.randint(0, w)\n        issalt = (np.random.rand(1) > 0.5)\n        image[hr, wr] = 255 if issalt else 0\n    return image\n```\n\n也可调用`scikit-image`库，需要注意的是，`skimage.util.random_noise()`会将原图数据转换为$(0, 1)$间的浮点数\n```\ndef noise(image, gaussian, salt, seed=None):\n    \"\"\" add noise to image TODO\n    Parameters:\n        image {ndarray(H, W, C)}\n        gaussian {bool}: \n        salt {bool}: \n    Notes:\n        Function to add random noise of various types to a floating-point image.\n    \"\"\"\n    dtype = image.dtype\n    if gaussian:\n        image = skimage.util.random_noise(image, mode='gaussian', seed=seed)\n    if salt:\n        image = skimage.util.random_noise(image, mode='s&p', seed=seed)\n\n    image = (image * 255).astype(dtype)\n    return image\n```\n\n## 亮度与对比度调整\n考虑到数据溢出，先转换为整形数据，再限制其值到$[0, 255]$\n> 注意数据类型\n\n\n```\ndef brightcontrast(image, brtadj=0, cstadj=1.0):\n    \"\"\" adjust bright and contrast value\n    Parameters:\n        image {ndarray(H, W, C)}\n        brtadj {int}    if true, adjust bright\n        cstadj {float}  if true, adjust contrast\n    \"\"\"\n    dtype = image.dtype\n    image = image.astype('int')*cstadj + brtadj\n    image = np.clip(image, 0, 255).astype(dtype)\n    return image\n```\n\n## 投射变换\n```\ndef perspective(image, prop):\n    \"\"\" 透射变换\n    Parameters:\n        image {ndarray(H, W, C)}\n        prop {float}: 在四个顶点多大的方格内选取新顶点，方格大小为(H*prop, W*prop)\n    Notes:\n        在四个顶点周围随机选取新的点进行仿射变换，四个点对应左上、右上、左下、右下\n    \"\"\"\n    (h, w) = image.shape[:2]\n\n    ptsrc = np.zeros(shape=(4, 2))\n    ptdst = np.array([[0, 0], [0, w], [h, 0], [h, w]])\n    for i in range(4):\n        hr = np.random.randint(0, int(h*prop))\n        wr = np.random.randint(0, int(w*prop))\n        if i == 0:\n            ptsrc[i] = np.array([hr, wr])\n        elif i == 1:\n            ptsrc[i] = np.array([hr, w - wr])\n        elif i == 2:\n            ptsrc[i] = np.array([h - hr, wr])\n        elif i == 3:\n            ptsrc[i] = np.array([h - hr, w - wr])\n    M = cv2.getPerspectiveTransform(ptsrc.astype('float32'), ptdst.astype('float32'))\n    image = cv2.warpPerspective(image, M, (w, h))\n    return image\n```","categories":["Deep Learning"]},{"title":"二次入坑raspberry-pi","url":"/2018/10/29/二次入坑raspberry-pi/","content":"\n# 前言\n距上一次搭建树莓派平台已经两年了，保存的镜像出了问题，重新搭建一下。\n\n# 系统\n## 下载\n从官网下载树莓派系统镜像，有以下几种可选\n> [Raspberry Pi — Teach, Learn, and Make with Raspberry Pi ](https://www.raspberrypi.org/)\n1. Raspbian & Raspbian Lite，基于Debian\n2. Noobs & Noobs Lite\n3. Ubuntu MATE\n4. Snappy Ubuntu Core\n5. Windows 10 IOT\n\n~~其余不太了解，之前安装的是Raspbian，对于Debian各种不适，换上界面优雅的Ubuntu Mate玩一下~~\n老老实实玩Raspbian，笑脸:-)\n\n\n## 安装\n比较简单，准备micro-SD卡，用Win32 Disk Imager烧写镜像\n> [Win32 Disk Imager download | SourceForge.net](https://sourceforge.net/projects/win32diskimager/)\n\n> ![Win32DiskImager](/二次入坑raspberry-pi/Win32DiskImager.jpg)\n\n安装完软件后可点击`Read`备份自己的镜像。\n\n注意第二次开机前需要配置`config.txt`文件，否则`hdmi`无法显示\n> [树莓派配置文档 config.txt 说明 | 树莓派实验室](http://shumeipai.nxez.com/2015/11/23/raspberry-pi-configuration-file-config-txt-nstructions.html)\n\n\n```\ndisable_overscan=1 \nhdmi_force_hotplug=1\nhdmi_group=2    # DMT\nhdmi_mode=32    # 1280x960\nhdmi_drive=2\nconfig_hdmi_boost=4\n```\n\n\n## 修改交换分区\n### Ubuntu Mate\n查看交换分区\n```\n$ free -m\n```\n\n未设置时如下\n```\ntotal     used     free   shared  buffers   cached\nMem:           435       56      379        0        3       16\n-/+ buffers/cache:       35      399\nSwap:            0        0        0\n```\n\n创建和挂载\n```\n# 获取权限\n$ sudo -i\n\n# 创建目录\n$ mkdir /swap\n$ cd /swap\n\n# 指定一个大小为1G的名为“swap”的交换文件\n$ dd if=/dev/zero of=swap bs=1M count=1k\n# 创建交换文件\n$ mkswap swap\n# 挂载交换分区\n$ swapon swap\n\n# 卸载交换分区\n# $ swapoff swap\n```\n\n查看交换分区\n```\n$ free -m\n```\n\n未设置时如下\n```\ntotal     used     free   shared  buffers   cached\nMem:           435       56      379        0        3       16\n-/+ buffers/cache:       35      399\nSwap:         1023        0     1023\n```\n\n### Raspbian\nWe will change the configuration in the file `/etc/dphys-swapfile`:\n```\n$ sudo nano /etc/dphys-swapfile\n```\n\nThe default value in Raspbian is:\n```\nCONF_SWAPSIZE=100\n```\n\nWe will need to change this to:\n```\nCONF_SWAPSIZE=1024\n```\n\nThen you will need to stop and start the service that manages the swapfile own Rasbian:\n```\n$ sudo /etc/init.d/dphys-swapfile stop\n$ sudo /etc/init.d/dphys-swapfile start\n```\n\nYou can then verify the amount of memory + swap by issuing the following command:\n```\n$ free -m\n```\n\nThe output should look like:\n```\ntotal     used     free   shared  buffers   cached\nMem:           435       56      379        0        3       16\n-/+ buffers/cache:       35      399\nSwap:         1023        0     1023\n```\n\n# 软件\n\n## 安装指令\n- `apt-get`\n    - 安装软件 \n    `apt-get install softname1 softname2 softname3 ...`\n    - 卸载软件 \n    `apt-get remove softname1 softname2 softname3 ...`\n    - 卸载并清除配置 \n    `apt-get remove --purge softname1`\n    - 更新软件信息数据库 \n    `apt-get update`\n    - 进行系统升级 \n    `apt-get upgrade`\n    - 搜索软件包 \n    `apt-cache search softname1 softname2 softname3 ...`\n    - 修正（依赖关系）安装：\n    `apt-get -f insta`\n\n- `dpkg`\n    - 安装`.deb`软件包 \n    `dpkg -i xxx.deb`\n    - 删除软件包 \n    `dpkg -r xxx.deb`\n    - 连同配置文件一起删除 \n    `dpkg -r --purge xxx.deb`\n    - 查看软件包信息 \n    `dpkg -info xxx.deb`\n    - 查看文件拷贝详情 \n    `dpkg -L xxx.deb`\n    - 查看系统中已安装软件包信息 \n    `dpkg -l`\n    - 重新配置软件包 \n    `dpkg-reconfigure xx`\n\n    - 卸载软件包及其配置文件，但无法解决依赖关系！\n    `sudo dpkg -p package_name`\n    - 卸载软件包及其配置文件与依赖关系包\n    `sudo aptitude purge pkgname`\n    - 清除所有已删除包的残馀配置文件 \n    `dpkg -l |grep ^rc|awk '{print $2}' |sudo xargs dpkg -P`\n\n    \n## 软件源\n1. 备份原始文件\n    ```\n    $ sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup\n    ```\n2. 修改文件并添加国内源\n    ```\n    $ vi /etc/apt/sources.list\n    ```\n3. 注释元文件内的源并添加如下地址\n    ```\n    #Mirror.lupaworld.com 源更新服务器（浙江省杭州市双线服务器，网通同电信都可以用，亚洲地区官方更新服务器）：\n    deb http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse\n    deb http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse\n    deb http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse\n    deb http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse\n    deb-src http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse\n    deb-src http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse\n    deb-src http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse\n    deb-src http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse\n\n    #Ubuntu 官方源 \n    deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse\n    deb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse\n    deb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse\n    deb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse\n    deb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse\n    deb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse\n    deb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse\n    deb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse\n    deb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse\n    deb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse\n    ```\n\n    或者\n    ```\n    #阿里云\n    deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse\n    deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse\n    deb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse\n    deb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse\n    deb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse\n    deb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse\n    deb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse\n    deb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse\n    deb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse\n    deb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse\n\n    #网易163\n    deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse\n    deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse\n    deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse\n    deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse\n    deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse\n    deb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse\n    deb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse\n    deb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse\n    deb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse\n    deb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse\n    ```\n\n4. 放置非官方源的包不完整，可在为不添加官方源\n    ```\n    deb http://archive.ubuntu.org.cn/ubuntu-cn/ feisty main restricted universe multiverse\n    ```\n5. 更新源\n    ```\n    $ sudo apt-get update\n    ```\n6. 更新软件\n    ```\n    $ sudo apt-get dist-upgrade\n    ```\n7. 常见的修复安装命令\n    ```\n    $ sudo apt-get -f install\n    ```\n\n## Python\n主要是`Python`和相关依赖包的安装，使用以下指令可导出已安装的依赖包\n```\n$ pip freeze > requirements.txt\n```\n\n并使用指令安装到树莓派\n```\n$ pip install -r requirements.txt\n```\n\n注意`pip`更新\n```\npython -m pip install --upgrade pip\n```\n\n最新版本会报错\n```\nImportError: cannot import name main\n```\n\n修改文件`/usr/bin/pip`\n```\nfrom pip import main\nif __name__ == '__main__':\n    sys.exit(main())\n```\n\n改为\n```\nfrom pip import __main__\nif __name__ == '__main__':\n    sys.exit(__main__._main())\n```\n\n---\n~~成功!!!~~\n失败了，笑脸:-)，手动安装吧。。。\n\n- 部分包可使用`pip3`\n    ```\n    $ pip3 install numpy\n    $ pip3 install pandas\n    $ pip3 install sklearn\n    ```\n\n    > 若需要权限，加入`--user`\n\n- 部分包用`apt-get`，但是优先安装到`Python2.7`版本，笑脸:-)\n    ```\n    $ sudo apt-get install python-scipy\n    $ sudo apt-get install python-matplotlib\n    $ sudo apt-get install python-opencv   \n    ```\n\n- 部分从`PIPY`下载`.whl`或`.tar.gz`文件\n    > [PyPI – the Python Package Index · PyPI](https://pypi.org/)\n    > - tensorboardX-1.4-py2.py3-none-any.whl\n    > - visdom-0.1.8.5.tar.gz\n\n    安装指令为\n    ```\n    $ pip3 install xxx.whl\n    ```\n\n    ```\n    $ tar -zxvf xxx.tar.gz\n    $ python setup.py install\n    ```\n\n- `Pytorch`源码安装\n    > [pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration ](https://github.com/pytorch/pytorch)\n\n    安装方法[Installation - From Source](https://github.com/pytorch/pytorch#from-source)\n\n    需要用到`miniconda`，安装方法如下，注意中间回车按慢一点，有两次输入。。。。。(行我慢慢看条款不行么。。笑脸:-))\n    - 第一次是是否同意条款，`yes`\n    - 第二次是添加到环境变量，`yes`，否则自己修改`/home/pi/.bashrc`添加到环境变量\n    ```\n    $ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh\n    $ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5\n    $ sudo /bin/bash Miniconda3-latest-Linux-armv7l.sh \n    # -> change default directory to /home/pi/miniconda3\n    $ sudo nano /home/pi/.bashrc \n    # -> add: export PATH=\"/home/pi/miniconda3/bin:$PATH\"\n    $ sudo reboot -h now\n    \n    $ conda \n    $ python --version\n    $ sudo chown -R pi miniconda3\n    ```\n\n    ~~然后就可以安装了~~没有对应版本的`mkl`，笑脸:-)\n    ```\n    export CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n    \n    # Disable CUDA\n    export NO_CUDA=1\n\n    # Install basic dependencies\n    conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing\n    conda install -c mingfeima mkldnn\n\n    # Install Pytorch\n    git clone --recursive https://github.com/pytorch/pytorch\n    cd pytorch\n    python setup.py install\n    ```\n\n- `tensorflow`\n    安装tensorflow需要的一些依赖和工具\n    ```\n    $ sudo apt-get update\n\n    # For Python 2.7\n    $ sudo apt-get install python-pip python-dev\n\n    # For Python 3.3+\n    $ sudo apt-get install python3-pip python3-dev\n    ```\n\n    安装`tensorflow`\n    > 若下载失败，手动打开下面网页下载`.whl`包\n\n    ```\n    # For Python 2.7\n    $ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp27-none-linux_armv7l.whl\n    $ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl\n\n    # For Python 3.4\n    $ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl\n    $ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl\n    ```\n\n    卸载，重装mock\n    ```\n    # For Python 2.7\n    $ sudo pip uninstall mock\n    $ sudo pip install mock\n\n    # For Python 3.3+\n    $ sudo pip3 uninstall mock\n    $ sudo pip3 install mock\n    ```\n\n    安装的版本`tensorflow v1.1.0`没有`models`，因为1.0版本以后models就被`Sam Abrahams`独立出来了，例如`classify_image.py`就在`models/tutorials/image/imagenet/`里\n    > [tensorflow/models](https://github.com/tensorflow/models)\n\n## 其余\n1. 输入法 \n   ```\n   $ sudo apt-get install fcitx fcitx-googlepinyin \n   $ fcitx-module-cloudpinyin fcitx-sunpinyin\n   ```\n2. `git`\n   ```\n   $ sudo apt-get install git\n   ```\n\n   配置`git`和`ssh`\n   ```\n   $ git config --global user.name \"Louis Hsu\"\n   $ git config --global user.email is.louishsu@foxmail.com\n\n   $ ssh-keygen -t rsa -C \"is.louishsu@foxmail.com\"\n   $ cat ~/.ssh/id_rsa.pub  # 添加到github\n   ```\n\n","tags":["Linux"],"categories":["Linux"]},{"title":"Underfitting & Overfitting","url":"/2018/10/26/Underfitting-Overfitting/","content":"\n# 原因分析\n放上一张非常经典的图，以下分别表示二分类模型中的欠拟合(underfit)、恰好(just right)、过拟合(overfit)，来自吴恩达课程笔记。\n![underfit_justright_overfit](/Underfitting-Overfitting/underfit_justright_overfit.png)\n\n- 欠拟合的成因大多是模型不够复杂、拟合函数的能力不够；\n- 过拟合成因是给定的数据集相对过于简单，使得模型在拟合函数时过分地考虑了噪声等不必要的数据间的关联，或者说相对于给定数据集，模型过于复杂、拟合能力过强。\n\n# 判别方法\n## 学习曲线\n可通过学习曲线`(Learning curve)`进行欠拟合与过拟合的判别。\n\n学习曲线就是通过画出**不同训练集大小**时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。\n\n## 绘制\n横轴为训练样本的数量，纵轴为损失或其他[评估准则]()。\n`sklearn`中学习曲线绘制例程如下\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndigits = load_digits(); X, y = digits.data, digits.target\n\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = GaussianNB()\ntrain_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5))\n\nplt.figure()\nplt.title(\"Learning Curves (Naive Bayes)\")\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"Score\")\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.fill_between(train_sizes, \n                train_scores_mean - train_scores_std,\n                train_scores_mean + train_scores_std,\n                alpha=0.1, color=\"r\")\nplt.fill_between(train_sizes,\n                test_scores_mean - test_scores_std,\n                test_scores_mean + test_scores_std,\n                alpha=0.1, color=\"g\")\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean,  'o-', color=\"g\", label=\"Cross-validation score\")\n\nplt.grid(); plt.legend(loc=\"best\")\n\nplt.show()\n```\n\n![learning_curve_nb](/Underfitting-Overfitting/learning_curve_nb.png)\n\n## 判别\n- **欠拟合**，即高偏差`(high bias)`，训练集和测试集的误差收敛但却很高；\n- **过拟合**，即高方差`(high variance)`，训练集和测试集的误差之间有大的差距。\n\n\n![learning_curve](/Underfitting-Overfitting/learning_curve.png)\n\n\n# 欠拟合解决方法\n- 增加迭代次数继续训练\n- 增加模型复杂度\n- 增加特征\n- 减少正则化程度\n- 采用Boosting等集成方法\n\n此时增加数据集并不能改善欠拟合问题。\n\n# 过拟合解决方法\n- 提前停止训练\n- 获取更多样本或数据扩增\n    - 重采样\n    - 上采样\n    - 增加随机噪声\n    - `GAN`\n    - 图像数据的空间变换（平移旋转镜像）\n    - 尺度变换（缩放裁剪）\n    - 颜色变换\n    - 改变分辨率\n    - 对比度\n    - 亮度\n- 降低模型复杂度\n- 减少特征\n- 增加正则化程度\n- 神经网络可采用`Dropout`\n- 多模型投票方法","categories":["Machine Learning","Deep Learning"]},{"title":"Cross Validation & Hyperparameter","url":"/2018/10/26/Cross-Validation-Hyperparameter/","content":"\n# 交叉验证与超参数选择\n\n## 交叉验证\n以下简称交叉验证`(Cross Validation)`为`CV`.`CV`是用来验证分类器的性能一种统计分析方法,基本思想是把在某种意义下将原始数据`(dataset)`进行分组,一部分做为训练集`(train set)`,另一部分做为验证集`(validation set)`,首先用训练集对分类器进行训练,在利用验证集来测试训练得到的模型`(model)`,以此来做为评价分类器的性能指标。\n\n### 交叉验证的几种方法\n\n- k折交叉验证(K-fold)\n    1. 将全部训练集$S$分成$k$个不相交的子集，假设$S$中的训练样例个数为$m$，则每个子集中有$(\\frac{m}{k})$个训练样例，相应子集称作$\\{s_1, s_2, ..., s_k\\}$；\n    2. 每次从分好的子集中，拿出$1$个作为测试集，其他$k-1$个作为训练集；\n    3. 在$k-1$个训练集上训练出学习器模型，将模型放到测试集上，得到分类率；\n    4. 计算k次求得的分类率平均值，作为该模型或者假设函数的真实分类率\n    ![k-fold](/Cross-Validation-Hyperparameter/k-fold.jpg)\n\n- 留一法交叉验证(Leave One Out - LOO)\n    假设有$N$个样本，将每个样本作为测试样本，其他$(N-1)$个样本作为训练样本。这样得到$N$个分类器，$N$个测试结果。用这$N$个结果的平均值衡量模型的性能。\n\n- 留P法交叉验证(Leave P Out - LPO)\n    将$P$个样本作为测试样本，其他$(N-P)$个样本作为训练样本。这样得到$\\left(\\begin{matrix}\n        P \\\\ N\n    \\end{matrix}\\right)$个训练测试对。当$P＞1$时，测试集会发生重叠。当$P=1$时，变成$LOO$。\n    ![LPO](/Cross-Validation-Hyperparameter/LPO.jpg)\n\n### `scikit-learn`中的交叉验证\n![cross_validation_sklearn](/Cross-Validation-Hyperparameter/cross_validation_sklearn.png)\n\n- K-fold\n    ```\n    >>> import numpy as np\n    >>> from sklearn.model_selection import KFold\n    >>> X = [\"a\", \"b\", \"c\", \"d\"]\n    >>> kf = KFold(n_splits=2)\n    >>> for train, test in kf.split(X):\n    ... print(\"%s %s\" % (train, test))\n    [2 3] [0 1]\n    [0 1] [2 3]\n    ```\n\n- Leave One Out (LOO)\n    ```\n    >>> from sklearn.model_selection import LeaveOneOut\n    >>> X = [1, 2, 3, 4]\n    >>> loo = LeaveOneOut()\n    >>> for train, test in loo.split(X):\n    ... print(\"%s %s\" % (train, test))\n    [1 2 3] [0]\n    [0 2 3] [1]\n    [0 1 3] [2]\n    [0 1 2] [3]\n    ```\n- Leave P Out (LPO)\n    ```\n    >>> from sklearn.model_selection import LeavePOut\n    >>> X = np.ones(4)\n    >>> lpo = LeavePOut(p=2)\n    >>> for train, test in lpo.split(X):\n    ... print(\"%s %s\" % (train, test))\n    [2 3] [0 1]\n    [1 3] [0 2]\n    [1 2] [0 3]\n    [0 3] [1 2]\n    [0 2] [1 3]\n    [0 1] [2 3]\n    ```\n\n## 使用交叉验证调整超参数\n\n超参数的定义：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。\n超参数例如\n- 模型（`SVM`，`Softmax`，`Multi-layer Neural Network`,...)；\n- 迭代算法（`Adam`, `SGD`, ...)(不同的迭代算法还有各种不同的超参数，如`beta1`,`beta2`等等，但常见的做法是使用默认值，不进行调参）；\n- 学习率（`learning rate`)；\n- 正则化方程的选择(`L0`,`L1`,`L2`)，正则化系数；\n- `dropout`的概率\n- ...\n\n\n### 确定调节范围\n超参数的种类多，调节范围大，需要先进行简单的测试确定调参范围。\n\n- 模型选择\n    模型的选择很大程度上取决于具体的实际问题，但必须通过几项基本测试。 \n    - 可以通过第一个epoch的loss，观察模型能否无BUG运行，注意此过程需要设置正则项系数为0，因为正则项引入的loss难以估算。 \n    - 模型必须可以对于小数据集过拟合，否则应该尝试其他或者更复杂的模型。\n    - 若训练集与验证集loss均较大，则应该尝试其他或者更复杂的模型。\n\n    > 模型选择的方法为：\n    > 1. 使用训练集训练出 10 个模型\n    > 2. 用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）\n    > 3. 选取代价函数值最小的模型\n    > 4. 用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）\n    > <p align=\"right\"> —— Andrew Ng, Stanford University </p>\n\n- 学习率\n    - loss基本不变：学习率过低 \n    - loss波动明显或者溢出：学习率过高 \n\n- 正则项系数\n    - val_acc与acc相差较大：正则项系数过小 \n    - loss逐渐增大：正则项系数过大 \n\n### 超参数的确定\n-  先粗调，再细调\n    先通过数量少，间距大的粗调确定细调的大致范围。然后在小范围内部进行间距小，数量大的细调。\n\n- 尝试在对数空间内进行调节\n    即在对数空间内部随机生成测试参数，而不是在原空间生成，通常用于学习率以及正则项系数等的调节。出发点是该超参数的指数项对于模型的结果影响更显著；而同阶的数据之间即便原域相差较大，对于模型结果的影响反而不如不同阶的数据差距大。\n\n- 超参数搜索\n    随机搜索参数值，而不是网格搜索。\n\n### 超参数搜索\n`scikit-learn`提供超参数搜索方法，可参考官方文档\n- 网格搜索\n    [3.2.1. Exhaustive Grid Search](http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#exhaustive-grid-search)\n    调用例程如下\n    ```\n    import numpy as np\n\n    from time import time\n    from scipy.stats import randint as sp_randint\n\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.datasets import load_digits\n    from sklearn.ensemble import RandomForestClassifier\n\n    # get some data\n    digits = load_digits()\n    X, y = digits.data, digits.target\n\n    # build a classifier\n    clf = RandomForestClassifier(n_estimators=20)\n\n\n    # Utility function to report best scores\n    def report(results, n_top=3):\n        for i in range(1, n_top + 1):\n            candidates = np.flatnonzero(results['rank_test_score'] == i)\n            for candidate in candidates:\n                print(\"Model with rank: {0}\".format(i))\n                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                    results['mean_test_score'][candidate],\n                    results['std_test_score'][candidate]))\n                print(\"Parameters: {0}\".format(results['params'][candidate]))\n                print(\"\")\n\n    # use a full grid over all parameters\n    param_grid = {\"max_depth\": [3, None],\n                \"max_features\": [1, 3, 10],\n                \"min_samples_split\": [2, 3, 10],\n                \"min_samples_leaf\": [1, 3, 10],\n                \"bootstrap\": [True, False],\n                \"criterion\": [\"gini\", \"entropy\"]}\n\n    # run grid search\n    grid_search = GridSearchCV(clf, param_grid=param_grid)\n    start = time()\n    grid_search.fit(X, y)\n\n    print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n        % (time() - start, len(grid_search.cv_results_['params'])))\n    report(grid_search.cv_results_)\n    ```\n    \n- 随机搜索\n    [3.2.2. Randomized Parameter Optimization](http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#randomized-parameter-optimization)\n    调用例程如下\n    ```\n    import numpy as np\n\n    from time import time\n    from scipy.stats import randint as sp_randint\n\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.datasets import load_digits\n    from sklearn.ensemble import RandomForestClassifier\n\n    # get some data\n    digits = load_digits()\n    X, y = digits.data, digits.target\n\n    # build a classifier\n    clf = RandomForestClassifier(n_estimators=20)\n\n\n    # Utility function to report best scores\n    def report(results, n_top=3):\n        for i in range(1, n_top + 1):\n            candidates = np.flatnonzero(results['rank_test_score'] == i)\n            for candidate in candidates:\n                print(\"Model with rank: {0}\".format(i))\n                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                    results['mean_test_score'][candidate],\n                    results['std_test_score'][candidate]))\n                print(\"Parameters: {0}\".format(results['params'][candidate]))\n                print(\"\")\n\n\n    # specify parameters and distributions to sample from\n    param_dist = {\"max_depth\": [3, None],\n                \"max_features\": sp_randint(1, 11),\n                \"min_samples_split\": sp_randint(2, 11),\n                \"min_samples_leaf\": sp_randint(1, 11),\n                \"bootstrap\": [True, False],\n                \"criterion\": [\"gini\", \"entropy\"]}\n\n    # run randomized search\n    n_iter_search = 20\n    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                    n_iter=n_iter_search)\n\n    start = time()\n    random_search.fit(X, y)\n    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n        \" parameter settings.\" % ((time() - start), n_iter_search))\n    report(random_search.cv_results_)\n    ```","categories":["Machine Learning"]},{"title":"Spam Classification","url":"/2018/10/26/Spam-Classification/","content":"\n> 踩坑？？？全部给我踩平！！！\n\n来自[LintCode垃圾短信分类](https://www.lintcode.com/ai/spam-message-classification/overview)\n[@Github: spam or ham](https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/tree/master/spam%20or%20ham)\n\n# 垒代码\n## 预处理及向量化\n观察各文本后，发现各文本中包含的单词多种多样，包含标点、数字等，例如\n```\n- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n- XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. \n- 07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. \n```\n\n且按空格分词后，部分单词中仍包含`whitespace`，故选择的预处理方案是，**去除分词后文本中的标点、数字、空格等，并将单词中字母全部转为小写**。\n> 中文分词可采用`jieba`(街霸？)\n\n预处理后，按当前的文本内容建立字典，并统计各样本的词数向量，详细代码如下\n```\nclass Words2Vector():\n    '''\n    建立字典，将输入的词列表转换为向量，表示各词出现的次数\n    '''\n    def __init__(self):\n        self.dict = None\n        self.n_word = None\n    def fit_transform(self, words):\n        self.fit(words)\n        return self.transform(words)\n    def fit(self, words):\n        \"\"\"\n        @param {list[list[str]]} words\n        \"\"\"\n        words = _flatten(words)                                                 # 展开为1维列表\n        words = self.filt(words)                                                # 滤除空格、数字、标点\n\n        self.word = list(set(words))                                            # 去重\n        self.n_word = len(set(words))                                           # 统计词的个数\n        self.dict = dict(zip(self.word, [_ for _ in range(self.n_word)]))       # 各词在字典中的位置\n    def transform(self, words):\n        \"\"\"\n        @param {list[list[str]]} words\n        @return {ndarray} retarray: vector\n        \"\"\"\n        retarray = np.zeros(shape=(len(words), self.n_word))                    # 返回的词数向量\n        for i in range(len(words)):\n            words[i] = self.filt(words[i])                                      # 滤除空格、数字、标点\n        for i in range(len(words)):\n            for w in words[i]:\n                if w in self.word:                                              # 是否在训练集生成的字典中\n                    retarray[i, self.dict[w]] += 1                              # 查询字典，找到对应特征的下标\n        return retarray\n    def filt(self, flattenWords):\n        retWords = []\n        en_stops = set(stopwords.words('english'))                              # 停用词列表\n        for word in flattenWords:\n            word = word.translate(str.maketrans('', '', string.whitespace))     # 去除空白\n            word = word.translate(str.maketrans('', '', string.punctuation))    # 去除标点\n            word = word.translate(str.maketrans('', '', string.digits))         # 去除数字\n            if word not in en_stops and (len(word) > 1):                        # 删除停用词，并除去长度小于等于2的词\n                retWords.append(word.lower())\n        return retWords\n```\n\n## TF-IDF方法\n由词数向量可计算词频，但只用词频忽略了各文本在不同文档中的重要程度，关于`TF-IDF`，在[另一篇博文](https://louishsu.xyz/2018/10/25/TF-IDF/)中详细说明。\n\n由于剔除了停用词等，部分向量不包含任何内容，即词数向量为$\\vec{0}$，这时计算词频和单位化时，会出现`nan`的运算结果，故只对非空向量进行计算。\n\n训练后需要保存的是`IDF`向量，`TF`向量在新样本输入后重新计算，故无需保存。\n\n```\nclass TfidfVectorizer():\n    def __init__(self):\n        self.idf = None\n    def fit_transform(self, num_vec):\n        self.fit(num_vec)\n        return self.transform(num_vec)\n    def fit(self, num_vec):\n        \"\"\"\n        @param {ndarray}: num_vec, shape(N_sample, N_feature)\n        \"\"\"\n        num_vec[num_vec>0] = 1\n        n_doc = num_vec.shape[0]\n        n_term = np.sum(num_vec, axis=0)    # 各词出现过的文档次数\n        self.idf = np.log((n_doc + 1) / (n_term + 1)) + 1\n        return self.idf\n    def transform(self, num_vec):\n        \"\"\"\n        @param {ndarray}: num_vec, shape(N_sample, N_feature)\n        \"\"\"\n        # 求解词频向量，由于部分向量为空，故下句会出现问题\n        # tf = num_vec / np.sum(num_vec, axis=1).reshape(-1, 1) => nan\n        # 解决方法：只对非空向量进行词频计算\n        tf = np.zeros(shape=num_vec.shape)\n        n_terms = np.sum(num_vec, axis=1); idx = (n_terms!=0)\n\n        tf[idx] = num_vec[idx] / n_terms[idx].reshape(-1, 1)            # 计算词频，只对非空向量进行\n        \n        tfidf = tf * self.idf\n        tfidf[idx] /= np.linalg.norm(tfidf, axis=1)[idx].reshape(-1, 1) # 单位化，只对非空向量进行\n        \n        return tfidf\n```\n\n## 贝叶斯决策\n各文本向量化后，就可通过机器学习算法进行模型的训练和预测，这里采用的是贝叶斯决策的方法，需要注意的有以下几点\n- 似然函数$p(x|c_k)$与[贝叶斯决策](https://louishsu.xyz/2018/10/18/Bayes-Decision/)文中例不同，这里宜采用高斯分布作为分布模型；\n- 按朴素贝叶斯计算$p(x|c_k)$，但注意此处不能将各维特征单独训练$1$维高斯分布模型，然后计算预测样本似然函数值时进行累乘，如下\n$$\np(x|c_k) = \\prod_{j=1}^{N_feature} p(x_j|c_k)\n$$\n因为特征维度特别高，各个特征单独用$1$维高斯分布描述，累乘计算会下溢，故这里采用多元高斯分布\n$$\np(x|c_k) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma_k|^{\\frac{1}{2}}} · \ne^{-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)}\n$$\n    - 且经主成分分析后，各维度间线性相关性降低，故假定\n$$\n\\Sigma_k = diag\\{\\sigma_{k1}, ..., \\sigma_{kn}\\}\n$$\n    - 但分母$(2\\pi)^{\\frac{n}{2}}|\\Sigma_k|^{\\frac{1}{2}}$在计算时不稳定，且各特征标准差大小相差无几，故这里假定\n$$\n\\Sigma_k = I\n$$\n\n    - 最终简化后的似然函数计算方法为\n$$\np(x|c_k) =  \ne^{-\\frac{1}{2} (x - \\mu_k)^T (x - \\mu_k)}\n$$\n\n### 贝叶斯决策模型训练\n\n基于上述假设，只需训练多元高斯分布的各维均值$\\mu_j$\n\n```\ndef fit(self, labels, text):\n    \"\"\"\n    @param {ndarray} labels: shape(N_samples, ), labels[i] \\in {0, 1}\n    @param {list[list[str]]} words\n        \"\"\"\n    labels = self.encodeLabel(labels); words = self.text2words(self.clean(text))\n\n    vecwords = self.numvectorizer.fit_transform(words)              # 向量化\n    vecwords = self.tfidfvectorizer.fit_transform(vecwords)         # tfidf, shape(N_samples, N_features)\n\n    isnotEmpty = (np.sum(vecwords, axis=1)!=0)                      # 去掉空的样本\n    vecwords = vecwords[isnotEmpty]; labels = labels[isnotEmpty]\n\n    # vecwords = self.reduce_dim.fit_transform(vecwords)              # 降维，计算量太大\n    self.n_features = vecwords.shape[1]\n\n    labels = OneHotEncoder().fit_transform(labels.reshape((-1, 1))).toarray()\n        self.priori = np.mean(labels, axis=0)                           # 先验概率\n\n    self.likelihood_mu = np.zeros(shape=(2, vecwords.shape[1]))\t    # 设似然函数p(x|c)为高斯分布\n    for i in range(2):\n        vec = vecwords[labels[:, i]==1]\n        self.likelihood_mu[i] = np.mean(vec, axis=0)\n```\n\n###贝叶斯决策模型预测\n决策函数为\n$$\nif　p(x|c_i)P(c_i) > p(x|c_j)P(c_j),　then　x \\in c_i\n$$\n\n但实际效果显示，等先验概率$P(c_j)$结果更好$(???)$\n\n```\ndef multigaussian(self, x, mu):\n    \"\"\" 简化\n    \"\"\"\n    x = x - mu\n    a = np.exp(-0.5 * x.T.dot(x))\n    return a\ndef predict(self, text):\n    \"\"\"\n    @param {list[list[str]]} words\n    @note:\n                      p(x|c)P(c)\n            P(c|x) = ------------\n                         p(x)\n    \"\"\"\n    pred_porba = np.ones(shape=(len(self.clean(text)), 2))      \n        \n    words = self.text2words(text)\n    vecwords = self.tfidfvectorizer.transform(\n                                self.numvectorizer.transform(words))    # 向量化\n\n    for i in range(vecwords.shape[0]):\n        for c in range(2):\n            # pred_porba[i, c] = self.priori[c] * self.multigaussian(vecwords[i], self.likelihood_mu[c])\n            pred_porba[i, c] = self.multigaussian(vecwords[i], self.likelihood_mu[c])\n\n    pred = np.argmax(pred_porba, axis=1)\n    return self.decodeLabel(pred)\n```\n\n# 调包\n主要用到了`scikit-learn`机器学习包以下几个功能\n- `sklearn.feature_extraction.text.TfidfVectorizer()`\n- `sklearn.decomposition.PCA()`\n- `sklearn.naive_bayes.BernoulliNB()`\n\n最终准确率在$97\\%$左右，代码比较简单，不进行说明。\n\n> 采用`sklearn.linear_model import.LogisticRegressionCV()`效果更佳\n\n\n```\ndef main():\n    trainfile = \"./data/train.csv\"\n    testfile = \"./data/test.csv\"\n    \n    # 读取原始数据\n    data_train = pd.read_csv(trainfile, names=['Label', 'Text'])\n    txt_train  = list(data_train['Text'])[1: ]; label_train = list(data_train['Label'])[1: ]\n    drop(txt_train)                                             # 删除数字和标点\n    txt_test   = list(pd.read_csv(testfile, names=['Text'])['Text'])[1: ]\n    drop(txt_test)                                              # 删除数字和标点\n\n    # 训练\n    vectorizer = TfidfVectorizer(stop_words='english')          # 删除英文停用词\n    vec_train = vectorizer.fit_transform(txt_train).toarray()   # 提取文本特征向量\n    # reduce_dim = PCA(n_components = 4096)\n    # vec_train = reduce_dim.fit_transform(vec_train)\n    estimator = BernoulliNB()\n    estimator.fit(vec_train, label_train)                       # 训练朴素贝叶斯模型\n\n    # 测试\n    label_train_pred = estimator.predict(vec_train)\n    acc = np.mean((label_train_pred==label_train).astype('float'))\n    \n    # 预测\n    vec_test = vectorizer.transform(txt_test).toarray()\n    # vec_test = reduce_dim.transform(vec_test)\n    label_test_pred = estimator.predict(vec_test)\n    with open('./data/sampleSubmission.txt', 'w') as f:\n        for i in range(label_test_pred.shape[0]):\n            f.write(label_test_pred[i] + '\\n')\n```","categories":["Machine Learning"]},{"title":"TF-IDF","url":"/2018/10/25/TF-IDF/","content":"\n# 引言\n正在做[LintCode](https://www.lintcode.com/)上的垃圾邮件分类，使用[朴素贝叶斯](https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/)方法解决，涉及到文本特征的提取。\nTF-IDF（词频-逆文档频率）算法是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n\n# 计算步骤\n## 词频(TF)\n`Term Frequency`，就是某个关键字出现的频率，具体来讲，就是词库中的**某个词**在**当前文章**中出现的频率。那么我们可以写出它的计算公式：\n$$\nTF_{ij} = \\frac{n_{ij}}{\\sum_k n_{i, k}}\n$$\n\n其中，$n_{ij}$表示关键词$j$在文档$i$中的出现次数。\n\n单纯使用TF来评估关键词的重要性忽略了常用词的干扰。常用词就是指那些文章中大量用到的，但是不能反映文章性质的那种词，比如：因为、所以、因此等等的连词，在英文文章里就体现为and、the、of等等的词。这些词往往拥有较高的TF，所以仅仅使用TF来考察一个词的关键性，是不够的。\n\n## 逆文档频率(IDF)\n`Inverse Document Frequency`，文档频率就是一个词在整个文库词典中出现的频率，逆文档频率用下式计算\n$$\nIDF_j = \\log \\frac{|D|}{|D_j| + 1}\n$$\n\n其中，$|D|$表示总的文档数目，$|D_j|$表示关键词$j$出现过的文档数目\n\n`scikit-learn`内为\n$$\nIDF_j = \\log \\frac{|D| + 1}{|D_j| + 1} + 1\n$$\n\n![sklearn_tfidf](/TF-IDF/sklearn.jpg)\n\n## 词频-逆文档频率(TF-IDF)\n$$\nTF-IDF_{i} = TF_i × IDF\n$$\n\n# 举例\n例如有如下$3$个文本\n```\n文本1：My dog ate my homework.\n文本2：My cat ate the sandwich.\n文本3：A dolphin ate the homework.\n```\n\n提取字典，一般需要处理大小写、去除停用词`a`，处理结果为\n```\nate, cat, dog, dolphin, homework, my, sandwich, the\n```\n\n故各个文本的词数向量为\n```\n文本1：[1, 0, 1, 0, 1, 2, 0, 0]\n文本2：[1, 1, 0, 0, 0, 1, 1, 1]\n文本3：[1, 0, 0, 1, 1, 0, 0, 1]\n```\n\n各个文本的词频向量(TF)\n```\n文本1：[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ]\n文本2：[0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ]\n文本3：[0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]\n```\n\n各词出现过的文档次数\n```\n[3, 1, 1, 1, 2, 2, 1, 2]\n```\n\n总文档数为$3$，各词的逆文档频率(IDF)向量\n> 这里使用`scikit-learn`内的方法求解\n\n```\n[1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207,  1.28768207, 1.69314718, 1.28768207]\n```\n\n故各文档的TF-IDF向量为\n```\n文本1：\n[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ]\n文本2：\n[0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641]\n文本3：\n[0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]\n```\n\n经单位化后，有\n```\n文本1：\n[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ]\n文本2：\n[0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178]\n文本3：\n[0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]\n```\n\n```\n>>> import numpy as np\n>>> vec_num = np.array([\n\t[1, 0, 1, 0, 1, 2, 0, 0],\n\t[1, 1, 0, 0, 0, 1, 1, 1],\n\t[1, 0, 0, 1, 1, 0, 0, 1]\n\t])\n>>> vec_tf = vec_num / np.sum(vec_num, axis=1).reshape(-1, 1)\n>>> vec_tf\narray([[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ],\n       [0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ],\n       [0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]])\n\n>>> vec_num[vec_num>0] = 1\n>>> n_showup = np.sum(vec_num, axis=0)\n>>> n_showup\narray([3, 1, 1, 1, 2, 2, 1, 2])\n\n>>> d = 3\n>>> vec_idf = np.log((d + 1) / (n_showup + 1)) + 1\n>>> vec_idf\narray([1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207])\n\n>>> vec_tfidf = vec_tf * vec_idf\n>>> vec_tfidf\narray([[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ],\n       [0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641],\n       [0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]])\n\n>>> vec_tfidf = vec_tfidf / np.linalg.norm(vec_tfidf, axis=1).reshape((-1, 1))\n>>> vec_tfidf\narray([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805, 0.73861611, 0.        , 0.        ],\n       [0.31544415, 0.53409337, 0.        , 0.        , 0.        , 0.40619178, 0.53409337, 0.40619178],\n       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 , 0.        , 0.        , 0.4804584 ]])\n```\n\n# 验证\n\n使用`scikit-learn`机器学习包计算结果\n```\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> vectorizer = TfidfVectorizer()\n>>> text = [\n\t\"My dog ate my homework\",\n\t\"My cat ate the sandwich\",\n\t\"A dolphin ate the homework\"]\n>>> vectorizer.fit_transform(text).toarray()\narray([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ],\n       [0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178],\n       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]])\n>>> vectorizer.get_feature_names()\n['ate', 'cat', 'dog', 'dolphin', 'homework', 'my', 'sandwich', 'the']\n```","categories":["Practice"]},{"title":"SVD","url":"/2018/10/23/SVD/","content":"\n# 引言\n奇异值分解`Singular Value Decomposition`是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。\n\n# 原理\n## 从特征值分解(EVD)讲起\n我们知道对于一个$n$阶方阵$A_{n×n}$，有\n$$\nA\\alpha_i = \\lambda_i \\alpha_i　i = 1, ..., n\n$$\n\n取\n$$\nP = \\left[\\alpha_1, \\alpha_2, ..., \\alpha_n\\right]\n$$\n\n有下式成立\n$$\nAP = P\\Lambda\n$$\n\n其中\n$$\n\\Lambda = \\left[\n        \\begin{matrix}\n            \\lambda_1 & & \\\\\n            & ... & \\\\\n            & & \\lambda_n \\\\\n        \\end{matrix}\n\\right]\n$$\n\n> 特征值一般从大到小排列\n\n利用该式可将方阵$A_{n×n}$化作对角阵$\\Lambda_{n×n}$\n$$\n\\Lambda = P^{-1}AP\n$$\n\n或者\n$$\nA = P \\Lambda P^{-1} = \\sum_{i=1}^n \\lambda_i (P_{,i})(P_{,i})^{-1}\n$$\n\n> “$_{i}$”表示第$i$行，“$_{,i}$”表示第$i$列\n\n这样我们就可以理解为，矩阵$A$是由$n$个$n$阶矩阵$P_{,i}P^{-1}_{i}$加权组成，特征值$\\lambda_i$即为权重。\n\n> 以上为个人理解，不妥之处可以指出。\n\n## 奇异值分解(SVD)\n### 定义\n对于长方阵$A_{m×n}$，不能进行特征值分解，可进行如下分解\n$$\nA_{m×n} = U_{m×m} \\Sigma_{m×n} V_{n×n}^T\n$$\n\n其中$U \\in \\mathbb{R}^{m×m}, V \\in \\mathbb{R}^{n×n}$，均为正交矩阵。矩阵$\\Sigma_{m×n}$如下\n- 对于$m>n$\n    $$\n    \\Sigma_{m×n} = \\left[\n            \\begin{matrix}\n                S_{n×n} \\\\\n                --- \\\\\n                O_{(m-n)×n}\n            \\end{matrix}\n    \\right]\n    $$\n\n- 对于$m<n$\n    $$\n    \\Sigma_{m×n} = \\left[\n            \\begin{matrix}\n                S_{m×m} & | & O_{m×(n-m)}\n            \\end{matrix}\n    \\right]\n    $$\n\n矩阵$S_{n×n}$为对角阵，对角元素从大到小排列\n$$\nS_{n×n} = \\left[\n    \\begin{matrix}\n        \\sigma_1 & & \\\\\n         & ... & \\\\\n         & & \\sigma_n\\\\\n    \\end{matrix}\n\\right]\n$$\n\n直观表示`SVD`分解如下\n![直观表示SVD](/SVD/直观表示SVD.jpg)\n\n当取$r<n$时，有部分奇异值分解，可用于降维\n$$\nA_{m×n} = U_{m×r} \\Sigma_{r×r} V_{r×n}^T\n$$\n\n### 计算\n\n> 以下仅考虑$m>n$的情况\n\n\n1. 令矩阵$A^T$与$A$相乘，有\n    $$\n    A^TA = (U \\Sigma V^T)^T (U \\Sigma V^T)\n    $$\n\n    $$\n    = V \\Sigma^T U^T U \\Sigma V^T\n    $$\n\n    $$\n    A^TA = V \\Sigma^T \\Sigma V^T\n    $$\n\n    > 矩阵$U$为正交阵，即满足$U^TU=I$\n\n    其中\n    $$\n    \\Sigma^T \\Sigma = \n            \\left[\n                \\begin{matrix}\n                    S^T_{n×n} & | & O^T_{n×(m-n)}\n                \\end{matrix}\n            \\right]\n            \\left[\n                \\begin{matrix}\n                    S_{n×n} \\\\\n                    --- \\\\\n                    O_{(m-n)×n}\n                \\end{matrix}\n            \\right]\n    $$\n\n    $$\n    = S_{n×n}^2 \n    = \\left[\n        \\begin{matrix}\n            \\sigma_1^2 & & \\\\\n            & ... & \\\\\n            & & \\sigma_n^2\\\\\n        \\end{matrix}\n    \\right]\n    $$\n\n    则\n    $$\n    A^T A = V S^2  V^T\n    $$\n\n    即矩阵$A^T A$相似对角化为$S^2$，对角元素$\\sigma_i^2$与矩阵$V$的列向量$v_i(i=1, ..., n)$为矩阵$A^T A$的特征对。\n\n    那么对矩阵$A^T A$进行特征值分解，有\n    $$\n    (A^T A) \\alpha^{(1)}_i = \\lambda^{(1)}_i \\alpha^{(1)}_i\n    $$\n\n    则\n    $$\n    v_i = \\alpha^{(1)}_i　\\sigma_i = \\sqrt{\\lambda^{(1)}_i}\n    $$\n\n    > 注：对于二次型$x^T (A^T A) x$\n    > $$\n    > x^T (A^T A) x = (Ax)^T(Ax) \\geq 0\n    > $$\n    > \n    > 故矩阵$A^T A$半正定，$\\sigma_i = \\sqrt{\\lambda_i}$有解\n\n\n2. 同理，令矩阵$A$与$A^T$相乘，可证得\n    $$\n    A A^T = U \\Sigma \\Sigma^T U^T\n    $$\n\n    其中\n    $$\n    \\Sigma \\Sigma^T = \n            \\left[\n                \\begin{matrix}\n                    S_{n×n} \\\\\n                    --- \\\\\n                    O_{(m-n)×n}\n                \\end{matrix}\n            \\right]\n            \\left[\n                \\begin{matrix}\n                    S^T_{n×n} & | & O^T_{n×(m-n)}\n                \\end{matrix}\n            \\right]\n    $$\n\n    $$\n    = \\left[\n        \\begin{matrix}\n            S^2_{n×n} & O_{n×(m-n)} \\\\\n            O_{(m-n)×n} & O_{(m-n)×(m-n)}\n        \\end{matrix}\n    \\right]\n    $$\n\n    即矩阵$A A^T$相似对角化，对角元素$\\sigma_i^2$与矩阵$U$的列向量$u_i(i=1, ..., m)$为矩阵$A A^T$的特征对。\n\n    对矩阵$A A^T$进行特征值分解，有\n    $$\n    (A^T A) \\alpha^{(2)}_i = \\lambda^{(2)}_i \\alpha^{(2)}_i\n    $$\n\n    则\n    $$\n    u_i = \\alpha^{(2)}_i　\\sigma_i = \\sqrt{\\lambda^{(2)}_i}\n    $$\n\n    > 同理可证得$A A^T$半正定，略。\n\n\n一般来说，为减少计算量，计算奇异值分解只进行一次特征值分解，如对于矩阵$X_{m×n}(m>n)$，选取$n$阶矩阵$X^T X$进行特征值分解计算$v_i$，计算$u_i$方法下面介绍。\n\n根据前面推导，我们有特征值分解\n$$\n(A^T A) \\alpha^{(1)}_i = \\lambda^{(1)}_i \\alpha^{(1)}_i\n$$\n\n$$\n(A A^T) \\alpha^{(2)}_i = \\lambda^{(2)}_i \\alpha^{(2)}_i\n$$\n\n其中$\\lambda^{(1)}_i = \\lambda^{(2)}_i = \\sigma_i^2$，$v_i = \\alpha^{(1)}_i$，$u_i = \\alpha^{(2)}_i$，即\n$$\nA^T A v_i = \\sigma_i^2 v_i \\tag{1}\n$$\n\n$$\nA A^T u_i = \\sigma_i^2 u_i \\tag{2}\n$$\n\n$(1)$式左右乘$A$，有\n$$\nA A^T A v_i = \\sigma_i^2 A v_i\n$$\n\n发现什么？这是另一个特征值分解的表达式！\n$$\n(A A^T) (A v_i) = \\sigma_i^2 (A v_i)\n$$\n\n故\n$$\nu_i \\propto A v_i　或　\nu_i = k · A v_i \\tag{3}\n$$\n\n现在求解系数$k$，根据定义\n$$\nA = U \\Sigma V^T　\\Rightarrow　AV = U \\Sigma\n$$\n\n则\n$$\nA v_i = \\sigma_i u_i　\\Rightarrow　u_i = \\frac{1}{\\sigma_i} A v_i\n$$\n\n或者\n$$\nU = A V \\Sigma^{-1}\n$$\n\n> 注：只能求前$n$个$u_i$，之后的需要列写方程求解\n\n# 举栗\n将矩阵$A$进行分解\n$$\nA = \\left[\n    \\begin{matrix}\n        0 & 1 \\\\\n        1 & 1 \\\\\n        1 & 0\n    \\end{matrix}\n\\right]\n$$\n\n为减少计算量，取$A^T A$计算\n$$\nA^T A = \\left[\n    \\begin{matrix}\n        2 & 1 \\\\\n        1 & 2 \n    \\end{matrix}\n\\right]\n$$\n\n特征值分解，有\n$$\nA\\left[\n    \\begin{matrix}\n        \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n        \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n    \\end{matrix} \n\\right]\n= \\left[\n    \\begin{matrix}\n        \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n        \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n    \\end{matrix} \n\\right]\n\\left[\n    \\begin{matrix}\n        3 &  \\\\\n          & 1\n    \\end{matrix} \n\\right]\n$$\n\n故\n$$\n\\Sigma = \\left[\n    \\begin{matrix}\n        \\sqrt{3} &  \\\\\n          & 1\n    \\end{matrix} \n\\right]　\nV = \\left[\n    \\begin{matrix}\n        \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n        \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n    \\end{matrix} \n\\right]\n$$\n\n$$\nU = A V \\Sigma^{-1} = \\left[\n    \\begin{matrix}\n        \\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{2}} \\\\\n        \\frac{2}{\\sqrt{6}} & 0 \\\\\n        \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{2}}\n    \\end{matrix} \n\\right]\n$$\n\n```\n>>> import numpy as np\n>>> A = np.array([\n\t[0, 1], [1, 1], [1, 0]\n\t])\n>>> ATA = A.T.dot(A)\n>>> eigval, eigvec= np.linalg.eig(ATA)\n>>> V = eigvec.copy()\n>>> S = np.diag(np.sqrt(eigval))\n>>> U = A.dot(V).dot(np.linalg.inv(S))\n>>> U\narray([[ 0.40824829,  0.70710678],\n       [ 0.81649658,  0.        ],\n       [ 0.40824829, -0.70710678]])\n>>> S\narray([[1.73205081, 0.        ],\n       [0.        , 1.        ]])\n>>> V\narray([[ 0.70710678, -0.70710678],\n       [ 0.70710678,  0.70710678]])\n>>> # 验证\n>>> U.dot(S).dot(V.T)\narray([[-2.23711432e-17,  1.00000000e+00],\n       [ 1.00000000e+00,  1.00000000e+00],\n       [ 1.00000000e+00, -2.23711432e-17]])\n```\n\n# 理解\n展开表达式，取$r \\leq n$时，\n$$\nA = U_{m×r} \\Sigma_{r×r} V_{r×n}^T = \\sum_{i=1}^r \\sigma_i (U_{,i}) (V_{,i})^T\n$$\n\n就得到与`PCA`相同的结论，矩阵$A$可由$r$个$m×n$的矩阵$(U_{,i}) (V_{,i})^T$加权组成。一般来说，前$10\\%$甚至$1\\%$的奇异值就占了全部奇异值之和的$99\\%$，极大地保留了信息，而大大减少了存储空间。\n\n> 以图片为例，若原有`24bit`图片，其大小为`(1024, 768)`，则不计图片信息，仅仅数据共占`1024×768×3 B`，或`2.25 MB`。用奇异值分解进行压缩，保留$60\\%$的奇异值，可达到几乎无损的程度，此时需要保存向量矩阵$U_{1024×60}$，$V_{60×768}$以及$60$个奇异值，以浮点数`float32`存储，一共占`420 KB`即可。\n> $$\n> (1024 × 60 + 60 × 768 + 60) × 4 / 2^{10} = 420.23\n> $$\n> 说句题外话，存储量的压缩必然以计算量的增大为代价，相反亦然，所以需要协调好`RAM`与`ROM`容量，考虑计算机的计算速度。换句话说，空间和时间上必然是互补的，哲学的味道hhhh。\n\n# 分解结果的信息保留\n分解后各样本间的欧式距离与角度信息应不变，给出证明如下\n设有$m$组$n$维样本样本\n$$\nX_{n×m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]\n$$\n\n经奇异值分解，有\n$$\nX_{n×m} = U_{n×r} \\Sigma_{r×r} V_{r×m}^T\n$$\n\n记\n$$\nZ_{r×m} = \\Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]\n$$\n\n有\n$$\nX = U Z\n$$\n\n- 欧式距离\n    $$\n    || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2\n    $$\n\n    $$\n    = \\left[ U (Z^{(i)} - Z^{(j)}) \\right]^T \\left[ U (Z^{(i)} - Z^{(j)}) \\right]\n    $$\n\n    $$\n    = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)})\n    $$\n\n    $$\n    = || Z^{(i)} - Z^{(j)} ||_2^2\n    $$\n\n    即\n    $$\n    || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2\n    $$\n\n- 角度信息\n    $$\n    \\frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2}\n    $$\n\n    $$\n    = \\frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2}\n    $$\n\n    $$\n    = \\frac{(UZ^{(i)})^T(UZ^{(j)})}{\\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \\sqrt{(UZ^{(j)})^T(UZ^{(j)})}}\n    $$\n\n    $$\n    = \\frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}\n    $$\n\n    即\n    $$\n    \\frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \n    \\frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}\n    $$\n\n# 代码\n[@Github: Code of SVD](https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p15_svd.py)\n对图片进行了分解\n```\nclass SVD():\n    \"\"\" Singular Value Decomposition\n    Attributes:\n        m {int}\n        n {int}\n        r {int}: if r == -1, then r = n\n        isTrains {bool}: isTrains = True if input.shape[0] < input.shape[1]\n        U {ndarray(m, r)}\n        S {ndarray(r, )}\n        V {ndarray(n, r)}\n    Notes:\n        - Transpose input matrix if m < n, and m, n := n, m\n        - Reassign r if eigvals contains zero\n        - Singular values are stored in a 1-dim array `S`\n        - X' = U S V^T\n    \"\"\"\n    def __init__(self, r=-1):\n        self.m = None\n        self.n = None\n        self.r = r\n        self.isTrans = False\n        self.U = None\n        self.S = None\n        self.V = None\n    def fit(self, X):\n        \"\"\" calculate components\n        Notes:\n            - Transpose input matrix if m < n, and m, n := n, m\n            - reassign self.r if eigvals contains zero\n        \"\"\"\n        (self.m, self.n) = X.shape\n        if self.m < self.n:\n            X = X.T\n            self.m, self.n = self.n, self.m\n            self.isTrans = True\n        self.r = self.n if (self.r == -1) else self.r\n\n        XTX = X.T.dot(X)\n        eigval, eigvec = np.linalg.eig(X.T.dot(X))\n        eigval, eigvec = np.real(eigval), np.real(eigvec)\n        \n        self.S = np.sqrt(np.clip(eigval, 0, float('inf')))\n        self.S = self.S[self.S > 0]\n        self.r = min(self.r, self.S.shape[0])               # reassign self.r\n        order = np.argsort(eigval)[::-1][: self.r]          # sort eigval from large to small\n        eigval = eigval[order]; eigvec = eigvec[:, order]\n        self.V = eigvec.copy()\n        self.U = X.dot(self.V).dot(\n                    np.linalg.inv(np.diag(self.S)))\n        return self.U, self.S, self.V\n    def compose(self, r=-1):\n        \"\"\" merge first r components\n        Parameters:\n            r {int}: if r==-1, merge all components\n        Returns:\n            X {ndarray(m, n)}\n        \"\"\"\n        if r == -1:\n            X = self.U.dot(np.diag(self.S)).dot(self.V.T)\n            X = X.T if self.isTrans else X\n        else:\n            (m, n) = (self.n, self.m) if self.isTrans else (self.m, self.n)\n            X = np.zeros(shape=(m, n))\n            for i in range(r):\n                X += self.__getitem__(i)\n        return X\n    def __getitem__(self, idx):\n        \"\"\" get a component\n        Parameters:\n            index {int}: range from (0, self.r)\n        \"\"\"\n        u = self.U[:, idx]\n        v = self.V[:, idx]\n        s = self.S[idx]\n        x = s * u.reshape(self.m, 1).\\\n                    dot(v.reshape(1, self.n))\n        x = x.T if self.isTrans else x\n        return x\n    def showComponets(self, r=-1):\n        \"\"\" display components\n        Notes:\n            - Resize components' shape into (40, 30)\n        \"\"\"\n        m, n = self.m, self.n\n        r = self.r if r==-1 else r\n        n_images = 10; m_images = r // n_images + 1\n        m_size, n_size = 40, 30\n        showfig = np.zeros(shape=(m_images*m_size, n_images*n_size))\n        for i in range(r):\n            m_pos = i // n_images\n            n_pos = i %  n_images\n            component = self.__getitem__(i)\n            component = component.T if self.isTrans else component\n            component = cv2.resize(component, (30, 40))\n            showfig[m_pos*m_size: (m_pos+1)*m_size, n_pos*n_size: (n_pos+1)*n_size] = component\n        plt.figure('components')\n        plt.imshow(showfig)\n        plt.show()\n```\n\n用上面的代码进行实验\n```\n# 读取一张图片\nX = load_images()[0].reshape((32, 32))\nshowmat2d(X)\n# 对图片进行奇异值分解\ndecomposer = SVD(r=-1)\ndecomposer.fit(X)\n# 显示一下分量\ndecomposer.showComponets(r=-1)\n# 将全部分量组合，并显示\nX_ = decomposer.compose(r=-1)\nshowmat2d(X_)\n# 将前5个分量组合，并显示\nX_ = decomposer.compose(r=5)\nshowmat2d(X_)\n```\n\n- 载入原图如下\n![source](/SVD/source.png)\n\n- 分量显示如下\n![components](/SVD/components.png)\n\n- 组合分量显示如下\n    - 组合全部\n        ![merge_all](/SVD/merge_all.png)\n    - 组合前5个分量\n        ![merge_5](/SVD/merge_5.png)","categories":["Machine Learning"]},{"title":"删除停用词","url":"/2018/10/23/删除停用词/","content":"\n> [删除停用词 - Python文本处理教程™](https://www.yiibai.com/python_text_processing/python_remove_stopwords.html)\n\n\n停用词是对句子没有多大意义的英语单词。 在不牺牲句子含义的情况下，可以安全地忽略它们。 例如，the, he, have等等的单词已经在名为语料库的语料库中捕获了这些单词。\n\n# 下载语料库\n- 安装`nltk`模块\n\t```\n\tpip install nltk\n\t```\n\n- 下载语料库\n\t```\n\timport nltk\n\tnltk.download('stopwords')\n\t```\n\n# 使用库料库\n- 验证停用词\n\t```\n\t>>> from nltk.corpus import stopwords\n\t>>> stopwords.words('english')\n\t['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n\t'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', \n\t'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n\t\"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', \n\t'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n\t'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", \n\t'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n\t'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n\t'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n\t'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n\t'with', 'about', 'against', 'between', 'into', 'through', \n\t'during', 'before', 'after', 'above', 'below', 'to', 'from', \n\t'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n\t'again', 'further', 'then', 'once', 'here', 'there', 'when',\n\t'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n\t'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n\t'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', \n\t'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \n\t'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \n\t\"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n\t\"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \n\t\"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n\t\"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \n\t\"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \n\t\"won't\", 'wouldn', \"wouldn't\"]\n\t```\n\n\t除了英语之外，具有这些停用词的各种语言如下。\n\t```\n\t>>> stopwords.fileids()\n\t['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', \n\t'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', \n\t'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian',\n\t'spanish', 'swedish', 'turkish']\n\t```\n\n- 示例\n\t从单词列表中删除停用词。\n\t```\n\t>>> from nltk.corpus import stopwords\n\t>>> en_stops = set(stopwords.words('english'))\n\t>>> \n\t>>> all_words = ['There', 'is', 'a', 'tree','near','the','river']\n\t>>> for word in all_words:\n\t\tif word not in en_stops:\n\t\t\tprint(word)\n\n\t\t\t\n\tThere\n\ttree\n\tnear\n\triver\n\t```","categories":["Python"]},{"title":"PCA","url":"/2018/10/22/PCA/","content":"\n# 引言\n`PCA`全称`Principal Component Analysis`，即主成分分析，是一种常用的数据降维方法。它可以通过线性变换将原始数据变换为一组各维度线性无关的表示，以此来提取数据的主要线性分量。\n\n# 向量的投影\n\n现有两个任意不共线向量$\\vec{u}, \\vec{v}$，将$\\vec{u}$投射到$\\vec{v}$上\n![向量投影](/PCA/向量投影.jpg)\n\n投影后，可以得到两个正交向量\n$$\n\\vec{u}' · (\\vec{u} - \\vec{u}') = 0\n$$\n\n我们设\n$$\n\\vec{u}' = \\mu \\vec{v} \\tag{1}\n$$\n\n代入后有\n$$\n\\mu \\vec{v} · (\\vec{u} - \\mu \\vec{v}) = 0\n$$\n\n引入矩阵运算，即\n$$\n(\\mu v)^T (u - \\mu v) = 0\n$$\n\n有\n$$\nv^T u = \\mu v^T v\n$$\n\n则得到$u'$以$v$为基向量的坐标\n$$\n\\mu  = (v^T v)^{-1} v^T u \\tag{2}\n$$\n\n所以得到\n$$\nu' = v (v^T v)^{-1} v^T u \\tag{*}\n$$\n\n> - 坐标变换求解投影向量：$u'$可视作$u$经坐标变换$u' = P u$得到，所以\n>   $$\n>   P = v (v^T v)^{-1} v^T\n>   $$\n> \n> - 推广至多个向量的投影，即得到\n>   $$\n>   P = X (X^T X)^{-1} X^T\n>   $$\n> \n>   这与[线性回归](https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/)中得到的结论一致。\n\n实际上\n$$\nu' = v (v^T v)^{-1} v^T u = \\frac{v}{||v||} (\\frac{v}{||v||})^T u\n$$\n\n记单位向量$\\frac{v}{||v||}$为$v_0$，得到\n$$\nu' = v_0 v_0^T u\n$$\n\n由几何关系，可以计算得投影后的长度为\n$$ \nd \n= ||u|| \\cos \\theta \n= ||u|| \\frac{v^T u}{||u||||v||}\n= v_0^T u\n$$\n\n所以在向量投影中，$u^T v_0$表示以$v_0$为基向量的坐标。\n\n# PCA\n现在有$N$维数据集$D=\\{x^{(1)}, x^{(2)}, ..., x^{(M)}\\}$，其中$x^{(i)} = \\left[x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_N\\right]^T$，各维特征$D_{j}$间存在线性相关性，利用主成分分析可使\n- 数据维度降低；\n- 提取主成分，且各成分间不相关。\n\n> 说明\n> - 由于选取的特征轴是正交的，所以计算结果线性无关；\n> - 提取了方差较大的几个特征，为主要线性分量。\n\n以二维空间中的数据$x^{(i)} = \\left[\\begin{matrix}\n    x^{(i)}_1 \\\\ x^{(i)}_2\n\\end{matrix}\\right]$为例，维度可降至一维，如下图所示。\n![PCA动态图](/PCA/PCA动态图.gif)\n\n主轴可有无穷多种选择，那么问题就是**如何选取最优的主轴**。先给出`PCA`的计算步骤。\n\n## 计算步骤\n输入的$M$个$N$维样本，有样本矩阵\n$$\nX_{N×M} = \\left[x^{(1)}, x^{(2)}, ..., x^{(M)} \\right]\n= \\left[\n    \\begin{matrix}\n        x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\\\\n        x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\\\\n        ... \\\\\n        x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\\\\n    \\end{matrix}\n\\right]\n$$\n### 投影\n1. 对每个维度(行)进行去均值化\n$$\nX_j := X_j - \\mu_j\n$$\n\n    其中$\\mu_j = \\overline{X_j}$，$j = 1, 2, ..., N$\n\n2. 求各维度间的协方差矩阵$\\Sigma_{N×N}$\n    $$ \\Sigma_{ij} = Cov(x_i, x_j) $$\n\n    或\n    $$\n    \\Sigma = \\frac{1}{M} X X^T\n    $$\n\n> 注：\n> 1. \n>   $$\n>   X X^T = \\left[\n>               \\begin{matrix}\n        \\sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 & \n        \\sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 &\n        ... &\n        \\sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\\\\n        \\sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 & \n        \\sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 &\n        ... &\n        \\sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\\\\n        ... &\n        ... &\n        ... &\n        ... \\\\\n        \\sum_{i=1}^M x^{(i)}_N x^{(i)}_1 & \n        \\sum_{i=1}^M x^{(i)}_N x^{(i)}_2 &\n        ... &\n        \\sum_{i=1}^M x^{(i)}_N x^{(i)}_N\n    \\end{matrix}\n>   \\right]\n>   $$\n> \n>   $$\n>   = \\sum_{i=1}^M \\left[\n>               \\begin{matrix}\n            x^{(i)}_1 x^{(i)}_1 & \n            x^{(i)}_1 x^{(i)}_2 &\n            ... &\n            x^{(i)}_1 x^{(i)}_N \\\\\n            x^{(i)}_2 x^{(i)}_1 & \n            x^{(i)}_2 x^{(i)}_2 &\n            ... &\n            x^{(i)}_2 x^{(i)}_N \\\\\n            ... &\n            ... &\n            ... &\n            ... \\\\\n            x^{(i)}_N x^{(i)}_1 & \n            x^{(i)}_N x^{(i)}_2 &\n            ... &\n            x^{(i)}_N x^{(i)}_N\n    \\end{matrix}\n>   \\right]\n>   $$\n> \n>   $$\n>   = \\sum_{i=1}^M x^{(i)} x^{(i)T}\n>   $$\n> \n> 2. 协方差定义式\n>   $$\n>       Cov(x,y)≝\\frac{1}{n-1} ∑_{i=1}^n (x_i−\\overline{x})^T(y_i−\\overline{y})\n>   $$\n>   其中$x=[x_1, x_2, ..., x_n]^T, y=[y_1, y_2, ..., y_n]^T$\n> \n> \n\n1. 求协方差矩阵$\\Sigma$的特征值$λ_i$及其对应特征向量$α_i$，$i=1, ..., N$；\n2. 按照特征值从大到小排列特征对$(λ_i,α_i)$，选取$K$个最大特征值对应的特征向量作为降维后的主轴$ \\beta_1, \\beta_2, ..., \\beta_K $，其中$\\beta_k$为单位向量\n   $$\n   \\beta_k = \\left[ \\beta_{k1}, \\beta_{k2}, ..., \\beta_{kN} \\right]^T\n   $$\n\n   记\n   $$\n   B_{N×K} = \\left[ \\beta_1, \\beta_2, ..., \\beta_K \\right]\n   $$\n\n   $K$的选取方法有如下两种：\n   - 指定选取$K$个主轴\n   - 保留$99\\%$的方差\n        $$ \\frac{\\sum_{i=1}^K \\lambda_i}{\\sum_{j=1}^N \\lambda_j} > 0.99$$\n\n    \n3. 将样本点投射到$K$维坐标系上\n    样本$X^{(i)}$投射到主成分轴$\\beta_k$上，其坐标表示为向量，为\n    $$\n    S^{(i)}_k = X^{(i)T}\\beta_k\n    $$\n\n    > 注意此时的基座标为$\\beta_k$，或者说$X'^{(i)} = S^{(i)} \\frac{\\beta_k}{||\\beta_k||}$\n\n    所有样本在主轴$\\beta_k$上的投影坐标即\n    $$\n    S = B^T X\n    $$\n\n    其中$S_{K×M}$，$B_{N×K}$，$X_{N×M}$\n\n> 注：若取$K=N$，可重建数据，如下\n> ![pca_restructure1](/PCA/pca_restructure1.png)\n> ![pca_restructure2](/PCA/pca_restructure2.png)\n\n### 复原\n第$5$步中，样本点向量$X^{(i)}$的主要分量投射到$K$个$N$维向量上，投影坐标为$S^{(i)}_k$，即\n$$\nX^{(i)} \\approx \\sum_{k=1}^K S^{(i)}_k \\beta_k\n$$\n\n以上就是样本点的复原公式，矩阵形式即\n$$\n\\hat{X} = BS\n$$\n\n其中$\\hat{X}_{N×M}$，$B_{N×K}$，$S_{K×M}$\n\n考虑到已去均值化，故\n$$\n\\hat{X}_j \\approx \\hat{X}_j + \\mu_j\n$$\n\n## 证明\n> 投影向量的$2$范数最大，或者说，投影后的坐标平方和最大\n\n当所有样本$X$投射到第一主轴$\\beta_1$上，其坐标为\n$$\nS_1 = X^T \\beta_1\n$$\n\n所有元素的平方和，或向量$S_1$的$2$范数为\n$$\n||S_1||_2^2 \n= S_1^T S_1 = \\beta_1^T X X^T \\beta_1 \\tag{1}\n$$\n\n即优化目标为\n$$\n\\max ||S_1||_2^2\n$$\n\n$$\ns.t.　||\\beta_1||_2^2 = 1\n$$\n\n矩阵$C=XX^T$为对称矩阵，故可单位正交化\n$$\nC = W \\Lambda W^T\n$$\n\n$$\nW = \\left[\\begin{matrix}\n    | & & |\\\\\n    w_1 & ... & w_M\\\\\n    | & & |\\\\\n\\end{matrix}\\right]　\n\\Lambda = \\left[\\begin{matrix}\n    \\lambda_1 &  & \\\\\n     & ... & \\\\\n     &  & \\lambda_M\\\\\n\\end{matrix}\\right]\n$$\n\n其中$\\lambda_1 > ...> \\lambda_M$，$w_i(i=1,...,M)$为矩阵$C$的特征向量(单位向量，互相正交)\n\n> 实际上$R(C) \\leq (n-1)$，即最多有$(n-1)$个特征值大于$0$。\n\n$$\n||S_1||_2^2\n= \\beta_1^T W \\Lambda W^T \\beta_1 \\tag{2}\n$$\n\n令$\\alpha_1 = W^T \\beta_1, \\beta_1 = W \\alpha_1$，可得\n$$\n||S_1||_2^2\n= \\alpha_1^T \\Lambda \\alpha_1 \\tag{3}\n$$\n\n即\n$$\n||S_1||_2^2 = \\sum_{i=1}^M \\lambda_i \\alpha_{1i}^2 \\tag{4}\n$$\n\n进一步\n$$\n\\sum_{i=1}^M \\lambda_i \\alpha_{1i}^2\n\\leq \\lambda_1 \\sum_{i=1}^M \\alpha_{1i}^2 \\tag{5}\n$$\n\n且由于$\\beta_1^T\\beta_1 = 1$，故\n$$\n1 = \\beta_1^T\\beta_1 = \\alpha_1^T W^T W \\alpha = \\alpha^T \\alpha = \n\\sum_{i=1}^M \\alpha_{1i}^2\n$$\n\n可得\n$$\n||S_1||_2^2\n= \\sum_{i=1}^M \\lambda_i \\alpha_{1i}^2\n\\leq \\lambda_1  \\tag{6}\n$$\n\n为使$(6)$取等号，即达最大值，可使\n$$\n\\begin{cases}\n    \\alpha_{11} = 1 \\\\\n    \\alpha_{12} = ... = \\alpha_{1M} = 0\n\\end{cases}\n$$\n\n即令\n$$\n\\beta_1 = W \\alpha_1 = w_1\n$$\n\n> $\\alpha_1 = [1, 0, ..., 0]^T$\n\n所以$\\beta_1$对应矩阵$C=XX^T$的特征向量$w_1$，且有\n$$\n||S_1||_2^2 = \\lambda_1\n$$\n\n> 或者第一主成分的证明也可以这样，建立优化目标\n> $$\n> \\beta_1 = \\arg \\max　||S_1||_2^2\n> $$\n> \n> $$ s.t.　||\\beta_1||_2^2 = 1 $$\n> \n> 构造拉格朗日函数\n> $$\n> L(\\beta_1, \\lambda_1) = ||S_1||_2^2 + \\lambda_1 (1 - ||\\beta_1||_2^2)\n> $$\n> \n> 也即\n> $$\n> L(\\beta_1, \\lambda_1) = \\beta_1^T X X^T \\beta_1 + \\lambda_1 (1 - \\beta_1^T \\beta_1)\n> $$\n> \n> 求其极值点\n> $$\n> ▽_{\\beta_1}L(\\beta_1) = 2 X X^T \\beta_1 - 2 \\lambda_1 \\beta_1 = 0\n> $$\n> \n> 有\n> $$\n> X X^T \\beta_1 = \\lambda_1 \\beta_1\n> $$\n> \n> 可见$\\beta_1$即方阵$X X^T$的特征向量\n\n\n当我们希望用更多的主成分刻画数据，如已经求得主成分$\\beta_1, ..., \\beta_{r-1}$，先需求解$\\beta_r$，引入正交约束$\\beta_r^T \\beta_i = 0$，即目标函数为\n$$\n||S_r||_2^2 = \\beta_r^T C \\beta_r\n$$\n\n$$\ns.t.　\\beta_r^T \\beta_i = 0, i = 1, ..., r-1\n$$\n\n$$\n||\\beta_r||_2^2 = 1\n$$\n\n令$\\beta_r = W \\alpha_r$，则\n$$\n||S_r||_2^2\n= \\alpha_r^T \\Lambda \\alpha_r\n= \\sum_i \\lambda_i \\alpha_{ri}^2\n$$\n\n而根据正交约束\n$$\n0 = \\beta_r^T \\beta_i = \\alpha_r^T W^T w_i = \\alpha_{ri},　\ni = 1, ..., r-1\n$$\n\n> $ W^T w_i = \\left[0, ..., 1_i, ..., 0\\right]^T$\n\n\n所以\n$$\n||S_r||_2^2 = \\sum_i \\lambda_i \\alpha_{ri}^2 = \\lambda_r \\alpha_{rr}^2 \\tag{5}\n$$\n\n又因为$\\beta_r^T \\beta_r = 1$(单位向量)，故\n$$\n\\beta_r^T \\beta_r = \\alpha_r^T W^T W \\alpha_r = \\alpha_r^T \\alpha_r = \\sum_i \\alpha_{ri}^2 = 1\n$$\n\n于是类似的，为使$(5)$取最大，取\n$$\n\\begin{cases}\n    \\alpha_{rr} = 1\\\\\n    \\alpha_{ri} = 0,　i = 1, ..., M, i \\neq r\n\\end{cases}\n$$\n\n> $\\alpha_r = [0, ..., 1_r, ..., 0]$\n\n\n则此时\n$$\n\\beta_r = W \\alpha_r = w_r\n$$\n\n且有\n$$\n||S_r||_2^2 = \\lambda_r\n$$\n\n证毕。\n\n\n## 白化(whitening)\n`whitening`的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。\n\n数据的`whitening`必须满足两个条件：\n1. 不同特征间相关性最小，接近$0$；\n2. 所有特征的方差相等（不一定为$1$）。\n\n常见的白化操作有`PCA whitening`和`ZCA whitening`。\n> [Whitening - Ufldl](http://deeplearning.stanford.edu/wiki/index.php/Whitening)\n\n\n- PCA whitening\n    `PCA whitening`指将数据$X$经过`PCA`降维为$S$后，可以看出$S$中每一维是独立的，满足`whitening`的第一个条件，这是只需要将$S$中的每一维都除以标准差就得到了每一维的方差为$1$，也就是说方差相等。\n    $$\n    X_{PCAwhite, j} = \\frac{X_{rot, j}}{\\sqrt{\\lambda_j}}\n    $$\n\n\n- ZCA whitening\n    `ZCA whitening`是指数据$X$先经过`PCA`变换为$S$，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足`whtienning`的第一个条件，特征间相互独立。然后同样进行方差为$1$的操作，最后将得到的矩阵左乘一个特征向量矩阵$U$即可。\n    $$\n    X_{ZCAwhite} = U · X_{PCAwhite}\n    $$\n\n# Kernel PCA\n`Kernel PCA`的思想是在高维的特征空间中求解协方差矩阵\n$$\n\\Sigma = \\frac{1}{M} \\sum_{i=1}^M \\Phi(X^{(i)}) \\Phi(X^{(i)})^T\n$$\n\n其中$\\Phi(X^{(i)})$表示将样本$i$映射到高维空间后中的向量，即\n$$\n\\Phi(X^{(i)}) = \\left[ \\phi^{(i)}_1, \\phi^{(i)}_2, ..., \\phi^{(i)}_{N'} \\right]^T\n$$\n\n其中$N' > N$，由于$\\Phi(X^{(i)})$为隐式的，故设置核函数求解，记\n$$\n\\kappa(i, j) = \\Phi(X^{(i)}) \\Phi(X^{(i)})^T\n$$\n\n> 关于核技巧，移步[非线性支持向量机]()\n\n![kernel_pca](/PCA/kernel_pca.jpg)\n\n# 应用\n可利用`PCA`与线性回归求解$3$维空间中平面的法向量\n1. 利用`PCA`重建数据(不降维，此时为$3$维)，此时第$1, 2$主成分轴可张成所求平面，即该平面可表示为\n    $$\n    \\Pi = span \\{ \\beta_1, \\beta_2 \\}\n    $$\n\n> 就是说，第一、二主成分是这些点“拉伸”最大的方向 :-)，好懂不？\n\n\n1. 由[正交投影](https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/)可知，平面外一点$y$可通过最小二乘(线性回归)的方法投射到平面上，向量运算，不考虑偏置项，即\n    $$\n    \\hat{y} \n    = \\theta_1 x_1 + \\theta_2 x_2 \\tag{*}\n    $$\n\n    其中$x_1, x_2$表示第一、第二主成分$\\beta_1, \\beta_2$，为$3$维向量\n    $$\n    \\hat{y} = \\left[\n        \\begin{matrix}\n            \\hat{y_1} \\\\\n            \\hat{y_2} \\\\\n            \\hat{y_3} \\\\\n        \\end{matrix}\n    \\right]　\n    x_i = \\left[\n        \\begin{matrix}\n            x_{i1} \\\\\n            x_{i2} \\\\\n            x_{i3} \\\\\n        \\end{matrix}\n    \\right]\n    $$\n\n    可利用公式求解回归参数$\\theta$\n    $$\n    \\theta = (X^TX+\\lambda I)^{-1} X^T y\n    $$\n\n    > 注意：$X(n\\_samples, n\\_features)$，这里把$(x_{1j}, x_{2j}, y_{j})作为一组样本$\n\n    此时该参数表示在主轴上的坐标$(\\theta_1, \\theta_2)$，带回$(*))$即可解得$\\hat{y}$\n\n    $$\n    \\hat{y} \n    = \\theta_1 \\beta_1 + \\theta_2 \\beta_2 \\tag{*}\n    $$\n\n    通俗理解，一掌把$y$拍平在了平面$\\Pi$上，变成了$\\hat{y}$，但是哪有这么好拍。。。这个时候刺在掌心里一定有一个垂直的向量分量，即为该平面的法向量\n    $$\n    \\vec{n} = y - \\hat{y}\n    $$\n\n    **也可使用粗暴一点的方法，直接将第三主成分作为法向量。**\n\n    > 或者直接上投影公式：\n    > $$\n    > \\hat{y} = Py\n    > $$\n    >\n    > $$　\n    > P = X (X^TX+\\lambda I)^{-1} X^T\n    > $$\n    \n\n    ![projection](/PCA/projection.jpg)\n\n    总体的运算流程如下\n    - 利用所有样本点(近似平面)计算主成分，第一、二主成分张成平面$\\Pi$；\n    - 选出其中一个样本点，将平行于平面$\\Pi$的成分投射到$\\Pi$上；\n    - 该样本点剩余分量即法向量；\n    - 一般来说，取所有点法向量的均值。\n\n# 程序\n[@Github: PCA](https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-3-pca)\n\n```\nclass PrincipalComponentAnalysis():\n    def __init__(self, n_component=-1):\n        self.n_component = n_component\n        self.meanVal = None\n        self.axis = None\n    def fit(self, X, prop=0.99):\n        '''\n        the parameter 'prop' is only for 'n_component = -1'\n        '''\n        # 第一步: 归一化\n        self.meanVal = np.mean(X, axis=0)                   # 训练样本每个特征上的的均值\n        X_normalized = (X - self.meanVal)                   # 归一化训练样本\n        # 第二步：计算协方差矩阵\n        # cov = X_normalized.T.dot(X_normalized)\n        cov = np.cov(X_normalized.T)                        # 协方差矩阵\n        eigVal, eigVec = np.linalg.eig(cov)                 # EVD\n        order = np.argsort(eigVal)[::-1]                    # 从大到小排序\n        eigVal = eigVal[order]\n        eigVec = eigVec.T[order].T\n        # 选择主成分的数量\n        if self.n_component == -1:\n            sumOfEigVal = np.sum(eigVal)\n            sum_tmp = 0\n            for k in range(eigVal.shape[0]):\n                sum_tmp += eigVal[k]\n                if sum_tmp > prop * sumOfEigVal:            # 平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值\n                    self.n_component = k + 1\n                    break\n        # 选择投影坐标轴\n        self.axis = eigVec[:, :self.n_component]            # 选择前n_component个特征向量作为投影坐标轴\n    def transform(self, X):\n        # 第一步：归一化\n        X_normalized = (X - self.meanVal)                   # 归一化测试样本\n        # 第二步：投影 X_nxk · V_kxk' = X'_nxk'\n        X_transformed = X_normalized.dot(self.axis)\n        return X_transformed\n    def fit_transform(self, X, prop=0.99):\n        self.fit(X, prop=prop)\n        return self.transform(X)\n    def transform_inv(self, X_transformed):\n        # 视投影向量长度为一个单位长度，投影结果为投影向量上的坐标\n        # X'_nxk' · V_kxk'.T = X''_nxk\n        X_restructed = X_transformed.dot(self.axis.T)\n        # 还原数据\n        X_restructed = X_restructed + self.meanVal\n        return X_restructed\n```\n\n实验结果\n- Demo1: PCA applied on 2-d datasets\n    ![2d_restructed](/PCA/2d_restructed.png)\n\n- Demo2: PCA applied on wild face\n    - origin\n    ![origin](/PCA/face_origin.png)\n    - reduced\n    ![reduced](/PCA/face_reduced.png)\n    - restructured\n    ![restructured](/PCA/face_restructed.png)\n  ","categories":["Machine Learning"]},{"title":"Activate Functions","url":"/2018/10/20/Activate-Functions/","content":"\n> [SigAI 理解神经网络的激活函数](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247483977&idx=1&sn=401b211bf72bc70f733d6ac90f7352cc&chksm=fdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3&scene=21#wechat_redirect)\n> [机器学习笔记：形象的解释神经网络激活函数的作用是什么？ - 不说话的汤姆猫 - 博客园](https://www.cnblogs.com/silence-tommy/p/7113405.html)\n\n\n# 激活函数的作用\n## 复合函数\n神经网络可以看作一个多层复合函数，以下图隐含层的激活函数为例，讲解其非线性作用。\n![激活函数的非线性作用](/Activate-Functions/激活函数的非线性作用.png)\n\n记激活函数为$\\sigma(·)$，上图神经网络各层间具有如下关系\n$$a = \\sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1)$$\n\n$$b = \\sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2)$$\n\n$$c = \\sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3)$$\n\n输出层采用线性单元\n$$\nA = w^{(2)}_{1}a + w^{(2)}_{2}b + w^{(2)}_{3}c + b^{(2)}\n$$\n\n<!-- 或者写作复合函数\n$$\nA = w^{(2)}_{1} \\sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1) + \n    w^{(2)}_{2} \\sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2) + \n    w^{(2)}_{3} \\sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3) + \n    b^{(2)}\n$$ -->\n\n为便于作图，固定参数\n$$\nW^{(1)} = \\left[\n    \\begin{matrix}\n        1   &  1 \\\\\n        0.1 & -1 \\\\\n        1   & -1\n    \\end{matrix}\n\\right],\nb^{(1)} = \\left[\n    \\begin{matrix}\n        -2  \\\\\n        1.5 \\\\\n        -1\n    \\end{matrix}\n\\right]\nW^{(2)} = \\left[\n    \\begin{matrix}\n        1 & 2 & 3\n    \\end{matrix}\n\\right],\nb^{(2)} = \\left[\n    \\begin{matrix}\n        -1\n    \\end{matrix}\n\\right]\n$$\n\n- 线性单元作为激活函数\n    此时神经网络的输出为\n    $$\n    A = (x + y - 2) + \n        2 (0.1x - y + 1.5) + \n        3 (x - y - 1)- 1\n    $$\n\n    可见仍为线性函数，做出图像如下所示\n    ![Linear](/Activate-Functions/Linear.png)\n\n- 非线性单元作为激活函数\n    此时神经网络的输出为\n    $$\n    A = \\sigma(x + y - 2) + \n        2 \\sigma(0.1x - y + 1.5) + \n        3 \\sigma(x - y - 1)- 1\n    $$\n\n    激活函数选择`Sigmoid`，做出图像如下所示\n    ![nonLinear](/Activate-Functions/nonLinear.png)\n\n## 分割平面\n神经网络可实现逻辑运算，各个神经元视作分割超平面时，可分割出不同形状的平面，在线性和非线性激活函数时分割效果如图。当神经元组合的情况更复杂时，表达能力就会更强。\n![](/Activate-Functions/激活函数的非线性作用.jpg)\n\n# 激活函数的性质\n已经证明，只要激活函数选择得当，神经元个数足够多，使用3层即包含一个隐含层的神经网络就可以实现对任何一个从输入向量到输出向量的连续映射函数的逼近，这个结论称为万能逼近（universal approximation）定理。\n> 如果$\\varphi(x)$是一个非常数、有界、单调递增的连续函数，$I_{m}$是$m$维的单位立方体，$I_{m}$中的连续函数空间为$C(I_{m})$。对于任意$\\varepsilon>0$以及函数$f\\in C(I_{m})$，存在整数$N$，实数$v_{i},b_{i}$，实向量$w_{i}\\in R^{m}$，通过它们构造函数$F(x)$作为函数$f$的逼近：\n$$\nF(x) = \\sum_{i=1}^N v_i \\varphi(w_i^T x + b_i)\n$$\n>\n> 对任意的$X\\in I_{m}$满足：\n$$\n| F(x) - f(x) | < \\varepsilon\n$$\n>\n> Cybenko, G. Approximation by superpositions of a sigmoid function. Mathematics of Control, Signals, and Systems, 2, 303-314, 1989.\n\n这个定理对激活函数的要求是**必须非常数、有界、单调递增，并且连续**。\n\n神经网络的训练使用梯度下降法进行求解，需要计算损失函数对参数的梯度值，涉及到计算激活函数的导数，因此激活函数必须是可导的。实际应用时并不要求它在定义域内处处可导，只要是几乎处处可导即可。\n\n> 定义$R$为一维欧氏空间，$E\\subset R$是它的一个子集，$mE$为点集$E$的**Lebesgue测度**。如果$E$为$R$中的可测集，$f(x)$为定义在上$E$的实函数，如果存在$N\\subset E$，满足：$mN=0$，对于任意的$x_{0}\\in E/N$函数$f(x)$在$x_{0}$处都可导，则称$f(x)$在$E$上几乎处处可导。\n\n如果将激活函数输入值$x$看做是随机变量，则它落在这些不可导点处的概率是$0$。在计算机实现时，因此有一定的概率会落在不可导点处，但概率非常小。\n> 例如ReLU函数在$x=0$处不可导\n> $$\nf(x) = \\begin{cases}\n    x & x \\geq 0 \\\\\n    0 & x < 0\n\\end{cases}\n$$\n\n# 常用的激活函数\n![常用的激活函数](/Activate-Functions/常用的激活函数.jpg)\n","categories":["Machine Learning","Deep Learning"]},{"title":"Feedforward Neural Network","url":"/2018/10/20/Feedforward-Neural-Network/","content":"\n# 前言\n前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一，既可以用于解决分类问题，也可以用于解决回归问题。\n\n\n# 简介\n前馈神经网络也叫作多层感知机，包含输入层，隐含层和输出层三个部分。它的目的是为了实现输入到输出的映射。\n$$\ny = f(x;W)\n$$\n\n由于各层采用了非线性激活函数，神经网络具有良好的非线性特性，如下图所示。\n- 激活函数为线性单元\n![Linear](/Feedforward-Neural-Network/Linear.png)\n- 激活函数为非线性单元\n![nonLinear](/Feedforward-Neural-Network/nonLinear.png)\n\n前馈神经网络可用于解决非线性的分类或回归问题，参数通过反向传播算法`(Back Propagation)`学习。\n\n\n# 结构\n## 神经元与网络结构图\n单个神经元的示意图如下，输入为前一层的输出参数$X^{(l-1)}$\n$$\nh_{w, b}(x) = \\sigma (WX + b)\n$$\n\n$\\sigma(·)$表示激活函数。\n\n![单个神经元示意图](/Feedforward-Neural-Network/单个神经元示意图.png)\n\n以下为典型的神经网络结构图\n![前馈神经网络结构图](/Feedforward-Neural-Network/前馈神经网络结构图.png)\n\n- 第一层为输入层`input layer`，一般不设置权值，预处理在输入网络前完成；\n- 最后一层为输出层`output layer`；\n- 其余层称为隐藏层`hidden layer`，隐藏层用于提取数据特征，隐藏层层数与各层神经元个数为超参数。\n\n\n> 神经元权值取值不同，可实现不同的逻辑运算，单个超平面只能进行二元划分，利用逻辑运算可将多个超平面划分的区域拼接起来，如图\n> ![超平面划分区域的拼接](/Feedforward-Neural-Network/超平面划分区域的拼接.jpg)\n>\n> 以下说明逻辑运算的实现方法\n> ![二元逻辑运算](/Feedforward-Neural-Network/二元逻辑运算.png)\n> 其中\n> $$\n> f(z) = \\begin{cases}\n    1 & z \\geq 0 \\\\\n    0 & otherwise\n\\end{cases}\n> $$\n> \n> - 与运算 $a ∧ b$\n>   $$w_1 = 20, w_2 = 20, b = -30$$\n> \n> - 或运算 $a ∧ b$\n>   $$w_1 = 20, w_2 = 20, b = -10$$\n> \n> - 非运算 $a = \\overline{b}$\n>   $$w_1 = -20, w_2 = 0, b = 0$$\n> \n> - 异或运算 $a \\bigoplus b$，可通过组合运算实现\n> $$ a \\bigoplus b = (\\overline{a} ∧ b) ∨ (a ∧ \\overline{b}) $$\n\n\n## 激活函数\n- 隐藏层的激活函数，详情可查看[另一篇博文：神经网络的激活函数](https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/)；\n\n- 输出层的激活函数\n    - 回归问题时，采用线性单元即可\n        $$\n        f(x) = x\n        $$\n\n    - 分类问题时，一般有以下几种选择\n        - 单类别概率输出\n            即每个神经元的输出对应该类别的$0-1$分布输出，这就需要将输出值限制在$[0, 1]$内，例如\n            $$ P(y=1|x )= max\\{0, min\\{1, z\\}\\} $$\n            \n            ![线性输出单元](/Feedforward-Neural-Network/clf_linearout.png)\n\n            但是可以看到，当$(w^Tx+b)$处于单位区间外时，模型的输出对它的参数的梯度都将为$0$ ，不利于网络的训练，故采用$S$形函数`Sigmoid`([详情](https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/))\n            $$\n            P(y=1|x ) = \\frac{1}{1+e^{-(w^Tx+b)}}\n            $$\n\n            > $(1)$ `Sigmoid`函数定义域为$(-\\infty, \\infty)$，值域为$(0, 1)$，且在整个定义域上单调递增，即为单值函数，故可将线性输出单元的结果映射到$(0, 1)$范围内；\n            > $(2)$ 在定义域上处处可导。\n\n        - 多类别的概率输出\n            即每个神经元的输出对应判别为该类别的概率，且有\n            $$\n            \\sum_{i=1}^C y_i = 1\n            $$\n\n            例如\n            $$\n            y_i = \\frac{z_i}{\\sum_j z_j}\n            $$\n\n            但是分式求导异常麻烦，故采用`Softmax`函数([详情](https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/))作为输出结点的激活函数，该函数求导结果比较简洁，且可利用输出计算导数，计算量减少。\n            $$\n            Softmax(x) = \\frac\n                        {1}\n                        {\\sum_{k=1}^K exp(x_k)}\n                        \\left[\n                            \\begin{matrix}\n                                exp(x_1)\\\\\n                                exp(x_2)\\\\\n                                ...\\\\\n                                exp(x_K)\n                            \\end{matrix}\n                        \\right]\n            $$\n\n# 损失函数\n- 回归问题\n    常见的用于回归问题的损失函数为`MSE`，即\n    $$\n    L(y, \\hat{y}) = \\frac{1}{2M} \\sum_{i=1}^M (\\hat{y}^{(i)} - y^{(i)})^2\n    $$\n\n- 分类问题\n    一般采用交叉熵作为损失函数，如下\n    $$\n    L(\\hat{y}, y) = - \\frac{1}{M} \\sum_{i=1}^M 1\\{y^{(i)}_j=k\\} \n    \\log (\\hat{y}^{(i)}_j)\n    $$\n\n    $$\n    1\\{y^{(i)}_j=k\\} = \n        \\begin{cases}\n            1 & y^{(i)}_j = k \\\\\n            0 & y^{(i)}_j \\neq k \n        \\end{cases}　j = 1, ..., N\n    $$\n\n    或者\n    $$\n    L(\\hat{y}, y) = - \\frac{1}{M} \\sum_{i=1}^M \n    y^{(i)T} \\log (\\hat{y}^{(i)})\n    $$\n\n    其中$y^{(i)}, \\hat{y}^{(i)}$均表示向量，采用`one-hot`编码。\n\n# 梯度推导\n以上内容网上资料一大堆，进入重点，反向传播时的梯度推导，给出网络结构如下。\n- 回归与分类在输出层有所区别；\n- 各层激活函数的输入变量以$z^{(l)}$表示，输出变量均以$x^{(l)}$表示；\n- $W^{(l)}$表示从第$l$层到第$(l+1)$层的权值矩阵，则$w^{(l)}_{ij}$表示第$l$层第$j$个神经元到$(l+1)$层第$i$个神经元的连接权值；\n- $b^{(l)}$表示第$l$层到第$(l+1)$层的偏置，则$b^{(l)}_i$表示到第$(l+1)$层第$i$个神经元的偏置值；\n- 各层变量维度推广为输入$d_{i}$，中间层$d_{h}$，输出层$d_{o}$；\n- 全连接，部分线条已省略，激活函数已省略；\n\n![FNN](/Feedforward-Neural-Network/fnn.jpg)\n\n则各层参数矩阵为\n$$\nW^{(1)} = \\left[\n        \\begin{matrix}\n            w^{(1)}_{11} & ... & w^{(1)}_{1d_i} \\\\\n            ... & ... & ... \\\\\n            w^{(1)}_{d_h1} & ... & w^{(1)}_{d_hd_i}\n        \\end{matrix}\n\\right]　\nb^{(1)} = \\left[\n        \\begin{matrix}\n            b^{(1)}_{1} \\\\\n            ... \\\\\n            b^{(1)}_{d_h}\n        \\end{matrix}\n\\right]\n$$\n\n$$\nW^{(2)} = \\left[\n        \\begin{matrix}\n            w^{(2)}_{11} & ... & w^{(2)}_{1d_h} \\\\\n            ... & ... & ... \\\\\n            w^{(2)}_{d_o1} & ... & w^{(2)}_{d_od_h}\n        \\end{matrix}\n\\right]　\nb^{(2)} = \\left[\n        \\begin{matrix}\n            b^{(2)}_{1} \\\\\n            ... \\\\\n            b^{(2)}_{d_o}\n        \\end{matrix}\n\\right]\n$$\n\n有\n$$\nZ^{(2)} = W^{(1)} X^{(1)} + b^{(1)}\n$$\n\n$$\nX^{(2)} = \\sigma_1 (Z^{(2)})\n$$\n\n$$\nZ^{(3)} = W^{(2)} X^{(2)} + b^{(2)}\n$$\n\n$$\nX^{(3)} = \\sigma_2 (Z^{(3)})\n$$\n\n$$\nX^{(1)} = X　\\hat{Y} = X^{(3)}\n$$\n\n## 回归问题\n损失函数采用`MSE`，即\n$$\nL(Y, \\hat{Y}) = \\frac{1}{M} \\sum_{i=1}^M L(Y^{(i)}, \\hat{Y}^{(i)})\n$$\n\n$$\nL(Y^{(i)}, \\hat{Y}^{(i)}) \n= \\frac{1}{2} || \\hat{Y}^{(i)} - Y^{(i)} ||_2^2\n= \\frac{1}{2} \n\\sum_{d_2=1}^{d_o}\n(\\hat{y}^{(i)}_{d_2} - y^{(i)}_{d_2})^2\n$$\n\n下面推导单个样本的损失函数的梯度，该批数据的梯度为均值。\n> 省略样本标记`$^{(i)}$`\n\n- 隐含层到输出层\n    - 对权值矩阵的梯度\n        $$\n        \\frac{∂L}{∂w^{(2)}_{ij}}\n        = \\frac{∂}{∂w^{(2)}_{ij}} \\frac{1}{2} \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2})^2 \n        $$\n\n        $$\n        = \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2}) \\frac{∂}{∂w^{(2)}_{ij}} \\hat{y}_{d_2} \\tag{1}\n        $$\n\n        其中\n        $$\n        \\begin{cases}\n            \\hat{y}_{d_2} = \\sigma_2 (z_{d_2}^{(3)}) \\\\\n            z_{d_2}^{(3)} = \\sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}\n        \\end{cases}\n        $$\n        \n        且\n        $$\n        \\frac{∂}{∂w^{(2)}_{ij}} \\hat{y}_{d_2}\n        = \\sigma_2' (z_{d_2}^{(3)}) \\frac{∂z_{d_2}^{(3)}}{∂w^{(2)}_{ij}} \\tag{2}\n        $$\n        \n        $$\n        \\frac{∂}{∂w^{(2)}_{ij}} z_{d_2}^{(3)} = \n            \\begin{cases}\n                x^{(2)}_{d_1} & d_1 = j, d_2 = i \\\\\n                0 & otherwise\n            \\end{cases} \\tag{3}\n        $$\n        \n        $(3)$代入$(2)$，再代入$(1)$可得到\n        $$\n        \\frac{∂L}{∂w^{(2)}_{ij}}\n        = (\\hat{y}_{d_2} - y_{d_2}) \\sigma_2' (z_{d_2}^{(3)}) x^{(2)}_{d_1} | _{d_1=j, d_2=i}\n        = (\\hat{y}_{i} - y_{i}) \\sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \\tag{*1}\n        $$\n\n    - 对偏置矩阵的梯度\n        $$\n        \\frac{∂L}{∂b^{(2)}_i}\n        = \\frac{∂}{∂b^{(2)}_i} \\frac{1}{2} \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2})^2 \n        $$\n\n        $$\n        = \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2}) \\frac{∂}{∂b^{(2)}_i} \\hat{y}_{d_2} \\tag{4}\n        $$\n\n        其中\n        $$\n        \\begin{cases}\n            \\hat{y}_{d_2} = \\sigma_2 (z_{d_2}^{(3)}) \\\\\n            z_{d_2}^{(3)} = \\sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}\n        \\end{cases}\n        $$\n\n        有\n        $$\n        \\frac{∂}{∂b^{(2)}_i} z_{d_2}^{(3)} = \n            \\begin{cases}\n                1 &  d_2 = i \\\\\n                0 & otherwise\n            \\end{cases} \\tag{5}\n        $$\n\n        所以\n        $$\n        \\frac{∂L}{∂b^{(2)}_i} = (\\hat{y}_{d_2} - y_{d_2}) \\sigma_2' (z_{d_2}^{(3)}) | _{d_2=i}\n        = (\\hat{y}_{i} - y_{i}) \\sigma_2' (z_i^{(3)}) \\tag{*2}\n        $$\n\n- 输入层到隐含层\n    - 对权值矩阵的梯度\n        $$\n        \\frac{∂L}{∂w^{(1)}_{ij}}\n        = \\frac{∂}{∂w^{(1)}_{ij}} \\frac{1}{2} \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2})^2\n        $$\n\n        $$\n        = \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2}) \\frac{∂}{∂w^{(1)}_{ij}} \\hat{y}_{d_2} \\tag{6}\n        $$\n\n        其中\n        $$\n        \\begin{cases}\n            \\hat{y}_{d_2} = \\sigma_2 (z_{d_2}^{(3)}) \\\\\n            z_{d_2}^{(3)} = \\sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \\\\\n            x^{(2)}_{d_1} = \\sigma_1 (z_{d_1}^{(2)}) \\\\\n            z_{d_1}^{(2)} = \\sum_{d_0=1}^{d_i} w^{(1)}_{d_1 d_0} x^{(1)}_{d_0} + b^{(1)}_{d_1}\n        \\end{cases}\n        $$\n\n        故\n        $$\n        \\frac{∂}{∂w^{(1)}_{ij}} \\hat{y}_{d_2}\n        = \\frac{∂\\hat{y}_{d_2}}{∂z_{d_2}^{(3)}} \n            \\frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \\tag{7}\n        $$\n\n        其中\n        $$\n        \\frac{∂\\hat{y}_{d_2}}{∂z_{d_2}^{(3)}} \n        = \\sigma_2' (z_{d_2}^{(3)})  \\tag{8}\n        $$\n\n        $$\n        \\frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \n        = \\sum_{d1=1}^{d_h} w^{(2)}_{d_2 d_1} \\frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} \\tag{9}\n        $$\n        \n        $$\n        \\frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}}\n        = \\frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}}\n            \\frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}}  \\tag{10}\n        $$ \n\n        而其中\n        $$\n        \\frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}} = x^{(2)}_{d_1} = \\sigma_1' (z_{d_1}^{(2)}) \\tag{11}\n        $$\n\n        $$\n        \\frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}} = \n        \\begin{cases}\n            x^{(1)}_{d_0} & d_1 = i, d_0 = j\\\\\n            0 & otherwise\n        \\end{cases} \\tag{12}\n        $$ \n\n        $(11),(12)$代入$(10)$得到\n        $$\n        \\frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} = \n        \\sigma_1' (z_{d_1}^{(2)})\n            x^{(1)}_{d_0} | _{d_1 = i, d_0 = j} \\tag{13}\n        $$\n\n        $(13)$代回$(9)$，有\n        $$\n        \\frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \n        = \\sum_{d1=1}^{d_h} \n        \\left[\n            w^{(2)}_{d_2 d_1} \n            \\sigma_1' (z_{d_1}^{(2)})\n            x^{(1)}_{d_0}\n        \\right] | _{d_1 = i, d_0 = j}\n        $$\n\n        $$\n        = w^{(2)}_{d_2 i} \n            \\sigma_1' (z_i^{(2)})\n            x^{(1)}_j \\tag{14}\n        $$\n\n        将$(8),(14)$代入$(7)$得到\n        $$\n        \\frac{∂}{∂w^{(1)}_{ij}} \\hat{y}_{d_2}\n        = \\sigma_2' (z_{d_2}^{(3)}) \n             w^{(2)}_{d_2 i} \n            \\sigma_1' (z_i^{(2)})\n            x^{(1)}_j \\tag{15}\n        $$\n\n        $(15)$代入$(6)$有\n        $$\n        \\frac{∂L}{∂w^{(1)}_{ij}}\n        = \\sum_{d_2=1}^{d_o} \n            (\\hat{y}_{d_2} - y_{d_2})      \n            \\sigma_2' (z_{d_2}^{(3)}) \n             w^{(2)}_{d_2 i} \n            \\sigma_1' (z_i^{(2)})\n            x^{(1)}_j \\tag{*3}\n        $$\n\n    - 对偏置矩阵的梯度\n        $$\n        \\frac{∂L}{∂b^{(1)}_i}\n        = \\frac{∂}{∂b^{(1)}_i} \\frac{1}{2} \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2})^2\n        $$\n    \n        $$\n        = \\sum_{d_2=1}^{d_o} (\\hat{y}_{d_2} - y_{d_2}) \\frac{∂}{∂b^{(1)}_i} \\hat{y}_{d_2} \\tag{16}\n        $$\n\n        同理可得\n        $$\n        \\frac{∂}{∂b^{(1)}_i} \\hat{y}_{d_2}\n        = \\sigma_2' (z_{d_2}^{(3)}) \n             w^{(2)}_{d_2 i} \n            \\sigma_1' (z_i^{(2)})  \\tag{17}\n        $$\n        \n        所以\n        $$\n        \\frac{∂L}{∂b^{(1)}_i} =\n        \\sum_{d_2=1}^{d_o} \n            (\\hat{y}_{d_2} - y_{d_2})      \n            \\sigma_2' (z_{d_2}^{(3)}) \n             w^{(2)}_{d_2 i} \n            \\sigma_1' (z_i^{(2)}) \\tag{*4} \n        $$\n\n综上所述\n$$\n\\frac{∂L}{∂w^{(2)}_{ij}}\n= (\\hat{y}_{i} - y_{i}) \n    \\sigma_2' (z_{i}^{(3)}) x^{(2)}_{j}\n$$\n\n$$\n\\frac{∂L}{∂b^{(2)}_i} \n= (\\hat{y}_{i} - y_{i}) \n    \\sigma_2' (z_i^{(3)})\n$$\n\n$$\n\\frac{∂L}{∂w^{(1)}_{ij}}\n= \\sum_{d_2=1}^{d_o} \n    (\\hat{y}_{d_2} - y_{d_2})      \n    \\sigma_2' (z_{d_2}^{(3)}) \n    w^{(2)}_{d_2 i} \n    \\sigma_1' (z_i^{(2)})\n    x^{(1)}_j\n$$\n\n$$\n\\frac{∂L}{∂b^{(1)}_i} \n= \\sum_{d_2=1}^{d_o} \n    (\\hat{y}_{d_2} - y_{d_2})      \n    \\sigma_2' (z_{d_2}^{(3)}) \n    w^{(2)}_{d_2 i} \n    \\sigma_1' (z_i^{(2)})\n$$\n\n令\n$$\n\\begin{cases}\n    \\delta^{(2)}_i \n    = (\\hat{y}_{i} - y_{i}) \n        \\sigma_2' (z_i^{(3)}) \\\\\n    \\delta^{(1)}_i \n    = \\sum_{d_2=1}^{d_o} \n        \\delta^{(2)}_{d_2} \n        w^{(2)}_{d_2 i} \n        \\sigma_1' (z_i^{(2)})\n\\end{cases}\n$$\n\n有\n$$\n\\begin{cases}\n    \\frac{∂L}{∂w^{(2)}_{ij}} = \\delta^{(2)}_i x^{(2)}_{j}\\\\\n    \\frac{∂L}{∂b^{(2)}_i}    = \\delta^{(2)}_i\\\\\n    \\frac{∂L}{∂w^{(1)}_{ij}} = \\delta^{(1)}_i x^{(1)}_j\\\\\n    \\frac{∂L}{∂b^{(1)}_i}    = \\delta^{(1)}_i\n\\end{cases}\n$$\n\n至此推导完毕。\n\n> 当隐藏层采用`Sigmoid`函数，输出层采用线性单元，可得到\n> $$\n> \\sigma_1' (z_i^{(2)}) \n> = \\sigma_1 (z_i^{(2)}) \n>     \\left[1 - \\sigma_1 (z_i^{(2)}) \\right]\n> = x_i^{(2)} (1 - x_i^{(2)})\n> $$\n> \n> $$\n> \\sigma_2' (z_i^{(3)}) = z_i^{(3)}\n> $$\n> \n> 此时\n> $$\n> \\begin{cases}\n>     \\frac{∂L}{∂w^{(2)}_{ij}} = (\\hat{y}_{i} - y_{i}) z_i^{(3)} x^{(2)}_{j} \\\\\n>     \\frac{∂L}{∂b^{(2)}_i}    = (\\hat{y}_{i} - y_{i}) z_i^{(3)} \\\\\n>     \\frac{∂L}{∂w^{(1)}_{ij}} = \\sum_{d_2=1}^{d_o} \\delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} x^{(1)}_j \\\\\n>     \\frac{∂L}{∂b^{(1)}_i}    = \\sum_{d_2=1}^{d_o} \\delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)}\n> \\end{cases}\n> $$\n> \n> 可以看到，计算梯度时使用的数据在上一次前向传播时已计算得，故可减少计算量。\n\n## 分类问题\n损失函数采用`Cross Entropy`，即\n$$\nL(\\hat{y}, y) = \n\\frac{1}{M} \\sum_{i=1}^M L(\\hat{y}^{(i)}, y^{(i)})\n$$\n\n$$\nL(\\hat{y}^{(i)}, y^{(i)}) \n= - y^{(i)T} \\log (\\hat{y}^{(i)})\n$$\n\n上式中，$y^{(i)}, \\hat{y}^{(i)}$均为列向量，且$y^{(i)}$表示`one-hot`编码后的标签向量，也可写作\n$$\nL(\\hat{y}^{(i)}, y^{(i)})\n= - \\log \\hat{y}^{(i)}_{y^{(i)}}\n$$\n\n- 由该式可以看出，若输出层激活函数采用`Sigmoid`作为激活函数，则隐藏层——输出层之间权值矩阵$W^{(2)}$只会更新$w^{(i)}_{y^{(i)} d_1}, d_1 = 1, ..., d_h$；\n- 一般采用`SoftMax`作为输出层激活函数，`Sigmoid`下面不作推导。\n\n> 关于`SoftMax`的梯度，移步[SoftMax Regression](https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/)中查看详细推导过程，这里直接给出结论。\n> 对于\n> $$\n> S(x) = \\frac {1} {\\sum_{k=1}^K exp(x_k)} \\left[ \\begin{matrix} exp(x_1)\\\\ exp(x_2)\\\\ ...\\\\ exp(x_K) \\end{matrix} \\right]\n> $$\n> \n> 其梯度为\n> $$\n> \\frac{∂S(x)}{∂x_i}_{K×1} =  \\left[ \\begin{matrix} 0\\\\ ...\\\\ p_i\\\\ ...\\\\ 0 \\end{matrix} \\right] -  \\left[ \\begin{matrix} p_i p_1\\\\ ...\\\\ p_i^2\\\\ ...\\\\ p_i p_K \\end{matrix} \\right]\n> = \\left( \\left[ \\begin{matrix}  0 \\\\ ...\\\\ 1\\\\ ...\\\\ 0 \\end{matrix} \\right] - p \\right)p_i\n> $$\n\n\n> 省略样本标记`$^{(i)}$`\n\n\n- 隐含层到输出层\n    - 对权值矩阵的梯度\n        $$\n        \\frac{∂L}{∂w^{(2)}_{ij}}\n        = - \\frac{∂}{∂w^{(2)}_{ij}} \\log \\hat{y}_{y}\n        = - \\frac{1}{\\hat{y}_y} \n            \\frac{∂\\hat{y}_{y}}{∂w^{(2)}_{ij}} \\tag{18}\n        $$\n\n        其中$\\hat{y}_{y}$与$z^{(3)}_{d_2}(d_2 = 1, ..., d_o) $均有联系，故\n        $$\n        \\frac{∂\\hat{y}_{y}}{∂w^{(2)}_{ij}}\n        = \\sum_{d2=1}^{d_o} \n            \\frac{∂\\hat{y}_{y}}{∂z^{(3)}_{d_2}} \n            \\frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}} \\tag{19}\n        $$\n\n        而\n        $$\n        \\frac{∂\\hat{y}_{y}}{∂z^{(3)}_{d_2}}\n        = \\begin{cases}\n            \\hat{y}_{y} (1 - \\hat{y}_{d_2}) & d_2 = y \\\\\n            - \\hat{y}_{y} \\hat{y}_{d_2} & otherwise\n        \\end{cases}\n        $$\n\n        $$\n        \\frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}}\n        = \\begin{cases}\n            x^{(2)}_{d_1} & i = d_2, j = d_1 \\\\\n            0 & otherwise\n        \\end{cases}\n        $$\n\n        > $z^{(3)}_{d_2} = \\sum_{d_1=0}^{d_h} w^{(2)}_{d_2d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}$\n\n        代回$(19)$，再带回$(18)$，有\n        $$\n        \\frac{∂L}{∂w^{(2)}_{ij}}\n        = - \\frac{1}{\\hat{y}_{y}} \n            \\sum_{d_2=1}^{d_o} \n            \\frac{∂\\hat{y}_{y}}{∂z^{(3)}_{d_2}} \n            x^{(2)}_{d_1} | _{d_2=i, d_1=j}\n        $$\n\n        $$\n        = \\begin{cases}\n            - \\frac{1}{\\hat{y}_{y}} \\hat{y}_{y} (1 - \\hat{y}_i) x^{(2)}_j & i = y \\\\\n            - \\frac{1}{\\hat{y}_{y}} (- \\hat{y}_{y} \\hat{y}_i) x^{(2)}_j & otherwise\n        \\end{cases}\n        $$\n\n        $$\n        = \\begin{cases}\n            (\\hat{y}_i - 1) x^{(2)}_j & i = y \\\\\n            \\hat{y}_i x^{(2)}_j & otherwise\n        \\end{cases}\n        $$\n\n        即\n        $$\n        \\frac{∂L}{∂w^{(2)}_{ij}}\n        = (\\hat{y}_i - y_i) x^{(2)}_j \\tag{*5}\n        $$\n\n    - 对偏置矩阵的梯度\n        $$\n        \\frac{∂L}{∂b^{(2)}_i}\n        = \\hat{y}_i - y_i \\tag{*6}\n        $$\n\n- 输入层到隐含层\n    - 对权值矩阵的梯度\n        $$\n        \\frac{∂L}{∂w^{(1)}_{ij}}\n        = - \\frac{∂}{∂w^{(1)}_{ij}} \\log \\hat{y}_{y}\n        = - \\frac{1}{\\hat{y}_{y}} \n            \\frac{∂\\hat{y}_{y}}{∂w^{(1)}_{ij}} \\tag{20}\n        $$\n\n        其中\n        $$\n        \\frac{∂\\hat{y}_{y}}{∂w^{(1)}_{ij}}\n        = \\sum_{d_2=1}^{d_o} \n            \\frac{∂\\hat{y}_{y}}{∂z^{(3)}_{d_2}} \n            \\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}} \\tag{21}\n        $$\n\n        $\\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}$部分与回归相同，有\n        $$\n        \\frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}\n        = w^{(2)}_{d_2 i} \\sigma_1' (z_i^{(2)}) x^{(1)}_j\n        $$\n\n        由上面分析可得\n        $$\n        \\frac{∂\\hat{y}_{y}}{∂z^{(3)}_{d_2}}\n        = \\begin{cases}\n            \\hat{y}_{y} (1 - \\hat{y}_{d_2}) & d_2 = y \\\\\n            - \\hat{y}_{y} \\hat{y}_{d_2} & otherwise\n        \\end{cases}\n        $$\n\n        故代回$(20)$可得到\n        $$\n        \\frac{∂L}{∂w^{(1)}_{ij}}\n        = - \\frac{1}{\\hat{y}_{y}}\n            \\sum_{d_2=1}^{d_o} \n            \\frac{∂\\hat{y}_{y}}{∂z^{(3)}_{d_2}} \n            \\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}\n        $$\n\n        $$\n        = - \\frac{1}{\\hat{y}_{y}}\n            \\sum_{d_2=1}^{d_o} \n            \\frac{∂\\hat{y}_{y}}{∂z^{(3)}_{d_2}} \n            w^{(2)}_{d_2 i} \\sigma_1' (z_i^{(2)}) x^{(1)}_j\n        $$\n\n        $$\n        = \\left[ \n            \\sum_{d_2=1, d_2 \\neq y}^{d_o} \\hat{y}_{d_2} w^{(2)}_{d_2 i} + \n            (\\hat{y}_y - 1) w^{(2)}_{y i} \n        \\right] \\sigma_1' (z_i^{(2)}) x^{(1)}_j\n        $$\n\n        $$\n        = \\left[ \n            \\sum_{d_2=1}^{d_o} \\hat{y}_{d_2} w^{(2)}_{d_2 i} - \n            w^{(2)}_{y i}\n        \\right] \\sigma_1' (z_i^{(2)}) x^{(1)}_j \\tag{*7}\n        $$\n\n    - 对偏置矩阵的梯度\n        $$\n        \\frac{∂L}{∂b^{(1)}_i}\n        = \\left[ \n            \\sum_{d_2=1}^{d_o} \\hat{y}_{d_2} w^{(2)}_{d_2 i} - \n            w^{(2)}_{y i}\n        \\right] \\sigma_1' (z_i^{(2)}) \\tag{*8}\n        $$\n\n至此推导完毕。\n\n> 这个推导，仅供参考\n\n\n# 过拟合问题\n和其他算法一样，前馈神经网络也存在过拟合的问题，解决方法有以下几种\n- 正则化\n    与线性回归类似，神经网络也可以加入范数惩罚项，以下$C$表示普通的损失函数，$\\lambda$为惩罚系数，$n$为样本数目，$w$表示权值参数。\n    - `L1`正则化\n        惩罚项为网络所有权值的绝对值之和。\n        $$\n        C = C_0 + \\frac{\\lambda}{n} \\sum_w |w|\n        $$\n    - `L2`正则化\n        又称权值衰减`weights decay`，惩罚项为网络所有权值的平方和。\n        $$\n        C = C_0 + \\frac{\\lambda}{2n} \\sum_w w^2\n        $$\n\n- Dropout\n    以概率大小为`p`使部分神经元输出值直接为0，如此可以使反向传播时相关权值系数不做更新，只有被保留下来的权值和偏置值会被更新。\n    ![dropout_1](/Feedforward-Neural-Network/dropout_1.png)\n    ![dropout_2](/Feedforward-Neural-Network/dropout_2.png)\n\n- 增加训练数据大小\n    可在原数据上加以变换或噪声，图像的扩增方法可查看[图像数据集扩增](https://louishsu.xyz/2018/11/02/%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%89%A9%E5%A2%9E-Augment-%E6%96%B9%E6%B3%95/)。\n\n\n# 程序\n[@Github: Code of Neural Network](https://github.com/isLouisHsu/Python-Examples-for-PyTorch-Tutorial/blob/master/NeuralNetwork_ANN_MNIST.py)\n\n使用`PyTorch`实现神经网络，以下为模型定义\n```\nclass AnnNet(nn.Module):\n    def __init__(self):\n        super(AnnNet, self).__init__()\n        self.input_size = 28 * 28\n        self.hidden_size = 100\n        self.output_size = 10\n        self.fc1 = nn.Linear(self.input_size,  self.hidden_size)    # input   - hidden\n        self.fc2 = nn.Linear(self.hidden_size, self.output_size )   # hidden  - output\n        # self.activate = nn.Sigmoid()  # 参数更新非常慢，特别是层数多时\n        self.activate = nn.ReLU()       # 事实证明ReLU作为激活函数更加合适\n        self.softmax = nn.Softmax()\n    def forward(self, X):\n        h = self.activate(self.fc1(X))\n        y_pred = self.softmax(self.fc2(h))\n        return y_pred\n```"},{"title":"分类问题的决策平面","url":"/2018/10/19/分类问题的决策平面/","content":"\n# 引言\n对于分类问题，计算结果一般为概率值，那么如何根据计算得的概率进行判别分类呢？\n> 这部分理解后，[Logistic回归](https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/)与[Softmax回归](https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/)的模型就很容易推得。\n\n# 判别函数\n对于一个类别为$K$的分类问题，如果对于所有的$ i,j=1,...,K, j\\neq i$，有\n$$\ng_i(x) > g_j(x)\n$$\n则此分类器将这个样本对应的特征向量$x$判别为$w_i$，则此分类器的作用是，计算$K$个判别函数并选取与最大判别值最大对应的类别。\n> 判别函数的形式并不唯一，可以将所有的判别函数乘上相同的正常数或者加上一个相同的常量而不影响其判决结果。更一般的情况下，我们使用单调递增函数$f(·)$进行映射，将每一个$g_i(x)$替换成$f(g_i(x))$，分类结果不变。\n>\n> <div style=\"text-align: right\"> ——《模式识别原理与应用课程笔记》</div>\n\n例如[最小风险贝叶斯决策](https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/)\n\n# 正态分布下的判别函数\n\n> [多元高斯分布（The Multivariate normal distribution） - bingjianing - 博客园](https://www.cnblogs.com/bingjianing/p/9117330.html)\n\n\n由大数定理可知，在样本足够的情况下，数据服从正态分布。多元正态分布形式如下\n$$\nf(x) = \\frac{1}{ (2\\pi)^{\\frac{n}{2}} |\\Sigma|^{\\frac{1}{2}}} \nexp(-\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu))\n$$\n\n其中\n$$ x = [x_1, ..., x_n]^T $$\n\n$$ \\mu = [\\mu_1, ..., \\mu_n]^T $$\n\n$$ \\Sigma_{ij} = cov(x_i, x_j) $$\n<!-- $$\n\\Sigma =  \n\\left[\n​    \\begin{matrix}\n​        1 & 2 & 3 \\\\\n​        4 & 5 & 6 \\\\\n​        7 & 8 & 9\n​    \\end{matrix}\n\\right] \\tag{3}\n$$ -->\n\n在[最小错误率判别](https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/)时\n\n$$g_i(x) = P(x|c_i)P(c_i)$$\n\n即\n$$\ng_i(x) = \n\\frac{1}{ (2\\pi)^{\\frac{n}{2}} |\\Sigma_i|^{\\frac{1}{2}}}\nexp(-\\frac{1}{2} (x-\\mu_i)^T \\Sigma^{-1} (x-\\mu_i)) ·\nP(c_i)\n$$\n\n取对数运算，并舍去常数项，展开整理得\n$$ g_i(x) = -\\frac{1}{2}x^T \\Sigma_i ^{-1} x + \\mu_i^T \\Sigma_i ^{-1} x  -\\frac{1}{2} \\mu_i ^T \\Sigma_i ^{-1} \\mu_i + ln P(c_i) \\tag{0}$$\n> `注：` 协方差矩阵 $ \\Sigma^T = \\Sigma $\n\n## 1. $\\Sigma_i = \\sigma^2 I$\n$\\Sigma_i^{-1} = \\frac{1}{\\sigma^2} I$代入$(0)$，有\n$$\ng_i(x) = \\frac{1}{\\sigma^2}\\mu_i^T x - \\frac{1}{2\\sigma^2} (x^Tx + \\mu_i ^T \\mu_i) + ln P(c_i)\\tag{1}\n$$\n\n定义\n$$ w_i = \\frac{1}{\\sigma^2}\\mu_i^T $$\n\n$$ w_0 =  - \\frac{1}{2\\sigma^2} (x^Tx + \\mu_i ^T \\mu_i) + ln P(c_i) $$\n\n有一般形式如下，表示取$c_i$的概率\n$$g_i(x) = w_i x + w_0\\tag{2}$$\n\n设决策平面为\n$$ w^T (x−x_0)=0\\tag{3} $$\n\n决策平面上，取$c_i$和$c_j$的概率相等，即\n$$ g_i(x) = g_j(x) $$\n\n可得\n$$ (\\mu_i - \\mu_j)^Tx = \\frac{1}{2} (\\mu_i ^T \\mu_i - \\mu_j ^T \\mu_j) -ln \\frac{P(c_i)}{P(c_j)} \\tag{4}$$\n\n> 推导过程如下，将$(1)$代入上式\n> $ \\frac{1}{\\sigma^2}\\mu_i^T x - \\frac{1}{2\\sigma^2} (x^Tx + \\mu_i ^T \\mu_i) + ln P(c_i) = \\frac{1}{\\sigma^2}\\mu_j^T x - \\frac{1}{2\\sigma^2} (x^Tx + \\mu_j ^T \\mu_j) + ln P(c_j) $\n> $ \\mu_i^T x - \\frac{1}{2} \\mu_i ^T \\mu_i + ln P(c_i) = \\mu_j^T x - \\frac{1}{2} \\mu_j ^T \\mu_j + ln P(c_j) $\n> $ (\\mu_i - \\mu_j)^Tx = \\frac{1}{2} (\\mu_i ^T \\mu_i - \\mu_j ^T \\mu_j) -ln \\frac{P(c_i)}{P(c_j)} $\n\n由$(3)$$(4)$，利用待定系数法，可得\n$$ w = \\mu_i - \\mu_j $$\n\n$$\nw^T x_0 = \\frac{1}{2} (\\mu_i ^T \\mu_i - \\mu_j ^T \\mu_j) -ln \\frac{P(c_i)}{P(c_j)}\n$$\n\n特别地，当等先验概率时，即$P(c_i) = P(c_j)$时\n$$\nw^T x_0 = \\frac{1}{2} (\\mu_i ^T \\mu_i - \\mu_j ^T \\mu_j)\n$$\n\n故\n$$\nx_0 = \\frac{1}{2}(\\mu_i + \\mu_j)\n$$\n\n结论：等先验概率时超平面$ w^T (x−x_0)=0 $平分判别空间\n> $\\mu_i$与$\\mu_j$分别表示两个类别的中心，由向量运算，$x_0$为两类中心的连线的中点。\n\n## 2. $\\Sigma_i = \\Sigma$\n代入$(0)$后可得\n$$ g_i(x) =  \\mu_i^T \\Sigma ^{-1} x - \\frac{1}{2}x^T \\Sigma ^{-1} x   -\\frac{1}{2} \\mu_i ^T \\Sigma ^{-1} \\mu_i + ln P(c_i) \\tag{5}$$\n\n定义\n$$ w_i = \\mu_i^T \\Sigma ^{-1} $$\n\n$$ w_0 = - \\frac{1}{2}x^T \\Sigma ^{-1} x   -\\frac{1}{2} \\mu_i ^T \\Sigma ^{-1} \\mu_i + ln P(c_i) $$\n\n有一般形式如下，表示取$c_i$的概率\n$$g_i(x) = w_i x + w_0\\tag{6}$$\n\n同样的，设决策平面为\n$$ w^T (x−x_0)=0\\tag{7} $$\n\n决策平面上，取$c_i$和$c_j$的概率相等，即\n$$ g_i(x) = g_j(x) $$\n\n有\n$$ (\\mu_i - \\mu_j)^T \\Sigma ^{-1} x = \\frac{1}{2} (\\mu_i - \\mu_j)^T \\Sigma ^{-1} (\\mu_i - \\mu_j) - ln \\frac{P(c_i)}{P(c_j)} \n$$\n> $ \\mu_i^T \\Sigma ^{-1} x - \\frac{1}{2}x^T \\Sigma ^{-1} x   -\\frac{1}{2} \\mu_i ^T \\Sigma ^{-1} \\mu_i + ln P(c_i) = \\mu_j^T \\Sigma ^{-1} x - \\frac{1}{2}x^T \\Sigma ^{-1} x   -\\frac{1}{2} \\mu_j ^T \\Sigma ^{-1} \\mu_j + ln P(c_j) $\n> $ \\mu_i^T \\Sigma ^{-1} x -\\frac{1}{2} \\mu_i ^T \\Sigma ^{-1} \\mu_i + ln P(c_i) = \\mu_j^T \\Sigma ^{-1} x -\\frac{1}{2} \\mu_j ^T \\Sigma ^{-1} \\mu_j + ln P(c_j) $\n> $ (\\mu_i - \\mu_j)^T \\Sigma ^{-1} x = \\frac{1}{2} (\\mu_i ^T \\Sigma ^{-1} \\mu_i + \\mu_j ^T \\Sigma ^{-1} \\mu_j) - ln \\frac{P(c_i)}{P(c_j)} $\n\n特别的，当取等先验概率时\n$$\n(\\mu_i - \\mu_j)^T \\Sigma ^{-1} x = \\frac{1}{2} (\\mu_i ^T \\Sigma ^{-1} \\mu_i + \\mu_j ^T \\Sigma ^{-1} \\mu_j)\n$$\n由$(7)$$(8)$，利用待定系数法\n$$ w^T = (\\mu_i - \\mu_j)^T \\Sigma^{-1} $$\n\n$$ w^T x_0 = \\frac{1}{2} (\\mu_i ^T \\Sigma ^{-1} \\mu_i + \\mu_j ^T \\Sigma ^{-1} \\mu_j) $$\n\n> `注：` 协方差矩阵 $ \\Sigma^T = \\Sigma $\n\n$$ w = \\Sigma^{-1}(\\mu_i - \\mu_j) $$\n\n$$ x_0 = \\frac{1}{2} (\\mu_i + \\mu_j) $$\n\n由于通常$w=Σ^{−1}(μ_i−μ_j)$并非朝着$(μ_i−μ_j)$的方向，因而通常分离两类的超平面也并非与均值的连线垂直正交。但是， 如果先验概率相等，其判定面确实是与均值连线交于中点$x_0$处的。如果先验概率不等，最优边界超平面将远离可能性较大的均值。同前，如果偏移量足够大，判定面可以不落在两个均值向量之间。\n\n## 3. $\\Sigma_i = \\Sigma_i(∀) $\n$$ g_i(x) =  -\\frac{1}{2}x^T \\Sigma_i ^{-1} x + \\mu_i^T \\Sigma_i ^{-1} x  -\\frac{1}{2} \\mu_i ^T \\Sigma_i ^{-1} \\mu_i + ln P(c_i) $$\n\n定义\n$$ W_i = -\\frac{1}{2} \\Sigma_i ^{-1} $$\n\n$$ w_i = \\mu_i^T \\Sigma_i ^{-1} $$\n\n$$ w_0 = -\\frac{1}{2} \\mu_i ^T \\Sigma_i ^{-1} \\mu_i + ln P(c_i) $$\n\n有\n$$\ng_i(x) = x^TW_ix + w_ix + w_0\n$$","categories":["Machine Learning"]},{"title":"Bayes Decision","url":"/2018/10/18/Bayes-Decision/","content":"\n# 原理\n基于贝叶斯公式\n$$P(c_k|x)=\\frac{p(x|c_k)P(c_k)}{p(x)}$$\n$$P(x)=\\sum_j p(x|c_j)P(c_j)$$\n\n# 几种常用的贝叶斯决策\n## 最小错误率贝叶斯决策\n在分类问题中，我们往往希望尽可能减少分类错误，即目标是追求最小错误率。假设有$K$分类问题，由贝叶斯公式\n$$ P(c_k|x)=\\frac{p(x|c_k)P(c_k)}{p(x)} $$\n\n上式中$ k=1,...,K  $，各部分定义如下\n> $P(c_k|x)$——`后验概率(posteriori probability)`\n> $P(c_k)$——`先验概率(priori probability)`，\n> $p(x|c_k)$——$c_k$关于$x$的`似然函数(likelihood)`，\n> $p(x)$——`证据因子(evidence)`，\n\n证据因子由下式计算\n$$p(x)=\\sum_{j=0}^K p(x|c_j)P(c_j)$$\n\n以上就是从样本中训练的参数，在预测阶段，定义决策规则为\n> $if$ $P(c_i|x)>P(c_j|x)$, $then$ $ x \\in c_i $\n\n\n由于分母为标量，对于任意输入的样本特征$x$，$P(x)$一定，故决策规则可简化为\n> $if$ $P(x|c_i)P(c_i)>P(x|c_j)P(c_j)$, $then$ $ x \\in c_i $\n\n而对于分类错误的样本，如样本$x$属于分类$c_i$，但错误分类为$c_{err}, err \\neq i$，样本的错误分类概率为\n$$P(error|x) = P(c_{err}|x)$$\n\n上式被称作`误差概率`，某类后验概率越大，则相应的误差概率就越小，定义平均误差概率\n$$P_{mean} = \\int P(error|x)P(x)dx$$\n\n\n## 带有拒绝域的最小错误率贝叶斯决策\n一些情况下，某样本对应特征$x$计算结果中，属于各类别的概率没有显著比较大的数值，换句话说都比较小，那么对这次的判别就不太信任，选择拒绝决策结果。\n将决策平面划分为两个区域\n$$ Acquired = \\{x|max_j P(c_j|x)\\geq 1-t\\} $$\n$$ Rejected = \\{x|max_j P(c_j|x) < 1-t\\} $$\n\n其中$t$为阈值，$t$越小时，拒绝域$Rejected$越大，当满足\n$$ 1-t \\leq \\frac{1}{K} $$\n或者 \n$$ t \\geq \\frac{K-1}{K} $$\n\n此时拒绝域为\n$$ Rejected = \\{x|max_j P(c_j|x) < \\frac{1}{K}\\} $$\n\n而当且仅当各分类概率相等时才有 $ max_j P(c_j|x) = \\frac{1}{K} $，因此此时拒绝域为空，接受所有决策结果\n\n\n## 最小风险贝叶斯决策\n在决策过程中，不同类型的决策错误所产生的代价是不同的。引入风险函数\n$$ \\lambda_{i, j} = \\lambda (\\alpha_i|c_j) $$\n表示实际类别为$c_j$时，采取错误判断为$c_i$的行为$\\alpha_i$所产生的损失。该函数称为损失函数，通常它可以用表格的形式给出，叫做决策表，形如\n![决策表](/Bayes-Decision/decision_table.jpg)\n定义条件风险\n$$ R(\\alpha_i|c_j) = \\sum_j \\lambda (\\alpha_i|c_j) P(c_j|x) $$\n特别地，取$0-1$损失时，即最小错误率贝叶斯决策\n$$\n\\lambda (\\alpha_i|c_j) = \\begin{cases}\n0 & i = j \\\\\n1 & i \\neq j\n\\end{cases}\n$$\n<!-- \n$$\n函数名 = \\begin{cases}\n公式1 & 条件1 \\\\\n公式2 & 条件2 \\\\\n公式3 & 条件3 \n\\end{cases}\n$$\n-->\n\n可能比较抽象，这里举了一个例子\n<div style=\"align: center\">\n<img src=/Bayes-Decision/最小风险贝叶斯决策例.png/>\n</div>\n\n# 关于判别函数\n可查看[分类问题的决策平面](https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/)\n\n# 程序\n\n![例4.1](/Bayes-Decision/李航-例4.1.png)\n\n为帮助理解，先手动计算一遍结果\n> 先验概率(`priori probability`):\n> $ P(Y = -1) = \\frac{6}{15} $\n> $ P(Y = 1) = \\frac{9}{15} $\n> 似然函数(`likelihood`)\n> $ P(X^{(1)} = 1|Y=-1) = \\frac{3}{6}$\n> $ P(X^{(1)} = 2|Y=-1) = \\frac{2}{6}$\n> $ P(X^{(1)} = 3|Y=-1) = \\frac{1}{6}$\n> $ P(X^{(2)} = S|Y=-1) = \\frac{3}{6}$\n> $ P(X^{(2)} = M|Y=-1) = \\frac{2}{6}$\n> $ P(X^{(2)} = L|Y=-1) = \\frac{1}{6}$\n> $ P(X^{(1)} = 1|Y=1) = \\frac{2}{9}$\n> $ P(X^{(1)} = 2|Y=1) = \\frac{3}{9}$\n> $ P(X^{(1)} = 3|Y=1) = \\frac{4}{9}$\n> $ P(X^{(2)} = S|Y=1) = \\frac{1}{9}$\n> $ P(X^{(2)} = M|Y=1) = \\frac{4}{9}$\n> $ P(X^{(2)} = L|Y=1) = \\frac{4}{9}$\n\n\n注意：证据因子(`evidence`)不能用如下朴素贝叶斯求解\n$$ P(X) = P(X^{(1)}) P(X^{(2)})$$\n\n\n而是\n$$ P(X) =  P(X^{(1)}|Y=-1)P(Y = -1) + P(X^{(2)}|Y=-1)P(Y = -1)$$\n\n一般分子用朴素贝叶斯求解\n$$ P(X|Y) = P(X^{(1)}|Y) P(X^{(2)}|Y) $$\n\n将其加和作为分母\n$$ c_k: P(X)_k = \\sum_{k=0}^2 P(X^{(1)}|Y=k) P(X^{(2)}|Y=k) $$\n\n$$ P(Y_k|X) = \\frac{P(X|Y_k)P(Y_k)}{P(X)_k} $$\n\n选取最大概率的$ k $类别作为判别类别\n$$ k = argmax_k P(Y_k|X) $$\n\n\n## 代码\n[@Github: Code for Naive Bayes Decision](https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Statistical%20Learning%20Method%2C%20Li%20Hang/naive_bayes_algorithm_demo.py)\n\n### training step\n```\ndef fit(self, X, y):\n    X_encoded = self.featureEncoder.fit_transform(X).toarray()\n    y_encoded = OneHotEncoder().fit_transform(y.reshape((-1, 1))).toarray()\n    self.P_X = np.mean(X_encoded, axis=0)                           # one-hot编码下，各列的均值即各特征的概率\n    self.P_Y = np.mean(y_encoded, axis=0)                           # one-hot编码下，各列的均值即各了别的概率\n    self.n_labels, self.n_features = y_encoded.shape[1], X_encoded.shape[1]   \n    self.P_X_Y = np.zeros(shape=(self.n_labels, self.n_features))   # 各个类别下，分别统计各特征的概率\n    for i in range(self.n_labels):\n        X_encoded_of_yi = X_encoded[y_encoded[:, i]==1]             # 取出属于i类别的样本\n        self.P_X_Y[i] = np.mean(X_encoded_of_yi, axis=0)            # one-hot编码下，各列的均值即各特征的概率\n```\n### predict step\n```\ndef predict(self, X):\n    X_encoded = self.featureEncoder.transform(X).toarray()\n    n_samples = X_encoded.shape[0]\n    y_pred_prob = np.zeros(shape=(n_samples, self.n_labels))\n    for i in range(n_samples):\n        for j in range(self.n_labels):\n            P_Xi_encoded_Yj = X_encoded[i] * self.P_X_Y[j]          # 在Yj类别下，选出输入样本Xi对应的条件概率\n            P_Xi_encoded_Yj[P_Xi_encoded_Yj==0.0] = 1.0             # 将为0值替换为1，便于求解ΠP(Xi|yc)，只要将各元素累乘即可\n            y_pred_prob[i, j] = self.P_Y[j] * P_Xi_encoded_Yj.prod()\n        y_pred_prob[i] /= np.sum(y_pred_prob[i])                    # 分母一般是将分子加和，不能假定各特征独立并用朴素贝叶斯计算分母\n    return np.argmax(y_pred_prob, axis=1)\n```\n### main\n```\nX = [\n    [1, 0], [1, 1], [1, 1], [1, 0], [1, 0],\n    [2, 0], [2, 1], [2, 1], [2, 2], [2, 2],\n    [3, 2], [3, 1], [3, 2], [3, 2], [3, 2]\n]\ny = [0 ,0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n\nestimator = NaiveBayes()\nestimator.fit(X, y)\n\nX_test = np.array([[2, 0], [1, 1]])\ny_pred = estimator.predict(X_test)\n```","categories":["Machine Learning"]},{"title":"Softmax Regression","url":"/2018/10/18/Softmax-Regression/","content":"\n> [Unsupervised Feature Learning and Deep Learning Tutorial](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)\n\n# 引言\n`Logistic Regression`中采用的非线性函数为`Sigmoid`，将输出值映射到$(0, 1)$之间作为概率输出，处理的是二分类问题，那么对于多分类的问题怎么处理呢？\n\n# 模型\n> 由[Logistic回归](https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/)推广而来\n\n## Softmax\n`Softmax`在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类$(K>2)$问题，分类器最后的输出单元需要`Softmax`函数进行数值处理。\n$$\nS(x) = \\frac\n            {1}\n            {\\sum_{k=1}^K exp(x_k)}\n            \\left[\n                \\begin{matrix}\n                    exp(x_1)\\\\\n                    exp(x_2)\\\\\n                    ...\\\\\n                    exp(x_K)\n                \\end{matrix}\n            \\right]\n$$\n\n其中$x$为矩阵形式的向量，其维度为$(K×1)$，$K$为类别数目。`Softmax`的输出向量维度与$x$相同，各元素$x_i$加和为$1$，可用于表示取各个类别的概率。\n\n注意到，对于函数$e^x$\n$$\\lim_{x \\rightarrow - \\infty} e^x = 0 $$\n\n$$\\lim_{x \\rightarrow + \\infty} e^x = +\\infty$$\n\n> 假设所有的$x_i$等于某常数$c$，理论上对所有$x_i$上式结果为$\\frac{1}{n}$\n> - 若$c$为很小的负数，$e^c$下溢，结果为$NaN$；\n> - 若$c$量级很大，$e^c$上溢，结果为$NaN$。\n\n在数值计算时并不稳定，但是`Softmax`所有输入增加同一常数时，输出不变，得稳定版本：\n$$\nS(x) := S(x - max(x_i))\n$$\n\n> $$ e^{x_{max} - max(x_i)} = 1 $$\n> - 减去最大值导致$e^x$最大为$1$，排除上溢；\n> - 分母中至少有一项为$1$，排除分母下溢导致处以$0$的情况。\n\n\n> 其对数\n> $$\n> log S(x)_i = x_i - log ({\\sum_{k=1}^K exp(x_k)})\n> $$\n> - 注意到，第一项表示输入$x_i$总是对代价函数有直接的贡献。这一项不会饱和，所以即使$x_i$对上式的第二项的贡献很小，学习依然可以进行；\n> - 当最大化对数似然时，第一项鼓励$x_i$被推高，而第二项则鼓励所有的$x$被压低；\n> - 第二项$log ({\\sum_{k=1}^K exp(x_k)})$可以大致近似为$max(x_k)$，这种近似是基于对任何明显小于$max(x_k)$的$x_k$都是不重要的，**负对数似然代价函数总是强烈地惩罚最活跃的不正确预测**\n> - 除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习\n> ---------------------\n> 作者：NirHeavenX \n> 来源：CSDN \n> 原文：https://blog.csdn.net/qsczse943062710/article/details/61912464 \n> 版权声明：本文为博主原创文章，转载请附上博文链接！\n\n\n## Softmax解决多分类问题\n\n对于具有$K$个分类的问题，每个类别训练一组参数$ w_k $\n$$z_k^{(i)} = w_k^Tx^{(i)}$$\n\n或写作矩阵形式\n$$ z^{(i)} = W^Tx^{(i)} $$\n\n其中\n$$\nx^{(i)} = \n    \\left[\n        \\begin{matrix}\n            x_0^{(i)}\\\\\n            x_1^{(i)}\\\\\n            ...\\\\\n            x_n^{(i)}\n        \\end{matrix}\n    \\right]_{n×1},\nx_0^{(i)}=1\n$$\n\n$$\nW = [w_1, w_2, ..., w_K]_{(n+1)×K}\n$$\n\n$$\nw_i = \n    \\left[\n        \\begin{matrix}\n            w_{i0}\\\\\n            w_{i1}\\\\\n            ...\\\\\n            w_{in}\n        \\end{matrix}\n    \\right]_{n×1}\n$$\n\n最终各类别输出概率为\n$$\n\\hat{y}^{(i)} = Softmax(z^{(i)})\n$$\n\n> **产生了一个奇怪的脑洞。。。**\n> 二分类问题\n> $$\n> p(x_1) = \\frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \\frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }\n> $$\n> \n> 定义二分类线性单元输出的差值为\n> $$\n> z = x_1 - x_2\n> $$\n> \n> 得到\n> $$\n> p(x_1) = \\frac{1}{1 + e^{-z}}\n> $$\n> \n> 以$x_1 = [x_{11}, x_{12}]^T$为例(二维特征)，取$w_1=1, w_2=2, b=3$\n> $$\n> p(x_1) = \\frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}}\n> $$\n> \n> ![特征为2时的决策平面](Softmax-Regression/Sigmoid_2dim.png)\n> \n> 而多分类问题，以$3$分类为例\n> $$\n> p(x_1) = \\frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \\frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }\n> $$\n> \n> 定义线性单元输出的差值为\n> $$\n> z_{12} = x_1 - x_2\n> $$\n> \n> $$\n> z_{13} = x_1 - x_3\n> $$\n> \n> $$ \n> p(x_1) = \\frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }\n> $$\n> \n> 做出图像为\n> ![3D_sigmoid_1](Softmax-Regression/3D_sigmoid_1.png)\n> ![3D_sigmoid_2](Softmax-Regression/3D_sigmoid_2.png)\n\n# 损失函数\n## 由交叉熵理解\n$$ CrossEnt = \\sum_j p_j log \\frac{1}{q_j} $$\n\n而对于样本$ (X^{(i)}, y^{(i)}) $，为确定事件，故标签概率各元素的取值$p_j$为$ y^{(i)}_j ∈ \\{0,1\\}$，$ q_j即预测输出的概率值\\hat{y}^{(i)}_j$\n\n一般取各个样本损失的均值$(\\frac{1}{N})$\n$$\nL(\\hat{y}, y) = - \\frac{1}{N} \\sum_{i=1}^N 1\\{y^{(i)}_j=k\\} \nlog (\\hat{y}^{(i)}_j)\n$$\n\n$$\n1\\{y^{(i)}_j=k\\} = \n    \\begin{cases}\n        1 & y^{(i)}_j = k \\\\\n        0 & y^{(i)}_j \\neq k \n    \\end{cases}\n$$\n\n可对实际标签$y^{(i)}$采取`One-Hot`编码，便于计算\n$$\ny^{(i)} = \\left[ \n        \\begin{matrix}\n            0, ..., 1_{y^{(i)}}, ..., 0\n        \\end{matrix}\n     \\right]^T\n$$\n\n则\n$$\nL(\\hat{y}, y) = - \\frac{1}{N} \\sum_{i=1}^N y^{(i)T}\nlog (\\hat{y}^{(i)})\n$$\n\n## 由决策平面理解\n\n从[贝叶斯决策](https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/)和[分类问题的决策平面](https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/)可知，对于类别$c_i$，有\n$$\nP(c_i|x) = \\frac{P(x|c_i)}{\\sum_{j=0}^KP(x|c_j)}\n$$\n\n>假设每个类别的样本服从正态分布，先验概率相等，各类别样本特征间协方差相等。证明略.\n\n\n# 梯度推导\n\n## Softmax函数的导数\n对于\n$$\nS(x) = \\frac\n            {1}\n            {\\sum_{k=1}^K exp(x_k)}\n            \\left[\n                \\begin{matrix}\n                    exp(x_1)\\\\\n                    exp(x_2)\\\\\n                    ...\\\\\n                    exp(x_K)\n                \\end{matrix}\n            \\right]\n$$\n\n一般输出作为概率值，记\n$$ P = S(x) $$\n\n$$ p_i = S(x)_i $$\n\n对向量$x$中某元素求导\n$$\n\\frac{∂S(x)}{∂x_i} = \\frac{∂}{∂x_i}\n                    \\left[\n                        \\begin{matrix}\n                            ...\\\\\n                            \\frac{exp(x_k)}{\\sum_{j=1}^K exp(x_j)}\\\\\n                            ...\\\\\n                        \\end{matrix}\n                    \\right]\n$$\n\n> $(1)$ $i=k$ \n> $\n> \\frac{∂}{∂x_i} \\frac{exp(x_i)}{\\sum_{j=1}^K exp(x_j)}$ \n> $ = \\frac{exp'(x_i)·\\sum_{j=1}^K exp(x_j) - exp(x_i)·(\\sum_{j=1}^K exp(x_j))'}\n> {(\\sum_{j=1}^K exp(x_j))^2}$ \n> $ = \\frac{exp(x_i)·\\sum_{j=1}^K exp(x_j) - exp^2(x_i)}\n> {(\\sum_{j=1}^K exp(x_j))^2}$\n> $ = \\frac{exp(x_i)}{\\sum_{j=1}^K exp(x_j)} - \n> (\\frac{exp(x_i)}{\\sum_{j=1}^K exp(x_j)})^2\n> $\n> $ = p_i (1 - p_i)\n> $\n> \n> $(2)$ $i\\neq k$\n> $\n> \\frac{∂}{∂x_i} \\frac{exp(x_k)}{\\sum_{j=1}^K exp(x_j)}$ \n> $ = \\frac{exp'(x_k)·\\sum_{j=1}^K exp(x_j) - exp(x_k)·(\\sum_{j=1}^K exp(x_j))'}\n> {(\\sum_{j=1}^K exp(x_j))^2}$ \n> $ = \\frac{- exp(x_k)exp(x_i)}\n> {(\\sum_{j=1}^K exp(x_j))^2}$\n> $= - p_i p_k$\n> \n> 综上\n> $$\n> \\frac{∂S(x)}{∂x_i}_{K×1} =  \\left[\n>                           \\begin{matrix}\n>                               0\\\\\n>                               ...\\\\\n>                               p_i\\\\\n>                               ...\\\\\n>                               0\n>                            \\end{matrix}\n>                       \\right] - \n>                       \\left[\n>                           \\begin{matrix}\n                                p_i p_1\\\\\n                                ...\\\\\n                                p_i^2\\\\\n                                ...\\\\\n                                p_i p_K\n                            \\end{matrix}\n>                       \\right]\n> =     \\left(\n>                       \\left[\n>                           \\begin{matrix}\n>                               0\\\\\n>                               ...\\\\\n>                               1\\\\\n>                               ...\\\\\n>                               0\n>                            \\end{matrix}\n>                       \\right] - \n>                       p\n>       \\right)p_i\n> $$\n> \n\n## 损失函数梯度\n在`OneHot`编码下，损失函数形式为\n$$\nL(\\hat{y},y) = \\frac{1}{N} \n\\sum_{i=1}^N \nL (y^{(i)}, \\hat{y}^{(i)})\n$$\n\n$$\nL (y^{(i)}, \\hat{y}^{(i)}) = - \ny^{(i)T}\nlog \\hat{y}^{(i)}\n$$\n\n$$\n\\hat{y}^{(i)} = S(z^{(i)})\n$$\n\n$$\nz^{(i)} = W^T x^{(i)}\n$$\n\n即只考虑实际分类对应的概率值\n$$\nL (y^{(i)}, \\hat{y}^{(i)}) = - log \\hat{y}^{(i)}_{y^{(i)}}\n$$\n\n\n> 由于 $S(z^{(i)})_{t^{(i)}}$与$z^{(i)}$向量各个元素都有关，由链式求导法则\n> $$ \\frac{∂ L^{(i)} }{∂w_{pq}} = - \n> \\frac{1}{ \\hat{y}^{(i)}_{y^{(i)}} } \n> (\n> \\sum_{k=1}^K\n>   \\frac{∂ \\hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}\n>   \\frac{∂z^{(i)}_k}{∂w_{pq}}\n> )\n> $$\n> $1.$ 考察 $\\frac{∂ \\hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}$\n> $$\n>   \\frac{∂ \\hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} = \n> ​      \\begin{cases}\n> ​          \\hat{y}^{(i)}_{y^{(i)}} (1 - \\hat{y}^{(i)}_k) & k=y^{(i)} \\\\\n> ​          - \\hat{y}^{(i)}_{y^{(i)}} \\hat{y}^{(i)}_k & k \\neq y^{(i)} \n> ​      \\end{cases}\n$$\n> \n> $2.$ 考察 $\\frac{∂z^{(i)}_k}{∂w_{pq}}$\n> $$\n>   \\frac{∂z^{(i)}_k}{∂w_{pq}} = \n>       \\begin{cases}\n            \\frac{∂z^{(i)}_k}{∂w_{pq}} = x^{(i)}_p & k=q\\\\\n            \\frac{∂z^{(i)}_k}{∂w_{pq}} = 0 & k \\neq q\n        \\end{cases}\n> $$\n\n\n综上所述\n$$ \\frac{∂ L^{(i)} }{∂w_{pq}} \n= - \n\\frac{1}{ \\hat{y}^{(i)}_{y^{(i)}} } \n  \\frac{∂ \\hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}\n  \\frac{∂z^{(i)}_q}{∂w_{pq}} \n$$\n\n其中\n$$\n\\frac{∂ \\hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}\n= \\begin{cases}\n        \\hat{y}^{(i)}_{y^{(i)}} (1 - \\hat{y}^{(i)}_q) & q = y^{(i)}\\\\\n        - \\hat{y}^{(i)}_{y^{(i)}} \\hat{y}^{(i)}_q & q \\neq y^{(i)}\n    \\end{cases}\n$$\n\n$$\n\\frac{∂z^{(i)}_q}{∂w_{pq}} = x^{(i)}_p\n$$\n\n故对于单个样本$(X^{(i)}, y^{(i)})$，当样本标签采用$OneHot$编码时\n$$ \n\\frac{∂L^{(i)}}{∂w_{pq}}  \n= - \\frac{1}{ \\hat{y}^{(i)}_{y^{(i)}} } \n  \\frac{∂ \\hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}\n  x^{(i)}_p\n= \\begin{cases}\n    (\\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\\\\n    \\hat{y}^{(i)}_qx^{(i)}_p & q \\neq y^{(i)}\n\\end{cases}\n$$\n\n> 注： 这里可以约分去掉$\\hat{y}^{(i)}_{y^{(i)}}$\n\n\n$$\n\\frac{∂L^{(i)}}{∂w_{pq}} \n= ( \\hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_p\n$$\n\n更一般的，写成矩阵形式，记$X = [x_1, x_2, ..., x_m]^T$，$x_i$为样本特征(列向量)\n$$\n∇_W L = X^T(\\hat{Y} - Y)\n$$\n\n> **用线性模型解决分类和回归问题时，形式竟如此统一!**\n\n\n至此为止，梯度推导结束，利用梯度下降法迭代求解参数矩阵$W$即可。\n$$\nW := W - \\alpha ∇_W L\n$$\n\n# 代码\n[@GitHub: Code of Softmax Regression](https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex3-2-softmax_regression)\n\n## Softmax\n```\ndef softmax(X):\n    \"\"\" 数值计算稳定版本的softmax函数\n    @param {ndarray} X: shape(batch_size, n_labels)\n    \"\"\"\n    X_max = np.max(X, axis=1).reshape((-1, 1))  # 每行的最大值\n    X = X - X_max\t                        # 每行减去最大值\n    X = np.exp(X)\n    return X / np.sum(X, axis=1).reshape((-1, 1))\n```\n\n## cost function\n```\ndef crossEnt(self, y_label_true, y_prob_pred):\n    \"\"\" 计算交叉熵损失函数\n    @param {ndarray} y_label_true: 真实标签 shape(batch_size,)\n    @param {ndarray} y_prob_pred: 预测输出 shape(batch_size, n_labels)\n    \"\"\"\n    mask = self.encoder.transform(y_label_true.reshape(-1, 1)).toarray()  # shape(batch_size, n_labels)\n    y_prob_masked = np.sum(mask * y_prob_pred, axis=1)          # 每行真实标签对应的预测输出值\n    y_prob_masked[y_prob_masked==0.] = 1.\n    y_loss = np.log(y_prob_masked)\n    loss = - np.mean(y_loss)                                    # 求各样本损失的均值\n    return loss\n```\n\n## gradient\n```\ndef grad(self, X_train, y_train, y_prob_pred):\n    \"\"\" 计算梯度 \\frac {∂L} {∂W_{pq}}\n    @param X_train: 训练集特征\n    @param y_train: 训练集标签\n    @param y_prob_pred:  训练集预测概率输出\n    @param y_label_pred: 训练集预测标签输出\n    \"\"\"\n    y_train = self.encoder.transform(y_train)\n    dW = X_train.T.dot(y_prob_pred - y_train)\n    return dW\n```\n\n## training step\n省略可视化和验证部分的代码\n```\ndef fit(self, X_train, X_valid, y_train, y_valid, min_acc=0.95, max_epoch=20, batch_size=20):\n    \"\"\" 训练\n    \"\"\"\n    # 添加首1列，输入到偏置w0\n    X_train = np.c_[np.ones(shape=(X_train.shape[0],)), X_train]\n    X_valid = np.c_[np.ones(shape=(X_valid.shape[0],)), X_valid]\n    X_train = self.scaler.fit_transform(X_train)    # 尺度归一化\n    X_valid = self.scaler.transform(X_valid)        # 尺度归一化\n    self.encoder.fit(y_train.reshape(-1, 1))\n    self.n_features = X_train.shape[1]\n    self.n_labels = self.encoder.transform(y_train).shape[1]\n    # 初始化参数\n    self.W = np.random.normal(loc=0, scale=1.0, size=(self.n_features, self.n_labels))\n    n_batch = X_train.shape[0] // batch_size\n        \n    # 可视化相关\n    plt.ion()\n    plt.figure('loss'); plt.figure('accuracy')\n    loss_train_epoch = []; loss_valid_epoch = []\n    acc_train_epoch = [];  acc_valid_epoch = []\n    for i_epoch in range(max_epoch):\n        for i_batch in range(n_batch):              # 批处理梯度下降\n            n1, n2 = i_batch * batch_size, (i_batch + 1) * batch_size\n            X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2]\n            # 预测\n            y_prob_train = self.predict(X_train_batch, preprocessed=True)\n            # 计算损失\n            loss_train_batch = self.crossEnt(y_train_batch, y_prob_train)\n            # 计算准确率\n            y_label_train = np.argmax(y_prob_train, axis=1)\n            a = y_train_batch.reshape((-1,))\n            acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((-1,))).astype('float'))\n            # 计算梯度 dW\n            dW = self.grad(X_train_batch, y_train_batch, y_prob_train)\n            # 更新参数\n            self.W -= self.lr * dW\n```\n## predict step\n```\ndef predict(self, X, preprocessed=False):\n    \"\"\" 对输入的样本进行预测，输出标签\n    @param {ndarray} X: shape(batch_size, n_features)\n    @return {ndarray} y_prob: probability, shape(batch_size, n_labels)\n            {ndarray} y_label: labels, shape(batch_size,)\n    \"\"\"\n    if not preprocessed:    # 训练过程中调用此函数时，不用加首1列\n        X = np.c_[np.ones(shape=(X.shape[0],)), X]              # 添加首1项，输入到偏置w0\n    X = self.scaler.transform(X)\n\n    y_prob = softmax(X.dot(self.W))                             # 预测概率值 shape(batch_size, n_labels)\n    return y_prob\n```\n## 实验结果\n以下蓝线为训练集参数，红线为验证集参数，若稳定训练(如`batch_size = 20`的结果)，最终准确率在$80\\%$左右。\n> - 由于`随机梯度下降(SGD)`遍历次数太多，运行较慢，没有用`SGD`方法训练，就前几个`epoch`来看，效果没有`batch_size = 20`的好；\n> - 添加隐含层形成三层结构的`前馈神经网络`，可提高准确率；\n> - 还有一点，使用`批处理梯度下降(n_batch = 1)`训练时，可以看到损失值已经趋于$0$，但准确率却很低，说明已经陷入局部最优解。\n\n- batch size = 20\n    - 损失\n    ![loss_batchsize_20](/Softmax-Regression/loss_batchsize_20.png)\n    - 准确率\n    ![accuracy_batchsize_20](/Softmax-Regression/accuracy_batchsize_20.png)\n\n- batch_size = 200\n    - 损失\n    ![loss_batchsize_200](/Softmax-Regression/loss_batchsize_200.png)\n    - 准确率\n    ![accuracy_batchsize_200](/Softmax-Regression/accuracy_batchsize_200.png)\n\n- n_batch = 1\n    - 损失\n    ![loss_batch_1](/Softmax-Regression/loss_batch_1.png)\n    - 准确率\n    ![accuracy_batch_1](/Softmax-Regression/accuracy_batch_1.png)\n\n# 感悟\n推公式要我老命。。。。\n\n\n`Softmax`回归可以视作**不含隐含层的[前馈神经网络](https://louishsu.xyz/2018/10/20/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/)**。","categories":["Machine Learning"]},{"title":"Logistic Regression","url":"/2018/10/18/Logistic-Regression/","content":"\n# 引言\n逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。\n\n# 模型\n先给出模型，推导过程稍后给出，逻辑回归包含`Sigmoid`函数\n$$f(z) = \\frac{1}{1+e^{-z}}$$\n\n其图像如下\n![`Sigmod函数`](Logistic-Regression/Sigmoid.png)\n\n定义\n$$z = w^Tx$$\n\n其中$x=[x_0, x_1, ..., x_n]^T, x_0=1$\n$$h_w(x) = g(z) =  \\frac{1}{1+e^{-z}}$$\n\n# 损失函数\n\n## 由最大似然估计推导\n对于二元分类问题，其取值作为随机变量，服从二项分布 $B(1, p)$，其中$p$即为预测输出概率$\\hat{y}$\n$$ P(y_i^{(i)}) = (\\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\\hat{y}_i^{(i)})^{1-y_i^{(i)}} $$\n由极大似然估计\n$$L = \\prod_{i=0}^N P(y_i^{(i)}) = \\prod_{i=0}^N (\\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\\hat{y}_i^{(i)})^{1-y_i^{(i)}} $$\n\n取对数似然函数\n$$logL = \\sum_{i=0}^N [y_i^{(i)} log \\hat{y}_i^{(i)} + (1-y_i^{(i)}) log (1-\\hat{y}_i^{(i)})]$$\n\n优化目标是\n$$ w = argmax_w logL $$\n\n优化问题一般表述成`minimize`问题，添加负号，构成`Neg Log Likelihood`损失\n$$ w = argmin_w (-logL) $$\n\n一般取均值\n$$L(\\hat{y}, y)=- \\frac{1}{N} \\sum_i [y_i^{(i)} log(\\hat{y}_i^{(i)})+(1 - y_i^{(i)})log(1-\\hat{y}_i^{(i)})]$$\n\n其中$y_i$表示真实值，$\\hat{y}_i$表示预测值\n\n\n## 从交叉熵理解\n已知交叉熵`cross entropy`定义如下\n$$ CrossEnt = \\sum_i p_i log \\frac{1}{q_i} $$\n\n而对于样本$ (X_i, y_i) $，为确定事件，故标签概率的取值为$ p_i = y_i ∈ \\{0,1\\}$，$ q_i即预测输出的概率值\\hat{y}_i $，可得到与上面相同的推导结论\n\n\n## 从决策平面和贝叶斯决策理解\n相关内容查看[分类问题的决策平面](https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/)和[贝叶斯决策](https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/)，逻辑回归考虑的一般是等先验概率问题，故决策函数定义为\n> $if$ $P(c_i|x)>P(c_j|x)$ $then$ $ x \\in c_i $,  $ i, j = 1, 2 $\n\n从贝叶斯决策可知，对于类别$c_1$，有\n$$\nP(c_1|x) = \\frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}\n$$\n\n设在各个类别下，特征$x$服从正态分布\n$$\nP(x|c_i) = \\frac{1}{ (2\\pi)^{\\frac{n}{2}} |\\Sigma_i|^{\\frac{1}{2}}}\nexp(-\\frac{1}{2} (x-\\mu_i)^T \\Sigma^{-1} (x-\\mu_i))\n$$\n\n则\n$$\nP(c_1|x) = \\frac\n{1}\n{\n    1 + exp(-z)\n} \n$$\n\n$$\nP(c_2|x) = 1 - P(c_1|x) = \\frac{exp(-z)}{1+exp(-z)}\n$$\n\n> $\nP(c_1|x) = \\frac\n{exp(-\\frac{1}{2} (x-\\mu_1)^T \\Sigma_1^{-1} (x-\\mu_1)}\n{exp(-\\frac{1}{2} (x-\\mu_1)^T \\Sigma_1^{-1} (x-\\mu_1) + exp(-\\frac{1}{2} (x-\\mu_2)^T \\Sigma_2^{-1} (x-\\mu_2)}\n$\n>\n> $\nP(c_1|x) = \\frac\n{1}\n{1 + \\frac{exp(-\\frac{1}{2} (x-\\mu_2)^T \\Sigma_2^{-1} (x-\\mu_2)}{exp(-\\frac{1}{2} (x-\\mu_1)^T \\Sigma_1^{-1} (x-\\mu_1)}\n}\n$\n>\n> 假定各分类的样本方差相等，$ \\Sigma_1 = \\Sigma_2 = \\sigma^2 I $\n> \n> $ P(c_1|x) = \\frac {1}{1 + exp(- [ \\frac{1}{\\sigma^2} (\\mu_1-\\mu_2)^T x - \\frac{1}{2 \\sigma^2} (\\mu_1^T\\mu_1 - \\mu_2^T\\mu_2) ])} \n> $\n>\n> 令\n> $$ w = \\frac{1}{\\sigma^2} (\\mu_1 -\\mu_2) $$\n> \n> $$b = - \\frac{1}{2\\sigma^2}(\\mu_1^T \\mu_1 - \\mu_2^T \\mu_2)$$\n> \n> 即可得到\n> $$\nP(c_1|x) = \\frac\n{1}\n{\n    1 + exp(-z)\n} \n$$\n>\n> 其中\n> $$ z = w^T x + b $$\n\n# 梯度推导\n先推导`Sigmoid`函数的导数\n$$\nf'(z) = (1 - f(z))f(z)\n$$\n值得注意的是，从$f'(z)$的图像可以看到，在$ x=0 $处$f'(z)$取极大值，且\n$$\nf'(z)_{max} = f'(z)|_{z=0} = 0.25\n$$\n\n$$\n\\lim_{z \\rightarrow \\infty} f'(z) = 0\n$$\n\n在多层神经网络反向传播更新参数时，由于梯度多次累乘，`Sigmoid`作为[激活函数](https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/)会存在“梯度消失”的问题，使得参数更新非常缓慢。\n\n![`Sigmod导函数`](Logistic-Regression/Sigmoid_gradient.png)\n\n> $ f'(z) $\n> $ = (\\frac{1}{1+e^{-z}})' $\n> $ = \\frac\n> ​          {-(1+e^{-z})'}\n> ​          {(1+e^{-z})^2} $\n> $ = \\frac\n> ​          {e^{-z}}\n> ​          {(1+e^{-z})^2} $\n> $ = \\frac\n> ​          {e^{-z}}\n> ​          {1+e^{-z}} \n> ​    \\frac\n> ​          {1}\n> ​          {1+e^{-z}}$\n> $ = (1 - f(z))f(z)$\n\n\n利用链式求导法则可得\n> $\\frac{∂L}{∂w_j}$\n> $= -\\frac{∂}{∂w_j} \\frac{1}{N} \\sum_i [y^{(i)} log(\\hat{y}^{(i)})+(1-y^{(i)})log(1-\\hat{y}^{(i)})]$  \n> $= - \\frac{1}{N} \\sum_i [y^{(i)} \\frac{1}{\\hat{y}^{(i)}}\\frac{∂}{∂w_j}\\hat{y}^{(i)}-(1-y^{(i)})\\frac{1}{1-\\hat{y}^{(i)}}\\frac{∂}{∂w_j}\\hat{y}^{(i)}]$\n> $= - \\frac{1}{N} \\sum_i [y^{(i)} \\frac{1}{\\hat{y}^{(i)}}\\hat{y}^{(i)}(1-\\hat{y}^{(i)})w_j-(1-y^{(i)})\\frac{1}{1-\\hat{y}^{(i)}}\\hat{y}^{(i)}(1-\\hat{y}^{(i)})w_j]$\n> $= - \\frac{1}{N} \\sum_i [y^{(i)} (1-\\hat{y}^{(i)})w_j-(1-y^{(i)}) y^{(i)} w_j]$\n> $=  \\frac{1}{N} \\sum_i (\\hat{y}^{(i)} - y^{(i)})w_j $\n\n写作矩阵形式，记$X = [x_1, x_2, ..., x_m]^T$，$x_i$为样本特征(列向量)\n$$\n∇_w L = X^T (\\hat{Y} - Y)\n$$\n\n# 训练\n和线性回归一样，采用梯度下降法求解\n$$ w := w - \\alpha ∇_w L $$\n\n# 处理多分类问题\n假设有$K$个类别，则依次以类别$c_i$为正样本训练模型，一共训练$K$个。测试样本在每个模型上计算，最终将概率最大的作为分类结果。\n> 这样划分数据集，会使训练集正负样本数目严重不对称，特别是类别很多的情况，对结果会产生影响。可推广至[softmax回归](https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/)解决这个问题。\n\n# 程序\n\n## 代码\n[@Github: Code for Logistic Regression](https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex2-logisticregression/LogReg.py)\n## cost function\n```\ndef lossFunctionDerivative(self, X, theta, y_true):\n    '''\n    计算损失函数对参数theta的梯度\n    对theta[j]的梯度为：(y_pred - y_true)*x[j]\n    '''\n    err = self.predict_prob(X, theta) - y_true\n    return X.T.dot(err)/y_true.shape[0]\ndef lossFunction(self, y_pred_prob, y_true):\n    '''\n    未使用\n    计算损失值: Cross-Entropy\n    y_pred_prob, y_true: NumPy array, shape=(n,)\n    '''\n    tmp = y_true*np.log(y_pred_prob) + (1 - y_true)*np.log(1 - y_pred_prob)\n    return np.mean(-tmp)\n```\n## training step\n```\ndef gradDescent(self, min_acc, learning_rate=0.01, max_iter=10000):\n    acc = 0; n_iter = 0\n    for n_iter in range(max_iter):\n        for n in range(self.n_batch):\n            X_batch = self.X[n*self.batch_size:(n+1)*self.batch_size]\n            t_batch = self.t[n*self.batch_size:(n+1)*self.batch_size]\n            grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch)\n            self.theta -= learning_rate * grad # 梯度下降\n            acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t)\n            if acc > min_acc:\n                print('第%d次迭代, 第%d批数据' % (n_iter, n))\n                print(\"当前总体样本准确率为: \", acc)\n                print(\"当前参数值为: \", self.theta)\n                return self.theta\n        if n_iter%100 == 0:\n            print('第%d次迭代' % n_iter)\n            print('准确率： ', acc)\n    print(\"超过迭代次数\")\n    print(\"当前总体样本准确率为: \", acc)\n    print(\"当前参数值为: \", self.theta)\n    return self.theta\n```\n## 实验结果\n![实验结果](Logistic-Regression/logistic_regression_result.png)","categories":["Machine Learning"]},{"title":"Linear Regression","url":"/2018/10/18/Linear-Regression/","content":"\n# 引言\n线性回归可以说是机器学习最基础的算法\n# 模型\n$$\\hat{y}^{(i)} = w^Tx^{(i)}$$\n\n其中\n$$x^{(i)}=[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T, x_0^{(i)}=1$$\n\n这里$x_0^{(i)}=1$表示偏置$b$，即$b=w_0$\n$$\\hat{y}^{(i)} = w^Tx^{(i)} + b$$\n> `注`：对于非线性的数据，可构造高次特征。\n\n\n# 损失函数\n## 定义误差\n$$e^{(i)} = \\hat{y}^{(i)} - y^{(i)}$$\n其中$y^{(i)}$表示真实值\n\n## 定义损失函数\n单个样本的误差定义为\n$$L_{single}(\\hat{y}^{(i)}, y^{(i)})=\\frac{1}{2}||e^{(i)}||_2^2=\\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$$\n\n所有样本的误差定义为\n$$L(y, t)=\\frac{1}{2N}\\sum_i (\\hat{y}^{(i)}-y^{(i)})^2$$\n也可以定义为误差的和而不是均值，对结果无影响，可视作学习率$α$除去一个常数\n\n# 梯度推导\n\n> $\\frac{∂L}{∂w_j}$\n> $= \\frac{∂}{∂w_j}\\frac{1}{2N}\\sum_i(\\hat{y}^{(i)}-y^{(i)})^2$\n> $= \\frac{1}{2N} \\sum_i \\frac{∂}{∂w_j} (\\hat{y}^{(i)}-y^{(i)})^2$\n> $= \\frac{1}{N} \\sum_i (\\hat{y}^{(i)}-y^{(i)}) \\frac{∂t^{(i)}}{∂w_j}$\n> $=  \\frac{1}{N} \\sum_i (\\hat{y}^{(i)}-y^{(i)}) x_j^{(i)}$\n\n或者使用矩阵推导，记$X = [x_1, x_2, ..., x_m]^T$，$x_i$为样本特征(列向量)\n$$\nL = \\frac{1}{2}(Xw-Y)^T(Xw-Y)\n$$\n\n$$\n∇_w L = X^T(\\hat{Y}-Y)\n$$\n\n> $∇_w L$\n> $= \\frac{1}{2} ∇_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY)$\n> $= \\frac{1}{2} (2X^TXw - X^TY - X^TY)$\n> $= X^T(Xw-Y) $\n\n在梯度为$\\vec{0}$的点，即$∇_w L = \\vec{0}$时对应最优解\n$$X^T(Xw-Y) = 0$$\n> 令$$X^T(Xw-Y) = 0$$\n> \n> 有$$X^TXw = X^TY$$\n> \n> $$w^*=(X^TX+\\lambda I)^{-1}X^TY$$\n\n其中$X^+=(X^TX+\\lambda I)^{-1}X^T$，表示矩阵$X_{m×n}$的伪逆\n\n## 训练\n采用梯度下降法求解\n$$ w := w - \\alpha ∇_w L $$\n其中$w$表示参数向量\n\n> 进一步思考：为什么使用梯度下降可以求取最优解呢？\n> $$ ∇_w^2 L = ∇_w X^T(Xw-Y) = X^TX $$\n> 而对于矩阵 $ X^TX $\n> $$ u^T(X^TX)u = (Xu)^T(Xu) \\geq 0$$\n> \n> 即损失函数的`Hessian`矩阵$∇_w^2 L$为正定矩阵，$L$为凸函数，存在全局最优解\n\n\n# 从投影的角度理解线性回归\n\n![投影理解](/Linear-Regression/projection_linreg2.png)\n\n![用投影推导最优解](Linear-Regression/projection_linreg3.png)\n\n# 线性回归的正则化\n为克服过拟合问题，可加入正则化项$||w||_2^2$，此时损失函数定义为\n$$L(\\hat{y}, y)=\\frac{1}{2N} ||\\hat{y}^{(i)}-y^{(i)}||_2^2 + \\lambda ||w||_2^2$$\n\n或者\n$$L(\\hat{y}, y)=\\frac{1}{2N} \\sum_i (\\hat{y}^{(i)}-y^{(i)})^2 +  \\frac{\\lambda}{2N}\\sum_j w_j^2$$\n\n其中$i = 1, ..., N_{sample}; j = 1, ..., N_{feature},j>0 $\n\n此时梯度为\n$$\\frac{∂L}{∂w_j} = \\frac{1}{N} \\sum_i (\\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{N}w_j$$\n\n其中$j = 1, ..., N_{feature},j>0 $\n\n# 局部加权线性回归\n\n目标函数定义为\n$$L(y, t)=\\frac{1}{2N}\\sum_i w^{(i)} (\\hat{y}^{(i)}-y^{(i)})^2$$\n\n其中\n$$w^{(i)} = e^{-\\frac{(x^{(i)}-x)^2}{2\\tau^2}}$$\n\n$x$表示输入的预测样本，$x^{(i)}$表示训练样本\n<div style=\"align: center\">\n<img src=Linear-Regression/w_i_x_i.png/>\n</div>\n离很近的样本，权值接近于1，而对于离很远的样本，此时权值接近于0，这样就是在局部构成线性回归，它依赖的也只是周边的点。\n\n对于线性回归算法，一旦拟合出适合训练数据的参数$w$，保存这些参数$w$，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。而对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数$w$），没有固定的参数$w$，所以是非参数算法。\n\n\n# 代码\n[@Github: Code for Linear Regression](https://github.com/isLouisHsu/Python-Examples-for-ML/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex5-regularizedllinearregression)\n## training step\n```\ndef fit(self, X, y, learning_rate=0.01, max_iter=5000, min_loss=10):\n    # --------------- 数据预处理部分 ---------------\n    # 加入全1列\n    X = np.c_[np.ones(shape=(X.shape[0])), X]\n    # 构造高次特征\n    if self.n_ploy > 1:\n        for i in range(2, self.n_ploy + 1):\n            X = np.c_[X, X[:, 1]**i]\n    # ---------------- 参数迭代部分 ----------------\n    # 初始化参数\n    self.theta = np.random.uniform(-1, 1, size=(X.shape[1],))\n    # 数据批次\n    n_batch = X.shape[0] if self.n_batch==-1 else self.n_batch\n    batch_size = X.shape[0] // n_batch\n    # 停止条件\n    n_iter = 0; loss = float('inf')\n    # 开始迭代\n    for n_iter in range(max_iter):\n        for n in range(n_batch):\n            n1, n2 = n*batch_size, (n+1)*batch_size\n            X_batch = X[n1: n2]; y_batch = y[n1: n2]\n            \n            grad = self.lossFunctionDerivative(X_batch, y_batch)\n            self.theta -= learning_rate * grad\n            \n            loss = self.score(y_batch, self.predict(X_batch))\n            if loss < min_loss:\n                print('第%d次迭代, 第%d批数据' % (n_iter, n))\n                print(\"当前总体样本损失为: \", loss)\n                return self.theta\n        if n_iter%100 == 0:\n            print('第%d次迭代' % n_iter)\n            print(\"当前总体样本损失为: \", loss)\n    print(\"超过迭代次数\")\n    print(\"当前总体样本损失为: \", loss)\n    return self.theta\n\ndef lossFunctionDerivative(self, X, y):\n    y_pred = self.predict(X)\n    # theta = self.theta;     # ！注意：theta = self.theta 不仅仅是赋值，类似引用，修改theta会影响self.theta\n    theta = self.theta.copy()\n    theta[0] = 0            # θ0不需要正则化\n    return (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[0]\n```\n\n## predict step\n```\ndef predict(self, X, preprocessed=False):\n    if preprocessed:\n        # 加入全1列\n        X = np.c_[np.ones(shape=(X.shape[0])), X]\n        # 构造高次特征\n        if self.n_ploy > 1:\n            for i in range(2, self.n_ploy + 1):\n                X = np.c_[X, X[:, 1]**i]\n    return X.dot(self.theta)\n```\n\n## 运行结果\n- 无正则化\n    ![无正则化的线性回归结果](Linear-Regression/result_linreg_noreg.png)\n\n- 正则化\n    ![正则化的线性回归结果](Linear-Regression/result_linreg_reg.png)","categories":["Machine Learning"]},{"title":"Linux","url":"/Linux/index.html"},{"title":"Machine Learning","url":"/Machine-Learning/index.html"},{"title":"Darknet","url":"/Darknet/index.html"},{"title":"Deep Learning","url":"/Deep-Learning/index.html"},{"title":"Practice","url":"/Practice/index.html"},{"title":"Others","url":"/Others/index.html"},{"title":"Tensorflow","url":"/Tensorflow/index.html"},{"title":"Pytorch","url":"/Pytorch/index.html"},{"title":"Windows","url":"/Windows/index.html"},{"title":"Python","url":"/Python/index.html"},{"title":"about","url":"/about/index.html"},{"title":"archives","url":"/archives/index.html"},{"title":"categories","url":"/categories/index.html"},{"title":"tags","url":"/tags/index.html"},{"title":"冯唐","url":"/冯唐/index.html"}]