<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Eigenface and Fisherface</title>
      <link href="/2019/08/20/Eigenface-and-Fisherface/"/>
      <url>/2019/08/20/Eigenface-and-Fisherface/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在众多人脸图像中，能否找到特一组特征脸，用于表征其他人脸呢？在<a href="https://louishsu.xyz/2018/10/22/PCA/" target="_blank" rel="noopener">PCA</a>和<a href="https://louishsu.xyz/2019/04/22/LDA/" target="_blank" rel="noopener">LDA</a>中分别介绍了两种线性降维方法，本文介绍一种使用以上两种算法的特征提取方法。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="Eigenface"><a href="#Eigenface" class="headerlink" title="Eigenface"></a>Eigenface</h2><p><code>Eigenface</code>由Sirovich与Kirby在1987年提出，认为人脸图像可由一系列特征图加权组合重构而成，即</p><script type="math/tex; mode=display">F = F_m + \sum_i w_i F_i \tag{1}</script><p><code>Eigenface</code>可通过<code>PCA</code>生成，在大量的人脸数据图像上进行分析，选取主分量作为特征脸，详细步骤如下</p><ol><li><p>假设有图像尺寸为$(H, W)$的人脸数据，共$N$张。将单张的灰度图片展开成为维数为$H \times W$的向量$x^{(i)}$，构成数据矩阵$X$</p><script type="math/tex; mode=display">X_{(H \times W) \times N} = \begin{bmatrix} x^{(1)} & x^{(2)} & \ldots & x^{(N)} \end{bmatrix} \tag{2.1}</script></li><li><p>计算各样本向量的均值$\mu$，将各样本去均值化，生成数据矩阵$\overline{X}$</p><script type="math/tex; mode=display">\mu = \frac{1}{N} \sum_{i=1}^{N} x^{(i)}; \quad \overline{x}^{(i)} = x^{(i)} - \mu \tag{2.2}</script></li><li><p>计算协方差矩阵$C$</p><script type="math/tex; mode=display">C = \frac{1}{N} \overline{X} \cdot \overline{X}^T \tag{2.3}</script></li><li><p>将协方差矩阵进行特征值分解</p><script type="math/tex; mode=display">C \alpha_i = \lambda_i \alpha_i \tag{2.4}</script><blockquote><p>注意到，在图像尺寸为$(H, W)$的情况下，协方差矩阵的尺寸为$(H \times W, H \times W)$，本文使用数据库内图像为$112 \times 92$，存储为浮点类型<code>(4 byte)</code>，也就是说，该协方差矩阵所占内存</p><script type="math/tex; mode=display">(112 \times 92)^2 \text{pixel} \times 4 \text{byte/pixel} = 441 \rm{kb} \text{(阵亡。。。)}</script><p>因此，需要转换一下求解问题，查看<a href="#eigenface%e6%b1%82%e8%a7%a3">Eigenface求解</a>。</p></blockquote></li><li><p>按特征值降序，重新排列特征对$(\lambda_i, \alpha_i)$，选取前$K$个特征向量作为<code>Eigenface</code></p><script type="math/tex; mode=display">E_{(H \times W) \times K} = \begin{bmatrix} \alpha_1 & \alpha_2 & \ldots & \alpha_K \end{bmatrix} \tag{2.5}</script></li></ol><p>此时将人脸数据$x$投影到各主分量上，可获得相应系数，该系数向量可用于表征该人脸的特征，即</p><script type="math/tex; mode=display">\vec{w} = \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix} \tag{3}</script><p>其中$w_i = x^T \alpha_i$。</p><p>将各主轴恢复原图像尺寸后，其可视化输出如下<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_30_0.png" alt="output_30_0"></p><h2 id="FisherFace"><a href="#FisherFace" class="headerlink" title="FisherFace"></a>FisherFace</h2><p><code>Fisherface</code>基本思路与<code>Eigenface</code>一致，也是寻找一组特征脸，用于表征人脸特征。可通过<code>LDA</code>生成，<code>LDA</code>考虑类内与类间散布，与<code>PCA</code>不同，为有监督学习。详细步骤如下</p><ol><li><p>同样的，获取数据矩阵$X$</p><script type="math/tex; mode=display">X_{(H \times W) \times N} = \begin{bmatrix} x^{(1)} & x^{(2)} & \ldots & x^{(N)} \end{bmatrix} \tag{4.1}</script></li><li><p>计算所有数据的均值向量$\mu$与各类别的均值向量$\mu_j$</p><script type="math/tex; mode=display">\mu = \frac{1}{N} \sum_{i=1}^{N} x^{(i)}; \quad \mu_j = \frac{1}{N_j} \sum_{i=1}^{N_j} x^{(i)}, x^{(i)} \in C_j \tag{4.2}</script></li><li><p>计算类内离散度矩阵$S_W$与类间离散度矩阵$S_B$</p><script type="math/tex; mode=display"> \begin{aligned}  S_W = \sum_{j=1}^{C} \frac{N_j}{N} \left[ \frac{1}{N_j} \sum_{i=1}^{N_j} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T \right] \\ = \frac{1}{N} \sum_{j=1}^{C} \sum_{i=1}^{N_j} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T; \quad x^{(i)} \in C_j \end{aligned} \tag{4.3}</script><script type="math/tex; mode=display">S_B = \sum_{j=1}^{C} \frac{N_j}{N} (\mu_j - \mu) (\mu_j - \mu)^T \tag{4.4}</script></li><li><p>求解广义特征值问题</p><script type="math/tex; mode=display">S_B \alpha_i = \lambda_i S_W \alpha_i \tag{4.5}</script><blockquote><p>通常该问题可通过分解矩阵$S_W^{-1} S_B$进行求解</p><script type="math/tex; mode=display">S_W^{-1} S_B \alpha_i = \lambda_i \alpha_i</script><p>但由于函数<code>numpy.linalg.eig(a)</code>问题，在求解$S_W^{-1} S_B$特征对时出现复数。更加令人疑惑的是，作为实对称矩阵$S_W^{-1}$，其特征对用该函数求解时，也会出现复数。实际上，<code>numpy</code>提供了函数<code>numpy.linalg.eigh(a)</code>专门求解实对称矩阵或<code>Hermite</code>矩阵的特征对，故需要对该特征求解进行一定处理，查看<a href="#%e5%b9%bf%e4%b9%89%e7%89%b9%e5%be%81%e5%80%bc%e9%97%ae%e9%a2%98sb-alphai--lambdai-sw-alphai">广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$</a>。<br><strong>性质1：</strong> 实对称矩阵(满足$A^T = A$)的特征值都是实数。<br><strong>性质2：</strong> 实对称矩阵(满足$A^T = A$)属于不同特征值的特征向量正交。</p></blockquote></li><li><p>按特征值降序，重新排列特征对$(\lambda_i, \alpha_i)$，选取前$K$个特征向量作为<code>FisherFace</code></p><script type="math/tex; mode=display">F_{(H \times W) \times K} = \begin{bmatrix} \alpha_1 & \alpha_2 & \ldots & \alpha_K \end{bmatrix} \tag{4.6}</script></li></ol><p>类似的，将人脸数据$x$投影到各主分量上，可获得相应系数，该系数向量可用于表征该人脸的特征，即</p><script type="math/tex; mode=display">\vec{w} = \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix} \tag{5}</script><p>其中$w_i = x^T \alpha_i$。</p><p>将各主轴恢复原图像尺寸后，其可视化输出如下<br><img src="/2019/08/20/Eigenface-and-Fisherface/markdown/output_33_0.png" alt="output_33_0"></p><p>实际上，若输入的数据矩阵$X$不做降维处理，计算得到矩阵$S_W$与$S_B$尺寸为$(H \times W) \times (H \times W)$，也是一个令人头疼的计算量问题。而与<a href="#eigenface">上面</a>不同，这是无法避免的。所以考虑到这一点，需要对原始数据进行降维，可利用<code>PCA</code></p><script type="math/tex; mode=display">\tilde{X}_{D \times N} = \text{pca}(X_{(H \times W) \times N}) \tag{6.1}</script><p>对矩阵$\tilde{X}$进行<code>LDA</code>计算后，得到<code>Fisherface</code>序列$\{\alpha_1, \alpha_2, \ldots\}$，其中$\alpha_i$维度为$D \times 1$，若需可视化结果，利用计算得到的<code>PCA</code>模型将其重建即可</p><script type="math/tex; mode=display">A_i = \text{pca}^{-1}(\alpha_i) \tag{6.2}</script><h1 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h1><h2 id="Eigenface求解"><a href="#Eigenface求解" class="headerlink" title="Eigenface求解"></a>Eigenface求解</h2><p>矩阵$\overline{X}$可经$SVD$分解为</p><script type="math/tex; mode=display">\overline{X} = U \Sigma V^T \tag{7}</script><p>对于协方差矩阵$C_{(H \times W) \times (H \times W)} = \frac{1}{N} \overline{X} \cdot \overline{X}^T$，对其进行特征值分解如下</p><script type="math/tex; mode=display">\overline{X} \cdot \overline{X}^T u_i = \lambda_i u_i \tag{8}</script><p>实际上，$(8)$为求解矩阵$\overline{X}$左奇异向量$u_i, i = 1, 2, \ldots, H \times W$的过程，然而其计算量过大，考虑矩阵$\overline{X}$右奇异向量$v_i, i = 1, 2, \ldots, N$。</p><script type="math/tex; mode=display">\overline{X}^T \overline{X} v_i = \lambda_i v_i \tag{9}</script><p>由$(7)$可得</p><script type="math/tex; mode=display">\begin{aligned}    \overline{X} V = U \Sigma & 或 & \overline{X} v_i = \sigma_i u_i \end{aligned}</script><p>所以</p><script type="math/tex; mode=display">\begin{aligned}    u_i = \frac{1}{\sigma_i} \overline{X} v_i & 或 & U = \overline{X} V \Sigma^{-1}\end{aligned} \tag{10}</script><p>其中$V$与$\Lambda$由式$(9)$已知，奇异值$\sigma_i = \sqrt{\lambda_i}$，故</p><script type="math/tex; mode=display">\begin{aligned}    u_i = \frac{1}{\sqrt{\lambda_i}} \overline{X} v_i & 或 & U = \overline{X} V \Lambda^{-\frac{1}{2}}\end{aligned} \tag{11}</script><p>实际上，由于$\text{rank}(\overline{X}) \leq N$，故后$(H \times W) - N$个特征值均为$0$，对应特征向量无意义，不予求解。</p><h2 id="广义特征值问题-S-B-alpha-i-lambda-i-S-W-alpha-i"><a href="#广义特征值问题-S-B-alpha-i-lambda-i-S-W-alpha-i" class="headerlink" title="广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$"></a>广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$</h2><p>由于函数<code>numpy.linalg.eig(a)</code>问题，在求解$S_W^{-1} S_B$特征对时出现复数。故作如下处理</p><p>$S_W$为实对称矩阵，故可使用函数<code>numpy.linalg.eigh(a)</code>解得其特征对，即</p><script type="math/tex; mode=display">S_W = P \Lambda P^T \tag{12}</script><p>代入$S_W^{-1} S_B \alpha_i = \lambda_i \alpha_i$并作相应变换</p><script type="math/tex; mode=display">(P \Lambda P^T)^{-1} S_B \alpha_i = \lambda_i \alpha_i</script><script type="math/tex; mode=display">\underbrace{P \Lambda^{-\frac{1}{2}} \Lambda^{-\frac{1}{2}} P^T}_{S_W} \cdot S_B \cdot \underbrace{P \Lambda^{-\frac{1}{2}} \Lambda^{\frac{1}{2}} P^T}_I \cdot \alpha_i = \lambda_i \alpha_i</script><script type="math/tex; mode=display">\underbrace{\Lambda^{-\frac{1}{2}} P \cdot S_B \cdot P \Lambda^{-\frac{1}{2}}}_{A} \underbrace{\Lambda^{\frac{1}{2}} P^T \cdot \alpha_i}_{\beta_i} = \lambda_i \underbrace{\Lambda^{\frac{1}{2}} P^T \cdot \alpha_i}_{\beta_i} \tag{13}</script><p>其中$A = \Lambda^{-\frac{1}{2}} P \cdot S_B \cdot P \Lambda^{-\frac{1}{2}}$也为对称矩阵，可由用函数<code>numpy.linalg.eigh(a)</code>解得其特征对$(\lambda_i, \beta_i)$，则</p><script type="math/tex; mode=display">\beta_i = \Lambda^{\frac{1}{2}} P^T \cdot \alpha_i</script><script type="math/tex; mode=display">\alpha_i = P \Lambda^{-\frac{1}{2}} \beta_i \tag{14}</script><h1 id="实现及实验"><a href="#实现及实验" class="headerlink" title="实现及实验"></a>实现及实验</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>实验中数据集选用<a href="https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" target="_blank" rel="noopener">ORL Face</a>，包含40名人员的10份人脸数据，包括拍摄时间、光照、表情(睁/闭眼、笑/不笑)、面部细节(眼镜)的变化。图片保存为<code>.pgm</code>格式，可用<code>OpenCV</code>进行读取。</p><p>其数据预览如下</p><p><img src="/2019/08/20/Eigenface-and-Fisherface/faces.gif" alt="preview"></p><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Principal Components Analysis</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        components_: &#123;ndarray(n_components, n_features)&#125;</span></span><br><span class="line"><span class="string">        means_:      &#123;ndarray(n_components)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.components_  = <span class="keyword">None</span></span><br><span class="line">        self.means_       = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">''' train the model</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">        self.means_ = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_ = X - self.means_</span><br><span class="line">        eigval, eigvec = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> n_samples &lt; n_features:</span><br><span class="line">            eigval, u = np.linalg.eig(X_.dot(X_.T))</span><br><span class="line">            eigvec = X_.T.dot(u).dot(np.diag(<span class="number">1</span> / eigval))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            covar_ = X_.T.dot(X_)</span><br><span class="line">            eigval, eigvec = np.linalg.eig(covar_)</span><br><span class="line"></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>]</span><br><span class="line">        eigval = eigval[order]</span><br><span class="line">        eigvec = eigvec.T[order].T</span><br><span class="line"></span><br><span class="line">        self.components_ = eigvec[:, :self.n_components].T</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X_:&#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            X'_&#123;nxk'&#125; · V_&#123;kxk'&#125;^T = X''_&#123;nxk&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X - self.means_</span><br><span class="line">        X_ = X_.dot(self.components_.T)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        self.fit(X)</span><br><span class="line">        <span class="keyword">return</span> self.transform(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X_:&#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_) + self.means_</span><br><span class="line">        <span class="keyword">return</span> X_</span><br></pre></td></tr></table></figure><h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LDA</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        components_:  &#123;ndarray(n_components, n_features)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.components_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">""" train the model</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:      &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">            y:      &#123;ndarray(n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        labels = list(set(list(y)))</span><br><span class="line">        n_class = len(labels)</span><br><span class="line">        n_samples, n_feats = X.shape</span><br><span class="line"></span><br><span class="line">        S_W = np.zeros(shape=(n_feats, n_feats))</span><br><span class="line">        S_B = np.zeros(shape=(n_feats, n_feats))</span><br><span class="line">        mean_ = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> i_class <span class="keyword">in</span> range(n_class):</span><br><span class="line">            X_ = X[y==labels[i_class]]</span><br><span class="line">            means_ = np.mean(X_, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            X_ = X_ - means_</span><br><span class="line">            means_ = (means_ - mean_).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            S_W += (X_.T).dot(X_) * (<span class="number">1</span> / n_samples)</span><br><span class="line">            S_B += (means_.T).dot(means_) * (X_.shape[<span class="number">0</span>] / n_samples)</span><br><span class="line"></span><br><span class="line">        s, u = np.linalg.eigh(S_W)</span><br><span class="line">        s_sqrt = np.diag(np.sqrt(s))</span><br><span class="line">        s_sqrt_inv = np.linalg.inv(s_sqrt)</span><br><span class="line"></span><br><span class="line">        A = s_sqrt_inv.dot(u.T).dot(S_B).dot(u).dot(s_sqrt_inv)</span><br><span class="line">        eigval, P = np.linalg.eigh(A)</span><br><span class="line">        eigvec = u.dot(s_sqrt_inv).dot(P)</span><br><span class="line"></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>]</span><br><span class="line">        eigval = eigval[order]</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line"></span><br><span class="line">        self.components_ = eigvec[:, :self.n_components].T</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_.T)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.fit(X, y)</span><br><span class="line">        X_ = self.transform(X)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_features</span><span class="params">(X, dsize, title=<span class="string">"features"</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Show features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(N, n_features)&#125;</span></span><br><span class="line"><span class="string">        dsize: &#123;tuple(H, W)&#125;</span></span><br><span class="line"><span class="string">        title: &#123;str&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    SUBPLOT = <span class="string">"19&#123;&#125;"</span></span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">2</span>))</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        plt.subplot(int(SUBPLOT.format(i+<span class="number">1</span>)))</span><br><span class="line">        plt.imshow(X[i].reshape(dsize), cmap=<span class="string">"gray"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ol><li><p>Eigenface</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=n_features); pca.fit(X)</span><br><span class="line">show_features(pca.components_, DSIZE, title=<span class="string">"Eigenface &#123;&#125;"</span>.format(repr(DSIZE)))</span><br></pre></td></tr></table></figure><p> <img src="/2019/08/20/Eigenface-and-Fisherface/output_30_0.png" alt="output_30_0"></p></li><li><p>FisherFace</p><p> 为减少计算量，将原始数据<code>PCA</code>降维后，再用于<code>LDA</code>计算<code>FisherFace</code>，故维度数目选取会影响实验结果，以下分别选择不同维数时产生的<code>FisherFace</code>序列，显示前9个特征脸。可见主分量数目越少，特征脸越清晰，但包含的细节也越少。</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n_decomposed = n_samples - n_classes - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=n_decomposed)</span><br><span class="line">X_decomposed = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line">lda = LDA(n_components=n_classes - <span class="number">1</span>)</span><br><span class="line">lda.fit(X_decomposed, y)</span><br><span class="line"></span><br><span class="line">components_ = pca.transform_inv(lda.components_)</span><br><span class="line">show_features(components_, DSIZE, title=<span class="string">"Fisherface &#123;&#125; &#123;&#125;"</span>.format(repr(DSIZE), n_decomposed))</span><br></pre></td></tr></table></figure><ul><li>199<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_33_0.png" alt="output_33_0"></li><li>279<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_33_2.png" alt="output_33_2"></li><li>359<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_33_4.png" alt="output_33_4"></li></ul></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" target="_blank" rel="noopener">The Database of Faces</a></li><li><a href="https://www.xuebuyuan.com/3231919.html" target="_blank" rel="noopener">人脸识别之—-FisherFace - 学步园</a></li><li><a href="https://bytefish.de/pdf/facerec_python.pdf" target="_blank" rel="noopener">Face Recognition with Python, by Philipp Wagner</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征提取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neighborhood Preserving Embedding</title>
      <link href="/2019/08/12/Neighborhood-Preserving-Embedding/"/>
      <url>/2019/08/12/Neighborhood-Preserving-Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在<a href="https://louishsu.xyz/2019/08/09/Locally-Linear-Embedding/" target="_blank" rel="noopener">Locally Linear Embedding</a>一节中介绍了非线性降维方法<code>LLE</code>，原数据到低维数据没有指定映射方法，故不适用于新数据点。本文介绍的<code>NPE</code>是在其基础上的改进。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>设有$M$个$N$维数据，构成矩阵$X$</p><script type="math/tex; mode=display">X_{N \times M} = \left[ \begin{matrix}    |             & |             &        & |              \\    \vec{x}^{(1)} & \vec{x}^{(2)} & \cdots & \vec{x}^{(M)}  \\    |             & |             &        & |\end{matrix} \right] \tag{1.1}</script><p>其中</p><script type="math/tex; mode=display">\vec{x}^{(i)}_{N \times 1} = \left[ \begin{matrix}    \vec{x}^{(i)}_1 & \vec{x}^{(i)}_2 & \cdots & \vec{x}^{(i)}_N \end{matrix} \right] ^T \tag{1.2}</script><h2 id="高维到低维的映射"><a href="#高维到低维的映射" class="headerlink" title="高维到低维的映射"></a>高维到低维的映射</h2><p>在<code>LLE</code>基础上，将数据的映射方法指定为</p><script type="math/tex; mode=display">\vec{y}^{(i)} = P^T · \vec{x}^{(i)} \tag{*1}</script><blockquote><script type="math/tex; mode=display">\vec{y}^{(i)}_j = \vec{p}_j^T · \vec{x}^{(i)}</script></blockquote><p>其中</p><script type="math/tex; mode=display">P_{N \times D} = \left[ \begin{matrix}    |         & |         &        & |          \\    \vec{p}_1 & \vec{p}_2 & \cdots & \vec{p}_D  \\    |         & |         &        & |\end{matrix} \right] \tag{2.1}</script><script type="math/tex; mode=display">\vec{p}_{i_{N \times 1}} = \left[ \begin{matrix}    \vec{p}_{i1} & \vec{p}_{i2} & \cdots & \vec{p}_{iN}\end{matrix} \right] ^T \tag{2.2}</script><script type="math/tex; mode=display">\vec{y}^{(i)}_{D \times 1} = \left[ \begin{matrix}    \vec{y}^{(i)}_1 & \vec{y}^{(i)}_2 & \cdots & \vec{y}^{(i)}_D \end{matrix} \right] ^T \tag{2.3}</script><h2 id="高维空间的空间结构特征"><a href="#高维空间的空间结构特征" class="headerlink" title="高维空间的空间结构特征"></a>高维空间的空间结构特征</h2><p>与<a href="https://louishsu.xyz/2019/08/09/Locally-Linear-Embedding/" target="_blank" rel="noopener">Locally Linear Embedding</a>一致，通过矩阵$\dot{W}$保存空间结构特征</p><script type="math/tex; mode=display">J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2 \tag{3.1}</script><script type="math/tex; mode=display">\text{s.t.} \quad \sum_{j=1}^K w_{ij} = 1 \quad \text{or} \quad \vec{w}_i^T \vec{1} = 1 \tag{3.2}</script><p>解得</p><script type="math/tex; mode=display">\vec{w}_{i_{K \times 1}} = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}</script><p>其中</p><script type="math/tex; mode=display">Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)})</script><script type="math/tex; mode=display">X^{(i)} = \left[ \begin{matrix}    |             & |             &        & |              \\    \vec{x}^{(i)} & \vec{x}^{(i)} & \cdots & \vec{x}^{(i)}  \\    |             & |             &        & |\end{matrix} \right]</script><script type="math/tex; mode=display">N^{(i)} = \left[ \begin{matrix}    |               & |               &        & |               \\    \vec{x}^{(1)}_N & \vec{x}^{(2)}_N & \cdots & \vec{x}^{(K)}_N \\    |               & |               &        & |\end{matrix} \right]</script><p>解得矩阵</p><script type="math/tex; mode=display">W_{K \times M} = \left[ \begin{matrix}    |         & |         &        & |          \\    \vec{w}_1 & \vec{w}_2 & \cdots & \vec{w}_M  \\    |         & |         &        & |\end{matrix} \right]</script><h2 id="低维空间保持同样的空间结构"><a href="#低维空间保持同样的空间结构" class="headerlink" title="低维空间保持同样的空间结构"></a>低维空间保持同样的空间结构</h2><p>在低维空间中，损失定义为</p><script type="math/tex; mode=display">J(Y) = \sum_{i} || \vec{y}^{(i)} - \sum_j w_{ij} \vec{y}^{(j)} ||_2^2 \tag{4}</script><p>由于低维空间中近邻情况未知，故将矩阵$W$扩充为$\dot{W}$</p><script type="math/tex; mode=display">\dot{W}_{M \times M} = \left[ \begin{matrix}    |               & |               &        & |               \\    \dot{\vec{w}}_1 & \dot{\vec{w}}_2 & \cdots & \dot{\vec{w}}_M \\    |               & |               &        & |\end{matrix} \right] \tag{5.1}</script><script type="math/tex; mode=display">\dot{w}_{ij} = \begin{cases}    w_{ik}  & x^{(j)} = N^{(i)}_k \\    0       & \text{otherwise}\end{cases} \tag{5.2}</script><p>相应的，$\vec{y}^{(i)}_{D \times 1}$扩充为$\dot{\vec{y}}^{(i)}_{M \times 1}$。</p><p>则式$(4)$可变换为</p><script type="math/tex; mode=display">J(\dot{Y}) = \sum_{i} || \dot{\vec{y}}^{(i)} - \sum_j \dot{w}_{ij} \dot{\vec{y}}^{(j)} ||_2^2 \tag{6.1}</script><p>增加约束条件，与<code>LLE</code>略有不同</p><script type="math/tex; mode=display">\dot{\vec{y}}^{(i)^T} \dot{\vec{y}}^{(i)} = 1 \tag{6.2}</script><p>写作矩阵形式，即</p><script type="math/tex; mode=display">J(\dot{Y}) = \text{tr} \left[ \dot{Y} (I - \dot{W}) (I - \dot{W})^T \dot{Y}^T \right] \tag{7}</script><script type="math/tex; mode=display">\text{s.t.} \dot{\vec{y}}^{(i)^T} \dot{\vec{y}}^{(i)} = 1</script><p>其中</p><script type="math/tex; mode=display">\dot{Y}_{M \times M} = \dot{P}^T_{(N \times M)^T} X_{N \times M} \tag{8.1}</script><script type="math/tex; mode=display">\dot{\vec{y}^{(i)}}_{M \times 1} = \dot{P}^T_{(N \times M)^T} \dot{\vec{x}^{(i)}}_{N \times 1} \tag{8.2}</script><p>则优化问题转换为</p><script type="math/tex; mode=display">J(\dot{P}) = \text{tr} \left[\dot{P}^T X (I - \dot{W}) (I - \dot{W})^T X^T \dot{P} \right] \tag{9}</script><script type="math/tex; mode=display">\text{s.t.} \quad \dot{\vec{x}^{(i)}}^T \dot{P} \dot{P}^T \dot{\vec{x}^{(i)}} = 1</script><p>记</p><script type="math/tex; mode=display">M = (I - \dot{W}) (I - \dot{W})^T \tag{10}</script><p>列写拉格朗日函数</p><script type="math/tex; mode=display">L(\dot{P}) = \text{tr} \left[\dot{P}^T X (I - \dot{W}) (I - \dot{W})^T X^T \dot{P} \right] + \lambda (\dot{\vec{x}^{(i)}}^T \dot{P} \dot{P}^T \dot{\vec{x}^{(i)}} - 1) \tag{11}</script><script type="math/tex; mode=display">\frac{\nabla L(\dot{P})}{\nabla \dot{P}} = 2 X M X^T \dot{P} + 2 \lambda X X^T \dot{P}</script><script type="math/tex; mode=display">\Rightarrow \quad X M X^T \dot{P} = \lambda X X^T \dot{P} \tag{*3}</script><blockquote><script type="math/tex; mode=display">\frac{\nabla b^T X^T X c}{\nabla X} = X(bc^T + cb^T)</script></blockquote><p>同<code>LLE</code>，当低维数据维度为$D$时，按特征值升序排序约化矩阵，即选择最前的$D$个特征向量组成投影矩阵</p><script type="math/tex; mode=display">P = \left[ \begin{matrix}    |               & |               &        & |               \\    \vec{\alpha}_1  & \vec{\alpha}_2  & \cdots & \vec{\alpha}_D  \\    |               & |               &        & |\end{matrix} \right] \tag{*4}</script><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeighborhoodPreservingEmbedding</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Neighborhood Preserving Embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_neighbors:  &#123;int&#125;</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        W_: &#123;ndarray&#125; </span></span><br><span class="line"><span class="string">        components_:    &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_neighbors, n_components=<span class="number">2</span>, k_skip=<span class="number">1</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.n_neighbors  = n_neighbors</span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.k_skip = k_skip</span><br><span class="line"></span><br><span class="line">        self.W_ = <span class="keyword">None</span></span><br><span class="line">        self.components_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KDTree</span><br><span class="line">        kdtree = KDTree(X, metric=<span class="string">'euclidean'</span>)</span><br><span class="line">        </span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        self.W_ = np.zeros((n_samples, n_samples))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line"></span><br><span class="line">            <span class="comment">## 获取近邻样本点</span></span><br><span class="line">            x = X[i]</span><br><span class="line">            idx = kdtree.query(x.reshape(<span class="number">1</span>, <span class="number">-1</span>), self.n_neighbors + <span class="number">1</span>, return_distance=<span class="keyword">False</span>)[<span class="number">0</span>][<span class="number">1</span>: ]</span><br><span class="line">            <span class="comment">## 求取矩阵 Z = (x - N).dot((x - N).T)</span></span><br><span class="line">            N = X[idx]</span><br><span class="line">            Z = (x - N).dot((x - N).T)</span><br><span class="line">            <span class="comment">## 求取权重 w_i</span></span><br><span class="line">            Z_inv = np.linalg.inv(Z + np.finfo(float).eps * np.eye(self.n_neighbors))</span><br><span class="line">            w = np.sum(Z_inv, axis=<span class="number">1</span>) / np.sum(Z_inv)</span><br><span class="line">            <span class="comment">## 保存至 W</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_neighbors):</span><br><span class="line">                self.W_[idx[j], i] = w[j]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 求取矩阵 M = (I - W)(I - W)^T</span></span><br><span class="line">        I = np.eye(n_samples)</span><br><span class="line">        M = (I - self.W_).dot((I - self.W_).T)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 求解 X M X^T \alpha = \lambda X X^T \alpha</span></span><br><span class="line">        A1 = X.T.dot(M).dot(X)</span><br><span class="line">        A2 = X.T.dot(X)</span><br><span class="line">        eps = np.finfo(float).eps * np.eye(A2.shape[<span class="number">0</span>])</span><br><span class="line">        A = np.linalg.inv(A2 + eps).dot(A1)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 对 A 进行特征分解，并按特征值升序排序</span></span><br><span class="line">        eigval, eigvec = np.linalg.eig(A)</span><br><span class="line">        eigvec = eigvec[:, np.argsort(eigval)]</span><br><span class="line">        eigval = eigval[np.argsort(eigval)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 选取 D 维</span></span><br><span class="line">        self.components_ = eigvec[:, self.k_skip: self.n_components + self.k_skip]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Y = X.dot(self.components_)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.fit(X)</span><br><span class="line">        Y = self.transform(X)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits(n_class=<span class="number">6</span>)</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">images = digits.images</span><br><span class="line"></span><br><span class="line">npe = NeighborhoodPreservingEmbedding(<span class="number">30</span>, <span class="number">2</span>, k_skip=<span class="number">3</span>)</span><br><span class="line">X_npe = npe.fit_transform(X)</span><br><span class="line"></span><br><span class="line">plot_embedding(X_npe, y, images, title=<span class="keyword">None</span>, t=<span class="number">2e-3</span>, figsize=(<span class="number">12</span>, <span class="number">9</span>))</span><br></pre></td></tr></table></figure><p><img src="/2019/08/12/Neighborhood-Preserving-Embedding/Figure_1.png" alt="Figure_1"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
            <tag> manifold </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>挪威的森林</title>
      <link href="/2019/08/10/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97/"/>
      <url>/2019/08/10/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/08/10/挪威的森林/image.jpg" alt="挪威的森林"></p><blockquote><p>没有人喜欢孤独，只是不愿失望。</p></blockquote><ol><li>时至今日，我才恍然领悟到直子之所以求我别忘掉她的原因。直子当然知道，知道她在我心目中的记忆迟早要被冲淡。也惟其如此，她才强调说：希望你能记住我，记住我曾这样存在过。 想到这里，我就悲哀得难以自禁。因为，直子连爱都没爱过我的。</li><li>死并非生的对立面，而作为生的一部分永存。</li><li>或许我的心包有一层硬壳，能破壳而入的东西是极其有限的。所以我才不能对人一往情深。</li><li>他也背负着他的十字架匍匐在人生征途中。</li><li>那正是同被直子盯视眼睛时所感到的同一性质的悲哀。这种莫可名状的心绪，我既不能将其排遣于外，又不能将其深藏于内。它像掠身而去的阵风一样没有轮廓，没有重量。</li><li>这种莫可名状的心绪，我既不能将其排遣于外，又不能将其深藏于内。它像掠身而去的阵风一样没有轮廓，没有重量。我甚至连把它裹在身上都不可能。</li><li>“哪里会有人喜欢孤独！不过是不乱交朋友罢了。那样只能落得失望。”我说。</li><li>“绅士就是：所做的，不是自己想做之事，而是自己应做之事。”</li><li>说不定那时我们是为相遇而相遇的。纵令那时未能相遇，也会在别的地方相遇—倒没什么根据，但我总是有这种感觉。</li><li>也许等得过久了。我追求的是十二分完美无缺的东西，所以才这么难。</li><li>孤零零一个人，觉得身体就像一点点腐烂似的。渐渐腐烂、融化，最后变成一洼黏糊糊的绿色液体，再被吸进地底下去，剩下来的只是衣服—就是这种感觉，在干等一天的时间里。</li><li>每个人无不显得很幸福。至于他们是真的幸福还是仅仅表面看上去如此，就无从得知了。</li><li>什么是美好的以及如何获得幸福之类。对我毋宁说是个十分烦琐而错综复杂的命题，从而使我转求其他的标准，诸如公正、正直、普遍性等。”</li><li>倘若我在你心中留下什么创伤，那不仅仅是你一个人的，也是我的创伤。</li><li>所以如此，是因为什么，而它又意味什么，为什么等等。至于这种分析是将世界简单化还是条理化，我却是不明不白。</li><li>普通人啊。生在普通家庭，长在普通家庭，一张普通的脸，普通的成绩，想普通的事情。</li><li>人若要在某件事上扯谎，就势必为此编造出一大堆相关的谎言。</li><li>世界上，有人喜欢查时刻表一查就整整一天；也有的人把火柴棍拼在一起，准备造一艘一米长的船。所以说，这世上有一两个要理解你的人也没什么不自然的吧？</li><li>世上是有这种人的：尽管有卓越的天赋才华，却承受不住使之系统化的训练，而终归将才华支离破碎地挥霍掉。</li><li>现实世界里，很多方面人们都在互相强加，以邻为壑，否则就活不下去。</li><li>“那不是努力，只是劳动。”永泽断然说道，“我所说的努力与这截然不同。所谓努力，指的是主动而有目的的活动。”</li><li>就在这种气势夺人的暮色当中，我猛然想起了初美，并且这时才领悟她给我带来的心灵震颤究竟是什么东西—它类似一种少年时代的憧憬，一种从来不曾实现而且永远不可能实现的憧憬。这种直欲燃烧般的天真烂漫的憧憬，我在很早以前就已遗忘在什么地方了，甚至在很长时间里我连它曾在我心中存在过都未曾记起。而初美所摇撼的恰恰就是我身上长眠未醒的“我自身的一部分”。</li><li>“可爱极了！”<br>“绿子，”她说，“要加上名字。”<br>“可爱极了，绿子。”我补充道。<br>“极了是怎么个程度？”<br>“山崩海枯那样可爱。”<br>绿子扬着脸看着我：“你用词倒还不同凡响。”<br>“给你这么一说，我心里也暖融融的。”我笑道。<br>“来句更棒的。”<br>“最最喜欢你，绿子。”<br>“什么程度？”<br>“像喜欢春天的熊一样。”<br>“春天的熊？”绿子再次扬起脸，“什么春天的熊？”<br>“春天的原野里，你正一个人走着，对面走来一只可爱的小熊，浑身的毛活像天鹅绒，眼睛圆鼓鼓的。它这么对你说道：‘你好，小姐，和我一块打滚玩好么？’接着你就和小熊抱在一起，顺着长满三叶草的山坡咕噜咕噜滚下去，整整玩了一大天。你说棒不棒？”<br>“太棒了。”<br>“我就这么喜欢你。”</li><li>“喜欢我喜欢到什么程度？”绿子问。<br>“整个世界森林里的老虎全都融化成黄油。”</li><li>我则几乎没有抬头，日复一日地打发时光。在我眼里，只有漫无边际的泥沼。往前落下右脚，拔起左脚，再拔起右脚。我判断不出我位于何处，也不具有自己是在朝正确方向前进的信心。我之所以一步步挪动步履，只是因为我必须挪动，而无论去哪里。</li><li>在我眼里，春夜里的樱花，宛如从开裂的皮肤中鼓胀出来的烂肉，整个院子都充满烂肉那甜腻而沉闷的腐臭气味。</li><li>同情自己是卑劣懦夫干的勾当。</li><li>“饼干罐不是装有各种各样的饼干，喜欢的和不大喜欢的不都在里面吗？如果先一个劲儿地挑你喜欢的吃，那么剩下的就全是不大喜欢的。每次遇到麻烦我就总这样想：先把这个应付过去，往下就好过了。人生就是饼干罐。”</li><li>纵令听其自然，世事的长河也还是要流往其应流的方向，而即使再竭尽人力，该受伤害的人也无由幸免。所谓人生便是如此。</li><li>死并非生的对立面，死潜伏在我们的生之中。</li><li>“信终归不过是信。”我说，“即使烧了，该留在心里的自然留下；就算保存在那里，留不下来的照样留不下。”</li><li>我给绿子打电话，告诉她：自己无论如何都想和她说话，有满肚子话要说，有满肚子非说不可得话。整个世界上除了她别无他求。相见她想同她说话，两人一切从头开始。<br>绿子在电话的另一头久久默然不语，如同全世界的细雨落在全世界所有的草坪上一般的沉默在持续。这时间里，我一直合着双眼，把额头顶在电话亭玻璃上。良久，绿子用沉静的声音开口道：“你现在在哪里？”<br>我现在在哪里？<br>我拿着听筒扬起脸，飞快读环视电话亭四周。我现在在哪里？我不知道这里是哪里，全然摸不着头脑。这里究竟是哪里？目力所及，无不是不知走去哪里的无数男男女女。我在哪里也不是的场所的中央，不断地呼唤着绿子。 </li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 村上春树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Locally Linear Embedding</title>
      <link href="/2019/08/09/Locally-Linear-Embedding/"/>
      <url>/2019/08/09/Locally-Linear-Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>高维数据可视化需要通过降维的方式，而PCA、LDA等线性降维算法，忽视了数据点间的空间结构特征，本文介绍的局部线性嵌入<code>(LLE)</code>算法为非线性降维方法，基于谱的降维方法，所以很靠谱 :-)。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>设有$M$个$N$维数据点，构成矩阵$X$，如下</p><script type="math/tex; mode=display">X = \left[ \begin{matrix} \vec{x}^{(1)} & \vec{x}^{(2)} & \cdots & \vec{x}^{(M)} \end{matrix} \right] \tag{1}</script><p>其中$\vec{x}^{(i)}$表示样本点向量</p><script type="math/tex; mode=display">\vec{x}^{(i)} = \left[ \begin{matrix} x^{(i)}_1 & x^{(i)}_2 & \cdots & x^{(i)}_3 \end{matrix} \right]^T \tag{2}</script><p><strong><code>LLE</code>的基本思路：</strong> 任一数据点$\vec{x}^{(i)}$可由其的$K$个近邻点$\mathcal{N}_K(\vec{x}^{(i)})$线性表出，可作为数据样本的结构特征，即</p><script type="math/tex; mode=display">\hat{\vec{x}}^{(i)} = \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} \tag{3.1}</script><p>其中</p><script type="math/tex; mode=display">\vec{w}_i = \left[ \begin{matrix} w_{i1} & w_{i2} & \cdots & w_{ik} \end{matrix} \right]^T</script><p>若$\vec{x}^{(i)}$映射到低维空间中对应特征点为$\vec{y}^{(i)}$，对相同参数$w_{ij}$，$\vec{x}^{(i)}$也可由其K个近邻点$\mathcal{N}_K(\vec{y}^{(i)})$表出，即</p><script type="math/tex; mode=display">\hat{\vec{y}}^{(i)} = \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} w_{ij} \vec{y}^{(j)} \tag{3.2}</script><h2 id="数据结构参数"><a href="#数据结构参数" class="headerlink" title="数据结构参数"></a>数据结构参数</h2><p>求解参数向量$\vec{w}_i$，可利用最小二乘法，即目标函数定义为</p><script type="math/tex; mode=display">J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2 \tag{4.1}</script><p>对参数向量$\vec{w}_i$归一化，即加入约束项</p><script type="math/tex; mode=display">\sum_{j=1}^K w_{ij} = 1 \quad \text{or} \quad \vec{w}_i^T \vec{1} = 1 \tag{4.2}</script><p>式$(4.1)$可作如下变换</p><script type="math/tex; mode=display">J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2</script><script type="math/tex; mode=display">= || \sum_{j=1}^K w_{ij} \vec{x}^{(i)} - \sum_{j=1}^K w_{ij} \vec{x}^{(j)} ||_2^2</script><script type="math/tex; mode=display">= || \sum_{j=1}^K w_{ij} (\vec{x}^{(i)} - \vec{x}^{(j)}) ||_2^2</script><script type="math/tex; mode=display">= || (X^{(i)} - N^{(i)}) \vec{w}_i ||_2^2 \tag{5.1}</script><p>其中</p><script type="math/tex; mode=display">X^{(i)} = \left[ \begin{matrix} \vec{x}^{(i)} & \vec{x}^{(i)} & \cdots & \vec{x}^{(i)} \end{matrix} \right] \tag{5.1.1}</script><script type="math/tex; mode=display">N^{(i)} = \left[ \begin{matrix} \vec{x}_N^{(1)} & \vec{x}_N^{(2)} & \cdots & \vec{x}_N^{(K)} \end{matrix} \right] \tag{5.1.2}</script><p>则</p><script type="math/tex; mode=display">J(\vec{w}_i) = || (X^{(i)} - N^{(i)}) \vec{w}_i ||_2^2 \tag{5.1}</script><script type="math/tex; mode=display">= \left[ (X^{(i)} - N^{(i)}) \vec{w}_i \right]^T \left[ (X^{(i)} - N^{(i)}) \vec{w}_i \right]</script><script type="math/tex; mode=display">= \vec{w}_i^T (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \vec{w}_i \tag{5.2}</script><p>记矩阵</p><script type="math/tex; mode=display">Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \tag{5.3}</script><p>最终优化目标为</p><script type="math/tex; mode=display">J(\vec{w}_i) = \vec{w}_i^T Z^{(i)} \vec{w}_i \tag{*1}</script><script type="math/tex; mode=display">\text{s.t.} \quad \vec{w}_i^T \vec{1} = 1</script><p>利用拉格朗日乘子法，构造拉格朗日函数，有</p><script type="math/tex; mode=display">L(\vec{w}_i) = \vec{w}_i^T Z^{(i)} \vec{w}_i + \lambda (\vec{w}_i^T \vec{1} - 1) \tag{6.1}</script><p>则</p><script type="math/tex; mode=display">\begin{cases}    \frac{\nabla L(\vec{w}_i)}{\nabla \vec{w}_i} = 2 Z^{(i)} \vec{w}_i  + \lambda \vec{1} = \vec{0} \\    \frac{\nabla L(\vec{w}_i)}{\nabla \lambda} = \vec{w}_i^T \vec{1} - 1 = 0\end{cases} \tag{6.2}</script><p>由式$1$得到</p><script type="math/tex; mode=display">\vec{w}_i = - \frac{\lambda}{2} Z^{(i)-1} \vec{1} \tag{6.3}</script><p>代入式$2$有</p><script type="math/tex; mode=display">\left( - \frac{\lambda}{2} Z^{(i)-1} \vec{1} \right)^T \vec{1} = 1 \tag{6.4}</script><p>解得</p><script type="math/tex; mode=display">\lambda = - \frac{2}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{6.5}</script><p>代回$(6.3)$得到</p><script type="math/tex; mode=display">\vec{w}_i = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}</script><h2 id="低维数据的求解"><a href="#低维数据的求解" class="headerlink" title="低维数据的求解"></a>低维数据的求解</h2><p>由式$(3.2)$，即</p><script type="math/tex; mode=display">\hat{\vec{y}}^{(i)} = \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} \dot{w}_{ij} \vec{y}^{(j)} \tag{3.2}</script><p>同样的，利用最小二乘法，构建目标函数为</p><script type="math/tex; mode=display">J(Y) = \sum_{i=1}^M || \vec{y}^{(i)} - \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} \dot{w}_{ij} \vec{y}^{(j)} ||_2^2 \tag{7.1}</script><p>对低维数据进行标准化，即加入约束</p><script type="math/tex; mode=display">\begin{cases}    \sum_{i=1}^M \vec{y}^{(i)} = \vec{0} \\    \frac{1}{M} \sum_{i=1}^M \vec{y}^{(i)} \vec{y}^{(i)T} = I\end{cases} \tag{7.2}</script><p>矩阵形式为</p><script type="math/tex; mode=display">Y Y^T = M·I \tag{7.3}</script><p>由于当前数据点具体分布未知，故$\vec{y}^{(i)}$的近邻点$\mathcal{N}_K(\vec{y}^{(i)})$无法得知，故将矩阵$W_{M \times K}$补全为矩阵$\dot{W}_{M \times M}$，即</p><script type="math/tex; mode=display">\dot{w}_{ij} = \begin{cases}    w_{ik}  & x^{(j)} = N^{(i)}_k \\    0       & \text{otherwise}\end{cases} \tag{8}</script><blockquote><p>相当于邻接矩阵。</p></blockquote><p>同样的，$\vec{y}^{(i)}$也需在维度上进行补全为$\dot{\vec{y}}^{(i)}$，记</p><script type="math/tex; mode=display">\dot{Y}_{M \times M} = \left[ \begin{matrix} \dot{\vec{y}}^{(1)} & \dot{\vec{y}}^{(2)} & \cdots & \dot{\vec{y}}^{(M)} \end{matrix} \right] \tag{9}</script><p>则</p><script type="math/tex; mode=display">J(\dot{Y}) = \sum_{i=1}^M || \dot{\vec{y}}^{(i)} - \sum_{\dot{\vec{y}}^{(j)} \in \mathcal{N}_K(\dot{\vec{y}}^{(i)})} \dot{w}_{ij} \dot{\vec{y}}^{(j)} ||_2^2 \tag{7.1}</script><script type="math/tex; mode=display">= \sum_{i=1}^M || \dot{Y} \vec{1}_i - \dot{Y} \dot{\vec{w}_i} ||_2^2 = \sum_{i=1}^M || \dot{Y} (\vec{1}_i - \dot{\vec{w}_i}) ||_2^2 \tag{10.1}</script><script type="math/tex; mode=display">= \text{tr} \left[ \dot{Y} (I - \dot{W}) (I - \dot{W})^T \dot{Y}^T \right] \tag{10.2}</script><blockquote><script type="math/tex; mode=display">(10.1) \rightarrow (10.2): \mathcal{WTF}</script></blockquote><p>实际上</p><blockquote><p>由于</p><script type="math/tex; mode=display">d_i = \sum_{j=1}^M \dot{w}_{ij} = 1</script><p>所以由拉普拉斯矩阵定义</p><script type="math/tex; mode=display">L = D - W</script><p>$I - \dot{W}$即邻接矩阵$\dot{W}$的拉普拉斯矩阵。</p></blockquote><p>其中</p><script type="math/tex; mode=display">\vec{1}_i = \left[ \begin{matrix} 0 & \cdots & 1_i & \cdots & 0 \end{matrix} \right]^T</script><script type="math/tex; mode=display">I = \left[\begin{matrix}    1 &         &   \\      & \cdots  &   \\      &         & 1 \end{matrix}\right]</script><p>记$A = (I - \dot{W}) (I - \dot{W})^T$, 最终优化目标为</p><script type="math/tex; mode=display">J(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] \tag{*3}</script><script type="math/tex; mode=display">\text{s.t.} \quad \dot{Y} \dot{Y}^T = M·I</script><p>构造拉格朗日函数</p><script type="math/tex; mode=display">L(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] + \lambda (\dot{Y} \dot{Y}^T - M · I) \tag{11}</script><script type="math/tex; mode=display">\begin{cases}    \frac{\nabla L(\dot{Y})}{\nabla \dot{Y}} = 2 A \dot{Y}^T + 2 \lambda \dot{Y}^T = 0 \\    \frac{\nabla L(\dot{Y})}{\nabla \lambda} = \dot{Y} \dot{Y}^T - M · I = 0\\\end{cases}</script><blockquote><script type="math/tex; mode=display">\frac{\nabla \text{tr}[F(\vec{x})]}{\nabla \vec{x}} = f(\vec{x})^T</script></blockquote><p>注意$1$式，可变换为</p><script type="math/tex; mode=display">A \dot{Y}^T = \hat{\lambda} \dot{Y}^T \tag{12.1}</script><p>其中$\hat{\lambda} = - \lambda$，又$\dot{Y} \dot{Y}^T = M·I$，为<strong>正交相似变换</strong>，所以</p><script type="math/tex; mode=display">\dot{Y}^T = P = \left[ \begin{matrix} \vec{\alpha}_1 & \vec{\alpha}_2 & \cdots & \vec{\alpha}_M \end{matrix} \right] \tag{12.2}</script><p>由于最小化目标为</p><script type="math/tex; mode=display">J(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] = \text{tr} (\Lambda) = \sum_{i=1}^M \lambda_i</script><p>故选择最小的特征值$\lambda_i$及其对应的特征向量$\alpha_i$，要得到$D$维数据集，将$\dot{Y}^T$进行约化，即</p><script type="math/tex; mode=display">Y^T = \left[ \begin{matrix} \vec{\alpha}_{M + 1 - D} & \cdots & \vec{\alpha}_M \end{matrix} \right] \tag{13}</script><p>由于$\vec{w}_i^T \vec{1} = 1$，即</p><script type="math/tex; mode=display">\dot{W}^T \vec{1} = \vec{1} \tag{14.1}</script><p>移项整理得</p><script type="math/tex; mode=display">(\dot{W} - I)^T \vec{1} = \vec{0}</script><p>$\vec{1} \neq \vec{0}$，所以</p><script type="math/tex; mode=display">(\dot{W} - I)^T = 0 \tag{14.2}</script><p>左边同乘$\dot{W} - I$得到</p><script type="math/tex; mode=display">(\dot{W} - I) (\dot{W} - I)^T \vec{1} = A · \vec{1} = 0 · \vec{1} \tag{14.3}</script><p>所以</p><script type="math/tex; mode=display">\lambda_M = 0, \quad \alpha_M = \vec{1} \tag{14.4}</script><p>特征值为$0$表示不能反映数据特征，故低维数据应为</p><script type="math/tex; mode=display">Y^T = \left[ \begin{matrix} \vec{\alpha}_{M - D} & \cdots & \vec{\alpha}_{M-1} \end{matrix} \right] \tag{*4}</script><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><ol><li><p>计算原始样本空间中，每个样本$x^{(i)}$的近邻点$N^{(i)}$，并求取权值$w_{ij}$作为邻接权重，存储为矩阵$\dot{W}_{M \times M}$；</p><script type="math/tex; mode=display">Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \tag{5.3}</script><script type="math/tex; mode=display">\vec{w}_i = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}</script><script type="math/tex; mode=display"> \dot{w}_{ij} =  \begin{cases}     w_{ik}  & x^{(j)} = N^{(i)}_k \\     0       & \text{otherwise} \end{cases} \tag{8}</script></li><li><p>求取矩阵$A$，并将其特征分解</p><script type="math/tex; mode=display">A = (I - \dot{W}) (I - \dot{W})^T</script><script type="math/tex; mode=display">A \dot{Y}^T = \hat{\lambda} \dot{Y}^T \tag{12.1}</script></li><li><p>选择最小的特征值$\lambda_i$及其对应的特征向量$\alpha_i$，约化为$D$维数据，作为低维特征点</p><script type="math/tex; mode=display">Y^T = \left[ \begin{matrix} \vec{\alpha}_{M - D} & \cdots & \vec{\alpha}_{M-1} \end{matrix} \right] \tag{*4}</script></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocallyLinearEmbedding</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Locally Linear Embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_neighbors:  &#123;int&#125;</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        W: &#123;ndarray&#125; </span></span><br><span class="line"><span class="string">            $$ W = \left[ \begin&#123;matrix&#125; w_1 &amp; w_2 &amp; \cdots &amp; w_&#123;n_samples&#125; \end&#123;matrix&#125; \right] $$</span></span><br><span class="line"><span class="string">            $$ w_i = \left[ \begin&#123;matrix&#125; w_&#123;i1&#125; &amp; w_&#123;i2&#125; &amp; \cdots &amp; w_&#123;i, n_samples&#125; \end&#123;matrix&#125; \right]^T $$</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_neighbors, n_components=<span class="number">2</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.n_neighbors  = n_neighbors</span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.W = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            W: &#123;ndarray(n_samples, n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KDTree</span><br><span class="line">        kdtree = KDTree(X, metric=<span class="string">'euclidean'</span>)</span><br><span class="line">        </span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        self.W = np.zeros((n_samples, n_samples))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line"></span><br><span class="line">            <span class="comment">## 获取近邻样本点</span></span><br><span class="line">            x = X[i]</span><br><span class="line">            idx = kdtree.query(x.reshape(<span class="number">1</span>, <span class="number">-1</span>), self.n_neighbors + <span class="number">1</span>, return_distance=<span class="keyword">False</span>)[<span class="number">0</span>][<span class="number">1</span>: ]</span><br><span class="line">            <span class="comment">## 求取矩阵 Z = (x - N).dot((x - N).T)</span></span><br><span class="line">            N = X[idx]</span><br><span class="line">            Z = (x - N).dot((x - N).T)</span><br><span class="line">            <span class="comment">## 求取权重 w_i</span></span><br><span class="line">            Z_inv = np.linalg.inv(Z + np.finfo(float).eps * np.eye(self.n_neighbors))</span><br><span class="line">            w = np.sum(Z_inv, axis=<span class="number">1</span>) / np.sum(Z_inv)</span><br><span class="line">            <span class="comment">## 保存至 W</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_neighbors):</span><br><span class="line">                self.W[idx[j], i] = w[j]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.W</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 求取矩阵 A = (I - W)(I - W)^T</span></span><br><span class="line">        I = np.eye(n_samples)</span><br><span class="line">        A = (I - self.W).dot((I - self.W).T)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 对 A 进行特征分解，并按特征值升序排序</span></span><br><span class="line">        eigval, eigvec = np.linalg.eig(A)</span><br><span class="line">        eigvec = eigvec[:, np.argsort(eigval)]</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 选取 D 维</span></span><br><span class="line">        k_skip = <span class="number">1</span></span><br><span class="line">        Y = eigvec[:, k_skip: self.n_components + k_skip]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.fit(X)</span><br><span class="line">        Y = self.transform(X)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在<code>scikit learn</code>官网有具体实现<a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py" target="_blank" rel="noopener">基于手写数据集的几种基于谱的降维方法对比</a>。</p><p><img src="/2019/08/09/Locally-Linear-Embedding/sklearn.png" alt="sphx_glr_plot_lle_digits_008"></p><p>以下为上述代码实现的结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets  <span class="keyword">import</span> load_digits</span><br><span class="line"></span><br><span class="line">digits = load_digits(n_class=<span class="number">6</span>)</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">images = digits.images</span><br><span class="line"></span><br><span class="line">lle = LocallyLinearEmbedding(<span class="number">30</span>, <span class="number">2</span>)</span><br><span class="line">X_lle = lle.fit_transform(X)</span><br><span class="line"></span><br><span class="line">plot_embedding(X_lle, y, images, title=<span class="keyword">None</span>, t=<span class="number">2e-3</span>, figsize=(<span class="number">12</span>, <span class="number">9</span>))</span><br></pre></td></tr></table></figure></p><p><img src="/2019/08/09/Locally-Linear-Embedding/custom.png" alt="custom"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.jianshu.com/p/25a2a47bb60b" target="_blank" rel="noopener">(十二)LLE局部线性嵌入降维算法 - 简书</a></li><li><a href="https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding" target="_blank" rel="noopener">2.2.3. Locally Linear Embedding - scikit learn</a></li><li><a href="https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf" target="_blank" rel="noopener">An Introduction to Locally Linear Embedding</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
            <tag> manifold </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git工具-submodules</title>
      <link href="/2019/08/09/Git%E5%B7%A5%E5%85%B7-submodules/"/>
      <url>/2019/08/09/Git%E5%B7%A5%E5%85%B7-submodules/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>当需要在一个项目中使用另一个项目时，可以将后者作为子模块加入前者。</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="新建带子模块的仓库"><a href="#新建带子模块的仓库" class="headerlink" title="新建带子模块的仓库"></a>新建带子模块的仓库</h2><p>例如在本地新建仓库<code>Repository</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir Repository &amp;&amp; cd Repository</span><br><span class="line"><span class="meta">$</span> git init</span><br><span class="line">Initialized empty Git repository in C:/Users/islou/Desktop/Repository/.git/</span><br></pre></td></tr></table></figure></p><p>若在本仓库中，需要使用该仓库<a href="https://github.com/isLouisHsu/Games" target="_blank" rel="noopener">isLouisHsu/Games</a><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git submodule add https://github.com/isLouisHsu/Games</span><br><span class="line">Cloning into 'C:/Users/islou/Desktop/Repository/Games'...</span><br><span class="line">remote: Enumerating objects: 30, done.</span><br><span class="line">remote: Counting objects: 100% (30/30), done.</span><br><span class="line">remote: Compressing objects: 100% (27/27), done.</span><br><span class="line">remote: Total 30 (delta 5), reused 0 (delta 0), pack-reused 0</span><br><span class="line">Unpacking objects: 100% (30/30), done.</span><br></pre></td></tr></table></figure></p><p>在当前仓库<code>Repository</code>中可以看到生成了文件<code>.gitmodules</code>与子仓库<code>Games</code>，<code>.gitmodules</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[submodule &quot;Games&quot;]</span><br><span class="line">path = Games</span><br><span class="line">url = https://github.com/isLouisHsu/Games</span><br></pre></td></tr></table></figure></p><p>此时，若在当前仓库查询状态，显示更改内容<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git status</span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use "git rm --cached &lt;file&gt;..." to unstage)</span><br><span class="line"></span><br><span class="line">        new file:   .gitmodules</span><br><span class="line">        new file:   Games</span><br><span class="line"><span class="meta">$</span></span><br><span class="line"><span class="meta">$</span> cd Games</span><br><span class="line"><span class="meta">$</span> git status</span><br><span class="line">On branch master</span><br><span class="line">Your branch is up to date with 'origin/master'.</span><br><span class="line"></span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure></p><h2 id="克隆带子仓库的模块"><a href="#克隆带子仓库的模块" class="headerlink" title="克隆带子仓库的模块"></a>克隆带子仓库的模块</h2><ol><li><p>方法1</p><p> 克隆这类仓库时，默认包含该子模块目录，但其中没有文件，需要在仓库目录下运行</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git submodule init</span><br><span class="line"><span class="meta">$</span> git submodule update</span><br></pre></td></tr></table></figure><p> <code>git submodule init</code> 用来初始化本地配置文件，而 <code>git submodule update</code> 则从该项目中抓取所有数据并检出父项目中列出的合适的提交</p></li><li><p>方法2</p><p> 直接使用<code>--recursive</code>参数，自动初始化并更新仓库中的每一个子模块</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git clone --recursive https://github.com/louishsu/Repository</span><br></pre></td></tr></table></figure></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://git-scm.com/book/zh/v2/Git-%E5%B7%A5%E5%85%B7-%E5%AD%90%E6%A8%A1%E5%9D%97" target="_blank" rel="noopener">7.11 Git工具-子模块 - git</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> github </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python生成Markdown表格</title>
      <link href="/2019/08/09/Python%E7%94%9F%E6%88%90Markdown%E8%A1%A8%E6%A0%BC/"/>
      <url>/2019/08/09/Python%E7%94%9F%E6%88%90Markdown%E8%A1%A8%E6%A0%BC/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h1 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h1><p>Markdown中编辑表格比较繁琐，如编辑下表时，需要按字符输入</p><div class="table-container"><table><thead><tr><th style="text-align:center">姓名\科目</th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td style="text-align:center">小妖</td><td>3</td><td>4</td><td>5</td><td>3</td></tr><tr><td style="text-align:center">小怪</td><td>4</td><td>5</td><td>3</td><td>4</td></tr><tr><td style="text-align:center">小兽</td><td>5</td><td>3</td><td>4</td><td>5</td></tr></tbody></table></div><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| 姓名\科目 | A | B | C | D |</span><br><span class="line">| :------: | - | - | - | - |</span><br><span class="line">| 小妖     | 3 | 4 | 5 | 3 |</span><br><span class="line">| 小怪     | 4 | 5 | 3 | 4 |</span><br><span class="line">| 小兽     | 5 | 3 | 4 | 5 |</span><br></pre></td></tr></table></figure><p>可借助字符串操作生成表格。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_markdown_table_2d</span><span class="params">(head_name, rows_name, cols_name, data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        head_name: &#123;str&#125; 表头名， 如"count\比例"</span></span><br><span class="line"><span class="string">        rows_name, cols_name: &#123;list[str]&#125; 项目名， 如 1,2,3</span></span><br><span class="line"><span class="string">        data: &#123;ndarray(H, W)&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        table: &#123;str&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ELEMENT = <span class="string">" &#123;&#125; |"</span></span><br><span class="line"></span><br><span class="line">    H, W = data.shape</span><br><span class="line">    LINE = <span class="string">"|"</span> + ELEMENT * W</span><br><span class="line">    </span><br><span class="line">    lines = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 表头部分</span></span><br><span class="line">    lines += [<span class="string">"| &#123;&#125; | &#123;&#125; |"</span>.format(head_name, <span class="string">' | '</span>.join(cols_name))]</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 分割线</span></span><br><span class="line">    SPLIT = <span class="string">"&#123;&#125;:"</span></span><br><span class="line">    line = <span class="string">"| &#123;&#125; |"</span>.format(SPLIT.format(<span class="string">'-'</span>*len(head_name)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(W):</span><br><span class="line">        line = <span class="string">"&#123;&#125; &#123;&#125; |"</span>.format(line, SPLIT.format(<span class="string">'-'</span>*len(cols_name[i])))</span><br><span class="line">    lines += [line]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## 数据部分</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(H):</span><br><span class="line">        d = list(map(str, list(data[i])))</span><br><span class="line">        lines += [<span class="string">"| &#123;&#125; | &#123;&#125; |"</span>.format(rows_name[i], <span class="string">' | '</span>.join(d))]</span><br><span class="line"></span><br><span class="line">    table = <span class="string">'\n'</span>.join(lines)</span><br><span class="line">    <span class="keyword">return</span> table</span><br></pre></td></tr></table></figure><p>终端中运行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; from temp import gen_markdown_table_2d</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; head_name = "姓名\\科目"</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; rows_name = ["小妖", "小怪", "小兽"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; cols_name = ["A", "B", "C", "D"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; data = np.arange(4*3).reshape(3, 4)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; table = gen_markdown_table_2d(head_name, rows_name, cols_name, data)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; table</span><br><span class="line">'| 姓名\\科目 | A | B | C | D |\n| -----: | -: | -: | -: | -: |\n| 小妖 | 0 | 1 | 2 | 3 |\n| 小怪 | 4 | 5 | 6 | 7 |\n|  小兽 | 8 | 9 | 10 | 11 |'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(table)</span><br><span class="line">| 姓名\科目 | A | B | C | D |</span><br><span class="line">| -----: | -: | -: | -: | -: |</span><br><span class="line">| 小妖 | 0 | 1 | 2 | 3 |</span><br><span class="line">| 小怪 | 4 | 5 | 6 | 7 |</span><br><span class="line">| 小兽 | 8 | 9 | 10 | 11 |</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自动化脚本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ID3, C4.5, CART, RF</title>
      <link href="/2019/08/02/ID3-C4-5-CART-RF/"/>
      <url>/2019/08/02/ID3-C4-5-CART-RF/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h1 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h1><p>决策树(Decision Tree)是一种基本的分类与回归方法，本文主要讨论分类决策树，它可被认作<code>if-then</code>的集合，也可以认作是定义在特征空间与类空间上的条件概率分布。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>决策树模型形式如下，由结点<code>(node)</code>，有向边<code>(directed edge)</code>组成，节点包含两种：内部节点<code>(internal node)</code>和叶节点<code>(leaf node)</code>，内部节点表示一个特征或属性，叶节点包含一个类。</p><p><img src="/2019/08/02/ID3-C4-5-CART-RF/model.jfif" alt="model"></p><p>决策树需要满足一个重要性质：互斥且完备。即每个实例<strong>都被且仅被</strong>一条路径或一条<code>if-then</code>规则覆盖。并且生成的决策树深度不能过大。</p><p>决策树学习主要有3个步骤： 特征选择、决策树生成、决策树修剪。现以下某贷款申请样本数据表为例进行解释说明。</p><div class="table-container"><table><thead><tr><th>ID</th><th>年龄</th><th>工作</th><th>有房</th><th>信贷情况</th><th>类别</th></tr></thead><tbody><tr><td>1</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>2</td><td>青年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>3</td><td>青年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>4</td><td>青年</td><td>是</td><td>是</td><td>一般</td><td>是</td></tr><tr><td>5</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>6</td><td>中年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>7</td><td>中年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>8</td><td>中年</td><td>是</td><td>是</td><td>好</td><td>是</td></tr><tr><td>9</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>10</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>11</td><td>老年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>12</td><td>老年</td><td>否</td><td>是</td><td>好</td><td>是</td></tr><tr><td>13</td><td>老年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>14</td><td>老年</td><td>是</td><td>否</td><td>非常好</td><td>是</td></tr><tr><td>15</td><td>老年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr></tbody></table></div><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><p>设数据集为$D$，$|D|$表示样本总数。设有$K$个类别$C_k(k=1, \cdots, K)$，$|C_k|$为类别$k$的样本数目，那么</p><script type="math/tex; mode=display">|D| = \sum_{k=1}^K |C_k|</script><p>设特征$A$有$N$个不同的取值$\{a_1, a_2, \cdots, a_N\}$，则根据不同的取值，可将数据集$D$划分为$N$组$\{D_1, D_2, \cdots, D_N\}$，也有</p><script type="math/tex; mode=display">|D| = \sum_{n=1}^N |D_n|</script><p>记子集$D_n$中属于类别$C_k$的样本集合为$D_{nk}$，即</p><script type="math/tex; mode=display">D_{nk} = D_n \bigcap C_k</script><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>在某个内部节点上，需要选择最具有代表性，或区分度最高的特征。可选用的准则有信息增益（条件熵）、基尼系数等。</p><ol><li><p>ID3：以信息增益为准则来选择最优划分属性</p><p> <strong>定义(信息增益)</strong>：特征$A$对训练数据集$D$的信息增益$g(D, A)$，定义为集合$D$的经验熵$H(D)$与特征A给定条件下$D$的禁言条件上$H(D|A)$之差，即</p><script type="math/tex; mode=display">g(D, A) = H(D) - H(D|A) \tag{1.1}</script><p> 等价与训练数据集中类与特征的互信息<code>(mutual information)</code>。其中</p><script type="math/tex; mode=display">H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} \tag{1.2}</script><p> 上式即，类别分布的熵。</p><script type="math/tex; mode=display">H(D|A) = \sum_{n=1}^N \frac{|D_n|}{|D|} H(D_n) = - \sum_{n=1}^N \frac{|D_n|}{|D|} \sum_{k=1}^K \frac{|D_{nk}|}{|D_i|} \log_2 \frac{|D_{nk}|}{|D_i|} \tag{1.3}</script><p> 上式即，特征$A$不同取值下，类别分布的熵的加权和。</p><blockquote><p>注：</p><ol><li>熵<script type="math/tex; mode=display">H(X) = - \sum_x p(x) \log p(x)</script></li><li>条件熵<script type="math/tex; mode=display">H(X|Y) = - \sum_{x, y} p(x, y) \log p(y|x)</script></li></ol><p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息<code>(mutual information)</code>。</p></blockquote></li><li><p>C4.5：基于信息增益率准则选择最优分割属性的算法</p><p> 与<code>ID3</code>不同之处是，以信息增益比<code>(information gain ratio)</code>作为特征选择的准则。</p><script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)} \tag{2.1}</script><p> 其中</p><script type="math/tex; mode=display">H_A(D) = - \sum_{n=1}^N \frac{|D_n|}{|D|} \log_2 \frac{|D_n|}{|D|} \tag{2.2}</script></li><li><p>CART：以基尼系数为准则选择最优划分属性，可以应用于分类和回归</p><p> <strong>定义(基尼系数)</strong>：分类问题中，假设有$K$个类别，样本点属于第$k$类的概率为$p_k$，则基尼系数定义为</p><script type="math/tex; mode=display">\text{Gini}(p) = \sum_{k=1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2 \tag{3.1}</script><ul><li><p>分类树</p><p>  特征选择准则为</p><script type="math/tex; mode=display">g_G(D, A) = \text{Gini}(D) - \text{Gini}(D|A)  \tag{3.2}</script><p>  其中</p><script type="math/tex; mode=display">\text{Gini}(D) = 1 - \sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right)^2 \tag{3.3}</script><script type="math/tex; mode=display">\text{Gini}(D|A) = \sum_{n=1}^N \frac{|D_n|}{|D|} \text{Gini}(D_n) = \sum_{n=1}^N \frac{|D_n|}{|D|} \left[ 1 - \sum_{k=1}^K \left( \frac{|D_{nk}|}{|D_n|} \right)^2 \right] \tag{3.4}</script></li><li><p>回归树</p><p>  寻找最佳切分点，将输入空间划分为多个区域。</p></li></ul></li></ol><h2 id="决策树生成"><a href="#决策树生成" class="headerlink" title="决策树生成"></a>决策树生成</h2><p>决策树生成使用递归方法，将该节点处所属数据子集$D_s$按<a href="#%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9">特征选择准则</a>，选择当期节点最优划分，然后在划分后的子节点处，继续调用该函数进行递归生成。</p><p>伪代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">输入：数据集D，特征集A = &#123;a_1, a_2, ..., a_N&#125;，阈值eps；</span><br><span class="line">输出：节点node</span><br><span class="line"></span><br><span class="line">CREAT-NODE(D, A, eps)</span><br><span class="line">    if D中所有样本属于类别C_k</span><br><span class="line">        return Node(C_k)</span><br><span class="line"></span><br><span class="line">    if A为空集</span><br><span class="line">        C = argmax(|D_k|, k = 1, ..., K)</span><br><span class="line">        return Node(C)</span><br><span class="line"></span><br><span class="line">    # 计算特征选择准则，这里采用互信息g(D, A)</span><br><span class="line">    for n = 1 upto N</span><br><span class="line">        g(D, A_n) = H(D) - H(D|A_n)</span><br><span class="line">    </span><br><span class="line">    # 选择最优特征</span><br><span class="line">    a_g = argmax(g(D, A_n), n = 1, ..., N)</span><br><span class="line"></span><br><span class="line">    # 终止条件</span><br><span class="line">    if g(D, a_g) &lt; eps</span><br><span class="line">        C = argmax(|D_k|, k = 1, ..., K)</span><br><span class="line">        return Node(C)</span><br><span class="line">    </span><br><span class="line">    node = Node(a_g)</span><br><span class="line"></span><br><span class="line">    # 递归</span><br><span class="line">    for i = 1 upto a_g.length</span><br><span class="line">        node.sub[i] = CREAT-NODE(D_i, A - a_g, eps)</span><br><span class="line"></span><br><span class="line">    return node</span><br></pre></td></tr></table></figure></p><h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><p>略</p><h1 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h1><p>以第一个内部节点为例，选用信息增益作为分类特征。</p><p>类别标签有两种</p><script type="math/tex; mode=display">D.y = \{ 是， 否 \}</script><p>则</p><script type="math/tex; mode=display">p(y = 是) = \frac{9}{15},\quad p(y = 否) = \frac{6}{15}</script><script type="math/tex; mode=display">H(D) = - \frac{9}{15} \log_2 \frac{9}{15} - \frac{6}{15} \log_2 \frac{6}{15} = 0.971</script><p>当前特征集为</p><script type="math/tex; mode=display">A = \{ 年龄， 有工作， 有自己的房子， 信贷情况 \}</script><ol><li><p>以年龄为特征<br> 年龄含青年、中年、老年，则</p><script type="math/tex; mode=display">p(青年) = \frac{5}{15}, \quad p(中年) = \frac{5}{15}, \quad p(老年) = \frac{5}{15}</script><script type="math/tex; mode=display">p(y = 是|青年) = \frac{2}{5}, \quad p(y = 否|青年) = \frac{3}{5}</script><script type="math/tex; mode=display">p(y = 是|中年) = \frac{3}{5}, \quad p(y = 否|中年) = \frac{2}{5}</script><script type="math/tex; mode=display">p(y = 是|老年) = \frac{4}{5}, \quad p(y = 否|老年) = \frac{1}{5}</script><p> 则</p><script type="math/tex; mode=display">H(D|青年) = - \frac{2}{5} \log_2 \frac{2}{5} - \frac{3}{5} \log_2 \frac{3}{5} = 0.971</script><script type="math/tex; mode=display">H(D|中年) = - \frac{3}{5} \log_2 \frac{3}{5} - \frac{2}{5} \log_2 \frac{2}{5} = 0.971</script><script type="math/tex; mode=display">H(D|老年) = - \frac{4}{5} \log_2 \frac{4}{5} - \frac{1}{5} \log_2 \frac{1}{5} = 0.722</script><p> 则</p><script type="math/tex; mode=display">H(D|年龄) = \sum_{年龄=青年}^{老年} p(年龄) H(D|年龄) = 0.888</script><p> 所以</p><script type="math/tex; mode=display">g(D, 年龄) = H(D) - H(D|年龄) = 0.083</script></li><li><p>以工作为特征</p><script type="math/tex; mode=display">g(D, 工作) = H(D) - H(D|工作) = 0.324</script></li><li><p>以房子为特征</p><script type="math/tex; mode=display">g(D, 房子) = H(D) - H(D|房子) = 0.420</script></li><li><p>以信贷为特征</p><script type="math/tex; mode=display">g(D, 信贷) = H(D) - H(D|信贷) = 0.363</script></li></ol><p>由于$g(D, 房子)$最大，故当前节点选择房子作为分类特征，然后按此特征可分成2个子集合$\{D_{有房}，D_{无房}\}$，递归即可。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>详细查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/p110_decision_tree.py" target="_blank" rel="noopener">Github: isLouisHsu/Basic-Machine-Learning-Algorithm</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        index: 子树分类标签, 若为叶节点, 则为None</span></span><br><span class="line"><span class="string">        childNode: 子树，若为叶节点, 则为分类标签; 否则为字典</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.index = <span class="keyword">None</span></span><br><span class="line">        self.childNode = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @note:  </span></span><br><span class="line"><span class="string">        - categorical features;</span></span><br><span class="line"><span class="string">        - ID3</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.tree = <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">     </span><br><span class="line">        self.tree = self._creatNode(X, y)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_creatNode</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    </span><br><span class="line">        node = Node()</span><br><span class="line">        <span class="comment"># 若只含一种类别，则返回叶节点</span></span><br><span class="line">        <span class="keyword">if</span> len(set(y)) == <span class="number">1</span>: node.childNode = list(set(y))[<span class="number">0</span>]; <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line">        <span class="comment"># entropy: H(D)</span></span><br><span class="line">        y_encoded = OneHotEncoder().fit_transform(y.reshape(<span class="number">-1</span>, <span class="number">1</span>)).toarray()</span><br><span class="line">        p_y = np.mean(y_encoded, axis=<span class="number">0</span>); p_y[p_y==<span class="number">0.0</span>] = <span class="number">1.0</span>       <span class="comment"># 因为 0*np.log(0)结果为nan, 而不是0, 用 1*np.log(1)替代</span></span><br><span class="line">        H_D = - np.sum(p_y * np.log(p_y))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># conditional entropy: H(D|A)</span></span><br><span class="line">        H_D_A = np.zeros(shape=(X.shape[<span class="number">1</span>],))                       <span class="comment"># initialize</span></span><br><span class="line">        <span class="keyword">for</span> i_feature <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">            X_feature = X[:, i_feature]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> len(set(X_feature)) == <span class="number">1</span>: </span><br><span class="line">                H_D_A[i_feature] = float(<span class="string">'inf'</span>); <span class="keyword">continue</span>           <span class="comment"># 若该特征只有一种取值，表示已使用该列作为分类特征</span></span><br><span class="line">        </span><br><span class="line">            X_feature_encoded = OneHotEncoder().fit_transform(X_feature.reshape((<span class="number">-1</span>, <span class="number">1</span>))).toarray()</span><br><span class="line">            p_X = np.mean(X_feature_encoded, axis=<span class="number">0</span>)                <span class="comment"># 每个取值的概率</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> j_feature <span class="keyword">in</span> range(X_feature_encoded.shape[<span class="number">1</span>]):     <span class="comment"># 该特征取值有几种，编码后就有几列</span></span><br><span class="line">                y_encoded_feature = y_encoded[X_feature_encoded[:, j_feature]==<span class="number">1</span>]   <span class="comment"># 该特征某种取值下，其对应的标签值</span></span><br><span class="line">                p_y_X = np.mean(y_encoded_feature, axis=<span class="number">0</span>); p_y_X[p_y_X==<span class="number">0.0</span>] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">                H_D_feature = - np.sum(p_y_X * np.log(p_y_X))</span><br><span class="line">                H_D_A[i_feature] += p_X[j_feature] * H_D_feature    <span class="comment"># 条件熵</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># information gain: g(D, A) = H(D) - H(D|A)</span></span><br><span class="line">        g_D_A = H_D - H_D_A</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选出最大的作为分类特征</span></span><br><span class="line">        node.index = np.argmax(g_D_A)</span><br><span class="line">        X_selected = X[:, node.index]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分类后继续建立树</span></span><br><span class="line">        node.childNode = dict()</span><br><span class="line">        <span class="keyword">for</span> val <span class="keyword">in</span> set(X_selected):</span><br><span class="line">            valIndex = (X_selected==val)</span><br><span class="line">            X_val, y_val = X[valIndex], y[valIndex]</span><br><span class="line">            node.childNode[val] = self._creatNode(X_val, y_val)     <span class="comment"># 存储在字典中，键为分类值，值为子树</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        y_pred = np.zeros(shape=(X.shape[<span class="number">0</span>],))</span><br><span class="line">        <span class="keyword">for</span> i_sample <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">            currentNode = self.tree                                 <span class="comment"># 初始化为父节点</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> currentNode.index==<span class="keyword">None</span>:                      <span class="comment"># 若为None, 表示为叶子结点</span></span><br><span class="line">                val = X[i_sample, currentNode.index]                <span class="comment"># 当前样本在分类特征上的值</span></span><br><span class="line">                currentNode = currentNode.childNode[val]            <span class="comment"># 递归</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_pred[i_sample] = currentNode.childNode</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><p>最终生成分类树如下<br><img src="/2019/08/02/ID3-C4-5-CART-RF/tree.jpg" alt="tree"></p><h1 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/34534004" target="_blank" rel="noopener">ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结</a></li><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">统计学习方法，李航，第5章</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>人间失格</title>
      <link href="/2019/07/21/%E4%BA%BA%E9%97%B4%E5%A4%B1%E6%A0%BC/"/>
      <url>/2019/07/21/%E4%BA%BA%E9%97%B4%E5%A4%B1%E6%A0%BC/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/07/21/人间失格/image.jpg" alt="image"></p><p>人类是群居的孤独者。</p><h1 id="第一手札"><a href="#第一手札" class="headerlink" title="第一手札"></a>第一手札</h1><ol><li>尽管如此，他们却能够不死自杀，免于疯狂，纵谈政治，竟不绝望，不屈不挠，继续与生活搏斗。他们不是并不痛苦吗？他们使自己成为彻底的利己主义者，并虔信那一些里所担任，曾几何时怀疑过自己呢？这样一来，不是很轻松惬意吗？</li></ol><blockquote><p>太平洋战争爆发后，当时的许多作家纷纷创造起了激发人们战斗欲望的小说作品，唯独太宰治不一样，他从不去碰战争题材，相反的，太宰治为了让那些为了度过困难时期而奋斗着的人们于百忙之中能得到片刻慰藉，于是发明了一种新的写作方式——私小说，并创作了令人轻松愉悦捧腹大笑的作品——《御伽草纸》。<br>一直到日本二战战败以后，许多曾经崇拜战争的作家们纷纷“临阵倒戈”。太宰治发现，人们似乎根本没有意识到自己的罪恶，并创作了以二战为背景的战争题材小说《斜阳》：“被记者追捧者，鼓吹民主主义什么的，我不干，所有日本人都参与了战争。”</p></blockquote><ol><li>这是我向人类最后的求爱。尽管我对人类极度恐惧，但似乎始终割不断对人类的缘情，于是借着装傻这一缕细丝，来维系与人类的贯联。表面上我总是笑脸迎人，暗中则是拼了死命，战战兢兢，如履薄冰般才艰难万分做出这样的奉侍。</li></ol><blockquote><p>戴上人性的面具。</p></blockquote><ol><li>我却从人们动怒的面孔中发现了比狮子、鳄鱼、巨龙更可怕的动物本性。平常他们总是隐藏起这种动物本性，可一旦遇到某个时机，他们就会像那些温文尔雅地躺在草地上歇息的牛，蓦然甩动尾巴抽死肚皮上的牛虻一般，暴露出人的这种本性。</li><li>对讨厌的事不能说讨厌，而对喜欢的事呢，也是一样。</li><li>我对受人尊敬这一状态进行了如下定义：近于完美无缺地蒙骗别人，尔后又被某个全知全能之人识破真相，最终原形毕露，被迫当众出丑，以致于比死亡更难堪更困窘。</li><li>那时，我被男女佣人教唆者做出了可悲的丑事。事到如今我认为，对年幼者干出那种事情，无疑是人类所能犯下的罪孽中最丑恶最卑劣的行径。</li><li>不公平现象是必然存在的。这点是明摆着的事实。向人诉苦不过是徒劳，与其如此，不如默默承受。</li><li>相互欺骗，却又令人惊奇地不受到任何伤害，甚至于就好像没有察觉到彼此在欺骗似的，这种不加掩饰从而显得清冽、豁达的互不信任的例子，在人类生活中比比皆是。</li></ol><h1 id="第二手札"><a href="#第二手札" class="headerlink" title="第二手札"></a>第二手札</h1><ol><li>在迄今为止的生涯中，我曾经无数次祈望过自己被杀死，却从来也没有动过杀死别人的念头。这是因为我觉得，那样做只会给可怕的对手带来幸福的缘故。</li><li>“迷恋”、“被迷恋”这些措辞本身就是粗俗不堪而又戏弄人的说法，给人一种装腔作势的感觉。无论是多么“严肃”的场合，只要让这些词语抛头露面，忧郁的伽蓝就会顷刻间分崩离析，变得索然无味。</li><li>对人感到过分恐惧的人，反倒更加迫切地希望用自己地眼睛去看更可怕的妖怪；越是容易对事物感到胆怯的神经质的人，就越是渴望暴风雨降临得更加猛烈。</li><li>人啊，明明一点儿也不了解对方，错看对方，却视彼此为独一无二的挚友，一生不解对方的真性情，待一方撒手西去，还要为其哭泣，念诵悼词。</li><li>竭力想把觉得美的东西原封不动地描绘为美是幼稚和愚蠢乃至完全谬误的。</li><li>胆小鬼连幸福都会害怕，碰到棉花都会受伤，有时还被幸福所伤。</li></ol><h1 id="第三手札"><a href="#第三手札" class="headerlink" title="第三手札"></a>第三手札</h1><ol><li>人怎么能如此轻易地变得面目全非呢？这令我感到可耻，不，毋宁说是滑稽。</li><li>世上所有人的说法，总是显得拐弯抹角，含糊不清，其中有一种试图逃避责任似的微妙性和复杂性。</li><li>那以后我也尝试过画各种各样的画，但都远远及不上那记忆中的杰作，以致于我总是被一种失落感所折磨着，恍若整个胸膛都变成了一个空洞。</li><li>“可事实上，我是多么畏惧他们啊！我越是畏惧他们，就越是博得他们的喜欢，而越是博得他们的喜欢，我就越是畏惧他们，并不得不里他们远去。”</li><li>莫非在别人眼里，我那种畏惧他人、躲避他人、搪塞他人的性格，竟然与遵从俗话所说的那种“明哲保身、得过且过”的处世训条的做法，在表现形式上是相同的吗？</li><li>所谓世间，又是什么呢？是人的复数吗？可哪儿存在着“世间”这个东西的实体呢？迄今为止，我一直以为它是一种苛烈、严酷、而且可怕的东西，并且一直生活在这种想法之中，如今被崛木一说，有一句话差点就脱口而出：“所谓的世人，不就是你吗？”……打那时候起，我开始萌发了一种可以称之为“思想”的念头：所谓世间，不就是个人吗？</li><li>第二天也重复着同一件事情/只需遵从与昨天相同的习性/倘若愿意避免狂喜狂乐/大惊大悲就不会降临/躲开前方的挡路巨石/像蟾蜍一般迂回前进。</li><li>我知道有人是爱我的，但我好像缺乏爱人的能力。</li><li>所谓世间的真相，就是个人与个人之间的争斗，而且是即时即地的斗争。人需要在那种争斗中当场取胜，人是绝不可能服从他人的。即使是当努力，也会以努力的方式进行卑屈的反击。所以，人除了当场一决胜负外，不可能有别的生存方式。虽然人们提倡大义名分，但努力的目标毕竟是属于个人的，超越了个人之后依旧还是个人，世间的不可思议其实也就是个人的不可思议。</li><li>难道纯真无瑕的信赖之心真的是罪恶之源吗？难道纯真无瑕的信赖之心也算是罪过吗？</li><li>我的不幸，恰恰在于我缺乏拒绝的能力。我害怕一旦拒绝别人，便会在彼此心里留下永远无法愈合的裂痕。</li><li>但是，被关进这所医院的人全是狂人，而逍遥在外的全都是正常人。我问神灵：难道不反抗也是一种罪过吗？</li><li>我已丧失做人的资格。我已经彻底变成一个废人了。</li></ol><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>大庭叶藏是善良的人，心思细腻敏感却怯懦敏感，性格孤独，渴望被爱，在世间选择了极端的生活方式，根源最主要还是软弱的灵魂，没有自我</p><ul><li>自小充满幻想，对幻想破灭而大觉扫兴；</li><li>见到静子母女的背影，黯然离去，不忍打扰；</li><li>良子过于信任他人，与书商发生关系，而叶藏反复认定这是因为自己欣赏良子的纯真无暇，这不是她的错；</li><li>堀木作为叶藏的“朋友”，对叶藏一直抱有轻蔑的态度，叶藏看破不说破，并从堀木家中看到东京人真实的生活状态；</li><li>在药店老板的诱导下，吸毒上瘾；</li><li>不忍拒绝他人的请求；</li><li>“我们所认识的阿叶，又诚实又乖巧，要是不喝酒的话，不，即使是喝酒……也是一个神一样的好孩子呐。”</li></ul><p>想要成为一个人，就必须接受自己会犯错，会让人讨厌的事实，不要在乎他人的眼光。</p>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SphereFace, CosFace, ArcFace</title>
      <link href="/2019/07/13/SphereFace-CosFace-ArcFace/"/>
      <url>/2019/07/13/SphereFace-CosFace-ArcFace/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/feature.gif" alt="feature"></p><blockquote><p>文中合成<code>gif</code>图不清晰，原图可在<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/tree/master/2019/07/13/SphereFace-CosFace-ArcFace/gif" target="_blank" rel="noopener">该页面</a>下载获取</p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>深度神经网络可将样本映射到超空间中的嵌入向量，然而若不在损失中增加几何约束，该超空间中的嵌入向量不具有几何意义。现对空间中嵌入向量间的余弦距离研究甚是火热，可用于<code>open-set</code>数据集的识别问题，特别是人脸验证问题。</p><blockquote><p>所谓<code>open-set</code>与<code>close-set</code>，是指训练集中是否包含测试集中的类别，如人脸问题中，训练集不可能包含所有人的人脸数据，则如何识别未出现在训练集中的样本成为一个问题。<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/open-close-set.jpg" alt="open-close-set"></p></blockquote><p>目前对嵌入向量的约束方案有如下几种</p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/intra-interstrategy.jpg" alt="intra-interstrategy"></p><ul><li>Centre Loss，惩罚样本对应特征与其对应类别中心之间的欧氏距离，以获得类内紧致性，本文不做介绍；</li><li>Triplet Loss，在训练样本中寻找三元组，组合数量爆炸，且计算量较大，本文不做介绍；</li><li>CosFace: Additive Cosine Margin；</li><li>SphereFace: Multiplicative Angular Margin；</li><li>ArcFace: Additive Angular Margin。</li></ul><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="Maltilabel-Softmax-CrossEnt"><a href="#Maltilabel-Softmax-CrossEnt" class="headerlink" title="Maltilabel: Softmax + CrossEnt"></a>Maltilabel: Softmax + CrossEnt</h2><p>在多分类问题中，网络输出层一般设置为<code>Softmax</code>层</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_j = \frac{e^{f^{(i)}_j}}{\sum_k e^{f^{(i)}_k}} \tag{1}</script><p>其中$f^{(i)}_j$表示<code>Softmax</code>层输入，一般为前一层网络的线性输出。损失函数一般采用交叉熵<code>Cross Entropy</code></p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \tilde{y}^{(i)}_{y^{(i)}} \tag{2}</script><p>其中，$N$为<code>batchsize</code>大小，$\tilde{y}^{(i)}_j$表示样本$X^{(i)}$预测为第$j$类的概率输出，$y^{(i)}$为其对应真实标签。</p><p>也即</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{f^{(i)}_{y^{(i)}}}}{\sum_k e^{f^{(i)}_k}} \tag{*}</script><blockquote><script type="math/tex; mode=display">H(p, q) = \sum_x p(x) \log \frac{1}{q(x)}</script><p>其中$p(x), q(x)$为概率分布。</p></blockquote><h2 id="Geometric-Modified-Softmax-Loss"><a href="#Geometric-Modified-Softmax-Loss" class="headerlink" title="Geometric: Modified Softmax Loss"></a>Geometric: Modified Softmax Loss</h2><p>由于<code>Softmax</code>不具有几何意义，对其进行修改，在前一层与<code>Softmax</code>层之间添加线性层，即</p><script type="math/tex; mode=display">g^{(i)}_j = W_j^T f^{(i)} + b_j \tag{3}</script><p>其中</p><script type="math/tex; mode=display">W_j^T f^{(i)} = ||W_j|| ||f^{(i)}|| \cos \theta^{(i)}_j \tag{4}</script><p>$\theta^{(i)}_j$表示$W_j$与$f^{(i)}$夹角，$f^{(i)}$为上一层输出，其维数为$D$，则参数$W, b$维数应为$C \times D, C \times 1$。</p><p>此时类别$c_1$与$c_2$间决策平面为</p><script type="math/tex; mode=display">W_{c_1}^T f^{(i)} + b_{c_1} = W_{c_2}^T f^{(i)} + b_{c_2}</script><p>令$||W_j|| = 1, b_j = 0$，有</p><script type="math/tex; mode=display">g^{(i)}_j = W_j^T f^{(i)} + b_j = ||f^{(i)}|| \cos \theta^{(i)}_j \tag{5}</script><p>则判决平面为</p><script type="math/tex; mode=display">||f^{(i)}|| \cos \theta^{(i)}_{c_1} = ||f^{(i)}|| \cos \theta^{(i)}_{c_2}</script><p>也即</p><script type="math/tex; mode=display">\cos \theta^{(i)}_{c_1} = \cos \theta^{(i)}_{c_2}\tag{6}</script><p>则此时向量夹角大小可作为判决平面，赋予角度上的几何意义，将$g^{(i)}$输入<code>Softmax</code>层与<code>CrossEnt</code>损失</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_j = \frac{e^{g^{(i)}_j}}{\sum_k e^{g^{(i)}_k}} = \frac{e^{||f^{(i)}|| \cos \theta^{(i)}_j}}{\sum_k e^{||f^{(i)}|| \cos \theta^{(i)}_k}}</script><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \tilde{y}^{(i)}_{y^{(i)}}</script><p>写成一个等式，在<code>SphereFace</code>一文中，称下式为<code>Modified Softmax Loss</code></p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{||f^{(i)}|| \cos \theta^{(i)}_{y^{(i)}}}}{\sum_k e^{||f^{(i)}|| \cos \theta^{(i)}_k}} \tag{*1}</script><p>其与<code>Softmax</code>对比如下<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/softmax-a-softmax.jpg" alt="softmax-a-softmax"></p><h2 id="CosFace-Additive-Cosine-Margin"><a href="#CosFace-Additive-Cosine-Margin" class="headerlink" title="CosFace: Additive Cosine Margin"></a>CosFace: Additive Cosine Margin</h2><p>在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin。</p><p>希望对于样本$X^{(i)}$，其对应的嵌入向量$f^{(i)}$与其所属类中心向量$W_{y^{(i)}}$间夹角$\theta^{(i)}_{y^{(i)}}$愈小愈好，由余弦函数单调性，可得</p><script type="math/tex; mode=display">\min \theta^{(i)}_{y^{(i)}} \Rightarrow \max \cos \theta^{(i)}_{y^{(i)}}</script><p>引入Margin参数$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong>，也即</p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = \arccos (\cos \theta^{(i)}_{c_{y^{(i)}}} - m) > \theta^{(i)}_{c_{y^{(i)}}}</script><p>从而</p><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos \theta^{(i)}_{c_{y^{(i)}}} - m < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{7}</script><p>所以</p><script type="math/tex; mode=display">m > 0</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">\cos \theta^{(i)}_{c_{y^{(i)}}} - m = \cos \theta^{(i)}_{c_j}</script><p>那么$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{||f^{(i)}|| (\cos \theta^{(i)}_{y^{(i)}} - m)}}{e^{||f^{(i)}|| (\cos \theta^{(i)}_{y^{(i)}} - m)} + \sum_{k \neq y^{(i)}} e^{||f^{(i)}|| \cos \theta^{(i)}_k}}, 其中m > 0 \tag{8}</script><p>由于决策平面取决于角度，与$||f^{(i)}||$分布无关，故设置为常数$s$，有<code>Additive Cosine Margin Loss</code></p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s (\cos \theta^{(i)}_{y^{(i)}} - m)}}{e^{s (\cos \theta^{(i)}_{y^{(i)}} - m)} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m > 0 \tag{*2}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>$m$典型值为</p><script type="math/tex; mode=display">m = 0.35</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cosface.jpg" alt="cosface"></p><blockquote><p>实际上，$s$可控制决策平面的陡峭程度，如在二分类问题中，采用<code>Softmax</code>进行决策时</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_1 = \frac{e^{s f^{(i)}_1}}{e^{s f^{(i)}_1} + e^{s f^{(i)}_2}} = \frac{1}{1 + e^{-s (f^{(i)}_1 - f^{(i)}_2)}}</script><p>记$z^{(i)}_1 = f^{(i)}_1 - f^{(i)}_2$，有<code>Sigmoid</code>函数</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_1 = \frac{1}{1 + e^{-s z^{(i)}_1}}</script><p>设置不同的$s$参数，得到图像如下<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/parameter_s.png" alt="parameter_s"></p><p>从另一角度理解，可设$s = \frac{1}{\sigma^2}$，则$s$也可用于描述分布的方差，本文中，这里指角度的分布方差。</p></blockquote><h2 id="SphereFace-Multiplicative-Angular-Margin"><a href="#SphereFace-Multiplicative-Angular-Margin" class="headerlink" title="SphereFace: Multiplicative Angular Margin"></a>SphereFace: Multiplicative Angular Margin</h2><p>与<a href="#CosFace-Additive-Cosine-Margin">CosFace: Additive Cosine Margin</a>思路一致，在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin，不同的是对角度增加乘法裕度。</p><p>引入乘子$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong></p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = m \theta^{(i)}_{y^{(i)}}</script><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos m \theta^{(i)}_{y^{(i)}} < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{9}</script><p>所以$m &gt; 1$，原文中去整数，即$m \geq 2$，同时，沿用<code>CosFace</code>中对$||f^{(i)}||$处理，$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos m \theta^{(i)}_{y^{(i)}}}}{e^{s \cos m \theta^{(i)}_{y^{(i)}}} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m(\text{integer}) \geq 2 \tag{*3}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">m \theta^{(i)}_{c_{y^{(i)}}} = \theta^{(i)}_{c_j}</script><p>$m$典型值为</p><script type="math/tex; mode=display">m = 2</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/sphereface.jpg" alt="sphereface"></p><h2 id="ArcFace-Additive-Angular-Margin"><a href="#ArcFace-Additive-Angular-Margin" class="headerlink" title="ArcFace: Additive Angular Margin"></a>ArcFace: Additive Angular Margin</h2><p>与<a href="#CosFace-Additive-Cosine-Margin">CosFace: Additive Cosine Margin</a>思路一致，在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin，对角度增加加法裕度。</p><p>引入$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong></p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = \theta^{(i)}_{y^{(i)}} + m</script><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos (\theta^{(i)}_{y^{(i)}} + m) < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{9}</script><p>所以$m &gt; 0$，同时，沿用<code>CosFace</code>中对$||f^{(i)}||$处理，$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos (\theta^{(i)}_{y^{(i)}} + m)}}{e^{s \cos (\theta^{(i)}_{y^{(i)}} + m)} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m > 0 \tag{*4}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>$m$典型值为</p><script type="math/tex; mode=display">m = 0.5</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">\theta^{(i)}_{c_{y^{(i)}}} + m = \theta^{(i)}_{c_j}</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arcface.jpg" alt="arcface"></p><h1 id="可改进之处"><a href="#可改进之处" class="headerlink" title="可改进之处"></a>可改进之处</h1><h2 id="CosMulFace"><a href="#CosMulFace" class="headerlink" title="CosMulFace?"></a>CosMulFace?</h2><p>好像还有一个空余的位置可以添加参数，笑:-)。</p><p>与<a href="#CosFace-Additive-Cosine-Margin">CosFace: Additive Cosine Margin</a>思路一致，在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin，不同的是对余弦值增加乘法裕度。</p><p>引入乘子$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong></p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = \arccos (m \cos \theta^{(i)}_{c_{y^{(i)}}}) > \theta^{(i)}_{c_{y^{(i)}}}</script><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = m \cos \theta^{(i)}_{c_{y^{(i)}}} < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{10}</script><p>由于函数<code>cos(x)</code>有</p><script type="math/tex; mode=display">\begin{cases}    \cos x \geq 0 & x \in [-\frac{\pi}{2} + 2k\pi, \frac{\pi}{2} + 2k\pi] \\    \cos x \leq 0 & x \in [\frac{\pi}{2} + 2k\pi, \frac{3\pi}{2} + 2k\pi]\end{cases}, 且 -1 \leq \cos x \leq 1</script><p>所以为使得对于乘子$m$，均有式$(9)$成立，令</p><script type="math/tex; mode=display">\cos \theta^{(i)}_j := \cos \theta^{(i)}_j - 1 \tag{11}</script><p>所以$m &gt; 1$，同时，沿用<code>CosFace</code>中对$||f^{(i)}||$处理，$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s m (\cos \theta^{(i)}_{y^{(i)}} - 1)}}{e^{s m (\cos \theta^{(i)}_{y^{(i)}} - 1)} + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}}, 其中 m > 1 \tag{*5}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">m (\cos \theta^{(i)}_{c_{y^{(i)}}} - 1) = \cos \theta^{(i)}_{c_j} - 1 \tag{12}</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cosmulface.jpg" alt="cosmulface"></p><h2 id="AdaptiveFace-Adaptive-Margin"><a href="#AdaptiveFace-Adaptive-Margin" class="headerlink" title="AdaptiveFace: Adaptive Margin?"></a>AdaptiveFace: Adaptive Margin?</h2><p>Margin必须为固定常数？</p><p>设置可变参数$m_i, i = 1, \cdots, 4$</p><p>可令</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cdot m^4 \left[\cos (m^1 \theta^{(i)} + m^2) - m^3 - 1\right] }}{e^{s \cdot m^4 \left[\cos (m^1 \theta^{(i)} + m^2) - m^3 - 1\right] } + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}} \tag{*6}</script><script type="math/tex; mode=display">\begin{cases}    \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\    m_1 > 1 \\    m_2 > 0 \\    m_3 > 0 \\    m_4 > 1\end{cases}</script><p>反向传播时，同时更新$m_i, i = 1, \cdots, 4$。</p><p>注意点：参数下限，每次参数更新后，需对其限制，即剪裁。</p><script type="math/tex; mode=display">f(x) = \begin{cases}    \text{min} & x < \text{min} \\    x          & otherwise \\    \text{max} & x > \text{max}\end{cases}</script><h2 id="船新版本"><a href="#船新版本" class="headerlink" title="船新版本"></a>船新版本</h2><!-- 先保密hhh，见[实验3](#3-%E4%BC%A0%E6%96%B0%E7%89%88%E6%9C%AC)。 --><p>在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上，增加损失项</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos \theta^{(i)}_{y^{(i)}} }}{e^{s \cos \theta^{(i)}_{y^{(i)}} } + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}} + \lambda || \sum_j W_j ||_2 \tag{*7}</script><script type="math/tex; mode=display">s.t.\begin{cases}    \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\    || W_j ||_2 = 1 \\\end{cases}</script><p>即使得各类别所属向量的<strong>矢量和</strong>长度趋向$0$，由于最小化损失项$|| \sum_j W_j ||_2$，其最优解为</p><script type="math/tex; mode=display">W_j = \vec{0}</script><p>或者说$W_j$长度衰减至$0$，所以需要加上限制，<strong>在每次参数迭代后，重新归一化该向量，使其为单位向量</strong>，即</p><script type="math/tex; mode=display">W_j := \frac{W_j}{||W_j||}</script><h1 id="CosFace-SphereFace-ArcFace-CosMulFace"><a href="#CosFace-SphereFace-ArcFace-CosMulFace" class="headerlink" title="CosFace + SphereFace + ArcFace + CosMulFace"></a>CosFace + SphereFace + ArcFace + CosMulFace</h1><p>综合<a href="#CosFace-Additive-Cosine-Margin">CosFace</a>, <a href="#SphereFace-Multiplicative-Angular-Margin">SphereFace</a>, <a href="#ArcFace-Additive-Angular-Margin">ArcFace</a>, <a href="#CosMulFace">CosMulFace</a>, 得到</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cdot m_4 \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 - 1 \right] }}{e^{s \cdot m_4 \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 - 1 \right] } + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}} \tag{**}</script><script type="math/tex; mode=display">s.t. \qquad\begin{cases}    \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\    m_1 \geq 2 \\    m_2 > 0 \\    m_3 > 0 \\    m_4 > 1\end{cases}</script><p>通过组合可能获得更好的实验结果。</p><h1 id="方法对比"><a href="#方法对比" class="headerlink" title="方法对比"></a>方法对比</h1><p><code>ArcFace</code>与<code>CosFace</code>, <code>SphereFace</code>对比有如下特点</p><ol><li><p>数值计算稳定<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arcface1.jpg" alt="arcface1"></p></li><li><p>几何意义不同，决策平面区分度更好<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arcface2.jpg" alt="arcface2"></p></li></ol><h1 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h1><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/process.jpg" alt="process"></p><ol><li>初始化各类中心，记作矩阵$W_{C \times D}$，其中$C$表示类别数目， $D$表示特征维数；</li><li>迭代过程中，某批输入的样本记作$X_N$，其中$N$表示批次数据数；</li><li>计算该批中各样本$X^{(i)}$提取的特征$f^{(i)}$到各类别中心$W_j$的余弦值$\cos \theta^{(i)}_j$，保存为矩阵$Cos_{N \times C}$；</li><li><p>各样本真实标签对应的余弦值加上相应Margin，即</p><script type="math/tex; mode=display"> \cos \phi^{(i)}_j = \begin{cases}     m_4 (\text{monocos} (m_1 \theta^{(i)}_j + m_2) - m_3 - 1) & j = y^{(i)} \\     \cos \theta^{(i)}_j - 1 & \text{otherwise} \end{cases} \tag{14}</script><blockquote><p><code>monocos</code>在<a href="#cos%E5%87%BD%E6%95%B0%E7%9A%84%E5%8D%95%E8%B0%83%E6%80%A7%E9%97%AE%E9%A2%98">cos函数的单调性问题</a>中说明</p></blockquote></li><li><p>将计算得到的矩阵代入<code>Softmax</code>层，计算该样本属于各类别的概率</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_j = \frac{e^{\cos \phi^{(i)}_j}}{\sum_k e^{\cos \phi^{(i)}_k}} \tag{15}</script></li><li><p>代入<code>Cross Entropy</code>计算损失值</p><script type="math/tex; mode=display">L^{(i)} = - \log \tilde{y}^{(i)}_{y^{(i)}} \tag{16}</script></li><li><p>则该批次的损失值为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i L^{(i)} \tag{17}</script></li></ol><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>详情查看<a href="https://github.com/isLouisHsu/Toy-Problem-based-on-MNIST" target="_blank" rel="noopener">isLouisHsu/Toy-Problem-based-on-MNIST - Github</a>。</p><p>在实现过程中，有两个注意点：</p><ul><li>反余弦函数<code>arccos(x)</code>在$x = \pm 1$处不可导问题；</li><li>余弦函数<code>cos(x)</code>的单调性问题；</li></ul><p>对于上述两个问题，进行以下处理</p><h2 id="arccos函数不可导点问题"><a href="#arccos函数不可导点问题" class="headerlink" title="arccos函数不可导点问题"></a>arccos函数不可导点问题</h2><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arccos.png" alt="arccos"></p><p>由于函数$\arccos(x)$的导函数为</p><script type="math/tex; mode=display">\frac{d}{dx} \arccos(x) = - \frac{1}{\sqrt{1 - x^2}} \tag{18}</script><p>特征$f^{(i)}$与各类中心$W_j$余弦值范围为</p><script type="math/tex; mode=display">\cos \theta^{(i)}_j \in [-1, 1]</script><p>则当$\cos \theta^{(i)}_j = \pm 1$时</p><script type="math/tex; mode=display">\lim_{\theta \rightarrow \pm 1} \frac{d}{dx} \arccos(x) = \infty</script><p>则无法使用BP算法进行参数更新，因此，使用泰勒展开式近似计算$\theta^_j$</p><script type="math/tex; mode=display">\arccos x = \frac{\pi}{2} - x - \frac{1}{2} \cdot \frac{x^3}{3} - \cdots - \frac{(2n-1)!}{(2n)!} \cdot \frac{x^{2n+1}}{2n+1}- \cdots \tag{19}</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/taylor_arccos,_n_=_5.png" alt="taylor_arccos,_n_=_5"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">taylorArccos</span><span class="params">(x, n)</span>:</span></span><br><span class="line"></span><br><span class="line">    general_term = <span class="keyword">lambda</span> x, n: (math.factorial(<span class="number">2</span> * n - <span class="number">1</span>) /\</span><br><span class="line">                                math.factorial(<span class="number">2</span> * n)) *\</span><br><span class="line">                                (x**(<span class="number">2</span> * n + <span class="number">1</span>) / (<span class="number">2</span> * n + <span class="number">1</span>))</span><br><span class="line">    y = np.pi / <span class="number">2</span> - x</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        y -= general_term(x, i)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><h2 id="cos函数的单调性问题"><a href="#cos函数的单调性问题" class="headerlink" title="cos函数的单调性问题"></a>cos函数的单调性问题</h2><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cos.png" alt="cos"></p><p>由于特征$f^{(i)}$与各类中心$W_j$余弦值范围为</p><script type="math/tex; mode=display">\cos \theta^{(i)}_j \in [-1, 1]</script><p>利用反三角函数<code>arccos(x)</code>计算其角度后，有</p><script type="math/tex; mode=display">\theta^{(i)}_j \in [0, \pi]</script><p>仅考虑余弦函数内部部分，添加Margin后，应有</p><script type="math/tex; mode=display">\phi^{(i)}_j = m_1 \theta^{(i)}_j + m_2 \in [m_2, m_2 + m_1 \pi]</script><p>此时仍然满足</p><script type="math/tex; mode=display">\phi^{(i)}_j > \theta^{(i)}_j</script><p>然而经过余弦函数后，可能由于其单调性问题，<strong>不一定满足下式</strong></p><script type="math/tex; mode=display">\cos \phi^{(i)}_j < \cos \theta^{(i)}_j</script><p>所以定义如下函数，使其在$[0, \infty]$单调递减</p><script type="math/tex; mode=display">\text{monocos}(\theta) = \cos (\theta - n \pi) - 2n 其中 n = \lfloor{} \frac{\theta}{\pi} \rfloor{} \tag{19}</script><p>此时必满足</p><script type="math/tex; mode=display">\text{monocos} \phi^{(i)}_j < \text{monocos} \theta^{(i)}_j</script><blockquote><p>由于$\phi^{(i)}_j &gt; \theta^{(i)}_j$，所以满足$[0, \infty]$即可，实际上，该函数在$[-\infty, \infty]$均单调递减。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">monocos</span><span class="params">(x)</span>:</span></span><br><span class="line">    n = x // np.pi</span><br><span class="line">    y = np.cos(x - np.pi*n) - <span class="number">2</span>*n</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>其函数图像如下<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cos_&amp;_monocos.png" alt="cos_&amp;_monocos"></p><h2 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        weight: &#123;Parameter(num_classes, feature_size)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, feature_size)</span>:</span></span><br><span class="line">        super(CosineLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.weights = Parameter(torch.Tensor(num_classes, feature_size))</span><br><span class="line">        nn.init.xavier_uniform_(self.weights)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            x: &#123;tensor(N, feature_size)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            \cos \theta^&#123;(i)&#125;_j = \frac&#123;W_j^T f^&#123;(i)&#125;&#125;&#123;||W_j|| ||f^&#123;(i)&#125;||&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = F.linear(F.normalize(x), F.normalize(self.weights))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NetworkMargin</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, feature_size)</span>:</span></span><br><span class="line">        super(NetworkMargin, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.pre_layers = nn.Sequential(</span><br><span class="line">            nn.Conv2d(   <span class="number">1</span>,  <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(  <span class="number">64</span>,  <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d( <span class="number">64</span>,  feature_size, <span class="number">7</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.cosine_layer = CosineLayer(num_classes, feature_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        x = self.pre_layers(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line">        x = self.get_feature(x)</span><br><span class="line">        x = self.cosine_layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  x</span><br></pre></td></tr></table></figure><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarginProduct</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string">        \text&#123;softmax&#125; = \frac&#123;1&#125;&#123;N&#125; \sum_i -\log \frac&#123;e^&#123;\tilde&#123;y&#125;_&#123;y_i&#125;&#125;&#125;&#123;\sum_i e^&#123;\tilde&#123;y&#125;_i&#125;&#125;</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        $\text&#123;where&#125;$</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string">        \tilde&#123;y&#125; = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">            s(m4 \cos(m_1 \theta_&#123;j, i&#125; + m_2) + m_3) &amp; j = y_i \\</span></span><br><span class="line"><span class="string">            s(m4 \cos(    \theta_&#123;j, i&#125;))             &amp; j \neq y_i</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, s=<span class="number">32.0</span>, m1=<span class="number">2.00</span>, m2=<span class="number">0.50</span>, m3=<span class="number">0.35</span>, m4=<span class="number">2.00</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(MarginProduct, self).__init__()</span><br><span class="line">        self.s = s</span><br><span class="line">        self.m1 = m1</span><br><span class="line">        self.m2 = m2</span><br><span class="line">        self.m3 = m3</span><br><span class="line">        self.m4 = m4</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, cosTheta, label)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            cosTheta: &#123;tensor(N, n_classes)&#125; 每个样本(N)，到各类别(n_classes)矢量的余弦值</span></span><br><span class="line"><span class="string">            label:  &#123;tensor(N)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output: &#123;tensor(N, n_classes)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        one_hot = torch.zeros(cosTheta.size(), device=<span class="string">'cuda'</span> <span class="keyword">if</span> \</span><br><span class="line">                        torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">        one_hot.scatter_(<span class="number">1</span>, label.view(<span class="number">-1</span>, <span class="number">1</span>).long(), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># theta  = torch.acos(cosTheta)</span></span><br><span class="line">        theta  = arccos(cosTheta)</span><br><span class="line">        cosPhi = self.m4 * (monocos(self.m1*theta + self.m2) - self.m3 - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        output = torch.where(one_hot &gt; <span class="number">0</span>, cosPhi, cosTheta - <span class="number">1</span>)</span><br><span class="line">        output = self.s * output</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarginLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, s=<span class="number">32.0</span>, m1=<span class="number">2.00</span>, m2=<span class="number">0.50</span>, m3=<span class="number">0.35</span>, m4=<span class="number">2.00</span>)</span>:</span></span><br><span class="line">        super(MarginLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.margin = MarginProduct(s, m1, m2, m3, m4)</span><br><span class="line">        self.crossent = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, gt)</span>:</span></span><br><span class="line"></span><br><span class="line">        output = self.margin(pred, gt)</span><br><span class="line">        loss   = self.crossent(output, gt)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><strong>注意，以下实验网络的参数初始均相同。</strong></p><h2 id="1-设置不同参数，对比实验结果"><a href="#1-设置不同参数，对比实验结果" class="headerlink" title="1.设置不同参数，对比实验结果"></a>1.设置不同参数，对比实验结果</h2><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cdot \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 \right] }}{e^{s \cdot \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 \right] } + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}} \tag{**}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><div class="table-container"><table><thead><tr><th>Margin type</th><th>Modified</th><th>CosFace</th><th>SphereFace</th><th>ArcFace</th><th>ArcFace</th><th>ArcFace</th><th>ArcFace</th></tr></thead><tbody><tr><td>$s$</td><td>8</td><td>8</td><td>8</td><td><strong>16</strong></td><td><strong>8</strong></td><td><strong>4</strong></td><td><strong>1</strong></td></tr><tr><td>$m_1(m_1&gt;=2)$</td><td>1</td><td>1</td><td><strong>2</strong></td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>$m_2(m_2&gt;0)$</td><td>0</td><td>0</td><td>0</td><td><strong>0.5</strong></td><td><strong>0.5</strong></td><td><strong>0.5</strong></td><td><strong>0.5</strong></td></tr><tr><td>$m_3(m_3&gt;0)$</td><td>0</td><td><strong>0.35</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div><ol><li>参数$s$对实验结果影响不大；</li><li><code>CosFace</code>, <code>SphereFace</code>, <code>ArcFace</code>三种损失作用下，角度的区分度均比<code>Modified Softmax</code>效果好。</li></ol><h3 id="嵌入向量维度为2"><a href="#嵌入向量维度为2" class="headerlink" title="嵌入向量维度为2"></a>嵌入向量维度为2</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp1_dim2.png" alt="exp1_dim2"></p><h3 id="嵌入向量维度为3"><a href="#嵌入向量维度为3" class="headerlink" title="嵌入向量维度为3"></a>嵌入向量维度为3</h3><blockquote><p>使用<code>imageio.mimread()</code>函数，读取出<code>.gif</code>会改变原图颜色，很迷。</p></blockquote><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp1_dim3.gif" alt="exp1_dim3"></p><h2 id="2-改进的方法实验"><a href="#2-改进的方法实验" class="headerlink" title="2. 改进的方法实验"></a>2. 改进的方法实验</h2><div class="table-container"><table><thead><tr><th>Margin type</th><th>CosMulFace</th><th>CosMulFace</th><th>CosMulFace</th><th>AdaptiveFace</th></tr></thead><tbody><tr><td>$s$</td><td>8</td><td>8</td><td>8</td><td>8</td></tr><tr><td>$m_1(m_1&gt;=2)$</td><td>1</td><td>1</td><td>1</td><td>/</td></tr><tr><td>$m_2(m_2&gt;0)$</td><td>0</td><td>0</td><td>0</td><td>/</td></tr><tr><td>$m_3(m_3&gt;0)$</td><td>0</td><td>0</td><td>0</td><td>/</td></tr><tr><td>$m_4(m_4 &gt; 1)$</td><td><strong>2.00</strong></td><td><strong>3.00</strong></td><td><strong>4.00</strong></td><td>/</td></tr></tbody></table></div><ul><li><code>CosMulFace</code>在$m=2, 4$时效果良好。</li><li><code>AdaptiveFace</code>得到自适应参数如下(?)<script type="math/tex; mode=display">  \begin{cases}      m_1 = 1 \\      m_2 = 0 \\      m_3 = 0 \\      m_4 = 6.8345 \\  \end{cases}</script></li></ul><h3 id="嵌入向量维度为2-1"><a href="#嵌入向量维度为2-1" class="headerlink" title="嵌入向量维度为2"></a>嵌入向量维度为2</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp2_dim2.png" alt="exp2_dim2"></p><h3 id="嵌入向量维度为3-1"><a href="#嵌入向量维度为3-1" class="headerlink" title="嵌入向量维度为3"></a>嵌入向量维度为3</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp2_dim3.gif" alt="exp2_dim3"></p><h2 id="3-传新版本"><a href="#3-传新版本" class="headerlink" title="3. 传新版本"></a>3. 传新版本</h2><p>可见其角度分布更加均匀，从而区分度更大。</p><h3 id="嵌入向量维度为2-2"><a href="#嵌入向量维度为2-2" class="headerlink" title="嵌入向量维度为2"></a>嵌入向量维度为2</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp3_dim2.png" alt="exp3_dim2"></p><h3 id="嵌入向量维度为3-2"><a href="#嵌入向量维度为3-2" class="headerlink" title="嵌入向量维度为3"></a>嵌入向量维度为3</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp3_dim3.gif" alt="exp3_dim3"></p><p>上图和#$%@%一样，如下为<code>scatter_lda8</code>原图</p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda8.gif" alt="scatter_lda8"></p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda8_spherized.gif" alt="scatter_lda8_spherized"></p><p>数据类别过多可能不够明显，现选择4类，在设置$\lambda=0$与$\lambda=16$时，各类别的三维分布如下两图，区别很明显</p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda0_c4_.gif" alt="scatter_lda0_c4_"></p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda16_c4_.gif" alt="scatter_lda16_c4_"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1801.07698v1" target="_blank" rel="noopener">ArcFace: Additive Angular Margin Loss for Deep Face Recognition - arxiv.org</a></li><li><a href="https://arxiv.org/abs/1801.09414" target="_blank" rel="noopener">CosFace: Large Margin Cosine Loss for Deep Face Recognition - arxiv.org</a></li><li><a href="https://arxiv.org/abs/1704.08063" target="_blank" rel="noopener">SphereFace: Deep Hypersphere Embedding for Face Recognition - arxiv.org</a></li><li><a href="https://github.com/deepinsight/insightface" target="_blank" rel="noopener">deepinsight/insightface - Github</a></li><li><a href="https://github.com/wy1iu/sphereface" target="_blank" rel="noopener">wy1iu/sphereface - Github</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python迭代器与生成器</title>
      <link href="/2019/07/11/Python%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/"/>
      <url>/2019/07/11/Python%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在读取大数据量的文件时，使用迭代器和生成器可减少内存开销。</p><h1 id="迭代器-Iterator"><a href="#迭代器-Iterator" class="headerlink" title="迭代器(Iterator)"></a>迭代器(Iterator)</h1><p>迭代是Python访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。</p><h2 id="内建函数iter-与next"><a href="#内建函数iter-与next" class="headerlink" title="内建函数iter()与next()"></a>内建函数iter()与next()</h2><p>该函数将可迭代对象转换为迭代器，支持的数据容器对象如<code>string</code>、<code>list</code>、<code>dict</code>、<code>tuple</code>等，使用方法如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 创建可迭代对象</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; a = "iterator"</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; b = [_ for _ in a]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; c = dict(zip(a, range(len(a))))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; d = tuple(a)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; a</span><br><span class="line">'iterator'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; b</span><br><span class="line">['i', 't', 'e', 'r', 'a', 't', 'o', 'r']</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; c</span><br><span class="line">&#123;'i': 0, 't': 5, 'e': 2, 'r': 7, 'a': 4, 'o': 6&#125;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; d</span><br><span class="line">('i', 't', 'e', 'r', 'a', 't', 'o', 'r')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 调用`iter()`函数将可迭代对象转换为迭代器</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; all = [a, b, c, d]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; iterAll = list(map(iter, all))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; all</span><br><span class="line">['iterator', ['i', 't', 'e', 'r', 'a', 't', 'o', 'r'], &#123;'i': 0, 't': 5, 'e': 2, 'r': 7, 'a': 4, 'o': 6&#125;, ('i', 't', 'e', 'r', 'a', 't', 'o', 'r')]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; iterAll</span><br><span class="line">[&lt;str_iterator object at 0x00000216C399D6A0&gt;, &lt;list_iterator object at 0x00000216C399D6D8&gt;, &lt;dict_keyiterator object at 0x00000216C37672C8&gt;, &lt;tuple_iterator object at 0x00000216C399D710&gt;]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 反复调用`next()`函数取出迭代器值</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'i'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'t'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'e'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'r'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'a'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'t'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'o'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'r'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 迭代结束</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure></p><h2 id="for语法糖"><a href="#for语法糖" class="headerlink" title="for语法糖"></a>for语法糖</h2><p>实际中对可迭代对象进行迭代时，上述这般麻烦。在Python的循环语句<code>for</code>已对其进行包装，内部调用函数<code>iter()</code>和<code>next()</code>，返回可迭代对象元素<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; a</span><br><span class="line">'iterator'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for i in a:</span><br><span class="line">...     print(i)</span><br><span class="line">...</span><br><span class="line">i</span><br><span class="line">t</span><br><span class="line">e</span><br><span class="line">r</span><br><span class="line">a</span><br><span class="line">t</span><br><span class="line">o</span><br><span class="line">r</span><br></pre></td></tr></table></figure></p><h2 id="自定义迭代器-敲黑板划重点！"><a href="#自定义迭代器-敲黑板划重点！" class="headerlink" title="自定义迭代器(敲黑板划重点！)"></a>自定义迭代器(敲黑板划重点！)</h2><p>将自定义类定义为可迭代对象的实现方法是，实现魔术方法<code>__iter__()</code>与<code>__next__()</code>。</p><p>例如，要求返回斐波那契数列前20个值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fibonacci</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, m)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.m = m</span><br><span class="line">        self.n, self.a, self.b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.n &lt; self.m:</span><br><span class="line">            t = self.b</span><br><span class="line">            self.a, self.b = self.b, self.a + self.b</span><br><span class="line">            self.n += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> t</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br></pre></td></tr></table></figure></p><p>在命令行中执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from iter import Fibonacci</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g = Fibonacci(20)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g</span><br><span class="line">&lt;iter.Fibonacci object at 0x000001F359927E10&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; while True:</span><br><span class="line">...     try:</span><br><span class="line">...             next(g)</span><br><span class="line">...     except StopIteration:</span><br><span class="line">...             break</span><br><span class="line">...</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">5</span><br><span class="line">8</span><br><span class="line">13</span><br><span class="line">21</span><br><span class="line">34</span><br><span class="line">55</span><br><span class="line">89</span><br><span class="line">144</span><br><span class="line">233</span><br><span class="line">377</span><br><span class="line">610</span><br><span class="line">987</span><br><span class="line">1597</span><br><span class="line">2584</span><br><span class="line">4181</span><br><span class="line">6765</span><br></pre></td></tr></table></figure></p><blockquote><p>在<code>Pytorch</code>中，数据集<code>Dataset</code>定义时，重写函数<code>__getitem__()</code>与<code>__len__()</code>，并不是可迭代对象，而<code>Dataloader</code>为可迭代对象，详细源码可查看<a href="https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py/" target="_blank" rel="noopener">Github: Pytorch</a></p></blockquote><h1 id="生成器-Generator"><a href="#生成器-Generator" class="headerlink" title="生成器(Generator)"></a>生成器(Generator)</h1><p>调用一个生成器函数，返回的是一个迭代器对象。在调用生成器运行的过程中，每次遇到<code>yield</code>时函数会暂停并保存当前所有的运行信息，返回<code>yield</code>的值, 并在下一次执行<code>next()</code>方法时从当前位置继续运行。</p><h2 id="推导式定义生成器"><a href="#推导式定义生成器" class="headerlink" title="推导式定义生成器"></a>推导式定义生成器</h2><p>语法类似列表推导式，不同的是将<code>[]</code>改为<code>()</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; g = (i for i in range(20))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g</span><br><span class="line">&lt;generator object &lt;genexpr&gt; at 0x00000161D16F44F8&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">0</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">2</span><br></pre></td></tr></table></figure></p><h2 id="将函数定义为生成器"><a href="#将函数定义为生成器" class="headerlink" title="将函数定义为生成器"></a>将函数定义为生成器</h2><p>利用函数打印斐波那契数列<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(m)</span>:</span></span><br><span class="line">    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; m:</span><br><span class="line">        print(b)</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        n = n + <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>若需得到生成器，将<code>print()</code>改为<code>yield()</code>即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci_gen</span><span class="params">(m)</span>:</span></span><br><span class="line">    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; m:</span><br><span class="line">        <span class="keyword">yield</span>(b)</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        n = n + <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>在命令行中执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from iter import fibonacci_gen</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g = fibonacci_gen(20)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g</span><br><span class="line">&lt;generator object fibonacci_gen at 0x000001F036DA44F8&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">2</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">3</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">5</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.runoob.com/python3/python3-iterator-generator.html" target="_blank" rel="noopener">Python3 迭代器与生成器 - 菜鸟教程</a></li><li><a href="https://www.cnblogs.com/tkqasn/p/5984090.html" target="_blank" rel="noopener">python 迭代器和生成器详解 - 博客园</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WSL - Windows Subsystem for Linux</title>
      <link href="/2019/07/11/WSL-Windows-Subsystem-for-Linux/"/>
      <url>/2019/07/11/WSL-Windows-Subsystem-for-Linux/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Windows远程登陆服务器，需要借助<a href="https://xshell.en.softonic.com/" target="_blank" rel="noopener">xshell</a>等软件。其实安装完子系统后，即可使用<code>ssh</code>登录，并且子系统与原系统隔离较好，不会产生影响。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ol><li><p>呼叫小娜，打开“启用或关闭Windows功能”，勾选“适用于Linux的Windows子系统”，并重启；<br> <img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl1.jpg" alt="wsl1"><br> <img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl2.jpg" alt="wsl2"></p></li><li><p>在巨硬软件商城(Microsoft Store)中下载合适的WSL，选择喜好的WSL进行安装</p></li></ol><ul><li><a href="https://www.microsoft.com/zh-cn/p/ubuntu-1604-lts/9pjn388hp8c9?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Ubuntu 16.04 LTS</a></li><li><a href="https://www.microsoft.com/zh-cn/p/ubuntu-1804-lts/9n9tngvndl3q?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Ubuntu 18.04 LTS</a></li><li><a href="https://www.microsoft.com/zh-cn/p/opensuse-leap-15/9n1tb6fpvj8c?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">OpenSUSE Leap 15</a></li><li><a href="https://www.microsoft.com/zh-cn/p/opensuse-leap-42/9njvjts82tjx?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">OpenSUSE Leap 42</a></li><li><a href="https://www.microsoft.com/zh-cn/p/suse-linux-enterprise-server-12/9p32mwbh6cns?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">SUSE Linux Enterprise Server 12</a></li><li><a href="https://www.microsoft.com/zh-cn/p/suse-linux-enterprise-server-15/9pmw35d7fnlx?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">SUSE Linux Enterprise Server 15</a></li><li><a href="https://www.microsoft.com/zh-cn/p/kali-linux/9pkr34tncv07?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Kali Linux</a></li><li><a href="https://www.microsoft.com/zh-cn/p/debian/9msvkqc78pk6?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Debian GNU/Linux</a></li><li><a href="https://www.microsoft.com/zh-cn/p/fedora-remix-for-wsl/9n6gdm4k2hnc?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Fedora Remix for WSL</a></li><li><a href="https://www.microsoft.com/zh-cn/p/pengwin/9nv1gv1pxz6p?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Pengwin</a></li><li><a href="https://www.microsoft.com/zh-cn/p/pengwin-enterprise/9n8lp0x93vcp?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Pengwin Enterprise</a></li><li><a href="https://www.microsoft.com/zh-cn/p/alpine-wsl/9p804crf0395?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Alpine WSL</a></li></ul><ol><li>启动WSL，并添加用户和密码，即可使用<br><img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl3.jpg" alt="wsl3"><br><img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl4.jpg" alt="wsl4"></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10" target="_blank" rel="noopener">Windows Subsystem for Linux Installation Guide for Windows 10 - Microsoft</a></li><li><a href="https://linux.cn/article-9545-1.html" target="_blank" rel="noopener">如何在Windows 10上开启WSL之旅</a></li><li><a href="https://www.cnblogs.com/skyshalo/p/7724072.html" target="_blank" rel="noopener">关于WSL(Windows上的Linux子系统)的简单介绍及安装 - CNBLOG</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Useful Terminal Control Sequences</title>
      <link href="/2019/05/28/Useful-Terminal-Control-Sequences/"/>
      <url>/2019/05/28/Useful-Terminal-Control-Sequences/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>ANSI</code>定义了用于屏幕显示的<code>Escape</code>屏幕控制码，打印输出到终端时，可指定输出颜色、格式等。</p><h1 id="基本格式"><a href="#基本格式" class="headerlink" title="基本格式"></a>基本格式</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\033[&lt;background color&gt;;&lt;front color&gt;m string to print \033[0m</span><br></pre></td></tr></table></figure><ul><li><code>\033[ xxxx m</code>为一个句段；</li><li><code>\033[0m</code>关闭所有属性；</li></ul><h1 id="光标控制"><a href="#光标控制" class="headerlink" title="光标控制"></a>光标控制</h1><div class="table-container"><table><thead><tr><th>ANSI控制码</th><th>含义</th></tr></thead><tbody><tr><td>\033[nA</td><td>光标上移n行</td></tr><tr><td>\033[nB</td><td>光标下移n行</td></tr><tr><td>\033[nC</td><td>光标右移n行</td></tr><tr><td>\033[nD</td><td>光标左移n行</td></tr><tr><td>\033[y;xH</td><td>设置光标位置</td></tr><tr><td>\033[2J</td><td>清屏</td></tr><tr><td>\033[K</td><td>清除从光标到行尾的内容</td></tr><tr><td>\033[s</td><td>保存光标位置</td></tr><tr><td>\033[u</td><td>恢复光标位置</td></tr><tr><td>\033[?25l</td><td>隐藏光标</td></tr><tr><td>\033[?25h</td><td>显示光标</td></tr></tbody></table></div><h1 id="颜色控制"><a href="#颜色控制" class="headerlink" title="颜色控制"></a>颜色控制</h1><div class="table-container"><table><thead><tr><th>ANSI控制码</th><th>含义</th></tr></thead><tbody><tr><td>\033[m</td><td>NONE</td></tr><tr><td>\033[0;32;31m</td><td>RED</td></tr><tr><td>\033[1;31m</td><td>LIGHT RED</td></tr><tr><td>\033[0;32;32m</td><td>GREEN</td></tr><tr><td>\033[1;32m</td><td>LIGHT GREEN</td></tr><tr><td>\033[0;32;34m</td><td>BULE</td></tr><tr><td>\033[1;34m</td><td>LIGHT BLUE</td></tr><tr><td>\033[1;30m</td><td>GRAY</td></tr><tr><td>\033[0;36m</td><td>CYAN</td></tr><tr><td>\033[1;36m</td><td>LIGHT CYAN</td></tr><tr><td>\033[0;35m</td><td>PURPLE</td></tr><tr><td>\033[1;35m</td><td>LIAGHT PURPLE</td></tr><tr><td>\033[0;33m</td><td>BROWN</td></tr><tr><td>\033[1;33m</td><td>YELLO</td></tr><tr><td>\033[0;37m</td><td>LIGHT GRAY</td></tr><tr><td>\033[1;37m</td><td>WHITE</td></tr></tbody></table></div><p>背景色与字体颜色符号不同</p><div class="table-container"><table><thead><tr><th>背景色</th><th>字体色</th></tr></thead><tbody><tr><td>40: 黑</td><td>30: 黑</td></tr><tr><td>41: 红</td><td>31: 红</td></tr><tr><td>42: 绿</td><td>32: 绿</td></tr><tr><td>43: 黄</td><td>33: 黄</td></tr><tr><td>44: 蓝</td><td>34: 蓝</td></tr><tr><td>45: 紫</td><td>35: 紫</td></tr><tr><td>46: 深绿</td><td>36: 深绿</td></tr><tr><td>47: 白色</td><td>37: 白色</td></tr></tbody></table></div><h1 id="格式控制"><a href="#格式控制" class="headerlink" title="格式控制"></a>格式控制</h1><div class="table-container"><table><thead><tr><th>ANSI控制码</th><th>含义</th></tr></thead><tbody><tr><td>\033[0m</td><td>关闭所有属性</td></tr><tr><td>\033[1m</td><td>设置高亮度</td></tr><tr><td>\033[4m</td><td>下划线</td></tr><tr><td>\033[5m</td><td>闪烁</td></tr><tr><td>\033[7m</td><td>反显</td></tr><tr><td>\033[8m</td><td>消隐</td></tr></tbody></table></div><h1 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h1><p>例如用python打印输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"\007"</span>)                       <span class="comment"># 发出提示音</span></span><br><span class="line">print(<span class="string">"\033[42:31m hello! \033[0m"</span>) <span class="comment"># 绿底红字` hello! ` </span></span><br><span class="line">print(<span class="string">"\033[4m"</span>)                    <span class="comment"># 开启下划线</span></span><br><span class="line">print(<span class="string">"\033[42:31m hello! \033[0m"</span>) <span class="comment"># 下划线绿底红字` hello! ` </span></span><br><span class="line">print(<span class="string">"\033[0m"</span>)                    <span class="comment"># 关闭所有格式</span></span><br><span class="line">print(<span class="string">"\033[2J"</span>)                    <span class="comment"># 清屏</span></span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://blog.csdn.net/lzuacm/article/details/8993785" target="_blank" rel="noopener">“\\033”(ESC)的用法-ANSI的Esc屏幕控制 - CSDN</a></li><li><a href="https://www.student.cs.uwaterloo.ca/~cs452/terminal.html" target="_blank" rel="noopener">Useful Terminal Control Sequences - student.cs.uwaterloo.ca</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="/2019/05/27/Support-Vector-Machine/"/>
      <url>/2019/05/27/Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/p97_svm.py" target="_blank" rel="noopener">Github: isLouisHsu/Basic-Machine-Learning-Algorithm/algorithm/p97_svm.py</a></p><p>补一补支持向量机的笔记。</p><p>支持向量机<code>(SVM)</code>为有监督学习算法，可用于回归、分类甚至聚类(支持向量聚类)，其主要特点为</p><ul><li>将样本表示为超空间中的点；</li><li>求取支持向量，其余样本点对超平面无影响；</li><li>对于线性不可分问题，利用核函数映射到高维空间，使其线性可分；</li></ul><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>以下介绍支持向量机的基本原理，首先对一些概念作补充</p><h2 id="概念补充"><a href="#概念补充" class="headerlink" title="概念补充"></a>概念补充</h2><ol><li><p>$n$维空间与超平面<br> 如对于$n$维数据集，其所在空间即为$n$维欧式空间，则在该空间中，余维度为$1$的线性子空间，或称$n-1$维仿射子空间，称为超平面，为$n-1$维，可由下式确定</p><script type="math/tex; mode=display">w^T x + b = 0 \tag{1}</script><p> 其中$w,x$为$n$维列向量，$x$表示超平面上的点($n$维)，$w$表示超平面的法向量，决定超平面的方向，$b$为实数。</p><script type="math/tex; mode=display">x = (x_1, x_2, \cdots, x_n)^T</script><script type="math/tex; mode=display">w = (w_1, w_2, \cdots, w_n)^T</script><p> 例如$3$维空间中的$2$维平面方程为</p><script type="math/tex; mode=display">Ax + By + Cz + D = 0</script></li><li><p>点到超平面的距离<br> <img src="/2019/05/27/Support-Vector-Machine/distance.jpg" alt="distance"></p><p> 对于样本空间中任一点$x$，到超平面$\mathcal{P}_{w,b}$的距离，可表示为</p><script type="math/tex; mode=display">\mathcal{D} = \frac{|w^T x + b|}{||w||} \tag{2}</script><p> 其中$||w||$为向量$w$的$2$范数，即</p><script type="math/tex; mode=display">||w|| = \sqrt{w_1^2 + w_2^2 + \cdots + w_n^2} \tag{3}</script><blockquote><p>证明：假设超平面上一向量点为$x_0$，则向量$x-x_0$在单位法向量$\frac{w}{||w||}$上的投影$d$，即为向量点到超平面的距离</p><script type="math/tex; mode=display">d =  |\frac{w^T}{||w||} (x-x_0)|</script><p>而</p><script type="math/tex; mode=display">w^T x + b = 0 \Rightarrow w^T = -b</script><p>所以</p><script type="math/tex; mode=display">d =  | \frac{- w^T x_0 - b}{||w||} | = \frac{|w^T x_0 + b|}{||w||}</script></blockquote></li></ol><h2 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在前言中说到支持向量机使用核函数映射低维空间到高维空间，这是如何做到的？<br>假设有两个$n$维向量$x_i, x_j$，设有映射$\mathit{\Phi}$使其维度扩张到$n’$维，即</p><script type="math/tex; mode=display">x' = \mathit{\Phi}(x)</script><p>则定义核函数为</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = \mathit{\Phi}(x_i)^T \mathit{\Phi}(x_j) \tag{4}</script><h3 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h3><ol><li>线性<code>(linear)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = x_i^T x_j</script></li><li>多项式<code>(Polynomial)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = (\gamma x_i^T x_j + c)^n</script></li><li>高斯<code>(Gaussian)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}}</script></li><li>拉普拉斯<code>(Laplace)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||}{2\sigma}}</script></li><li>Sigmoid核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = \tanh (\gamma x_i^T x_j + c)</script></li></ol><p>等等。</p><p>至于如何选取核函数，需要技术人员一定的先验知识，或者使用超参数搜索确定。</p><h3 id="实例分析核函数的作用"><a href="#实例分析核函数的作用" class="headerlink" title="实例分析核函数的作用"></a>实例分析核函数的作用</h3><ol><li><p>多项式核函数<br> 指定多项式核函数参数$ \gamma = 1, c = 0, n = 2 $，即</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = (x_i^T x_j)^2 \tag{4.a}</script><p> 设原始空间为$2$维，即</p><script type="math/tex; mode=display">x_i = (x_{i1}, x_{i2})^T</script><p> 代入核函数$(4.a)$，有</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = \left[(x_{i1}, x_{i2}) (x_{j1}, x_{j2})^T \right]^2 = (x_{i1}x_{j1} + x_{i2}x_{j2})^2</script><script type="math/tex; mode=display">= x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2</script><p> 把以上$3$项多项式表示成$3$维向量内积，即</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = \left[ \begin{matrix} x_{i1}^2 & \sqrt{2}x_{i1}x_{i2} & x_{i2}^2 \end{matrix} \right]                          \left[ \begin{matrix} x_{j1}^2 & \sqrt{2}x_{j1}x_{j2} & x_{j2}^2 \end{matrix} \right]^T</script><p> 由上式与式$(4)$可得，该核函数对应的映射函数为</p><script type="math/tex; mode=display">\mathit{\Phi}(x) = \left[ \begin{matrix} x_1^2 & \sqrt{2}x_1x_2 & x_2^2 \end{matrix} \right]^T \tag{4.b}</script><p> 这样就把$2$维空间中的点映射到了$3$维空间，作图如下。<br> <img src="/2019/05/27/Support-Vector-Machine/kernel_poly.jpg" alt="kernel_poly"></p></li></ol><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X = np.array([</span><br><span class="line">[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">1</span>],[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">5</span>,<span class="number">2</span>],</span><br><span class="line">[<span class="number">6</span>,<span class="number">1</span>],[<span class="number">6</span>,<span class="number">2</span>],[<span class="number">6</span>,<span class="number">3</span>],[<span class="number">6</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">])</span><br><span class="line">Y=np.array([<span class="number">-1</span>] * <span class="number">14</span> + [<span class="number">1</span>] * <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">0</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据映射到高维后显示</span></span><br><span class="line"><span class="comment"># 映射函数为Φ(x)=[x1^2 √2*x1*x2 x2^2]</span></span><br><span class="line">FX = np.zeros((<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">    tmp = np.array([X[i, <span class="number">0</span>]**<span class="number">2</span>, np.sqrt(<span class="number">2</span>)*X[i, <span class="number">0</span>]*X[i, <span class="number">1</span>], X[i, <span class="number">1</span>]**<span class="number">2</span>]).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    FX = np.r_[FX, tmp]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(<span class="number">1</span>)</span><br><span class="line">figAx = Axes3D(fig)</span><br><span class="line">figAx.scatter(FX[:, <span class="number">0</span>], FX[:, <span class="number">1</span>], FX[:, <span class="number">2</span>], c=Y)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></code></pre><ol><li><p>高斯核函数</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}} = e^{c||x_i - x_j||^2} \tag{4.c}</script><p> 其中包含指数函数，其级数展开式为</p><script type="math/tex; mode=display">e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!} = \sum_{i=0}^{n} \frac{x^i}{i!} + R(n)</script><p> 因此可将样本点映射到无穷维度。</p><ul><li><p>输入维度为$1$</p><script type="math/tex; mode=display">||x_i - x_j||^2 = (x_i - x_j)^2 =x_i^2 - 2x_ix_j + x_j^2</script><p>  则代入核函数$(4.c)$得到</p><script type="math/tex; mode=display">e^{c||x_i - x_j||^2}   = e^{c(x_i^2 - 2x_ix_j + x_j^2)}  = e^{c(x_i^2 + x_j^2)} e^{-2c x_i x_j}</script><p>  其中</p><script type="math/tex; mode=display">  e^{-2c x_i x_j} = \sum_{n=0}^{\infty} \frac{(-2c)^n x_i^n x_j^n}{n!} = 1 - 2cx_ix_j + \frac{4 c^2 x_i^2 x_j^2}{2} - \frac{8 c^3 x_i^3 x_j^3}{6} + \cdots</script><script type="math/tex; mode=display">  = \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_i^n}, \cdots \end{matrix} \right]  \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_j^n}, \cdots \end{matrix} \right]^T</script><p>  所以</p><script type="math/tex; mode=display">  e^{c||x_i - x_j||^2}  = e^{c(x_i^2 + x_j^2)} \sum_{n=0}^{\infty} \frac{(-2c)^n x_i^n x_j^n}{n!}</script><script type="math/tex; mode=display">  = \left\{ e^{cx_i^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_i^n}, \cdots \end{matrix} \right] \right\}  \left\{ e^{cx_j^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_j^n}, \cdots \end{matrix} \right] \right\}^T</script><p>  则可得映射函数为</p><script type="math/tex; mode=display">\mathit{\Phi}(x) = e^{cx^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x^n}, \cdots \end{matrix} \right] \tag{4.d}</script></li><li><p>输入维度为$d$</p><script type="math/tex; mode=display">||x_i - x_j||^2 = (x_i - x_j)^T (x_i - x_j) = x_i^T x_i - 2 x_i^T x_j + x_j^T x_j</script><p>  代入核函数$(4.c)$得到</p><script type="math/tex; mode=display">e^{c||x_i - x_j||^2} = e^{c(x_i^T x_i + x_j^T x_j)} e^{-2c x_i^T x_j}</script><p>  其中</p><script type="math/tex; mode=display">e^{-2c x_i^T x_j} = \sum_{n=0}^{\infty} \frac{(-2c)^n (x_i^T x_j)^n}{n!}</script><p>  故</p><script type="math/tex; mode=display">e^{c||x_i - x_j||^2} = e^{c(x_i^T x_i + x_j^T x_j)} \sum_{n=0}^{\infty} \frac{(-2c)^n (x_i^T x_j)^n}{n!}</script><p>  特殊化，对于$d=2$，展开至$3$阶，得到</p><script type="math/tex; mode=display">  e^{c||x_i - x_j||^2} = e^{c(x_{i1}^2 + x_{i2}^2 + x_{j1}^2 + x_{j2}^2)} \left[ 1 - 2c (x_{i1}x_{j1} + x_{i2}x_{j2}) + 4c^2 (x_{i1}x_{j1} + x_{i2}x_{j2})^2 -8c^3 (x_{i1}x_{j1} + x_{i2}x_{j2})^3 \right]</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      x_{i1}x_{j1} + x_{i2}x_{j2} \\      (x_{i1}x_{j1} + x_{i2}x_{j2})^2 = x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2 \\      (x_{i1}x_{j1} + x_{i2}x_{j2})^3 = x_{i1}^3 x_{j1}^3 + 3x_{i1}x_{j1}x_{i2}^2x_{j2}^2 + 3x_{i1}^2x_{j1}^2x_{i2}x_{j2} + x_{i2}^3 x_{j2}^3 \\  \end{cases}</script><p>  带入后得</p><script type="math/tex; mode=display">  e^{c||x_i - x_j||^2} = e^{c(x_{i1}^2 + x_{i2}^2 + x_{j1}^2 + x_{j2}^2)} [ 1 - 2c (x_{i1}x_{j1} + x_{i2}x_{j2}) + 4c^2 (x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2) -</script><script type="math/tex; mode=display">  8c^3 (x_{i1}^3 x_{j1}^3 + 3x_{i1}x_{j1}x_{i2}^2x_{j2}^2 + 3x_{i1}^2x_{j1}^2x_{i2}x_{j2} + x_{i2}^3 x_{j2}^3) ]</script><p>  所以映射函数为</p><script type="math/tex; mode=display">  \mathit{\Phi}(x) = e^{c(x_1^2 + x_2^2)} \left[       \begin{matrix}          1, \sqrt{-2c} x_1, \sqrt{-2c} x_2, \sqrt{4c^2} x_1^2, \sqrt{8c^2} x_1x_2, \sqrt{4c^2} x_2^2, \sqrt{-8c^3}x_1^3, \sqrt{-24c^3} x_1 x_2^2, \sqrt{-24c^3} x_1^2 x_2, \sqrt{-8c^3} x_1^3      \end{matrix} \right]^T \tag{4.e}</script><p>  即升维后维数为$10$。</p></li></ul></li></ol><h2 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h2><p>对于线性可分得情况，目标为求解一个超平面$w^T \mathit{\Phi}(x) + b = 0$使两类点落在超平面两侧。</p><p><img src="/2019/05/27/Support-Vector-Machine/svm-3-conditions.jpg" alt="svm-3-conditions"></p><p>观察以上两图，图$(c)$分割最佳，应有</p><ul><li>超平面在两类点间隔内，可平移距离最大；<script type="math/tex; mode=display">r = \max (r_+ - r_-)</script></li><li>分割平面$\mathcal{P}$到$\mathcal{P}_+, \mathcal{P}_-$的距离相等，即；<script type="math/tex; mode=display">r_- = r_+ = \frac{r}{2}</script></li><li>落在支撑超平面$\mathcal{P}_+, \mathcal{P}_-$上的点称为<strong>支持向量<code>Support Vector</code></strong>，可记作$x_{+/-}^{sup}$；</li></ul><p>设分割超平面$\mathcal{P}$为</p><script type="math/tex; mode=display">g(x) = w^T \mathit{\Phi}(x) + b = 0 \tag{5}</script><p>判别方程可定义为</p><script type="math/tex; mode=display">\hat{y} = \text{sign} \left[  w^T \mathit{\Phi}(x) + b \right] \tag{6}</script><p>则平面$\mathcal{P}$上下平移后得到平面$\mathcal{P}_+, \mathcal{P}_-$，即</p><script type="math/tex; mode=display">g_+(x) = w^T \mathit{\Phi}(x) + b - C(常数) = 0</script><script type="math/tex; mode=display">g_-(x) = w^T \mathit{\Phi}(x) + b + C(常数) = 0</script><p>作归一化处理，两边同除以$C(常数)$，则支撑超平面方程为</p><script type="math/tex; mode=display">g_+(x) = w^T \mathit{\Phi}(x) + b - 1 = 0</script><script type="math/tex; mode=display">g_-(x) = w^T \mathit{\Phi}(x) + b + 1 = 0</script><p>对于正负样本$x_{+/-}$，分别满足</p><script type="math/tex; mode=display">\begin{cases}    w^T \mathit{\Phi}(x_+) + b > 1 \\    y = 1\end{cases}\begin{cases}    w^T \mathit{\Phi}(x_-) + b < - 1 \\    y = -1\end{cases}</script><p>即</p><script type="math/tex; mode=display">y \left[ w^T \mathit{\Phi}(x) + b \right] > 1</script><p>现希望优化两个平面间的距离，使其达到最大，即优化目标为</p><script type="math/tex; mode=display">w, b = \arg \max r</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] > 1 \tag{7}</script><p><strong>那么如何求解支撑超平面间距离$r$呢？</strong>，有两种思路</p><ol><li><p>思路一<br> <img src="/2019/05/27/Support-Vector-Machine/r.jpg" alt="r"></p><p> 过超平面$\mathcal{P}$任一点$\mathit{\Phi}(x)$作垂线，分别交$\mathcal{P}_{+/-}$于向量$\mathit{\Phi}(x_{+/-})$，则</p><script type="math/tex; mode=display"> \begin{cases}     w^T \mathit{\Phi}(x_+) + b = + 1 \\     w^T \mathit{\Phi}(x_-) + b = - 1  \end{cases}</script><p> 利用距离公式求解点$\mathit{\Phi}(x)$到超平面$\mathcal{P}$的距离</p><script type="math/tex; mode=display">r_+ = r_- = \frac{r}{2} = \frac{| w^T \mathit{\Phi}(x_{+/-}) + b |}{||w||} = \frac{1}{||w||}</script><p> 所以</p><script type="math/tex; mode=display">r = \frac{2}{||w||} \tag{8.a}</script></li><li><p>思路二</p><script type="math/tex; mode=display">r_+ = r_- = \frac{r}{2} = \min_i \frac{| w^T \mathit{\Phi}(x^{(i)}) + b |}{||w||}, \quad i = 1, ..., N</script><p> 由支持向量定义</p><script type="math/tex; mode=display">| w^T \mathit{\Phi}(x_{+/-}^{sup}) + b | = \min | w^T \mathit{\Phi}(x^{(i)}) + b |</script><p> 且</p><script type="math/tex; mode=display">| w^T \mathit{\Phi}(x_{+/-}^{sup}) + b | = 1</script><p> 所以</p><script type="math/tex; mode=display">r = \frac{2}{||w||} \tag{8.b}</script></li></ol><p>所以优化目标为</p><script type="math/tex; mode=display">w, b = \arg \max \frac{2}{||w||}</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{9.a}</script><p>为方便求解，相当于</p><script type="math/tex; mode=display">w, b = \arg \min \frac{1}{2} ||w||^2</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{10.b}</script><h1 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h1><p>回归模型的目标是让训练集中每个样本点$(x^{(i)}, y^{(i)})$尽量拟合到一个线性模型上</p><script type="math/tex; mode=display">y^{(i)} = w^T \mathit{\Phi}(x^{(i)}) + b</script><p>对于一般的回归模型，一般使用$MSE$作为损失函数，但是在$y^{(i)} \neq w^T \mathit{\Phi}(x^{(i)}) + b$时就会有损失，所以$SVM$不采用。</p><p>定义一个常量$\epsilon &gt; 0$，对于某个样本点$(x^{(i)}, y^{(i)})$，其损失定义如下，即在支撑超平面间隔内的样本点是没有损失的</p><script type="math/tex; mode=display">L^{(i)} = \begin{cases}    0 & | y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) - b | \leq \epsilon \\    | y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) - b | - \epsilon & \text{otherwise}\end{cases}</script><p><img src="/2019/05/27/Support-Vector-Machine/regression.jpg" alt="regression"></p><h1 id="优化问题的求解与SMO算法"><a href="#优化问题的求解与SMO算法" class="headerlink" title="优化问题的求解与SMO算法"></a>优化问题的求解与SMO算法</h1><h2 id="带约束的优化问题求解"><a href="#带约束的优化问题求解" class="headerlink" title="带约束的优化问题求解"></a>带约束的优化问题求解</h2><ol><li><p>纯等式约束<br> 一般形式为</p><script type="math/tex; mode=display">\vec{w} = \arg \min f(\vec{w})</script><script type="math/tex; mode=display">s.t. \qquad h_j(\vec{w}) = 0, \quad j = 1, 2, \cdots, m \tag{11}</script><p> 其中$f(\vec{w}), h_j(\vec{w})$均可导</p><p> 列写拉格朗日函数</p><script type="math/tex; mode=display">L(\vec{w}, \vec{\lambda}) = f(\vec{w}) + \sum_{j=0}^m \lambda_j h_j(\vec{w}) \tag{11.a}</script><p> 求取极点</p><script type="math/tex; mode=display"> \begin{cases} \frac{\partial L}{\partial w_i} = \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^m \lambda_j \frac{\partial h_j(\vec{w})}{\partial w_i} = 0 \\ \frac{\partial L}{\partial \lambda_j} = h_j(\vec{w}) = 0 \end{cases} \tag{11.b}</script><blockquote><p>如 $f(x, y) = x^2 + 3xy + y^2, \quad s.t. \quad x + y = 100 $</p></blockquote></li><li><p>纯不等式约束<br> 一般形式为</p><script type="math/tex; mode=display">\vec{w} = \arg \min f(\vec{w})</script><script type="math/tex; mode=display">s.t. \qquad g_j(\vec{w}) \leq 0, \quad j = 1, 2, \cdots, p \tag{12}</script><p> 对于上不等式约束，引入松弛变量$\epsilon_j^2$，使其转换为等式约束</p><script type="math/tex; mode=display">s.t. \qquad g_j(\vec{w}) + \epsilon_j^2 = 0, \quad j = 1, 2, \cdots, p</script><blockquote><p>注意，这里引入的松弛变量为平方项，如此可避免增加约束$\epsilon_j \geq 0$</p></blockquote><p> 列写拉格朗日函数</p><script type="math/tex; mode=display">L(\vec{w}, \vec{\mu}, \vec{\epsilon^2}) = f(\vec{w}) + \sum_{j=0}^p \mu_j (g_j(\vec{w}) + \epsilon_j^2) \tag{12.a}</script><p> 求取极值点</p><script type="math/tex; mode=display"> \begin{cases} \frac{\partial L}{\partial w_i} = \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\ \frac{\partial L}{\partial \mu_j} = g_j(\vec{w}) + \epsilon_j^2 = 0 \\ \frac{\partial L}{\partial \epsilon_j} = 2 \mu_j \epsilon_j = 0 \\ \mu_j \geq 0 \end{cases} \tag{12.b}</script><p> 注意等式三</p><ol><li>若$\mu_j = 0$，即对应不等式$g_j(\vec{w}) \leq 0$未起到约束作用；</li><li><p>若$\mu_j \neq 0$，则$\epsilon_j = 0$，那么$ g_j(\vec{w}) = 0$；<br>则总结可得</p><script type="math/tex; mode=display">\mu_j g_j(\vec{w}) = 0</script><p>故$(12.b)$转化为</p><script type="math/tex; mode=display">\begin{cases}\frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\\mu_j g_j(\vec{w}) = 0 \\\mu_j \geq 0\end{cases} \tag{12.c}</script></li></ol></li></ol><ol><li><p>混合条件约束<br> 一般形式为</p><script type="math/tex; mode=display">\vec{w} = \arg \min f(\vec{w})</script><script type="math/tex; mode=display">s.t. \qquad h_j(\vec{w}) = 0, \quad j = 1, 2, \cdots, m</script><script type="math/tex; mode=display">\quad \qquad g_k(\vec{w}) \leq 0, \quad k = 1, 2, \cdots, p \tag{13}</script><p> 经上述内容，可得</p><script type="math/tex; mode=display">L(\vec{w}, \vec{\mu}, \vec{\epsilon^2}) = f(\vec{w}) + \sum_{j=0}^m \lambda_j h_j(\vec{w}) + \sum_{j=0}^p \mu_j (g_j(\vec{w}) + \epsilon_j^2) \tag{13.a}</script><p> 求取极值点</p><script type="math/tex; mode=display"> \begin{cases} \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^m \lambda_j \frac{\partial h_j(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\ h_j(\vec{w}) = 0 \\ \mu_j g_j(\vec{w}) = 0 \\ \mu_j \geq 0 \end{cases} \tag{13.b}</script><p> 上式即$K.K.T.$条件。</p></li></ol><h2 id="线性可分情况下求解"><a href="#线性可分情况下求解" class="headerlink" title="线性可分情况下求解"></a>线性可分情况下求解</h2><p><img src="/2019/05/27/Support-Vector-Machine/linear.jpg" alt="linear"></p><p>对于优化问题$(10.b)$</p><script type="math/tex; mode=display">w, b = \arg \max \frac{2}{||w||}</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{10.b}</script><p>利用拉格朗日乘子法</p><script type="math/tex; mode=display">L(w, b, \mu) = \frac{1}{2} ||w||^2 + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} \tag{14}</script><p>则带求解问题为</p><script type="math/tex; mode=display">w, b, \mu = \arg \min_{w, b} \max_{\mu} L(w, b, \mu) \tag{15.a}</script><p>转化为上式的<strong>对偶问题</strong></p><script type="math/tex; mode=display">w, b, \mu = \arg \min_{\mu} \max_{w,b} L(w, b, \mu) \tag{15.b}</script><blockquote><p>凸优化</p></blockquote><p>分别对参数求偏导</p><script type="math/tex; mode=display">\frac{\partial}{\partial w_j} L(w, b, \mu) = w_j - \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j)</script><script type="math/tex; mode=display">\frac{\partial}{\partial b} L(w, b, \mu) = - \sum_i \mu^{(i)} y^{(i)}</script><blockquote><p>$ \frac{\partial}{\partial w_j} \frac{1}{2} ||w||^2 = w_j; \quad \frac{\partial}{\partial w_j} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = \mathit{\Phi}(x^{(i)}_j) $，注意这里还没有用到映射函数$\mathit{\Phi}(x)$计算。</p></blockquote><p>该式为不等式约束，由$K.K.T.$条件，联立得到</p><script type="math/tex; mode=display">\begin{cases}    \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) = w_j \\    \sum_i \mu^{(i)} y^{(i)} = 0 \\    \\    1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 0 \\    \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0 \\    \mu^{(i)} \geq 0\end{cases} \tag{16}</script><p>有</p><script type="math/tex; mode=display">\tilde{L}(\mu) = \frac{1}{2} w^T w + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\}</script><script type="math/tex; mode=display">\qquad \qquad \qquad \qquad = \frac{1}{2} w^T w + \sum_i \mu^{(i)} - w^T \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) + b \sum_i \mu^{(i)} y^{(i)} \tag{17.a}</script><p>将$w = \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}), \sum_i \mu^{(i)} y^{(i)} = 0$代入，消去变量$w, b$，有</p><script type="math/tex; mode=display">\tilde{L}(\mu) = \sum_i \mu^{(i)} - \frac{1}{2} w^T w \tag{17.b}</script><p>其中</p><script type="math/tex; mode=display">w^T w = \left[ \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \right]^T \left[ \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \right]</script><script type="math/tex; mode=display">\qquad = \sum_i \left[ \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T \sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)}) \right]</script><script type="math/tex; mode=display">\quad = \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{17.c}</script><p>代回$(17.b)$得到</p><script type="math/tex; mode=display">\tilde{L}(\mu) = \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{17}</script><blockquote><p>这里出现了<strong>核函数</strong>：$\kappa(x^{(i)}, x^{(j)}) = \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})$</p></blockquote><p>那么优化问题现在转化为</p><script type="math/tex; mode=display">\mu = \arg \max_{\mu} \tilde{L}(\mu) = \arg \max_{\mu} \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})</script><script type="math/tex; mode=display">s.t.\qquad \mu^{(i)} \geq 0, \quad \sum_i \mu^{(i)} y^{(i)} = 0 \tag{18}</script><p>其中$i = 1, \cdots, N$，现在只需优化$\mu$即可，该式使用$SMO$或梯度下降法算法求解，再用下式求解参数$w, b$</p><script type="math/tex; mode=display">\begin{cases}    w = \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \\    y^{sup} \left[ w^T \mathit{\Phi}(x^{sup}) + b \right] = 1\end{cases} \tag{19}</script><p>可能存在多个支持向量，均满足等式$(19.2)$。</p><h2 id="线性不可分情况下求解"><a href="#线性不可分情况下求解" class="headerlink" title="线性不可分情况下求解"></a>线性不可分情况下求解</h2><p><img src="/2019/05/27/Support-Vector-Machine/non-linear.jpg" alt="non-linear"></p><p>如上图，存在部分样本点线性不可分，有两种方法可解决</p><ol><li>核函数</li><li>软间隔支持向量机</li></ol><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>上面介绍了核函数，其作用是将样本特征升维，添加的维度与已存在的特征是线性不相关的，例如我们有样本点</p><script type="math/tex; mode=display">x^{(i)} = \left[\begin{matrix}    x^{(i)}_1, \cdots, x^{(i)}_n\end{matrix}\right]^T</script><p>可以增加多项式维，例如$(x^{(i)}_j)^n, \prod_{k \leq n}^{k \leq K \leq N} x^{(i)}_k$等，或者添加其他形式的非线性函数，但我们知道，所有函数均可在某点$x = x_0$处展开为幂级数，本质上一致</p><script type="math/tex; mode=display">f(x) = \sum_n \frac{f^{(n)}(x - x_0)}{n!} (x - x_0)^{(n)}</script><p>核函数就是利用级数展开的概念，构造多项式维度，将样本点升维，例如高斯核函数$\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}}$将其升高到无穷维。机器学习算法求解各维度的权值系数，特征越重要，系数值占比越大。</p><h3 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h3><p><strong>本质上仍为线性支持向量机</strong>，对于线性不可分的情况，应允许部分样本点不满足条件</p><script type="math/tex; mode=display">y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1</script><p>可对每个样本引入松弛变量$\epsilon^{(i)}$，即</p><script type="math/tex; mode=display">y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 - \epsilon^{(i)} \tag{20}</script><blockquote><p>仅仅对于落在支撑超平面间的样本点满足$\epsilon^{(i)} &gt; 0$</p></blockquote><p>形象解释，即对于<strong>落在支撑超平面间的样本点$x^{(i)}$</strong>，视其为<strong>软间隔支持向量(自创)</strong>，调整$\epsilon^{(i)}$将支撑超平面进行微量的位移$d_{\epsilon^{(i)}}$，如下图<br><img src="/2019/05/27/Support-Vector-Machine/epsilon.jpg" alt="epsilon"></p><p>但是呢，也要对不满足该条件的样本个数进行限制，希望越少越好。那么加入<strong>惩罚系数$C$(超参数)</strong>，对不满足条件的样本进行惩罚，使$\sum_i \epsilon^{(i)}$越小越好，则优化目标变更为</p><script type="math/tex; mode=display">w, b = \arg \min_{w,b} \left( \frac{1}{2} ||w||^2 + C \sum_i \epsilon^{(i)} \right)</script><script type="math/tex; mode=display">s.t. \qquad y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 - \epsilon^{(i)}</script><script type="math/tex; mode=display">\qquad \epsilon^{(i)} \geq 0 \tag{21}</script><p>同样的，构造拉格朗日函数</p><script type="math/tex; mode=display">L(w, b, \epsilon, \mu_1, \mu_2) = \left( \frac{1}{2} ||w||^2 + C \sum_i \epsilon^{(i)} \right) + \sum_i \mu_1^{(i)} \left\{ 1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} + \sum_i \mu_2^{(i)} \left( - \epsilon^{(i)} \right) \tag{22}</script><p>根据$K.K.T.$条件</p><script type="math/tex; mode=display">\begin{cases}    \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) = w_j \\    \sum_i \mu_1^{(i)} y^{(i)} = 0 \\    C - \mu_1^{(i)} - \mu_2^{(i)} = 0 \\    \\    1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 0 \\    \mu_1^{(i)} \left\{ 1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0 \\    \mu_1^{(i)} \geq 0 \\    \\    - \epsilon^{(i)} \leq 0 \\    \mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0 \\    \mu_2^{(i)} \geq 0\end{cases} \tag{23}</script><p>消除$w, b, \epsilon$</p><script type="math/tex; mode=display">\tilde{L}(\mu_1, \mu_2) = \frac{1}{2} w^T w + C \sum_i \epsilon^{(i)} + \sum_i \mu_1^{(i)} \left( 1 - \epsilon^{(i)} \right) - w^T \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) - b \sum_i \mu_1^{(i)} y^{(i)} - \sum_i \mu_2^{(i)} \epsilon^{(i)}</script><p>其中$w = \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}); \quad \sum_i \mu_1^{(i)} y^{(i)} = 0$</p><script type="math/tex; mode=display">\tilde{L}(\mu_1, \mu_2) = - \frac{1}{2} w^T w  + C \sum_i \epsilon^{(i)} + \sum_i \mu_1^{(i)} - \sum_i \left( \mu_1^{(i)} + \mu_2^{(i)} \right) \epsilon^{(i)}</script><p>其中$\mu_1^{(i)} + \mu_2^{(i)} = C$，所以同$(17)$</p><script type="math/tex; mode=display">\tilde{L}(\mu_1) = \sum_i \mu_1^{(i)} - \frac{1}{2} w^T w = \sum_i \mu_1^{(i)} - \frac{1}{2} \sum_i \sum_j \mu_1^{(i)} \mu_1^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{24}</script><p>那么优化问题现在转化为</p><script type="math/tex; mode=display">\mu_1 = \arg \max_{\mu_1} \tilde{L}(\mu_1) = \arg \max_{\mu_1} \sum_i \mu_1^{(i)} - \frac{1}{2} \sum_i \sum_j \mu_1^{(i)} \mu_1^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})</script><script type="math/tex; mode=display">s.t.\qquad \mu_1^{(i)} \geq 0, \quad \mu_2^{(i)} \geq 0</script><script type="math/tex; mode=display">\sum_i \mu_1^{(i)} y^{(i)} = 0</script><script type="math/tex; mode=display">C - \mu_1^{(i)} - \mu_2^{(i)} = 0 \tag{25}</script><p>对于上式，有如下分析</p><script type="math/tex; mode=display">\begin{cases} \mu_1^{(i)} \geq 0 \\ \mu_2^{(i)} \geq 0 \\ \mu_2^{(i)} = C - \mu_1^{(i)} \end{cases} \Rightarrow C \geq \mu_1^{(i)} \geq 0 \tag{26.a}</script><ol><li>$C = \mu_1^{(i)}$时，$\mu_2^{(i)} = 0$，由$\mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0$，可得$\epsilon^{(i)} \geq 0$<ol><li>$\epsilon^{(i)} = 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1$，该点为支撑向量；</li><li>$\epsilon^{(i)} &gt; 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] &lt; 1$，该点在支撑超平面间；</li></ol></li><li>$C \neq \mu_1^{(i)}$即$C &gt; \mu_1^{(i)} \geq 0$时，$\mu_2^{(i)} \neq 0$，则由$\mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0$，可得$\epsilon^{(i)} = 0$，那么$\mu_1^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0$<ol><li>$\mu_1^{(i)} &gt; 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1$，该点为支撑向量；</li><li>$\mu_1^{(i)} = 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1$，该点分类正确，在支持超平面上或者两边</li></ol></li></ol><p>总结一下，即</p><script type="math/tex; mode=display">\begin{cases}    \mu_1^{(i)} = 0 \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 \\    0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 \\    \mu_1^{(i)} = C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1\end{cases} \tag{27}</script><p>同样利用$SMO$或梯度下降法求解$\mu_1$，然后以下式求解$w, b, \epsilon, \mu_2$</p><script type="math/tex; mode=display">\begin{cases}    \mu_2 = C - \mu_1 \\    w_j = \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) \\    y^{sup} \left[ w^T \mathit{\Phi}(x^{sup}) + b \right] = 1 \\    y^{sup'} \left[ w^T \mathit{\Phi}(x^{sup'}) + b \right] = 1 - \epsilon^{sup'}\end{cases} \tag{28}</script><blockquote><p>注：$(x^{sup’}, y^{sup’})$表示“软间隔支持向量”。<br>注：(1) 并非所有的样本点都有一个松弛变量与其对应。实际上只有“离群点”才有，所有没离群的点松弛变量都等于0<br>(2) 松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远<br>(3) 惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题<br>(4) 惩罚因子C不是一个变量，整个优化问题在解的时候，C是一个你必须事先指定的值，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个C的值，再解一次优化问题，得到另一个分类器，再看看效果，如此就是一个参数寻优的过程，但这和优化问题本身决不是一回事，优化问题在解的过程中，C一直是定值<br>(5) 尽管加了松弛变量这么一说，但这个优化问题仍然是一个优化问题（汗，这不废话么），解它的过程比起原始的硬间隔问题来说，没有任何更加特殊的地方(C≥$\mu^{(i)}$≥0)<br>(6) 完全可以给每一个离群点都使用不同的C，这时就意味着你对每个样本的重视程度都不一样，有些样本丢了也就丢了，错了也就错了，这些就给一个比较小的C；而有些样本很重要，决不能分类错误（比如中央下达的文件啥的，笑），就给一个很大的C。<br><strong>以上忘记从哪里摘抄的了:-(</strong> </p></blockquote><h2 id="Sequential-Minimal-Optimization-SMO"><a href="#Sequential-Minimal-Optimization-SMO" class="headerlink" title="Sequential Minimal Optimization(SMO)"></a>Sequential Minimal Optimization(SMO)</h2><p>以上我们得到优化目标</p><script type="math/tex; mode=display">\mu = \arg \max_{\mu} \tilde{L}(\mu) = \arg \max_{\mu} \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})</script><script type="math/tex; mode=display">s.t.\qquad \mu^{(i)} \geq 0</script><script type="math/tex; mode=display">\quad \sum_i \mu^{(i)} y^{(i)} = 0</script><script type="math/tex; mode=display">\quad C \geq \mu^{(i)} \geq 0 \tag{29}</script><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>把对整个λ向量的优化转化为对每一对$\mu^{(i)},\mu^{(j)}$的优化，如果我们把其他λ先固定，仅仅优化某一对$\mu^{(i)},\mu^{(j)}$，那么我们可以通过解析式（即通过确定的公式来计算）来优化$\mu^{(i)},\mu^{(j)}$ 。而且此时$K.K.T.$条件很重要，之前说过最优解是一定会满足$K.K.T.$条件的，所以如果我们优化使所有$\mu$都满足了$K.K.T.$条件，那么这样最优解就会找到。</p><h3 id="选择优化对的方法"><a href="#选择优化对的方法" class="headerlink" title="选择优化对的方法"></a>选择优化对的方法</h3><p>寻找两个参数时，应找那些违反$K.K.T.$条件的，具体过程可分为外层循环和内层循环，利用启发式规则寻找待优化参数对。</p><ol><li>启发式规则1<ol><li>在<strong>所有样本</strong>中选择违反$K.K.T.$条件的一个乘子$\mu^{(i)}$，用启发式规则2选择另一个乘子$\mu^{(j)}$，对这两个乘子进行优化；</li><li>接着，从<strong>所有非边界样本</strong>中，选择违反$K.K.T.$条件的一个乘子作为最外层循环，用启发式规则2选择另一个乘子进行这两个乘子的优化；</li><li>最后，若上述非边界样本中没有违反$K.K.T.$条件的样本，则<strong>再从整个样本中</strong>去找，直到所有样本中没有需要改变的乘子，或满足其他停止条件为止。</li></ol></li><li>启发式规则2<ol><li>首先在<strong>非边界乘子</strong>中获得$|E_1−E_2|$最大的样本$\mu^{(j)}$；</li><li>如果1中没有找到，则从<strong>所有样本中</strong>随机确定$\mu^{(j)}$</li></ol></li></ol><p>满足式$(27)$即满足$K.K.T.$条件</p><script type="math/tex; mode=display">\begin{cases}    \mu_1^{(i)} = 0 \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 \\    0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 \\    \mu_1^{(i)} = C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1\end{cases} \tag{27}</script><p>那么以下情况不满足$K.K.T.$条件</p><script type="math/tex; mode=display">\begin{cases}    y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 时，\mu_1^{(i)} > 0\\    y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 时，\mu_1^{(i)} = 0 或 \mu_1^{(i)} = 1 \\    y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1 时，\mu_1^{(i)} < C\end{cases} \tag{30}</script><blockquote><p><a href="https://www.cnblogs.com/xxrxxr/p/7538430.html" target="_blank" rel="noopener">第三部分：SMO算法的个人理解 - cnblogs</a><br>找第一个参数的具体过程是这样的：</p><ol><li>遍历一遍整个数据集，对每个不满足$K.K.T.$条件的参数，选作第一个待修改参数</li><li>在上面对整个数据集遍历一遍后，选择那些参数满足$0 &lt; \mu &lt; C$的子集，开始遍历，如果发现一个不满足$K.K.T.$条件的，作为第一个待修改参数，然后找到第二个待修改的参数并修改，修改完后，重新开始遍历这个子集</li><li>遍历完子集后，重新开始①②，直到在执行①和②时没有任何修改就结束<br>（为什么要这样遍历，我现在还是不太明白）</li></ol><p>找第二个参数的过程是这样的：</p><ol><li>启发式找，找能让下式最大的</li><li>寻找一个随机位置的满足下式的可以优化的参数进行修改</li><li>在整个数据集上寻找一个随机位置的可以优化的参数进行修改</li><li>都不行那就找下一个第一个参数</li></ol></blockquote><h3 id="两个变量的优化问题"><a href="#两个变量的优化问题" class="headerlink" title="两个变量的优化问题"></a>两个变量的优化问题</h3><p>以$\mu^{(1)}, \mu^{(2)}$为例</p><script type="math/tex; mode=display">\min_{\mu^{(1)}, \mu^{(2)}} \left[ \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - \sum_i \mu^{(i)} \right]</script><script type="math/tex; mode=display">= \min_{\mu^{(1)}, \mu^{(2)}} \left[ \frac{1}{2} \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T \sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)}) - \sum_i \mu^{(i)} \right] \tag{31.a}</script><p>其中</p><script type="math/tex; mode=display">\sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T = \mu^{(1)} y^{(1)} \mathit{\Phi}(x^{(1)})^T + \mu^{(2)} y^{(2)} \mathit{\Phi}(x^{(2)})^T + \sum_{i=3} \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T</script><script type="math/tex; mode=display">\sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)})^T = \mu^{(1)} y^{(1)} \mathit{\Phi}(x^{(1)})^T + \mu^{(2)} y^{(2)} \mathit{\Phi}(x^{(2)})^T + \sum_{j=3} \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)})^T</script><script type="math/tex; mode=display">\sum_i \mu^{(i)} = \mu^{(1)} + \mu^{(2)} + \sum_{i=3} \mu^{(i)}</script><p>代回$(31.a)$，多项式展开整理得到</p><script type="math/tex; mode=display">(31.b) = \min_{\mu^{(1)}, \mu^{(2)}} [\frac{1}{2} \mu^{(1)2} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(1)}) + \frac{1}{2} \mu^{(2)2} \mathit{\Phi}(x^{(2)})^T \mathit{\Phi}(x^{(2)}) + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(2)}) +</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} \sum_{i=3} \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(i)}) + \mu^{(2)} y^{(2)} \sum_{j=3} \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(2)})^T \mathit{\Phi}(x^{(j)}) +</script><script type="math/tex; mode=display">\frac{1}{2} \sum_{i=3} \sum_{j=3} \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - (\mu^{(1)} + \mu^{(2)}) - \sum_{i=3} \mu^{(i)}] \tag{31.b}</script><p>令核函数</p><script type="math/tex; mode=display">K_{ij} = \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{32.a}</script><script type="math/tex; mode=display">const = \frac{1}{2} \sum_{i=3} \sum_{j=3} \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - \sum_{i=3} \mu^{(i)} \tag{32.b}</script><p>定义</p><script type="math/tex; mode=display">f(x^{(i)}) = \sum_m \mu^{(m)} y^{(m)} K_{im} + b \tag{32.c}</script><script type="math/tex; mode=display">E^{(i)} = f(x^{(i)}) - y^{(i)}</script><script type="math/tex; mode=display">v^{(i)} = \sum_{m=3} \mu^{(m)} y^{(m)} K_{im} = f(x^{(i)}) - \sum_{m=1}^2 \mu^{(m)} y^{(m)} K_{im} - b \tag{32.e}</script><p>以上，代回$(31.b)$，优化问题转化为</p><script type="math/tex; mode=display">\min_{\mu^{(1)}, \mu^{(2)}} [\frac{1}{2} \mu^{(1)2} K_{11} + \frac{1}{2} \mu^{(2)2} K_{22} + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} K_{12} +</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} v^{(1)} + \mu^{(2)} y^{(2)} v^{(2)} -(\mu^{(1)} + \mu^{(2)}) + const]</script><script type="math/tex; mode=display">s.t.\qquad \mu^{(i)} \geq 0</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} + \mu^{(2)} y^{(2)} = - \sum_{i=3} \mu^{(i)} y^{(i)}</script><script type="math/tex; mode=display">C \geq \mu^{(i)} \geq 0 \tag{33}</script><p>记</p><script type="math/tex; mode=display">g(\mu^{(1)}, \mu^{(2)}) = \frac{1}{2} \mu^{(1)2} K_{11} + \frac{1}{2} \mu^{(2)2} K_{22} + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} K_{12} +</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} v^{(1)} + \mu^{(2)} y^{(2)} v^{(2)} -(\mu^{(1)} + \mu^{(2)}) + const \tag{34}</script><p>记$\epsilon = - \sum_{i=3} \mu^{(i)} y^{(i)}$，则</p><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} + \mu^{(2)} y^{(2)} = \epsilon</script><p>消去$\mu^{(1)}$</p><script type="math/tex; mode=display">\mu^{(1)} = \frac{\epsilon - \mu^{(2)} y^{(2)}}{y^{(1)}} \quad 同 \quad \mu^{(1)} = y^{(1)}\epsilon - y^{(1)} y^{(2)} \mu^{(2)}</script><p>记$\gamma = y^{(1)}\epsilon, \quad s = y^{(1)} y^{(2)} $，则</p><script type="math/tex; mode=display">\mu^{(1)} = \gamma - s \mu^{(2)} \tag{35}</script><blockquote><script type="math/tex; mode=display">s^2 = 1, \quad \gamma s = y^{(2)}\epsilon</script></blockquote><p>代入函数$(34)$，有</p><script type="math/tex; mode=display">\tilde{g}(\mu^{(2)}) = \frac{1}{2} (K_{11} + K_{22} - 2 K_{12}) \mu^{(2)2} +</script><script type="math/tex; mode=display">\left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} +</script><script type="math/tex; mode=display">(\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{36}</script><p>转化为单变量$\mu^{(2)}$的二次优化问题。</p><h3 id="剪裁边界"><a href="#剪裁边界" class="headerlink" title="剪裁边界"></a>剪裁边界</h3><p>需考虑$\mu^{(2)}$的取值范围，即编程时$\mu^{(2)}$的<strong>剪裁边界</strong>，设</p><script type="math/tex; mode=display">L \leq \mu^{(2)}_{new} \leq H \tag{37}</script><p>综合条件</p><script type="math/tex; mode=display">\mu^{(1)}_{new} y^{(1)} + \mu^{(2)}_{new} y^{(2)} = \epsilon</script><script type="math/tex; mode=display">0 \leq \mu^{(i)} \leq C</script><blockquote><p>$\mu^{(1)}_{new} y^{(1)} + \mu^{(2)}_{new} y^{(2)} = \epsilon \Rightarrow \mu^{(1)}_{new} + \mu^{(2)}_{new} y^{(1)} y^{(2)} = \epsilon  y^{(1)}$ </p></blockquote><ol><li><p>$y^{(1)} \neq y^{(2)}$时，$y^{(1)} y^{(2)} = -1$</p><script type="math/tex; mode=display">\mu^{(1)}_{old} - \mu^{(2)}_{old} = \epsilon_{\neq} (常数) \quad 则 \quad \mu^{(2)}_{old} = \mu^{(1)}_{old} - \epsilon_{\neq} \tag{38.a}</script><p> 考虑</p><script type="math/tex; mode=display">0 \leq \mu^{(i)} \leq C</script><p> 则</p><script type="math/tex; mode=display"> \begin{cases}     0 \leq \mu^{(1)}_{old} \leq C \\     0 \leq \mu^{(2)}_{old} \leq C \\ \end{cases} \Rightarrow 0 - \epsilon_{\neq} \leq \mu^{(2)}_{old} = \mu^{(1)}_{old} - \epsilon_{\neq} \leq C - \epsilon_{\neq}</script><p> <img src="/2019/05/27/Support-Vector-Machine/y1neqy2.jpg" alt="y1neqy2"></p><p> 此时上下界为</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - \epsilon_{\neq} \} \\     H = \min \{ C, C - \epsilon_{\neq} \} \\ \end{cases} \tag{38.b}</script><p> $(38.a)$代入$(38.b)$，得到迭代式</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\     H = \min \{ C, C - \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\ \end{cases} \tag{38}</script></li><li><p>$y^{(1)} = y^{(2)}$时，$y^{(1)} y^{(2)} = 1$</p><script type="math/tex; mode=display">\mu^{(1)}_{old} + \mu^{(2)}_{old} = \epsilon_{=} (常数) \quad 则 \quad \mu^{(2)}_{old} = - \mu^{(1)}_{old} + \epsilon_{=} \tag{39.a}</script><p> 考虑</p><script type="math/tex; mode=display">0 \leq \mu^{(i)} \leq C</script><p> 则</p><script type="math/tex; mode=display"> \begin{cases}     0 \leq \mu^{(1)}_{old} \leq C \\     0 \leq \mu^{(2)}_{old} \leq C \\ \end{cases} \Rightarrow - C + \epsilon_{=} \leq - \mu^{(1)}_{old} + \epsilon_{=} \leq 0 + \epsilon_{=}</script><p> <img src="/2019/05/27/Support-Vector-Machine/y1eqy2.jpg" alt="y1eqy2"></p><p> 此时上下界为</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - C + \epsilon_{=} \} \\     H = \min \{ C, 0 + \epsilon_{=} \} \\ \end{cases} \tag{39.b}</script><p> $(39.a)$代入$(39.b)$，得到迭代式</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, \mu^{(1)}_{old} + \mu^{(2)}_{old} - C \} \\     H = \min \{ C, \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\ \end{cases} \tag{39}</script></li></ol><h3 id="单变量的二次优化"><a href="#单变量的二次优化" class="headerlink" title="单变量的二次优化"></a>单变量的二次优化</h3><script type="math/tex; mode=display">\tilde{g}(\mu^{(2)}) = \frac{1}{2} (K_{11} + K_{22} - 2 K_{12}) \mu^{(2)2} +</script><script type="math/tex; mode=display">\left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} +</script><script type="math/tex; mode=display">(\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{36}</script><ol><li><p>二次项系数$K_{11} + K_{22} - 2 K_{12} &gt; 0$时<br> 求极值点</p><script type="math/tex; mode=display">\frac{\partial \tilde{g}(\mu^{(2)})}{\partial \mu^{(2)}} = 0 \Rightarrow \tilde{\mu}^{(2)} \tag{40.a}</script><ol><li>若$L \leq \tilde{\mu}^{(2)} \leq H$，则最小值点即为$\mu^{(2)} = \tilde{\mu}^{(2)}$；</li><li>否则在边界处取得最小值。</li></ol></li><li><p>二次项系数$K_{11} + K_{22} - 2 K_{12} = 0$时</p><script type="math/tex; mode=display"> \tilde{g}(\mu^{(2)}) =  \left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} +  (\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{40.b}</script><p> $\tilde{g}(\mu^{(2)})$为一次函数，在边界处取得最小值。</p></li><li><p>二次项系数$K_{11} + K_{22} - 2 K_{12} &lt; 0$时<br> $\tilde{g}(\mu^{(2)})$为开口向下的二次函数，在边界处取得最小值。</p></li></ol><p>综上所述，$\mu^{(2)}$更新的解析解为</p><script type="math/tex; mode=display">\mu^{(2)}_{new, clip} = \begin{cases}    H & \mu^{(2)}_{new} > H \\    \mu^{(2)}_{new} & L \leq \mu^{(2)}_{new} \leq H \\    L & \mu^{(2)}_{new} < L \\\end{cases} \tag{40}</script><p>又因为</p><script type="math/tex; mode=display">\mu^{(1)}_{old} = \gamma - s \mu^{(2)}_{old} \tag{41.a}</script><script type="math/tex; mode=display">\mu^{(1)}_{new} = \gamma - s \mu^{(2)}_{new, clip} \tag{41.b}</script><p>两式相减，得到$\mu^{(1)}$更新的增量形式</p><script type="math/tex; mode=display">\mu^{(1)}_{new} = \mu^{(1)}_{old} + y^{(1)} y^{(2)} \left( \mu^{(2)}_{old} - \mu^{(2)}_{new, clip} \right) \tag{41}</script><p>更新$\mu^{(1)}, \mu^{(2)}$后，重新计算$b$，因为$b$影响到$E^{(i)}$的计算，由$(27)$</p><script type="math/tex; mode=display">0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1</script><p>此时对应支撑向量$x^{sup}$，上右式两边同乘$y^{(i)}$，得到</p><script type="math/tex; mode=display">w^T \mathit{\Phi}(x^{(i)}) + b = y^{(i)}</script><script type="math/tex; mode=display">\Rightarrow b = y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) = y^{(i)} - \sum_m \mu^{(m)} y^{(m)} K_{im}</script><blockquote><p>疑问：$w^T \mathit{\Phi}(x^{(i)}) = \sum_m \mu^{(m)} y^{(m)} K_{im}?$</p></blockquote><p>所以</p><script type="math/tex; mode=display">b^{(1)}_{new} = y^{(1)} - \sum_{m=3} \mu^{(m)} y^{(m)} K_{1m} - \mu^{(1)}_{new} y^{(1)} K_{11} - \mu^{(2)}_{new} y^{(2)} K_{12} \tag{42.a}</script><p>其中</p><script type="math/tex; mode=display">\sum_{m=3} \mu^{(m)} y^{(m)} K_{1m} = f(x^{(1)}) - \sum_{m=1}^2 \mu^{(m)}_{old} y^{(m)} K_{1m} - b_{old} \tag{42.b}</script><blockquote><script type="math/tex; mode=display">v^{(i)} = \sum_{m=3} \mu^{(m)} y^{(m)} K_{im} = f(x^{(i)}) - \sum_{m=1}^2 \mu^{(m)} y^{(m)} K_{im} - b \tag{32.e}</script></blockquote><p>所以</p><script type="math/tex; mode=display">b^{(1)}_{new} = y^{(1)} - f(x^{(1)}) - \mu^{(1)}_{old} y^{(1)} K_{11} + \mu^{(2)}_{old} y^{(2)} K_{12} + b_{old} - \mu^{(1)}_{new} y^{(1)} K_{11} - \mu^{(2)}_{new} y^{(2)} K_{12}</script><script type="math/tex; mode=display">= - E^{(1)} + (\mu^{(1)}_{old} - \mu^{(1)}_{new}) y^{(1)} K_{11} + (\mu^{(2)}_{old} - \mu^{(2)}_{new}) y^{(2)} K_{12} + b_{old} \tag{42}</script><p>同理</p><script type="math/tex; mode=display">b^{(2)}_{new} = - E^{(2)} + (\mu^{(1)}_{old} - \mu^{(1)}_{new}) y^{(1)} K_{21} + (\mu^{(2)}_{old} - \mu^{(2)}_{new}) y^{(2)} K_{22} + b_{old} \tag{43}</script><p>当$b^{(1)}$和$b^{(2)}$均有效时</p><script type="math/tex; mode=display">b_{new} = b^{(1)}_{new} = b^{(2)}_{new} \tag{44.a}</script><p>当两个乘子都在边界上时，则$b$阈值与$K.K.T.$条件一致时，不满足时取中点</p><script type="math/tex; mode=display">b = \begin{cases}    b^{(1)} & 0 < \mu^{(1)}_{new} < C \\    b^{(2)} & 0 < \mu^{(2)}_{new} < C \\    \frac{1}{2} (b^{(1)} + b^{(2)}) & \text{otherwise}\end{cases} \tag{44.b}</script><h3 id="梳理"><a href="#梳理" class="headerlink" title="梳理"></a>梳理</h3><ol><li><p>计算误差</p><script type="math/tex; mode=display">E^{(i)} = f(x^{(i)}) - y^{(i)} = \sum_i \mu^{(m)} y^{(m)} K_{im} + b -y^{(i)}</script></li><li><p>计算上下界</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - \mu^{(i)}_{old} + \mu^{(j)}_{old} \};\quad H = \min \{ C, C - \mu^{(i)}_{old} + \mu^{(j)}_{old} \} & y^{(i)} \neq y^{(j)} \\     L = \max \{ 0, \mu^{(i)}_{old} + \mu^{(j)}_{old} - C \};\quad H = \min \{ C, \mu^{(i)}_{old} + \mu^{(j)}_{old} \} & y^{(i)} = y^{(j)}  \end{cases}</script></li><li><p>计算$\eta$</p><script type="math/tex; mode=display">\eta = K_{ii} + K_{jj} - 2K_{ij}</script></li><li><p>更新$\mu^{(j)}$</p><script type="math/tex; mode=display">\mu^{(j)}_{new} = \mu^{(j)}_{old} + \frac{y^{(j)}(E^{(i)} - E^{(j)})}{\eta}</script></li><li><p>修剪$\mu^{(j)}$</p><script type="math/tex; mode=display"> \mu^{(j)}_{new, clip} = \begin{cases}     H & \mu^{(j)}_{new} > H \\     \mu^{(j)}_{new} & L \leq \mu^{(j)}_{new} \leq H \\     L & \mu^{(j)}_{new} < L \\ \end{cases}</script></li><li><p>更新$\mu^{(i)}$</p><script type="math/tex; mode=display">\mu^{(i)}_{new} = \mu^{(i)}_{old} + y^{(i)} y^{(j)} \left( \mu^{(j)}_{old} - \mu^{(j)}_{new, clip} \right)</script></li><li><p>更新$b^{(i)}, b^{(j)}$</p></li></ol><script type="math/tex; mode=display">b^{(i)}_{new} = - E^{(i)} + (\mu^{(i)}_{old} - \mu^{(i)}_{new}) y^{(i)} K_{ii} + (\mu^{(j)}_{old} - \mu^{(j)}_{new}) y^{(j)} K_{ij} + b_{old}</script><script type="math/tex; mode=display">b^{(j)}_{new} = - E^{(j)} + (\mu^{(i)}_{old} - \mu^{(i)}_{new}) y^{(i)} K_{ji} + (\mu^{(j)}_{old} - \mu^{(j)}_{new}) y^{(j)} K_{jj} + b_{old}</script><ol><li>修剪$b$<script type="math/tex; mode=display">b = \begin{cases}    b^{(i)} & 0 < \mu^{(i)}_{new} < C \\    b^{(j)} & 0 < \mu^{(j)}_{new} < C \\    \frac{b^{(i)} + b^{(j)}}{2} & \text{otherwise}\end{cases}</script></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA" target="_blank" rel="noopener">支持向量机 - 维基百科，自由的百科全书</a></li><li><a href="https://scikit-learn.org/stable/modules/svm.html" target="_blank" rel="noopener">1.4. Support Vector Machine - scikit learn</a></li><li><a href="https://www.jiqizhixin.com/articles/2017-02-06-3" target="_blank" rel="noopener">详解支持向量机 - 机器之心</a></li><li><a href="https://blog.csdn.net/lijil168/article/details/69395023" target="_blank" rel="noopener">深入理解拉格朗日乘子法（Lagrange Multiplier) 和KKT条件 - CSDN</a></li><li><a href="https://zhuanlan.zhihu.com/p/26514613" target="_blank" rel="noopener">浅谈最优化问题的KKT条件 - 知乎</a></li><li><a href="https://www.cnblogs.com/xxrxxr/p/7538430.html" target="_blank" rel="noopener">第三部分：SMO算法的个人理解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mAP</title>
      <link href="/2019/05/26/mAP/"/>
      <url>/2019/05/26/mAP/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文介绍一种目标检测任务的评价指标<code>mean Average Precison(mAP)</code>，有以下几个重点</p><ul><li>如何绘制<code>Precision-Recall</code>曲线；</li><li>如何用插值方法计算<code>Average Precison</code>；</li><li>理解目标检测中<code>mAP</code>的计算；</li></ul><h1 id="Precision-Recall-Average-Precision"><a href="#Precision-Recall-Average-Precision" class="headerlink" title="Precision, Recall, Average Precision"></a>Precision, Recall, Average Precision</h1><h2 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h2><p>以上定义查看<a href="https://louishsu.xyz/2018/11/21/Metrics/" target="_blank" rel="noopener">Metrics/分类(classification)评估指标</a>，计算方式如下</p><h3 id="精确率-Precision"><a href="#精确率-Precision" class="headerlink" title="精确率(Precision)"></a>精确率<code>(Precision)</code></h3><p>即所有真实正样本中，被预测为正样本的比例</p><script type="math/tex; mode=display">P = \frac{TP}{TP + FN}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precision_score</span><span class="params">(gt, pred)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        gt:     &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">        pred:   &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">    Returns：</span></span><br><span class="line"><span class="string">        p:      &#123;float&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    index = pred == <span class="number">1</span></span><br><span class="line">    _gt = gt[index]</span><br><span class="line">    tp = _gt[_gt == <span class="number">1</span>].shape[<span class="number">0</span>]</span><br><span class="line">    pp = _gt.shape[<span class="number">0</span>]</span><br><span class="line">    p = tp / pp <span class="keyword">if</span> pp != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><h3 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率(Recall)"></a>召回率<code>(Recall)</code></h3><p>即所有预测的正样本中，真正为正样本的比例</p><script type="math/tex; mode=display">R = \frac{TP}{TP + NP}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recall_score</span><span class="params">(gt, pred)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        gt:     &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">        pred:   &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">    Returns：</span></span><br><span class="line"><span class="string">        r:      &#123;float&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    index = gt == <span class="number">1</span></span><br><span class="line">    _pred = pred[index]</span><br><span class="line">    tp = _pred[_pred == <span class="number">1</span>].shape[<span class="number">0</span>]</span><br><span class="line">    gp = _pred.shape[<span class="number">0</span>]</span><br><span class="line">    r = tp / gp <span class="keyword">if</span> gp != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure><h3 id="平均精确度-Average-Precision"><a href="#平均精确度-Average-Precision" class="headerlink" title="平均精确度(Average Precision)"></a>平均精确度<code>(Average Precision)</code></h3><p>依次设定不同的阈值，根据预测评分，得到不同的预测标签结果，那么就可以计算得到不同的$P$和$R$，做出$P-R$曲线，其与坐标轴面积即平均精确度。</p><h4 id="Average-Precision"><a href="#Average-Precision" class="headerlink" title="Average Precision"></a>Average Precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall.png" alt="2-class-precision-recall"></p><blockquote><p>该图来自<a href="https://arleyzhang.github.io/articles/c521a01c/" target="_blank" rel="noopener">目标检测评价标准-AP mAP</a>，侵删。</p></blockquote><p>那么AP计算公式为</p><script type="math/tex; mode=display">AP = \int_0^1 P(r) dr \tag{1}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        AP = \int_0^1 P(r) dr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = len(p)</span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        deltaR = r[i] - r[i<span class="number">-1</span>]</span><br><span class="line">        P_inter = (p[i] + p[i<span class="number">-1</span>]) / <span class="number">2</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 梯形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h4 id="Approximated-Average-Precision"><a href="#Approximated-Average-Precision" class="headerlink" title="Approximated Average Precision"></a>Approximated Average Precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall-approximated.png" alt="2-class-precision-recall-approximated"></p><p>注意图中折线上每个点代表一个样本，则将$(1)$离散化，累积<strong>每个样本点</strong>带来的面积变化</p><script type="math/tex; mode=display">AP_{approx} = \sum_{k=1}^N P(k) \Delta r(k) \tag{2}</script><p>其中$N$为样本总数，$k$为样本数，且</p><script type="math/tex; mode=display">\Delta r(k) = r(k) - r(k-1)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_approximated</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        AP_&#123;approx&#125; = \sum_&#123;k=1&#125;^N P(k) \Delta r(k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = len(p)</span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        deltaR = r[i] - r[i<span class="number">-1</span>]</span><br><span class="line">        P_inter = p[i]          <span class="comment"># 每个点处的Precision</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 矩形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h4 id="Interpolated-average-precision"><a href="#Interpolated-average-precision" class="headerlink" title="Interpolated average precision"></a>Interpolated average precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall-interpolated.png" alt="2-class-precision-recall-interpolated"></p><p>换一种插值方法计算$P(k)$</p><script type="math/tex; mode=display">P_{inter}(k) = \max_{\hat{k} \geq k} P(\hat{k})</script><script type="math/tex; mode=display">AP_{inter} = \sum_{k=1}^{N} P_{inter}(k) \Delta r(k) \tag{3}</script><p>其中$\hat{k}$为第$k$个样本点后的样本索引，也即，<strong>$P_{inter}(k)$为第$k$个样本点后，最大的<code>Precision</code>值</strong>。</p><p>因为正样本影响<code>Recall</code>阈值，故式$(3)$也可写作</p><script type="math/tex; mode=display">P_{inter}(k) = \max_{\hat{k} \geq k} P(\hat{k})</script><script type="math/tex; mode=display">AP_{inter} = \sum_{k=1}^{K} P_{inter}(k) \Delta r(k) \tag{4}</script><p>其中$K$表示正样本的个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_interpolated</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        P_&#123;inter&#125;(k) = \max_&#123;\hat&#123;k&#125; \geq k&#125; P(\hat&#123;k&#125;)</span></span><br><span class="line"><span class="string">        AP_&#123;inter&#125; = \sum_&#123;k=1&#125;^&#123;N&#125; P_&#123;inter&#125;(k) \Delta r(k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = len(p)</span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        deltaR = r[i] - r[i<span class="number">-1</span>]</span><br><span class="line">        P_inter = max(p[i:])    <span class="comment"># 每个点后最大的Precision</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 矩形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h4 id="11-points-Interpolated-Average-Precision"><a href="#11-points-Interpolated-Average-Precision" class="headerlink" title="11 points Interpolated Average Precision"></a>11 points Interpolated Average Precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall-11points.png" alt="2-class-precision-recall-11points"></p><p>固定选取$\{0, 0.1, \cdots, 1.0\}$ 11个<code>Recall</code>阈值，<strong>选取$P_{inter}(k)$为每个阈值点后最大<code>Precision</code>值</strong>，进行计算</p><script type="math/tex; mode=display">P_{inter}(k) = \max_{r(\hat{k}) \geq R(k)} P(\hat{k}), R \in \{0.0, 0.1, \cdots, 1.0\}</script><script type="math/tex; mode=display">AP_{inter} = \sum_{k=1}^{K} P_{inter}(k) \Delta r(k) \tag{4}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_11_points</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        P_&#123;inter&#125;(k) = \max_&#123;r(\hat&#123;k&#125;) \geq R(k)&#125; P(\hat&#123;k&#125;), R \in \&#123;0.0, 0.1, \cdots, 1.0\&#125;</span></span><br><span class="line"><span class="string">        AP_&#123;inter&#125; = \sum_&#123;k=1&#125;^&#123;K&#125; P_&#123;inter&#125;(k) \Delta r(k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    deltaR = <span class="number">0.1</span></span><br><span class="line">    p = np.array(p); r = np.array(r)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">11</span>):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">10</span>: </span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        R = i*deltaR</span><br><span class="line">        P_inter = np.max(p[r&gt;R])<span class="comment"># 每个阈值点后最大的Precision</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 矩形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>以一个二分类任务为例，假设我们得到样本真实标签与预测评分如下</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">Ground Truth</th><th style="text-align:center">Score</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.4</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0.7</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0.6</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0.45</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">1</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">1</td><td style="text-align:center">0.3</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">1</td><td style="text-align:center">0.6</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">1</td><td style="text-align:center">0.9</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">1</td><td style="text-align:center">0.8</td></tr></tbody></table></div><p>可依次设定阈值</p><script type="math/tex; mode=display">thresh = 0.2, 0.2, 0.3, 0.4, 0.45, 0.6, 0.6, 0.7, 0.8, 0.9</script><p>特别注意以下几个正样本Score对应阈值点，得表格如下</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">Ground Truth</th><th style="text-align:center">Score</th><th style="text-align:center">thresh=0.2</th><th style="text-align:center">thresh=0.3</th><th style="text-align:center">thresh=0.6</th><th style="text-align:center">thresh=0.8</th><th style="text-align:center">thresh=0.9</th><th style="text-align:center">thresh=1.0</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.4</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0.7</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0.6</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0.45</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">0.2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">1</td><td style="text-align:center">0.2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">1</td><td style="text-align:center">0.3</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">1</td><td style="text-align:center">0.6</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">1</td><td style="text-align:center">0.9</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">1</td><td style="text-align:center">0.8</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center"><strong>recall</strong></td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">1</td><td style="text-align:center">0.8</td><td style="text-align:center">0.6</td><td style="text-align:center">0.4</td><td style="text-align:center">0.2</td><td style="text-align:center">0.0</td></tr><tr><td style="text-align:center"><strong>precision</strong></td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">5/10</td><td style="text-align:center">4/8</td><td style="text-align:center">3/5</td><td style="text-align:center">2/2</td><td style="text-align:center">1/1</td><td style="text-align:center">1.0</td></tr></tbody></table></div><p>$P-R$曲线如下<br><img src="/2019/05/26/mAP/P-R.png" alt="P-R"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 准备数据</span></span><br><span class="line">gt = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=<span class="string">'int'</span>)</span><br><span class="line">pred = np.array([<span class="number">0.4</span>, <span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.45</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.9</span>, <span class="number">0.8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 作P-R曲线，每个点均需计算</span></span><br><span class="line">thresh = list(np.sort(pred))[::<span class="number">-1</span>]</span><br><span class="line">p = []; r = []</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> thresh:</span><br><span class="line">    label = score2label(pred, t)</span><br><span class="line">    n = pred[pred==t].shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        p += [precision_score(gt, label)]</span><br><span class="line">        r += [recall_score(gt, label)]</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="string">"P-R"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"recall"</span>); plt.ylabel(<span class="string">"precision"</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">1.2</span>])</span><br><span class="line">plt.grid()</span><br><span class="line">plt.plot(r, p)</span><br><span class="line">plt.scatter(r, p)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h1><p>对每一类计算<code>AP</code>，求其均值</p><script type="math/tex; mode=display">mAP = \frac{1}{C} \sum_{i=0}^{C} AP_i</script><h1 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h1><p>原文<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/devkit_doc.pdf" target="_blank" rel="noopener">devkit_doc/3.4.1</a>如下</p><blockquote><p>3.4.1 Average Precision (AP) The computation of the average precision (AP) measure was changed in 2010 to improve precision and ability to measure differences between methods with low AP. It is computed as follows: </p><ol><li>Compute a version of the measured precision/recall curve with precision monotonically decreasing, by setting the precision for recall r to the maximum precision obtained for any recall $r′ \geq r$.</li><li>Compute the AP as the area under this curve by numerical integration. No approximation is involved since the curve is piecewise constant.</li></ol><p>Note that prior to 2010 the AP is computed by sampling the monotonically decreasing curve at a fixed set of uniformly-spaced recall values $0, 0.1, 0.2, \cdots, 1.0$. By contrast, VOC2010–2012 effectively samples the curve at all unique recall values.</p></blockquote><p>记真实回归框为$B_{gt}$，预测回归框为$B_p$，则当预测框与真实框<code>IoU</code>大于阈值$0.5$时，认定物体被检测到，即</p><script type="math/tex; mode=display">IoU_{gt, p} = \frac{area(B_p \bigcap B_{gt})}{area(B_p \bigcup B_{gt})} \geq 0.5</script><p>其计算方法更加粗暴，计算每个<code>Recall</code>阈值点处最大<code>Precision</code>的均值</p><script type="math/tex; mode=display">P_{inter}(r) = \max_{\hat{r}: \hat{r} \geq r} P(\hat{r})</script><script type="math/tex; mode=display">AP_{voc} = \frac{1}{11} \sum_{r \in \{0.0, 0.1, \cdots, 1.0\}} P_{inter}(r) \tag{5}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_voc</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        P_&#123;inter&#125;(r) = \max_&#123;\hat&#123;r&#125;: \hat&#123;r&#125; \geq r&#125; P(\hat&#123;r&#125;)</span></span><br><span class="line"><span class="string">        AP_&#123;voc&#125; = \frac&#123;1&#125;&#123;11&#125; \sum_&#123;r \in \&#123;0.0, 0.1, \cdots, 1.0\&#125;&#125; P_&#123;inter&#125;(r)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    deltaR = <span class="number">0.1</span></span><br><span class="line">    p = np.array(p); r = np.array(r)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">11</span>):</span><br><span class="line">        R = i*deltaR</span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">10</span>: </span><br><span class="line">            P_inter = np.max(p[r&gt;R])<span class="comment"># 每个阈值点后最大的Precision</span></span><br><span class="line">        AP += P_inter           <span class="comment"># 矩形面积</span></span><br><span class="line">    AP = AP / <span class="number">11</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h1 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h1><p>COCO评估指标较多，共有12项，详细查看<a href="http://cocodataset.org/#detection-eval" target="_blank" rel="noopener">Detection Evaluation</a>，以下仅介绍其<code>AP</code>计算方法，原文如下。</p><blockquote><p>Average Precision (AP):</p><ol><li><p>primary challenge metric($AP$)</p><script type="math/tex; mode=display">IoU=.50:.05:.95</script></li><li><p>PASCAL VOC metric($AP^{IoU}=.50$)</p><script type="math/tex; mode=display">IoU=.50</script></li><li><p>strict metric($AP^{IoU}=.75$) </p><script type="math/tex; mode=display">IoU=.75</script></li></ol></blockquote><p>与<code>VOC</code>类似，但选取更多阈值的<code>IoU</code>，从中标记预测正确的样本进行计算。当$IoU = 0.5$时，即<code>VOC</code>计算方法。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/devkit_doc.pdf" target="_blank" rel="noopener">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</a></li><li><a href="https://arleyzhang.github.io/articles/c521a01c/" target="_blank" rel="noopener">目标检测评价标准-AP mAP</a></li><li><a href="https://www.zhihu.com/question/41540197" target="_blank" rel="noopener">mean average precision（MAP）在计算机视觉中是如何计算和应用的？</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NMS &amp; soft-NMS</title>
      <link href="/2019/05/26/NMS-softer-NMS/"/>
      <url>/2019/05/26/NMS-softer-NMS/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在目标检测中，若多个回归框<code>(Bounding Box)</code>重叠内容较多，则可以保留其中几个，删除其余冗余的框，非极大值抑制<code>(Non-Maximum Suppression, NMS)</code>算法可实现该功能。</p><p>那么用什么指标评价两个回归框重叠过多呢，以下介绍图像的交并比<code>IoU(Intersection over Union)</code>。</p><h1 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><code>IoU</code>的思路非常简单，即计算两个回归框交集部分面积与并集面积的比值，如下图，其计算公式为</p><script type="math/tex; mode=display">IoU_{a, b} = \frac{Area_{inter}}{Area_{union}}</script><p>其中</p><script type="math/tex; mode=display">Area_{union} = Area_{a} + Area_{b} - Area_{inter}</script><script type="math/tex; mode=display">Area_{inter} = w_{inter}*h_{inter}</script><script type="math/tex; mode=display">w_{inter} = x^a_2 - x^b_1</script><script type="math/tex; mode=display">h_{inter} = y^a_2 - y^b_1</script><p><img src="/2019/05/26/NMS-softer-NMS/IoU1.jpg" alt="iou1"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>算法实现中，比较关键的一点是计算交集部分的坐标点，记两个回归框$a, b$左上角坐标为$(x^<em>_1, y^</em>_1)$，右下角为$(x^<em>_2, y^</em>_2)$</p><p>则应有</p><script type="math/tex; mode=display">x^{inter}_1 = \max \{x^a_1, x^b_1\}, y^{inter}_1 = \max \{y^a_1, y^b_1\}</script><script type="math/tex; mode=display">x^{inter}_2 = \min \{x^a_2, x^b_2\}, y^{inter}_2 = \min \{y^a_2, y^b_2\}</script><p><img src="/2019/05/26/NMS-softer-NMS/IoU2.jpg" alt="IoU2"></p><p>使用C/C++实现如下<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="keyword">float</span> x1, y1, x2, y2;</span><br><span class="line">&#125;bbox;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_overlap</span><span class="params">(<span class="keyword">float</span> ax1, <span class="keyword">float</span> ax2, <span class="keyword">float</span> bx1, <span class="keyword">float</span> bx2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> left  = _max(ax1, bx1);</span><br><span class="line">    <span class="keyword">float</span> right = _min(ax2, bx2);</span><br><span class="line">    <span class="keyword">float</span> gap = right - left;</span><br><span class="line">    <span class="keyword">return</span> gap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_intersection</span><span class="params">(bbox a, bbox b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> w = bbox_overlap(a.x1, a.x2, b.x1, b.x2);</span><br><span class="line">    <span class="keyword">float</span> h = bbox_overlap(a.y1, a.y2, b.y1, b.y2);</span><br><span class="line">    <span class="keyword">if</span>(w &lt; <span class="number">0</span> || h &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> area = w*h;</span><br><span class="line">    <span class="keyword">return</span> area;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_area</span><span class="params">(bbox a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> w = a.x2 - a.x1;</span><br><span class="line">    <span class="keyword">float</span> h = a.y2 - a.y1;</span><br><span class="line">    <span class="keyword">return</span> w*h;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_union</span><span class="params">(bbox a, bbox b, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> u = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">float</span> area_a = bbox_area(a);</span><br><span class="line">    <span class="keyword">float</span> area_b = bbox_area(b);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mode == <span class="number">0</span>)&#123;</span><br><span class="line">        u = area_a + area_b - bbox_intersection(a, b);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (mode == <span class="number">1</span>)&#123;</span><br><span class="line">        u = _min(area_a, area_b);</span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> u;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_iou</span><span class="params">(bbox a, bbox b, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> i = bbox_intersection(a, b);</span><br><span class="line">    <span class="keyword">float</span> u = bbox_union(a, b, mode);</span><br><span class="line">    <span class="keyword">return</span> i/u;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a>NMS</h1><p>以下图为例，其中三个候选框$a,b,c$，其评分依次为0.8, 0.7, 0.9，设定<code>IoU</code>阈值</p><script type="math/tex; mode=display">thresh=0.4</script><p>计算步骤如下<br><img src="/2019/05/26/NMS-softer-NMS/NMS1.jpg" alt="NMS1"></p><ol><li>将其排序，如降序排序， 得到结果$c,a,b$；</li><li>保存当前评分最高的回归框，即$c$；</li><li>计算$c$与剩余框，即$a,b$的<code>IoU</code>，即<script type="math/tex; mode=display">IoU_{c,a} = \frac{1×8}{10×9 + 9×11 - 1×8} = 0.044</script><script type="math/tex; mode=display">IoU_{c,b} = \frac{4×6}{10×9 + 10×11 - 4×6} = 0.136</script></li><li>无大于阈值的框，故无框被删除；</li><li>保存当前评分最高的回归框，即$a$；</li><li>计算$a$与剩余框即$b$的<code>IoU</code>，即<script type="math/tex; mode=display">IoU_{a,b} = \frac{7×9}{9×11 + 10×11 - 7×9} = 0.432</script></li><li>大于阈值，故删除$b$；</li><li>最终保留框$a,c$；</li></ol><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><ol><li><p>Python</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_nms</span><span class="params">(dets, thresh, mode=<span class="string">"Union"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        dets:   &#123;ndarray(n_boxes, 5)&#125; x1, y1, x2, y2 score</span></span><br><span class="line"><span class="string">        thresh: &#123;float&#125; retain overlap &lt;= thresh</span></span><br><span class="line"><span class="string">        mode:   &#123;str&#125; 'Union' or 'Minimum'</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        idx:   &#123;list[int]&#125; indexes to keep</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        greedily select boxes with high confidence</span></span><br><span class="line"><span class="string">        idx boxes overlap &lt;= thresh</span></span><br><span class="line"><span class="string">        rule out overlap &gt; thresh</span></span><br><span class="line"><span class="string">        if thresh==1.0, keep all</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x1 = dets[:, <span class="number">0</span>]</span><br><span class="line">    y1 = dets[:, <span class="number">1</span>]</span><br><span class="line">    x2 = dets[:, <span class="number">2</span>]</span><br><span class="line">    y2 = dets[:, <span class="number">3</span>]</span><br><span class="line">    scores = dets[:, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line">    order = scores.argsort()[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    idx = []</span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</span><br><span class="line">        i = order[<span class="number">0</span>]</span><br><span class="line">        idx.append(i)</span><br><span class="line"></span><br><span class="line">        xx1 = np.maximum(x1[i], x1[order[<span class="number">1</span>:]])</span><br><span class="line">        yy1 = np.maximum(y1[i], y1[order[<span class="number">1</span>:]])</span><br><span class="line">        xx2 = np.minimum(x2[i], x2[order[<span class="number">1</span>:]])</span><br><span class="line">        yy2 = np.minimum(y2[i], y2[order[<span class="number">1</span>:]])</span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)</span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        inter = w * h</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"Union"</span>:</span><br><span class="line">            ovr = inter / (areas[i] + areas[order[<span class="number">1</span>:]] - inter)</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">"Minimum"</span>:</span><br><span class="line">            ovr = inter / np.minimum(areas[i], areas[order[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">        inds = np.where(ovr &lt;= thresh)[<span class="number">0</span>]</span><br><span class="line">        order = order[inds + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure></li><li><p>C/C++</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">detect</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">float</span> score;    <span class="comment">/* 该框评分 */</span></span><br><span class="line">    bbox bx;        <span class="comment">/* 回归方框 */</span></span><br><span class="line">    bbox offset;    <span class="comment">/* 偏置 */</span></span><br><span class="line">    landmark mk;    <span class="comment">/* 位置 */</span></span><br><span class="line">&#125; detect;</span><br><span class="line"></span><br><span class="line"><span class="comment">// decending order, bubble</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bsort</span><span class="params">(detect** dets, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i++ )&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n - <span class="number">1</span> - i; j++ )&#123;</span><br><span class="line">            <span class="keyword">float</span> a = (*dets)[j].score;</span><br><span class="line">            <span class="keyword">float</span> b = (*dets)[j+<span class="number">1</span>].score;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (a &lt; b)&#123;</span><br><span class="line">                detect tmp = (*dets)[j];</span><br><span class="line">                (*dets)[j] = (*dets)[j+<span class="number">1</span>];</span><br><span class="line">                (*dets)[j+<span class="number">1</span>] = tmp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> _nms(detect* dets, <span class="keyword">int</span> n, <span class="keyword">float</span> thresh, <span class="keyword">int</span> mode)</span><br><span class="line">&#123;</span><br><span class="line">    bsort(&amp;dets, n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (dets[i].score == <span class="number">0</span>) <span class="keyword">continue</span>;   <span class="comment">// 表示该框已被删除</span></span><br><span class="line">        bbox a = dets[i].bx;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            bbox b = dets[j].bx;</span><br><span class="line">            <span class="keyword">if</span> (bbox_iou(a, b, mode) &gt; thresh)</span><br><span class="line">                dets[j].score = <span class="number">0</span>;          <span class="comment">// 删除该框</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h1 id="Soft-NMS"><a href="#Soft-NMS" class="headerlink" title="Soft-NMS"></a>Soft-NMS</h1><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>对于两个相邻较近的同类物体，如下图，<code>NMS</code>可能将左框删除，这是由于在删除同类别框时，只考虑了<code>IoU</code>重叠大小，并没有将两个框的评分引入计算，当两个框相邻很近但评分都较高时，不能简单地删除。</p><p><img src="/2019/05/26/NMS-softer-NMS/soft-NMS1.jpg" alt="soft-NMS1"></p><p>改进后的<code>soft-NMS</code>伪代码如下<br><img src="/2019/05/26/NMS-softer-NMS/NMS2.jpg" alt="NMS2"></p><p>也可统一写作</p><script type="math/tex; mode=display">s_i \leftarrow s_i f(iou(\mathcal{M}, b_i))</script><p>其中<code>NMS</code>算法引入<code>hard threshold</code>，即</p><script type="math/tex; mode=display">f(iou(\mathcal{M}, b_i)) = \begin{cases}    1 & iou(\mathcal{M}, b_i) < N_t \\    0 & iou(\mathcal{M}, b_i) \geq N_t\end{cases}</script><p><img src="/2019/05/26/NMS-softer-NMS/f1.png" alt="f1"></p><p>应考虑以下因素</p><ul><li>相邻检测的分数应该降低到它们具有增加假阳性率<code>(false positive rate)</code>的可能性较小的程度，同时在排序的检测列表中高于明显的假阳性。</li><li>完全去除具有低NMS阈值的相邻检测将是次优的并且当在高重叠阈值处执行评估时将增加未命中率。</li><li>当使用高NMS阈值时，在一系列重叠阈值上测量的平均精度将下降。</li></ul><p>基于以上分析，改进$f(iou(\mathcal{M}, b_i))$为</p><script type="math/tex; mode=display">f(iou(\mathcal{M}, b_i)) = \begin{cases}    1 & iou(\mathcal{M}, b_i) < N_t \\    1 - iou(\mathcal{M}, b_i) & iou(\mathcal{M}, b_i) \geq N_t\end{cases}</script><p>其函数图像如下，在超过阈值时，为线性函数，且重叠越多，其抑制效果越大或称惩罚越多<br><img src="/2019/05/26/NMS-softer-NMS/f2.png" alt="f2"></p><p>但该函数不连续，可能导致NMS结果的突然改变，故修改为非线性连续函数如下</p><script type="math/tex; mode=display">f(iou(\mathcal{M}, b_i)) = \exp (-\frac{iou(\mathcal{M}, b_i)^2}{\sigma}), \forall b_i \notin \mathcal{D}</script><blockquote><p>注意到$x = \sqrt{\frac{\sigma}{2}}$为函数$f(x)$的拐点</p></blockquote><p><img src="/2019/05/26/NMS-softer-NMS/f3.png" alt="f3"></p><h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><p>对上面代码作如下修改</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> _f(<span class="keyword">float</span> x, <span class="keyword">float</span> sigma, <span class="keyword">int</span> soft)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">float</span> y = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (soft == <span class="number">0</span>)&#123;</span><br><span class="line">        y = x &lt; sigma? <span class="number">1</span>: <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        y = <span class="built_in">exp</span>(- <span class="built_in">pow</span>(x, <span class="number">2</span>) / sigma);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> _nms(detect* dets, <span class="keyword">int</span> n, <span class="keyword">float</span> sigma, <span class="keyword">int</span> mode, <span class="keyword">int</span> soft)</span><br><span class="line">&#123;</span><br><span class="line">    bsort(&amp;dets, n);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do nms</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (dets[i].score == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">        bbox a = dets[i].bx;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            bbox b = dets[j].bx;</span><br><span class="line">            dets[j].score *= _f(bbox_iou(a, b, mode), sigma, soft);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://ieeexplore.ieee.org/abstract/document/1699659" target="_blank" rel="noopener">Efficient Non-Maximum Suppression</a></li><li><a href="https://zhuanlan.zhihu.com/p/37489043" target="_blank" rel="noopener">非极大值抑制(Non-Maximum Suppression)</a></li><li><a href="https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/" target="_blank" rel="noopener">Non-Maximum Suppression for Object Detection in Python</a></li><li><a href="https://arxiv.org/abs/1704.04503" target="_blank" rel="noopener">Improving Object Detection With One Line of Code</a></li><li><a href="https://blog.csdn.net/u014380165/article/details/79502197" target="_blank" rel="noopener">Soft NMS算法笔记</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github清除commit记录</title>
      <link href="/2019/05/22/Github%E6%B8%85%E9%99%A4commit%E8%AE%B0%E5%BD%95/"/>
      <url>/2019/05/22/Github%E6%B8%85%E9%99%A4commit%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>当commit记录过多时，仓库会过大难以下载，本文介绍删除commit记录的方法。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>新建无任何文件的分支</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout --orphan new</span><br></pre></td></tr></table></figure></li><li><p>添加文件并提交</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add -A</span><br><span class="line">git commit -am "recommit"</span><br></pre></td></tr></table></figure></li><li><p>删除主分支</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -D master</span><br></pre></td></tr></table></figure></li><li><p>重命名当前分支</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m master</span><br></pre></td></tr></table></figure></li><li><p>强制更新远程仓库</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -f origin master</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> github </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Install nVidia drivers on Ubuntu</title>
      <link href="/2019/05/08/Install-nVidia-drivers-on-Ubuntu/"/>
      <url>/2019/05/08/Install-nVidia-drivers-on-Ubuntu/</url>
      
        <content type="html"><![CDATA[<h1 id="PPA安装"><a href="#PPA安装" class="headerlink" title="PPA安装"></a>PPA安装</h1><ol><li>禁用<code>nouveau</code>驱动<br>先将<code>Ubuntu</code>系统集成的显卡驱动程序<code>nouveau</code>从<code>linux</code>内核卸载</li></ol><p>查看当前驱动状态<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> lsmod | grep nouveau</span><br><span class="line">nouveau              1851392  1</span><br><span class="line">mxm_wmi                16384  1 nouveau</span><br><span class="line">i2c_algo_bit           16384  2 i915,nouveau</span><br><span class="line">ttm                   110592  1 nouveau</span><br><span class="line">drm_kms_helper        172032  2 i915,nouveau</span><br><span class="line">drm                   458752  8 drm_kms_helper,i915,ttm,nouveau</span><br><span class="line">wmi                    24576  3 wmi_bmof,mxm_wmi,nouveau</span><br><span class="line">video                  45056  3 thinkpad_acpi,i915,nouveau</span><br></pre></td></tr></table></figure></p><p>添加黑名单<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ll /etc/modprobe.d/blacklist.conf</span><br><span class="line">-rw-r--r-- 1 root root 1667 11月 13 05:54 /etc/modprobe.d/blacklist.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo chmod 666 /etc/modprobe.d/blacklist.conf</span><br><span class="line">[sudo] password for louishsu: </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo vim /etc/modprobe.d/blacklist.conf </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo chmod 644 /etc/modprobe.d/blacklist.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo update-initramfs -u</span><br><span class="line">update-initramfs: Generating /boot/initrd.img-4.18.0-17-generic</span><br><span class="line">...</span><br><span class="line">I: Set the RESUME variable to override this.</span><br></pre></td></tr></table></figure></p><p>重启后查看驱动状态，无输出表示禁用成功<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> lsmod | grep nouveau</span><br></pre></td></tr></table></figure></p><ol><li>安装驱动</li></ol><p>这里使用<code>PPA</code>方式安装，首先添加源<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line"><span class="meta">$</span> sudo apt-get update</span><br></pre></td></tr></table></figure></p><p>查看合适的驱动版本，如下<code>recommended</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ubuntu-drivers devices</span><br></pre></td></tr></table></figure></p><p>按<code>ctrl+alt+F1</code>进入<code>tty</code>模式，关闭图形桌面显示管理器<code>LightDM</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> service lightdm stop</span><br></pre></td></tr></table></figure></p><p>安装驱动<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo apt-get install nvidia-418</span><br><span class="line"><span class="meta">$</span> sudo reboot</span><br></pre></td></tr></table></figure></p><ol><li>查看安装情况<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo nvidia-smi</span><br><span class="line">Wed May  8 20:22:55 2019       </span><br><span class="line">+------------------------------------------------------+                       </span><br><span class="line">| NVIDIA-SMI 340.107    Driver Version: 340.107        |                       </span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GT 730M     Off  | 0000:04:00.0     N/A |                  N/A |</span><br><span class="line">| N/A   46C    P0    N/A /  N/A |    185MiB /  1023MiB |     N/A      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Compute processes:                                               GPU Memory |</span><br><span class="line">|  GPU       PID  Process name                                     Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    0            Not Supported                                               |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo nvidia-settings</span><br></pre></td></tr></table></figure></li></ol><h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><p>查看<a href="https://louishsu.xyz/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/" target="_blank" rel="noopener">Ubuntu编译安装Tensorflow</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://blog.csdn.net/10km/article/details/61191230" target="_blank" rel="noopener">ubuntu16.04下NVIDIA GTX965M显卡驱动PPA安装</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Face Detection: MTCNN</title>
      <link href="/2019/05/05/Face-Detection-MTCNN/"/>
      <url>/2019/05/05/Face-Detection-MTCNN/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>MTCNN即Multi-task Cascaded Convolutional Networks，利用深度学习方法进行人脸识别、检测与关键点定位，可谓结合机器视觉领域三大任务为一体。</p><p>该算法中，人脸检测与识别视作分类任务，即判别框内图像是否包含人脸；定位视作回归任务，共需确定7个坐标点，依次为：回归框左上、右下坐标，左眼、右眼、鼻尖、左嘴角、右嘴角坐标。</p><p>在设计损失函数时，分类任务采用交叉熵<code>(Cross Entropy)</code>，回归任务采用均方误差<code>(MSE)</code>，并且三个任务的损失，可给定不同的系数进行网络训练，使各网络侧重点不同，即<code>PNet</code>与<code>RNet</code>侧重于人脸识别与回归框定位，<code>ONet</code>侧重于关键点定位。</p><h1 id="算法对比"><a href="#算法对比" class="headerlink" title="算法对比"></a>算法对比</h1><p><img src="/2019/05/05/Face-Detection-MTCNN/detect_algorithm.jpg" alt="detect_algorithm"></p><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>共设计3个卷积网络，每个网络均可输出识别概率(1)、回归框坐标(2×2)、关键点坐标(5×2)，三个网络级联以获得良好的预测结果。各网络结构图如下</p><p><img src="/2019/05/05/Face-Detection-MTCNN/mtcnn.png" alt="mtcnn"></p><p>分类任务输出层可采用<code>softmax</code>，即视作多分类任务；或者采用<code>sigmoid</code>，视作二分类任务。</p><h2 id="1-P-Net：-Proposal-Network"><a href="#1-P-Net：-Proposal-Network" class="headerlink" title="1. P-Net： Proposal Network"></a>1. P-Net： Proposal Network</h2><p>检测任务比较重要的一步是产生数目足够多的候选框，<code>PNet</code>设计全卷积网络，可接受任意大小的图片输入，利用输出的特征图生成候选框，具体生成算法在检测算法中说明。该网络在训练时接受$12×12×3$的图像输入，各层参数如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size           input                output</span><br><span class="line">0 conv      10      3 x 3 / 1    12 x  12 x   3   -&gt;    10 x  10 x  10  0.000 BFLOPs</span><br><span class="line">1 max               2 x 2 / 2    10 x  10 x  10   -&gt;     5 x   5 x  10</span><br><span class="line">2 conv      16      3 x 3 / 1     5 x   5 x  10   -&gt;     3 x   3 x  16  0.000 BFLOPs</span><br><span class="line">3 conv      32      3 x 3 / 1     3 x   3 x  16   -&gt;     1 x   1 x  32  0.000 BFLOPs</span><br><span class="line">4 conv      15      1 x 1 / 1     1 x   1 x  32   -&gt;     1 x   1 x  15  0.000 BFLOPs</span><br></pre></td></tr></table></figure><h2 id="2-R-Net：-Refine-Network"><a href="#2-R-Net：-Refine-Network" class="headerlink" title="2. R-Net： Refine Network"></a>2. R-Net： Refine Network</h2><p><code>PNet</code>产生候选框后，将这些候选框内的图像数据分割并缩放到统一大小，输入到<code>RNet</code>改善识别结果。该网络最后增加全连接层，仅接受$24×24×3$的图像输入，各层参数如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size           input                output</span><br><span class="line">0 conv     28       3 x 3 / 1    24 x  24 x   3   -&gt;    22 x  22 x  28  0.001 BFLOPs</span><br><span class="line">1 max               3 x 3 / 2    22 x  22 x  28   -&gt;    11 x  11 x  28</span><br><span class="line">2 conv     48       3 x 3 / 1    11 x  11 x  28   -&gt;     9 x   9 x  48  0.002 BFLOPs</span><br><span class="line">3 max               3 x 3 / 2     9 x   9 x  48   -&gt;     4 x   4 x  48</span><br><span class="line">4 conv     64       2 x 2 / 1     4 x   4 x  48   -&gt;     3 x   3 x  64  0.000 BFLOPs</span><br><span class="line">5 connected                                 576   -&gt;               128</span><br><span class="line">6 connected                                 128   -&gt;                15</span><br></pre></td></tr></table></figure><h2 id="3-O-Net：-Output-Network"><a href="#3-O-Net：-Output-Network" class="headerlink" title="3. O-Net： Output Network"></a>3. O-Net： Output Network</h2><p>该网络功能与<code>RNet</code>相同，不同的是分辨率更高，网络层次更深，且训练过程中，损失的设置更偏重于关键点的回归。接受$48×48×3$的图像输入，各层参数如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size              input                output</span><br><span class="line">0 conv     32       3 x 3 / 1    48 x  48 x   3   -&gt;    46 x  46 x  32  0.004 BFLOPs</span><br><span class="line">1 max               3 x 3 / 2    46 x  46 x  32   -&gt;    23 x  23 x  32</span><br><span class="line">2 conv     64       3 x 3 / 1    23 x  23 x  32   -&gt;    21 x  21 x  64  0.016 BFLOPs</span><br><span class="line">3 max               3 x 3 / 2    21 x  21 x  64   -&gt;    10 x  10 x  64</span><br><span class="line">4 conv     64       3 x 3 / 1    10 x  10 x  64   -&gt;     8 x   8 x  64  0.005 BFLOPs</span><br><span class="line">5 max               2 x 2 / 2     8 x   8 x  64   -&gt;     4 x   4 x  64</span><br><span class="line">6 conv    128       2 x 2 / 1     4 x   4 x  64   -&gt;     3 x   3 x 128  0.001 BFLOPs</span><br><span class="line">7 connected                                1152   -&gt;               256</span><br><span class="line">8 connected                                 256   -&gt;                15</span><br></pre></td></tr></table></figure><h1 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h1><h2 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1. 数据集"></a>1. 数据集</h2><ul><li><p>人脸检测<br>  <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" rel="noopener">WIDER FACE</a>是目前最常用的训练集，也是目前最大的公开训练集，人工标注的风格比较友好，适合训练。总共32203图像，393703标注人脸，目前难度最大，各种难点比较全面：尺度，姿态，遮挡，表情，化妆，光照等。</p><p>  <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" rel="noopener">WIDER FACE</a>有以下特点：</p><ul><li>图像分辨率普遍偏高，所有图像的宽都缩放到1024，最小标注人脸10*10，都是彩色图像；</li><li>每张图像的人脸数据偏多，平均12.2人脸/图，密集小人脸非常多；</li><li>分训练集train/验证集val/测试集test，分别占40%/10%/50%，而且测试集的标注结果(ground truth)没有公开，需要提交结果给官方比较，更加公平公正，而且测试集非常大，结果可靠性极高；</li><li>根据EdgeBox的检测率情况划分为三个难度等级：Easy, Medium, Hard。</li></ul></li></ul><p><img src="/2019/05/05/Face-Detection-MTCNN/detect_demo.png" alt="detect_demo"></p><ul><li><p>关键点定位<br>  <a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm" target="_blank" rel="noopener">CNN FACE POINT</a>，包含5个关键点位置。</p><blockquote><p>Training set: <a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN/data/train.zip" target="_blank" rel="noopener">Download</a><br>  It contains 5,590 LFW images and 7,876 other images downloaded from the web. The training set and validation set are defined in trainImageList.txt and testImageList.txt, respectively. Each line of these text files starts with the image name, followed by the boundary positions of the face bounding box retured by our face detector, then followed by the positions of the five facial points.<br>Testing set: <a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN/data/test.zip" target="_blank" rel="noopener">Download</a><br>  It contains the 1,521 BioID images, 781 LFPW training images, and 249 LFPW test images used in our testing, together with the text files recording the boundary positions of the face bounding box retured by our face detector for each dataset. A few images that our face detector failed are not listed in the text files. LFPW images are renamed for the convenience of processing.</p></blockquote><p>  <img src="/2019/05/05/Face-Detection-MTCNN/landmark_demo.png" alt="landmark_demo"></p></li></ul><h2 id="2-训练数据生成"><a href="#2-训练数据生成" class="headerlink" title="2. 训练数据生成"></a>2. 训练数据生成</h2><p>采用了<code>Hard Example Mining</code>，后一网络的训练数据由前一网络结果生成，即先用训练好的前一网络进行数据评估，在评分较低、难以检测的数据中继续采样。在生成数据时，使用数据增广。</p><p>七个关键点坐标转换为偏移量<code>(offset)</code>，使其不受图像缩放影响，且数值较小便于网络收敛，计算方法如下</p><p><img src="/2019/05/05/Face-Detection-MTCNN/offsets.jpg" alt="offsets"></p><ul><li><p>Bounding Box</p><script type="math/tex; mode=display">  offset_{x_1'} = \frac{x_1' - x_1}{size}</script><script type="math/tex; mode=display">  offset_{x_2'} = \frac{x_2' - x_2}{size}</script><ul><li>$y_n$同$x_n$;</li><li>$(x_1, y_1)$表示左上角点位置，$(x_2, y_2)$表示右下角点位置;</li><li>$(x_1’, y_1’)$表示<code>ground true</code>矩形方框位置;</li><li>$size$表示方形回归框边长，即$size = x_2 - x_1 = y_2 - y_1$;</li></ul></li><li><p>Landmark<br>  均以<strong>正方形回归框左上方点</strong>坐标作为基准</p><script type="math/tex; mode=display">  offset_{x_n''} = \frac{x_n'' - x_1}{size}</script><script type="math/tex; mode=display">  offset_{y_n''} = \frac{y_n'' - y_1}{size}</script><ul><li>$(x_n’’, y_n’’)$表示五个关键点坐标;</li><li>$(x_1’, y_1’)$表示回归框左上角点位置;</li><li>$size$表示方形回归框边长，即$size = x_2 - x_1 = y_2 - y_1$;</li></ul></li></ul><p>根据<code>groudtruth</code>随机偏移和旋转，切割人脸数据，根据<code>iou</code>评分，共记作3种标签的样本，其标签分别为</p><ul><li><p><code>Positive(1)</code>:<br>  $iou &gt; 0.65$，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/positive/404031.jpg 1.0 0.18 -0.09 0.15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br></pre></td></tr></table></figure></li><li><p><code>Negative(0)</code>:<br>  $iou &lt; 0.3$，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/negative/92073.jpg 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br></pre></td></tr></table></figure></li><li><p><code>Part(-1)</code>:<br>  $0.4 \leq iou \leq 0.65$，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/part/749569.jpg -1.0 -0.04 0.07 -0.15 0.36 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br></pre></td></tr></table></figure></li><li><p>此外，关键点数据标签记作<code>Landmark(-2)</code>，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/train_PNet_landmark_aug/4.jpg -2.0 0.0 0.0 0.0 0.0 0.24367088607594936 0.17405063291139242 0.7563291139240507 0.2310126582278481 0.48417721518987344 0.6170886075949367 0.2310126582278481 0.8069620253164557 0.6677215189873418 0.8575949367088608</span><br></pre></td></tr></table></figure></li></ul><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="1-Classification-loss"><a href="#1-Classification-loss" class="headerlink" title="1. Classification loss"></a>1. Classification loss</h2><p>人脸识别为分类任务，采用二分类交叉熵损失函数，即</p><script type="math/tex; mode=display">L^{(i)}_{cls} = - y^{(i)} \log (p^{(i)}) - (1 - y^{(i)}) \log (1 - p^{(i)})</script><p>注意，在计算分类损失时，将<code>Negative(0)</code>, <code>Part(-1)</code>, <code>Landmark(-2)</code>标签均视作<code>0</code>。</p><h2 id="2-Regression-loss"><a href="#2-Regression-loss" class="headerlink" title="2. Regression loss"></a>2. Regression loss</h2><p>回归任务，采用最小方差准则，即</p><script type="math/tex; mode=display">L^{(i)}_{bbox} = \frac{1}{4} \sum_{j=1}^4 (pred^{(i)}_j - gt^{(i)}_j)^2</script><script type="math/tex; mode=display">L^{(i)}_{landmark} = \frac{1}{10} \sum_{j=1}^{10} (pred^{(i)}_j - gt^{(i)}_j)^2</script><p>注意，在计算回归框损失时，仅对<code>Positive(1)</code>, <code>Part(-1)</code>样本进行，计算关键点损失时，仅对<code>Landmark(-2)</code>样本进行。</p><h2 id="3-OHEM"><a href="#3-OHEM" class="headerlink" title="3. OHEM"></a>3. OHEM</h2><p><code>OHEM</code>即<code>Online Hard Example Mining</code>，在线硬样本数据挖掘，即对于当前批次数据，计算损失时，选取总损失值最大的<code>k</code>个样本，计算其均值作为该批次的损失值。</p><h2 id="4-Total-Loss"><a href="#4-Total-Loss" class="headerlink" title="4. Total Loss"></a>4. Total Loss</h2><p>三个网络赋予各损失的系数不同， 使其偏重于其中某个任务</p><script type="math/tex; mode=display">L_{total} = \frac{1}{topk} \sum_{i=0}^{topk} coef_{cls} L^{(i)}_{cls} + coef_{bbox} L^{(i)}_{bbox} + coef_{landmark} L^{(i)}_{landmark}</script><p>其中$topk$即<code>OHEM</code>计算得到的样本数。</p><details><summary>PyTorch实现</summary><pre><code>class MtcnnLoss(nn.Module):    def __init__(self, cls, bbox, landmark, ohem=0.7):        super(MtcnnLoss, self).__init__()        self.cls = cls        self.bbox = bbox        self.landmark = landmark        self.ohem = ohem        self.bce = nn.BCEWithLogitsLoss(reduction='none')        self.mse = nn.MSELoss(reduction='none')    def forward(self, pred, gt):        """        Params:            pred:   {tensor(N, n) or tensor(N, n, 1, 1)}            gt:     {tensor(N, n)}        Notes:            y_true        """        N = pred.shape[0]        pred = pred.view(N, -1)        ## origin label        gt_labels = gt[:, 0]        ## pos -> 1, neg -> 0, others -> 0        pred_cls = pred[:, 0]        gt_cls = gt_labels.clone(); gt_cls[gt_labels!=1.0] = 0.0        loss_cls = self.bce(pred_cls, gt_cls)        # ohem        n_keep = int(self.ohem * loss_cls.shape[0])        loss_cls = torch.mean(torch.topk(loss_cls, n_keep)[0])        ## label=1 or label=-1 then do regression        idx = (gt_labels==1)^(gt_labels==-1)        pred_bbox = pred[idx, 1: 5]        gt_bbox = gt[idx, 1: 5]        loss_bbox = self.mse(pred_bbox, gt_bbox)        loss_bbox = torch.mean(loss_bbox, dim=1)        # ohem        n_keep = int(self.ohem * loss_bbox.shape[0])        loss_bbox = torch.mean(torch.topk(loss_bbox, n_keep)[0])            ## keep label =-2  then do landmark detection        idx = gt_labels==-2        pred_landmark = pred[idx, 5:]        gt_landmark = gt[idx, 5:]        loss_landmark = self.mse(pred_landmark, gt_landmark)        loss_landmark = torch.mean(loss_landmark, dim=1)        # ohem        n_keep = int(self.ohem * loss_landmark.shape[0])        loss_landmark = torch.mean(torch.topk(loss_landmark, n_keep)[0])        ## total loss        loss_total = self.cls*loss_cls + self.bbox*loss_bbox + self.landmark*loss_landmark        return loss_total, loss_cls, loss_bbox, loss_landmarkloss_coef = {    'PNet': [1.0, 0.5, 0.5],    'RNet': [1.0, 0.5, 0.5],    'ONet': [1.0, 0.5, 1.0],}</code></pre></details><h1 id="检测算法"><a href="#检测算法" class="headerlink" title="检测算法"></a>检测算法</h1><p>检测算法以功能区分，主要分成两个部分：候选框生成与候选框筛除。</p><h2 id="1-候选框生成"><a href="#1-候选框生成" class="headerlink" title="1. 候选框生成"></a>1. 候选框生成</h2><p><img src="/2019/05/05/Face-Detection-MTCNN/pnet1.jpg" alt="pnet1"></p><p><img src="/2019/05/05/Face-Detection-MTCNN/pnet2.jpg" alt="pnet2"></p><p><img src="/2019/05/05/Face-Detection-MTCNN/pnet_gen_box.jpg" alt="pnet_gen_box"></p><p>该步骤使用的网络为<code>PNet</code>。指定超参数<code>minface</code>，对于任意尺寸输入的图像<code>H × W</code>，先将其缩放</p><script type="math/tex; mode=display">H_c = H × \frac{12}{minface}</script><script type="math/tex; mode=display">W_c = W × \frac{12}{minface}</script><p>指定超参数<code>factor</code>，更新缩小尺度，将图片缩小，在每个尺度上进行计算，即</p><script type="math/tex; mode=display">H_c := H_c × factor</script><script type="math/tex; mode=display">W_c := W_c × factor</script><p>对于某一尺度下的图片得到的运算特征图，</p><script type="math/tex; mode=display">Feat_{h×w×15} = PNet(input)</script><p>提取其分类层输出特征图$Feat_{cls}$，设定阈值<code>thresh</code>，对于大于阈值的点，按下式生成候选框</p><script type="math/tex; mode=display">x_1 = stride × i × \frac{1}{scale}</script><script type="math/tex; mode=display">y_1 = stride × j × \frac{1}{scale}</script><script type="math/tex; mode=display">x_2 = x_1 + \frac{cellsize}{scale}</script><script type="math/tex; mode=display">y_2 = y_1 + \frac{cellsize}{scale}</script><p>其中<code>cellsize</code>为超参数，一般指定为<code>cellsize=12</code></p><h2 id="2-候选框筛除"><a href="#2-候选框筛除" class="headerlink" title="2. 候选框筛除"></a>2. 候选框筛除</h2><p><img src="/2019/05/05/Face-Detection-MTCNN/rnet.jpg" alt="rnet"></p><p>改步使用网络<code>PNet</code>与<code>ONet</code>，依次对上一层网络进行<code>refine</code>，计算方法一致。</p><ul><li>获取上一层网络输出回归框，截取图片中相应位置的图像数据，并缩放到对应尺寸；</li><li>前向计算，得到特征输出；</li><li>设定阈值，只保留分类评估大于阈值的结果；</li><li>对剩余结果进行<code>NMS</code>，输出结果；</li></ul><h2 id="3-NMS"><a href="#3-NMS" class="headerlink" title="3. NMS"></a>3. NMS</h2><p><img src="/2019/05/05/Face-Detection-MTCNN/nms.jpg" alt="nms"></p><p>例如，有中三个候选框a, b, c，其评分依次为0.8, 0.7, 0.9，设定NMS阈值</p><script type="math/tex; mode=display">thresh=0.4</script><ol><li>先将其排序，以降序排序为c, a, b；</li><li>保存当前评分最高的回归框，即c；</li><li><p>计算c与a, b的IoU，计算方法如下</p><p> <img src="/2019/05/05/Face-Detection-MTCNN/iou.jpg" alt="iou"></p><script type="math/tex; mode=display"> IoU = \frac{Intersection}{Union}</script><p> 其中</p><script type="math/tex; mode=display"> Intersection = w_{inter} × h_{inter}</script><script type="math/tex; mode=display"> w_{inter} = \min (x^a_2, x^b_2) - \max (x^a_1, x^b_1)</script><script type="math/tex; mode=display"> h_{inter} = \min (y^a_2, y^b_2) - \max (y^a_1, y^b_1)</script><p> 而</p><script type="math/tex; mode=display"> Union = Area_a + Area_b - Intersection</script></li><li><p>则c与a, b的IoU为</p><script type="math/tex; mode=display"> IoU_{a,c} = \frac{1×8}{10×9+9×11-1×8}=0.044</script><script type="math/tex; mode=display"> IoU_{a,b} = \frac{4×6}{10×9+10×11-4×6}=0.136</script><p> 均小于阈值，故无框被删除。</p></li><li><p>保存当前评分最高的回归框，即a；</p></li><li><p>计算a与b的IoU</p><script type="math/tex; mode=display"> IoU_{a,b} = \frac{7×9}{9×11+10×11-7×9} = 0.432</script><p> 大于阈值，删除候选框b</p></li><li>最终保留a，c。</li></ol><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>详情查看<a href="https://github.com/isLouisHsu/MTCNN_Darknet/tree/master/torch_mtcnn/detector.py" target="_blank" rel="noopener">isLouisHsu/MTCNN_Darknet/torch_mtcnn/detector.py - Github</a>。</p><h1 id="附：利用关键点进行图像对齐"><a href="#附：利用关键点进行图像对齐" class="headerlink" title="附：利用关键点进行图像对齐"></a>附：利用关键点进行图像对齐</h1><ol><li><p>变换矩阵$M$的求解<br> 例如现有$n$个关键点</p><script type="math/tex; mode=display"> xy = \left[\begin{matrix}     x_1 & y_1 \\     x_2 & y_2 \\     ... & ... \\     x_n & y_n \\ \end{matrix}\right]</script><p> 希望对齐后的坐标点为</p><script type="math/tex; mode=display"> \hat{xy} = \left[\begin{matrix}     \hat{x_1} & \hat{y_1} \\     \hat{x_2} & \hat{y_2} \\     ...  & ...  \\     \hat{x_n} & \hat{y_n} \\ \end{matrix}\right]</script><p> 构造矩阵</p><script type="math/tex; mode=display"> X_{2n\times4} = \left[\begin{matrix}     \vec{x} &  \vec{y} & \vec{1} & \vec{0} \\     \vec{y} & -\vec{x} & \vec{0} & \vec{1} \end{matrix}\right]</script><script type="math/tex; mode=display"> b_{2n} = \left[\begin{matrix}     \hat{x_1} & \hat{x_2} & \cdots & \hat{x_n} &     \hat{y_1} & \hat{y_2} & \cdots & \hat{y_n} \end{matrix}\right]^T</script><p> 其中</p><script type="math/tex; mode=display"> \vec{x} = \left[\begin{matrix}     x_1 & x_2 & \cdots & x_n \end{matrix}\right]^T</script><script type="math/tex; mode=display"> \vec{y} = \left[\begin{matrix}     y_1 & y_2 & \cdots & y_n \end{matrix}\right]^T</script><script type="math/tex; mode=display"> \vec{1} = \left[\begin{matrix}     1 & 1 & \cdots & 1 \end{matrix}\right]^T</script><script type="math/tex; mode=display"> \vec{0} = \left[\begin{matrix}     0 & 0 & \cdots & 0 \end{matrix}\right]^T</script><p> 求解下式解向量$r_{4\times1}$</p><script type="math/tex; mode=display"> X \cdot r = b</script><p> 注意增广矩阵的秩</p><script type="math/tex; mode=display">\text{rank}(X) < rank([X | b])</script><p> 上式无解，可使用伪逆求解</p><script type="math/tex; mode=display"> r = (X^T X + \lambda I)^{-1} X^T b</script><p> 构造矩阵</p><script type="math/tex; mode=display"> R = \left[\begin{matrix}     r_1 & -r_2 & 0 \\     r_2 &  r_1 & 0 \\     r_3 & -r_4 & 1 \\ \end{matrix}\right]</script><p> 则变换矩阵$M$可由下式求解</p><script type="math/tex; mode=display"> \left[\begin{matrix}     M^T & \begin{matrix}         0 \\ 0 \\ 1     \end{matrix} \end{matrix}\right] = R^{-1}</script><p> 即$M$为$R^{-1}$的前两列。</p></li><li><p>坐标变换</p><script type="math/tex; mode=display"> M = \left[\begin{matrix}     m_{11} & m_{12} & m_{13} \\ m_{21} & m_{22} & m_{23} \end{matrix}\right]</script><p> 对于坐标$(x, y)$，其变换后的坐标$(\hat{x}, \hat{y})$为</p><script type="math/tex; mode=display"> \left[\begin{matrix}     \hat{x} \\ \hat{y} \\ \end{matrix}\right] = M \left[\begin{matrix}     x \\ y \\ 1 \end{matrix}\right]</script></li></ol><p>几个关键的函数，C/C++实现如下，详情可查看<a href="https://github.com/isLouisHsu/MobileFaceNet_Darknet/blob/master/src/cp2form.c" target="_blank" rel="noopener">isLouisHsu/MobileFaceNet_Darknet/src/cp2form.c</a><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * @param</span></span><br><span class="line"><span class="comment"> *      uv: [u, v]， Nx2</span></span><br><span class="line"><span class="comment"> *      xy: [x, y]， Nx2</span></span><br><span class="line"><span class="comment"> * @return</span></span><br><span class="line"><span class="comment"> * @notes</span></span><br><span class="line"><span class="comment"> * -    Xr = Y   ===&gt;  r = (X^T X + \lambda I)^&#123;-1&#125; X^T Y</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">CvMat* _findNonreflectiveSimilarity(<span class="keyword">const</span> CvMat* uv, <span class="keyword">const</span> CvMat* xy)</span><br><span class="line">&#123;</span><br><span class="line">    CvMat* X = _stitch(xy);                         <span class="comment">// 2N x  4</span></span><br><span class="line"></span><br><span class="line">    CvMat* XT = cvCreateMat(X-&gt;cols, X-&gt;rows, CV_32F);  </span><br><span class="line">    cvTranspose(X, XT);                             <span class="comment">//  4 x 2N</span></span><br><span class="line"></span><br><span class="line">    CvMat* XTX = cvCreateMat(XT-&gt;rows, X-&gt;cols, CV_32F);</span><br><span class="line">    cvMatMul(XT, X, XTX);                           <span class="comment">//  4 x  4</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; XTX-&gt;rows; i++) XTX-&gt;data.fl[i*XTX-&gt;rows + i] += <span class="number">1e-15</span>;</span><br><span class="line"></span><br><span class="line">    CvMat* XTXi = cvCreateMat(XTX-&gt;rows, XTX-&gt;cols, CV_32F); </span><br><span class="line">    cvInvert(XTX, XTXi, CV_LU);                     <span class="comment">//  4 x  4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// -----------------------------------------------------------------------</span></span><br><span class="line">    </span><br><span class="line">    CvMat* uvT = cvCreateMat(uv-&gt;cols, uv-&gt;rows, CV_32F); </span><br><span class="line">    cvTranspose(uv, uvT);                           <span class="comment">//  2 x  N</span></span><br><span class="line">    CvMat header; </span><br><span class="line">    CvMat* YT = cvReshape(uvT, &amp;header, <span class="number">0</span>, <span class="number">1</span>);      <span class="comment">//  1 x 2N    TODO</span></span><br><span class="line">    CvMat* Y = cvCreateMat(YT-&gt;cols, YT-&gt;rows, CV_32F);  </span><br><span class="line">    cvTranspose(YT, Y);                             <span class="comment">// 2N x  1</span></span><br><span class="line">    </span><br><span class="line">    CvMat* XTXiXT = cvCreateMat(XTXi-&gt;rows, XT-&gt;cols, CV_32F);</span><br><span class="line">    CvMat* r = cvCreateMat(XTXiXT-&gt;rows, Y-&gt;cols, CV_32F);</span><br><span class="line">    cvMatMul(XTXi, XT, XTXiXT); cvMatMul(XTXiXT, Y, r);       <span class="comment">//  4 x  1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// -----------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;X); cvReleaseMat(&amp;XT); </span><br><span class="line">    cvReleaseMat(&amp;XTX); cvReleaseMat(&amp;XTXi); cvReleaseMat(&amp;XTXiXT);</span><br><span class="line">    cvReleaseMat(&amp;uvT); cvReleaseMat(&amp;Y);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// =======================================================================</span></span><br><span class="line"></span><br><span class="line">    CvMat* R = cvCreateMat(<span class="number">3</span>, <span class="number">3</span>, CV_32F);</span><br><span class="line">    R-&gt;data.fl[<span class="number">0</span> * <span class="number">3</span> + <span class="number">0</span>] = r-&gt;data.fl[<span class="number">0</span>]; R-&gt;data.fl[<span class="number">0</span> * <span class="number">3</span> + <span class="number">1</span>] = -r-&gt;data.fl[<span class="number">1</span>]; R-&gt;data.fl[<span class="number">0</span> * <span class="number">3</span> + <span class="number">2</span>] = <span class="number">0.</span>;</span><br><span class="line">    R-&gt;data.fl[<span class="number">1</span> * <span class="number">3</span> + <span class="number">0</span>] = r-&gt;data.fl[<span class="number">1</span>]; R-&gt;data.fl[<span class="number">1</span> * <span class="number">3</span> + <span class="number">1</span>] =  r-&gt;data.fl[<span class="number">0</span>]; R-&gt;data.fl[<span class="number">1</span> * <span class="number">3</span> + <span class="number">2</span>] = <span class="number">0.</span>;</span><br><span class="line">    R-&gt;data.fl[<span class="number">2</span> * <span class="number">3</span> + <span class="number">0</span>] = r-&gt;data.fl[<span class="number">2</span>]; R-&gt;data.fl[<span class="number">2</span> * <span class="number">3</span> + <span class="number">1</span>] =  r-&gt;data.fl[<span class="number">3</span>]; R-&gt;data.fl[<span class="number">2</span> * <span class="number">3</span> + <span class="number">2</span>] = <span class="number">1.</span>;</span><br><span class="line">    </span><br><span class="line">    CvMat* Ri = cvCreateMat(R-&gt;cols, R-&gt;rows, CV_32F);</span><br><span class="line">    cvInvert(R, Ri, CV_LU);</span><br><span class="line"></span><br><span class="line">    CvMat* MT = cvCreateMat(<span class="number">3</span>, <span class="number">2</span>, CV_32F);</span><br><span class="line">    cvGetSubRect(Ri, MT, cvRect(<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>));</span><br><span class="line">    </span><br><span class="line">    CvMat* M = cvCreateMat(MT-&gt;cols, MT-&gt;rows, CV_32F);</span><br><span class="line">    cvTranspose(MT, M);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// -----------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;r); cvReleaseMat(&amp;R); cvReleaseMat(&amp;Ri); cvReleaseMat(&amp;MT);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * @param</span></span><br><span class="line"><span class="comment"> *      uv: [u, v]， Nx2</span></span><br><span class="line"><span class="comment"> *      xy: [x, y]， Nx2</span></span><br><span class="line"><span class="comment"> * @return</span></span><br><span class="line"><span class="comment"> * @notes</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">CvMat* _findReflectiveSimilarity(<span class="keyword">const</span> CvMat* uv, <span class="keyword">const</span> CvMat* xy)</span><br><span class="line">&#123;</span><br><span class="line">    CvMat* xyR = cvCloneMat(xy);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> r = <span class="number">0</span>; r &lt; xyR-&gt;rows; r++) xyR-&gt;data.fl[r*xyR-&gt;cols] *= <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    CvMat* M1 = _findNonreflectiveSimilarity(uv, xy);</span><br><span class="line">    CvMat* M2 = _findNonreflectiveSimilarity(uv, xyR);</span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;xyR);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> r = <span class="number">0</span>; r &lt; M2-&gt;rows; r++) M2-&gt;data.fl[r*M2-&gt;cols] *= <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    CvMat* xy1 = _tformfwd(M1, uv);</span><br><span class="line">    CvMat* xy2 = _tformfwd(M2, uv);</span><br><span class="line">    cvSub(xy1, xy, xy1, <span class="literal">NULL</span>); cvSub(xy2, xy, xy2, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> norm1 = _matrixNorm2(xy1);</span><br><span class="line">    <span class="keyword">float</span> norm2 = _matrixNorm2(xy2);</span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;xy1); cvReleaseMat(&amp;xy2);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (norm1 &lt; norm2)&#123;</span><br><span class="line">        cvReleaseMat(&amp;M2);</span><br><span class="line">        <span class="keyword">return</span> M1;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cvReleaseMat(&amp;M1);</span><br><span class="line">        <span class="keyword">return</span> M2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1604.02878" target="_blank" rel="noopener">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a></li><li><a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" target="_blank" rel="noopener">kpzhang93/MTCNN_face_detection_alignment</a></li><li><a href="https://github.com/AITTSMD/MTCNN-Tensorflow" target="_blank" rel="noopener">AITTSMD/MTCNN-Tensorflow</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>无所畏</title>
      <link href="/2019/04/30/%E6%97%A0%E6%89%80%E7%95%8F/"/>
      <url>/2019/04/30/%E6%97%A0%E6%89%80%E7%95%8F/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/04/30/无所畏/1.jpg" alt="1"></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>世间没有佛，但是有带着佛性的人。</p><h1 id="精选"><a href="#精选" class="headerlink" title="精选"></a>精选</h1><h2 id="壹-成功十要素"><a href="#壹-成功十要素" class="headerlink" title="壹 成功十要素"></a>壹 成功十要素</h2><center>一命二运三风水<br>四积阴德五读书<br>六名七相八敬神<br>九交贵人十养生<br></center><ol><li>曾几何时，我们除了未来一无所有，我们充满好奇，我们有使不完的力气，我们不怕失去，我们眼里有光，我们为建设祖国而读书，我们下身肿胀，我们激素吱吱作响，我们热爱姑娘，我们万物生长。</li><li><p>如何避免成为油腻中年男</p><ul><li>不要成为胖子，曾经玉树临风，现在风狂树残；</li><li>不要停止学习，吹牛能让我们有瞬间快感，但不能改变我们对一些事物所知甚少的事实；</li><li>不要待着不动，说走就走，去散步，去旅行，也好；</li><li>不要当众谈性，关于眼神(盯着女生看)的告诫，也适用于权、钱等其他领域；</li><li>不要追忆从前，积攒唠叨从前的力气，再创业，再创造，再恋爱，我们还能攻城略地，杀伐战取；</li><li>不要教育晚辈，不愤不启；</li><li>不要给别人添麻烦；</li><li>不要停止购物，完全没了欲望，失去对美好事物的贪心，生命也就没有乐趣；</li><li>不要脏兮兮，少年时代的脏是不羁，中年时代的脏是真脏；</li><li><p>不要鄙视和年龄无关的人类习惯，所有的世道变坏，都是从鄙视文艺开始的；</p><p>因为苦逼而牛逼，因为逗逼而二逼，因为装逼而傻逼。愿我们原理油腻和猥琐，敬爱女生，过好余生，让世界更美好。</p></li></ul></li><li><p>更可怕的是成为了油腻青年</p><ul><li>装懂，多学习，多研究，对真正热爱之事，真正投入精力，向那些可以就防晒美白详细说出八种不同方法的女性好好学习；</li><li>着急，“夫水之己也不厚，其负大舟也无力”；</li><li>逐利，只有对钱的热忱却没有理想，即使站在了风口，也不会成为那只“会飞的猪”；</li><li>不要迷恋肉身；</li><li>迷恋手机，比起摸不到心爱的姑娘的手，摸不到自己的手机似乎要严重百倍；</li><li>不靠谱，将来总有一天，你会明白，困境、死境都是自己曾经立起又自己放倒的目标；</li><li>不敢真，对爱的人不敢说“爱”，对不爽的事不敢说“不”，不敢承认自己的处境，不敢承认失败然后从头再来。时过境迁，回过头来，要拿真心对世界的时候，大抵已经找不到心在哪儿了；</li><li>假佛系，假装自己无欲无求，其实只是懒得追求，你就不厌倦自己吗？</li><li>审美差，如今担心会跟审美不好的人撞了女朋友的脸(整容)，哈哈哈哈；</li><li>不要“脸”，所谓“相由心生”，脸上的油光，就是心里的油渣；</li></ul></li><li><p>“极品”男人如何极致装逼</p><ul><li>写信，比如冯唐、比如曾巩；</li><li>跑步；</li><li>喝茶；</li><li>古物，从骨子里明白拥有只是暂时，“欣于所遇，暂得于己，快然自足”；</li><li>言语，极致地吹牛逼也是极致装逼的一种，立言也是立德、立功、立言三不朽的一种；</li><li>读书；</li><li>情怀，极致装逼如下，“为天地立心，为生民立命，为往圣继绝学，为万世开太平”；</li><li>喝酒，和好玩儿的人喝，喝完能背出很多唐诗和楚辞；</li><li>养生，高逼格的养生是乐生，是在乐生的基础上长生。我老爸抽烟，从十二岁开始抽，现在八十三岁了，他的口头禅是：“天亮了，又赚了。”</li><li><p>修佛，高逼格的修佛是在日常的劳作里、阳光里、花花草草里、众生皆苦里、生命终极无意义里，试图体会到蹦蹦跳跳的快活；</p><p>其实，如果志存高远，“三观”正，逼格正，装逼装久了，就是身、心、灵的一部分了。装逼装极致了，就得大成就了。装逼的过程就是学习的过程，就是感受活着的过程，就是实现理想的过程。</p></li></ul></li><li><p>那些爱我们或者爱过我们的女生，在她们的一生中要花很多时间陪护我们这些装腔犯，安静地、积极地、有创造力地陪我们装逼好多年。……。如果爱不在了，那就不用管上面说的一切了，让他找别的姑娘配他装逼陪他飞吧。</p></li><li><p>天天临深履薄，这辈子好惨，而且睡眠毁了、人毁了，也就什么都没了。我不想这样一辈子，我不想总梦见那些提心吊胆的事儿，我还想梦见我以前那些美丽的女朋友以及那些被梨花照过的时光，我提笔在笔记本的扉页上，郑重地写下了我的九字真言：“不着急，不害怕，不要脸。”</p><ul><li>对时间的态度：不着急。有时候，关切是不问；有时候，不做比做什么都强；</li><li>对结果的态度：不害怕；</li><li><p>对他评的态度：不要脸。“是非审之于己，毁誉听之于人，得失安之于数”；</p><blockquote><p>补充一点变成四点吧：不着急，不害怕，不要脸，不抱怨。</p></blockquote></li></ul></li><li><p>“这是最好的时代，这是最坏的时代；这是智慧的时代，这是愚蠢的时代；这是信仰的时期，这是怀疑的时期；这是光明的季节，这是黑暗的季节；这是希望之春，这是失望之冬；人们面前有着各样事物，人们面前一无所有；人们正在直登天堂，人们正在直下地狱。”</p></li><li>女生把自己整修得越来越像孪生姐妹，男生把自己禅修得越来越无聊。菜越来越没有菜味儿，肉越来越没有肉味儿，街上早就没有野花可以摘了，街上早就没有板砖可以拍了。</li><li>面对我们阻止不了的时代变化，多使用肉体，多去狂喜与伤心，多去创造，活出更多人样儿。</li><li>人类改变不了人性中的恶，创造完成后保护，保护不住后破坏，破坏后再创造，永陷轮回。</li><li>亲爱的，给我写首情诗好吗？越虐心越好。</li><li>降维攻击定义：你有道德我没道德，你死，我活；你我都是人你还要做人，我自降为禽兽，你死，我活。</li></ol><h2 id="贰-爱情如何对抗时间"><a href="#贰-爱情如何对抗时间" class="headerlink" title="贰 爱情如何对抗时间"></a>贰 爱情如何对抗时间</h2><center>女人还是要自强<br>不容易生病的身体<br>够用的收入<br>养心的爱好<br>强大到浑蛋的小宇宙</center><blockquote><p>男人同~</p></blockquote><ol><li>再过一些年，或许宇宙这盆火也会最终熄灭，世界彻底安静下来，时间也瘫倒在空间里，仿佛一只死狗瘫倒在地板上。</li><li>爱情大概始于一些及其美妙的刹那。……。在刹那间，希望时间停滞，甚至无疾而终，在刹那间，就此死去。……。幸或者不幸的是，人想死的时候很难死掉，梦幻泡影、闪电烟花之后，生活继续。爱情如何对抗那些璀璨一刹那之外的漫长时间？</li><li>你看他起高楼，你看他楼塌了。起高楼时，这个男的不一定能守得住底线；楼塌时，这个男的不一定能跑得了。</li><li>自己穿暖，才是真暖；自己真暖，才有资格相互温暖。</li><li>梦里三月桃花，二人一马。</li><li>身体极累的时候，心极伤的时候，身外有酒，白、黄、红，心里有姑娘，小鸟、小兽、小妖。白、黄、红流进身体，小鸟、小兽、小妖踏着云彩从心里溜达出来。身体更累，心更伤。风住了，风又起了。沿着伤口，就着酒，往下，再往下，潜水一样，掘井一样，运气好的时候，会看到世界里从来没有的景象。</li><li>男性在修炼成功之前(绝大多数在死前都没成功)，似乎总是有种不知进退而成为二逼的风险，过分执着到死拧，过分淡定到麻木，过分较真儿到迂腐，过分邋遢到鼻毛过唇。</li><li>只有克服了对于牛逼的过分追求，才能真正避免成为一个傻逼，特别是，随着年纪的增长，避免成为一个老傻逼。</li></ol><h2 id="叁-想起一生中后悔的事儿"><a href="#叁-想起一生中后悔的事儿" class="headerlink" title="叁 想起一生中后悔的事儿"></a>叁 想起一生中后悔的事儿</h2><center>只花时间给三类人：<br>好看的人，<br>好玩的人，<br>又好看又好玩的人。</center><ol><li>同样吃一串葡萄，有人先从最好的一颗吃起，好处时每次都吃到可得的最好的一颗；有人先从最差的一颗吃起，好处是每次都能吃到比之前更好的一颗。</li><li>前半生认识的朋友来看我，是因为想看我而来看我，而不是因为我在某大机构任职或者刚得了一个世界第一、宇宙无敌的文艺大奖。</li><li>一个人在二十岁之前呆过十年的地方，就是他真正的故乡。之后无论他活多久，去过多少地方，故乡都在骨头和血液里，挥之不去。</li><li>其实，人一起生活过一段时间，就没了生死的界限，除非彼此的爱意已经被彻底忘记。</li><li>我回到您面前，您总会给我一杯热茶，然后也不说话，手指一下，茶在那儿。您走了之后我才明白，一杯热茶之前，要有被子、茶、热水，要问很久、很多次：我儿子什么时候回来啊？</li><li>人皆草木不用成材。</li><li>万事都如甘蔗，哪有两头甜？</li><li>因为手写有人味儿。……。手写信，给心里真正放不下的人，贴张邮票，去邮局寄了。</li><li>连续七天，口袋里，书包里，我天天带着这只鸟(玉)，手没事儿的时候就摸着它，睡觉的时候也攥着。……。到了第八个晚上，一模那只鸟不见了。我的酒一下醒了，我把行李拆了，没有；我把全身衣服拆了，没有；我把房间拆了，没有；我沿着进房间的路，原路返回到下出租车的那块砖，没有。……。我度过了一个非常清醒、哲学而又精疲力竭的夜晚，和初恋分手的第一晚也比这一晚好过很多。……。我醒来的时候，觉得比睡着之前还要累。我洗把脸，阳光从窗帘缝隙间洒下来，那只玉鸟就安静地待在酒店书桌地一个角落，栖息在酒店的便笺上 —— 应该是我脱裤子之前无意识地把它放到了最安全的地方。……。如果我把那只玉鸟抓过来摔碎，我就成佛了。实际发生的是，在一刹那，我找了根结实的绳儿，穿过玉鸟翅膀上面古老的打眼儿，把玉鸟牢牢地栓在我裤子的皮带扣上。<blockquote><ol><li>人生喜悦，失而复得；</li><li>太过珍惜，却弄丢了，是人性的矛盾；</li><li>愿意被找到的东西，一直在那；不愿意被找到的东西，丢了就丢了把；</li><li>为外物而悲喜，这是人性的桎梏。</li></ol></blockquote></li></ol><h2 id="肆-天用云作字"><a href="#肆-天用云作字" class="headerlink" title="肆 天用云作字"></a>肆 天用云作字</h2><center>在此刻，天用云作字。<br>在未来某处，在未来某刻，<br>天也用我作字，<br>用我的手蘸着墨作字。</center><ol><li>你耐心再看看，再看看，再看看。</li><li>如果你有一个期望，长年挥之不去，而且需要别人来满足，这个期望就是妄念。</li><li>自责是负能量最大的一种情绪。<blockquote><p>任性是被低估的美德。</p></blockquote></li><li>“事情过去好久了，话也没啥可说的了，但是有时想起你，还是真他妈的难过啊。”</li><li><p>饮酒到微醺，脸红脖子粗，脚下多了一截弹簧，整个人一蹦一跳的，似乎手不抓牢栏杆，身体就随着灵魂飞离地面。</p><blockquote><p>饮酒的一个好处是，用肉的迷失换取灵的觉悟。放不下的、看不开的，几口下肚，眼清目明，仿佛都与自己无关了，不自觉地，脸上堆满了笑。</p></blockquote></li><li><p>看看就得了，不要临。字写得漂亮的人太多了，万一你写得漂亮了，再写丑就太难了，你就不是你了，老天给你手上的那一丁丁点独特的东西就没了。</p></li><li>佛界易入，魔界难入。佛界和魔界都入入，人更知道什么是佛、什么是魔，人更容易平衡一点儿，在世上能走的更远点儿。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 冯唐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[REPRODUCTION]A Recipe for Training Neural Networks</title>
      <link href="/2019/04/29/REPRODUCTION-A-Recipe-for-Training-Neural-Networks/"/>
      <url>/2019/04/29/REPRODUCTION-A-Recipe-for-Training-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<p><strong>转载自<a href="http://karpathy.github.io" target="_blank" rel="noopener">Andrej Karpathy blog</a></strong></p><h2 id="The-recipe"><a href="#The-recipe" class="headerlink" title="The recipe"></a>The recipe</h2><p>In light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.</p><h4 id="1-Become-one-with-the-data"><a href="#1-Become-one-with-the-data" class="headerlink" title="1. Become one with the data"></a>1. Become one with the data</h4><p>The first step to training a neural net is to not touch any neural net code at all and instead begin by <u>thoroughly inspecting your data</u>. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels?</p><blockquote><ol><li>data contained duplicate examples</li><li>corrupted images &amp; labels</li><li>data imbalances and biases</li><li>local features v.s. global context</li><li>quantity &amp; form of variation</li><li>preprocess out some variation</li><li>spatial position v.s. average pool</li><li>detail v.s. downsample</li><li>labels are noisy?</li></ol></blockquote><p>In addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to <u>look at your network (mis)predictions and understand where they might be coming from</u>. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.</p><p>Once you get a qualitative sense it is also a good idea to write some simple code to <u>search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis.</u> The outliers especially almost always uncover some bugs in data quality or preprocessing.</p><h4 id="2-Set-up-the-end-to-end-training-evaluation-skeleton-get-dumb-baselines"><a href="#2-Set-up-the-end-to-end-training-evaluation-skeleton-get-dumb-baselines" class="headerlink" title="2. Set up the end-to-end training/evaluation skeleton + get dumb baselines"></a>2. Set up the end-to-end training/evaluation skeleton + get dumb baselines</h4><p>Now that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. <u>Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments.</u> At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We’ll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way.</p><p>Tips &amp; tricks for this stage:</p><ul><li><strong>fix random seed</strong>. Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome. This removes a factor of variation and will help keep you sane.</li><li><strong>simplify</strong>. Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage. Data augmentation is a regularization strategy that we may incorporate later, but for now it is just another opportunity to introduce some dumb bug.</li><li><strong>add significant digits to your eval</strong>. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.</li><li><strong>verify loss @ init</strong>. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure <code>-log(1/n_classes)</code> on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc.</li><li><strong>init well</strong>. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias.</li><li><strong>human baseline</strong>. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth.</li><li><strong>input-indepent baseline</strong>. Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all?</li><li><strong>overfit one batch</strong>. Overfit a single batch of only a few examples (e.g. as little as two). To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). I also like to visualize in the same plot both the label and the prediction and ensure that they end up aligning perfectly once we reach the minimum loss. If they do not, there is a bug somewhere and we cannot continue to the next stage.</li><li><strong>verify decreasing training loss</strong>. At this stage you will hopefully be underfitting on your dataset because you’re working with a toy model. Try to increase its capacity just a bit. Did your training loss go down as it should?</li><li><strong>visualize just before the net</strong>. The unambiguously correct place to visualize your data is immediately before your <code>y_hat = model(x)</code> (or <code>sess.run</code> in tf). That is - you want to visualize <em>exactly</em> what goes into your network, decoding that raw tensor of data and labels into visualizations. This is the only “source of truth”. I can’t count the number of times this has saved me and revealed problems in data preprocessing and augmentation.</li><li><strong>visualize prediction dynamics</strong>. I like to visualize model predictions on a fixed test batch during the course of training. The “dynamics” of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network “struggle” to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter.</li><li><strong>use backprop to chart dependencies</strong>. Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I’ve come across a few times is that people get this wrong (e.g. they use <code>view</code> instead of <code>transpose/permute</code> somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss for some example <strong>i</strong> to be 1.0, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the <strong>i-th</strong> example. More generally, gradients give you information about what depends on what in your network, which can be useful for debugging.</li><li><strong>generalize a special case</strong>. This is a bit more of a general coding tip but I’ve often seen people create bugs when they bite off more than they can chew, writing a relatively general functionality from scratch. I like to write a very specific function to what I’m doing right now, get that to work, and then generalize it later making sure that I get the same result. Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time.</li></ul><blockquote><ol><li>固定随机种子，消除随机带来的误差</li><li>简单出发，先不使用数据集扩增</li><li>测试集不要画曲线，不然会疯的</li><li>评估起始损失值，<code>-log(1/n_classes)</code>，各类初始概率应大致相等；<strong>思路一致</strong>, $p\approx$</li><li>初始化最后一层权重很重要；<strong>一般都是采用随机初始化方法？？</strong></li><li>评估人的准确性并与模型比较</li><li>设置一个独立于输入的<code>baseline</code>，观察网络是否提取了想要的特征</li><li>反复训练同一个批次的数据，使网络过拟合，查看损失最低能到多少；<strong>确定网络结构没有问题</strong></li><li>试着增加模型<code>capacity</code>，观察训练集损失是否下降，以确定合适的网络容量；<strong>确定模型参数量</strong></li><li>输入网络前，可视化数据，查看数据是否正确；<strong>确定输入数据没有问题</strong></li><li>可视化一些相同数据的输出，观察输出波动；<strong>观察网络收敛情况</strong></li><li>用反向传播debug网络；<strong>高级技能？技能点还不够</strong></li><li>全循环慢慢改成矢量化代码；<strong>一些复杂的计算可参考</strong></li></ol></blockquote><h4 id="3-Overfit"><a href="#3-Overfit" class="headerlink" title="3. Overfit"></a>3. Overfit</h4><p>At this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.</p><p>The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.</p><p>A few tips &amp; tricks for this stage:</p><ul><li><strong>picking the model</strong>. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: <strong>Don’t be a hero</strong>. I’ve seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don’t be a hero and just copy paste a ResNet-50 for your first run. You’re allowed to do something more custom later and beat this.</li><li><strong>adam is safe</strong>. In the early stages of setting baselines I like to use Adam with a learning rate of <a href="https://twitter.com/karpathy/status/801621764144971776?lang=en" target="_blank" rel="noopener">3e-4</a>. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.)</li><li><strong>complexify only one at a time</strong>. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you’d expect. Don’t throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc.</li><li><strong>do not trust learning rate decay defaults</strong>. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you’re not training ImageNet then you almost certainly do not want this. If you’re not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.</li></ul><blockquote><ol><li>不要逞强，搭建各种奇奇怪怪的模型。先从相近任务效果良好的网络结构出发，慢慢改进再击败它；</li><li><code>Adam</code>对参数敏感性低，<code>SGD</code>往往效果更好；</li><li>慢慢提高输入数据的复杂性，如输入数据的特征数、图像尺寸；</li><li>根据自己的学习任务，调整学习率衰减参数；</li></ol></blockquote><h4 id="4-Regularize"><a href="#4-Regularize" class="headerlink" title="4. Regularize"></a>4. Regularize</h4><p>Ideally, we are now at a place where we have a large model that is fitting at least the training set. Now it is time to regularize it and gain some validation accuracy by giving up some of the training accuracy. Some tips &amp; tricks:</p><ul><li><strong>get more data</strong>. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.</li><li><strong>data augment</strong>. The next best thing to real data is half-fake data - try out more aggressive data augmentation.</li><li><strong>creative augmentation</strong>. If half-fake data doesn’t do it, fake data may also do something. People are finding creative ways of expanding datasets; For example, <a href="https://openai.com/blog/learning-dexterity/" target="_blank" rel="noopener">domain randomization</a>, use of <a href="http://vladlen.info/publications/playing-data-ground-truth-computer-games/" target="_blank" rel="noopener">simulation</a>, clever <a href="https://arxiv.org/abs/1708.01642" target="_blank" rel="noopener">hybrids</a> such as inserting (potentially simulated) data into scenes, or even GANs.</li><li><strong>pretrain</strong>. It rarely ever hurts to use a pretrained network if you can, even if you have enough data.</li><li><strong>stick with supervised learning</strong>. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).</li><li><strong>smaller input dimensionality</strong>. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don’t matter much try to input a smaller image.</li><li><strong>smaller model size</strong>. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.</li><li><strong>decrease the batch size</strong>. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale &amp; offset “wiggles” your batch around more.</li><li><strong>drop</strong>. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout <a href="https://arxiv.org/abs/1801.05134" target="_blank" rel="noopener">does not seem to play nice</a> with batch normalization.</li><li><strong>weight decay</strong>. Increase the weight decay penalty.</li><li><strong>early stopping</strong>. Stop training based on your measured validation loss to catch your model just as it’s about to overfit.</li><li><strong>try a larger model</strong>. I mention this last and only after early stopping but I’ve found a few times in the past that larger models will of course overfit much more eventually, but their “early stopped” performance can often be much better than that of smaller models.</li></ul><p>Finally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network’s first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems.</p><blockquote><p>略微升高一点训练数据的损失，换取验证集损失的下降。</p><ol><li>扩大数据集；<strong>拼的就是算力和数据量</strong></li><li>数据集扩增；<strong>来了</strong></li><li>使用一些生成的虚假数据；</li><li>预训练模型，即使有足够的数据量；</li><li>监督性学习比非监督好；</li><li>减少数据特征的冗余性，如删减特征、降低图像分辨率；<strong>数据冗余容易过拟合</strong></li><li>减少模型参数容量；<strong>过大可能过拟合</strong></li><li>增大批数据量；<strong>梯度下降方向更准确</strong></li><li><code>Dropout</code>；<strong><code>BatchNorm</code>效果更好？</strong></li><li>权重衰减；<strong>正则惩罚</strong></li><li><code>Early stopping</code>；<strong>提前中断训练防止过拟合</strong></li><li>尝试更大的模型，有时候更大容易过拟合，但是提前中断训练效果会更好；</li><li>第一层网络的权值是否有可解释性；<strong>WTF?</strong></li></ol></blockquote><h4 id="5-Tune"><a href="#5-Tune" class="headerlink" title="5. Tune"></a>5. Tune</h4><p>You should now be “in the loop” with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step:</p><ul><li><strong>random over grid search</strong>. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is <a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener">best to use random search instead</a>. Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter <strong>a</strong> matters but changing <strong>b</strong> has no effect then you’d rather sample <strong>a</strong> more throughly than at a few fixed points multiple times.</li><li><strong>hyper-parameter optimization</strong>. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding.</li></ul><blockquote><ol><li>随机搜索超参数，对重要参数调整更多；</li><li>招募一个实习生帮助自己调参hhhhhh；</li></ol></blockquote><h4 id="6-Squeeze-out-the-juice"><a href="#6-Squeeze-out-the-juice" class="headerlink" title="6. Squeeze out the juice"></a>6. Squeeze out the juice</h4><p>Once you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system:</p><ul><li><strong>ensembles</strong>. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can’t afford the computation at test time look into distilling your ensemble into a network using <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">dark knowledge</a>.</li><li><strong>leave it training</strong>. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).</li></ul><blockquote><ol><li>集成方法；</li><li>坚信模型能收敛并能取得良好的效果，不要手贱中断他。</li></ol></blockquote><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Once you make it here you’ll have all the ingredients for success: You have a deep understanding of the technology, the dataset and the problem, you’ve set up the entire training/evaluation infrastructure and achieved high confidence in its accuracy, and you’ve explored increasingly more complex models, gaining performance improvements in ways you’ve predicted each step of the way. You’re now ready to read a lot of papers, try a large number of experiments, and get your SOTA results. Good luck!</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>在宇宙间不易被风吹散</title>
      <link href="/2019/04/28/%E5%9C%A8%E5%AE%87%E5%AE%99%E9%97%B4%E4%B8%8D%E6%98%93%E8%A2%AB%E9%A3%8E%E5%90%B9%E6%95%A3/"/>
      <url>/2019/04/28/%E5%9C%A8%E5%AE%87%E5%AE%99%E9%97%B4%E4%B8%8D%E6%98%93%E8%A2%AB%E9%A3%8E%E5%90%B9%E6%95%A3/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/04/28/在宇宙间不易被风吹散/1.jpg" alt="1"><br><img src="/2019/04/28/在宇宙间不易被风吹散/3.jpg" alt="3"></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>喜欢冯唐的文集，用他自己文章里写的句子描述，“毫不掩饰的小说”，露骨但是真实，文字如锦如绣，文字间有墨香、有美人、有Kindle、有古玉和瓷器。</p><blockquote><p>你迷恋什么，什么就是你的障碍。有个笃定的核，在宇宙间不易被风吹散。</p></blockquote><h1 id="精选"><a href="#精选" class="headerlink" title="精选"></a>精选</h1><ol><li>人是需要有点精神的，有点通灵的精神，否则很容易出溜成行尸走肉，任由人性中暗黑的一面驱使自己禽兽一样的肉身，在世间做一些腐朽不堪的事情。</li><li>一杆进洞，四下无人，人生悲惨莫过于此。</li><li>一日茶，一夜酒，一部毫不掩饰的小说，一次没有目的的见面，一群不谈正经事的朋友，用美好的器物消磨必定留不住的时间。</li><li>做一个人性的矿工，挖一挖，再挖一挖，看看下面的下面还有什么。</li><li>我觉得眼睛看到的一切似乎想要告诉我世界是什么但是又不明说到底是什么。</li><li>在纸书里，在啤酒里，在阳光里，在暖气里，宅着，屌着，无所事事，随梦所之。</li><li>我的、我的、我的、我的，一瞬间的我执爆棚，真好。</li><li>宇宙间大多数现象超越人类的知识范围，不可解释的例子比比皆是，比如人骨骼为啥是206块骨头，比如我爱你你为什么不爱我。</li><li>人生苦短，不如不管，继续任性。</li><li>时间在床边和鬓边一路小跑，有些事物在不知不觉中浅吟低唱，明生暗长。</li><li>人又不是黄金，怎么能让所有人都喜欢？任何事做到顶尖，都是政治，都会被人妒忌；即使是黄金，也会被某些人说成是臭狗屎。</li><li>既然死了的人都没睡醒过，活着时候睡觉就是很吃亏的一件事。</li><li>白白的，小小的，紧紧的，香香的，佛说第一次触摸最接近佛。——《初恋》</li><li>有时候，人会因为一两个微不足道的美好，安安渴望一个巨大的负面，比如因为像有机会用一下图案撩骚的Zippo打火机而渴望抽烟，比如因为一把好乳或者一头长发而舍不得一个三观凌乱的悍妇，比如因为一个火炉而期待北京一个漫长而寒冷的冬天。</li><li>脱离长期背在身上的人的羁绊，让身体里的禽兽和仙人在山林里和酒里渐渐增加比例，裸奔、裸泳，在池塘里带着猴子捞月亮，在山顶问神仙：人到底是个什么东西？</li><li>想起后半生最不靠谱的事儿，结论是：最靠谱的还是买个酒庄。</li><li>天大理比不过“我喜欢”。</li><li>涉及终极的事儿，听天，听命，让自己和身体尽人力，其他不必去想，多想无益，徒增烦恼。</li><li>全球化了，各国的建筑师都到处串了，各种时装杂志都到处发行了，各地的楼宇和姑娘越来越像，像到面目模糊，天下一城。</li><li>最后一个能想到的原因，是随身佩带之后，无时无刻不提醒自己一些必须珍惜的事物和必须坚守的品质。君子无辜，玉不去身，时刻提醒自己，不要吃喝嫖赌抽、坑蒙拐骗偷。</li><li>科技的快速进步让很多人变得过时，也让很多器物变得多余。</li><li>我会老到有一天，不需要手表告诉我，时间是如何自己消失，也不需要靠名牌手表告诉周围人类我的品味、格调、富裕程度和牛逼等级。我会根据四季里的光线的变化，大致推断现在是几点了，根据肠胃的叫声决定是否该去街口的小馆儿了。</li><li>男人要有些士的精神，有所不为，有所必为，活着不是唯一的追求和最终的底线。</li><li>女人一头长发，骑匹大马，很迷人，非常迷人，而且，她是来救你的，就无比迷人。无论她要带你去哪，你都不要拒绝，先上马，然后闭嘴，什么都不要问。</li><li>买件立领风衣，浓个眉大个眼，一直走，不要往两边看，还能再混几十年。</li><li>家庭太复杂，涉及太多硬件和软件、生理和心理、现在和未来，一篇文章不容易讲透。</li><li>上天下地，背山面海，每天看看不一样的云，想想昨晚的梦，和自己聊一会天，日子容易丰盛起来。</li><li>朋友们就散住在附近几个街区，不用提前约，菜香升起时，几个电话就能聚起几个人，酒量不同，酒品相近，术业不同，三观接近。菜一般，就多喝点酒；酒不好，就再多喝点，很快就能高兴起来。</li><li>一生中，除了做自己喜欢的事儿，剩下最重要的就是和相看两不厌的人待在一起。</li><li>不和这个世界争，也不和别人争，更不要和自己争。争的结果可能是一时牛逼，也可能是心脑血管意外，后者造成的持续影响大很多。</li><li>尽管没去过南极，但是也见过了风雨，俗事已经懒得分析，不如一起一边慢跑，一边咒骂彼此生活中奇葩一样摇曳的傻逼。</li><li>世界这么多凶狠，他人心里那么多地狱，内心没有一点混蛋，如何走的下去？</li><li>其实我们跟鱼、植物甚至草履虫有很多相近的地方，人或如草木，人可以甚至应该偶尔禽兽。</li><li>如果没有真的存在，所谓的善只能是伪善，所谓的美也只能是妄美。</li><li>因为人是要死的，所以要常常叨念冯唐说的九字箴言：不着急，不害怕，不要脸。</li><li>钱超过一定数目就不是用来个人消费的了，个人能温饱就好。多处的个人欲望需要靠修行来消灭，而不能靠多花钱来满足。</li><li>有帽子是一种相，没帽子也是一种相。内心不必太执着于无帽子的相，也不必太执着于有帽子的相。有帽子，无帽子，都需要亲尝，皆为玩耍。</li><li>既然戴帽子是相，投射到不同人的心识里就是不同的相，何必强求赞美？何必强调一致？何必消除噪音？</li><li>“宇戴王冠，必承其重。不要低头，王冠会掉。不要哭泣，有人会笑。”这个态度也太励志、太谋权，放松，戴戴耍耍，不留神，王冠掉了，掉就掉了，掉了就索性长发飘飘。</li><li>能做到实事求是地自恋其实是自信和自尊。任何领域做到最好之后，人只能相信自己的判断，只能自恋。……。从这个意义上讲，自恋不应该是被诟病的对象，不能实事求是的傻逼才应该是被诟病的对象。……。实事求是地修炼，实事求是地恋他和自恋，让别人闹心去吧。</li><li>矮子更爱居高临下，傻子更容易认为自己充满道理。</li><li>非让矮子明白自己是矮子，非让傻子明白自己是傻子，也是很耗神费时的事儿。对付世间闹心的事儿，只需要搞清楚两件事，一件是“关我屁事”，另一件是“关你屁事”。</li><li>小孩在天地间疯跑，不知道名利为何物，学习基本常识，食蔬饮水，应付无聊的课程，傻愣愣地杀无聊的时间，骂所有看不上的人“傻逼”。本身近佛，不需要佛。</li><li>有多少似乎过不起的事儿过不去一年？有多少看上去的大事最后真是大事？名片上印不下的名头，敌不过左图且书、右琴与壶，抵不过不得不褪去时一颗好心脏、一个好女生。</li><li>这样简单下去，再简单下去，脑子没弯儿了，手脚有劲儿了，山顶慢慢低于脚面了，拉萨就在眼前了。你我竟然像山、云、湖水和星空一样，一直在老去，一直在变化，一直没问题。再简单下去，在这样下去，你我都是佛了。</li><li>我常年劳碌，尽管热爱妇女，但没时间，无法让任何妇女满意。情伤之后，“得不到”，“留不住”，“无可奈何，奈何奈何”，唯一疗伤的方式就是拿伤口当笔头，写几行诗，血干了，诗出了，心里放下了。</li><li>如果去一座荒岛，没电，没电视，没电脑，一片蛮荒。我想了想，如果只能带一个活物。我就带一个和我能聊很多天的女人；如果只能带一本书，我就带一本《唐诗三百首》。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 冯唐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LDA</title>
      <link href="/2019/04/22/LDA/"/>
      <url>/2019/04/22/LDA/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在<a href="https://louishsu.xyz/2018/10/22/PCA/" target="_blank" rel="noopener">PCA</a>中，介绍了无监督降维方法，主成分分析(Principal Components Analysis)。从投影后数据方差最大的角度出发，选取主轴。下面介绍一种有监督的降维方法，线性判别分析(Linear Discriminant Analysis)。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><p>设有$n$维样本集$D=\{x^{(1)}, …, x^{(i)}, …, x^{(m)}\}$，所属类别数目为$C$，现求其第一投影轴$u_1$，即</p><script type="math/tex; mode=display">\tilde{x}^{(i)}_1 = u_1^Tx^{(i)}</script><p>现希望投影后，<strong>类内距离越小越好，类间距离越大越好</strong>，定义衡量指标</p><ol><li><p>类内离差阵</p><script type="math/tex; mode=display"> S_W = \sum_{j=1}^C \frac{m_j}{m} \left[ \frac{1}{m_j} \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \right]</script><p> 即</p><script type="math/tex; mode=display"> S_W = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \tag{1}</script></li><li><p>类间离差阵</p><script type="math/tex; mode=display"> S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T \tag{2}</script></li></ol><p>则投影到第一主轴$u_1$后数据的类内离差阵和类间离差阵为</p><script type="math/tex; mode=display">\tilde{S_W} = \sum_{j=1}^C \frac{m_j}{m} \left[ \frac{1}{m_j} \sum_{i=1}^{m_j} (\tilde{x}^{(i)}_1 - \tilde{\mu}^{(j)}_1) (\tilde{x}^{(i)}_1 - \tilde{\mu}^{(j)}_1)^T \right]</script><script type="math/tex; mode=display">\tilde{S_B} = \sum_{j=1}^C \frac{m_j}{m} (\tilde{\mu}^{(j)}_1 - \tilde{\mu}_1) (\tilde{\mu}^{(j)}_1 - \tilde{\mu}_1)^T</script><p>其中</p><script type="math/tex; mode=display">\tilde{x}^{(i)}_1 = u_1^T x^{(i)}</script><script type="math/tex; mode=display">\tilde{\mu}^{(j)}_1 = u_1^T \mu^{(j)}</script><script type="math/tex; mode=display">\tilde{\mu}_1 = u_1^T \mu</script><p>带入后得到</p><script type="math/tex; mode=display">\tilde{S_W} = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (u_1^T x^{(i)} - u_1^T \mu^{(j)}) (u_1^T x^{(i)} - u_1^T \mu^{(j)})^T</script><script type="math/tex; mode=display">= \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} u_1^T(x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T u_1</script><script type="math/tex; mode=display">= u_1^T S_W u_1</script><p>同理</p><script type="math/tex; mode=display">\tilde{S_B} = u_1^T S_B u_1</script><p>定义优化目标</p><script type="math/tex; mode=display">J = \min \left\{ \frac{\tilde{S_W}}{\tilde{S_B}} \right\}= \min \left\{ \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \right\} \tag{3}</script><p>取</p><script type="math/tex; mode=display">L(u_1) = \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \tag{4}</script><p>则其极值为</p><script type="math/tex; mode=display">\frac{∂L}{∂u_1} = \frac{2(u_1^T S_B u_1) S_W u_1 - 2(u_1^T S_W u_1) S_B u_1}{(u_1^T S_B u_1)^2} = 0</script><p>得到</p><script type="math/tex; mode=display">(u_1^T S_B u_1) S_W u_1 = (u_1^T S_W u_1) S_B u_1</script><p>令$\lambda_1 = \frac{u_1^T S_B u_1}{u_1^T S_W u_1}$，有</p><script type="math/tex; mode=display">S_B u_1 = \lambda_1 S_W u_1 \tag{5}</script><p>当$m$较大时，$S_W$一般非奇异，故</p><script type="math/tex; mode=display">S_W^{-1} S_B u_1 = \lambda_1 u_1 \tag{*1}</script><p>即$\{\lambda_1, u_1\}$为矩阵$S_W^{-1} S_B$的特征对。</p><h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>特别地，对于二分类问题，有</p><script type="math/tex; mode=display">S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} (\mu^{(1)} - \mu) (\mu^{(1)} - \mu)^T + \frac{m_2}{m} (\mu^{(2)} - \mu) (\mu^{(2)} - \mu)^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - 2 \mu \left( \frac{m_1}{m} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)T} \right) + \mu \mu^T</script><p>其中</p><script type="math/tex; mode=display">\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)} = \mu \tag{6}</script><p>所以$(6)$代入$S_B$化简得</p><script type="math/tex; mode=display">S_B = \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - \mu \mu^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - (\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)}) (\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)})^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} \left(1-\frac{m_1}{m}\right) \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \left(1-\frac{m_2}{m}\right) \mu^{(2)} \mu^{(2)T} - \frac{m_1}{m} \frac{m_2}{m} \mu^{(2)} \mu^{(1)T} - \frac{m_1}{m} \frac{m_2}{m} \mu^{(1)} \mu^{(2)T}</script><script type="math/tex; mode=display">= \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T</script><p>上式代入$S_W^{-1} S_B u_1$，得</p><script type="math/tex; mode=display">左边 = S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T u_1</script><p>其中$\left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T u_1$为常数，记作$\alpha$，所以$\alpha S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) = \lambda_1 u_1$，也即</p><script type="math/tex; mode=display">u_1 = \frac{\alpha}{\lambda_1} S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)</script><p>其中常数$\frac{\alpha}{\lambda_1}$不影响投影结果，如取$1$，则得到</p><script type="math/tex; mode=display">u_1 = S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \tag{*2}</script><p>同理，可求得第二、三主成分轴</p><h1 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h1><p>给定数据集矩阵</p><script type="math/tex; mode=display">X = \left[ \begin{matrix}    ... \\    x^{(i)T} \\    ... \end{matrix} \right]</script><p>其中$x^{(i)} = \left[ x^{(i)}_1, …, x^{(i)}_j, … x^{(i)}_n \right]^T$</p><ol><li><p>计算类内离差阵$S_W$与类间离差阵$S_B$；</p><script type="math/tex; mode=display"> S_W = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \tag{1}</script><script type="math/tex; mode=display"> S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T \tag{2}</script></li><li><p>计算矩阵$S_W^{-1}S_B$的特征对$(\lambda_i, u_i)$；</p><script type="math/tex; mode=display">S_W^{-1}S_B u_i = \lambda_i u_i \tag{*1}</script></li><li><p>按特征值从大到小排序，选取最大的特征值作为第一主轴；</p></li><li>将数据投影到主轴上；</li></ol><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>LDA可用于分类，以二分类为例，在求取主轴$u_1$后，将各类中心投影到主轴上，即</p><script type="math/tex; mode=display">\begin{cases}    \tilde{\mu}^{(1)}_1 = u_1^T \mu^{(1)} \\    \tilde{\mu}^{(2)}_1 = u_1^T \mu^{(2)}\end{cases}</script><p>选取阈值，如</p><script type="math/tex; mode=display">\tilde{x}_1 = \frac{\tilde{\mu}^{(1)}_1 + \tilde{\mu}^{(2)}_1}{2}</script><p>则预测时，判决方程为</p><script type="math/tex; mode=display">\begin{cases}    u_1^T x^{(i)} < \tilde{x}_1 \Rightarrow x^{(i)} \in \omega_1 \\    u_1^T x^{(i)} > \tilde{x}_1 \Rightarrow x^{(i)} \in \omega_2\end{cases}</script><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>详情查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/lda.py" target="_blank" rel="noopener">Github</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LDA</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" Linear Discriminant Analysis </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        components_:  &#123;ndarray(n_components, n_features)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">    -   S_W = \frac&#123;1&#125;&#123;m&#125; \sum_&#123;j=1&#125;^C \sum_&#123;i=1&#125;^m_j (x^&#123;(i)&#125; - \mu^&#123;(j)&#125;) (x^&#123;(i)&#125; - \mu^&#123;(j)&#125;)^T</span></span><br><span class="line"><span class="string">    -   S_B = \sum_&#123;j=1&#125;^C \frac&#123;m_j&#125;&#123;m&#125; (\mu^&#123;(j)&#125; - \mu) (\mu^&#123;(j)&#125; - \mu)^T</span></span><br><span class="line"><span class="string">    -   S_W^&#123;-1&#125; S_B \alpha = \lambda \alpha</span></span><br><span class="line"><span class="string">    -   由于特征分解时，计算出现复数，故进行相应处理，详情查看https://louishsu.xyz/2019/08/20/Eigenface-and-Fisherface/</span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.components_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">""" train the model</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:      &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">            y:      &#123;ndarray(n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        labels = list(set(list(y)))</span><br><span class="line">        n_class = len(labels)</span><br><span class="line">        n_samples, n_feats = X.shape</span><br><span class="line"></span><br><span class="line">        S_W = np.zeros(shape=(n_feats, n_feats))</span><br><span class="line">        S_B = np.zeros(shape=(n_feats, n_feats))</span><br><span class="line">        mean_ = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> i_class <span class="keyword">in</span> range(n_class):</span><br><span class="line">            X_ = X[y==labels[i_class]]</span><br><span class="line">            means_ = np.mean(X_, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            X_ = X_ - means_</span><br><span class="line">            means_ = (means_ - mean_).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            S_W += (X_.T).dot(X_) * (<span class="number">1</span> / n_samples)</span><br><span class="line">            S_B += (means_.T).dot(means_) * (X_.shape[<span class="number">0</span>] / n_samples)</span><br><span class="line"></span><br><span class="line">        s, u = np.linalg.eigh(S_W)</span><br><span class="line">        s_sqrt = np.diag(np.sqrt(s))</span><br><span class="line">        s_sqrt_inv = np.linalg.inv(s_sqrt)</span><br><span class="line"></span><br><span class="line">        A = s_sqrt_inv.dot(u.T).dot(S_B).dot(u).dot(s_sqrt_inv)</span><br><span class="line">        eigval, P = np.linalg.eigh(A)</span><br><span class="line">        eigvec = u.dot(s_sqrt_inv).dot(P)</span><br><span class="line"></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>]</span><br><span class="line">        eigval = eigval[order]</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line"></span><br><span class="line">        self.components_ = eigvec[:, :self.n_components].T</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_.T)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.fit(X, y)</span><br><span class="line">        X_ = self.transform(X)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            y:  &#123;ndarray(n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        y = np.zeros(n_samples, dtype=np.int)</span><br><span class="line">        </span><br><span class="line">        X_ = self.transform(X)</span><br><span class="line">        means_ = self.transform(self.means_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            y[i] = np.argmin(np.linalg.norm(means_ - X_[i], axis=<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>与PCA投影结果对比如下</p><ul><li>PCA<br>  <img src="/2019/04/22/LDA/PCA.png" alt="PCA"></li><li>LDA<br>  <img src="/2019/04/22/LDA/LDA.png" alt="LDA"></li></ul><p>上图不明显，下图生成了两簇高斯分布的样本点，作出主轴显示，红色为LDA第一主轴方向，蓝色为PCA第一主轴方向<br><img src="/2019/04/22/LDA/Figure_1.png" alt="Figure_1"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html" target="_blank" rel="noopener">机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>N-dim Array PCA</title>
      <link href="/2019/04/18/n-dim-Array-PCA/"/>
      <url>/2019/04/18/n-dim-Array-PCA/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在<a href="https://louishsu.xyz/2018/10/22/PCA/" target="_blank" rel="noopener">PCA</a>中介绍了1维数据的主成分分析，那么对于多维数据，如何进行处理呢？有一种做法是，将单份的样本数据展开为1维向量，再进行PCA，例如著名的<a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E8%84%B8" target="_blank" rel="noopener">Eigenface</a>，如图所示</p><p><img src="/2019/04/18/n-dim-Array-PCA/Eigenfaces.png" alt="Eigenfaces"></p><p>这有一个缺点是，忽略了多维数据的空间信息，且计算时，若展开后维度过大，协方差矩阵的求逆过程非常耗时间，以下介绍多维主成分分析。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>对于单维$n$维度数据$X = \left[\begin{matrix} x^{(1)} \\ x^{(2)} \\ … \\ x^{(N)} \end{matrix}\right]$，我们有</p><script type="math/tex; mode=display">X' = XU</script><p>其中，$x^{(i)}$为$n$维行向量，组成样本矩阵$X_{N×n}$，$U_{n×n_1}$为投影矩阵，$X’_{N×n_1}$为降维后的样本矩阵。</p><p>对于多(n)维样本张量数据$X_{N×n_{d_1}×n_{d_2}×…×n_{d_n}}$，指定各维度降维顺序，在$d_i$维度上，我们将张量在该维度上展开为$1$维向量</p><script type="math/tex; mode=display">X_{N_{d_i}×n_{d_i}}</script><p>其中</p><script type="math/tex; mode=display">N_{d_i} = \prod_{j=0,j≠i}^n n_{d_j}</script><p>然后在$d_i$维度上进行降维，即</p><script type="math/tex; mode=display">X'_{N_{d_i}×n'_{d_i}} = X_{N_{d_i}×n_{d_i}} U^{d_i}</script><p>其中$U^{d_i}$表示$d_i$维度的投影矩阵，其大小为$n_{d_i}×n’_{d_i}$，然后，将该张量其余维度恢复，得到</p><script type="math/tex; mode=display">X^{d_i}_{N×n_{d_1}×..×n'_{d_i}×...×n_{d_n}}</script><p>如此循环，在各维度进行降维，得到张量</p><script type="math/tex; mode=display">X^{d_1, ..., d_n}_{N×n'_{d_1}×..×n'_{d_i}×...×n'_{d_n}}</script><p><strong>注意，不同的降维顺序得到的参数会存在差异。</strong></p><p>以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$，其降维过程表示如图</p><ul><li><p>在$H$维度上</p><p>  <img src="/2019/04/18/n-dim-Array-PCA/d1.JPG" alt="d1"></p></li><li><p>在$W$维度上</p><p>  <img src="/2019/04/18/n-dim-Array-PCA/d2.JPG" alt="d2"></p></li><li><p>在$C$维度上</p><p>  <img src="/2019/04/18/n-dim-Array-PCA/d3.JPG" alt="d3"></p></li></ul><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>同样的，以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$</p><ol><li><p>在$H$维度上</p><ol><li>将该张量转置，得到$X^{T_{d_1d_3}}(C, W, H)$</li><li>将其展开为$X_{flatten}(N_H×H)$，其中$N_H=C×W$</li><li>降维得到$X’_{flatten}(N_H×H’)$</li><li>重新将另外两维恢复，得到$X^{T_{d_1d_3}’}(C, W, H’)$</li><li>将张量转置，得到$X^{d_1}(H’, W, C)$</li></ol></li><li><p>在$W$维度上</p><ol><li>将$X^{d_1}(H’, W, C)$转置，得到$X^{T_{d_2d_3}}(H’, C, W)$</li><li>将其展开为$X_{flatten}(N_W×W)$，其中$N_W=H’×C$</li><li>降维得到$X’_{flatten}(N_W×W’)$</li><li>重新将另外两维恢复，得到$X^{T_{d_2d_3}’}(H’, C, W’)$</li><li>将张量转置，得到$X^{d_1d_2}(H’, W’, C)$</li></ol></li><li><p>在$C$维度上</p><ol><li>将$X^{d_1d_2}(H’, W’, C)$展开为$X_{flatten}(N_C×C)$，其中$N_C=H’×W’$</li><li>降维得到$X’_{flatten}(N_C×C’)$</li><li>重新将另外两维恢复，得到$X^{d_1d_2d_3}(H’, W’, C’)$</li></ol></li></ol><p>详细查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/tensor_pca.py" target="_blank" rel="noopener">GitHub</a>，其核心代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples, d0, d1, d2, ..., dn-1)&#125; n-dim array</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples, d0, d1, d2, ..., dn-1)&#125; n-dim array</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> self.n_dims == len(X.shape) - <span class="number">1</span>, <span class="string">'please check input dimension! '</span></span><br><span class="line">    idx = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X.shape))]   <span class="comment"># index of dimensions</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i_dim <span class="keyword">in</span> range(self.n_dims):</span><br><span class="line">            </span><br><span class="line">        <span class="comment">## transpose tensor</span></span><br><span class="line">        idx[<span class="number">-1</span>], idx[i_dim + <span class="number">1</span>] = idx[i_dim + <span class="number">1</span>], idx[<span class="number">-1</span>]</span><br><span class="line">        X = X.transpose(idx)</span><br><span class="line">        shape = list(X.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1-dim pca</span></span><br><span class="line">        X = X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>]))</span><br><span class="line">        X = self.decomposers[i_dim].transform(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## transpose tensor</span></span><br><span class="line">        X = X.reshape(shape[:<span class="number">-1</span>]+[X.shape[<span class="number">-1</span>]])</span><br><span class="line">        X = X.transpose(idx)</span><br><span class="line">        idx[<span class="number">-1</span>], idx[i_dim + <span class="number">1</span>] = idx[i_dim + <span class="number">1</span>], idx[<span class="number">-1</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h1 id="实验显示"><a href="#实验显示" class="headerlink" title="实验显示"></a>实验显示</h1><p>将2维数据降维，指定通道维度数目从$3$降至$1$，得到实验结果如下</p><ul><li><p>原始数据<br>  <img src="/2019/04/18/n-dim-Array-PCA/origin_data.png" alt="origin_data"></p></li><li><p>降维后数据<br>  <img src="/2019/04/18/n-dim-Array-PCA/transform.png" alt="transform"></p></li><li><p>重建数据<br>  <img src="/2019/04/18/n-dim-Array-PCA/inv_transform.png" alt="inv_transform"></p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E8%84%B8" target="_blank" rel="noopener">特征脸 - 维基百科，自由的百科全书</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python字符串格式化</title>
      <link href="/2019/02/19/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96/"/>
      <url>/2019/02/19/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Python操作字符串行云流水，当然也支持格式化字符串。</p><h1 id="通过格式符"><a href="#通过格式符" class="headerlink" title="通过格式符"></a>通过格式符</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;我叫%s, 今年%d岁&quot; % (&apos;Louis Hsu&apos;, 18))</span><br></pre></td></tr></table></figure><p>或者使用字典进行值传递<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;我叫%(name), 今年%(age)岁&quot; % &#123;&apos;name&apos;: &apos;Louis Hsu&apos;, &apos;age&apos;: 18&#125;)</span><br></pre></td></tr></table></figure></p><p><strong>typecode</strong><br>|  格式符 | 含义 |<br>| ——— | ———- |<br>| %s | 字符串 (采用str()的显示) |<br>| %r | 字符串 (采用repr()的显示) |<br>| %c | 单个字符 |<br>| %b | 二进制整数 |<br>| %d | 十进制整数 |<br>| %i | 十进制整数 |<br>| %o | 八进制整数 |<br>| %x | 十六进制整数 |<br>| %e | 指数 (基底写为e) |<br>| %E | 指数 (基底写为E) |<br>| %f | 浮点数 |<br>| %F | 浮点数，与上相同 |<br>| %g | 指数(e)或浮点数 (根据显示长度) |<br>| %G | 指数(E)或浮点数 (根据显示长度) |<br>| %% | 字符”%” | </p><p><strong>高阶</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">% [flags] [width].[precision] typecode</span><br><span class="line">- flags:        &apos;+&apos;(右对齐), &apos;-&apos;(左对齐), &apos; &apos;(左侧填充一个空格，与负数对齐), &apos;0&apos;(用0填充)</span><br><span class="line">- width:        显示宽度</span><br><span class="line">- precision:    小数精度位数，可使用&apos;*&apos;进行动态代入</span><br><span class="line">- typecode:     格式符</span><br></pre></td></tr></table></figure></p><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;pi is %+2.2f&apos; % (3.1415926)) -&gt; pi is +3.14</span><br><span class="line">print(&apos;pi is %-2.2f&apos; % (3.1415926)) -&gt; pi is 3.14</span><br><span class="line">print(&apos;pi is % 2.2f&apos; % (3.1415926)) -&gt; pi is  3.14</span><br><span class="line">print(&apos;pi is %02.2f&apos; % (3.1415926)) -&gt; pi is 3.14</span><br><span class="line"></span><br><span class="line"># 同</span><br><span class="line">print(&apos;pi is %+*.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is %-*.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is % *.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is %0*.*f&apos; % (2, 2, 3.1415926))</span><br></pre></td></tr></table></figure></p><h1 id="通过format"><a href="#通过format" class="headerlink" title="通过format"></a>通过format</h1><h2 id="位置传递"><a href="#位置传递" class="headerlink" title="位置传递"></a>位置传递</h2><ol><li><p>使用位置参数</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; li = [&apos;hoho&apos;,18]</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;&#125; ,age &#123;&#125;&apos;.format(&apos;hoho&apos;,18)</span><br><span class="line">&apos;my name is hoho ,age 18&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;1&#125; ,age &#123;0&#125;&apos;.format(10,&apos;hoho&apos;)</span><br><span class="line">&apos;my name is hoho ,age 10&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;1&#125; ,age &#123;0&#125; &#123;1&#125;&apos;.format(10,&apos;hoho&apos;)</span><br><span class="line">&apos;my name is hoho ,age 10 hoho&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;&#125; ,age &#123;&#125;&apos;.format(*li)</span><br><span class="line">&apos;my name is hoho ,age 18&apos;</span><br></pre></td></tr></table></figure></li><li><p>使用关键字参数</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; hash = &#123;&apos;name&apos;:&apos;hoho&apos;,&apos;age&apos;:18&#125;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;name&#125;,age is &#123;age&#125;&apos;.format(name=&apos;hoho&apos;,age=19)</span><br><span class="line">&apos;my name is hoho,age is 19&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;name&#125;,age is &#123;age&#125;&apos;.format(**hash)</span><br><span class="line">&apos;my name is hoho,age is 18&apos;</span><br></pre></td></tr></table></figure></li></ol><h2 id="格式限定"><a href="#格式限定" class="headerlink" title="格式限定"></a>格式限定</h2><p>基本格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;[:pad][align][sign][typecode]&#125;</span><br><span class="line">- :pad :    填充字符，空白位用该字符填充</span><br><span class="line">- align:    &apos;^&apos;, &apos;&lt;&apos;, &apos;&gt;&apos; 分别表示 &apos;居中&apos;, &apos;左对齐&apos;, &apos;右对齐&apos;(默认)，后面加宽度</span><br><span class="line">- sign :    &apos;+&apos;, &apos;-&apos; , &apos; &apos; 分别表示 &apos;正&apos;, &apos;负&apos;, &apos;正数前加空格&apos;</span><br><span class="line">- typecode: &apos;b&apos;, &apos;d&apos;, &apos;o&apos;, &apos;x&apos;, &apos;f&apos;, &apos;,&apos;, &apos;%&apos;, &apos;e&apos; 分别表示 &apos;二进制&apos;, &apos;十进制&apos;, &apos;八进制&apos;, &apos;十六进制&apos;, &apos;浮点数&apos;, &apos;逗号分隔&apos;, &apos;百分比格式&apos;,  &apos;指数记法&apos;</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>python之字符串格式化(format) - benric - 博客园 <a href="https://www.cnblogs.com/benric/p/4965224.html" target="_blank" rel="noopener">https://www.cnblogs.com/benric/p/4965224.html</a><br>Python format 格式化函数 | 菜鸟教程 <a href="http://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener">http://www.runoob.com/python/att-string-format.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python命令行参数解析</title>
      <link href="/2019/02/18/Python%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/02/18/Python%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="C-C-的参数传递"><a href="#C-C-的参数传递" class="headerlink" title="C/C++的参数传递"></a>C/C++的参数传递</h1><p>我们知道C/C++主函数形式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc,char * argv[],char * envp[])</span><br><span class="line">&#123;</span><br><span class="line">    // do something</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中各参数含义如下</p><ul><li><code>argc</code>：<code>argument count</code>，表示参数数量</li><li><code>argv</code>：<code>argument value</code>，表示参数值<br>  最后一个元素存放了一个NULL的指针</li><li><code>envp</code>：系统环境变量<br>  最后一个元素存放了一个NULL的指针</li></ul><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main(int argc,char * argv[],char * envp[])</span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;argc is %d \n&quot;, argc);</span><br><span class="line"> </span><br><span class="line">    int i;</span><br><span class="line">    for (i=0; i&lt;argc; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;arcv[%d] is %s\n&quot;, i, argv[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for (i=0; envp[i]!=NULL; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;envp[%d] is %s\n&quot;, i, envp[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试平台为Windows10，执行编译和运行操作，结果如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; gcc main.c -o main.exe</span><br><span class="line">&gt; ./main.exe param1 param2 param3 param4</span><br><span class="line">argc is 5</span><br><span class="line">arcv[0] is C:\OneDrive\▒ĵ▒\Louis&apos; Blog\source\_drafts\Python▒▒▒▒▒в▒▒▒▒▒▒▒\test.exe</span><br><span class="line">arcv[1] is param1</span><br><span class="line">arcv[2] is param2</span><br><span class="line">arcv[3] is param3</span><br><span class="line">arcv[4] is param4</span><br><span class="line">envp[0] is ACLOCAL_PATH=C:\MyApplications\Git\mingw64\share\aclocal;C:\MyApplica                                                                                                                tions\Git\usr\share\aclocal</span><br><span class="line">envp[1] is ALLUSERSPROFILE=C:\ProgramData</span><br><span class="line">...(省略)</span><br><span class="line">envp[71] is WINDIR=C:\WINDOWS</span><br><span class="line">envp[72] is _=./main.exe</span><br></pre></td></tr></table></figure></p><h1 id="Python的参数传递"><a href="#Python的参数传递" class="headerlink" title="Python的参数传递"></a>Python的参数传递</h1><p>可以使用sys模块得到命令行参数，主函数文件<code>main.py</code>如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    print(sys.argv)</span><br></pre></td></tr></table></figure></p><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; python main.py param1 param2 pram3</span><br><span class="line">[&apos;main.py&apos;, &apos;param1&apos;, &apos;param2&apos;, &apos;pram3&apos;]</span><br></pre></td></tr></table></figure></p><h1 id="getopt"><a href="#getopt" class="headerlink" title="getopt"></a>getopt</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import getopt</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    argv = sys.argv</span><br><span class="line">    </span><br><span class="line">    if len(argv) == 1:</span><br><span class="line">        print(</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            Usage: python main.py [option]</span><br><span class="line">            -h or --help:    显示帮助信息</span><br><span class="line">            -v or --version: 显示版本</span><br><span class="line">            -i or --input:   指定输入文件路径</span><br><span class="line">            -o or --output:  指定输出文件路径</span><br><span class="line"></span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        opts, args = getopt.getopt(args=argv[1:],</span><br><span class="line">                                   shortopts=&apos;hvi:o:&apos;,</span><br><span class="line">                                   longopts=[&apos;help&apos;, &apos;version&apos;, &apos;input=&apos;, &apos;output=&apos;]</span><br><span class="line">                                )</span><br><span class="line">    except getopt.GetoptError:</span><br><span class="line">        print(&quot;argv error,please input&quot;)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    for cmd, arg in opts:</span><br><span class="line"></span><br><span class="line">        if cmd in [&apos;-h&apos;, &apos;--help&apos;]:</span><br><span class="line">            print(&quot;help info&quot;)</span><br><span class="line">            sys.exit(0)</span><br><span class="line">        elif cmd in [&apos;-v&apos;, &apos;--version&apos;]:</span><br><span class="line">            print(&quot;main 1.0&quot;)</span><br><span class="line">            sys.exit(0)</span><br><span class="line"></span><br><span class="line">        if cmd in [&apos;-i&apos;, &apos;--input&apos;]:</span><br><span class="line">            input = arg</span><br><span class="line">        if cmd in [&apos;-o&apos;, &apos;--output&apos;]:</span><br><span class="line">            output = arg</span><br></pre></td></tr></table></figure><p>说明</p><ul><li><p><code>args=sys.argv[1:]</code><br>  传入的参数，除去<code>sys.argv[0]</code>，即主函数文件路径</p></li><li><p><code>shortopts=&#39;hvi:o:&#39;</code><br>  字符串，支持形如<code>-h</code>的选项</p><ul><li>若无需指定参数，形如<code>c</code>；</li><li>若必须指定参数，则需为<code>c:</code>；</li></ul></li><li><p><code>longopts=[&#39;help&#39;, &#39;version&#39;, &#39;input=&#39;, &#39;output=&#39;]</code><br>  字符串列表，可选参数，是否支持形如<code>--help</code>的选项</p><ul><li>若无需指定参数，形如<code>cmd</code>；</li><li>若必须指定参数，则需为<code>cmd=</code>；</li></ul></li></ul><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt; python main.py</span><br><span class="line"></span><br><span class="line">            Usage: python main.py [option]</span><br><span class="line">            -h or --help:    显示帮助信息</span><br><span class="line">            -v or --version: 显示版本</span><br><span class="line">            -i or --input:   指定输入文件路径</span><br><span class="line">            -o or --output:  指定输出文件路径</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt; python main.py -h</span><br><span class="line">help info</span><br><span class="line"></span><br><span class="line">&gt; python main.py -v</span><br><span class="line">main 1.0</span><br><span class="line"></span><br><span class="line">&gt; python main.py -i</span><br><span class="line">argv error,please input</span><br><span class="line"></span><br><span class="line">&gt; python main.py -i a.txt -o b.txt</span><br></pre></td></tr></table></figure></p><h1 id="argsparse"><a href="#argsparse" class="headerlink" title="argsparse"></a>argsparse</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">def show_args(args):</span><br><span class="line">    if args.opencv:</span><br><span class="line">        print(&quot;opencv is used &quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;opencv is not used &quot;)</span><br><span class="line"></span><br><span class="line">    print(args.steps)</span><br><span class="line">    print(args.file)</span><br><span class="line">    print(args.data)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = argparse.ArgumentParser(description=&quot;learn to use `argparse`&quot;)</span><br><span class="line"></span><br><span class="line">    # 标志位</span><br><span class="line">    parser.add_argument(&apos;--opencv&apos;, &apos;-cv&apos;, action=&apos;store_true&apos;, help=&apos;use opencv if set &apos;)</span><br><span class="line">    # 必需参数</span><br><span class="line">    parser.add_argument(&apos;--steps&apos;, &apos;-s&apos;, required=True, type=int, help=&apos;number of steps&apos;)</span><br><span class="line">    # 默认参数</span><br><span class="line">    parser.add_argument(&apos;--file&apos;, &apos;-f&apos;, default=&apos;a.txt&apos;)</span><br><span class="line">    # 候选参数</span><br><span class="line">    parser.add_argument(&apos;--data&apos;, &apos;-d&apos;, choices=[&apos;data1&apos;, &apos;data2&apos;])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    show_args(args)</span><br></pre></td></tr></table></figure><p>说明</p><ul><li>帮助信息<br>  参数<code>help</code>，用于显示在<code>-h</code>帮助信息中</li><li>标志位参数<br>  参数<code>action=&#39;store_true&#39;</code>，即保存该参数为<code>True</code></li><li>必需参数<br>  置位<code>required</code>，即运行该程序必须带上该参数，否则报错</li><li>默认参数<br>  参数<code>default</code>填写默认参数</li><li>候选参数<br>  参数<code>choices</code>填写候选参数列表</li></ul><p>运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># 显示帮助信息</span><br><span class="line">&gt; python main.py -h</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line"></span><br><span class="line">learn to use `argparse`</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message and exit</span><br><span class="line">  --opencv, -cv         use opencv if set</span><br><span class="line">  --steps STEPS, -s STEPS</span><br><span class="line">                        number of steps</span><br><span class="line">  --file FILE, -f FILE</span><br><span class="line">  --data &#123;data1,data2&#125;, -d &#123;data1,data2&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 测试必须参数</span><br><span class="line">&gt; python main.py</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line">main.py: error: the following arguments are required: --steps/-s</span><br><span class="line"></span><br><span class="line">&gt; python main.py -s 100</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试标志位参数</span><br><span class="line">&gt; python main.py -s 100 -cv</span><br><span class="line">opencv is used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试默认参数</span><br><span class="line">&gt; python main.py -s 100 -f b.txt</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">b.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试可选参数</span><br><span class="line">&gt; python main.py -s 100 -d data1</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">data1</span><br><span class="line"></span><br><span class="line">&gt; python main.py -s 100 -d data0</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line">main.py: error: argument --data/-d: invalid choice: &apos;data0&apos; (choose from &apos;data1&apos;, &apos;data2&apos;)</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Python命令行参数解析：getopt和argparse - 死胖子的博客 - CSDN博客 <a href="https://blog.csdn.net/lanzheng_1113/article/details/77574446" target="_blank" rel="noopener">https://blog.csdn.net/lanzheng_1113/article/details/77574446</a><br>Python模块之命令行参数解析 - 每天进步一点点！！！ - 博客园 <a href="https://www.cnblogs.com/madsnotes/articles/5687079.html" target="_blank" rel="noopener">https://www.cnblogs.com/madsnotes/articles/5687079.html</a><br>Python解析命令行读取参数 — argparse模块 - Arkenstone - 博客园 <a href="https://www.cnblogs.com/arkenstone/p/6250782.html" target="_blank" rel="noopener">https://www.cnblogs.com/arkenstone/p/6250782.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python生成词云图</title>
      <link href="/2019/02/17/Python%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91%E5%9B%BE/"/>
      <url>/2019/02/17/Python%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一个没什么用的小技能</p><h1 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h1><blockquote><p>wordcloud · PyPI <a href="https://pypi.org/project/wordcloud/" target="_blank" rel="noopener">https://pypi.org/project/wordcloud/</a></p></blockquote><p>安装该模块<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> pip install wordcloud</span><br></pre></td></tr></table></figure></p><p>主要用到的为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wordcloud.WordCloud(font_path=<span class="keyword">None</span>, width=<span class="number">400</span>, height=<span class="number">200</span>, margin=<span class="number">2</span>,</span><br><span class="line">                ranks_only=<span class="keyword">None</span>, prefer_horizontal=<span class="number">.9</span>, mask=<span class="keyword">None</span>, scale=<span class="number">1</span>,</span><br><span class="line">                color_func=<span class="keyword">None</span>, max_words=<span class="number">200</span>, min_font_size=<span class="number">4</span>,</span><br><span class="line">                stopwords=<span class="keyword">None</span>, random_state=<span class="keyword">None</span>, background_color=<span class="string">'black'</span>,</span><br><span class="line">                max_font_size=<span class="keyword">None</span>, font_step=<span class="number">1</span>, mode=<span class="string">"RGB"</span>,</span><br><span class="line">                relative_scaling=<span class="string">'auto'</span>, regexp=<span class="keyword">None</span>, collocations=<span class="keyword">True</span>,</span><br><span class="line">                colormap=<span class="keyword">None</span>, normalize_plurals=<span class="keyword">True</span>, contour_width=<span class="number">0</span>,</span><br><span class="line">                contour_color=<span class="string">'black'</span>, repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><h1 id="使用例程"><a href="#使用例程" class="headerlink" title="使用例程"></a>使用例程</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"></span><br><span class="line">font = <span class="string">'C:/Windows/Fonts/SIMYOU.TTF'</span>    <span class="comment"># 幼圆</span></span><br><span class="line">string = <span class="string">'LouisHsu 单键 小叔叔 想静静 95后 傲娇 skrrrrrrr 大猫座 佛了 要秃 嘤嘤嘤 真香'</span></span><br><span class="line"></span><br><span class="line">mask = cv2.imread(<span class="string">'./mask.jpg'</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">thresh, mask = cv2.threshold(mask, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">        font_path=font, </span><br><span class="line">        background_color=<span class="string">'white'</span>,</span><br><span class="line">        color_func=<span class="keyword">lambda</span> *args, **kwargs: (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>),</span><br><span class="line">        mask=mask,</span><br><span class="line">        max_words=<span class="number">500</span>,</span><br><span class="line">        min_font_size=<span class="number">4</span>,</span><br><span class="line">        max_font_size=<span class="keyword">None</span>,</span><br><span class="line">        contour_width=<span class="number">1</span>,</span><br><span class="line">        repeat=<span class="keyword">True</span>                     <span class="comment"># 允许词重复</span></span><br><span class="line">    )</span><br><span class="line">wc.generate_from_text(string)</span><br><span class="line">wc.to_file(<span class="string">'./wc.jpg'</span>)                  <span class="comment">#保存图片</span></span><br></pre></td></tr></table></figure><p>输入原图为<br><img src="/2019/02/17/Python生成词云图/mask.jpg" alt="mask"></p><p>生成图像<br><img src="/2019/02/17/Python生成词云图/wc.jpg" alt="wc"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>python WordCloud 简单实例 - 博客 - CSDN博客 <a href="https://blog.csdn.net/cy776719526/article/details/80171790" target="_blank" rel="noopener">https://blog.csdn.net/cy776719526/article/details/80171790</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Makefile简单教程</title>
      <link href="/2019/01/05/makefile%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B/"/>
      <url>/2019/01/05/makefile%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="准备源文件"><a href="#准备源文件" class="headerlink" title="准备源文件"></a>准备源文件</h1><p>新建目录<code>demo/</code>，其结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">demo</span><br><span class="line">├─bin# 二进制文件，即可执行文件</span><br><span class="line">├─include# 头文件`.h`</span><br><span class="line">│      hello.h</span><br><span class="line">│</span><br><span class="line">├─obj# 目标文件`.o`</span><br><span class="line">└─src# 源文件`.c`</span><br><span class="line">        hello.c</span><br><span class="line">        test.c</span><br></pre></td></tr></table></figure></p><p>编辑<code>hello.h</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __HELLO_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __HELLO_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> __hello();</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></p><p>编辑<code>hello.c</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"hello.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> __hello()</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Hello world!\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>编辑<code>test.h</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"hello.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">__hello();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="命令行编译"><a href="#命令行编译" class="headerlink" title="命令行编译"></a>命令行编译</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test.c</span><br><span class="line"><span class="meta">$</span> gcc test.c hello.c -o test</span><br><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test  test.c</span><br><span class="line"><span class="meta">$</span> ./test</span><br><span class="line">Hello world!</span><br></pre></td></tr></table></figure><p>但是这样编译会每次都重新编译整个工程，时间比较长，所以可以先生成<code>.o</code>文件，当<code>test.c</code>代码改动后，重新生成<code>test.o</code>即可，<code>hello.o</code>不用重新编译<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test.c</span><br><span class="line"><span class="meta">$</span> gcc test.c hello.c -c</span><br><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  hello.o  makefile  README.md  test.c  test.o</span><br><span class="line"><span class="meta">$</span> gcc test.o hello.o -o test</span><br><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  hello.o  makefile  README.md  test  test.c  test.o</span><br><span class="line"><span class="meta">$</span> ./test</span><br><span class="line">Hello world!</span><br></pre></td></tr></table></figure></p><h1 id="Makefile"><a href="#Makefile" class="headerlink" title="Makefile"></a>Makefile</h1><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;target&gt;: &lt;dependencies&gt;</span></span><br><span class="line">&lt;command&gt;<span class="comment"># [TAB]&lt;command&gt;</span></span><br></pre></td></tr></table></figure><p>例如<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">test: test.c</span></span><br><span class="line">gcc test.c -o test</span><br></pre></td></tr></table></figure></p><p>编辑<code>Makefile</code><br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># directories &amp; target name</span></span><br><span class="line">DIR_INC = ./<span class="keyword">include</span>  </span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line">DIR_BIN = ./bin</span><br><span class="line">TARGET  = test</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile macro  </span></span><br><span class="line">CC = gcc</span><br><span class="line">CFLAGS = -g -Wall -I$&#123;DIR_INC&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">all: $&#123;TARGET&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="section">$&#123;TARGET&#125;: $&#123;DIR_OBJ&#125;/hello.o $&#123;DIR_OBJ&#125;/test.o</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(DIR_OBJ)</span>/hello.o <span class="variable">$(DIR_OBJ)</span>/test.o -o $&#123;TARGET&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">$&#123;DIR_OBJ&#125;/hello.o: <span class="variable">$(DIR_SRC)</span>/hello.c</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(DIR_SRC)</span>/hello.c -o $&#123;DIR_OBJ&#125;/hello.o</span><br><span class="line"></span><br><span class="line"><span class="section">$&#123;DIR_OBJ&#125;/test.o: <span class="variable">$(DIR_SRC)</span>/test.c</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(DIR_SRC)</span>/test.c -o $&#123;DIR_OBJ&#125;/test.o</span><br></pre></td></tr></table></figure></p><p>或通用性格式<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># directories &amp; target name</span></span><br><span class="line">DIR_INC = ./<span class="keyword">include</span>  </span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line">DIR_BIN = ./bin</span><br><span class="line">TARGET  = test</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile macro  </span></span><br><span class="line">CC      = gcc</span><br><span class="line">CFLAGS  = -g -Wall -I$&#123;DIR_INC&#125;                 <span class="comment"># `-g`表示调试选项，`-Wall`表示编译后显示所有警告</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load source files</span></span><br><span class="line">SRC = <span class="variable">$(<span class="built_in">wildcard</span> $&#123;DIR_SRC&#125;/*.c)</span>                         <span class="comment"># 匹配目录中所有的`.c`文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build target</span></span><br><span class="line">OBJ = <span class="variable">$(<span class="built_in">patsubst</span> %.c, $&#123;DIR_OBJ&#125;/%.o, $&#123;<span class="built_in">notdir</span> $&#123;SRC&#125;&#125;)</span>  <span class="comment"># 由`SRC`字符串内容，指定生成`.o`文件的名称与目录</span></span><br><span class="line">BIN = $&#123;DIR_BIN&#125;/$&#123;TARGET&#125;                               <span class="comment"># 指定可执行文件名称与目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build</span></span><br><span class="line"><span class="section">$&#123;BIN&#125;: $&#123;OBJ&#125;</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(OBJ)</span> -o <span class="variable">$@</span>                               <span class="comment"># 即 `$ gcc ./obj/*.o -o ./bin/test`</span></span><br><span class="line"><span class="section">$&#123;DIR_OBJ&#125;/%.o: $&#123;DIR_SRC&#125;/%.c</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> -c <span class="variable">$&lt;</span> -o <span class="variable">$@</span>                      <span class="comment"># 即 `$ gcc ./src/*.c -g -Wall -I./include -c ./obj/*.o`</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># clean</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">.PHONY</span>: clean                                            # 伪目标</span></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">find $&#123;DIR_OBJ&#125; -name *.o -exec rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></figure></p><p>执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> make</span><br><span class="line">gcc -g -Wall -I./include   -c src/hello.c -o obj/hello.o</span><br><span class="line">gcc -g -Wall -I./include   -c src/test.c -o obj/test.o</span><br><span class="line">gcc  ./obj/hello.o  ./obj/test.o  -o bin/test</span><br><span class="line"><span class="meta">$</span> ls obj/</span><br><span class="line">hello.o  test.o</span><br><span class="line"><span class="meta">$</span> ls bin/</span><br><span class="line">test</span><br><span class="line"><span class="meta">$</span> ./bin/test </span><br><span class="line">Hello world!</span><br><span class="line"><span class="meta">$</span> make clean</span><br><span class="line">find ./obj -name *.o -exec rm -rf &#123;&#125; \;</span><br><span class="line"><span class="meta">$</span> ls obj/</span><br><span class="line"><span class="meta">$</span> ls bin/</span><br><span class="line">test</span><br></pre></td></tr></table></figure></p><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><ol><li>符号 <code>$@</code>, <code>$^</code>, <code>$&lt;</code>，<code>$?</code><ul><li><code>$@</code>: 表示目标文件</li><li><code>$^</code>: 表示所有的依赖文件</li><li><code>$&lt;</code>: 表示第一个依赖文件</li><li><code>$?</code>: 表示比目标还要新的依赖文件列表</li></ul></li><li><code>wildcard</code>，<code>notdir</code>，<code>patsubst</code><ul><li><code>wildcard</code>    : 扩展通配符<br>  <code>SOURCES = $(wildcard *.c)</code>: 产生一个所有以 ’.c’ 结尾的文件的列表，然后存入变量 SOURCES 里。 </li><li><code>notdir</code>    : 去除路径，可以在使用<code>wildcard</code>函数后，再配合使用<code>notdir</code>函数只得到文件名（不含路径）。</li><li><code>patsubst</code>    : 替换通配符，需要３个参数，第一个是个需要匹配的式样，第二个表示用什么来替换他，第三个是个需要被处理的由空格分隔的字列。<br>  <code>OBJS = $(patsubst %.c,%.o,$(SOURCES))</code><pre><code>  - 将处理所有在 SOURCES 字列中的字（一列文件名），如果他的 结尾是 `.c` ，就用 `.o` 把 `.c`取代  - 这里的 % 符号将匹配一个或多个字符，而他每次所匹配的字串叫做一个‘柄’(stem)   - 在第二个参数里， %被解读成用第一参数所匹配的那个柄。</code></pre></li></ul></li><li><code>-I</code>，<code>-L</code>，<code>-l</code><ul><li><code>-I</code>: 将指定目录作为第一个寻找头文件的目录</li><li><code>-L</code>: 将指定目录作为第一个寻找库文件的目录</li><li><code>-l</code>: 在库文件路径中寻找<code>.so</code>动态库文件（如果gcc编译选项中加入了<code>-static</code>表示寻找<code>.a</code>静态库文件）</li></ul></li><li><code>.PHONY</code>后面的<code>target</code>表示的也是一个伪造的<code>target</code>, 而不是真实存在的文件<code>target</code>，注意<code>Makefile</code>的<code>target</code>默认是文件。</li></ol><h2 id="关于三个函数的使用"><a href="#关于三个函数的使用" class="headerlink" title="关于三个函数的使用"></a>关于三个函数的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DIR_INC = ./include</span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line"></span><br><span class="line">SRC = $(wildcard $&#123;DIR_SRC&#125;/*.c)# 指定编译当前目录下所有`.c`文件，全路径`./src/*.c`</span><br><span class="line">DIR = $(notdir $&#123;SRC&#125;)# 去除路径名，只留下文件名`*.c`</span><br><span class="line">OBJ = $(patsubst %.c, $&#123;DIR_OBJ&#125;/%.o, $&#123;DIR&#125;)# 将`DIR`中匹配到的`%.c`，替换为`$&#123;DIR_OBJ&#125;/%.o`</span><br><span class="line"></span><br><span class="line">ALL:</span><br><span class="line">@echo $(SRC)</span><br><span class="line">@echo $(DIR)</span><br><span class="line">@echo $(OBJ)</span><br></pre></td></tr></table></figure><p>执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> make</span><br><span class="line">./src/hello.c ./src/test.c</span><br><span class="line">hello.c test.c</span><br><span class="line">./obj/hello.o ./obj/test.o</span><br></pre></td></tr></table></figure></p><blockquote><p>注：若<code>./src</code>目录下还有子目录<code>./src/inc</code>，则<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SRC = <span class="variable">$(<span class="built_in">wildcard</span> $&#123;DIR_SRC&#125;/*.c)</span> <span class="variable">$(<span class="built_in">wildcard</span> $&#123;DIR_SRC&#125;/inc/*.c)</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Makefile 使用总结 - wang_yb - 博客园 <a href="https://www.cnblogs.com/wang_yb/p/3990952.html" target="_blank" rel="noopener">https://www.cnblogs.com/wang_yb/p/3990952.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CMake编译库文件</title>
      <link href="/2019/01/05/Cmake%E7%BC%96%E8%AF%91%E5%BA%93%E6%96%87%E4%BB%B6/"/>
      <url>/2019/01/05/Cmake%E7%BC%96%E8%AF%91%E5%BA%93%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>库文件即源代码的二进制文件，我们通常把一些公用函数制作成函数库，供其它程序使用。函数库分为静态库和动态库两种。静态库在程序编译时会被连接到目标代码中，程序运行时将不再需要该静态库；动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此在程序运行时还需要动态库存在。</p><h1 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h1><p>以DarkNet为例，我们将其源代码编译成<code>.a</code>静态库文件。</p><ol><li><p>下载源码</p><blockquote><p>YOLO: Real-Time Object Detection <a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">https://pjreddie.com/darknet/yolo/</a></p></blockquote> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/pjreddie/darknet</span><br></pre></td></tr></table></figure></li><li><p>整理文件<br> 我们将<code>include/</code>与<code>src/</code>目录复制到新建文件夹<code>darknet/</code>。目录结构如下</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">darknet</span><br><span class="line">├── include</span><br><span class="line">│   └── darknet.h</span><br><span class="line">└── src</span><br><span class="line">    ├── activation_kernels.cu</span><br><span class="line">    ├── activation_layer.c</span><br><span class="line">    ├── activation_layer.h</span><br><span class="line">    ├── ...</span><br><span class="line">    ├── utils.c</span><br><span class="line">    ├── utils.h</span><br><span class="line">    ├── yolo_layer.c</span><br><span class="line">    └── yolo_layer.h</span><br></pre></td></tr></table></figure></li><li><p>在<code>darknet/</code>目录下编写<code>CmakeLists.txt</code>文件，内容如下</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">CMAKE_MINIMUM_REQUIRED(VERSION 2.8)                                # cmake需要的最小版本号</span><br><span class="line">PROJECT(darknet)                                        # 项目名</span><br><span class="line"></span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;----------------------------------------------------------------------&quot;</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;project name:      &quot; $&#123;PROJECT_NAME&#125;           # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;source directory:  &quot; $&#123;PROJECT_SOURCE_DIR&#125;     # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;binary directory:  &quot; $&#123;PROJECT_BINARY_DIR&#125;     # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;----------------------------------------------------------------------&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">INCLUDE_DIRECTORIES(                                # 头文件目录</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/include</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/src</span><br><span class="line">)                                                   </span><br><span class="line">AUX_SOURCE_DIRECTORY(                               # 源文件</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/src </span><br><span class="line">    lib_srcfile</span><br><span class="line">)                                                   </span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)  # 设置保存`.a`的目录</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">ADD_LIBRARY(                                        # 生成库文件，可选择`.a`或`.so`</span><br><span class="line">    $&#123;PROJECT_NAME&#125;</span><br><span class="line">    STATIC                                          # `.a`</span><br><span class="line">    # SHARED                                        # `.so`</span><br><span class="line">    $&#123;lib_srcfile&#125;</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">SET_TARGET_PROPERTIES(</span><br><span class="line">    $&#123;PROJECT_NAME&#125;</span><br><span class="line">    PROPERTIES</span><br><span class="line">    LINKER_LANGUAGE C</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>执行命令<br> 我们在<code>darknet/</code>目录打开终端</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir build</span><br><span class="line"><span class="meta">$</span> cd build</span><br><span class="line">/build$ cmake ..</span><br><span class="line">-- The C compiler identification is GNU 7.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 7.4.0</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/louishsu/Work/Codes/MTCNN_Darknet/darknet/build</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> make</span><br><span class="line">Scanning dependencies of target darknet</span><br><span class="line">[  2%] Building C object CMakeFiles/darknet.dir/src/activation_layer.c.o</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">[ 97%] Building C object CMakeFiles/darknet.dir/src/yolo_layer.c.o</span><br><span class="line"><span class="meta">[100%</span>] Linking C shared library lib/libdarknet.so</span><br><span class="line"><span class="meta">[100%</span>] Built target darknet</span><br></pre></td></tr></table></figure></li></ol><p>在<code>darknet/build/lib</code>目录下即可找到<code>libdarknet.a</code>库文件，<code>build</code>目录结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">build</span><br><span class="line">└── lib</span><br><span class="line">    └── libdarknet.so (*)</span><br></pre></td></tr></table></figure></p><h1 id="调用库函数"><a href="#调用库函数" class="headerlink" title="调用库函数"></a>调用库函数</h1><p>为测试该库函数是否编译成功，编写测试代码，新建目录<code>/test/</code>，其文件结构为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test</span><br><span class="line">├── include</span><br><span class="line">│   └── test.h</span><br><span class="line">├── src</span><br><span class="line">│   └── test.c</span><br><span class="line">├── build</span><br><span class="line">│   └── test</span><br><span class="line">└── CMakeLists.txt</span><br></pre></td></tr></table></figure></p><p>头文件<code>/include/test.h</code>内容为<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> TEST_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TEST_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"darknet.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></p><p>源文件<code>/src/test.c</code>内容如下<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"test.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Hello! Darknet!\n"</span>);</span><br><span class="line">matrix M = make_matrix(<span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"The size of matrix M is %ld bytes\n"</span>, <span class="keyword">sizeof</span>(M));</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>编译文件<code>CMakeLists.txt</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">CMAKE_MINIMUM_REQUIRED(VERSION 2.8)                        # cmake需要的最小版本号</span><br><span class="line">PROJECT(Test)                               # 项目名</span><br><span class="line"></span><br><span class="line">ADD_DEFINITIONS(-DOPENCV=1)</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">SET(DARKNET ../darknet)</span><br><span class="line">INCLUDE_DIRECTORIES(                                            # 头文件目录</span><br><span class="line">$&#123;DARKNET&#125;/include</span><br><span class="line">$&#123;DARKNET&#125;/src</span><br><span class="line">)          </span><br><span class="line">LINK_DIRECTORIES(                                               # 库文件目录</span><br><span class="line">    $&#123;DARKNET&#125;/build/lib</span><br><span class="line">)                  </span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">INCLUDE_DIRECTORIES(./include)                                # 当前项目头文件目录</span><br><span class="line">AUX_SOURCE_DIRECTORY(./src SRC_FILES)                          # 当前项目源文件目录</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">ADD_EXECUTABLE($&#123;PROJECT_NAME&#125; $&#123;SRC_FILES&#125;)                 # 添加可执行文件</span><br><span class="line">TARGET_LINK_LIBRARIES(# 引用库</span><br><span class="line">$&#123;PROJECT_NAME&#125;</span><br><span class="line">darknet# darknet</span><br><span class="line">m# 数学函数库</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>执行指令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir build</span><br><span class="line"><span class="meta">$</span> cd build</span><br><span class="line"><span class="meta">$</span> cmake ..</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/louishsu/Work/Codes/makefile/test/build</span><br><span class="line"><span class="meta">$</span> make</span><br><span class="line">Scanning dependencies of target Test</span><br><span class="line">[ 50%] Building C object CMakeFiles/Test.dir/src/test.c.o</span><br><span class="line"><span class="meta">[100%</span>] Linking C executable Test</span><br><span class="line"><span class="meta">[100%</span>] Built target Test</span><br></pre></td></tr></table></figure></p><p>运行可执行文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ./Test</span><br><span class="line">Hello! Darknet!</span><br><span class="line">The size of matrix M is 16 bytes</span><br></pre></td></tr></table></figure></p><p>查看结构体<code>matrix</code>定义<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">matrix</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> rows, cols;</span><br><span class="line">    <span class="keyword">float</span> **vals;</span><br><span class="line">&#125; matrix;</span><br></pre></td></tr></table></figure></p><p><code>int</code>占<code>32bit</code>，<code>float*</code>占<code>64bit</code>，故<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(32bit * 2 + 64bit) / 8 = 16byte</span><br></pre></td></tr></table></figure></p><p>运行成功。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>静态库和动态库的优缺点 - 默默淡然 - 博客园 <a href="https://www.cnblogs.com/liangxiaofeng/p/3228145.html" target="_blank" rel="noopener">https://www.cnblogs.com/liangxiaofeng/p/3228145.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu编译安装Tensorflow</title>
      <link href="/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/"/>
      <url>/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/</url>
      
        <content type="html"><![CDATA[<h1 id="非常重要"><a href="#非常重要" class="headerlink" title="非常重要"></a>非常重要</h1><p>如果中途出现错误，<code>xxxx</code>文件找不到，不要怀疑！就是大天朝的网络问题！推荐科学上网！</p><h1 id="安装CUDA与CUDNN"><a href="#安装CUDA与CUDNN" class="headerlink" title="安装CUDA与CUDNN"></a>安装CUDA与CUDNN</h1><p>首先查看显卡是否支持<code>CUDA</code>加速，输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure></p><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/nvidia-smi.png" alt="nvidia-smi"></p><p>在<code>Ubuntu16.04 LTS</code>下，推荐安装<code>CUDA9.0</code>和<code>CUDNN 7</code>。</p><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn1.png" alt="cuda_cudnn1"></p><ul><li><p>CUDA</p><blockquote><p>CUDA Toolkit 9.0 Downloads | NVIDIA Developer <a href="https://developer.nvidia.com/cuda-90-download-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-90-download-archive</a></p></blockquote><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda.png" alt="cuda"></p><p>  下载<code>.run</code>版本，安装方法如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chmod +x cuda_9.0.176_384.81_linux.run </span><br><span class="line">$ sudo sh ./cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure><p>  服务条款很长。。。。</p></li></ul><ul><li><p>CUDNN</p><blockquote><p>NVIDIA cuDNN | NVIDIA Developer <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a></p></blockquote><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cudnn1.png" alt="cudnn1"></p><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cudnn2.png" alt="cudnn2"></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz</span><br><span class="line">$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</span><br><span class="line">$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>  安装后进行验证</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cp -r /usr/src/cudnn_samples_v7/ $HOME</span><br><span class="line">$ cd  $HOME/cudnn_samples_v7/mnistCUDNN</span><br><span class="line">$ make clean &amp;&amp; make</span><br><span class="line">$ ./mnistCUDNN</span><br></pre></td></tr></table></figure><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying.png" alt="cuda_cudnn_verifying"></p><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying2.png" alt="cuda_cudnn_verifying2"></p></li></ul><h1 id="编译Tensorflow-CPU-version"><a href="#编译Tensorflow-CPU-version" class="headerlink" title="编译Tensorflow(CPU version)"></a>编译Tensorflow(CPU version)</h1><p>由于训练代码使用<code>Python</code>实现，故<code>C++</code>版本的<code>Tensorflow</code>不使用<code>GPU</code>，仅实现预测代码即可。</p><h2 id="bazel"><a href="#bazel" class="headerlink" title="bazel"></a>bazel</h2><blockquote><p>Installing Bazel on Ubuntu - Bazel <a href="https://docs.bazel.build/versions/master/install-ubuntu.html" target="_blank" rel="noopener">https://docs.bazel.build/versions/master/install-ubuntu.html</a><br>一定要用源码安装！！！</p></blockquote><p>download the Bazel binary installer named <code>bazel-&lt;version&gt;-installer-linux-x86_64.sh</code> from the <a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">Bazel releases page on GitHub</a>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python</span><br><span class="line">$ chmod +x bazel-&lt;version&gt;-installer-linux-x86_64.sh</span><br><span class="line">$ ./bazel-&lt;version&gt;-installer-linux-x86_64.sh --user</span><br><span class="line">$ sudo nano ~/.bashrc # export PATH=&quot;$PATH:$HOME/bin&quot;</span><br><span class="line">$ source ~/.bashrc </span><br><span class="line">$ bazel version</span><br></pre></td></tr></table></figure><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/bazel.png" alt="bazel"></p><h2 id="编译CPU版本的CPU"><a href="#编译CPU版本的CPU" class="headerlink" title="编译CPU版本的CPU"></a>编译CPU版本的CPU</h2><p>查看<code>java</code>版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br><span class="line">openjdk version &quot;1.8.0_191&quot;</span><br><span class="line">OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)</span><br><span class="line">OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br></pre></td></tr></table></figure></p><h2 id="安装依赖软件包环境"><a href="#安装依赖软件包环境" class="headerlink" title="安装依赖软件包环境"></a>安装依赖软件包环境</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install python3-dev</span><br><span class="line">$ pip3 install six</span><br><span class="line">$ pip3 install numpy</span><br><span class="line">$ pip3 instal wheel</span><br></pre></td></tr></table></figure><h2 id="下载Tensorflow源码"><a href="#下载Tensorflow源码" class="headerlink" title="下载Tensorflow源码"></a>下载<code>Tensorflow</code>源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/tensorflow/tensorflow</span><br></pre></td></tr></table></figure><h2 id="编译与安装"><a href="#编译与安装" class="headerlink" title="编译与安装"></a>编译与安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd tensorflow</span><br><span class="line">$ ./configure</span><br></pre></td></tr></table></figure><p>配置选项如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command "bazel shutdown".</span><br><span class="line">INFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5</span><br><span class="line">You have bazel 0.20.0 installed.</span><br><span class="line">Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Found possible Python library paths:</span><br><span class="line">  /usr/local/lib/python3.5/dist-packages</span><br><span class="line">  /usr/lib/python3/dist-packages</span><br><span class="line">Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n</span><br><span class="line">No XLA JIT support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n</span><br><span class="line">No OpenCL SYCL support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with ROCm support? [y/N]: n</span><br><span class="line">No ROCm support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N]: n</span><br><span class="line">No CUDA support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to download a fresh release of clang? (Experimental) [y/N]: n</span><br><span class="line">Clang will not be downloaded.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with MPI support? [y/N]: n</span><br><span class="line">No MPI support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Please specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -march=native -Wno-sign-compare]: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n</span><br><span class="line">Not configuring the WORKSPACE for Android builds.</span><br><span class="line"></span><br><span class="line">Preconfigured Bazel build configs. You can use any of the below by adding "--config=&lt;&gt;" to your build command. See .bazelrc for more details.</span><br><span class="line">--config=mkl         # Build with MKL support.</span><br><span class="line">--config=monolithic  # Config for mostly static monolithic build.</span><br><span class="line">--config=gdr         # Build with GDR support.</span><br><span class="line">--config=verbs       # Build with libverbs support.</span><br><span class="line">--config=ngraph      # Build with Intel nGraph support.</span><br><span class="line">--config=dynamic_kernels# (Experimental) Build kernels into separate shared objects.</span><br><span class="line">Preconfigured Bazel build configs to DISABLE default on features:</span><br><span class="line">--config=noaws       # Disable AWS S3 filesystem support.</span><br><span class="line">--config=nogcp       # Disable GCP support.</span><br><span class="line">--config=nohdfs      # Disable HDFS support.</span><br><span class="line">--config=noignite    # Disable Apacha Ignite support.</span><br><span class="line">--config=nokafka     # Disable Apache Kafka support.</span><br><span class="line">--config=nonccl      # Disable NVIDIA NCCL support.</span><br><span class="line">Configuration finished</span><br></pre></td></tr></table></figure></p><p>使用<code>bazel</code>编译<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bazel build --config=opt //tensorflow:libtensorflow_cc.so</span><br></pre></td></tr></table></figure></p><p>出现错误</p><blockquote><p>TF failing to build on Bazel CI · Issue #19464 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/issues/19464" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/19464</a><br>Failure to build TF 1.12 from source - multiple definitions in grpc · Issue #23402 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197</a><br>Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/pull/23583" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/pull/23583</a><br>Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5</a></p></blockquote><p>解决方法</p><ul><li>方法1<br>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/tools_bazel.rc.png" alt="tools_bazel.rc"></li><li><p>方法2<br>  将<code>tools/bazel.rc</code>中内容粘到<code>.tf_configure.bazelrc</code>中，每次重新配置后需要重新粘贴一次。</p></li><li><p>源码安装<code>protobuf3.6.0</code></p><blockquote><p><a href="https://github.com/protocolbuffers/protobuf" target="_blank" rel="noopener">https://github.com/protocolbuffers/protobuf</a></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></blockquote></li><li><p>下载其他文件</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./tensorflow/contrib/makefile/download_dependencies.sh</span><br><span class="line">mkdir /tmp/eigen</span><br></pre></td></tr></table></figure><ul><li>值得注意，<code>download_dependencies.sh</code>中下载依赖包时，需要用到<code>curl</code>，但是默认方式安装  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install curl</span><br></pre></td></tr></table></figure></li></ul></li></ul><pre><code>    &gt; 现在是2018/12/19/02:48，被这个问题折腾了3个小时。时不支持`https`协议，故需要安装`OpenSSL`，并源码安装，详细资料见[curl提示不支持https协议解决方法 - 标配的小号 - 博客园](https://www.cnblogs.com/biaopei/p/8669810.html)- 执行`./autogen.sh`时，发生错误`autoreconf: not found`，则安装    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install autoconf aotomake libtool</span><br><span class="line">$ sudo apt install libffi-dev</span><br></pre></td></tr></table></figure></code></pre><ul><li>源码安装<code>Eigen</code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd tensorflow/contrib/makefile/Downloads/eigen</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></li></ul><h1 id="调用C-版本的Tensorflow"><a href="#调用C-版本的Tensorflow" class="headerlink" title="调用C++版本的Tensorflow"></a>调用C++版本的Tensorflow</h1><p>创建文件目录如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">|-- tf_test</span><br><span class="line">    |-- build</span><br><span class="line">    |-- main.cpp</span><br><span class="line">    |-- CMakeLists.txt</span><br></pre></td></tr></table></figure></p><p><code>main.cpp</code>文件内容如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/cc/client/client_session.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/cc/ops/standard_ops.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/core/framework/tensor.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> tensorflow;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> tensorflow::ops;</span><br><span class="line">    Scope root = Scope::NewRootScope();</span><br><span class="line">    <span class="comment">// Matrix A = [3 2; -1 0]</span></span><br><span class="line">    <span class="keyword">auto</span> A = Const(root, &#123; &#123;<span class="number">3.f</span>, <span class="number">2.f</span>&#125;, &#123;<span class="number">-1.f</span>, <span class="number">0.f</span>&#125;&#125;);</span><br><span class="line">    <span class="comment">// Vector b = [3 5]</span></span><br><span class="line">    <span class="keyword">auto</span> b = Const(root, &#123; &#123;<span class="number">3.f</span>, <span class="number">5.f</span>&#125;&#125;);</span><br><span class="line">    <span class="comment">// v = Ab^T</span></span><br><span class="line">    <span class="keyword">auto</span> v = MatMul(root.WithOpName(<span class="string">"v"</span>), A, b, MatMul::TransposeB(<span class="literal">true</span>));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Tensor&gt; outputs;</span><br><span class="line">    <span class="function">ClientSession <span class="title">session</span><span class="params">(root)</span></span>;</span><br><span class="line">    <span class="comment">// Run and fetch v</span></span><br><span class="line">    TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs));</span><br><span class="line">    <span class="comment">// Expect outputs[0] == [19; -3]</span></span><br><span class="line">    LOG(INFO) &lt;&lt; outputs[<span class="number">0</span>].matrix&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>CMakeLists.txt</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required (VERSION 2.8.8)</span><br><span class="line">project (tf_example)</span><br><span class="line"></span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -g -std=c++11 -W&quot;)</span><br><span class="line"></span><br><span class="line">set(EIGEN_DIR /usr/local/include/eigen3)</span><br><span class="line">set(PROTOBUF_DIR/usr/local/include/google/protobuf)</span><br><span class="line">set(TENSORFLOW_DIR /home/louishsu/install/tensorflow-1.12.0)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line">$&#123;EIGEN_DIR&#125;</span><br><span class="line">$&#123;PROTOBUF_DIR&#125;</span><br><span class="line">   $&#123;TENSORFLOW_DIR&#125;</span><br><span class="line">$&#123;TENSORFLOW_DIR&#125;/bazel-genfiles</span><br><span class="line">$&#123;TENSORFLOW_DIR&#125;/tensorflow/contrib/makefile/downloads/absl</span><br><span class="line">)</span><br><span class="line">link_directories(</span><br><span class="line">/usr/local/lib</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">add_executable(</span><br><span class="line">tf_test</span><br><span class="line">main.cpp</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">target_link_libraries(</span><br><span class="line">tf_test</span><br><span class="line">tensorflow_cc</span><br><span class="line">tensorflow_framework</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir build &amp;&amp; cd build</span><br><span class="line">$ cmake .. &amp;&amp; make</span><br><span class="line">$ ./tf_test</span><br></pre></td></tr></table></figure><h1 id="install-tensorflow-gpu-for-python"><a href="#install-tensorflow-gpu-for-python" class="headerlink" title="install tensorflow-gpu for python"></a>install tensorflow-gpu for python</h1><p>可使用<code>pip</code>指令安装，推荐下载安装包，</p><blockquote><p>tensorflow · PyPI <a href="https://pypi.org/project/tensorflow/" target="_blank" rel="noopener">https://pypi.org/project/tensorflow/</a></p></blockquote><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/tensorflow_for_python.png" alt="tensorflow_for_python"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd ~/Downloads</span><br><span class="line">$ pip3 --default-timeout=1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user</span><br></pre></td></tr></table></figure><p>安装后进行验证<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> python3</span><br><span class="line">Python 3.5.2 (default, Nov 12 2018, 13:43:14) </span><br><span class="line">[GCC 5.4.0 20160609] on linux</span><br><span class="line">Type "help", "copyright", "credits" or "license" for more information.</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; import tensorflow as tf</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess = tf.Session()</span><br><span class="line">2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: </span><br><span class="line">name: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758</span><br><span class="line">pciBusID: 0000:04:00.0</span><br><span class="line">totalMemory: 983.44MiB freeMemory: 177.19MiB</span><br><span class="line">2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0</span><br><span class="line">2018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 </span><br><span class="line">2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N </span><br><span class="line">2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; a = tf.Variable([233])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; init = tf.initialize_all_variables()</span><br><span class="line">WARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.</span><br><span class="line">Instructions for updating:</span><br><span class="line">Use `tf.global_variables_initializer` instead.</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess.run(init)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess.run(a)</span><br><span class="line">array([233], dtype=int32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess.close()</span><br></pre></td></tr></table></figure></p><p>注意，如果异常中断程序，显存不会被释放，需要自行<code>kill</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure></p><p>获得<code>PID</code>序号，使用指令结束进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -9 pid</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>TensorFlow C++动态库编译 - 简书 <a href="https://www.jianshu.com/p/d46596558640" target="_blank" rel="noopener">https://www.jianshu.com/p/d46596558640</a><br>Tensorflow C++ 从训练到部署(1)：环境搭建 | 技术刘 <a href="http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu编译安装OpenCV</title>
      <link href="/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV/"/>
      <url>/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV/</url>
      
        <content type="html"><![CDATA[<h1 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h1><blockquote><p>OpenCV library <a href="https://opencv.org/" target="_blank" rel="noopener">https://opencv.org/</a></p></blockquote><h1 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h1><h2 id="依赖软件包"><a href="#依赖软件包" class="headerlink" title="依赖软件包"></a>依赖软件包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install cmake</span><br><span class="line">$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev</span><br></pre></td></tr></table></figure><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ unzip opencv-3.4.4.zip</span><br><span class="line">$ cd opencv-3.4.4</span><br><span class="line">$ mkdir build &amp;&amp; cd build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make -j4</span><br></pre></td></tr></table></figure><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo make install</span><br><span class="line">$ sudo nano /etc/ld.so.conf.d/opencv.conf # add `/usr/local/lib`</span><br><span class="line">$ sudo ldconfig</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p><code>OpenCV</code>自带验证程序，在<code>opencv-3.4.4/samples/cpp/example_cmake</code>中可以找到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cd opencv-3.4.4/samples/cpp/example_cmake</span><br><span class="line">$ cmake .</span><br><span class="line">$ make</span><br><span class="line">$ ./opencv_example</span><br></pre></td></tr></table></figure><p>如果没问题，可以看到你的大脸了~</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Ubuntu16.04安装openCV3.4.4 - 辣屁小心的学习笔记 - CSDN博客 <a href="https://blog.csdn.net/weixin_39992397/article/details/84345197" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39992397/article/details/84345197</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python读写配置文件</title>
      <link href="/2019/01/04/Python%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"/>
      <url>/2019/01/04/Python%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>在深度学习中，有许多运行参数需要指定，有几种方法可以解决</p><ul><li>定义<code>.py</code>文件存储变量</li><li>定义命名元组<code>collections.namedtuple()</code></li><li>创建<code>.config</code>，<code>.ini</code>等配置文件</li></ul><p>Python 读取写入配置文件很方便，使用内置模块<code>configparser</code>即可</p><h1 id="读出"><a href="#读出" class="headerlink" title="读出"></a>读出</h1><p>首先创建文件<code>test.config</code>或<code>test.ini</code>，写入如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[db]</span><br><span class="line">db_port = 3306</span><br><span class="line">db_user = root</span><br><span class="line">db_host = 127.0.0.1</span><br><span class="line">db_pass = test</span><br><span class="line"></span><br><span class="line">[concurrent]</span><br><span class="line">processor = 20</span><br><span class="line">thread = 10</span><br></pre></td></tr></table></figure></p><p>读取操作如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; import configparser</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; configfile = &quot;./test.config&quot;</span><br><span class="line">&gt;&gt;&gt; inifile = &quot;./test.ini&quot;</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf = configparser.ConfigParser()</span><br><span class="line">&gt;&gt;&gt; cf.read(configfile)                     # 读取文件内容</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; sections = cf.sections()                # 所有的section，以列表的形式返回</span><br><span class="line">&gt;&gt;&gt; sections</span><br><span class="line">[&apos;db&apos;, &apos;concurrent&apos;]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; options = cf.options(&apos;db&apos;)              # 该section的所有option</span><br><span class="line">&gt;&gt;&gt; options</span><br><span class="line">[&apos;db_port&apos;, &apos;db_user&apos;, &apos;db_host&apos;, &apos;db_pass&apos;]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; items = cf.items(&apos;db&apos;)                  # 该section的所有键值对</span><br><span class="line">&gt;&gt;&gt; items</span><br><span class="line">[(&apos;db_port&apos;, &apos;3306&apos;), (&apos;db_user&apos;, &apos;root&apos;), (&apos;db_host&apos;, &apos;127.0.0.1&apos;), (&apos;db_pass&apos;, &apos;test&apos;)]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; db_user = cf.get(&apos;db&apos;, &apos;db_user&apos;)       # section中option的值，返回为string类型</span><br><span class="line">&gt;&gt;&gt; db_user</span><br><span class="line">&apos;root&apos;</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; db_port = cf.getint(&apos;db&apos;, &apos;db_port&apos;)    # 得到section中option的值，返回为int类型</span><br><span class="line">&gt;&gt;&gt;                                         # 类似的还有getboolean()与getfloat()</span><br><span class="line">&gt;&gt;&gt; db_port</span><br><span class="line">3306</span><br></pre></td></tr></table></figure></p><h1 id="写入"><a href="#写入" class="headerlink" title="写入"></a>写入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; import configparser</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf = configparser.ConfigParser()</span><br><span class="line">&gt;&gt;&gt; cf.add_section(&apos;test1&apos;)                 # 新增section</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1)              # 新增option：错误示范</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;pyshell#7&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    cf.set(&quot;test&quot;, &quot;count&quot;, 1)</span><br><span class="line">  File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1192, in set</span><br><span class="line">    self._validate_value_types(option=option, value=value)</span><br><span class="line">  File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1177, in _validate_value_types</span><br><span class="line">    raise TypeError(&quot;option values must be strings&quot;)</span><br><span class="line">TypeError: option values must be strings</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, &apos;1&apos;)            # 新增option</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test1&quot;, &quot;opt1&quot;, &apos;ok&apos;)           # 新增option</span><br><span class="line">&gt;&gt;&gt; cf.remove_option(&quot;test1&quot;, &quot;opt1&quot;)       # 删除option</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.add_section(&apos;test2&apos;)                 # 新增section</span><br><span class="line">&gt;&gt;&gt; cf.remove_section(&apos;test2&apos;)              # 删除section</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; with open(&quot;./test_wr.config&quot;, &apos;w+&apos;) as f:</span><br><span class="line">        cf.write(f)                         # 写入文件test_wr.config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>现在目录已创建文件<code>test_wr.config</code>，打开可以看到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[test1]</span><br><span class="line">count = 1</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python更新安装的包</title>
      <link href="/2019/01/04/Python%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85/"/>
      <url>/2019/01/04/Python%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<p><code>pip</code>不提供升级全部已安装模块的方法，以下指令可查看更新信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip list --outdate</span><br></pre></td></tr></table></figure></p><p>得到输出信息如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Package           Version   Latest     Type</span><br><span class="line">----------------- --------- ---------- -----</span><br><span class="line">absl-py           0.3.0     0.6.1      sdist</span><br><span class="line">autopep8          1.3.5     1.4.2      sdist</span><br><span class="line">bleach            2.1.4     3.0.2      wheel</span><br><span class="line">certifi           2018.8.24 2018.10.15 wheel</span><br><span class="line">dask              0.20.0    0.20.1     wheel</span><br><span class="line">grpcio            1.14.1    1.16.0     wheel</span><br><span class="line">ipykernel         5.0.0     5.1.0      wheel</span><br><span class="line">ipython           7.0.1     7.1.1      wheel</span><br><span class="line">jedi              0.12.1    0.13.1     wheel</span><br><span class="line">jupyter-console   5.2.0     6.0.0      wheel</span><br><span class="line">Markdown          2.6.11    3.0.1      wheel</span><br><span class="line">MarkupSafe        1.0       1.1.0      wheel</span><br><span class="line">matplotlib        2.2.2     3.0.2      wheel</span><br><span class="line">mistune           0.8.3     0.8.4      wheel</span><br><span class="line">numpy             1.14.5    1.15.4     wheel</span><br><span class="line">opencv-python     3.4.2.17  3.4.3.18   wheel</span><br><span class="line">Pillow            5.2.0     5.3.0      wheel</span><br><span class="line">prometheus-client 0.3.1     0.4.2      sdist</span><br><span class="line">pyparsing         2.2.0     2.3.0      wheel</span><br><span class="line">python-dateutil   2.7.3     2.7.5      wheel</span><br><span class="line">pytz              2018.5    2018.7     wheel</span><br><span class="line">urllib3           1.23      1.24.1     wheel</span><br></pre></td></tr></table></figure></p><p>以下提供一键升级的方法，可能比较久hhhh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pip._internal.utils.misc import get_installed_distributions</span><br><span class="line">from subprocess import call</span><br><span class="line"> </span><br><span class="line">for dist in get_installed_distributions():</span><br><span class="line">    modulename = dist.project_name</span><br><span class="line">    print(&apos;start processing module &apos; + modulename)</span><br><span class="line">    call(&quot;pip install --upgrade &quot; + modulename, shell=True)</span><br><span class="line">    print(&apos;module &apos; + modulename + &apos;done!&apos;)</span><br></pre></td></tr></table></figure></p><p>另外，从已有的安装列表，安装所需要的包，可使用以下指令</p><ul><li><p>在已安装的机器中，生成列表</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; xxx.list</span><br></pre></td></tr></table></figure></li><li><p>在未安装的机器中，使用列表安装</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r xxx.list</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python记录日志</title>
      <link href="/2019/01/04/Python%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/"/>
      <url>/2019/01/04/Python%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>日志可以用来记录应用程序的状态、错误和信息消息，也经常作为调试程序的工具。<br><code>Python</code>提供了一个标准的日志接口，就是<code>logging</code>模块。日志级别有<code>DEBUG</code>、<code>INFO</code>、<code>WARNING</code>、<code>ERROR</code>、<code>CRITICAL</code>五种。</p><p><a href="https://docs.python.org/3/library/logging.html" target="_blank" rel="noopener">logging — Logging facility for Python — Python 3.7.1 documentation</a></p><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><h2 id="logger对象"><a href="#logger对象" class="headerlink" title="logger对象"></a><code>logger</code>对象</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import logging</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger = logging.getLogger(__name__)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger</span><br><span class="line">&lt;Logger __main__ (WARNING)&gt;</span><br></pre></td></tr></table></figure><h2 id="日志级别"><a href="#日志级别" class="headerlink" title="日志级别"></a>日志级别</h2><p>可输出五种不同的日志级别，分别为有<code>DEBUG</code>、<code>INFO</code>、<code>WARNING</code>、<code>ERROR</code>、<code>CRITICAL</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.debug('test log')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.info('test log')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.warning('test log')</span><br><span class="line">test log</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.error('test log')</span><br><span class="line">test log</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.critical('test log')</span><br><span class="line">test log</span><br></pre></td></tr></table></figure></p><p>可以看到只有<code>WARNING</code>及以上级别日志被输出，这是由于默认的日志级别是<code>WARNING</code> ，所以低于此级别的日志不会记录。</p><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.basicConfig(**kwarg)</span><br></pre></td></tr></table></figure><p><code>**kwarg</code>中部分参数如下</p><ul><li><p><code>format</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%(levelname)：日志级别的名字格式</span><br><span class="line">%(levelno)s：日志级别的数字表示</span><br><span class="line">%(name)s：日志名字</span><br><span class="line">%(funcName)s：函数名字</span><br><span class="line">%(asctime)：日志时间，可以使用datefmt去定义时间格式，如上图。</span><br><span class="line">%(pathname)：脚本的绝对路径</span><br><span class="line">%(filename)：脚本的名字</span><br><span class="line">%(module)：模块的名字</span><br><span class="line">%(thread)：thread id</span><br><span class="line">%(threadName)：线程的名字</span><br></pre></td></tr></table></figure></li><li><p><code>datefmt</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;%Y-%m-%d %H:%M:%S&apos;</span><br></pre></td></tr></table></figure></li><li><p><code>level</code><br>  默认为<code>ERROR</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">logging.DEBUG</span><br><span class="line">logging.INFO</span><br><span class="line">logging.WARNING</span><br><span class="line">logging.ERROR</span><br><span class="line">logging.CRITICAL</span><br></pre></td></tr></table></figure></li></ul><p>例如<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 未输出debug</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger = logging.getLogger()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.debug('test log')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 修改配置</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; log_format = '%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; log_datefmt = '%Y-%m-%d %H:%M:%S'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; log_level = logging.DEBUG</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logging.basicConfig(format=log_format, </span><br><span class="line">                        datefmt=log_datefmt, </span><br><span class="line">                        level=log_level)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 输出debug</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger = logging.getLogger()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.debug('test log')</span><br><span class="line">&lt;pyshell#8&gt; [2018-11-13 11:59:52] [DEBUG] test log</span><br></pre></td></tr></table></figure></p><h2 id="输出到日志文件"><a href="#输出到日志文件" class="headerlink" title="输出到日志文件"></a>输出到日志文件</h2><p>保存代码为文件<code>log_test.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">log_format = <span class="string">'%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'</span></span><br><span class="line">log_datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">log_level = logging.DEBUG</span><br><span class="line">log_filename = <span class="string">'./test.log'</span></span><br><span class="line">log_filemode = <span class="string">'a'</span>  <span class="comment"># 也可以为'w', 'w+'等</span></span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=log_format,</span><br><span class="line">                    datefmt=log_datefmt, </span><br><span class="line">                    level=log_level,</span><br><span class="line">                    filename=log_filename, </span><br><span class="line">                    filemode=log_filemode)</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line">logger.debug(<span class="string">'test log'</span>)</span><br><span class="line">logger.info(<span class="string">'test log'</span>)</span><br><span class="line">logger.warning(<span class="string">'test log'</span>)</span><br><span class="line">logger.error(<span class="string">'test log'</span>)</span><br><span class="line">logger.critical(<span class="string">'test log'</span>)</span><br></pre></td></tr></table></figure></p><p>运行完毕，打开<code>log_test.log</code>文件可以看到<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log_test.py [2018-11-13 12:11:04] [DEBUG] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [INFO] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [WARNING] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [ERROR] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [CRITICAL] test log</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+Github博客搭建</title>
      <link href="/2019/01/04/Github-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/01/04/Github-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>那么问题来了，现有的博客还是现有的这篇文章呢？</p><h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><p>安装<a href="https://nodejs.org/en/" target="_blank" rel="noopener">node.js</a>, <a href="https://git-scm.com/" target="_blank" rel="noopener">git</a>, <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a></p><h1 id="博客搭建"><a href="#博客搭建" class="headerlink" title="博客搭建"></a>博客搭建</h1><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>推荐使用<code>git</code>命令窗口，执行如下指令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir Blog</span><br><span class="line"><span class="meta">$</span> cd Blog</span><br><span class="line"><span class="meta">$</span> hexo init</span><br><span class="line">INFO  Cloning hexo-starter to ~\Desktop\Blog</span><br><span class="line">Cloning into 'C:\Users\LouisHsu\Desktop\Blog'...</span><br><span class="line">remote: Enumerating objects: 68, done.</span><br><span class="line">remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68</span><br><span class="line">Unpacking objects: 100% (68/68), done.</span><br><span class="line">Submodule 'themes/landscape' (https://github.com/hexojs/hexo-theme-landscape.git) registered for path 'themes/landscape'</span><br><span class="line">Cloning into 'C:/Users/LouisHsu/Desktop/Blog/themes/landscape'...</span><br><span class="line">remote: Enumerating objects: 1, done.</span><br><span class="line">remote: Counting objects: 100% (1/1), done.</span><br><span class="line">remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866</span><br><span class="line">Receiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.</span><br><span class="line">Resolving deltas: 100% (459/459), done.</span><br><span class="line">Submodule path 'themes/landscape': checked out '73a23c51f8487cfcd7c6deec96ccc7543960d350'</span><br><span class="line">Install dependencies</span><br><span class="line">npm WARN deprecated titlecase@1.1.2: no longer maintained</span><br><span class="line">npm WARN deprecated postinstall-build@5.0.3: postinstall-build's behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span> nunjucks@3.1.6 postinstall C:\Users\LouisHsu\Desktop\Blog\node_modules\nunjucks</span><br><span class="line"><span class="meta">&gt;</span> node postinstall-build.js src</span><br><span class="line"></span><br><span class="line">npm notice created a lockfile as package-lock.json. You should commit this file.</span><br><span class="line">npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):</span><br><span class="line">npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)</span><br><span class="line"></span><br><span class="line">added 422 packages from 501 contributors and audited 4700 packages in 59.195s</span><br><span class="line">found 0 vulnerabilities</span><br><span class="line"></span><br><span class="line">INFO  Start blogging with Hexo!</span><br></pre></td></tr></table></figure></p><p>生成目录结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\-- scaffolds</span><br><span class="line">\-- source</span><br><span class="line">    \-- _posts</span><br><span class="line">\-- themes</span><br><span class="line">|-- _config.yml</span><br><span class="line">|-- package.json</span><br></pre></td></tr></table></figure></p><p>继续<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> npm install</span><br><span class="line">npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):</span><br><span class="line">npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)</span><br><span class="line"></span><br><span class="line">audited 4700 packages in 5.99s</span><br><span class="line">found 0 vulnerabilities</span><br></pre></td></tr></table></figure></p><p>现在该目录执行指令，开启<code>hexo</code>服务器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo s</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure></p><p><img src="/2019/01/04/Github-Hexo博客搭建/hexo_server.png" alt="hexo_server"></p><h2 id="生成目录和标签"><a href="#生成目录和标签" class="headerlink" title="生成目录和标签"></a>生成目录和标签</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hexo n page about</span><br><span class="line"><span class="meta">$</span> hexo n page archives</span><br><span class="line"><span class="meta">$</span> hexo n page categories</span><br><span class="line"><span class="meta">$</span> hexo n page tags</span><br></pre></td></tr></table></figure><p>修改<code>/source/tags/index.md</code>，其他同理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">01| ---</span><br><span class="line">02| title: tags</span><br><span class="line">03| date: 2019-01-04 17:34:15</span><br><span class="line">04| ---</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">01| ---</span><br><span class="line">02| title: tags</span><br><span class="line">03| date: 2019-01-04 17:34:15</span><br><span class="line">04| type: &quot;tags&quot;</span><br><span class="line">05| comments: false</span><br><span class="line">06| ---</span><br></pre></td></tr></table></figure></p><h2 id="关联Github"><a href="#关联Github" class="headerlink" title="关联Github"></a>关联<code>Github</code></h2><p>在<code>Github</code>新建一个仓库，命名为<code>username.github.io</code>，例如<code>isLouisHsu.github.io</code>，新建时勾选<code>Initialize this repository with a README</code>，因为这个仓库必须不能为空。<br><img src="/2019/01/04/Github-Hexo博客搭建/github_io.png" alt="github_io"></p><p>打开博客目录下的<code>_config.yml</code>配置文件，定位到最后的<code>deploy</code>选项，修改如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repository: git@github.com:isLouisHsu/isLouisHsu.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure></p><p>安装插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>现在就可以将该目录内容推送到<code>Github</code>新建的仓库中了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure></p><h2 id="使用个人域名"><a href="#使用个人域名" class="headerlink" title="使用个人域名"></a>使用个人域名</h2><ol><li>在<code>source</code>目录下新建文件<code>CNAME</code>，输入解析后的个人域名</li><li>在<code>Github</code>主页修改域名</li></ol><h1 id="备份博客"><a href="#备份博客" class="headerlink" title="备份博客"></a>备份博客</h1><blockquote><p>没。没什么用<br>我。我不备份了<br>可以新建一个仓库专门保存文件试试</p></blockquote><p>现在博客的源文件仅保存在<code>PC</code>上， 我们对它们进行备份，并将仓库作为博客文件夹</p><ol><li>在仓库新建分支<code>hexo</code>，设置为默认分支<br> <img src="/2019/01/04/Github-Hexo博客搭建/create_branch_hexo.png" alt="create_branch_hexo"><br> <img src="/2019/01/04/Github-Hexo博客搭建/change_branch_hexo.png" alt="change_branch_hexo"></li><li><p>将仓库克隆至本地</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/isLouisHsu/isLouisHsu.github.io.git</span><br></pre></td></tr></table></figure></li><li><p>克隆文件<br> 将之前的Hexo文件夹中的</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scffolds/</span><br><span class="line">source/</span><br><span class="line">themes/</span><br><span class="line">.gitignore</span><br><span class="line">_config.yml</span><br><span class="line">package.json</span><br></pre></td></tr></table></figure><p> 复制到克隆下来的仓库文件夹<code>isLouisHsu.github.io</code><br> <img src="/2019/01/04/Github-Hexo博客搭建/backup_blog.png" alt="backup_blog"></p></li><li><p>安装包</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ npm install hexo --save</span><br><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p> 备份博客使用以下指令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;backup&quot;</span><br><span class="line">$ git push origin hexo</span><br></pre></td></tr></table></figure></li><li><p>部署博客指令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure></li><li><p><code>单键</code>提交<br> 编写脚本<code>commit.bat</code>，双击即可</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &apos;backup&apos;</span><br><span class="line">git push origin hexo</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure></li></ol><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><ul><li><p>目录结构</p><ul><li><code>public</code>  生成的网站文件，发布的站点文件。</li><li><code>source</code>  资源文件夹，用于存放内容。</li><li><code>tag</code>     标签文件夹。</li><li><code>archive</code> 归档文件夹。</li><li><code>category</code>分类文件夹。</li><li><code>downloads/code include code</code>文件夹。</li><li><code>:lang i18n_dir</code> 国际化文件夹。</li><li><code>_config.yml</code> 配置文件</li></ul></li><li><p>指令</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ hexo help</span><br><span class="line">Usage: hexo &lt;command&gt;</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">    clean     Remove generated files and cache.</span><br><span class="line">    config    Get or set configurations.</span><br><span class="line">    deploy    Deploy your website.</span><br><span class="line">    generate  Generate static files.</span><br><span class="line">    help      Get help on a command.</span><br><span class="line">    init      Create a new Hexo folder.</span><br><span class="line">    list      List the information of the site</span><br><span class="line">    migrate   Migrate your site from other system to Hexo.</span><br><span class="line">    new       Create a new post.</span><br><span class="line">    publish   Moves a draft post from _drafts to _posts folder.</span><br><span class="line">    render    Render files with renderer plugins.</span><br><span class="line">    server    Start the server.</span><br><span class="line">    version   Display version information.</span><br><span class="line"></span><br><span class="line">Global Options:</span><br><span class="line">    --config  Specify config file instead of using _config.yml</span><br><span class="line">    --cwd     Specify the CWD</span><br><span class="line">    --debug   Display all verbose messages in the terminal</span><br><span class="line">    --draft   Display draft posts</span><br><span class="line">    --safe    Disable all plugins and scripts</span><br><span class="line">    --silent  Hide output on console</span><br><span class="line"></span><br><span class="line">For more help, you can use &apos;hexo help [command]&apos; for the detailed information or you can check the docs: http://hexo.io/docs/</span><br></pre></td></tr></table></figure></li></ul><!-- # 修改主题 --><h1 id="拓展功能支持"><a href="#拓展功能支持" class="headerlink" title="拓展功能支持"></a>拓展功能支持</h1><h2 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><p>修改文件<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure></p><p>在执行<code>$ hexo n [layout] &lt;title&gt;</code>时会生成同名文件夹，把图片放在这个文件夹内，在<code>.md</code>文件中插入图片<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![image_name](/title/image_name.png)</span><br></pre></td></tr></table></figure></p><h2 id="搜索功能"><a href="#搜索功能" class="headerlink" title="搜索功能"></a>搜索功能</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-searchdb --save</span><br><span class="line">$ npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><p>站点配置文件<code>_config.yml</code>中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure></p><p>修改主题配置文件<code>/themes/xxx/_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure></p><h2 id="带过滤功能的首页插件"><a href="#带过滤功能的首页插件" class="headerlink" title="带过滤功能的首页插件"></a>带过滤功能的首页插件</h2><p>在首页只显示指定分类下面的文章列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-index2 --save</span><br><span class="line">$ npm uninstall hexo-generator-index --save</span><br></pre></td></tr></table></figure></p><p>修改<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">index_generator:</span><br><span class="line">  per_page: 10</span><br><span class="line">  order_by: -date</span><br><span class="line">  include:</span><br><span class="line">    - category Web  # 只包含Web分类下的文章</span><br><span class="line">  exclude:</span><br><span class="line">    - tag Hexo      # 不包含标签为Hexo的文章</span><br></pre></td></tr></table></figure></p><h2 id="数学公式支持"><a href="#数学公式支持" class="headerlink" title="数学公式支持"></a>数学公式支持</h2><p><code>hexo</code>默认的渲染引擎是<code>marked</code>，但是<code>marked</code>不支持<code>mathjax</code>。<code>kramed</code>是在<code>marked</code>的基础上进行修改。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-math --save              # 停止使用 hexo-math</span><br><span class="line">$ npm install hexo-renderer-mathjax --save    # 安装hexo-renderer-mathjax包：</span><br><span class="line">$ npm uninstall hexo-renderer-marked --save   # 卸载原来的渲染引擎</span><br><span class="line">$ npm install hexo-renderer-kramed --save     # 安装新的渲染引擎</span><br></pre></td></tr></table></figure></p><p>修改<code>/node_modules/kramed/lib/rules/inline.js</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">11| escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">...</span><br><span class="line">20| em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">11| escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">...</span><br><span class="line">20| em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><p>修改<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">64| // Change inline math rule</span><br><span class="line">65| function formatText(text) &#123;</span><br><span class="line">66|   // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">67|   return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">68| &#125;</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">64| // Change inline math rule</span><br><span class="line">65| function formatText(text) &#123;</span><br><span class="line">66|   // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">67|   // return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">68|   return text;</span><br><span class="line">69| &#125;</span><br></pre></td></tr></table></figure></p><p>在主题中开启<code>mathjax</code>开关，例如<code>next</code>主题中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br></pre></td></tr></table></figure></p><p>在文章中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: title.md</span><br><span class="line">date: 2019-01-04 12:47:37</span><br><span class="line">categories:</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">top:</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>测试</p><script type="math/tex; mode=display">A = \left[\begin{matrix}    a_{11} & a_{12} \\    a_{21} & a_{22}\end{matrix}\right]</script><h2 id="背景图片更换"><a href="#背景图片更换" class="headerlink" title="背景图片更换"></a>背景图片更换</h2><p>在主题配置文件夹中，如<code>next</code>主题，打开文件<code>hexo-theme-next/source/css/_custom/custom.styl</code>，修改为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Custom styles.</span><br><span class="line"></span><br><span class="line">// 添加背景图片</span><br><span class="line">body &#123;</span><br><span class="line">  background: url(/images/background.jpg);</span><br><span class="line">  background-size: cover;</span><br><span class="line">  background-repeat: no-repeat;</span><br><span class="line">  background-attachment: fixed;</span><br><span class="line">  background-position: 50% 50%;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 修改主体透明度</span><br><span class="line">.main-inner &#123;</span><br><span class="line">  background: #fff;</span><br><span class="line">  opacity: 0.95;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 修改菜单栏透明度</span><br><span class="line">.header-inner &#123;</span><br><span class="line">  opacity: 0.95;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="背景音乐"><a href="#背景音乐" class="headerlink" title="背景音乐"></a>背景音乐</h2><p>首先生成外链</p><p><img src="/2019/01/04/Github-Hexo博客搭建/bgm1.jpg" alt="bgm1"></p><p><img src="/2019/01/04/Github-Hexo博客搭建/bgm2.jpg" alt="bgm2"></p><p>添加到合适位置，如<code>Links</code>一栏后</p><p><img src="/2019/01/04/Github-Hexo博客搭建/bgm3.jpg" alt="bgm3"></p><h2 id="鼠标特效"><a href="#鼠标特效" class="headerlink" title="鼠标特效"></a>鼠标特效</h2><ol><li><p><a href="https://github.com/hustcc/canvas-nest.js" target="_blank" rel="noopener">hustcc/canvas-nest.js</a></p></li><li><p>点击文本特效<br>新建<code>hexo-theme-next/source/js/click_show_text.js</code></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a_idx = <span class="number">0</span>;</span><br><span class="line">jQuery(<span class="built_in">document</span>).ready(<span class="function"><span class="keyword">function</span>(<span class="params">$</span>) </span>&#123;</span><br><span class="line">    $(<span class="string">"body"</span>).click(<span class="function"><span class="keyword">function</span>(<span class="params">e</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> a = <span class="keyword">new</span> <span class="built_in">Array</span></span><br><span class="line">        (<span class="string">"for"</span>, <span class="string">"while"</span>, <span class="string">"catch"</span>, <span class="string">"except"</span>, <span class="string">"if"</span>, <span class="string">"range"</span>, </span><br><span class="line">        <span class="string">"class"</span>, <span class="string">"min"</span>, <span class="string">"max"</span>, <span class="string">"sort"</span>, <span class="string">"map"</span>, <span class="string">"filter"</span>, </span><br><span class="line">        <span class="string">"lambda"</span>, <span class="string">"switch"</span>, <span class="string">"case"</span>, <span class="string">"iter"</span>, <span class="string">"next"</span>, <span class="string">"enum"</span>, <span class="string">"struct"</span>,  </span><br><span class="line">        <span class="string">"void"</span>, <span class="string">"int"</span>, <span class="string">"float"</span>, <span class="string">"double"</span>, <span class="string">"char"</span>, <span class="string">"signed"</span>, <span class="string">"unsigned"</span>);</span><br><span class="line">        <span class="keyword">var</span> $i = $(<span class="string">"&lt;span/&gt;"</span>).text(a[a_idx]);</span><br><span class="line">        a_idx = (a_idx + <span class="number">3</span>) % a.length;</span><br><span class="line">        <span class="keyword">var</span> x = e.pageX, </span><br><span class="line">        y = e.pageY;</span><br><span class="line">        $i.css(&#123;</span><br><span class="line">            <span class="string">"z-index"</span>: <span class="number">5</span>,</span><br><span class="line">            <span class="string">"top"</span>: y - <span class="number">20</span>,</span><br><span class="line">            <span class="string">"left"</span>: x,</span><br><span class="line">            <span class="string">"position"</span>: <span class="string">"absolute"</span>,</span><br><span class="line">            <span class="string">"font-weight"</span>: <span class="string">"bold"</span>,</span><br><span class="line">            <span class="string">"color"</span>: <span class="string">"#333333"</span></span><br><span class="line">        &#125;);</span><br><span class="line">        $(<span class="string">"body"</span>).append($i);</span><br><span class="line">        $i.animate(&#123;</span><br><span class="line">            <span class="string">"top"</span>: y - <span class="number">180</span>,</span><br><span class="line">            <span class="string">"opacity"</span>: <span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line"><span class="number">3000</span>,</span><br><span class="line"><span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    $i.remove();</span><br><span class="line">&#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">    setTimeout(<span class="string">'delay()'</span>, <span class="number">2000</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">delay</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    $(<span class="string">".buryit"</span>).removeAttr(<span class="string">"onclick"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>在文件<code>hexo-theme-next/layout/_layout.swig</code>中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/click_show_text.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p><h2 id="看板娘"><a href="#看板娘" class="headerlink" title="看板娘"></a>看板娘</h2><p><a href="https://github.com/xiazeyu/live2d-widget-models" target="_blank" rel="noopener">xiazeyu/live2d-widget-models</a>，预览效果见<a href="https://huaji8.top/post/live2d-plugin-2.0/" target="_blank" rel="noopener">作者博客</a>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-helper-live2d</span><br><span class="line">npm install live2d-widget-model-hijiki</span><br></pre></td></tr></table></figure><p>站点配置文件添加<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">live2d:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">scriptFrom:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">model:</span> </span><br><span class="line"><span class="attr">use:</span> <span class="string">live2d-widget-model-hijiki</span> <span class="comment">#模型选择</span></span><br><span class="line"><span class="attr">display:</span> </span><br><span class="line"><span class="attr">position:</span> <span class="string">right</span>  <span class="comment">#模型位置</span></span><br><span class="line"><span class="attr">width:</span> <span class="number">150</span>       <span class="comment">#模型宽度</span></span><br><span class="line"><span class="attr">height:</span> <span class="number">300</span>      <span class="comment">#模型高度</span></span><br><span class="line"><span class="attr">mobile:</span> </span><br><span class="line"><span class="attr">show:</span> <span class="literal">false</span>      <span class="comment">#是否在手机端显示</span></span><br></pre></td></tr></table></figure></p><h2 id="人体时钟"><a href="#人体时钟" class="headerlink" title="人体时钟"></a>人体时钟</h2><p>新建<code>hexo-theme-next/source/js/honehone_clock_tr.js</code></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/******************************************************************************</span></span><br><span class="line"><span class="comment">初期設定</span></span><br><span class="line"><span class="comment">******************************************************************************/</span></span><br><span class="line"><span class="keyword">var</span> swfUrl = <span class="string">"http://chabudai.sakura.ne.jp/blogparts/honehoneclock/honehone_clock_tr.swf"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> swfTitle = <span class="string">"honehoneclock"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 実行</span></span><br><span class="line">LoadBlogParts();</span><br><span class="line"></span><br><span class="line"><span class="comment">/******************************************************************************</span></span><br><span class="line"><span class="comment">入力なし</span></span><br><span class="line"><span class="comment">出力document.writeによるHTML出力</span></span><br><span class="line"><span class="comment">******************************************************************************/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">LoadBlogParts</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line"><span class="keyword">var</span> sUrl = swfUrl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> sHtml = <span class="string">""</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;object classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://fpdownload.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=8,0,0,0" width="160" height="70" id="'</span> + swfTitle + <span class="string">'" align="middle"&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="allowScriptAccess" value="always" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="movie" value="'</span> + sUrl + <span class="string">'" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="quality" value="high" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="bgcolor" value="#ffffff" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="wmode" value="transparent" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;embed wmode="transparent" src="'</span> + sUrl + <span class="string">'" quality="high" bgcolor="#ffffff" width="160" height="70" name="'</span> + swfTitle + <span class="string">'" align="middle" allowScriptAccess="always" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;/object&gt;'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">document</span>.write(sHtml);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script charset=&quot;Shift_JIS&quot; src=&quot;/js/honehone_clock_tr.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><h2 id="代码雨"><a href="#代码雨" class="headerlink" title="代码雨"></a>代码雨</h2><p>新建<code>hexo-theme-next/source/js/digital_rain.js</code><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.onload = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="comment">//获取画布对象</span></span><br><span class="line">    <span class="keyword">var</span> canvas = <span class="built_in">document</span>.getElementById(<span class="string">"canvas"</span>);</span><br><span class="line">    <span class="comment">//获取画布的上下文</span></span><br><span class="line">    <span class="keyword">var</span> context =canvas.getContext(<span class="string">"2d"</span>);</span><br><span class="line">    <span class="keyword">var</span> s = <span class="built_in">window</span>.screen;</span><br><span class="line">    <span class="keyword">var</span> W = canvas.width = s.width;</span><br><span class="line">    <span class="keyword">var</span> H = canvas.height;</span><br><span class="line">    <span class="comment">//获取浏览器屏幕的宽度和高度</span></span><br><span class="line">    <span class="comment">//var W = window.innerWidth;</span></span><br><span class="line">    <span class="comment">//var H = window.innerHeight;</span></span><br><span class="line">    <span class="comment">//设置canvas的宽度和高度</span></span><br><span class="line">    canvas.width = W;</span><br><span class="line">    canvas.height = H;</span><br><span class="line">    <span class="comment">//每个文字的字体大小</span></span><br><span class="line">    <span class="keyword">var</span> fontSize = <span class="number">12</span>;</span><br><span class="line">    <span class="comment">//计算列</span></span><br><span class="line">    <span class="keyword">var</span> colunms = <span class="built_in">Math</span>.floor(W /fontSize);</span><br><span class="line">    <span class="comment">//记录每列文字的y轴坐标</span></span><br><span class="line">    <span class="keyword">var</span> drops = [];</span><br><span class="line">    <span class="comment">//给每一个文字初始化一个起始点的位置</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>;i&lt;colunms;i++)&#123;</span><br><span class="line">        drops.push(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//运动的文字</span></span><br><span class="line">    <span class="keyword">var</span> str =<span class="string">"WELCOME TO WWW.ITRHX.COM"</span>;</span><br><span class="line">    <span class="comment">//4:fillText(str,x,y);原理就是去更改y的坐标位置</span></span><br><span class="line">    <span class="comment">//绘画的函数</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">draw</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        context.fillStyle = <span class="string">"rgba(238,238,238,.08)"</span>;<span class="comment">//遮盖层</span></span><br><span class="line">        context.fillRect(<span class="number">0</span>,<span class="number">0</span>,W,H);</span><br><span class="line">        <span class="comment">//给字体设置样式</span></span><br><span class="line">        context.font = <span class="string">"600 "</span>+fontSize+<span class="string">"px  Georgia"</span>;</span><br><span class="line">        <span class="comment">//给字体添加颜色</span></span><br><span class="line">        context.fillStyle = [<span class="string">"#33B5E5"</span>, <span class="string">"#0099CC"</span>, <span class="string">"#AA66CC"</span>, <span class="string">"#9933CC"</span>, <span class="string">"#99CC00"</span>, <span class="string">"#669900"</span>, <span class="string">"#FFBB33"</span>, <span class="string">"#FF8800"</span>, <span class="string">"#FF4444"</span>, <span class="string">"#CC0000"</span>][<span class="built_in">parseInt</span>(<span class="built_in">Math</span>.random() * <span class="number">10</span>)];<span class="comment">//randColor();可以rgb,hsl, 标准色，十六进制颜色</span></span><br><span class="line">        <span class="comment">//写入画布中</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>;i&lt;colunms;i++)&#123;</span><br><span class="line">            <span class="keyword">var</span> index = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * str.length);</span><br><span class="line">            <span class="keyword">var</span> x = i*fontSize;</span><br><span class="line">            <span class="keyword">var</span> y = drops[i] *fontSize;</span><br><span class="line">            context.fillText(str[index],x,y);</span><br><span class="line">            <span class="comment">//如果要改变时间，肯定就是改变每次他的起点</span></span><br><span class="line">            <span class="keyword">if</span>(y &gt;= canvas.height &amp;&amp; <span class="built_in">Math</span>.random() &gt; <span class="number">0.99</span>)&#123;</span><br><span class="line">                drops[i] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            drops[i]++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">randColor</span>(<span class="params"></span>)</span>&#123;<span class="comment">//随机颜色</span></span><br><span class="line">        <span class="keyword">var</span> r = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">256</span>);</span><br><span class="line">        <span class="keyword">var</span> g = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">256</span>);</span><br><span class="line">        <span class="keyword">var</span> b = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">256</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"rgb("</span>+r+<span class="string">","</span>+g+<span class="string">","</span>+b+<span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    draw();</span><br><span class="line">    setInterval(draw,<span class="number">35</span>);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><code>hexo-theme-next/source/css/main.styl</code>添加<br><figure class="highlight styl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">canvas</span> &#123;</span><br><span class="line">  <span class="attribute">position</span>: fixed;</span><br><span class="line">  <span class="attribute">right</span>: <span class="number">0px</span>;</span><br><span class="line">  <span class="attribute">bottom</span>: <span class="number">0px</span>;</span><br><span class="line">  <span class="attribute">min-width</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">min-height</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">height</span>: auto;</span><br><span class="line">  <span class="attribute">width</span>: auto;</span><br><span class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>hexo-theme-next/layout/_layout.swig</code>添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;canvas id=&quot;canvas&quot; width=&quot;1440&quot; height=&quot;900&quot; &gt;&lt;/canvas&gt;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/DigitalRain.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h2 id="留言板"><a href="#留言板" class="headerlink" title="留言板"></a>留言板</h2><p>用<a href="https://www.livere.com/" target="_blank" rel="noopener">来比力</a>作为后台系统。</p><p>打开主题配置文件<code>hexo-theme-next/_config.yml</code>，修改<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Support for LiveRe comments system.</span></span><br><span class="line"><span class="comment"># You can get your uid from https://livere.com/insight/myCode (General web site)</span></span><br><span class="line"><span class="attr">livere_uid:</span> <span class="string">your</span> <span class="string">uid</span></span><br></pre></td></tr></table></figure></p><p>在<code>hexo-theme-next/layout/_scripts/third-party/comments/</code> 目录中添加<code>livere.swig</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id and not theme.gentie_productKey %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;% if theme.livere_uid %&#125;</span><br><span class="line">    &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">    (function(d, s) &#123;</span><br><span class="line">        var j, e = d.getElementsByTagName(s)[0];</span><br><span class="line"></span><br><span class="line">        if (typeof LivereTower === &apos;function&apos;) &#123; return; &#125;</span><br><span class="line"></span><br><span class="line">        j = d.createElement(s);</span><br><span class="line">        j.src = &apos;https://cdn-city.livere.com/js/embed.dist.js&apos;;</span><br><span class="line">        j.async = true;</span><br><span class="line"></span><br><span class="line">        e.parentNode.insertBefore(j, e);</span><br><span class="line">    &#125;)(document, &apos;script&apos;);</span><br><span class="line">    &lt;/script&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></p><p>在<code>hexo-theme-next/layout/_scripts/third-party/comments.swig</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include &apos;./comments/livere.swig&apos; %&#125;</span><br></pre></td></tr></table></figure></p><p><strong>评论无法保留？？？换成<code>Gitment</code>。</strong></p><p>安装模块<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i --save gitment</span><br></pre></td></tr></table></figure></p><p>在<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">New OAuth App</a>为博客应用一个密钥<br><img src="/2019/01/04/Github-Hexo博客搭建/new_oauth_app.png" alt="new_oauth_app"></p><p>定位到主题配置文件，填写<code>`enable</code>，<code>github_user</code>，<code>github_repo</code>，<code>client_id</code>，<code>client_secret</code><br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gitment</span></span><br><span class="line"><span class="comment"># Introduction: https://imsun.net/posts/gitment-introduction/</span></span><br><span class="line"><span class="attr">gitment:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  mint:</span> <span class="literal">true</span> <span class="comment"># RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway</span></span><br><span class="line"><span class="attr">  count:</span> <span class="literal">true</span> <span class="comment"># Show comments count in post meta area</span></span><br><span class="line"><span class="attr">  lazy:</span> <span class="literal">false</span> <span class="comment"># Comments lazy loading with a button</span></span><br><span class="line"><span class="attr">  cleanly:</span> <span class="literal">false</span> <span class="comment"># Hide 'Powered by ...' on footer, and more</span></span><br><span class="line"><span class="attr">  language:</span> <span class="comment"># Force language, or auto switch by theme</span></span><br><span class="line"><span class="attr">  github_user:</span> <span class="comment"># MUST HAVE, Your Github Username</span></span><br><span class="line"><span class="attr">  github_repo:</span> <span class="comment"># MUST HAVE, The name of the repo you use to store Gitment comments</span></span><br><span class="line"><span class="attr">  client_id:</span> <span class="comment"># MUST HAVE, Github client id for the Gitment</span></span><br><span class="line"><span class="attr">  client_secret:</span> <span class="comment"># EITHER this or proxy_gateway, Github access secret token for the Gitment</span></span><br><span class="line"><span class="attr">  proxy_gateway:</span> <span class="comment"># Address of api proxy, See: https://github.com/aimingoo/intersect</span></span><br><span class="line"><span class="attr">  redirect_protocol:</span> <span class="comment"># Protocol of redirect_uri with force_redirect_protocol when mint enabled</span></span><br></pre></td></tr></table></figure></p><p>如果遇到登陆不上的问题，转到<a href="https://gh-oauth.imsun.net/" target="_blank" rel="noopener">gh-oauth.imsun.net</a>页面，点高级-&gt;继续访问就可以了。</p><p><strong>服务器问题不能解决，换成<code>Gitalk</code>。</strong></p><p>定位到路径 themes/next/layout/_third-party/comments下面，创建一个叫做 gitalk.swig的文件，写入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125;</span><br><span class="line">  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt;</span><br><span class="line">  &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;script src=&quot;https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">   &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">        var gitalk = new Gitalk(&#123;</span><br><span class="line">          clientID: &apos;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&apos;,</span><br><span class="line">          clientSecret: &apos;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&apos;,</span><br><span class="line">          repo: &apos;&#123;&#123; theme.gitalk.repo &#125;&#125;&apos;,</span><br><span class="line">          owner: &apos;&#123;&#123; theme.gitalk.githubID &#125;&#125;&apos;,</span><br><span class="line">          admin: [&apos;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&apos;],</span><br><span class="line">          id: md5(window.location.pathname),</span><br><span class="line">          distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos;</span><br><span class="line">        &#125;)</span><br><span class="line">        gitalk.render(&apos;gitalk-container&apos;)</span><br><span class="line">       &lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>在 上面的同级目录下的 index.swig 里面加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include &apos;gitalk.swig&apos; %&#125;</span><br></pre></td></tr></table></figure><p>在使能化之前，我们还需要修改或者说是美化一下gitalk的默认样式，如果你不进行这一步也没有影响，可能结果会丑一点。<br>定位到： themes/next/source/css/_common/components/third-party. 然后你需要创建一个 gitalk.styl 文件。</p><p>这个文件里面写入：<br><figure class="highlight styl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.gt-header</span> <span class="selector-tag">a</span>, <span class="selector-class">.gt-comments</span> <span class="selector-tag">a</span>, <span class="selector-class">.gt-popup</span> a</span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line"><span class="selector-class">.gt-container</span> <span class="selector-class">.gt-popup</span> <span class="selector-class">.gt-action</span><span class="selector-class">.is--active</span>:before</span><br><span class="line">  <span class="attribute">top</span>: <span class="number">0.7em</span>;</span><br></pre></td></tr></table></figure></p><p>然后同样的，在 third-party.styl里面导入一下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@import "gitalk";</span><br></pre></td></tr></table></figure></p><p>在 layout/_partials/comments.swig 里面加入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% elseif theme.gitalk.enable %&#125;</span><br><span class="line">  &lt;div id=&quot;gitalk-container&quot;&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></p><p>在主题配置文件<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">gitalk:</span><br><span class="line">  enable: true</span><br><span class="line">  githubID:   # MUST HAVE, Your Github Username    </span><br><span class="line">  repo:       # MUST HAVE, The name of the repo you use to store Gitment comments</span><br><span class="line">  ClientID:   # MUST HAVE, Github client id for the Gitment</span><br><span class="line">  ClientSecret: # EITHER this or proxy_gateway, Github access secret token for the Gitment</span><br><span class="line">  adminUser: isLouisHsu</span><br><span class="line">  distractionFreeMode: true</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>基于hexo+github搭建一个独立博客 - 牧云云 - 博客园 <a href="https://www.cnblogs.com/MuYunyun/p/5927491.html" target="_blank" rel="noopener">https://www.cnblogs.com/MuYunyun/p/5927491.html</a><br>hexo+github pages轻松搭博客(1) | ex2tron’s Blog <a href="http://ex2tron.wang/hexo-blog-with-github-pages-1/" target="_blank" rel="noopener">http://ex2tron.wang/hexo-blog-with-github-pages-1/</a><br>hexo下LaTeX无法显示的解决方案 - crazy_scott的博客 - CSDN博客 <a href="https://blog.csdn.net/crazy_scott/article/details/79293576" target="_blank" rel="noopener">https://blog.csdn.net/crazy_scott/article/details/79293576</a><br>在Hexo中渲染MathJax数学公式 - 简书 <a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a><br>怎么去备份你的Hexo博客 - 简书 <a href="https://www.jianshu.com/p/baab04284923" target="_blank" rel="noopener">https://www.jianshu.com/p/baab04284923</a><br>Hexo中添加本地图片 - 蜕变C - 博客园 <a href="https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral" target="_blank" rel="noopener">https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral</a><br>hexo 搜索功能 - 阿甘的博客 - CSDN博客 <a href="https://blog.csdn.net/ganzhilin520/article/details/79047983" target="_blank" rel="noopener">https://blog.csdn.net/ganzhilin520/article/details/79047983</a><br>为 Hexo 博客主题 NexT 添加 LiveRe 评论支持 <a href="https://blog.smoker.cc/web/add-comments-livere-for-hexo-theme-next.html" target="_blank" rel="noopener">https://blog.smoker.cc/web/add-comments-livere-for-hexo-theme-next.html</a><br>终于！！！记录如何在hexo next主题下配置gitalk评论系统 <a href="https://jinfagang.github.io/2018/10/07/%E7%BB%88%E4%BA%8E%EF%BC%81%EF%BC%81%EF%BC%81%E8%AE%B0%E5%BD%95%E5%A6%82%E4%BD%95%E5%9C%A8hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E9%85%8D%E7%BD%AEgitalk%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/" target="_blank" rel="noopener">https://jinfagang.github.io/2018/10/07/%E7%BB%88%E4%BA%8E%EF%BC%81%EF%BC%81%EF%BC%81%E8%AE%B0%E5%BD%95%E5%A6%82%E4%BD%95%E5%9C%A8hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E9%85%8D%E7%BD%AEgitalk%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>scikit-learn: 处理特征</title>
      <link href="/2018/11/24/scikit-learn-%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81/"/>
      <url>/2018/11/24/scikit-learn-%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<h1 id="数据预处理-preprocessing"><a href="#数据预处理-preprocessing" class="headerlink" title="数据预处理(preprocessing)"></a>数据预处理(preprocessing)</h1><h2 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import sklearn.preprocessing as preprocessing</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; dir(preprocessing)</span><br><span class="line">['Binarizer', 'CategoricalEncoder', 'FunctionTransformer', 'Imputer', 'KBinsDiscretizer', </span><br><span class="line">'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MaxAbsScaler', 'MinMaxScaler', </span><br><span class="line">'MultiLabelBinarizer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PolynomialFeatures', </span><br><span class="line">'PowerTransformer', 'QuantileTransformer', 'RobustScaler', 'StandardScaler', </span><br><span class="line">'__all__', '__builtins__', '__cached__', '__doc__', '__file__', </span><br><span class="line">'__loader__', '__name__', '__package__', '__path__', '__spec__', </span><br><span class="line">'_discretization', '_encoders', '_function_transformer', </span><br><span class="line">'add_dummy_feature', 'base', 'binarize', 'data', </span><br><span class="line">'imputation', 'label', 'label_binarize', 'maxabs_scale', </span><br><span class="line">'minmax_scale', 'normalize', 'power_transform', </span><br><span class="line">'quantile_transform', 'robust_scale', 'scale']</span><br></pre></td></tr></table></figure><h1 id="特征抽取-feature-extraction"><a href="#特征抽取-feature-extraction" class="headerlink" title="特征抽取(feature extraction)"></a>特征抽取(feature extraction)</h1><h2 id="Module-1"><a href="#Module-1" class="headerlink" title="Module"></a>Module</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import sklearn.feature_extraction as feature_extraction</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; dir(feature_extraction)</span><br><span class="line">['DictVectorizer', 'FeatureHasher', </span><br><span class="line">'__all__', '__builtins__', '__cached__', '__doc__', '__file__', </span><br><span class="line">'__loader__', '__name__', '__package__', '__path__', '__spec__', </span><br><span class="line">'_hashing', 'dict_vectorizer', 'grid_to_graph', 'hashing', </span><br><span class="line">'image', 'img_to_graph', 'stop_words', 'text']</span><br></pre></td></tr></table></figure><h1 id="特征选择-feature-selection"><a href="#特征选择-feature-selection" class="headerlink" title="特征选择(feature selection)"></a>特征选择(feature selection)</h1><p>当数据预处理完成后，我们需要选择有意义的特征，将其输入到模型中训练，主要从两个方面考虑</p><ul><li><strong>特征是否发散</strong><br>  若一个特征不发散，其方差接近$0$，则表示该特征在各个样本上没有差别，对于样本的区分没什么用；</li><li><strong>特征与目标的相关性</strong><br>  与目标<code>(target)</code>相关性高的特征，应当优先选择。</li></ul><p>特征选择的方法可以根据特征选择的形式分为$3$种</p><ul><li><strong>过滤法<code>(Filter)</code></strong><br>  按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li><li><strong>包装法<code>(Wrapper)</code></strong><br>  根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。</li><li><strong>嵌入法<code>(Embedded)</code></strong><br>  先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于<code>Filter</code>方法，但是是通过训练来确定特征的优劣。</li></ul><h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><h3 id="移除低方差"><a href="#移除低方差" class="headerlink" title="移除低方差"></a>移除低方差</h3><blockquote><p><code>Removing features with low variance</code></p></blockquote><p>即移除那些方差较小的特征，当特征的取值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。</p><p>现实中这种方法作用不大，可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> class sklearn.feature_selection.VarianceThreshold(threshold=0.0)</span><br></pre></td></tr></table></figure><p>调用例程如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = [</span><br><span class="line">            [0, 0, 1], </span><br><span class="line">            [0, 1, 0], </span><br><span class="line">            [1, 0, 0], </span><br><span class="line">            [0, 1, 1], </span><br><span class="line">            [0, 1, 0], </span><br><span class="line">            [0, 1, 1],</span><br><span class="line">    ]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; feature_selection.VarianceThreshold(threshold=(.8 * (1 - .8))).fit_transform(X) </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [1, 0],</span><br><span class="line">       [0, 0],</span><br><span class="line">       [1, 1],</span><br><span class="line">       [1, 0],</span><br><span class="line">       [1, 1]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 选出了第2, 3列特征</span><br></pre></td></tr></table></figure></p><h3 id="单变量特征选择"><a href="#单变量特征选择" class="headerlink" title="单变量特征选择"></a>单变量特征选择</h3><blockquote><p><code>Univariate feature selection</code></p></blockquote><p>分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。</p><p>指标适用情况：</p><ol><li>对于分类问题(<code>target</code>离散)<br> 卡方检验，f_classif, mutual_info_classif，互信息</li><li>对于回归问题(<code>target</code>连续)</li></ol><blockquote><p>注：分类与回归在一定程度上可以互相转换，</p></blockquote><h2 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h2><h2 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h2><h2 id="Module-2"><a href="#Module-2" class="headerlink" title="Module"></a>Module</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import sklearn.feature_selection as feature_selection</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; dir(feature_selection)</span><br><span class="line">['GenericUnivariateSelect', 'RFE', 'RFECV', </span><br><span class="line">'SelectFdr', 'SelectFpr', 'SelectFromModel', 'SelectFwe', </span><br><span class="line">'SelectKBest', 'SelectPercentile', 'VarianceThreshold', </span><br><span class="line">'__all__', '__builtins__', '__cached__', '__doc__', '__file__', </span><br><span class="line">'__loader__', '__name__', '__package__', '__path__', '__spec__', </span><br><span class="line">'base', 'chi2', 'f_classif', 'f_oneway', </span><br><span class="line">'f_regression', 'from_model', 'mutual_info_', 'mutual_info_classif', </span><br><span class="line">'mutual_info_regression', 'rfe', 'univariate_selection', 'variance_threshold']</span><br></pre></td></tr></table></figure><h1 id="参考博客-reference"><a href="#参考博客-reference" class="headerlink" title="参考博客(reference)"></a>参考博客(reference)</h1><blockquote><p>使用sklearn优雅地进行数据挖掘 - jasonfreak - 博客园 <a href="http://www.cnblogs.com/jasonfreak/p/5448462.html" target="_blank" rel="noopener">http://www.cnblogs.com/jasonfreak/p/5448462.html</a><br>特征选择 (feature_selection) - 会飞的蝸牛 - 博客园 <a href="https://www.cnblogs.com/stevenlk/p/6543628.html" target="_blank" rel="noopener">https://www.cnblogs.com/stevenlk/p/6543628.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Metrics</title>
      <link href="/2018/11/21/Metrics/"/>
      <url>/2018/11/21/Metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="回归-regression-评估指标"><a href="#回归-regression-评估指标" class="headerlink" title="回归(regression)评估指标"></a>回归(regression)评估指标</h1><h2 id="解释方差-Explained-Variance"><a href="#解释方差-Explained-Variance" class="headerlink" title="解释方差(Explained Variance)"></a>解释方差(Explained Variance)</h2><script type="math/tex; mode=display">EV(\hat{y}, y)= 1 - \frac{Var(y-\hat{y})}{Var(y)}</script><p>解释方差越接近$1$表示回归效果越好。</p><h2 id="平均绝对误差-Mean-Absolute-Error-MAE"><a href="#平均绝对误差-Mean-Absolute-Error-MAE" class="headerlink" title="平均绝对误差(Mean Absolute Error - MAE)"></a>平均绝对误差(Mean Absolute Error - MAE)</h2><script type="math/tex; mode=display">MAE(\hat{y}, y) = E(||\hat{y} - y||_1)= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} |\hat{y}^{(i)} - y^{(i)}|</script><p>$MAE$越小表示回归效果越好。</p><h2 id="平均平方误差-Mean-Squared-Error-MSE"><a href="#平均平方误差-Mean-Squared-Error-MSE" class="headerlink" title="平均平方误差(Mean Squared Error - MSE)"></a>平均平方误差(Mean Squared Error - MSE)</h2><p>在<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归</a>一节，使用的损失函数即$MSE$</p><script type="math/tex; mode=display">MSE(\hat{y}, y) = E(||\hat{y} - y||_2^2)= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2</script><p>其中$y$与$\hat{y}$均为$1$维向量，$MSE$越小表示回归效果越好。</p><p>其含义比较直观，即偏差的平方和。也可以从最小化方差的角度解释，定义误差向量</p><script type="math/tex; mode=display">e = \hat{y} - y</script><p>我们假定其期望为$0$，即</p><script type="math/tex; mode=display">E(e) = 0　或　\overline{e} = 0</script><p>那么误差的方差为</p><script type="math/tex; mode=display">Var(e) = E[(e - \overline{e})^T (e - \overline{e})] = E(||e||_2^2)</script><p>也即$MSE$。</p><h2 id="均方根误差-Root-Mean-Squared-Error-RMSE"><a href="#均方根误差-Root-Mean-Squared-Error-RMSE" class="headerlink" title="均方根误差(Root Mean Squared Error - RMSE)"></a>均方根误差(Root Mean Squared Error - RMSE)</h2><script type="math/tex; mode=display">RMSE(\hat{y}, y) = \sqrt{\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2}</script><p>实质与$MSE$是一样的。只不过用于数据更好的描述，使计算得损失的值较小。$RMSE$越小表示回归效果越好。</p><h2 id="均方对数误差-Mean-Squard-Logarithmic-Error-MSLE"><a href="#均方对数误差-Mean-Squard-Logarithmic-Error-MSLE" class="headerlink" title="均方对数误差(Mean Squard Logarithmic Error - MSLE)"></a>均方对数误差(Mean Squard Logarithmic Error - MSLE)</h2><script type="math/tex; mode=display">MSLE(\hat{y}, y) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} \left[\log (1+y^{(i)}) - \log (1+\hat{y}^{(i)})\right]^2</script><p>通常用于输出指数增长的模型，如，人口统计，商品的平均销售量，以及一段时间内的平均销售量等。注意，由对数性质，这一指标对过小的预测的惩罚大于预测过大的预测的惩罚。</p><h2 id="中值绝对误差-Median-Absolute-Error-MedAE"><a href="#中值绝对误差-Median-Absolute-Error-MedAE" class="headerlink" title="中值绝对误差(Median Absolute Error - MedAE)"></a>中值绝对误差(Median Absolute Error - MedAE)</h2><script type="math/tex; mode=display">MedAE(\hat{y}, y) = median(|y - \hat{y}|)</script><h2 id="R决定系数-R2"><a href="#R决定系数-R2" class="headerlink" title="R决定系数(R2)"></a>R决定系数(R2)</h2><p>又称拟合优度，提供了一个衡量未来样本有多好的预测模型。最佳可能的分数是$1.0$，它可以是负的(因为模型可以任意恶化)。一个常数模型总是预测$y$的期望值，而不考虑输入特性，则得到$R^2$分数为$0.0$。</p><script type="math/tex; mode=display">R^2(\hat{y}, y) = 1 - \frac{\sum_{i=1}^{n_{samples}} (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{n_{samples}} (y^{(i)} - \overline{y})^2}</script><p>其中</p><script type="math/tex; mode=display">\overline{y} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} y^{(i)}</script><h1 id="分类-classification-评估指标"><a href="#分类-classification-评估指标" class="headerlink" title="分类(classification)评估指标"></a>分类(classification)评估指标</h1><p>先作如下定义<br><img src="/2018/11/21/Metrics/terminology_and_derivations_1.png" alt="terminology_and_derivations_1"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_2.png" alt="terminology_and_derivations_2"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_3.png" alt="terminology_and_derivations_3"></p><p><img src="/2018/11/21/Metrics/metrics_classification2.png" alt="metrics_classification2"></p><h2 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率(Accuracy)"></a>准确率(Accuracy)</h2><script type="math/tex; mode=display">Accuracy(y, \hat{y})= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} 1(y^{(i)}=\hat{y}^{(i)})</script><p>也即</p><script type="math/tex; mode=display">Accuracy= \frac{TN+TP}{TN+TP+FN+FP}</script><p>精度只是简单地计算出比例，但是没有对不同类别进行区分。因为不同类别错误代价可能不同。例如：判断这个病人是不是病危，如果不是病危错误判断为病危，那只是损失一点医务人员的时间和精力，如果是把病危的人判断为非病危状态，那损失的就是一条人命。他们之间存在重要性差异，这时候就不能用精度。对于样本不均衡的情况，也不是用精度来衡量。例如：有A类1000个，B类5个，如果我把这1005个样本都预测成A类，正确率=1000/1005=99.5%。</p><h2 id="精确率-Precision-与召回率-Recall"><a href="#精确率-Precision-与召回率-Recall" class="headerlink" title="精确率(Precision)与召回率(Recall)"></a>精确率(Precision)与召回率(Recall)</h2><ul><li><p>精确率<code>(Precision)</code><br>  即预测正样本中，实际为正样本的百分比，度量了分类器不会将真正的负样本错误地分为正样本的能力。</p><script type="math/tex; mode=display">  Precision = \frac{TP}{TP+FP}</script></li><li><p>召回率<code>(Recall)</code><br>  又称查全率，即实际正样本中，被预测为正样本的百分比，度量了分类器找到所有正样本的能力。</p><script type="math/tex; mode=display">  Recall = \frac{TP}{TP + FN}</script><p><img src="/2018/11/21/Metrics/precision_recall.png" alt="precision_recall"></p></li></ul><h2 id="F度量"><a href="#F度量" class="headerlink" title="F度量"></a>F度量</h2><blockquote><p><a href="https://en.wikipedia.org/wiki/F1_score" target="_blank" rel="noopener">F1 score - Wikipedia</a></p></blockquote><ul><li><p>$F_1$<br>  为精确率<code>(Precision)</code>与召回率<code>(Recall)</code>的调和均值<code>(harmonic mean)</code>。</p><script type="math/tex; mode=display">  \frac{1}{F_1}   = \frac{1}{2} (\frac{1}{Precision} + \frac{1}{Recall})</script><p>  也即</p><script type="math/tex; mode=display">  F_1 = 2 · \frac{Precision·Recall}{Precision + Recall}</script></li><li><p>$F_{\beta}$<br>  在$F_1$度量的基础上增加权值$\beta$，$\beta$越大，$Recall$的权重越大，否则$Precision$的权重越大。</p><script type="math/tex; mode=display">  \frac{1}{F_{\beta}} = \frac{1}{1+\beta^2} \frac{1}{Precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{Recall}</script><p>  也即</p><script type="math/tex; mode=display">  F_{\beta} = (1+\beta^2)·\frac{Precision·Recall}{(\beta^2·Precision) + Recall}</script></li></ul><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><code>Confusion matrix</code>，也被称作错误矩阵<code>(Error matrix)</code>，是一个特别的表。无监督学习中，通常称作匹配矩阵<code>(Matching matrix)</code>。每一列表达了分类器对样本的类别预测，每一行表达了样本所属的真实类别。</p><p>例如我们有$27$个待分类样本，将其划分为<code>Cat</code>，<code>Dog</code>，<code>Rabbit</code>，讲实际标签与预测标签数目统计后填入混淆矩阵。</p><p><img src="/2018/11/21/Metrics/confusion_matrix.png" alt="confusion_matrix"></p><p>例如实际上有$8$个样本为<code>Cat</code>，而该分类器将其中$3$个划分为<code>Dog</code>，将$2$个为<code>Dog</code>的样本划分为<code>Cat</code>。我们可以根据上述混淆矩阵得出结论，该分类器对<code>Dog</code>和<code>Cat</code>分类能力较弱，而对<code>Rabbit</code>分类能力较强。而且正确预测的样本数目都在对角线上，很容易直观地检查表中的预测错误。</p><p>以下为<code>scikit-learn</code>中混淆矩阵的<code>API</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.metrics import confusion_matrix</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]</span><br><span class="line">&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred)</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]</span><br><span class="line">&gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; # In the binary case, we can extract true positives, etc as follows:</span><br><span class="line">&gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()</span><br><span class="line">&gt;&gt;&gt; (tn, fp, fn, tp)</span><br><span class="line">(0, 2, 1, 1)</span><br></pre></td></tr></table></figure></p><h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p><code>Receiver Operating Characteristic</code>，是根据一系列不同的二分类方式(分界值或决定阈)，以召回率(真正率<code>TPR</code>、灵敏度)为纵坐标，<code>fall-out</code>(假正率<code>FPR</code>、$1$-特异度)为横坐标绘制的曲线。</p><ul><li><p><code>true positive rate - TPR</code><br>  所有阳性样本中有多少正确的阳性结果。</p><script type="math/tex; mode=display">  TPR = \frac{TP}{P} = \frac{TP}{TP + FN}</script></li><li><p><code>false positive rate - FPR</code><br>  所有阴性样本中有多少不正确的阳性结果。</p><script type="math/tex; mode=display">  FPR = \frac{FP}{N} = \frac{FP}{FP + TN}</script></li></ul><h3 id="ROC-space"><a href="#ROC-space" class="headerlink" title="ROC space"></a>ROC space</h3><p><img src="/2018/11/21/Metrics/模型的评估指标/ROC_space.png" alt="ROC_space"></p><ul><li>分别以<code>FPR</code>与<code>TPR</code>作为横纵轴(又称灵敏度-$1$特异度曲线<code>sensitivity vs (1 − specificity) plot</code>)；</li><li>每次预测结果或混淆矩阵的实例代表了<code>ROC</code>空间中的一个点；<br>  例如上图中$A, B, C, C’$是以下表数据计算得到的点。<br>  <img src="/2018/11/21/Metrics/ROC_space_samples.png" alt="ROC_space_samples"></li><li>在<code>ROC</code>空间中最左上方的点$(0, 1)$称作完美分类器<code>(perfect classification)</code>；</li><li>随机分类器的结果分布在<code>ROC space</code>对角线$(0, 0)-(1, 1)$上，当实验次数足够多，其分区趋向$(0.5, 0.5)$;</li><li>对角线以上的点代表好的分类结果(比随机的好)；线下的点代表坏的结果(比随机的差)；</li><li>注意，持续不良分类器的输出可以简单地反转以获得一个好的分类器，反转后的分类器与原分类器在平面上关于对角线对称，例如点$C’$。</li></ul><h3 id="ROC曲线的绘制"><a href="#ROC曲线的绘制" class="headerlink" title="ROC曲线的绘制"></a>ROC曲线的绘制</h3><p>若训练集样本中，正样本与负样本以正态分布的形式分布在样本平面上，如下图，左峰为负样本，右峰为正样本，存在部分重叠(不然就不用搞这么多分类算法了)。</p><p><img src="/2018/11/21/Metrics/ROC_curves.svg.png" alt="ROC_curves.svg"></p><p>若假设正样本概率密度为$f_1(x)$，负样本的概率密度为$f_0(x)$，给定阈值$T$，则右</p><script type="math/tex; mode=display">TPR(T) = \int_T^{\infty} f_1(x) dx</script><script type="math/tex; mode=display">FPR(T) = \int_T^{\infty} f_0(x) dx</script><p>选取不同的阈值划分分类器输出，就能得到<code>ROC</code>曲线。</p><p>在基于有限样本作<code>ROC</code>图时，可以看到曲线每次都是一个“爬坡”，遇到正例往上爬一格$(1/m+)$，错了往右爬一格$(1/m-)$，显然往上爬对于算法性能来说是最好的。<br><img src="/2018/11/21/Metrics/ROC_curves_up_right.png" alt="ROC_curves_up_right"></p><h3 id="Area-Under-the-Curve-AUC"><a href="#Area-Under-the-Curve-AUC" class="headerlink" title="Area Under the Curve - AUC"></a>Area Under the Curve - AUC</h3><p><code>ROC</code>曲线下的面积<code>AUC</code>物理意义为，任取一对正负样本，正样本的预测值大于负样本的预测值的概率。</p><script type="math/tex; mode=display">A = \int_{-\infty}^{\infty} TPR(T) dFPR(T)</script><script type="math/tex; mode=display">= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}I(T'> T)f_1(T') f_0(T)dT' dT</script><script type="math/tex; mode=display">= P(X_1 > X_0)</script><p>同样的，在有限个样本下，其面积用累加的方法计算(梯形面积)</p><p><img src="/2018/11/21/Metrics/ROC_curves_AUC.png" alt="ROC_curves_AUC"></p><script type="math/tex; mode=display">AUC = \sum_{i=1}^{m-1} \frac{1}{2} (y_{i+1} + y_i)(x_{i+1} - x_i)</script><ul><li>$AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>$0.5 &lt; AUC &lt; 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li><li>$AUC = 0.5$，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li><li>$AUC &lt; 0.5$，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li></ul><h3 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h3><p>以下为<code>scikit-learn</code>中混淆矩阵的<code>ROC</code>曲线<code>API</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y = np.array([1, 1, 2, 2])</span><br><span class="line">&gt;&gt;&gt; scores = np.array([0.1, 0.4, 0.35, 0.8])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)</span><br><span class="line">&gt;&gt;&gt; fpr</span><br><span class="line">array([ 0. ,  0.5,  0.5,  1. ])</span><br><span class="line">&gt;&gt;&gt; tpr</span><br><span class="line">array([ 0.5,  0.5,  1. ,  1. ])</span><br><span class="line">&gt;&gt;&gt; thresholds</span><br><span class="line">array([ 0.8 ,  0.4 ,  0.35,  0.1 ])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; metrics.auc(fpr, tpr)</span><br><span class="line">0.75</span><br></pre></td></tr></table></figure></p><h1 id="聚类-clustering-评估指标"><a href="#聚类-clustering-评估指标" class="headerlink" title="聚类(clustering)评估指标"></a>聚类(clustering)评估指标</h1><blockquote><ul><li><a href="https://blog.csdn.net/darkrabbit/article/details/80378597" target="_blank" rel="noopener">AI（005） - 笔记 - 聚类性能评估（Clustering Evaluation） - DarkRabbit的专栏 - CSDN博客 </a></li><li><a href="https://en.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener">Wikipedia, the free encyclopedia</a></li></ul></blockquote><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>聚类性能比较好，就是聚类结果簇内相似度<code>(intra-cluster similarity)</code>高，而簇间相似度<code>(inter-cluster similarity)</code>低，即同一簇的样本尽可能的相似，不同簇的样本尽可能不同。</p><p>聚类性能的评估（度量）分为两大类：</p><ul><li>外部评估<code>(external evaluation)</code>：将结果与某个参考模型<code>(reference model)</code>进行比较；</li><li>内部评估<code>(internal evaluation)</code>：直接考虑聚类结果而不利用任何参考模型。</li></ul><p>将$n_{samples}$个样本$\{x^{(1)}, …, x^{(n_{samples})}\}$用待评估聚类算法划分为$K$个类$\{X_1, …, X_K\}$，假定参考模型将其划分为$L$类$\{Y_1, …, Y_L\}$，将样本两辆匹配</p><script type="math/tex; mode=display">\begin{cases}    a = |SS| &  SS = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)}, x^{(j)} \in Y_l\} \\    b = |SD| &  SD = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \\    c = |DS| &  DS = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)}, x^{(j)} \in Y_l\} \\    d = |DD| &  DD = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\}\end{cases}</script><p>其中$k = 1, …, K; l = 1, …, L$</p><script type="math/tex; mode=display">a + b + c + d=   \left(        \begin{matrix}            n \\ 2        \end{matrix}    \right)= \frac{n(n-1)}{2}</script><blockquote><ul><li>$SS$包含两种划分中均属于同一类的样本对；</li><li>$SD$包含用待评估聚类算法划分中属于同一类，而在参考模型中属于不同类的样本对；</li><li>$DS$包含用待评估聚类算法划分中属于不同类，而在参考模型中属于同一类的样本对；</li><li>$DD$包含两种划分中均不属于同一类的样本对。</li></ul></blockquote><h2 id="常用外部评估-external-evaluation"><a href="#常用外部评估-external-evaluation" class="headerlink" title="常用外部评估(external evaluation)"></a>常用外部评估(external evaluation)</h2><h3 id="Rand-Index-RI"><a href="#Rand-Index-RI" class="headerlink" title="Rand Index(RI)"></a>Rand Index(RI)</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/Rand_index" target="_blank" rel="noopener">Rand index - Wikipedia</a></p></blockquote><script type="math/tex; mode=display">RI = \frac{a+d}{a + b + c + d} = \frac{a+d}{\left(\begin{matrix} n \\ 2 \end{matrix}\right)}</script><p>显然，结果值在$[0,1]$之间，且值越大越好。</p><ul><li>当为$0$时，两个聚类无重叠；</li><li>当为$1$时，两个聚类完全重叠。</li></ul><h3 id="Adjust-Rand-Index-ARI"><a href="#Adjust-Rand-Index-ARI" class="headerlink" title="Adjust Rand Index(ARI)"></a>Adjust Rand Index(ARI)</h3><p>让$RI$有了修正机会<code>(corrected-for-chance)</code>，在取值上从$[0,1]$变成$[-1, 1]$</p><p>对于$X$与$Y$的重叠可以用一个列联表<code>(contingency table)</code>表示，记作$[n_{ij}]$，$n_{ij} = |X_i \bigcap Y_j|$<br><img src="/2018/11/21/Metrics/聚类/ARI.svg" alt="ARI"></p><p>则定义$ARI$如下<br><img src="/2018/11/21/Metrics/聚类/ARI_Def.svg" alt="ARI_Def"></p><h3 id="互信息与调整互信息-Adjusted-Mutual-Information-AMI"><a href="#互信息与调整互信息-Adjusted-Mutual-Information-AMI" class="headerlink" title="互信息与调整互信息(Adjusted Mutual Information - AMI)"></a>互信息与调整互信息(Adjusted Mutual Information - AMI)</h3><blockquote><p>关于互信息可查看<a href="">熵</a>一节说明。</p></blockquote><p>$X_i$类别的概率定义为</p><script type="math/tex; mode=display">P(k) = \frac{|X_k|}{N}</script><p>则划分结果的熵定义为</p><script type="math/tex; mode=display">H(X) = - \sum_k P(k) \log P(k)</script><p>类似的</p><script type="math/tex; mode=display">P'(l) = \frac{|Y_l|}{N}</script><script type="math/tex; mode=display">H(Y) = - \sum_j P'(l) \log P'(l)</script><p>另外</p><script type="math/tex; mode=display">P(k, l) = \frac{|X_k, Y_l|}{N}</script><p>那么两种划分的互信息定义为</p><script type="math/tex; mode=display">MI(X, Y) = \sum_{k, l} P(k, l) \log \frac{P(k, l)}{P(k) P'(l)}</script><p>和$ARI$一样，我们对它进行调整。</p><script type="math/tex; mode=display">E[MI(X, Y)] = \sum_k \sum_l \sum_{n_{kl} = \max\{1, a_k + b_l - N\}}^{\min \{a_k, b_l\}}\frac{n_{kl}}{N}\log \left( \frac{N·n_{kl}}{a_k b_l} \right) ×</script><script type="math/tex; mode=display">\frac{a_k!b_l!(N-a_k)!(N-b_l)!}{N!n_{kl}!(a_k-n_{kl})!(b_l-n_{kl})!(N-a_k-b_l+n_{kl})!}</script><p>最终$AMI$表达式为</p><script type="math/tex; mode=display">AMI(X, Y) = \frac{MI(X, Y) - E[MI(X, Y)]}{\max \{H(X), H(Y)\} - E[MI(X, Y)]}</script><h3 id="同质性-Homogeneity-与完整性-Completeness"><a href="#同质性-Homogeneity-与完整性-Completeness" class="headerlink" title="同质性(Homogeneity)与完整性(Completeness)"></a>同质性(Homogeneity)与完整性(Completeness)</h3><p>这两个类似分类种的的准确率<code>(accuracy)</code>与召回率<code>(recall)</code>。</p><ul><li><p>同质性<code>(Homogeneity)</code><br>  即一个簇仅包含一个类别的样本</p><script type="math/tex; mode=display">  H = 1 - \frac{H(X|Y)}{H(X)}</script><p>  其中$H(X|Y)$为条件熵</p><script type="math/tex; mode=display">  H(X|Y) = \sum_k \sum_l P(X_k, Y_l) \log \frac{P(Y_l)}{P(X_k, Y_l)}  = \sum_k \sum_l \frac{n_{kl}}{N} \log \frac{n_{kl}}{N}</script></li><li><p>完整性<code>(Completeness)</code><br>  同类别样本被归类到相同簇中</p><script type="math/tex; mode=display">  C = 1 - \frac{H(Y|X)}{H(Y)}</script></li><li><p>$V-measure$<br>  <code>Homogeneity</code>和<code>Completeness</code>的调和平均</p><script type="math/tex; mode=display">  V = \frac{1}{\frac{1}{2} \left(\frac{1}{H} + \frac{1}{C}\right)} = \frac{2HC}{H + C}</script></li></ul><h3 id="Fowlkes-Mallows-index-FMI"><a href="#Fowlkes-Mallows-index-FMI" class="headerlink" title="Fowlkes-Mallows index(FMI)"></a>Fowlkes-Mallows index(FMI)</h3><p>成对精度和召回率的几何均值</p><blockquote><p><a href="https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index" target="_blank" rel="noopener">Fowlkes–Mallows index - Wikipedia</a></p></blockquote><p>定义</p><ul><li>$TP$ as the number of points that are present in the same cluster in both $A_1$ and $A_2$.</li><li>$FP$ as the number of points that are present in the same cluster in $A_1$ but not in $A_2$.</li><li>$FN$ as the number of points that are present in the same cluster in $A_2$ but not in $A_1$.</li><li>$TN$ as the number of points that are in different clusters in both $A_1$ and $A_2$.</li></ul><p>则</p><script type="math/tex; mode=display">TP + FP + TN + FN = \frac{n(n-1)}{2}</script><p>定义</p><script type="math/tex; mode=display">FMI = \sqrt{\frac{TP}{TP + FP} · \frac{TP}{TP + FN}}</script><h3 id="杰卡德系数-Jaccard-Coefficient-JC"><a href="#杰卡德系数-Jaccard-Coefficient-JC" class="headerlink" title="杰卡德系数(Jaccard Coefficient - JC)"></a>杰卡德系数(Jaccard Coefficient - JC)</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="noopener">Jaccard index - Wikipedia</a></p></blockquote><p>给定两个具有$n$个元素的集合$A, B$，定义</p><ul><li>$M_{11}$ represents the total number of attributes where $A$ and $B$ both have a value of $1$.</li><li>$M_{01}$ represents the total number of attributes where the attribute of $A$ is $0$ and the attribute of $B$ is $1$.</li><li>$M_{10}$ represents the total number of attributes where the attribute of $A$ is $1$ and the attribute of $B$ is $0$.</li><li>$M_{00}$ represents the total number of attributes where $A$ and $B$ both have a value of $0$.</li></ul><p>则有</p><script type="math/tex; mode=display">M_{11} + M_{01} + M_{10} + M_{00} = n</script><ul><li><p><code>Jaccard</code>相似度系数</p><script type="math/tex; mode=display">  J = \frac{M_{11}}{M_{01} + M_{10} + M_{11}}</script><blockquote><p>也即$J=\frac{A \cap B}{A \cup B}$</p></blockquote></li><li><p><code>Jaccard</code>距离</p><script type="math/tex; mode=display">  D_J = 1 - J</script></li></ul><h2 id="常用内部评估-internal-evaluation"><a href="#常用内部评估-internal-evaluation" class="headerlink" title="常用内部评估(internal evaluation)"></a>常用内部评估(internal evaluation)</h2><h3 id="轮廓系数-Silhouette-coefficient"><a href="#轮廓系数-Silhouette-coefficient" class="headerlink" title="轮廓系数(Silhouette coefficient)"></a>轮廓系数(Silhouette coefficient)</h3><p>又称侧影法，适用于实际类别信息未知的情况，对其中一个样本点$x^{(i)}$，记</p><ul><li>$a(i)$：到本簇其他样本点的距离的平均值</li><li>$b(i)$：该点到其他各个簇的样本点的平均距离的最小值</li></ul><p>定义轮廓系数</p><script type="math/tex; mode=display">S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}</script><p>或者</p><script type="math/tex; mode=display">S(i) = \begin{cases}    1 - \frac{a(i)}{b(i)} & a(i) < b(i) \\    0 & a(i) = b(i) \\    \frac{b(i)}{a(i)} - 1 & a(i) > b(i)\end{cases}</script><p>其含义如下</p><ul><li>当$a(i) \ll b(i)$时，无限接近于$1$，则意味着聚类合适；</li><li>当$a(i) \gg b(i)$时，无限接近于$-1$，则意味着把样本i聚类到相邻簇中更合适；</li><li>当$a(i)\approxeq b(i)$时，无限接近于$0$，则意味着样本在两个簇交集处。</li></ul><p>一般再对各个点的轮廓系数求均值</p><script type="math/tex; mode=display">\overline{S} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} S(i)</script><ul><li>当$\overline{S} &gt; 0.5$，表示聚类合适；</li><li>当$\overline{S} &lt; 0.2$，表示表明数据不存在聚类特征</li></ul><h3 id="Calinski-Harabaz-CH"><a href="#Calinski-Harabaz-CH" class="headerlink" title="Calinski-Harabaz(CH)"></a>Calinski-Harabaz(CH)</h3><p>也适用于实际类别信息未知的情况，以$K$分类为例</p><ul><li><p>类内散度$W$</p><script type="math/tex; mode=display">  W(K) = \sum_k \sum_{C(j)=k} ||x_j - \overline{x_k}||^2</script></li><li><p>类间散度$B$</p><script type="math/tex; mode=display">  B(K) = \sum_k a_k ||\overline{x_k} - \overline{x}||^2</script></li><li><p>$CH$</p><script type="math/tex; mode=display">  CH(K) = \frac{B(K)(N-K)}{W(K)(K-1)}</script></li></ul><h3 id="Davies-Bouldin-Index-DBI"><a href="#Davies-Bouldin-Index-DBI" class="headerlink" title="Davies-Bouldin Index(DBI)"></a>Davies-Bouldin Index(DBI)</h3><p>定义</p><ul><li>$c_k$：簇$C_k$的中心点</li><li>$\sigma_k$：簇$C_k$中所有元素到$c_k$的距离的均值</li><li>$d(c_i, c_j)$：簇中心$c_i$与$c_j$之间的距离</li></ul><p>则</p><script type="math/tex; mode=display">DBI = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)</script><p>$DBI$越小越好</p><h3 id="Dunn-index-DI"><a href="#Dunn-index-DI" class="headerlink" title="Dunn index(DI)"></a>Dunn index(DI)</h3><p>定义</p><ul><li>$d(i,j)$：两类簇的距离，定义方法多样，例如两类簇中心的距离；</li><li>$d’(k)$：簇$C_k$的类内距离，同样的，可定义多种，例如簇$C_k$中任意两点距离的最大值。</li></ul><p>则</p><script type="math/tex; mode=display">DI = \frac{\min_{1 \leq i < j \leq K} d(i, j)}{\max_{1 \leq k \leq K} d'(k)}</script><h1 id="sklearn中的评价指标"><a href="#sklearn中的评价指标" class="headerlink" title="sklearn中的评价指标"></a>sklearn中的评价指标</h1><blockquote><p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/model_evaluation.html" target="_blank" rel="noopener">3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.19.0 documentation - ApacheCN</a></p></blockquote><p><img src="/2018/11/21/Metrics/sklearn_metrics.png" alt="sklearn_metrics"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; dir(metrics)</span><br><span class="line">[&apos;SCORERS&apos;, &apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, </span><br><span class="line">&apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;,</span><br><span class="line"> &apos;accuracy_score&apos;, &apos;adjusted_mutual_info_score&apos;, &apos;adjusted_rand_score&apos;, </span><br><span class="line"> &apos;auc&apos;, &apos;average_precision_score&apos;, &apos;balanced_accuracy_score&apos;, </span><br><span class="line"> &apos;base&apos;, &apos;brier_score_loss&apos;, &apos;calinski_harabaz_score&apos;, &apos;check_scoring&apos;, </span><br><span class="line"> &apos;classification&apos;, &apos;classification_report&apos;, &apos;cluster&apos;, &apos;cohen_kappa_score&apos;, </span><br><span class="line"> &apos;completeness_score&apos;, &apos;confusion_matrix&apos;, &apos;consensus_score&apos;, </span><br><span class="line"> &apos;coverage_error&apos;, &apos;davies_bouldin_score&apos;, &apos;euclidean_distances&apos;, </span><br><span class="line"> &apos;explained_variance_score&apos;, &apos;f1_score&apos;, &apos;fbeta_score&apos;, </span><br><span class="line"> &apos;fowlkes_mallows_score&apos;, &apos;get_scorer&apos;, &apos;hamming_loss&apos;, &apos;hinge_loss&apos;, </span><br><span class="line"> &apos;homogeneity_completeness_v_measure&apos;, &apos;homogeneity_score&apos;, </span><br><span class="line"> &apos;jaccard_similarity_score&apos;, &apos;label_ranking_average_precision_score&apos;, </span><br><span class="line"> &apos;label_ranking_loss&apos;, &apos;log_loss&apos;, &apos;make_scorer&apos;, &apos;matthews_corrcoef&apos;, </span><br><span class="line"> &apos;mean_absolute_error&apos;, &apos;mean_squared_error&apos;, &apos;mean_squared_log_error&apos;, </span><br><span class="line"> &apos;median_absolute_error&apos;, &apos;mutual_info_score&apos;, </span><br><span class="line"> &apos;normalized_mutual_info_score&apos;, &apos;pairwise&apos;, &apos;pairwise_distances&apos;, </span><br><span class="line"> &apos;pairwise_distances_argmin&apos;, &apos;pairwise_distances_argmin_min&apos;, </span><br><span class="line"> &apos;pairwise_distances_chunked&apos;, &apos;pairwise_fast&apos;, &apos;pairwise_kernels&apos;, </span><br><span class="line"> &apos;precision_recall_curve&apos;, &apos;precision_recall_fscore_support&apos;, </span><br><span class="line"> &apos;precision_score&apos;, &apos;r2_score&apos;, &apos;ranking&apos;, &apos;recall_score&apos;, &apos;regression&apos;, </span><br><span class="line"> &apos;roc_auc_score&apos;, &apos;roc_curve&apos;, &apos;scorer&apos;, &apos;silhouette_samples&apos;, </span><br><span class="line"> &apos;silhouette_score&apos;, </span><br><span class="line"> &apos;v_measure_score&apos;, </span><br><span class="line"> &apos;zero_one_loss&apos;]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Entropy</title>
      <link href="/2018/11/21/Entropy/"/>
      <url>/2018/11/21/Entropy/</url>
      
        <content type="html"><![CDATA[<h1 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h1><p>概率$p$是对确定性的度量，那么信息量就是对不确定性的度量，公式定义为</p><script type="math/tex; mode=display">I(x) = - \log p(x) \tag{1}</script><p>信息量也被称为随机变量$x$的自信息<code>(self-information)</code></p><blockquote><p>底数为$2$时，单位为<code>bit</code>，底数为$e$时，单位为<code>nat</code></p></blockquote><p><img src="/2018/11/21/Entropy/信息量.png" alt="信息量"></p><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>信息熵<code>(information entropy)</code>定义为</p><script type="math/tex; mode=display">H(X) = - \sum_{x} p(x) \log p(x) \tag{2}</script><p>可看作<strong>信息量的期望</strong>,在$0-1$分布的信息熵为</p><script type="math/tex; mode=display">H(p) = - p \log p - (1 - p) \log (1 - p)</script><p>图像如下，可见在$p=0.5$时，熵最大。<br><img src="/2018/11/21/Entropy/entropy_of_01.png" alt="entropy_of_01"></p><blockquote><p>函数$y=x \log x$的图像<br><img src="/2018/11/21/Entropy/xlogx.png" alt="xlogx"><br>有</p><script type="math/tex; mode=display">\lim_{x \rightarrow 0} y = \lim_{x \rightarrow 1} y = 0</script></blockquote><h1 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h1><p>根据信息熵的定义，推广到多维随机变量，就得到联合熵的定义式，以$2$维随机变量为例</p><script type="math/tex; mode=display">H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y) \tag{3}</script><p>可推广至多维。</p><!-- 机器学习笔记十：各种熵总结 - 谢小小XH - CSDN博客 https://blog.csdn.net/xierhacker/article/details/53463567 --><h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>现在有关于样本集的两个概率分布$p(x)$和$q(x)$，其中$p(x)$为真实分布，$q(x)$非真实分布。</p><p>如果用真实分布$p(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p) = - \sum_x p(x) \log p(x)</script><p>如果用非真实分布$q(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p, q) = - \sum_x p(x) \log q(x) \tag{4}</script><p>注意</p><script type="math/tex; mode=display">H(p, q) - H(p)= \sum_x p(x) \log \frac{p(x)}{q(x)}= D_{KL}(p||q)</script><p>当用非真实分布$q(x)$得到的平均码长比真实分布$p(x)$得到的平均码长多出的比特数就是相对熵。我们希望通过最小化相对熵$D_{KL}(p||q)$使$q(x)$尽量趋近$p(x)$，即</p><script type="math/tex; mode=display">q(x) = \arg \min_{q(x)} D_{KL} (p||q)</script><p>而$H(p)$是样本集的熵，为固定的值，故</p><script type="math/tex; mode=display">q(x) = \arg \min_{q(x)} H(p, q)</script><p>即等价于最小化交叉熵。</p><h1 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h1><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。定义为在给定$X$下$Y$的条件概率分布的熵对$X$的期望，即</p><script type="math/tex; mode=display">H(Y|X) = E_{p(x)} H(Y|X=x)= \sum_x p(x) H(Y|X=x) \tag{5}</script><p>其中</p><script type="math/tex; mode=display">H(Y|X=x) = - \sum_y p(y|x) \log p(y|x)</script><p>故</p><script type="math/tex; mode=display">H(Y|X) = \sum_x p(x) \left[- \sum_y p(y|x) \log p(y|x)\right]</script><script type="math/tex; mode=display">= - \sum_x \sum_y p(x, y) \log p(y|x)</script><p>即</p><script type="math/tex; mode=display">H(Y|X) = - \sum_{x, y} p(x, y) \log p(y|x) \tag{6}</script><p>实际上，条件熵满足</p><script type="math/tex; mode=display">H(Y|X) = H(X, Y) - H(X) \tag{7}</script><blockquote><p>证明：<br>已知</p><script type="math/tex; mode=display">H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y)</script><script type="math/tex; mode=display">H(X) = - \sum_{x} p(x) \log p(x)</script><p>则</p><script type="math/tex; mode=display">H(X, Y) - H(X)</script><script type="math/tex; mode=display">= - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x} p(x) \log p(x)</script><script type="math/tex; mode=display">= - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x, y} p(x, y) \log p(x)</script><script type="math/tex; mode=display">= \sum_{x, y} p(x, y) \log \frac{p(x)}{p(x, y)}</script><script type="math/tex; mode=display">= \sum_{x, y} p(x, y) \log p(y|x)</script><script type="math/tex; mode=display">= H(Y|X)</script></blockquote><h1 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h1><p>相对熵<code>(relative entropy)</code>，又称<code>KL</code>散度<code>(Kullback–Leibler divergence)</code>。可以用来衡量两个概率分布之间的差异，就是求$p(x)$与$q(x)$之间的对数差在 pp 上的期望值。</p><script type="math/tex; mode=display">D_{KL} (p||q) = E_{p(x)} \log \frac{p(x)}{q(x)}= \sum_x p(x) \log \frac{p(x)}{q(x)} \tag{8}</script><p>注意</p><ul><li><p>相对熵不具有对称性，即</p><script type="math/tex; mode=display">  D_{KL} (p||q) \neq D_{KL} (q||p)</script></li><li><p>$D_{KL} (p||q) \geq 0$</p><blockquote><p>证明：</p><script type="math/tex; mode=display">D_{KL} (p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = - \sum_x p(x) \log \frac{q(x)}{p(x)}</script><p>由<code>Jensen inequality</code></p><script type="math/tex; mode=display">\sum_x p(x) \log \frac{q(x)}{p(x)}\leq \log \sum_x p(x) \frac{q(x)}{p(x)}= \log \sum_x q(x)</script><p>所以</p><script type="math/tex; mode=display">D_{KL} (p||q) \geq - \log \sum_x q(x)</script><p>而$0 \leq q(x) \leq 1$，故</p><script type="math/tex; mode=display">D_{KL} (p||q) \geq 0</script></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Non-parameter Estimation</title>
      <link href="/2018/11/19/Non-parameter-Estimation/"/>
      <url>/2018/11/19/Non-parameter-Estimation/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>若参数估计时我们不知道样本的分布形式，那么就无法确定需要估计的概率密度函数，无法用<a href="https://louishsu.xyz/2018/10/22/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">最大似然估计、贝叶斯估计等参数估计方法</a>，应该用非参数估计方法。</p><p>需要知道的是，作为非参数方法的共同问题是对样本数量需求较大，只要样本数目足够大众可以保证收敛于任何复杂的位置密度，但是计算量和存储量都比较大。当样本数很少时，如果能够对密度函数有先验认识，则参数估计能取得更好的估计效果。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>若有$M$个样本$x^{(1)}, …, x^{(M)}$，依概率密度函数$p(x)$独立同分布抽样得到。</p><p>一个样本$x$落在区域$R$中的概率$P$可表示为</p><script type="math/tex; mode=display">P = \int_R p(x) dx \tag{1}</script><p>我们通过计算$P$来估计概率密度$p(x)$。</p><p>$K$个样本落入区域$R$的概率$P_K$为二项分布，即$K \sim B(M, P)$</p><script type="math/tex; mode=display">P_K = \left(\begin{matrix} M\\K \end{matrix}\right) P^K (1-P)^{M-K} \tag{2}</script><p>则$K$的期望与方差分别为</p><script type="math/tex; mode=display">E(K) = MP;　D(K) = MP(1-P)</script><p>样本个数$M$越多，$D(K)$越大，即$K$在期望附近的波峰越明显，因此样本足够多时，用$K/M$作为$P$的一个估计非常准确，即</p><script type="math/tex; mode=display">P \approx \frac{K}{M} \tag{3}</script><p>若我们假设$p(x)$是连续的，且区域$R$足够小，记其体积为$V$，那么有</p><script type="math/tex; mode=display">P = \int_R p(x)dx \approx p(x) V \tag{4}</script><p>所以根据$(3)(4)$，得到</p><script type="math/tex; mode=display">p(x) \approx \frac{K/M}{V} \tag{*}</script><p>但是我们获得的其实为平滑后的概率密度函数</p><script type="math/tex; mode=display">\frac{P}{V} = \frac{\int_R p(x)dx}{\int_R dx}</script><p>我们希望其尽可能地趋近$p(x)$，那么必须要求$V \rightarrow 0$，但是这样就可能不包含任何样本，那么$p(x)\approx 0$，这样估计的结果毫无意义。</p><p>所以在实际中，一般构造多个包含样本$x$的区域$R_1, …, R_i, …, R_n$，第$i$个区域使用$i$个样本，记$V_i$为$R_i$的体积，$M_i$为落在$R_i$中的样本个数，则对$p(x)$第$i$次估计$p_i(x)$表示为</p><script type="math/tex; mode=display">p_i(x) \approx \frac{M_i / M}{V_i} \tag{5}</script><p>若要求$p_i(x)$收敛到$p(x)$，则必须满足</p><ul><li>$\lim_{i\rightarrow \infty} V_i = 0$</li><li>$\lim_{i\rightarrow \infty} M_i = 0$</li><li>$\lim_{i\rightarrow \infty} \frac{M_i}{M} = 0$</li></ul><h1 id="直方图法"><a href="#直方图法" class="headerlink" title="直方图法"></a>直方图法</h1><p>记不记得小学时的直方图统计，直方图方法的思想就是这样，以$1$维样本为例，我们将$x$的取值范围平均等分为$K$个区间，统计每个区间内样本的个数，由此计算区间的概率密度。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>若共有$N$维样本$M$组，在每个维度上$K$等分，就有$K^N$个小空间，每个小空间的体积$V_i$可以定义为</p><script type="math/tex; mode=display">V_i = \prod_{n=1}^N d_n,　i=1,...,K^N</script><p>其中</p><script type="math/tex; mode=display">d_n = \frac{\max x_n - \min x_n}{K}</script><p>假设样本落到各个小空间的概率相同，若第$i$个小空间包含$M_i$个样本，则该空间的概率密度$\hat{p_i}$为</p><script type="math/tex; mode=display">\hat{p_i} = \frac{M_i / M}{V_i} \tag{6}</script><p>估计的效果与小区间的大小密切相连，如果区域选择过大，会导致最终估计出来的概率密度函数非常粗糙；如果区域的选择过小，可能会导致有些区域内根本没有样本或者样本非常少，这样会导致估计出来的概率密度函数很不连续。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p><p>我们可以用<code>matplotlib.pyplot.hist()</code>或<code>numpy.histogram()</code>实现</p><ul><li><p><code>matplotlib</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n, bins, patches = plt.hist(arr, bins=10, normed=0, facecolor=&apos;black&apos;, edgecolor=&apos;black&apos;,alpha=1，histtype=&apos;bar&apos;)</span><br></pre></td></tr></table></figure><ul><li><code>Args</code><br>  参数很多，选几个常用的讲解<ul><li>arr: 需要计算直方图的一维数组</li><li>bins: 直方图的柱数，可选项，默认为10</li><li>normed: 是否将得到的直方图向量归一化。默认为0</li><li>facecolor: 直方图颜色</li><li>edgecolor: 直方图边框颜色</li><li>alpha: 透明度</li><li>histtype: 直方图类型，‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’</li></ul></li><li><code>Returns</code><ul><li>n: 直方图向量，是否归一化由参数normed设定</li><li>bins: 返回各个bin的区间范围</li><li>patches: 返回每个bin里面包含的数据，是一个list</li></ul></li></ul></li><li><p><code>numpy</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hist, bin_edges = histogram(a, bins=10, range=None, normed=None, weights=None, density=None)</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def histEstimate(X, n_bins, showfig=False):</span><br><span class="line">    &quot;&quot;&quot; 直方图密度估计</span><br><span class="line">    Args:</span><br><span class="line">        n_bins: &#123;int&#125; 直方图的条数</span><br><span class="line">    Returns:</span><br><span class="line">        hist: &#123;ndarray(n_bins,)&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    n, bins, patches = plt.hist(X, bins=n_bins, normed=1, facecolor=&apos;lightblue&apos;, edgecolor=&apos;white&apos;)</span><br><span class="line">    if showfig: plt.show()</span><br><span class="line">    return n, bins, patches</span><br></pre></td></tr></table></figure><p><code>matplotlib</code>直方图显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_matplotlib.png" alt="hist_matplotlib"></p><p>拟合各中心点显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_ploy.png" alt="hist_ploy"></p><h1 id="K-n-近邻估计法"><a href="#K-n-近邻估计法" class="headerlink" title="$K_n$近邻估计法"></a>$K_n$近邻估计法</h1><p>随着样本数的增加，区域的体积应该尽可能小，同时又必须保证区域内有充分多的样本，但是每个区域的样本数有必须是总样本数的很小的一部分，而不是与直方图估计那样体积不变。</p><p>那么我们想，能否根据样本的分布调整分区大小呢，$K$近邻估计法就是一种采用可变大小区间的密度估计方法。</p><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>根据总样本确定参数$K_n$，在求样本$x$处的密度估计$\hat{p}(x)$时，调整区域体积$V(x)$，直到区域内恰好落入$K_n$个样本，估计公式为</p><script type="math/tex; mode=display">\hat{p}(x) = \frac{K_n/M}{V(x)} \tag{7}</script><p>一般指定超参数$a$，取</p><script type="math/tex; mode=display">K_n = a × \sqrt{M} \tag{8}</script><blockquote><script type="math/tex; mode=display">\hat{p}(x) = \frac{a × \sqrt{M} /M}{V(x)} = \frac{K_n'/M}{V'(x)}</script><p>其中$K_n’ = a,V’(x) = V(x)×\frac{1}{\sqrt{M}}$</p></blockquote><p>在样本密度比较高的区域的体积就会比较小，而在密度低的区域的体积则会自动增大，这样就能够较好的兼顾在高密度区域估计的分辨率和在低密度区域估计的连续性。</p><h1 id="Parzen窗法"><a href="#Parzen窗法" class="headerlink" title="Parzen窗法"></a>Parzen窗法</h1><p>又称核密度估计。</p><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>我们暂时假设待估计点$x$的附近区间$R$为一个$N$维的<strong>超立方体</strong>，用$h$表示边的长度，那么</p><script type="math/tex; mode=display">V_i = h^N</script><p>即<br><img src="/2018/11/19/Non-parameter-Estimation/Parzen_window.jpg" alt="Parzen_window"><br>定义窗函数$\varphi(·)$，表示落入以$x$为中心的超立方体的区域的点</p><script type="math/tex; mode=display">\varphi \left(\frac{x_i-x}{h}\right) = \begin{cases}    1 & \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2},　n=1,...,N \\    0 & otherwise\end{cases} \tag{9}</script><blockquote><script type="math/tex; mode=display">\frac{|x_{in}-x_n|}{h} \leq \frac{1}{2}　即　(x_i-x)_n \leq \frac{h}{2}</script><p>这里的$h$起到单位化的作用，便于推广</p></blockquote><p>那么落入以$x$为中心的<strong>超立方体</strong>的区域的点的个数为</p><script type="math/tex; mode=display">M_i = \sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right) \tag{10}</script><p>代入$p(x) \approx \frac{M_i/M}{V_i}$，我们得到</p><script type="math/tex; mode=display">p(x) \approx \frac{\sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right)/M}{V_i}= \frac{1}{M} \sum_{i=1}^M \frac{1}{V_i} \varphi \left(\frac{x_i-x}{h}\right) \tag{11}</script><p>我们定义核函数(或称“窗函数”)</p><script type="math/tex; mode=display">\kappa(z) = \frac{1}{V_i} \varphi(z) \tag{12}</script><p>核函数反应了一个观测样本$x_i$对在$x$处的概率密度估计的贡献，与样本$x_i$和$x$的距离有关。而概率密度估计就是在这一点上把所有观测样本的贡献进行平均</p><script type="math/tex; mode=display">p(x) \approx \frac{1}{M} \sum_{i=1}^M \kappa\left(\frac{x_i-x}{h}\right) \tag{13}</script><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>核函数应满足概率密度的要求，即</p><script type="math/tex; mode=display">\kappa(z) \geq 0　\And　\int \kappa(z)dz = 1</script><p>通常有以下几种核函数</p><ul><li><p>均匀核</p><script type="math/tex; mode=display">  \kappa(z)  = \begin{cases}      1 & |z_n| \leq \frac{1}{2},　n=1,...,N \\      0 & otherwise  \end{cases}</script></li><li><p>高斯核(正态核)<br>  高斯核是将窗放大到整个空间，各个观测样本$x_i$对待观测点$x$的加权和(越远权值越小)。</p><script type="math/tex; mode=display">  \kappa(z)  = \frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}}  \exp \left(-\frac{1}{2} (z - \mu)^T \Sigma^{-1} (z - \mu)\right)</script></li><li><p>超球窗</p><script type="math/tex; mode=display">  \kappa(z)  = \begin{cases}      V^{-1} & ||z|| \leq 1 \\      0 & otherwise  \end{cases}</script><blockquote><p>$z=\frac{x_i-x}{h}$，故$||z||\leq 1$即$||x_i-x||^2\leq h^2$<br>此时$h$表示超球体的半径</p></blockquote></li></ul><h2 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h2><p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" target="_blank" rel="noopener">sklearn.neighbors.KernelDensity — scikit-learn 0.19.0 documentation - ApacheCN</a><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.neighbors import KernelDensity</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kde.score_samples(X)</span><br><span class="line">array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,</span><br><span class="line">       -0.41076071])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kde.sample(10)</span><br><span class="line">array([[ 1.80042291,  1.1030739 ],</span><br><span class="line">       [ 0.87299669,  1.0762352 ],</span><br><span class="line">       [-2.40180586, -1.19554374],</span><br><span class="line">       [-1.97985919, -1.19361193],</span><br><span class="line">       [-2.95866231, -2.1972637 ],</span><br><span class="line">       [-1.12739556, -0.80851063],</span><br><span class="line">       [ 1.03756706,  1.24855099],</span><br><span class="line">       [ 1.21729703,  1.02345815],</span><br><span class="line">       [-2.11816867, -1.0486257 ],</span><br><span class="line">       [-1.04875537, -0.89928711]])</span><br></pre></td></tr></table></figure></p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p>具体代码见<br><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p><p>定义核函数如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 高斯核</span></span><br><span class="line">gaussian = <span class="keyword">lambda</span> z: np.exp(<span class="number">-0.5</span>*(np.linalg.norm(z)**<span class="number">2</span>)) / np.sqrt(<span class="number">2</span>*np.pi)</span><br><span class="line"><span class="comment"># 均匀核</span></span><br><span class="line">square = <span class="keyword">lambda</span> z: <span class="number">1</span> <span class="keyword">if</span> (np.linalg.norm(z) &lt;= <span class="number">0.5</span>) <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p><p>密度估计函数如下，需要对连续范围内的各个点，即$x \in [min(X), max(X)]$进行估计获得<code>p</code>，作图显示$x-p$即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parzenEstimate</span><span class="params">(X, kernel, h, n_num=<span class="number">50</span>)</span>:</span></span><br><span class="line">    <span class="string">""" 核参数估计</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples,)&#125;</span></span><br><span class="line"><span class="string">        kernel: &#123;function&#125; 可调用的核函数</span></span><br><span class="line"><span class="string">        h: &#123;float&#125; 核函数的参数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n_num,)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        - 一维，故`V_i = h`</span></span><br><span class="line"><span class="string">        - p(x) = \frac&#123;1&#125;&#123;M&#125; \sum_&#123;i=1&#125;^M \kappa \left( \frac&#123;x_i - x&#125;&#123;h&#125; \right)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = np.linspace(np.min(X), np.max(X), num=n_num)</span><br><span class="line">    p = np.zeros(shape=(x.shape[<span class="number">0</span>],))</span><br><span class="line">    z = <span class="keyword">lambda</span> x, x_i, h: (x - x_i) / h</span><br><span class="line">    V_i = h; n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">            p[idx] += kernel(z(x[idx], X[i], h)) / V_i</span><br><span class="line">        p[idx] /= n_samples</span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure></p><h3 id="均匀核"><a href="#均匀核" class="headerlink" title="均匀核"></a>均匀核</h3><ul><li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.5.png" alt="01_h_0.5"></p></li><li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.8.png" alt="01_h_0.8"></p></li><li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_1.0.png" alt="01_h_1.0"></p></li><li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_2.0.png" alt="01_h_2.0"></p></li></ul><h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><ul><li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.5.png" alt="gaussian_h_0.5"></p></li><li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.8.png" alt="gaussian_h_0.8"></p></li><li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_1.0.png" alt="gaussian_h_1.0"></p></li><li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_2.0.png" alt="gaussian_h_2.0"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Parameter Estimation</title>
      <link href="/2018/11/19/Parameter-Estimation/"/>
      <url>/2018/11/19/Parameter-Estimation/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/20587681/answer/17435552" target="_blank" rel="noopener">贝叶斯学派与频率学派有何不同？ - 任坤的回答 - 知乎</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>参数估计<code>(parameter estimation)</code>，统计推断的一种。根据从总体中抽取的随机样书．来估计总体分布中未知参数的过程。主要介绍最大似然估计<code>(MLE: Maximum Likelihood Estimation)</code>，最大后验概率估计<code>(MAP: Maximum A Posteriori Estimation)</code>，贝叶斯估计<code>(Bayesian Estimation)</code>。</p><blockquote><p>解释一下“似然函数”和“后验概率”，在<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>一节，给出定义如下</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><p>上式中$ k=1,…,K  $，各部分定义如下<br>$P(c_k|x)$——<code>后验概率(posteriori probability)</code><br>$P(c_k)$——<code>先验概率(priori probability)</code><br>$p(x|c_k)$——$c_k$关于$x$的<code>似然函数(likelihood)</code><br>$p(x)$——<code>证据因子(evidence)</code></p></blockquote><h1 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h1><p>以最经典的掷硬币实验为例，假设有一枚硬币，投掷一次出现正面记$”1”$，投掷$10$次的实验结果如下</p><script type="math/tex; mode=display">\{ 0， 1， 1， 1， 1， 0， 1， 1， 1，0 \}</script><p>记硬币投掷结果为随机变量$X$，且$ x \in \{0, 1\}$，硬币投掷一次服从二项分布，估计二项分布的参数$\theta$</p><h1 id="最大似然估计-MLE"><a href="#最大似然估计-MLE" class="headerlink" title="最大似然估计(MLE)"></a>最大似然估计(MLE)</h1><h2 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h2><blockquote><p><a href="https://en.wikipedia.org/wiki/Likelihood_function#Definition" target="_blank" rel="noopener">Likelihood function - Wikipedia</a></p></blockquote><ul><li><p>离散型</p><script type="math/tex; mode=display">L(x | \theta) = p_{\theta}(x)=P_{\theta}(X = x)</script></li><li><p>连续型</p><script type="math/tex; mode=display">L(x | \theta) = f_{\theta}(x)</script></li></ul><blockquote><p>很多人能讲出一大堆哲学理论来阐明这一对区别。<br>但我觉得，从工程师角度来讲，这样理解就够了:<br>频率 $vs$ 贝叶斯 = $P(X; w)$ $vs$ $P(X|w)$ 或 $P(X,w)$</p><p>作者：许铁-巡洋舰科技<br>链接：<a href="https://www.zhihu.com/question/20587681/answer/122348889" target="_blank" rel="noopener">https://www.zhihu.com/question/20587681/answer/122348889</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>有数据集$D = \{x_1, x_2, …, x_N\}$，按$c$个类别分成$\{D_1, D_2, …, D_C\}$，各个类别服从的概率分布密度函数模型已给出，估计参数$\hat{\Theta} = \{\hat{\theta}_{c_1}, \hat{\theta}_{c_2}, …, \hat{\theta}_{c_C}\} $</p><p><strong>假定</strong></p><ul><li>类别间独立，且各自服从概率分布密度函数为$p(x|c_j)$</li><li>各类别的概率密度$p(x|c_j)$以参数$\theta_{c_j}$确定，即$p(x|c_j; \theta_{c_j})$</li></ul><p>故似然函数为</p><script type="math/tex; mode=display">L(D | \Theta) = P(x_1, x_2, ..., x_N | \Theta) = \prod_{i=1}^N p(x_i | \theta_{x_i \in c_j})</script><blockquote><p>理解为，在参数$\Theta$为何值的条件下，实验结果出现数据集$D$的概率最大</p></blockquote><p>求取其极大值对应的参数即可</p><ul><li>一般取对数似然函数<script type="math/tex; mode=display">\log L(D | \Theta) = \sum_{i=1}^N \log p(x_i | \theta_{x_i \in c_j})</script></li><li>极大值即对应梯度为$\vec{0}$的位置，即<script type="math/tex; mode=display">∇_\Theta  \log L(D | \Theta) = \vec{0}\Rightarrow\hat{\Theta}</script></li></ul><blockquote><p>Some comments about ML</p><ul><li>ML estimation is usually simpler than alternative methods. </li><li>Has good convergence properties as the number of training samples increases. </li><li>If the model chosen for p(x|θ) is correct, and independence assumptions among variables are true, ML will give very good results.</li><li>If the model is wrong, ML will give poor results.<div style="text-align: right"> —— Zhao Haitao. Maximum Likelihood and Bayes Estimation </div></li></ul></blockquote><h2 id="例：正态分布的最大似然估计"><a href="#例：正态分布的最大似然估计" class="headerlink" title="例：正态分布的最大似然估计"></a>例：正态分布的最大似然估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的的最大似然估计</p><script type="math/tex; mode=display">P(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">L(D | \mu, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}=\left( \frac{1}{\sqrt{2\pi} \sigma } \right)^N \prod_{i=1}^N e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><p>取对数似然</p><script type="math/tex; mode=display">\log L(D | \mu, \sigma^2) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2</script><h3 id="1-参数-mu-的估计"><a href="#1-参数-mu-的估计" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><script type="math/tex; mode=display">\frac{∂}{∂\mu} L(D | \mu, \sigma^2) = \frac{1}{\sigma^2} (\sum_{i=1}^N x_i - N\mu)= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\mu}= \frac{1}{N}\sum_{i=1}^N x_i</script><h3 id="2-参数-sigma-2-的估计"><a href="#2-参数-sigma-2-的估计" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2} \log L(D | \mu, \sigma^2) = - \frac{N}{2\sigma^2} +\frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\sigma^2} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2</script><blockquote><p>参数$\hat{\mu}, \hat{\sigma}^2$的值与样本均值和样本方差相等</p></blockquote><h1 id="最大后验概率估计-MAP"><a href="#最大后验概率估计-MAP" class="headerlink" title="最大后验概率估计(MAP)"></a>最大后验概率估计(MAP)</h1><!-- > [高斯混合模型(GMM)与最大期望算法(EM)]() --><h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>最大似然估计是求参数$\theta$, 使似然函数$P(D | \theta)$最大，最大后验概率估计则是求$\theta$使$P(\theta | D)$最大</p><blockquote><p>理解为，在已出现的实验样本$D$上，参数$\theta$取何值的概率最大</p></blockquote><p>且注意到</p><script type="math/tex; mode=display">P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}</script><p>故$MAP$不仅仅使似然函数$P(D | \theta)$最大，而且使$P(\theta)$最大，即</p><script type="math/tex; mode=display">\theta = argmax L(\theta | D)</script><script type="math/tex; mode=display">L(\theta | D) = P(\theta) P(D | \theta)= P(\theta) \prod_{i=1}^N p(x_i | \theta)</script><blockquote><p>比$ML$多了一项$P(\theta)$</p></blockquote><ul><li><p>取对数后</p><script type="math/tex; mode=display">\log L(\theta | D) = \sum_{i=1}^N \log p(x_i | \theta) + \log P(\theta)</script></li><li><p>求取极大值</p><script type="math/tex; mode=display">∇_\theta L(\theta | D) = 0\Rightarrow\hat{\theta}</script></li></ul><blockquote><p>$MAP$和$MLE$的区别：<br>$MAP$允许我们把先验知识加入到估计模型中，这在<strong>样本很少</strong>的时候是很有用的，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如<code>beta</code>分布的$\alpha, \beta$，我们还可以调节把估计的结果“拉”向先验的幅度，$\alpha, \beta$越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。<br><a href="https://blog.csdn.net/hustlx/article/details/51144710" target="_blank" rel="noopener">极大似然估计，最大后验概率估计(MAP)，贝叶斯估计 - 李鑫o_O - CSDN博客</a></p></blockquote><h2 id="例：正态分布的最大后验概率估计"><a href="#例：正态分布的最大后验概率估计" class="headerlink" title="例：正态分布的最大后验概率估计"></a>例：正态分布的最大后验概率估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的最大后验概率估计</p><script type="math/tex; mode=display">p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><blockquote><script type="math/tex; mode=display">\log p(x_i | \mu, \sigma^2)= - \frac{1}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (x_i - \mu)^2</script></blockquote><h3 id="1-参数-mu-的估计-1"><a href="#1-参数-mu-的估计-1" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p><script type="math/tex; mode=display">p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><blockquote><script type="math/tex; mode=display">\log p(\mu)= - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script></blockquote><p>则</p><script type="math/tex; mode=display">\log L(\mu, \sigma^2 | D)= - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script><p>则</p><script type="math/tex; mode=display">\frac{∂}{∂\mu} \log L(\mu, \sigma^2 | D)= \frac{1}{\sigma^2} \sum_{i=0}^N (x_i - \mu) - \frac{1}{\sigma_{\mu_0}^2} (\mu - \mu_0)= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\mu}= \frac{\mu_0 \sigma^2 + \sigma_{\mu_0}^2 \sum_{i=0}^N x_i}{\sigma^2 + N \sigma_{\mu_0}^2}= \frac{\mu_0 + \frac{\sigma_{\mu_0}^2}{\sigma^2} \sum_{i=0}^N x_i}{1 + \frac{\sigma_{\mu_0}^2}{\sigma^2} N }</script><h3 id="2-参数-sigma-2-的估计-1"><a href="#2-参数-sigma-2-的估计-1" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><p>给定先验条件：$\sigma^2$服从正态分布$N(\sigma_0^2, \sigma_{\sigma_0^2}^2)$，即</p><script type="math/tex; mode=display">p(\sigma^2) = \frac{1}{\sqrt{2\pi} \sigma_{\sigma_0^2}} e^ {-\frac{(\sigma^2- \sigma_0^2)^2}{2 \sigma_{\sigma_0^2} ^2}}</script><blockquote><script type="math/tex; mode=display">\log p(\sigma^2) = - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script></blockquote><p>则</p><script type="math/tex; mode=display">\log L(\mu, \sigma^2 | D)= - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script><p>则</p><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2} \log L(\mu, \sigma^2 | D)= - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2\sigma_{\sigma_0}^2} \frac{\sigma - \sigma_0}{\sigma}\Rightarrow\hat{\sigma^2}(略)</script><blockquote><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2}(\sigma - \sigma_0)^2= 2(\sigma - \sigma_0)\frac{∂}{∂\sigma^2} (\sigma - \sigma_0)= \frac{\sigma - \sigma_0}{\sigma}</script></blockquote><h1 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h1><h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><!-- 贝叶斯公式：$$P(c_i|x) = \frac{p(x|c_i)p(c_i)}{\sum_j p(x|c_j)p(c_j)}$$在给定数据集$D=\{x_1, x_2, ..., x_N\}$的情况下，可从数据中估计先验概率和似然函数，即$$P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i, D)}{\sum_j p(x|c_j, D)p(c_j, D)}$$**假定**- 先验概率密度函数为$p(c_i)$已知- 抽样结果几乎与真实分布一致，即$ p(c_i, D) \approx p(c_i) $则$$P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i)}{\sum_j p(x|c_j, D)p(c_j)}$$只需从样本中，估计每个类别的似然函数$p(x|c_i, D)$即可>-----------------------------------------------现考虑**单个类别**中抽取的数据集$D$，如何估计该类别的似然函数$p(x | \theta)$参数$\theta$呢？若概率密度函数为$p(x | \theta)$，记从数据集中估计得到的似然函数为$p(x | D)$，有$$p(x | \theta) \approx p(x | D)$$> $p(x | D)$ would be the estimate of $p(x | \theta)$ given $D$且$$p(x | D) = \int p(x, \theta | D) d \theta= \int p(x | \theta, D) p(\theta | D) d \theta$$> Links $p(x | D)$ with $p(θ | D)$其中- $p(x | \theta, D) \approx p(x | \theta) $- $p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)}= \frac{p(\theta) \prod_{i=1}^N p(x_i | \theta)}{P(D)}$故$$p(x | D) = \int p(x | \theta) p(\theta | D) d \theta$$总的来说 --><script type="math/tex; mode=display">p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)}= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)</script><p>其中$a$是使</p><script type="math/tex; mode=display">\int p(\theta | D)  = 1</script><p>利用“质心公式”求解贝叶斯的点估计</p><script type="math/tex; mode=display">θ_{Bayes} = \int θ·p(θ|D) d θ</script><h2 id="例：正态分布的贝叶斯估计"><a href="#例：正态分布的贝叶斯估计" class="headerlink" title="例：正态分布的贝叶斯估计"></a>例：正态分布的贝叶斯估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的贝叶斯估计</p><script type="math/tex; mode=display">p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><h3 id="参数-mu-的估计"><a href="#参数-mu-的估计" class="headerlink" title="参数$\mu$的估计"></a>参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p><script type="math/tex; mode=display">p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><p>则</p><script type="math/tex; mode=display">P(\mu | D) = a · p(\mu) \prod_{i=1}^N p(x_i | \mu)= a · \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}\prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">= a · \left( \frac{1}{\sqrt{2\pi}} \right)^{N + 1}\frac{1}{\sigma_{\mu_0} \sigma^N}e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2} -\sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2}}</script><p>易证</p><blockquote><p></p><p align="right">我已经想到了一个绝妙的证明,但是这台电脑的硬盘太小了,写不下。</p><br><!-- > <p align="right">诶嘿，显然成立，2333333</p> --><p></p></blockquote><script type="math/tex; mode=display">p(\mu | D) = \frac{1}{\sqrt{2\pi}\sigma_N} e^ {-\frac{(\mu - \mu_N)^2}{2\sigma_N^2}}</script><p>其中</p><script type="math/tex; mode=display">\mu_N = \frac{N \sigma_0^2}{N \sigma_0^2 + \sigma^2}\frac{1}{N} \sum_{i=1}^N x_i+\frac{\sigma^2}{N \sigma_0^2 + \sigma^2}\mu_0</script><script type="math/tex; mode=display">\sigma_N^2 = \frac{\sigma_0^2 \sigma^2}{N \sigma_0^2 + \sigma^2}</script><blockquote><p><strong>与$MLE$，$MAP$的区别</strong></p><ul><li>相比较$MLE$与$MAP$的点估计，贝叶斯估计得到的结果是参数$\theta$的密度函数$p(\theta | D)$</li><li><p>最大后验概率估计为求取对应最大后验概率的点</p><script type="math/tex; mode=display">\theta = argmax_\theta p(\theta | D)</script></li><li><p>贝叶斯估计为求取整个取值范围的概率密度$p(\theta | D)$，既然如此，必有</p><script type="math/tex; mode=display">\int p(\theta | D) d\theta = 1</script></li></ul><p><a href="https://www.cnblogs.com/zjh225901/p/7495505.html" target="_blank" rel="noopener">统计学习方法学习笔记（一）—极大似然估计与贝叶斯估计原理及区别 - YJ-20 - 博客园</a></p><script type="math/tex; mode=display">p(\theta | D) = \frac{p(\theta) \prod_{i=1}^N p(x_i | \theta)}{\int_\theta p(\theta) \prod_{i=1}^N p(x_i | \theta) d\theta}</script><p>由于$\theta$是满足一定概率分布的变量，所以在计算的时候需要将考虑所有$\theta$取值的情况，在计算过程中不可避免地高复杂度。所以计算时并不把所有地后验概率$p(\theta | D)$都找出来，而是采用类似于极大似然估计地思想，来极大化后验概率，得到这种有效的叫做$MAP$</p></blockquote><h1 id="引例的求解"><a href="#引例的求解" class="headerlink" title="引例的求解"></a>引例的求解</h1><p>已知硬币投掷结果服从$Bernoulli$分布</p><table>  <tr>    <th>X</th>    <th>0</th>    <th>1</th>  </tr>  <tr>    <td>P</td>    <td>1-θ</td>    <td>θ</td>  </tr></table><p>或者</p><script type="math/tex; mode=display">P(X_i) = \theta ^{X_i} (1 - \theta) ^{1 - X_i}</script><h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><p>实验结果中正面出现$7$次，反面出现$3$次，似然函数为</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{10} \theta ^{X_i} (1 - \theta) ^{1 - X_i} = \theta ^7 (1 - \theta) ^3</script><p>取对数似然函数并求极大值</p><script type="math/tex; mode=display">\log L(\theta) = 7 \log \theta + 3 \log (1 - \theta)</script><p>令</p><script type="math/tex; mode=display">\frac{∂}{∂ \theta} \log L(\theta)= \frac{7}{\theta} - \frac{3}{1-\theta} = 0</script><p>解得</p><script type="math/tex; mode=display">\theta = 0.7</script><p>即硬币服从$B(1, 0.7)$的概率分布</p><blockquote><p>做出$L(\theta)$图像验证，如下<br><img src="/2018/11/19/Parameter-Estimation/最大似然估计.png" alt="最大似然估计"></p></blockquote><h2 id="最大后验概率估计"><a href="#最大后验概率估计" class="headerlink" title="最大后验概率估计"></a>最大后验概率估计</h2><p>给定先验条件</p><script type="math/tex; mode=display">\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><p>则最大化</p><script type="math/tex; mode=display">L(\theta | D) = \theta ^7 (1 - \theta) ^3 · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}}</script><p>取对数</p><script type="math/tex; mode=display">\log L(\theta | D)= 7 \log \theta + 3 \log (1 - \theta) - \frac{1}{2} \log(2\pi \sigma_{\theta_0}^2) - \frac{1}{2\sigma_{\theta_0}^2} (\theta - \theta_0)^2</script><p>求取极大值点</p><script type="math/tex; mode=display">\frac{∂}{∂\theta} \log L(\theta | D) = \frac {7}{\theta} - \frac{3}{1-\theta} - \frac{\theta - \theta_0}{\sigma_{\theta_0}^2} = 0</script><p>得到</p><script type="math/tex; mode=display">\theta^3 - (\theta_0 + 1) \theta^2 + (\theta_0 - 10\sigma_{\theta_0}^2) \theta + 7\sigma_{\theta_0}^2 = 0</script><p>以下为选取不同先验条件时的$L(\theta | D)$图像，用于对比</p><blockquote><ul><li>第一张图为极大似然估计$L(D|\theta)$</li><li>第二张图为先验概率密度函数$P(\theta)$</li><li>第三张图为最大后验概率估计$L(\theta | D)$，$\hat{\theta}$由查表法求解<br>代码见<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/tree/Hexo/source/_posts//参数估计的几种方法/temp.py" target="_blank" rel="noopener">仓库</a></li></ul></blockquote><ul><li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.42$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.56$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\Rightarrow$ $\hat{\theta} = 0.50$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p></li></ul><blockquote><p>结论</p><ul><li>由图$1, 2, 3$，可以看到当$\theta_0$偏移$0.7$时，$MAP$结果也相应偏移；</li><li>由图$2, 4, 5$，可以看到当$\sigma_{\theta_0}^2$越小，即越确定先验概率分布时，$MAP$结果也越趋向于先验概率分布。</li></ul></blockquote><h2 id="贝叶斯估计-1"><a href="#贝叶斯估计-1" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>先验条件为正态分布</p><script type="math/tex; mode=display">\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><script type="math/tex; mode=display">p(\theta | D)= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)= a · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}} · \theta ^7 (1 - \theta) ^3</script><blockquote><p>参数$a$使用<code>scipy.integrate.quad</code>求解</p></blockquote><p>选取不同先验条件时的$L(\theta | D)$图像，用于对比</p><ul><li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Clustering</title>
      <link href="/2018/11/16/Clustering/"/>
      <url>/2018/11/16/Clustering/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="距离度量方法"><a href="#距离度量方法" class="headerlink" title="距离度量方法"></a>距离度量方法</h2><p>机器学习中距离度量方法有很多，以下简单介绍几种。</p><blockquote><p><a href="https://blog.csdn.net/taotiezhengfeng/article/details/80492128" target="_blank" rel="noopener">机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客</a><br><a href="https://blog.csdn.net/u014782458/article/details/58180885" target="_blank" rel="noopener">算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦123的博客 - CSDN博客 </a></p></blockquote><p>定义两个$n$维向量</p><script type="math/tex; mode=display">x = [x_1, x_2, ..., x_n]^T</script><script type="math/tex; mode=display">y = [y_1, y_2, ..., y_n]^T</script><ul><li><p>曼哈顿距离<code>(Manhattan Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_1 = \sum_i |x_i - y_i|</script></li><li><p>欧氏距离<code>(Euclidean Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_2 = \sqrt{\sum_i (x_i - y_i)^2}</script></li><li><p>闽可夫斯基距离<code>(Minkowski Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_p = \left(\sum_i | x_i - y_i |^{p} \right)^{\frac{1}{p}}</script><p>  当$p$取$1$时为曼哈顿距离，取$2$时为欧式距离。</p></li><li><p>余弦距离<code>(Cosine)</code></p><script type="math/tex; mode=display">  d = \frac{x^T y}{||x||_2 ||y||_2} = \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}}</script><blockquote><p>突然想到为什么向量的夹角余弦是怎么来的，高中学习一直背的公式，现在给一下证明。<br>证明：向量的夹角公式<br><img src="/2018/11/16/Clustering/cosine_distance.png" alt="cosine_distance"></p><p>从余弦定理(余弦定理用几何即可)出发，有</p><script type="math/tex; mode=display">\cos \theta = \frac{a^2+b^2-c^2}{2ab}</script><p>其中</p><script type="math/tex; mode=display">||\vec{a}|| = \sqrt{x_1^2 + y_1^2}</script><script type="math/tex; mode=display">||\vec{b}|| = \sqrt{x_2^2 + y_2^2}</script><script type="math/tex; mode=display">||\vec{c}|| = \sqrt{(x_1 - x_2)^2 + (x_2 - y_2)^2}</script><p>故</p><script type="math/tex; mode=display">\cos \theta = \frac  {(\sqrt{x_1^2 + y_1^2})^2 + (\sqrt{x_2^2 + y_2^2})^2 - (\sqrt{(x_1 - x_2)^2 + (x_2 - y_2))^2}}  {2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}</script><script type="math/tex; mode=display">= \frac  {x_1 x_2 + y_1 y_2}  {\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}  = \frac{a^T b}{||a||·||b||}</script></blockquote></li></ul><h2 id="hard-vs-soft-clustering"><a href="#hard-vs-soft-clustering" class="headerlink" title="hard vs. soft clustering"></a>hard vs. soft clustering</h2><ul><li>硬聚类<code>(hard clustering)</code><br>  计算的是一个硬分配<code>(hard ssignment)</code>过程,即每个样本仅仅属于一个簇。</li><li>软聚类<code>(soft clustering)</code><br>  分配过程是软的，即一个样本的分配结果是在所有簇上的一个分布，在软分配结果中，一个样本可能对多个簇都具有隶属度。</li></ul><h2 id="聚类方法的分类"><a href="#聚类方法的分类" class="headerlink" title="聚类方法的分类"></a>聚类方法的分类</h2><ul><li>划分方法<br>  <code>K-means</code>，<code>K-medoids</code>，<code>GMM</code>等。</li><li>层次方法<br>  <code>AGNES</code>，<code>DIANA</code>，<code>BIRCH</code>，<code>CURE</code>和<code>CURE-NS</code>等。</li><li>基于密度的方法<br>  <code>DBSCAN</code>，<code>OPTICS</code>，<code>DENCLUE</code>等。</li><li>其他<br>  如<code>STING</code>等。</li></ul><h1 id="常用聚类方法"><a href="#常用聚类方法" class="headerlink" title="常用聚类方法"></a>常用聚类方法</h1><h2 id="K均值-K-means"><a href="#K均值-K-means" class="headerlink" title="K均值(K-means)"></a>K均值(K-means)</h2><p>是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一，通常用于寻找次优解，再通过其他算法(如<code>GMM</code>)寻找更优的聚类结果。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>给定$N$维数据集</p><script type="math/tex; mode=display">X = [x^{(1)}, x^{(2)}, ..., x^{(M)}]</script><p>指定类别数$K$与初始中心点$\mu^{(0)}$，将样本划分到中心点距离其最近的簇中，再根据本次划分更新各簇的中心$\mu^{(t)}$，如此迭代直至得到最好的聚类结果。预测测试样本时，将其划分到中心点距其最近的簇，也可通过<code>KNN</code>等方法。</p><p>一般使用欧式距离度量样本到各中心点的距离，也可选择余弦距离等，这也是<code>K-means</code>算法的关键</p><script type="math/tex; mode=display">D(x^{(i)}, \mu_k) = || x^{(i)} - \mu_k ||_2^2</script><p>定义损失函数为</p><script type="math/tex; mode=display">J(\Omega) = \sum_i \sum_k r^{(i)}_k D(x^{(i)}, \mu_k)</script><p>其中</p><script type="math/tex; mode=display">r^{(i)}_k = \begin{cases}    1 & x^{(i)} \in C_k \\    0 & otherwise\end{cases}</script><p>或表示为</p><script type="math/tex; mode=display">r^{(i)} = [0, ..., 1_k, ..., 0]^T</script><p>在迭代过程中，损失函数的值不断下降，优化目标为</p><script type="math/tex; mode=display">\min J(\Omega)</script><h3 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h3><ol><li>随机选取$K$个中心点；</li><li>遍历所有数据，计算每个点到各中心点的距离；</li><li>将每个数据划分到最近的中心点中；</li><li>计算每个聚类的平均值，作为新的中心点；</li><li>重复步骤2-步骤4，直到这k个中线点不再变化(收敛)，或执行了足够多的迭代；</li></ol><p><code>K-means</code>更新迭代过程如下图<br><img src="/2018/11/16/Clustering/kmeans_example.gif" alt="kmeans_example"></p><h3 id="缺点与部分解决方法"><a href="#缺点与部分解决方法" class="headerlink" title="缺点与部分解决方法"></a>缺点与部分解决方法</h3><ul><li>局部最优</li><li>初值敏感<br>  初始点的选择会影响<code>K-means</code>聚类的结果，即可能会陷入局部最优解，如下图<br>  <img src="/2018/11/16/Clustering/k_means_init.png" alt="k_means_init"><br>  可通过如下方法解决<ul><li>多次选择初始点运行<code>K-means</code>算法，选择最优的作为输出结果；</li><li><code>K-means++</code></li></ul></li><li>需要定义<code>mean</code>，对于标称型<code>(categorical)</code>数据不适用</li><li>需要给定聚类簇数目$K$<br>  这里给出一种选择簇数目的方法，选择多个$K$值进行聚类，计算代价函数，做成折线图后如下，可以看到在$K=3$处损失值的变化率出现较大变化，则可选择簇的数目为$3$。<br>  <img src="/2018/11/16/Clustering/k_means_choose_K.png" alt="k_means_choose_K"></li><li>噪声数据干扰大</li><li>对于非凸集<code>(non-convex)</code>数据无能为力<br>  谱聚类可解决非凸集数据的聚类问题。</li></ul><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><ul><li><code>K-means++</code><br>  改进初始点选择方法，第$1$个中心点随机选择；之后的初始中心点根据前面选择的中心点决定，若已选取$n$个初始聚类中心$(0&lt;n&lt;K)$，选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。</li><li><code>ISODATA</code><br>  思想：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别.</li><li><code>Kernel K-means</code><br>  参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类。</li></ul><h3 id="类似的算法"><a href="#类似的算法" class="headerlink" title="类似的算法"></a>类似的算法</h3><p>与<code>K-means</code>类似的算法有很多，例如</p><ul><li><code>K-medoids</code><br>  <code>K-means</code>的取值范围可以是连续空间中的任意值，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。<code>K-medoids</code>的取值是数据样本范围中的样本，且可应用在非数值型数据样本上。</li><li><code>k-medians</code><br>  $K$中值，选择中位数更新各簇的中心点。</li><li><code>K-centers</code><br>  <a href="https://www.bjdxs.com/xueshu/28151.html" target="_blank" rel="noopener">混合类型数据的K-Centers聚类算法/The K-Centers Clustering Algorithm for Categorical and Mixe</a></li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-1-kmeans/KMeans.py" target="_blank" rel="noopener">@Github: K-Means</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">class KMeans():</span><br><span class="line">    def __init__(self, n_cluster, mode):</span><br><span class="line">        self.n_cluster = n_cluster  # 簇的个数</span><br><span class="line">        self.mode = mode            # 距离度量方式</span><br><span class="line">        self.centroids = None       # 簇的中心</span><br><span class="line">        self.loss = float(&apos;inf&apos;)    # 优化目标值</span><br><span class="line">        plt.ion()</span><br><span class="line">    def fit(self, X, max_iter=5, min_move=0.1, display=False):</span><br><span class="line">        def initializeCentroids():</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            选择初始点</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroid = np.zeros(shape=(self.n_cluster, X.shape[1])) # 保存选出的点</span><br><span class="line">            pointIdx = []                                           # 保存已选出的点的索引</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                idx = np.random.randint(0, X.shape[0])              # 随机选择一个点</span><br><span class="line">                while idx in pointIdx:                              # 若该点已选出，则丢弃重新选择</span><br><span class="line">                    idx = np.random.randint(0, X.shape[0])</span><br><span class="line">                pointIdx.append(idx)</span><br><span class="line">                centroid[n] = X[idx]</span><br><span class="line">            return centroid</span><br><span class="line">        def dist2Centroids(x, centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            返回向量x到k个中心点的距离值</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            d = np.zeros(shape=(self.n_cluster,))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                d[n] = mathFunc.distance(x, centroids[n], mode)</span><br><span class="line">            return d</span><br><span class="line">        def nearestInfo(centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            每个点最近的簇中心索引、距离</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            ctIdx = -np.ones(shape=(X.shape[0],), dtype=np.int8)    # 每个点最近的簇中心索引，初始化为-1，可作为异常条件</span><br><span class="line">            ctDist = np.ones(shape=(X.shape[0],), dtype=np.float)   # 每个点到最近簇中心的距离</span><br><span class="line">            for i in range(X.shape[0]):</span><br><span class="line">                dists = dist2Centroids(X[i], centroids, mode)</span><br><span class="line">                if mode == &apos;Euclidean&apos;: ctIdx[i] = np.argmin(dists)</span><br><span class="line">                elif mode == &apos;Cosine&apos;:  ctIdx[i] = np.argmax(dists)</span><br><span class="line">                ctDist[i] = dists[ctIdx[i]]             # 保存最相似的距离度量，用于计算loss</span><br><span class="line">            return ctIdx, ctDist</span><br><span class="line">        def updateCentroids(ctIdx):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            更新簇中心</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroids = np.zeros(shape=(self.n_cluster, X.shape[1]))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                X_ = X[ctIdx == n]                      # 筛选出离簇中心Cn最近的样本点</span><br><span class="line">                centroids[n] = np.mean(X_, axis=0)      # 根据筛选出的样本点更新中心值</span><br><span class="line">            return centroids</span><br><span class="line">        def loss(dist):</span><br><span class="line">            return np.mean(dist**2)</span><br><span class="line">        # -----------------------------------------</span><br><span class="line">        loss_min = float(&apos;inf&apos;)                         # 最优分类时的损失值，最小</span><br><span class="line">        n_iter = 0     </span><br><span class="line">        while n_iter &lt; max_iter:                        # 每次迭代选择不同的初始点</span><br><span class="line">            n_iter += 1; isDone = False                 # 表示本次迭代是否已收敛</span><br><span class="line">            centroids_tmp = initializeCentroids()       # 选择本次迭代的初始点</span><br><span class="line">            loss_last = float(&apos;inf&apos;)                    # 本次迭代中，中心点更新前的损失值</span><br><span class="line">            n_update = 0                                # 本次迭代的更新次数计数</span><br><span class="line">            while not isDone:</span><br><span class="line">                n_update += 1</span><br><span class="line">                ctIdx, ctDist = nearestInfo(centroids_tmp, mode=self.mode)</span><br><span class="line">                centroids_tmp = updateCentroids(ctIdx)  # 更新簇中心</span><br><span class="line">                # --- 可视化 ---</span><br><span class="line">                if (display==True) and (X.shape[1] == 2):</span><br><span class="line">                    plt.ion()</span><br><span class="line">                    plt.figure(n_iter); plt.cla()</span><br><span class="line">                    plt.scatter(X[:, 0], X[:, 1], c=ctIdx)</span><br><span class="line">                    plt.scatter(centroids_tmp[:, 0], centroids_tmp[:, 1], c=&apos;r&apos;)</span><br><span class="line">                    plt.pause(0.5)</span><br><span class="line">                # -------------</span><br><span class="line">                loss_now = loss(ctDist); moved = np.abs(loss_last - loss_now)</span><br><span class="line">                if moved &lt; min_move:                    # 若移动过小，则本次迭代收敛</span><br><span class="line">                    isDone = True</span><br><span class="line">                    print(&apos;第%d次迭代结束，中心点更新%d次&apos; % (n_iter, n_update))</span><br><span class="line">                else: loss_last = loss_now</span><br><span class="line">            if loss_now &lt; loss_min:</span><br><span class="line">                self.centroids = centroids_tmp          # 保存损失最小的模型(最优)</span><br><span class="line">                loss_min = loss_now</span><br><span class="line">                # print(&apos;聚类结果已更新&apos;)</span><br><span class="line">        self.loss = loss_min</span><br><span class="line">        print(&apos;=========== 迭代结束 ===========&apos;)</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        各个样本的最近簇中心索引</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        labels = -np.ones(shape=(X.shape[0],), dtype=np.int)    # 初始化为-1，可用作异常条件</span><br><span class="line">        for i in range(X.shape[0]):</span><br><span class="line">            dists_i = np.zeros(shape=(self.n_cluster,))         # 保存X[i]到中心点Cn的距离</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                dists_i[n] = mathFunc.distance(X[i], self.centroids[n], mode=self.mode)</span><br><span class="line">            if self.mode == &apos;Euclidean&apos;:</span><br><span class="line">                labels[i] = np.argmin(dists_i)</span><br><span class="line">            elif self.mode == &apos;Cosine&apos;:</span><br><span class="line">                labels[i] = np.argmax(dists_i)</span><br><span class="line">        return labels</span><br></pre></td></tr></table></figure></p><p>簇数的选择代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def chooseBestK(X, start, stop, step=1, mode=&apos;Euclidean&apos;):</span><br><span class="line">    Ks = np.arange(start, stop + 1, step, dtype=np.int) # 待选择的K</span><br><span class="line">    Losses = np.zeros(shape=Ks.shape)                   # 保存不同K值时的最小损失值</span><br><span class="line">    for k in range(1, Ks.shape[0] + 1):                 # 对于不同的K，训练模型，计算损失</span><br><span class="line">        print(&apos;K = %d&apos;, k)</span><br><span class="line">        estimator = KMeans(n_cluster=k, mode=mode)</span><br><span class="line">        estimator.fit(X, max_iter=10, min_move=0.01, display=False)</span><br><span class="line">        Losses[k - 1] = estimator.loss</span><br><span class="line">    plt.ioff()</span><br><span class="line">    plt.figure(); plt.xlabel(&apos;n_clusters&apos;); plt.ylabel(&apos;loss&apos;)</span><br><span class="line">    plt.plot(Ks, Losses)                                # 做出loss-K曲线</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="均值漂移-Meanshift"><a href="#均值漂移-Meanshift" class="headerlink" title="均值漂移(Meanshift)"></a>均值漂移(Meanshift)</h2><p>本质是一个迭代的过程，能够在一组数据的密度分布中寻找到局部极值，比较稳定，而且是无参密度估计(不需要事先知道样本数据的概率密度分布函数，完全依靠对样本点的计算)，而且在采样充分的情况下，一定会收敛，即可以对服从任意分布的数据进行密度估计。</p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>有一个滑动窗口的思想，即利用当前中心点一定范围内(通常为球域)的点迭代更新中心点，重复移动窗口，直到满足收敛条件。简单的说，<code>Meanshift</code>就是沿着密度上升的方向寻找同属一个簇的数据点。</p><p>定义点$x_0$的$\epsilon$球域如下</p><script type="math/tex; mode=display">S_h(x_0) = \{ x | (x - x_0)^T (x - x_0) \leq \epsilon \}</script><p>若有$n$个点$(x_1, …, x_n)$落在中心点$ptCentroid$的邻域内，其分布如图<br><img src="/2018/11/16/Clustering/DBSCAN4.jpg" alt="DBSCAN4"></p><p>则偏移向量计算方式为</p><script type="math/tex; mode=display">vecShift = \frac{1}{n} \sum_{i=1}^n (x_i - ptCentroid)</script><p>中心点更新公式为</p><script type="math/tex; mode=display">ptCentroid := ptCentroid + vecShift</script><blockquote><p>展开后可发现，其更新公式即</p><script type="math/tex; mode=display">vecShift= \frac{1}{n} \sum_{i=1}^n x_i - ptCentroid</script><script type="math/tex; mode=display">ptCentroid :=  \frac{1}{n} \sum_{i=1}^n x_i</script></blockquote><p><img src="/2018/11/16/Clustering/DBSCAN3.jpg" alt="DBSCAN3"></p><p>一个滑动窗口的动态更新过程如下图<br><img src="/2018/11/16/Clustering/meanshift_example1.gif" alt="meanshift_example1"><br>初始化多个滑动窗口进行<code>MeanShift</code>算法，其更新过程如下，其中每个黑点代表滑动窗口的质心，每个灰点代表一个数据点<br><img src="/2018/11/16/Clustering/meanshift_example2.gif" alt="meanshift_example2"></p><h3 id="高斯权重"><a href="#高斯权重" class="headerlink" title="高斯权重"></a>高斯权重</h3><p>基本思想是，距离当前中心点近的向量对更新结果权重大，而远的权重小，可减小远点的干扰，如下图，$vecShift_2$为高斯权重下的偏移向量<br><img src="/2018/11/16/Clustering/DBSCAN5.jpg" alt="DBSCAN5"></p><p>其偏移向量计算方式为</p><script type="math/tex; mode=display">vecShift = \frac{1}{n} \sum_{i=1}^n w_i · (x_i - ptCentroid)</script><script type="math/tex; mode=display">w_i = \frac{\kappa(x_i - ptCentroid)}{\sum_j \kappa(x_j - ptCentroid)}</script><p>其中</p><script type="math/tex; mode=display">\kappa(z) = \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{||z||^2}{2\sigma^2} \right)</script><p>中心点更新公式仍然为</p><script type="math/tex; mode=display">ptCentroid := ptCentroid + vecShift</script><blockquote><p>展开也可得到</p><script type="math/tex; mode=display">ptCentroid := \frac{\sum_{i=1}^n w_i x_i}{\sum_j w_j}</script></blockquote><h3 id="计算步骤-1"><a href="#计算步骤-1" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$\epsilon_0$，终止条件参数$\epsilon_1$，簇合并参数$\epsilon_2$，并指定样本距离度量方式，目标为将其划分为$K$个簇。</p><ol><li>初始化：<ul><li>在样本集中随机选择$K_0(K_0 \gg K)$个样本作为初始中心点，以邻域大小为$\epsilon_0$建立滑动窗口；</li><li>各个样本初始化一个标记向量，用于记录被各类别访问的次数；</li></ul></li><li>以单个滑动窗口分析，记其中心点为$ptCentroid$，找到滑动窗口内的所有点，记作集合$M$，认为这些点属于该滑动窗口所属的簇类别，同时，这些点被该簇访问的次数$+1$；</li><li>以$ptCentroid$为中心，计算其到集合$M$中各个元素的向量，以这些向量计算得到偏移向量$vecShift$；</li><li>更新中心点：$ptCentroid = ptCentroid + vecShift$，即滑动窗口沿着$vecShift$方向移动，距离为$||vecShift||$；</li><li>重复步骤$2-4$，直到$||vecShift||&lt;\epsilon_1$，保存当前中心点；</li><li>如果收敛时当前簇$ptCentroid$与其它已经存在的簇的中心的距离小于阈值$\epsilon_2$，那么这两个簇合并。否则，把当前簇作为新的簇类，增加$1$类；</li><li>重复迭代直到所有的点都被标记访问；</li><li>根据每个样本被各簇的访问频率，取访问频率最大的那个簇类别作为当前点集的所属类。</li></ol><blockquote><p>即不同类型的滑窗沿着密度上升的方向进行移动，对各样本点进行标记，最后将样本划分为标记最多的类别；当两类非常接近时，合并为一类。</p></blockquote><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p71_meanshift.py" target="_blank" rel="noopener">@Github: MeanShift</a></p><p>先定义了窗格对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SlidingWindow</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        centroid: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        epsilon: &#123;float&#125; 滑动窗格大小，为半径的平方</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125; 高斯核函数的参数</span></span><br><span class="line"><span class="string">        label: &#123;int&#125; 该窗格的标记</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        containIdx: &#123;ndarray(n_contain,)&#125; 窗格内包含点的索引</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, centroid, epsilon, sigma, label, X)</span>:</span></span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.label = label</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">k</span><span class="params">(self, z)</span>:</span></span><br><span class="line">        <span class="string">""" 高斯核函数</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            z: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - \kappa(z) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \exp \left( - \frac&#123;||z||^2&#125;&#123;2\sigma^2&#125; \right)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        norm = np.linalg.norm(z)</span><br><span class="line">        <span class="keyword">return</span> np.exp(- <span class="number">0.5</span> * (norm / self.sigma)**<span class="number">2</span>) / np.sqrt(<span class="number">2</span>*np.pi)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 更新滑动窗格的中心点和所包含点</span></span><br><span class="line"><span class="string">        Returns: &#123;float&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dshift = self.shift(X)</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shift</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 移动窗格</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vecShift: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dshift: &#123;float&#125; 移动的距离</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        n_contain = self.containIdx.shape[<span class="number">0</span>]</span><br><span class="line">        contain_weighted_sum = np.zeros(shape=(n_features, ))</span><br><span class="line">        weight_sum = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 按包含的点进行移动</span></span><br><span class="line">        <span class="keyword">for</span> i_contain <span class="keyword">in</span> range(n_contain):</span><br><span class="line">            vector = X[self.containIdx[i_contain]] - self.centroid</span><br><span class="line">            weight = self.k(vector)</span><br><span class="line">            contain_weighted_sum += weight*X[self.containIdx[i_contain]]</span><br><span class="line">            weight_sum += weight</span><br><span class="line">        centroid = contain_weighted_sum / weight_sum </span><br><span class="line">        <span class="comment"># 计算移动的距离   </span></span><br><span class="line">        dshift = np.linalg.norm(self.centroid - centroid)</span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateContain</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 更新窗格内的点索引</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - 用欧式距离作为度量</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        d = <span class="keyword">lambda</span> x_i, x_j: np.linalg.norm(x_i - x_j)</span><br><span class="line">        n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">        containIdx = np.array([], dtype=<span class="string">'int'</span>)</span><br><span class="line">        <span class="keyword">for</span> i_samples <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">if</span> d(X[i_samples], self.centroid) &lt; self.epsilon:</span><br><span class="line">                containIdx = np.r_[containIdx, i_samples]</span><br><span class="line">        <span class="keyword">return</span> containIdx</span><br></pre></td></tr></table></figure></p><p>聚类算法如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanShift</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_clusters: &#123;int&#125; 划分簇的个数</span></span><br><span class="line"><span class="string">        n_windows: &#123;int&#125; 滑动窗格的个数</span></span><br><span class="line"><span class="string">        epsilon: &#123;float&#125; 滑动窗格的大小</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125; &#123;float&#125; 高斯核参数</span></span><br><span class="line"><span class="string">        thresh: &#123;float&#125; 若两个窗格中心距离小于thresh，则合并两类簇</span></span><br><span class="line"><span class="string">        min_move: &#123;float&#125; 终止条件</span></span><br><span class="line"><span class="string">        windows: &#123;list[class SlidingWindow()]&#125;</span></span><br><span class="line"><span class="string">    Note:</span></span><br><span class="line"><span class="string">        - 假设所有点均被窗格划过</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_clusters, n_windows=<span class="number">-1</span>, epsilon=<span class="number">0.5</span>, sigma=<span class="number">2</span>, thresh=<span class="number">1e-2</span>, min_move=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.n_windows = <span class="number">5</span>*n_clusters <span class="keyword">if</span> (n_windows == <span class="number">-1</span>) <span class="keyword">else</span> n_windows</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.thresh = thresh</span><br><span class="line">        self.min_move = min_move</span><br><span class="line">        self.windows = []</span><br><span class="line">        self.centroids = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        <span class="comment"># 创建窗格</span></span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            idx = np.random.randint(n_samples)</span><br><span class="line">            window = SlidingWindow(X[idx], self.epsilon,</span><br><span class="line">                            self.sigma, i_windows, X)</span><br><span class="line">            <span class="comment"># 将各窗格包含的点标记</span></span><br><span class="line">            n_contain = window.containIdx.shape[<span class="number">0</span>]</span><br><span class="line">            self.windows.append(window)</span><br><span class="line"></span><br><span class="line">        dshift = float(<span class="string">'inf'</span>)   <span class="comment"># 初始化为无穷大</span></span><br><span class="line">        plt.figure(); plt.ion()</span><br><span class="line">        <span class="keyword">while</span> dshift &gt; self.min_move:</span><br><span class="line">            <span class="comment"># ------ 做图显示 ------</span></span><br><span class="line">            plt.cla()</span><br><span class="line">            plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=<span class="string">'b'</span>)</span><br><span class="line">            <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">                centroid = self.windows[i_windows].centroid</span><br><span class="line">                plt.scatter(centroid[<span class="number">0</span>], centroid[<span class="number">1</span>], c=<span class="string">'r'</span>)</span><br><span class="line">            plt.pause(<span class="number">0.5</span>)</span><br><span class="line">            <span class="comment"># ---------------------</span></span><br><span class="line">            dshift = self.step(X)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 合并窗格</span></span><br><span class="line">        dists = np.zeros(shape=(self.n_windows, self.n_windows))</span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            <span class="keyword">for</span> j_windows <span class="keyword">in</span> range(i_windows):</span><br><span class="line">                centroid_i = self.windows[i_windows].centroid</span><br><span class="line">                centroid_j = self.windows[j_windows].centroid</span><br><span class="line">                dists[i_windows, j_windows] = np.linalg.norm(centroid_i-centroid_j)</span><br><span class="line">                dists[j_windows, i_windows] = dists[i_windows, j_windows]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获得距离相近索引</span></span><br><span class="line">        index = np.where(dists&lt;self.thresh)</span><br><span class="line">        <span class="comment"># 用于标记类别</span></span><br><span class="line">        winlabel = np.zeros(shape=(self.n_windows,), dtype=<span class="string">'int'</span>)</span><br><span class="line">        label = <span class="number">1</span>; winlabel[<span class="number">0</span>] = label</span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            idx_row = index[<span class="number">0</span>][i_windows]</span><br><span class="line">            idx_col = index[<span class="number">1</span>][i_windows]</span><br><span class="line">            <span class="comment"># 若其中一个点被标记，则将令一个点并入该类</span></span><br><span class="line">            <span class="keyword">if</span> winlabel[idx_row]!=<span class="number">0</span>:</span><br><span class="line">                winlabel[idx_col] = winlabel[idx_row]</span><br><span class="line">            <span class="keyword">elif</span> winlabel[idx_col]!=<span class="number">0</span>:</span><br><span class="line">                winlabel[idx_row] = winlabel[idx_col]</span><br><span class="line">            <span class="comment"># 否则新创建类别</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label += <span class="number">1</span></span><br><span class="line">                winlabel[idx_row] = label</span><br><span class="line">                winlabel[idx_col] = label</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将标签一样的窗格合并</span></span><br><span class="line">        labels = list(set(winlabel))                            <span class="comment"># 去重后的标签</span></span><br><span class="line">        n_labels = len(labels)                                  <span class="comment"># 标签种类数</span></span><br><span class="line">        self.centroids = np.zeros(shape=(n_labels, n_features)) <span class="comment"># 记录最终聚类中心</span></span><br><span class="line">        <span class="keyword">for</span> i_labels <span class="keyword">in</span> range(n_labels):</span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">                <span class="keyword">if</span> winlabel[i_windows] == labels[i_labels]:</span><br><span class="line">                    self.centroids[i_labels] += self.windows[i_windows].centroid</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">            self.centroids[i_labels] /= cnt                    <span class="comment"># 取同类窗格中心点的均值</span></span><br><span class="line">        <span class="keyword">return</span> self.centroids</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" update all sliding windows</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dshift: \sum_i^&#123;n_windows&#125; dshift_&#123;i&#125;</span></span><br><span class="line"><span class="string">        """</span> </span><br><span class="line">        dshift = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            dshift += self.windows[i_windows].step(X)</span><br><span class="line">            <span class="comment"># label the points</span></span><br><span class="line">            n_contain = self.windows[i_windows].containIdx.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 简单的用近邻的方法求</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        dists = np.zeros(shape=(n_samples, self.n_clusters))</span><br><span class="line">        <span class="keyword">for</span> i_samples <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> i_clusters <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                dists[i_samples, i_clusters] = np.linalg.norm(X[i_samples]-self.centroids[i_clusters])</span><br><span class="line">        <span class="keyword">return</span> np.argmin(dists, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h2 id="谱聚类-Spectral-Clustering"><a href="#谱聚类-Spectral-Clustering" class="headerlink" title="谱聚类(Spectral Clustering)"></a>谱聚类(Spectral Clustering)</h2><p>谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用，比起传统的<code>K-Means</code>算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多。</p><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><blockquote><p><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">谱聚类（spectral clustering）原理总结 - 刘建平Pinard - 博客园 </a></p></blockquote><h4 id="无向权重图"><a href="#无向权重图" class="headerlink" title="无向权重图"></a>无向权重图</h4><p>我们用点的集合$V$和边的集合$E$描述一个图，即$G(V, E)$，其中$V$即数据集中的点</p><script type="math/tex; mode=display">V = [v_1, v_2, ..., v_n]</script><p>而点$v_i, v_j$间连接权值$w_{ij}$组成邻接矩阵$W$，由于为无向图，故满足$w_{ij}=w_{ji}$</p><script type="math/tex; mode=display">W = \left[    \begin{matrix}        w_{11} & ... & w_{1n} \\        ... & ... & ... \\        w_{n1} & ... & w_{nn} \\    \end{matrix}    \right]</script><p>对于图中的任意一个点$v_i$，定义其度$d_i$为</p><script type="math/tex; mode=display">d_i = \sum_{j=1}^n w_{ij}</script><p>则我们可以得到一个度矩阵$D=diag(d_1, …, d_n)$</p><script type="math/tex; mode=display">D = \left[        \begin{matrix}            d_1 & & \\            & ... & \\             & & d_n\\        \end{matrix}    \right]</script><p>除此之外，对于$V$中子集$V_{sub} \subset V$，定义子集$V_{sub}$点的个数为</p><script type="math/tex; mode=display">|V_{sub}| := n_{sub}</script><p>另外，定义该子集中点的度之和为</p><script type="math/tex; mode=display">vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script><h4 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h4><p>上面讲到的邻接矩阵$W$可以指定权值，但对于数据量庞大的数据集，这显然不是一个$wise$的选择。我们可以用相似矩阵$S$来获得邻接矩阵$W$，基本思想是，距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高。</p><p>构建邻接矩阵$W$的方法有三类：$\epsilon$-邻近法，$K$邻近法和全连接法，定义距离</p><script type="math/tex; mode=display">d_{ij} = ||x^{(i)} - x^{(j)}||_2^2</script><ul><li><p>$\epsilon$-邻近法<br>  设置距离阈值$\epsilon$，用欧式距离度量两点的距离$d_{ij}$，然后通过下式确定邻接权值$w_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      0 & d_{ij} > \epsilon \\      \epsilon & otherwise  \end{cases}</script><blockquote><p>两点间的权重要不就是$\epsilon$，要不就是0，距离远近度量很不精确，因此在实际应用中，我们很少使用$\epsilon$-邻近法。</p></blockquote></li><li><p>$K$邻近法</p><ul><li><p>第一种<br>  只要一个点在另一个点的$K$近邻中，则保留$d_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　or　x^{(j)} \in KNN(x^{(i)}) \\      0 & otherwise  \end{cases}</script></li><li><p>第二种<br>  互为$K$近邻时保留$d_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　and　x^{(j)} \in KNN(x^{(i)}) \\      0 & otherwise  \end{cases}</script></li></ul></li><li><p>全连接法<br>  可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和<code>Sigmoid</code>核函数。最常用的是高斯核函数<code>RBF</code>，此时相似矩阵和邻接矩阵相同</p><script type="math/tex; mode=display">  w_{ij} = \exp \left( -\frac{d_{ij}}{2\sigma^2} \right)</script></li></ul><h4 id="拉普拉斯矩阵-Graph-Laplacians"><a href="#拉普拉斯矩阵-Graph-Laplacians" class="headerlink" title="拉普拉斯矩阵(Graph Laplacians)"></a>拉普拉斯矩阵(Graph Laplacians)</h4><p>定义</p><script type="math/tex; mode=display">L = D - W</script><p>正则化的拉普拉斯矩阵为</p><script type="math/tex; mode=display">L = D^{-1} (D - W)</script><p>具有的性质如下</p><ol><li>$L^T = L$</li><li>其特征值均为实数，即$\lambda_i \in \mathbb{R}$</li><li>正定性：$\lambda_i \geq 0$</li><li><p>对于任意向量$x$，都有</p><script type="math/tex; mode=display">x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script><blockquote><p>证明：</p><script type="math/tex; mode=display">x^T L x = x^T D x - x^T W x = \sum_i d_i x_i^2 - \sum_{ij} w_{ij} x_i x_j</script><script type="math/tex; mode=display">= \frac{1}{2} \left[ \sum_i d_i x_i^2 - 2\sum_{ij} w_{ij} x_i x_j + \sum_j d_j x_j^2 \right]</script><p>其中$ d_i = \sum_j w_{ij} $，所以</p><script type="math/tex; mode=display">x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script></blockquote></li></ol><h4 id="无向图的切图"><a href="#无向图的切图" class="headerlink" title="无向图的切图"></a>无向图的切图</h4><h5 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h5><p>我们希望把一张无向图$G(V, E)$按一定方法切成多个子图，各个子图间无连接，每个子图的点集为$V_1, …, V_K$，满足</p><ul><li>$\bigcup_{k=1}^K V_k = V$</li><li>$V_i \cap V_j = \emptyset$</li></ul><p>定义两个子图点集合$A, B$之间的切图权重为</p><script type="math/tex; mode=display">W(A, B) = \sum_{i \in A, j \in B} w_{ij}</script><blockquote><p>共有$n_A × n_B$个权值作累加</p></blockquote><p>那么对于$K$个子图点的集合$V_1, …, V_K$，定义切图为</p><script type="math/tex; mode=display">cut(V_1, ..., V_K) = \frac{1}{2} \sum_{i=1}^K cut(V_i, \overline{V_i})</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><p>其中$\overline{V_i}$表示$V_i$的补集，或者</p><script type="math/tex; mode=display">\overline{V_i} = \bigcup_{k \neq i} V_k</script><p>通过最小化$cut(V_1, …, V_K)$使子图内权重和大，而子图间权重和小。但是这种方法存在问题，如下图<br><img src="/2018/11/16/Clustering/cut.jpg" alt="cut"></p><p>选择一个权重最小的边缘的<strong>点</strong>，比如$C$和$H$之间进行$cut$，这样可以最小化$cut(V_1, …, V_K)$，但是却不是最优的切图。</p><p>为解决上述问题，需要对每个子图的规模做出限定，以下介绍两种切图方式。</p><h5 id="Ratio-Cut"><a href="#Ratio-Cut" class="headerlink" title="Ratio Cut"></a>Ratio Cut</h5><p>不仅考虑最小化$cut(V_1, …, V_K)$，而且最大化每个子图的点个数，即</p><script type="math/tex; mode=display">RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{|V_k|}</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote><ul><li>$W(V_k, \overline{V_k}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}$</li><li>$|V_k| = n_k$</li></ul></blockquote><p>如果按照遍历的方法求解，由前面分析，$W(V_k, \overline{V_k})$需计算$n_{V_k} × n_{\overline{V_k}}$次累加，计算量庞大，那么如何求解呢？</p><p>定义指示向量$h_k$，其构成矩阵$H$</p><script type="math/tex; mode=display">H = [ h_1, ..., h_k, ..., h_K]</script><p>其中</p><script type="math/tex; mode=display">h_k = \left[h_{k1}, h_{k2}, , ..., h_{kM} \right]^T</script><script type="math/tex; mode=display">h_{ki} = \begin{cases}    \frac{1}{\sqrt{|V_k|}} & x^{(i)}\in V_k \\    0 & otherwise\end{cases}</script><blockquote><p>$h_k$为单位向量，且两两正交</p><script type="math/tex; mode=display">h_i^T h_j =       \begin{cases}          \sum_{|V_i|} \frac{1}{|V_i|} = |V_i| · \frac{1}{|V_i|} = 1 & i = j \\          0 & i \neq j      \end{cases}</script></blockquote><p>那么由拉式矩阵性质$4$</p><script type="math/tex; mode=display">h_k^T L h_k = \frac{1}{2} \sum_{i,j} w_{ij} (h_{ki} - h_{kj})^2</script><script type="math/tex; mode=display">= \frac{1}{2}     [        \sum_{i \in V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 +         \sum_{i \notin V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 +</script><script type="math/tex; mode=display">        \sum_{i \in V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 +         \sum_{i \notin V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2    ]</script><script type="math/tex; mode=display">= \frac{1}{2}    [        \sum_{i \in V_k, j \in V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - \frac{1}{\sqrt{|V_k|}})^2 +         \sum_{i \notin V_k, j \in V_k} w_{ij} (0 - \frac{1}{\sqrt{|V_k|}})^2 +</script><script type="math/tex; mode=display">        \sum_{i \in V_k, j \notin V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - 0)^2 +        \sum_{i \notin V_k, j \notin V_k} w_{ij} (0 - 0)^2    ]</script><script type="math/tex; mode=display">= \frac{1}{2}    [        \sum_{i \notin V_k, j \in V_k} w_{ij} \frac{1}{|V_k|} +        \sum_{i \in V_k, j \notin V_k} w_{ij} \frac{1}{|V_k|}    ]</script><blockquote><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}</script></blockquote><script type="math/tex; mode=display">h_k^T L h_k = \frac{1}{2} [\frac{1}{|V_k|} cut(V_k, \overline{V_k}) + \frac{1}{|V_k|} cut(V_k, \overline{V_k})]</script><script type="math/tex; mode=display">= \frac{1}{|V_k|} cut(V_k, \overline{V_k})</script><p>推到这里就能理解为什么要定义$h_k$了</p><script type="math/tex; mode=display">RatioCut(V_1, ..., V_K)= \frac{1}{2} \sum_k h_k^T L h_k</script><p>并且</p><script type="math/tex; mode=display">h_k^T L h_k = tr(H^T L H)</script><blockquote><script type="math/tex; mode=display">H^T L H = \left[      \begin{matrix}          — & h_1^T & — \\           & ... &  \\          — & h_K^T & — \\      \end{matrix}\right]L\left[          \begin{matrix}            | & & | \\            h_1 & ... & h_K \\            | & & |        \end{matrix}    \right]</script><script type="math/tex; mode=display">= \left[      \begin{matrix}          h_1^T L h_1 & ... & h_1^T L h_K \\          ... & ... & ... \\          h_K^T L h_K & ... & h_K^T L h_K \\      \end{matrix}\right]</script></blockquote><p>所以最终优化目标为</p><script type="math/tex; mode=display">\min_H tr(H^T L H)</script><script type="math/tex; mode=display">s.t.　H^T H = I</script><blockquote><script type="math/tex; mode=display">H^T H = \left[      \begin{matrix}          h_1^T h_1 & ... & h_1^T h_K \\          ... & ... & ... \\          h_K^T h_K & ... & h_K^T h_K \\      \end{matrix}\right]</script></blockquote><p>而矩阵的正交相似变换$A = P \Lambda P^{-1}$满足</p><script type="math/tex; mode=display">tr(A) = tr(\Lambda) = \sum_i \lambda_i</script><p>故</p><script type="math/tex; mode=display">tr(H^T L H) = tr(L) = \sum_{i=1}^M \lambda_i</script><p>$\lambda_i$为矩阵$L$的特征值。</p><p>我们再进行维度规约，将维度从$M$降到$k_1$，即找到$k_1$个最小的特征值之和。</p><h5 id="N-Cut"><a href="#N-Cut" class="headerlink" title="N Cut"></a>N Cut</h5><p>推导过程与<code>RatioCut</code>完全一致，只是将分母$|V_i|$换成$vol(V_i)$</p><script type="math/tex; mode=display">NCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{vol(V_i)}</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote><script type="math/tex; mode=display">vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script></blockquote><h3 id="计算步骤-2"><a href="#计算步骤-2" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，将其划分为$K$类$(C_1, …, C_K)$</p><ol><li>根据输入的相似矩阵的生成方式构建样本的相似矩阵$S_{M×M}$；</li><li>根据相似矩阵$S$构建邻接矩阵$W_{M×M}$；</li><li>构建度矩阵$D_{M×M}$；</li><li>计算拉普拉斯矩阵$L_{M×M}$，可进行规范化$ L := D^{-1}L $；</li><li>对$L$进行特征值分解<code>(EVD)</code>，得到特征对$ (\lambda_i, \alpha_i), i=1,…,M $；</li><li>指定超参数$K_1$，选取$K_1$个最小特征值对应的特征向量组成矩阵$F_{M×K_1}$，并将其按行标准化；</li><li>以$F$的行向量作为新的样本数($k_1$维，这里也有降维操作)进行聚类，划分为$K$类，可使用<code>K-means</code>；</li><li>聚类结果即为输出结果</li></ol><p>注意是$K_1$个最小特征值对应的特征向量，别问我为什么知道。。。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p86_spectral_clustering.py" target="_blank" rel="noopener">@Github: Spectral Clustering</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpectralClustering</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        k: &#123;int&#125;, k &lt; n_samples</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        Steps:</span></span><br><span class="line"><span class="string">            - similarity matrix [W_&#123;n×n&#125;]</span></span><br><span class="line"><span class="string">            - diagonal matrix [D_&#123;n×n&#125;] is defined as</span></span><br><span class="line"><span class="string">                    D_&#123;ii&#125; = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">                                \sum_j W_&#123;ij&#125; &amp; i \neq j \\</span></span><br><span class="line"><span class="string">                                0 &amp; i = j</span></span><br><span class="line"><span class="string">                            \end&#123;cases&#125;</span></span><br><span class="line"><span class="string">            - Laplacian matrix [L_&#123;n×n&#125;], Laplacian matrix is defined as</span></span><br><span class="line"><span class="string">                    L = D - W or L = D^&#123;-1&#125; (D - W)</span></span><br><span class="line"><span class="string">            - EVD: L \alpha_i = \lambda_i \alpha_i</span></span><br><span class="line"><span class="string">            - takes the eigenvector corresponding to the largest eigenvalue as</span></span><br><span class="line"><span class="string">                    B_&#123;n×k&#125; = [\beta_1, \beta_2, ..., \beta_k]</span></span><br><span class="line"><span class="string">            - apply K-Means to the row vectors of matrix B</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k, n_clusters=<span class="number">2</span>, sigma=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">        self.kmeans = KMeans(n_clusters=n_clusters)</span><br><span class="line">        self.k = k</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># step 1</span></span><br><span class="line">        kernelGaussian = <span class="keyword">lambda</span> z, sigma: np.exp(<span class="number">-0.5</span> * np.square(z/sigma))</span><br><span class="line">        W = np.zeros((n_samples, n_samples))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                W[i, j] = kernelGaussian(np.linalg.norm(X[i] - X[j]), self.sigma)</span><br><span class="line">                W[j, i] = W[i, j]</span><br><span class="line">        <span class="comment"># step 2</span></span><br><span class="line">        D = np.diag(np.sum(W, axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># step 3</span></span><br><span class="line">        L = D - W</span><br><span class="line">        L = np.linalg.inv(D).dot(L)</span><br><span class="line">        <span class="comment"># step 4</span></span><br><span class="line">        eigval, eigvec = np.linalg.eig(L)</span><br><span class="line">        <span class="comment"># step 5</span></span><br><span class="line">        order = np.argsort(eigval)</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line">        beta = eigvec[:, :self.k]</span><br><span class="line">        <span class="comment"># step 6</span></span><br><span class="line">        self.kmeans.fit(beta)</span><br><span class="line">        <span class="keyword">return</span> self.kmeans.labels_</span><br></pre></td></tr></table></figure></p><p><img src="/2018/11/16/Clustering/spectral_clustering.png" alt="spectral_clustering"></p><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><p><code>DBSCAN(Density-Based Spatial Clustering of Applications with Noise)</code>，具有噪声的基于密度的聚类方法。假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的。</p><blockquote><p><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">DBSCAN密度聚类算法 - 刘建平Pinard - 博客园</a></p></blockquote><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>先介绍几个关于密度的概念</p><ul><li><p>$\epsilon$-邻域<br>  对于样本$x^{(i)}$，其$\epsilon$-邻域包含样本集中与$x^{(i)}$距离不大于$\epsilon$的子样本集，其样本个数记作$|N_{\epsilon}(x^{(i)})|$。</p><script type="math/tex; mode=display">  N_{\epsilon}(x^{(i)}) = \{ x^{(j)} | d_{ij} \leq \epsilon \}</script></li><li><p>核心对象<br>  对于任一样本$x^{(i)}$，若其$\epsilon$-邻域$N_{\epsilon}(x^{(i)})$至少包含$minPts$个样本，则该样本为核心对象。如图，选择若选取$\epsilon=5$，则红点均为核心对象</p></li><li>密度直达<br>  若样本$x^{(j)} \in N_{\epsilon}(x^{(i)})$，且$x^{(i)}$为核心对象，则称$x^{(j)}$由$x^{(i)}$密度直达。不满足对称性，即反之不一定成立，除非$x^{(j)}$也为核心对象。如图，$x^{(8)}$可由$x^{(6)}$密度直达，而反之$x^{(6)}$不可由$x^{(8)}$密度直达，因为$x^{(8)}$不为核心对象。</li><li>密度可达<br>  若存在样本序列$p_1, p_2, …, p_T$，满足$p_1 = x^{(i)}, p_T = x^{(j)}$，且$p_{t+1}$可由$p_t$密度直达，也就是说$p_1, p_2, …, p_{T-1}$均为核心对象，则称$x^{(j)}$由$x^{(i)}$密度可达。也不满足对称性。如图，$x^{(4)}$可由$x^{(1)}$密度可达，而$x^{(2)}$不可由$x^{(4)}$密度可达，因为$x^{(4)}$不为核心对象。</li><li>密度相连<br>  存在核心对象$x^{(k)}$，使得$x^{(i)}$与$x^{(j)}$均由$x^{(k)}$密度可达，则称$x^{(i)}$与$x^{(j)}$密度相连。注意密度相连满足对称性。如图，$x^{(8)}$与$x^{(4)}$均可由$x^{(1)}$密度可达，则$x^{(8)}$与$x^{(4)}$密度相连。</li></ul><p><img src="/2018/11/16/Clustering/DBSCAN1.jpg" alt="DBSCAN1"></p><h3 id="计算思想"><a href="#计算思想" class="headerlink" title="计算思想"></a>计算思想</h3><p><code>DBSCAN</code>的聚类思想是，由<strong>密度可达关系</strong>导出的最大密度相连的样本集合，即为我们最终聚类的一个簇，这个簇里可能只有一个核心对象，也可能有多个核心对象，若有多个，则簇里的任意一个核心对象的$\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。</p><p>另外，考虑以下三个问题</p><ul><li>噪音点<br>  一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，这些样本点标记为噪音点，<code>with Noise</code>就是这个意思。</li><li>距离的度量<br>  一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和<code>KNN</code>算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用<code>KDTree</code>或者球树来快速的搜索最近邻。</li><li>类别重复时的判别<br>  某些样本可能到两个核心对象的距离都小于$\epsilon$，但是这两个核心对象如下图所示，不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？<br>  <img src="/2018/11/16/Clustering/DBSCAN2.jpg" alt="DBSCAN2"><br>  一般来说，此时<code>DBSCAN</code>采用<strong>先来后到</strong>，先进行聚类的类别簇会标记这个样本为它的类别。也就是说<code>BDSCAN</code>不是完全稳定的算法。</li></ul><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$(\epsilon, minPts)$与样本距离度量方式，将其划分为$K$类。</p><ol><li>检测数据库中尚未检查过的对象$p$，如果$p$未被处理(归为某个簇或者标记为噪声)，则检查其邻域：<ul><li>若包含的对象数不小于$minPts$，建立新簇$C$，将其中的所有点加入候选集$N$；</li></ul></li><li>对候选集$N$中所有尚未被处理的对象$q$，检查其邻域：<ul><li>若至少包含$minPts$个对象，则将这些对象加入$N$；</li><li>如果$q$未归入任何一个簇，则将$q$加入$C$；</li></ul></li><li>重复步骤$2$，继续检查$N$中未处理的对象，直到当前候选集$N$为空；</li><li>重复步骤$1$-$3$，直到所有对象都归入了某个簇或标记为噪声。</li></ol><h2 id="高斯混合模型-GMM"><a href="#高斯混合模型-GMM" class="headerlink" title="高斯混合模型(GMM)"></a>高斯混合模型(GMM)</h2><p>详情查看<a href="https://louishsu.xyz/2018/11/12/EM%E7%AE%97%E6%B3%95-GMM%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">EM算法 &amp; GMM模型</a>。</p><h2 id="层次聚类-Hierarchical-Clustering"><a href="#层次聚类-Hierarchical-Clustering" class="headerlink" title="层次聚类(Hierarchical Clustering)"></a>层次聚类(Hierarchical Clustering)</h2><p>层次聚类更多的是一种思想，而不是方法，通过从下往上不断合并簇，或者从上往下不断分离簇形成嵌套的簇。例如上面讲到的<code>DBSCAN</code>最后簇的合并就有这种思想。</p><p>层次的类通过“树状图”来表示，如下<br><img src="/2018/11/16/Clustering/层次聚类.png" alt="层次聚类"></p><p>主要的思想或方法有两种</p><ul><li>自底向上的凝聚方法<code>(agglomerative hierarchical clustering)</code><br>  如<code>AGNES</code>。</li><li>自上向下的分裂方法<code>(divisive hierarchical clustering)</code><br>  如<code>DIANA</code>。</li></ul><h2 id="图团体检测-Graph-Community-Detection"><a href="#图团体检测-Graph-Community-Detection" class="headerlink" title="图团体检测(Graph Community Detection)"></a>图团体检测(Graph Community Detection)</h2><p>略</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EM &amp; GMM</title>
      <link href="/2018/11/12/EM-GMM/"/>
      <url>/2018/11/12/EM-GMM/</url>
      
        <content type="html"><![CDATA[<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p><code>Expectation Maximization Algorithm</code>，是 Dempster, Laind, Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 <code>MLE</code> 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据。</p><h2 id="引例：先挖个坑"><a href="#引例：先挖个坑" class="headerlink" title="引例：先挖个坑"></a>引例：先挖个坑</h2><p>给出李航《统计学习方法》的三硬币模型例子，假设有$3$枚硬币$A, B, C$，各自出现正面的概率分别为$\pi, p, q$，先进行如下实验：先投掷硬币$A$，若结果为正面，则选择硬币$B$投掷一次，否则选择$C$，记录投掷结果如下</p><script type="math/tex; mode=display">1, 1, 0, 1, 0, 0, 1, 0, 1, 1</script><p>只能观测到实验结果，而投掷过程未知，即硬币$A$的投掷结果未知，现欲估计三枚硬币的参数$\pi, p, q$。</p><p><strong>解</strong>：根据题意可以得到三个随机变量$X_1, X_2, X_3$的概率分布如下</p><script type="math/tex; mode=display">P(X_1) = \pi ^ {X_1} (1 - \pi) ^ {1 - X_1}</script><script type="math/tex; mode=display">P(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2}</script><script type="math/tex; mode=display">P(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}</script><p>定义随机变量$X$表示观测结果为正面，由全概率公式可以得到</p><script type="math/tex; mode=display">P(X) = P(X|X_1)P(X_1) + P(X|\overline{X_1})P(\overline{X_1})= \pi p + (1 - \pi) q</script><script type="math/tex; mode=display">P(\overline{X}) = P(\overline{X}|X_1)P(X_1) + P(\overline{X}|\overline{X_1})P(\overline{X_1})= \pi (1 - p) + (1 - \pi) (1 - q)</script><p>即</p><script type="math/tex; mode=display">P(X) = [\pi p + (1 - \pi) q] ^ {X} [\pi (1 - p) + (1 - \pi) (1 - q)] ^ {1 - X}</script><p>利用最大似然估计，有</p><script type="math/tex; mode=display">\log L(D | \theta) = 6 \log [\pi p + (1 - \pi) q] + 4 \log [\pi (1 - p) + (1 - \pi) (1 - q)]</script><p>至此，我们一定能想到通过求似然函数极值来求解参数</p><script type="math/tex; mode=display">\frac{∂ }{∂ \pi} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><script type="math/tex; mode=display">\frac{∂ }{∂ p} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><script type="math/tex; mode=display">\frac{∂ }{∂ q} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><p>但是好像出了问题，并不能求解，所以我们引入<code>EM算法</code>迭代求解。</p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>以$x^{(i)}$表示训练数据，$w_k$表示类别，设当前迭代参数为$\theta^{(t)}$，则下一次迭代应有</p><script type="math/tex; mode=display">\theta^{(t+1)} = \arg \max \sum_i \log P(x^{(i)}|\theta) \tag{1}</script><p>由边缘概率公式</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \tag{2}</script><blockquote><p>$P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)}|x^{(i)}, \theta)$<br>至此已得出引例中的表达式，其中$P(w_k^{(i)}|x^{(i)}, \theta)$与$P(x^{(i)} | w_k^{(i)}, \theta)$均未知，而通过求极值不能解得参数。</p></blockquote><p>我们引入迭代参数$\theta^{(t)}$，即第$t$次迭代时的参数$\theta$，该参数为已知变量</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)} | \theta^{(t)})}{P(w_k^{(i)} | \theta^{(t)})}</script><blockquote><p>$P(w_k^{(i)}|\theta^{(t)})$表示样本$x^{(i)}$类别为$w_k^{(i)}$的概率，注意上标。</p></blockquote><p>引入<code>Jensen不等式</code>：</p><blockquote><p>For a real convex function $\varphi$, numbers $x_1, …, x_n$ in its domain, and positive weights $a_i$, Jensen’s inequality can be stated as:</p><script type="math/tex; mode=display">\varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right)\leq \frac{\sum a_i \varphi(x_i)}{\sum a_i}</script><p>and the inquality is reversed if $\varphi$ is concave, which is</p><script type="math/tex; mode=display">\varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right)\geq \frac{\sum a_i \varphi(x_i)}{\sum a_i}</script><p>Equality holds if and only if $x_1 = … = x_n$ or $\varphi$ is linear.</p></blockquote><p>$\log(·)$为凹函数<code>(concave)</code>，且满足</p><script type="math/tex; mode=display">\sum_k P(w_k^{(i)} | \theta^{(t)}) = 1</script><p>所以有</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">\geq \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{3}</script><p>此时我们得到似然函数$\sum_i \log P(x^{(i)}|\theta)$的一个下界，但必须保证这个下界是紧的，也就是至少有点能使等号成立</p><blockquote><p>由<code>Jensen不等式</code>，当且仅当$　P(x^{(i)}, w_k^{(i)}|\theta)=C　$时取等号</p></blockquote><p>定义</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) - P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)})</script><p>其中第一项即期望</p><script type="math/tex; mode=display">E_w\left[    \log P(X, w|\theta) | X, \theta^{(t)}\right] = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) \tag{4}</script><p>第二项为$P(w | X, \theta^{(t)})$的信息熵</p><script type="math/tex; mode=display">H[P(w | X, \theta^{(t)})] = - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5}</script><p>即</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= E_w\left[    \log P(X, w|\theta) | X, \theta^{(t)}\right] +H[P(w | X, \theta^{(t)})] \tag{E-step}</script><blockquote><p>注意到$H[P(w | X, \theta^{(t)})]$项为常数，故也可设</p><script type="math/tex; mode=display">Q(\theta|\theta^{(t)}) = E_w\left[\log P(X, w|\theta) | X, \theta^{(t)} \right]</script></blockquote><p>代回$(1)$，得到优化目标</p><script type="math/tex; mode=display">\theta^{(t+1)} = \arg \max L(\theta|\theta^{(t)}) \tag{M-step}</script><p>我们需要不断最大化$L(\theta | \theta^{(t)})$来不断优化，这就是所谓的<code>EM算法</code>，<code>E-step</code>是指求出期望，<code>M-step</code>是指迭代更新参数<br><img src="/2018/11/12/EM-GMM/EM算法图解.png" alt="EM算法图解"></p><p>伪代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">According to prior knowledge set </span><br><span class="line">    $\theta$</span><br><span class="line">Repeat until convergence&#123;</span><br><span class="line">    E-step: The expectation of hidden variables</span><br><span class="line">    M-step: Finding the maximum of likelihood function</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>实际上，从边缘概率与条件概率入手</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta)</script><script type="math/tex; mode=display">= \sum_i \log \sum_k P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta)</script><script type="math/tex; mode=display">\geq \sum_i \sum_k P(w_k^{(i)} | \theta) \log P(x^{(i)} | w_k^{(i)}, \theta) \tag{Jensen inequality}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)}</script><p>而由$(3)$，引入迭代变量可以得到</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)\geq L(\theta|\theta^{(t)})</script><p>其中</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><p>则</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)})</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} -\sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta)}</script><p>而由<code>KL散度( Kullback–Leibler divergence)</code>(又称<code>相对熵(relative entropy)</code>)定义</p><blockquote><script type="math/tex; mode=display">D(P||Q) = \sum P(x) \log \frac{P(x)}{Q(x)}</script></blockquote><p>可知</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = D\left[ P(w_k^{(i)}|\theta^{(t)}) || P(w_k^{(i)}|\theta) \right]</script><p>即迭代的$P(w_k^{(i)}|\theta^{(t)})$与真实的$P(w_k^{(i)}|\theta)$之间的相对熵！</p><blockquote><p>这里关于<code>K-L散度</code>的困扰了$N$久，终于搞出来了。</p></blockquote><!-- 另外，附上证明一则> 证明对数似然函数$\sum_i \log P(x^{(i)} | \theta)$满足> $$> \sum_i \log P(x^{(i)} | \theta^{(t+1)}) > \geq \sum_i \log P(x^{(i)} | \theta^{(t)})> $$> > 证明：由$(M-step)$> $$\sum_i \log P(x^{(i)} | \theta^{(t+1)})> = \max L(\theta | \theta^{(t)})$$> > 而$\theta^{(t+1)}为函数L(\theta|\theta^{(t)})极值点$，所以> $$\max L(\theta | \theta^{(t)})> \geq L(\theta | \theta^{(t)})$$> > 其中> $$> L(\theta | \theta^{(t)})> = \sum_i \log P(x^{(i)} | \theta^{(t)})> $$> > 故> $$> \sum_i \log P(x^{(i)} | \theta^{(t+1)}) > \geq \sum_i \log P(x^{(i)} | \theta^{(t)})> $$ --><h2 id="引例的求解"><a href="#引例的求解" class="headerlink" title="引例的求解"></a>引例的求解</h2><blockquote><p>$Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$</p></blockquote><p>此题中</p><script type="math/tex; mode=display">P(w_k|\pi) = \pi^{w_k}(1-\pi)^{1-w_k}</script><script type="math/tex; mode=display">P(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}}</script><script type="math/tex; mode=display">P(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}}</script><ul><li><p>$E-step$</p><script type="math/tex; mode=display">  Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)})  = \sum_i \sum_k P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})   \log P(x^{(i)}, w_k^{(i)} | \pi, p, q)</script><p>  先求$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$，即第一次投掷结果为$w_k$的概率</p><script type="math/tex; mode=display">  P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})  = \frac  {\left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_k}}  {\sum_j \left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_j}  \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_j}}</script><p>  即</p><script type="math/tex; mode=display">  \begin{cases}      P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}}          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} +           (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\      P(w_2^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac          {(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} +           (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}  \end{cases}</script><p>  记</p><script type="math/tex; mode=display">\mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})</script><script type="math/tex; mode=display">\mu_2^{(i)} = 1 - \mu_1^{(i)}</script><blockquote><p>注意$w^{(i)}_k$上标<code>^{(i)}</code></p></blockquote><p>  再求$P(x^{(i)}, w_k^{(i)} | \pi, p, q)$，已知</p><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)} | \pi, p, q)  = P(x^{(i)} | w_k^{(i)}, \pi, p, q)  P(w_k^{(i)} | \pi, p, q)</script><p>  所以</p><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)} | \pi, p,q)  = \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k}</script><p>  综上</p><script type="math/tex; mode=display">  Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)})  = \sum_i \sum_{k=1}^2 \mu^{(i)}_k \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k}</script><script type="math/tex; mode=display">  = \sum_i \mu_1^{(i)} \log \pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \mu_1^{(i)}) \log (1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}</script></li><li><p>$M-step$</p><ul><li><p>$\frac{∂Q}{∂\pi} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂\pi}  = \sum_i \mu_1^{(i)}   \frac  {p^{x^{(i)}}(1-p)^{1-x^{(i)}}}  {\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} +   (1 - \mu_1^{(i)})  \frac  {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}}  {(1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}}</script><script type="math/tex; mode=display">  = \sum_i \frac{\mu_1^{(i)}}{\pi} + \frac{\mu_1^{(i)} - 1}{1 - \pi}  = \sum_i \frac{\mu_1^{(i)} - \pi}{\pi(1 - \pi)}  = \frac{\sum_i \mu_1^{(i)} - n\pi}{\pi(1 - \pi)} = 0</script><script type="math/tex; mode=display">  \Rightarrow \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)}</script></li><li><p>$\frac{∂Q}{∂p} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂p}  = \sum_i \mu_1^{(i)}   \left[      \frac{x^{(i)}}{p} - \frac{1 - x^{(i)}}{1 - p}  \right]</script><script type="math/tex; mode=display">  = \frac{1}{p(1 - p)}   \sum_i \mu_1^{(i)} (x^{(i)} - p)  = \frac{1}{p(1 - p)}  \left[      \sum_i \mu_1^{(i)} x^{(i)} - p \sum_i \mu_1^{(i)}  \right] = 0</script><script type="math/tex; mode=display">  \Rightarrow p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}}</script></li><li><p>$\frac{∂Q}{∂q} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂q}  = \sum_i (1 - \mu_1^{(i)})   \left[      \frac{x^{(i)}}{q} - \frac{1 - x^{(i)}}{1 - q}  \right]</script><script type="math/tex; mode=display">  = \frac{1}{q(1 - q)}  \sum_i (1 - \mu_1^{(i)})   (x^{(i)} - q)</script><script type="math/tex; mode=display">  = \frac{1}{q(1 - q)}  \left[      \sum_i (1 - \mu_1^{(i)}) x^{(i)} -      q \sum_i (1 - \mu_1^{(i)})  \right] = 0</script><script type="math/tex; mode=display">  \Rightarrow q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})}</script></li></ul></li></ul><p>多次迭代即可求解，终止条件可设置为</p><script type="math/tex; mode=display">|| \theta^{(t+1)} - \theta^{(t)} || < \epsilon</script><p>或</p><script type="math/tex; mode=display">||Q(\theta^{(t+1)} | \theta^{(t)}) - Q(\theta^{(t)} |\theta^{(t)})|| < \epsilon</script><h1 id="GMM模型"><a href="#GMM模型" class="headerlink" title="GMM模型"></a>GMM模型</h1><p><code>Gaussian Mixture Model</code>，是一种无监督学习算法，常用于聚类。当聚类问题中各个类别的尺寸不同、聚类间有相关关系的时候，往往使用<code>GMM</code>更合适。对一个样本来说，<code>GMM</code>得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被成为软聚类。一般说来， 任意形状的概率分布都可以用多个高斯分布函数去近似，因而，<code>GMM</code>的应用也比较广泛。</p><p>高斯混合模型，指具有如下形式的概率分布模型：</p><script type="math/tex; mode=display">P(x|\mu_k, \Sigma_k)= \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)</script><p>其中</p><ul><li>$\pi_k(0 \leq \pi_k \leq 1)$是系数，且$\sum_k \pi_k = 1$</li><li>$N(x|\mu_k, \Sigma_k)$为高斯密度函数</li></ul><script type="math/tex; mode=display">N(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[    -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right]</script><blockquote><ul><li>即多个高斯分布叠加出来的玩意；</li><li>现在我们需要求取系数$\pi_k$及高斯模型的参数$(\mu_k, \Sigma_k)$；</li><li>与<code>K-Means</code>等聚类方法区别是，<code>GMM</code>求出的是连续的分布模型，可计算出“归属于”哪一类的概率。</li></ul></blockquote><h2 id="推导-1"><a href="#推导-1" class="headerlink" title="推导"></a>推导</h2><script type="math/tex; mode=display">\log P(X|\pi, \mu, \Sigma)= \sum_i \log \sum_k \pi_k N(x|\mu_k, \Sigma_k)</script><script type="math/tex; mode=display">s.t.　\sum_k \pi_k = 1</script><h3 id="暴力求解"><a href="#暴力求解" class="headerlink" title="暴力求解"></a>暴力求解</h3><p>以$1$维高斯分布为例</p><script type="math/tex; mode=display">N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}}</script><p>构造拉格朗日<code>(Lagrange)</code>函数</p><script type="math/tex; mode=display">L(\pi, \mu, \sigma^2) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \sigma_k^2) + \lambda \left(\sum_k \pi_k - 1 \right) \tag{5}</script><script type="math/tex; mode=display">\begin{cases}    \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2)         = \sum_i        \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\    \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\mu_k}N(x^{(i)}|\mu_k, \sigma_k^2) \\    \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\sigma_k^2}N(x^{(i)}|\mu_k, \sigma_k^2)\end{cases} \tag{6}</script><p>其中</p><script type="math/tex; mode=display">\frac{∂}{∂\mu_k} N(x|\mu_k, \sigma_k^2)= \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{x-\mu_k}{\sigma_k^2}= N(x|\mu_k, \sigma_k^2) · \frac{x-\mu_k}{\sigma_k^2}</script><script type="math/tex; mode=display">\frac{∂}{∂\sigma_k^2} N(x|\mu_k, \sigma_k^2)= \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right)</script><blockquote><p>$\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) = - \frac{\sigma_k^{-3}}{2};　\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) = - \frac{1}{\sigma_k^4}$</p></blockquote><script type="math/tex; mode=display">= \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(- \frac{\sigma_k^{-3}}{2}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \left(- \frac{1}{\sigma_k^4}\right)</script><script type="math/tex; mode=display">= N(x|\mu_k, \sigma_k^2) \left[    \frac{(x - \mu_k)^2}{\sigma_k^2} - 1\right] \frac{1}{2 \sigma_k^2}</script><p>代回$(6)$可以得到</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\    \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{x^{(i)}-\mu_k}{\sigma_k^2} \\    \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \left[    \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1\right] \frac{1}{2 \sigma_k^2}\end{cases} \tag{7}</script><p>令</p><script type="math/tex; mode=display">\gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \tag{8}</script><blockquote><p>通俗理解：$\gamma^{(i)}_k$表示样本$x^{(i)}$中来自类别$w_k$的“贡献百分比”</p></blockquote><ul><li><p>令$\frac{∂}{∂\mu_k} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到</p><script type="math/tex; mode=display">  \sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) = 0  \Rightarrow   \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k}</script></li><li><p>令$\frac{∂}{∂\sigma_k^2} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到</p><script type="math/tex; mode=display">  \sum_i      \gamma^{(i)}_k       \left[          \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1      \right] = 0  \Rightarrow  \sigma_k^2 = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k)^2}{\sum_i \gamma^{(i)}_k}</script></li><li><p>对于$\frac{∂}{∂\pi_k} \log P(X|\pi, \mu, \sigma^2) = 0$，需要做一点处理<br>  两边同乘$\pi_k$，得到</p><script type="math/tex; mode=display">  \sum_i \gamma^{(i)}_k = - \lambda \pi_k \tag{9}</script><p>  然后两边对$k$作累加</p><script type="math/tex; mode=display">  \sum_k \sum_i \gamma^{(i)}_k = - \lambda \sum_k \pi_k</script><blockquote><p>$\sum_k \sum_i \gamma^{(i)}_k = \sum_i \sum_k \gamma^{(i)}_k = N,　\sum_k \pi_k = 1$</p></blockquote><script type="math/tex; mode=display">  N = - \lambda　或　\lambda = -N \tag{10}</script><p>  代回$(9)$，得到</p><script type="math/tex; mode=display">  \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}</script></li></ul><p>综上，我们得到$4$个用于迭代的计算式，将其推广至多维即</p><script type="math/tex; mode=display">\gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)}</script><script type="math/tex; mode=display">\mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k}</script><script type="math/tex; mode=display">\Sigma_k = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)}_k}</script><script type="math/tex; mode=display">\pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}</script><h3 id="用EM算法求解"><a href="#用EM算法求解" class="headerlink" title="用EM算法求解"></a>用<code>EM算法</code>求解</h3><blockquote><p>$Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$</p></blockquote><script type="math/tex; mode=display">Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})\log P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k)</script><ul><li><p>$ M-step $</p><script type="math/tex; mode=display">  P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})  = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)}  = \gamma^{(i)}_k</script><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k)  = P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k)   P(w_k^{(i)}|\mu_k, \Sigma_k)  = \pi_k N(x^{(i)}|\mu_k, \Sigma_k)</script><p>  故</p><script type="math/tex; mode=display">  Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)})   = \sum_i \sum_k \gamma^{(i)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k)</script><p>  通过求解极值可得到与$\underline{暴力求解}$一样的等式，即</p><script type="math/tex; mode=display">  \gamma^{(i)(t)}_k = \frac{\pi^{(t)}_k N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})}</script><script type="math/tex; mode=display">  \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k}</script><script type="math/tex; mode=display">  \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k}</script><script type="math/tex; mode=display">  \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N}</script><p>  伪代码为</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">According to prior knowledge set</span><br><span class="line">    \pi^&#123;(t)&#125;(n_clusters,)</span><br><span class="line">    \mu^&#123;(t)&#125;(n_clusters, n_features)</span><br><span class="line">    \Sigma^&#123;(t)&#125;(n_clusters, n_features, n_features)</span><br><span class="line">Repeat until convergence&#123;</span><br><span class="line">    # E-step: calculate \gamma^&#123;(t)&#125;</span><br><span class="line">        \gamma(n_samples, n_clusters)</span><br><span class="line">    # M-step: update \pi, \mu, \Sigma</span><br><span class="line">        \pi^&#123;(t+1)&#125;(n_clusters,)</span><br><span class="line">        \mu^&#123;(t+1)&#125;(n_clusters, n_features)</span><br><span class="line">        \Sigma^&#123;(t+1)&#125;(n_clusters, n_features, n_features)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>初始点的选择可以随机选择，也可使用<code>K-Means</code></p></blockquote><p>  <code>GMM</code>算法收敛过程如下<br>  <img src="/2018/11/12/EM-GMM/gmm.gif" alt="gmm"></p></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p82_gmm.py" target="_blank" rel="noopener">@Github: GMM</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GMM</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Gaussian Mixture Model</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_clusters &#123;int&#125;</span></span><br><span class="line"><span class="string">        prior &#123;ndarray(n_clusters,)&#125;</span></span><br><span class="line"><span class="string">        mu &#123;ndarray(n_clusters, n_features)&#125;</span></span><br><span class="line"><span class="string">        sigma &#123;ndarray(n_clusters, n_features, n_features)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_clusters)</span>:</span></span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.prior = <span class="keyword">None</span></span><br><span class="line">        self.mu = <span class="keyword">None</span></span><br><span class="line">        self.sigma = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, delta=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">            delta &#123;float&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - Initialize with k-means</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize with k-means</span></span><br><span class="line">        clf = KMeans(n_clusters=self.n_clusters)</span><br><span class="line">        clf.fit(X)</span><br><span class="line">        self.mu = clf.cluster_centers_ </span><br><span class="line">        self.prior = np.zeros(self.n_clusters)</span><br><span class="line">        self.sigma = np.zeros((self.n_clusters, n_features, n_features))</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">            X_ = X[clf.labels_==k]</span><br><span class="line">            self.prior[k] = X_.shape[<span class="number">0</span>] / X_.shape[<span class="number">0</span>]</span><br><span class="line">            self.sigma[k] = np.cov(X_.T)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            mu_ = self.mu.copy()</span><br><span class="line">            <span class="comment"># E-step: updata gamma</span></span><br><span class="line">            gamma = np.zeros((n_samples, self.n_clusters))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                    denominator = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                        post = self.prior[k] *\</span><br><span class="line">                                    multiGaussian(X[i], self.mu[j], self.sigma[j])</span><br><span class="line">                        denominator += post</span><br><span class="line">                        <span class="keyword">if</span> j==k: numerator = post</span><br><span class="line">                    gamma[i, k] = numerator/denominator</span><br><span class="line">            <span class="comment"># M-step: updata prior, mu, sigma</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                sum1 = <span class="number">0</span></span><br><span class="line">                sum2 = <span class="number">0</span></span><br><span class="line">                sum3 = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">                    sum1 += gamma[i, k]</span><br><span class="line">                    sum2 += gamma[i, k] * X[i]</span><br><span class="line">                    x_ = np.reshape(X[i] - self.mu[k], (n_features, <span class="number">1</span>))</span><br><span class="line">                    sum3 += gamma[i, k] * x_.dot(x_.T)</span><br><span class="line">                self.prior[k]  = sum1 / n_samples</span><br><span class="line">                self.mu[k]     = sum2 / sum1</span><br><span class="line">                self.sigma[k]  = sum3 / sum1</span><br><span class="line">            <span class="comment"># to stop</span></span><br><span class="line">            mu_delta = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                mu_delta += nl.norm(self.mu[k] - mu_[k])</span><br><span class="line">            print(mu_delta)</span><br><span class="line">            <span class="keyword">if</span> mu_delta &lt; delta: <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> self.prior, self.mu, self.sigma</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_proba</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            y_pred_proba &#123;ndarray(n_samples, n_clusters)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        y_pred_proba = np.zeros((n_samples, self.n_clusters))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                y_pred_proba[i, k] = self.prior[k] *\</span><br><span class="line">                                multiGaussian(X[i], self.mu[k], self.sigma[k])</span><br><span class="line">        <span class="keyword">return</span> y_pred_proba</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            y_pred_proba &#123;ndarray(n_samples,)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        y_pred_proba = self.predict_proba(X)</span><br><span class="line">        <span class="keyword">return</span> np.argmax(y_pred_proba, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Data Augmentation</title>
      <link href="/2018/11/02/Data-Augmentation/"/>
      <url>/2018/11/02/Data-Augmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>“有时候不是由于算法好赢了。而是由于拥有很多其它的数据才赢了。”</p></blockquote><h1 id="数据集扩增"><a href="#数据集扩增" class="headerlink" title="数据集扩增"></a>数据集扩增</h1><p>在深度学习中,很多训练数据意味着能够用更深的网络，训练出更好的模型。既然这样，收集很多其它的数据不即可啦？假设能够收集很多其它能够用的数据当然好，比如<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>上图像数据量已达到$1400$万张，可是非常多时候，收集很多其它的数据意味着须要耗费很多其它的人力物力，这就需要使用一定的方法扩增数据集。</p><h1 id="图像扩增"><a href="#图像扩增" class="headerlink" title="图像扩增"></a>图像扩增</h1><p>大部分借助<code>OpenCV</code>库，这里推荐一位学长的博客，整理了大量的<code>OpenCV</code>使用方法.</p><blockquote><p><a href="http://ex2tron.wang/" target="_blank" rel="noopener">Ex2tron’s Blog</a></p></blockquote><p><code>TensorFlow</code>也提供相应图像处理方法<br><a href="https://tensorflow.google.cn/api_docs/python/tf/image" target="_blank" rel="noopener">Module: tf.image | TensorFlow </a></p><p>需要注意的是，扩增过程中，需注意图像数据类型，可以将数据归一化到$(0, 1)$间再进行处理</p><h2 id="翻转"><a href="#翻转" class="headerlink" title="翻转"></a>翻转</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flip</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    rand_var = np.random.random()</span><br><span class="line">    image = image[:, ::<span class="number">-1</span>, :] <span class="keyword">if</span> rand_var &gt; <span class="number">0.5</span> <span class="keyword">else</span> image</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(image, degree)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        degree &#123;float&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    (h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line">    center = (w // <span class="number">2</span>, h // <span class="number">2</span>)</span><br><span class="line">    random_angel = np.random.randint(-degree, degree)</span><br><span class="line">    M = cv2.getRotationMatrix2D(center, random_angel, <span class="number">1.0</span>)</span><br><span class="line">    image = cv2.warpAffine(image, M, (w, h))</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="噪声"><a href="#噪声" class="headerlink" title="噪声"></a>噪声</h2><p>可手动实现，如椒盐噪声代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saltnoise</span><span class="params">(image, salt=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">    <span class="string">""" add salt &amp; pepper and gaussian noise</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        salt &#123;float(0, 1)&#125; number of salt pixel = salt*h*w</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        TODO: gaussain noise</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    (h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line">    n_salt = int(salt * h * w)</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(n_salt):</span><br><span class="line">        hr = np.random.randint(<span class="number">0</span>, h)</span><br><span class="line">        wr = np.random.randint(<span class="number">0</span>, w)</span><br><span class="line">        issalt = (np.random.rand(<span class="number">1</span>) &gt; <span class="number">0.5</span>)</span><br><span class="line">        image[hr, wr] = <span class="number">255</span> <span class="keyword">if</span> issalt <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure></p><p>也可调用<code>scikit-image</code>库，需要注意的是，<code>skimage.util.random_noise()</code>会将原图数据转换为$(0, 1)$间的浮点数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">noise</span><span class="params">(image, gaussian, salt, seed=None)</span>:</span></span><br><span class="line">    <span class="string">""" add noise to image TODO</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        gaussian &#123;bool&#125;: </span></span><br><span class="line"><span class="string">        salt &#123;bool&#125;: </span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        Function to add random noise of various types to a floating-point image.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dtype = image.dtype</span><br><span class="line">    <span class="keyword">if</span> gaussian:</span><br><span class="line">        image = skimage.util.random_noise(image, mode=<span class="string">'gaussian'</span>, seed=seed)</span><br><span class="line">    <span class="keyword">if</span> salt:</span><br><span class="line">        image = skimage.util.random_noise(image, mode=<span class="string">'s&amp;p'</span>, seed=seed)</span><br><span class="line"></span><br><span class="line">    image = (image * <span class="number">255</span>).astype(dtype)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure></p><h2 id="亮度与对比度调整"><a href="#亮度与对比度调整" class="headerlink" title="亮度与对比度调整"></a>亮度与对比度调整</h2><p>考虑到数据溢出，先转换为整形数据，再限制其值到$[0, 255]$</p><blockquote><p>注意数据类型</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brightcontrast</span><span class="params">(image, brtadj=<span class="number">0</span>, cstadj=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">""" adjust bright and contrast value</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        brtadj &#123;int&#125;    if true, adjust bright</span></span><br><span class="line"><span class="string">        cstadj &#123;float&#125;  if true, adjust contrast</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dtype = image.dtype</span><br><span class="line">    image = image.astype(<span class="string">'int'</span>)*cstadj + brtadj</span><br><span class="line">    image = np.clip(image, <span class="number">0</span>, <span class="number">255</span>).astype(dtype)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="投射变换"><a href="#投射变换" class="headerlink" title="投射变换"></a>投射变换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perspective</span><span class="params">(image, prop)</span>:</span></span><br><span class="line">    <span class="string">""" 透射变换</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        prop &#123;float&#125;: 在四个顶点多大的方格内选取新顶点，方格大小为(H*prop, W*prop)</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        在四个顶点周围随机选取新的点进行仿射变换，四个点对应左上、右上、左下、右下</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    (h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    ptsrc = np.zeros(shape=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line">    ptdst = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, w], [h, <span class="number">0</span>], [h, w]])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        hr = np.random.randint(<span class="number">0</span>, int(h*prop))</span><br><span class="line">        wr = np.random.randint(<span class="number">0</span>, int(w*prop))</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            ptsrc[i] = np.array([hr, wr])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">            ptsrc[i] = np.array([hr, w - wr])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">2</span>:</span><br><span class="line">            ptsrc[i] = np.array([h - hr, wr])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">3</span>:</span><br><span class="line">            ptsrc[i] = np.array([h - hr, w - wr])</span><br><span class="line">    M = cv2.getPerspectiveTransform(ptsrc.astype(<span class="string">'float32'</span>), ptdst.astype(<span class="string">'float32'</span>))</span><br><span class="line">    image = cv2.warpPerspective(image, M, (w, h))</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>二次入坑raspberry-pi</title>
      <link href="/2018/10/29/%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi/"/>
      <url>/2018/10/29/%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>距上一次搭建树莓派平台已经两年了，保存的镜像出了问题，重新搭建一下。</p><h1 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>从官网下载树莓派系统镜像，有以下几种可选</p><blockquote><p><a href="https://www.raspberrypi.org/" target="_blank" rel="noopener">Raspberry Pi — Teach, Learn, and Make with Raspberry Pi </a></p><ol><li>Raspbian &amp; Raspbian Lite，基于Debian</li><li>Noobs &amp; Noobs Lite</li><li>Ubuntu MATE</li><li>Snappy Ubuntu Core</li><li>Windows 10 IOT</li></ol></blockquote><p><del>其余不太了解，之前安装的是Raspbian，对于Debian各种不适，换上界面优雅的Ubuntu Mate玩一下</del><br>老老实实玩Raspbian，笑脸:-)</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>比较简单，准备micro-SD卡，用Win32 Disk Imager烧写镜像</p><blockquote><p><a href="https://sourceforge.net/projects/win32diskimager/" target="_blank" rel="noopener">Win32 Disk Imager download | SourceForge.net</a></p><p><img src="/2018/10/29/二次入坑raspberry-pi/Win32DiskImager.jpg" alt="Win32DiskImager"></p></blockquote><p>安装完软件后可点击<code>Read</code>备份自己的镜像。</p><p>注意第二次开机前需要配置<code>config.txt</code>文件，否则<code>hdmi</code>无法显示</p><blockquote><p><a href="http://shumeipai.nxez.com/2015/11/23/raspberry-pi-configuration-file-config-txt-nstructions.html" target="_blank" rel="noopener">树莓派配置文档 config.txt 说明 | 树莓派实验室</a></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">disable_overscan=1 </span><br><span class="line">hdmi_force_hotplug=1</span><br><span class="line">hdmi_group=2    # DMT</span><br><span class="line">hdmi_mode=32    # 1280x960</span><br><span class="line">hdmi_drive=2</span><br><span class="line">config_hdmi_boost=4</span><br></pre></td></tr></table></figure><h2 id="修改交换分区"><a href="#修改交换分区" class="headerlink" title="修改交换分区"></a>修改交换分区</h2><h3 id="Ubuntu-Mate"><a href="#Ubuntu-Mate" class="headerlink" title="Ubuntu Mate"></a>Ubuntu Mate</h3><p>查看交换分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>未设置时如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:            0        0        0</span><br></pre></td></tr></table></figure></p><p>创建和挂载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 获取权限</span><br><span class="line">$ sudo -i</span><br><span class="line"></span><br><span class="line"># 创建目录</span><br><span class="line">$ mkdir /swap</span><br><span class="line">$ cd /swap</span><br><span class="line"></span><br><span class="line"># 指定一个大小为1G的名为“swap”的交换文件</span><br><span class="line">$ dd if=/dev/zero of=swap bs=1M count=1k</span><br><span class="line"># 创建交换文件</span><br><span class="line">$ mkswap swap</span><br><span class="line"># 挂载交换分区</span><br><span class="line">$ swapon swap</span><br><span class="line"></span><br><span class="line"># 卸载交换分区</span><br><span class="line"># $ swapoff swap</span><br></pre></td></tr></table></figure></p><p>查看交换分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>未设置时如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:         1023        0     1023</span><br></pre></td></tr></table></figure></p><h3 id="Raspbian"><a href="#Raspbian" class="headerlink" title="Raspbian"></a>Raspbian</h3><p>We will change the configuration in the file <code>/etc/dphys-swapfile</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nano /etc/dphys-swapfile</span><br></pre></td></tr></table></figure></p><p>The default value in Raspbian is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONF_SWAPSIZE=100</span><br></pre></td></tr></table></figure></p><p>We will need to change this to:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONF_SWAPSIZE=1024</span><br></pre></td></tr></table></figure></p><p>Then you will need to stop and start the service that manages the swapfile own Rasbian:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /etc/init.d/dphys-swapfile stop</span><br><span class="line">$ sudo /etc/init.d/dphys-swapfile start</span><br></pre></td></tr></table></figure></p><p>You can then verify the amount of memory + swap by issuing the following command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>The output should look like:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:         1023        0     1023</span><br></pre></td></tr></table></figure></p><h1 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h1><h2 id="安装指令"><a href="#安装指令" class="headerlink" title="安装指令"></a>安装指令</h2><ul><li><p><code>apt-get</code></p><ul><li>安装软件<br><code>apt-get install softname1 softname2 softname3 ...</code></li><li>卸载软件<br><code>apt-get remove softname1 softname2 softname3 ...</code></li><li>卸载并清除配置<br><code>apt-get remove --purge softname1</code></li><li>更新软件信息数据库<br><code>apt-get update</code></li><li>进行系统升级<br><code>apt-get upgrade</code></li><li>搜索软件包<br><code>apt-cache search softname1 softname2 softname3 ...</code></li><li>修正（依赖关系）安装：<br><code>apt-get -f insta</code></li></ul></li><li><p><code>dpkg</code></p><ul><li>安装<code>.deb</code>软件包<br><code>dpkg -i xxx.deb</code></li><li>删除软件包<br><code>dpkg -r xxx.deb</code></li><li>连同配置文件一起删除<br><code>dpkg -r --purge xxx.deb</code></li><li>查看软件包信息<br><code>dpkg -info xxx.deb</code></li><li>查看文件拷贝详情<br><code>dpkg -L xxx.deb</code></li><li>查看系统中已安装软件包信息<br><code>dpkg -l</code></li><li><p>重新配置软件包<br><code>dpkg-reconfigure xx</code></p></li><li><p>卸载软件包及其配置文件，但无法解决依赖关系！<br><code>sudo dpkg -p package_name</code></p></li><li>卸载软件包及其配置文件与依赖关系包<br><code>sudo aptitude purge pkgname</code></li><li>清除所有已删除包的残馀配置文件<br><code>dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P</code></li></ul></li></ul><h2 id="软件源"><a href="#软件源" class="headerlink" title="软件源"></a>软件源</h2><ol><li><p>备份原始文件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup</span><br></pre></td></tr></table></figure></li><li><p>修改文件并添加国内源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/apt/sources.list</span><br></pre></td></tr></table></figure></li><li><p>注释元文件内的源并添加如下地址</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#Mirror.lupaworld.com 源更新服务器（浙江省杭州市双线服务器，网通同电信都可以用，亚洲地区官方更新服务器）：</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#Ubuntu 官方源 </span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><p> 或者</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#阿里云</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#网易163</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure></li><li><p>放置非官方源的包不完整，可在为不添加官方源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deb http://archive.ubuntu.org.cn/ubuntu-cn/ feisty main restricted universe multiverse</span><br></pre></td></tr></table></figure></li><li><p>更新源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></li><li><p>更新软件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get dist-upgrade</span><br></pre></td></tr></table></figure></li><li><p>常见的修复安装命令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get -f install</span><br></pre></td></tr></table></figure></li></ol><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>主要是<code>Python</code>和相关依赖包的安装，使用以下指令可导出已安装的依赖包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure></p><p>并使用指令安装到树莓派<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install -r requirements.txt</span><br></pre></td></tr></table></figure></p><p>注意<code>pip</code>更新<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install --upgrade pip</span><br></pre></td></tr></table></figure></p><p>最新版本会报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: cannot import name main</span><br></pre></td></tr></table></figure></p><p>修改文件<code>/usr/bin/pip</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pip <span class="keyword">import</span> main</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sys.exit(main())</span><br></pre></td></tr></table></figure></p><p>改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pip <span class="keyword">import</span> __main__</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sys.exit(__main__._main())</span><br></pre></td></tr></table></figure></p><hr><p><del>成功!!!</del><br>失败了，笑脸:-)，手动安装吧。。。</p><ul><li><p>部分包可使用<code>pip3</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install numpy</span><br><span class="line">$ pip3 install pandas</span><br><span class="line">$ pip3 install sklearn</span><br></pre></td></tr></table></figure><blockquote><p>若需要权限，加入<code>--user</code></p></blockquote></li><li><p>部分包用<code>apt-get</code>，但是优先安装到<code>Python2.7</code>版本，笑脸:-)</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install python-scipy</span><br><span class="line">$ sudo apt-get install python-matplotlib</span><br><span class="line">$ sudo apt-get install python-opencv</span><br></pre></td></tr></table></figure></li><li><p>部分从<code>PIPY</code>下载<code>.whl</code>或<code>.tar.gz</code>文件</p><blockquote><p><a href="https://pypi.org/" target="_blank" rel="noopener">PyPI – the Python Package Index · PyPI</a></p><ul><li>tensorboardX-1.4-py2.py3-none-any.whl</li><li>visdom-0.1.8.5.tar.gz</li></ul></blockquote><p>  安装指令为</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install xxx.whl</span><br></pre></td></tr></table></figure>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf xxx.tar.gz</span><br><span class="line">$ python setup.py install</span><br></pre></td></tr></table></figure></li><li><p><code>Pytorch</code>源码安装</p><blockquote><p><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration </a></p></blockquote><p>  安装方法<a href="https://github.com/pytorch/pytorch#from-source" target="_blank" rel="noopener">Installation - From Source</a></p><p>  需要用到<code>miniconda</code>，安装方法如下，注意中间回车按慢一点，有两次输入。。。。。(行我慢慢看条款不行么。。笑脸:-))</p><ul><li>第一次是是否同意条款，<code>yes</code></li><li><p>第二次是添加到环境变量，<code>yes</code>，否则自己修改<code>/home/pi/.bashrc</code>添加到环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh</span><br><span class="line">$ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5</span><br><span class="line">$ sudo /bin/bash Miniconda3-latest-Linux-armv7l.sh </span><br><span class="line"># -&gt; change default directory to /home/pi/miniconda3</span><br><span class="line">$ sudo nano /home/pi/.bashrc </span><br><span class="line"># -&gt; add: export PATH=&quot;/home/pi/miniconda3/bin:$PATH&quot;</span><br><span class="line">$ sudo reboot -h now</span><br><span class="line"></span><br><span class="line">$ conda </span><br><span class="line">$ python --version</span><br><span class="line">$ sudo chown -R pi miniconda3</span><br></pre></td></tr></table></figure><p><del>然后就可以安装了</del>没有对应版本的<code>mkl</code>，笑脸:-)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; # [anaconda root directory]</span><br><span class="line"></span><br><span class="line"># Disable CUDA</span><br><span class="line">export NO_CUDA=1</span><br><span class="line"></span><br><span class="line"># Install basic dependencies</span><br><span class="line">conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing</span><br><span class="line">conda install -c mingfeima mkldnn</span><br><span class="line"></span><br><span class="line"># Install Pytorch</span><br><span class="line">git clone --recursive https://github.com/pytorch/pytorch</span><br><span class="line">cd pytorch</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>tensorflow</code><br>  安装tensorflow需要的一些依赖和工具</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line"></span><br><span class="line"># For Python 2.7</span><br><span class="line">$ sudo apt-get install python-pip python-dev</span><br><span class="line"></span><br><span class="line"># For Python 3.3+</span><br><span class="line">$ sudo apt-get install python3-pip python3-dev</span><br></pre></td></tr></table></figure><p>  安装<code>tensorflow</code></p><blockquote><p>若下载失败，手动打开下面网页下载<code>.whl</code>包</p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># For Python 2.7</span><br><span class="line">$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp27-none-linux_armv7l.whl</span><br><span class="line">$ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl</span><br><span class="line"></span><br><span class="line"># For Python 3.4</span><br><span class="line">$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl</span><br><span class="line">$ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl</span><br></pre></td></tr></table></figure><p>  卸载，重装mock</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># For Python 2.7</span><br><span class="line">$ sudo pip uninstall mock</span><br><span class="line">$ sudo pip install mock</span><br><span class="line"></span><br><span class="line"># For Python 3.3+</span><br><span class="line">$ sudo pip3 uninstall mock</span><br><span class="line">$ sudo pip3 install mock</span><br></pre></td></tr></table></figure><p>  安装的版本<code>tensorflow v1.1.0</code>没有<code>models</code>，因为1.0版本以后models就被<code>Sam Abrahams</code>独立出来了，例如<code>classify_image.py</code>就在<code>models/tutorials/image/imagenet/</code>里</p><blockquote><p><a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">tensorflow/models</a></p></blockquote></li></ul><h2 id="其余"><a href="#其余" class="headerlink" title="其余"></a>其余</h2><ol><li><p>输入法 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install fcitx fcitx-googlepinyin </span><br><span class="line">$ fcitx-module-cloudpinyin fcitx-sunpinyin</span><br></pre></td></tr></table></figure></li><li><p><code>git</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install git</span><br></pre></td></tr></table></figure><p>配置<code>git</code>和<code>ssh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name &quot;Louis Hsu&quot;</span><br><span class="line">$ git config --global user.email is.louishsu@foxmail.com</span><br><span class="line"></span><br><span class="line">$ ssh-keygen -t rsa -C &quot;is.louishsu@foxmail.com&quot;</span><br><span class="line">$ cat ~/.ssh/id_rsa.pub  # 添加到github</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Underfitting &amp; Overfitting</title>
      <link href="/2018/10/26/Underfitting-Overfitting/"/>
      <url>/2018/10/26/Underfitting-Overfitting/</url>
      
        <content type="html"><![CDATA[<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>放上一张非常经典的图，以下分别表示二分类模型中的欠拟合(underfit)、恰好(just right)、过拟合(overfit)，来自吴恩达课程笔记。<br><img src="/2018/10/26/Underfitting-Overfitting/underfit_justright_overfit.png" alt="underfit_justright_overfit"></p><ul><li>欠拟合的成因大多是模型不够复杂、拟合函数的能力不够；</li><li>过拟合成因是给定的数据集相对过于简单，使得模型在拟合函数时过分地考虑了噪声等不必要的数据间的关联，或者说相对于给定数据集，模型过于复杂、拟合能力过强。</li></ul><h1 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h1><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>可通过学习曲线<code>(Learning curve)</code>进行欠拟合与过拟合的判别。</p><p>学习曲线就是通过画出<strong>不同训练集大小</strong>时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。</p><h2 id="绘制"><a href="#绘制" class="headerlink" title="绘制"></a>绘制</h2><p>横轴为训练样本的数量，纵轴为损失或其他<a href="">评估准则</a>。<br><code>sklearn</code>中学习曲线绘制例程如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"></span><br><span class="line">digits = load_digits(); X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">100</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">estimator = GaussianNB()</span><br><span class="line">train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=<span class="number">4</span>, train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">"Learning Curves (Naive Bayes)"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Training examples"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Score"</span>)</span><br><span class="line"></span><br><span class="line">train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.fill_between(train_sizes, </span><br><span class="line">                train_scores_mean - train_scores_std,</span><br><span class="line">                train_scores_mean + train_scores_std,</span><br><span class="line">                alpha=<span class="number">0.1</span>, color=<span class="string">"r"</span>)</span><br><span class="line">plt.fill_between(train_sizes,</span><br><span class="line">                test_scores_mean - test_scores_std,</span><br><span class="line">                test_scores_mean + test_scores_std,</span><br><span class="line">                alpha=<span class="number">0.1</span>, color=<span class="string">"g"</span>)</span><br><span class="line">plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>, label=<span class="string">"Training score"</span>)</span><br><span class="line">plt.plot(train_sizes, test_scores_mean,  <span class="string">'o-'</span>, color=<span class="string">"g"</span>, label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line"></span><br><span class="line">plt.grid(); plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/2018/10/26/Underfitting-Overfitting/learning_curve_nb.png" alt="learning_curve_nb"></p><h2 id="判别"><a href="#判别" class="headerlink" title="判别"></a>判别</h2><ul><li><strong>欠拟合</strong>，即高偏差<code>(high bias)</code>，训练集和测试集的误差收敛但却很高；</li><li><strong>过拟合</strong>，即高方差<code>(high variance)</code>，训练集和测试集的误差之间有大的差距。</li></ul><p><img src="/2018/10/26/Underfitting-Overfitting/learning_curve.png" alt="learning_curve"></p><h1 id="欠拟合解决方法"><a href="#欠拟合解决方法" class="headerlink" title="欠拟合解决方法"></a>欠拟合解决方法</h1><ul><li>增加迭代次数继续训练</li><li>增加模型复杂度</li><li>增加特征</li><li>减少正则化程度</li><li>采用Boosting等集成方法</li></ul><p>此时增加数据集并不能改善欠拟合问题。</p><h1 id="过拟合解决方法"><a href="#过拟合解决方法" class="headerlink" title="过拟合解决方法"></a>过拟合解决方法</h1><ul><li>提前停止训练</li><li>获取更多样本或数据扩增<ul><li>重采样</li><li>上采样</li><li>增加随机噪声</li><li><code>GAN</code></li><li>图像数据的空间变换（平移旋转镜像）</li><li>尺度变换（缩放裁剪）</li><li>颜色变换</li><li>改变分辨率</li><li>对比度</li><li>亮度</li></ul></li><li>降低模型复杂度</li><li>减少特征</li><li>增加正则化程度</li><li>神经网络可采用<code>Dropout</code></li><li>多模型投票方法</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Cross Validation &amp; Hyperparameter</title>
      <link href="/2018/10/26/Cross-Validation-Hyperparameter/"/>
      <url>/2018/10/26/Cross-Validation-Hyperparameter/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉验证与超参数选择"><a href="#交叉验证与超参数选择" class="headerlink" title="交叉验证与超参数选择"></a>交叉验证与超参数选择</h1><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>以下简称交叉验证<code>(Cross Validation)</code>为<code>CV</code>.<code>CV</code>是用来验证分类器的性能一种统计分析方法,基本思想是把在某种意义下将原始数据<code>(dataset)</code>进行分组,一部分做为训练集<code>(train set)</code>,另一部分做为验证集<code>(validation set)</code>,首先用训练集对分类器进行训练,在利用验证集来测试训练得到的模型<code>(model)</code>,以此来做为评价分类器的性能指标。</p><h3 id="交叉验证的几种方法"><a href="#交叉验证的几种方法" class="headerlink" title="交叉验证的几种方法"></a>交叉验证的几种方法</h3><ul><li><p>k折交叉验证(K-fold)</p><ol><li>将全部训练集$S$分成$k$个不相交的子集，假设$S$中的训练样例个数为$m$，则每个子集中有$(\frac{m}{k})$个训练样例，相应子集称作$\{s_1, s_2, …, s_k\}$；</li><li>每次从分好的子集中，拿出$1$个作为测试集，其他$k-1$个作为训练集；</li><li>在$k-1$个训练集上训练出学习器模型，将模型放到测试集上，得到分类率；</li><li>计算k次求得的分类率平均值，作为该模型或者假设函数的真实分类率<br><img src="/2018/10/26/Cross-Validation-Hyperparameter/k-fold.jpg" alt="k-fold"></li></ol></li><li><p>留一法交叉验证(Leave One Out - LOO)<br>  假设有$N$个样本，将每个样本作为测试样本，其他$(N-1)$个样本作为训练样本。这样得到$N$个分类器，$N$个测试结果。用这$N$个结果的平均值衡量模型的性能。</p></li><li><p>留P法交叉验证(Leave P Out - LPO)<br>  将$P$个样本作为测试样本，其他$(N-P)$个样本作为训练样本。这样得到$\left(\begin{matrix}</p><pre><code>  P \\ N</code></pre><p>  \end{matrix}\right)$个训练测试对。当$P＞1$时，测试集会发生重叠。当$P=1$时，变成$LOO$。<br>  <img src="/2018/10/26/Cross-Validation-Hyperparameter/LPO.jpg" alt="LPO"></p></li></ul><h3 id="scikit-learn中的交叉验证"><a href="#scikit-learn中的交叉验证" class="headerlink" title="scikit-learn中的交叉验证"></a><code>scikit-learn</code>中的交叉验证</h3><p><img src="/2018/10/26/Cross-Validation-Hyperparameter/cross_validation_sklearn.png" alt="cross_validation_sklearn"></p><ul><li><p>K-fold</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.model_selection import KFold</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = ["a", "b", "c", "d"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kf = KFold(n_splits=2)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for train, test in kf.split(X):</span><br><span class="line">... print("%s %s" % (train, test))</span><br><span class="line">[2 3] [0 1]</span><br><span class="line">[0 1] [2 3]</span><br></pre></td></tr></table></figure></li><li><p>Leave One Out (LOO)</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.model_selection import LeaveOneOut</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = [1, 2, 3, 4]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; loo = LeaveOneOut()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for train, test in loo.split(X):</span><br><span class="line">... print("%s %s" % (train, test))</span><br><span class="line">[1 2 3] [0]</span><br><span class="line">[0 2 3] [1]</span><br><span class="line">[0 1 3] [2]</span><br><span class="line">[0 1 2] [3]</span><br></pre></td></tr></table></figure></li><li><p>Leave P Out (LPO)</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.model_selection import LeavePOut</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = np.ones(4)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; lpo = LeavePOut(p=2)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for train, test in lpo.split(X):</span><br><span class="line">... print("%s %s" % (train, test))</span><br><span class="line">[2 3] [0 1]</span><br><span class="line">[1 3] [0 2]</span><br><span class="line">[1 2] [0 3]</span><br><span class="line">[0 3] [1 2]</span><br><span class="line">[0 2] [1 3]</span><br><span class="line">[0 1] [2 3]</span><br></pre></td></tr></table></figure></li></ul><h2 id="使用交叉验证调整超参数"><a href="#使用交叉验证调整超参数" class="headerlink" title="使用交叉验证调整超参数"></a>使用交叉验证调整超参数</h2><p>超参数的定义：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。<br>超参数例如</p><ul><li>模型（<code>SVM</code>，<code>Softmax</code>，<code>Multi-layer Neural Network</code>,…)；</li><li>迭代算法（<code>Adam</code>, <code>SGD</code>, …)(不同的迭代算法还有各种不同的超参数，如<code>beta1</code>,<code>beta2</code>等等，但常见的做法是使用默认值，不进行调参）；</li><li>学习率（<code>learning rate</code>)；</li><li>正则化方程的选择(<code>L0</code>,<code>L1</code>,<code>L2</code>)，正则化系数；</li><li><code>dropout</code>的概率</li><li>…</li></ul><h3 id="确定调节范围"><a href="#确定调节范围" class="headerlink" title="确定调节范围"></a>确定调节范围</h3><p>超参数的种类多，调节范围大，需要先进行简单的测试确定调参范围。</p><ul><li><p>模型选择<br>  模型的选择很大程度上取决于具体的实际问题，但必须通过几项基本测试。 </p><ul><li>可以通过第一个epoch的loss，观察模型能否无BUG运行，注意此过程需要设置正则项系数为0，因为正则项引入的loss难以估算。 </li><li>模型必须可以对于小数据集过拟合，否则应该尝试其他或者更复杂的模型。</li><li><p>若训练集与验证集loss均较大，则应该尝试其他或者更复杂的模型。</p><blockquote><p>模型选择的方法为：</p><ol><li>使用训练集训练出 10 个模型</li><li>用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li><li>选取代价函数值最小的模型</li><li>用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）<p align="right"> —— Andrew Ng, Stanford University </p></li></ol></blockquote></li></ul></li><li><p>学习率</p><ul><li>loss基本不变：学习率过低 </li><li>loss波动明显或者溢出：学习率过高 </li></ul></li><li><p>正则项系数</p><ul><li>val_acc与acc相差较大：正则项系数过小 </li><li>loss逐渐增大：正则项系数过大 </li></ul></li></ul><h3 id="超参数的确定"><a href="#超参数的确定" class="headerlink" title="超参数的确定"></a>超参数的确定</h3><ul><li><p>先粗调，再细调<br> 先通过数量少，间距大的粗调确定细调的大致范围。然后在小范围内部进行间距小，数量大的细调。</p></li><li><p>尝试在对数空间内进行调节<br>  即在对数空间内部随机生成测试参数，而不是在原空间生成，通常用于学习率以及正则项系数等的调节。出发点是该超参数的指数项对于模型的结果影响更显著；而同阶的数据之间即便原域相差较大，对于模型结果的影响反而不如不同阶的数据差距大。</p></li><li><p>超参数搜索<br>  随机搜索参数值，而不是网格搜索。</p></li></ul><h3 id="超参数搜索"><a href="#超参数搜索" class="headerlink" title="超参数搜索"></a>超参数搜索</h3><p><code>scikit-learn</code>提供超参数搜索方法，可参考官方文档</p><ul><li>网格搜索<br>  <a href="http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#exhaustive-grid-search" target="_blank" rel="noopener">3.2.1. Exhaustive Grid Search</a><br>  调用例程如下  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># get some data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># build a classifier</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function to report best scores</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span><span class="params">(results, n_top=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_top + <span class="number">1</span>):</span><br><span class="line">        candidates = np.flatnonzero(results[<span class="string">'rank_test_score'</span>] == i)</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            print(<span class="string">"Model with rank: &#123;0&#125;"</span>.format(i))</span><br><span class="line">            print(<span class="string">"Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)"</span>.format(</span><br><span class="line">                results[<span class="string">'mean_test_score'</span>][candidate],</span><br><span class="line">                results[<span class="string">'std_test_score'</span>][candidate]))</span><br><span class="line">            print(<span class="string">"Parameters: &#123;0&#125;"</span>.format(results[<span class="string">'params'</span>][candidate]))</span><br><span class="line">            print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use a full grid over all parameters</span></span><br><span class="line">param_grid = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="keyword">None</span>],</span><br><span class="line">            <span class="string">"max_features"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">            <span class="string">"min_samples_split"</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">            <span class="string">"min_samples_leaf"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">            <span class="string">"bootstrap"</span>: [<span class="keyword">True</span>, <span class="keyword">False</span>],</span><br><span class="line">            <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run grid search</span></span><br><span class="line">grid_search = GridSearchCV(clf, param_grid=param_grid)</span><br><span class="line">start = time()</span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"GridSearchCV took %.2f seconds for %d candidate parameter settings."</span></span><br><span class="line">    % (time() - start, len(grid_search.cv_results_[<span class="string">'params'</span>])))</span><br><span class="line">report(grid_search.cv_results_)</span><br></pre></td></tr></table></figure></li></ul><ul><li>随机搜索<br>  <a href="http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#randomized-parameter-optimization" target="_blank" rel="noopener">3.2.2. Randomized Parameter Optimization</a><br>  调用例程如下  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># get some data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># build a classifier</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function to report best scores</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span><span class="params">(results, n_top=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_top + <span class="number">1</span>):</span><br><span class="line">        candidates = np.flatnonzero(results[<span class="string">'rank_test_score'</span>] == i)</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            print(<span class="string">"Model with rank: &#123;0&#125;"</span>.format(i))</span><br><span class="line">            print(<span class="string">"Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)"</span>.format(</span><br><span class="line">                results[<span class="string">'mean_test_score'</span>][candidate],</span><br><span class="line">                results[<span class="string">'std_test_score'</span>][candidate]))</span><br><span class="line">            print(<span class="string">"Parameters: &#123;0&#125;"</span>.format(results[<span class="string">'params'</span>][candidate]))</span><br><span class="line">            print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># specify parameters and distributions to sample from</span></span><br><span class="line">param_dist = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="keyword">None</span>],</span><br><span class="line">            <span class="string">"max_features"</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">            <span class="string">"min_samples_split"</span>: sp_randint(<span class="number">2</span>, <span class="number">11</span>),</span><br><span class="line">            <span class="string">"min_samples_leaf"</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">            <span class="string">"bootstrap"</span>: [<span class="keyword">True</span>, <span class="keyword">False</span>],</span><br><span class="line">            <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run randomized search</span></span><br><span class="line">n_iter_search = <span class="number">20</span></span><br><span class="line">random_search = RandomizedSearchCV(clf, param_distributions=param_dist,</span><br><span class="line">                                n_iter=n_iter_search)</span><br><span class="line"></span><br><span class="line">start = time()</span><br><span class="line">random_search.fit(X, y)</span><br><span class="line">print(<span class="string">"RandomizedSearchCV took %.2f seconds for %d candidates"</span></span><br><span class="line">    <span class="string">" parameter settings."</span> % ((time() - start), n_iter_search))</span><br><span class="line">report(random_search.cv_results_)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spam Classification</title>
      <link href="/2018/10/26/Spam-Classification/"/>
      <url>/2018/10/26/Spam-Classification/</url>
      
        <content type="html"><![CDATA[<blockquote><p>踩坑？？？全部给我踩平！！！</p></blockquote><p>来自<a href="https://www.lintcode.com/ai/spam-message-classification/overview" target="_blank" rel="noopener">LintCode垃圾短信分类</a><br><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/ZhaoHaitao%2C%20ECUST/spam%20or%20ham" target="_blank" rel="noopener">@Github: spam or ham</a></p><h1 id="垒代码"><a href="#垒代码" class="headerlink" title="垒代码"></a>垒代码</h1><h2 id="预处理及向量化"><a href="#预处理及向量化" class="headerlink" title="预处理及向量化"></a>预处理及向量化</h2><p>观察各文本后，发现各文本中包含的单词多种多样，包含标点、数字等，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</span><br><span class="line">- XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http://wap. </span><br><span class="line">- 07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder.</span><br></pre></td></tr></table></figure></p><p>且按空格分词后，部分单词中仍包含<code>whitespace</code>，故选择的预处理方案是，<strong>去除分词后文本中的标点、数字、空格等，并将单词中字母全部转为小写</strong>。</p><blockquote><p>中文分词可采用<code>jieba</code>(街霸？)</p></blockquote><p>预处理后，按当前的文本内容建立字典，并统计各样本的词数向量，详细代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Words2Vector</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    建立字典，将输入的词列表转换为向量，表示各词出现的次数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.dict = <span class="keyword">None</span></span><br><span class="line">        self.n_word = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        self.fit(words)</span><br><span class="line">        <span class="keyword">return</span> self.transform(words)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        words = _flatten(words)                                                 <span class="comment"># 展开为1维列表</span></span><br><span class="line">        words = self.filt(words)                                                <span class="comment"># 滤除空格、数字、标点</span></span><br><span class="line"></span><br><span class="line">        self.word = list(set(words))                                            <span class="comment"># 去重</span></span><br><span class="line">        self.n_word = len(set(words))                                           <span class="comment"># 统计词的个数</span></span><br><span class="line">        self.dict = dict(zip(self.word, [_ <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.n_word)]))       <span class="comment"># 各词在字典中的位置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">        @return &#123;ndarray&#125; retarray: vector</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        retarray = np.zeros(shape=(len(words), self.n_word))                    <span class="comment"># 返回的词数向量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</span><br><span class="line">            words[i] = self.filt(words[i])                                      <span class="comment"># 滤除空格、数字、标点</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> words[i]:</span><br><span class="line">                <span class="keyword">if</span> w <span class="keyword">in</span> self.word:                                              <span class="comment"># 是否在训练集生成的字典中</span></span><br><span class="line">                    retarray[i, self.dict[w]] += <span class="number">1</span>                              <span class="comment"># 查询字典，找到对应特征的下标</span></span><br><span class="line">        <span class="keyword">return</span> retarray</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filt</span><span class="params">(self, flattenWords)</span>:</span></span><br><span class="line">        retWords = []</span><br><span class="line">        en_stops = set(stopwords.words(<span class="string">'english'</span>))                              <span class="comment"># 停用词列表</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> flattenWords:</span><br><span class="line">            word = word.translate(str.maketrans(<span class="string">''</span>, <span class="string">''</span>, string.whitespace))     <span class="comment"># 去除空白</span></span><br><span class="line">            word = word.translate(str.maketrans(<span class="string">''</span>, <span class="string">''</span>, string.punctuation))    <span class="comment"># 去除标点</span></span><br><span class="line">            word = word.translate(str.maketrans(<span class="string">''</span>, <span class="string">''</span>, string.digits))         <span class="comment"># 去除数字</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> en_stops <span class="keyword">and</span> (len(word) &gt; <span class="number">1</span>):                        <span class="comment"># 删除停用词，并除去长度小于等于2的词</span></span><br><span class="line">                retWords.append(word.lower())</span><br><span class="line">        <span class="keyword">return</span> retWords</span><br></pre></td></tr></table></figure></p><h2 id="TF-IDF方法"><a href="#TF-IDF方法" class="headerlink" title="TF-IDF方法"></a>TF-IDF方法</h2><p>由词数向量可计算词频，但只用词频忽略了各文本在不同文档中的重要程度，关于<code>TF-IDF</code>，在<a href="https://louishsu.xyz/2018/10/25/TF-IDF/" target="_blank" rel="noopener">另一篇博文</a>中详细说明。</p><p>由于剔除了停用词等，部分向量不包含任何内容，即词数向量为$\vec{0}$，这时计算词频和单位化时，会出现<code>nan</code>的运算结果，故只对非空向量进行计算。</p><p>训练后需要保存的是<code>IDF</code>向量，<code>TF</code>向量在新样本输入后重新计算，故无需保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TfidfVectorizer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.idf = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, num_vec)</span>:</span></span><br><span class="line">        self.fit(num_vec)</span><br><span class="line">        <span class="keyword">return</span> self.transform(num_vec)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, num_vec)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_vec[num_vec&gt;<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        n_doc = num_vec.shape[<span class="number">0</span>]</span><br><span class="line">        n_term = np.sum(num_vec, axis=<span class="number">0</span>)    <span class="comment"># 各词出现过的文档次数</span></span><br><span class="line">        self.idf = np.log((n_doc + <span class="number">1</span>) / (n_term + <span class="number">1</span>)) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.idf</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, num_vec)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 求解词频向量，由于部分向量为空，故下句会出现问题</span></span><br><span class="line">        <span class="comment"># tf = num_vec / np.sum(num_vec, axis=1).reshape(-1, 1) =&gt; nan</span></span><br><span class="line">        <span class="comment"># 解决方法：只对非空向量进行词频计算</span></span><br><span class="line">        tf = np.zeros(shape=num_vec.shape)</span><br><span class="line">        n_terms = np.sum(num_vec, axis=<span class="number">1</span>); idx = (n_terms!=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        tf[idx] = num_vec[idx] / n_terms[idx].reshape(<span class="number">-1</span>, <span class="number">1</span>)            <span class="comment"># 计算词频，只对非空向量进行</span></span><br><span class="line">        </span><br><span class="line">        tfidf = tf * self.idf</span><br><span class="line">        tfidf[idx] /= np.linalg.norm(tfidf, axis=<span class="number">1</span>)[idx].reshape(<span class="number">-1</span>, <span class="number">1</span>) <span class="comment"># 单位化，只对非空向量进行</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> tfidf</span><br></pre></td></tr></table></figure><h2 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h2><p>各文本向量化后，就可通过机器学习算法进行模型的训练和预测，这里采用的是贝叶斯决策的方法，需要注意的有以下几点</p><ul><li>似然函数$p(x|c_k)$与<a href="https://louishsu.xyz/2018/10/18/Bayes-Decision/" target="_blank" rel="noopener">贝叶斯决策</a>文中例不同，这里宜采用高斯分布作为分布模型；</li><li><p>按朴素贝叶斯计算$p(x|c_k)$，但注意此处不能将各维特征单独训练$1$维高斯分布模型，然后计算预测样本似然函数值时进行累乘，如下</p><script type="math/tex; mode=display">p(x|c_k) = \prod_{j=1}^{N_feature} p(x_j|c_k)</script><p>因为特征维度特别高，各个特征单独用$1$维高斯分布描述，累乘计算会下溢，故这里采用多元高斯分布</p><script type="math/tex; mode=display">p(x|c_k) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}} · e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}</script><ul><li>且经主成分分析后，各维度间线性相关性降低，故假定<script type="math/tex; mode=display">\Sigma_k = diag\{\sigma_{k1}, ..., \sigma_{kn}\}</script></li><li><p>但分母$(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}$在计算时不稳定，且各特征标准差大小相差无几，故这里假定</p><script type="math/tex; mode=display">\Sigma_k = I</script></li><li><p>最终简化后的似然函数计算方法为</p><script type="math/tex; mode=display">p(x|c_k) =  e^{-\frac{1}{2} (x - \mu_k)^T (x - \mu_k)}</script></li></ul></li></ul><h3 id="贝叶斯决策模型训练"><a href="#贝叶斯决策模型训练" class="headerlink" title="贝叶斯决策模型训练"></a>贝叶斯决策模型训练</h3><p>基于上述假设，只需训练多元高斯分布的各维均值$\mu_j$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, labels, text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; labels: shape(N_samples, ), labels[i] \in &#123;0, 1&#125;</span></span><br><span class="line"><span class="string">    @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    labels = self.encodeLabel(labels); words = self.text2words(self.clean(text))</span><br><span class="line"></span><br><span class="line">    vecwords = self.numvectorizer.fit_transform(words)              <span class="comment"># 向量化</span></span><br><span class="line">    vecwords = self.tfidfvectorizer.fit_transform(vecwords)         <span class="comment"># tfidf, shape(N_samples, N_features)</span></span><br><span class="line"></span><br><span class="line">    isnotEmpty = (np.sum(vecwords, axis=<span class="number">1</span>)!=<span class="number">0</span>)                      <span class="comment"># 去掉空的样本</span></span><br><span class="line">    vecwords = vecwords[isnotEmpty]; labels = labels[isnotEmpty]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># vecwords = self.reduce_dim.fit_transform(vecwords)              # 降维，计算量太大</span></span><br><span class="line">    self.n_features = vecwords.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    labels = OneHotEncoder().fit_transform(labels.reshape((<span class="number">-1</span>, <span class="number">1</span>))).toarray()</span><br><span class="line">        self.priori = np.mean(labels, axis=<span class="number">0</span>)                           <span class="comment"># 先验概率</span></span><br><span class="line"></span><br><span class="line">    self.likelihood_mu = np.zeros(shape=(<span class="number">2</span>, vecwords.shape[<span class="number">1</span>]))    <span class="comment"># 设似然函数p(x|c)为高斯分布</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        vec = vecwords[labels[:, i]==<span class="number">1</span>]</span><br><span class="line">        self.likelihood_mu[i] = np.mean(vec, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="贝叶斯决策模型预测"><a href="#贝叶斯决策模型预测" class="headerlink" title="贝叶斯决策模型预测"></a>贝叶斯决策模型预测</h3><p>决策函数为</p><script type="math/tex; mode=display">if　p(x|c_i)P(c_i) > p(x|c_j)P(c_j),　then　x \in c_i</script><p>但实际效果显示，等先验概率$P(c_j)$结果更好$(???)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multigaussian</span><span class="params">(self, x, mu)</span>:</span></span><br><span class="line">    <span class="string">""" 简化</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = x - mu</span><br><span class="line">    a = np.exp(<span class="number">-0.5</span> * x.T.dot(x))</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">    @note:</span></span><br><span class="line"><span class="string">                      p(x|c)P(c)</span></span><br><span class="line"><span class="string">            P(c|x) = ------------</span></span><br><span class="line"><span class="string">                         p(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pred_porba = np.ones(shape=(len(self.clean(text)), <span class="number">2</span>))      </span><br><span class="line">        </span><br><span class="line">    words = self.text2words(text)</span><br><span class="line">    vecwords = self.tfidfvectorizer.transform(</span><br><span class="line">                                self.numvectorizer.transform(words))    <span class="comment"># 向量化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(vecwords.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            <span class="comment"># pred_porba[i, c] = self.priori[c] * self.multigaussian(vecwords[i], self.likelihood_mu[c])</span></span><br><span class="line">            pred_porba[i, c] = self.multigaussian(vecwords[i], self.likelihood_mu[c])</span><br><span class="line"></span><br><span class="line">    pred = np.argmax(pred_porba, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> self.decodeLabel(pred)</span><br></pre></td></tr></table></figure><h1 id="调包"><a href="#调包" class="headerlink" title="调包"></a>调包</h1><p>主要用到了<code>scikit-learn</code>机器学习包以下几个功能</p><ul><li><code>sklearn.feature_extraction.text.TfidfVectorizer()</code></li><li><code>sklearn.decomposition.PCA()</code></li><li><code>sklearn.naive_bayes.BernoulliNB()</code></li></ul><p>最终准确率在$97\%$左右，代码比较简单，不进行说明。</p><blockquote><p>采用<code>sklearn.linear_model import.LogisticRegressionCV()</code>效果更佳</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    trainfile = <span class="string">"./data/train.csv"</span></span><br><span class="line">    testfile = <span class="string">"./data/test.csv"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 读取原始数据</span></span><br><span class="line">    data_train = pd.read_csv(trainfile, names=[<span class="string">'Label'</span>, <span class="string">'Text'</span>])</span><br><span class="line">    txt_train  = list(data_train[<span class="string">'Text'</span>])[<span class="number">1</span>: ]; label_train = list(data_train[<span class="string">'Label'</span>])[<span class="number">1</span>: ]</span><br><span class="line">    drop(txt_train)                                             <span class="comment"># 删除数字和标点</span></span><br><span class="line">    txt_test   = list(pd.read_csv(testfile, names=[<span class="string">'Text'</span>])[<span class="string">'Text'</span>])[<span class="number">1</span>: ]</span><br><span class="line">    drop(txt_test)                                              <span class="comment"># 删除数字和标点</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    vectorizer = TfidfVectorizer(stop_words=<span class="string">'english'</span>)          <span class="comment"># 删除英文停用词</span></span><br><span class="line">    vec_train = vectorizer.fit_transform(txt_train).toarray()   <span class="comment"># 提取文本特征向量</span></span><br><span class="line">    <span class="comment"># reduce_dim = PCA(n_components = 4096)</span></span><br><span class="line">    <span class="comment"># vec_train = reduce_dim.fit_transform(vec_train)</span></span><br><span class="line">    estimator = BernoulliNB()</span><br><span class="line">    estimator.fit(vec_train, label_train)                       <span class="comment"># 训练朴素贝叶斯模型</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试</span></span><br><span class="line">    label_train_pred = estimator.predict(vec_train)</span><br><span class="line">    acc = np.mean((label_train_pred==label_train).astype(<span class="string">'float'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    vec_test = vectorizer.transform(txt_test).toarray()</span><br><span class="line">    <span class="comment"># vec_test = reduce_dim.transform(vec_test)</span></span><br><span class="line">    label_test_pred = estimator.predict(vec_test)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/sampleSubmission.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(label_test_pred.shape[<span class="number">0</span>]):</span><br><span class="line">            f.write(label_test_pred[i] + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TF-IDF</title>
      <link href="/2018/10/25/TF-IDF/"/>
      <url>/2018/10/25/TF-IDF/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>正在做<a href="https://www.lintcode.com/" target="_blank" rel="noopener">LintCode</a>上的垃圾邮件分类，使用<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">朴素贝叶斯</a>方法解决，涉及到文本特征的提取。<br>TF-IDF（词频-逆文档频率）算法是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p><h1 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h1><h2 id="词频-TF"><a href="#词频-TF" class="headerlink" title="词频(TF)"></a>词频(TF)</h2><p><code>Term Frequency</code>，就是某个关键字出现的频率，具体来讲，就是词库中的<strong>某个词</strong>在<strong>当前文章</strong>中出现的频率。那么我们可以写出它的计算公式：</p><script type="math/tex; mode=display">TF_{ij} = \frac{n_{ij}}{\sum_k n_{i, k}}</script><p>其中，$n_{ij}$表示关键词$j$在文档$i$中的出现次数。</p><p>单纯使用TF来评估关键词的重要性忽略了常用词的干扰。常用词就是指那些文章中大量用到的，但是不能反映文章性质的那种词，比如：因为、所以、因此等等的连词，在英文文章里就体现为and、the、of等等的词。这些词往往拥有较高的TF，所以仅仅使用TF来考察一个词的关键性，是不够的。</p><h2 id="逆文档频率-IDF"><a href="#逆文档频率-IDF" class="headerlink" title="逆文档频率(IDF)"></a>逆文档频率(IDF)</h2><p><code>Inverse Document Frequency</code>，文档频率就是一个词在整个文库词典中出现的频率，逆文档频率用下式计算</p><script type="math/tex; mode=display">IDF_j = \log \frac{|D|}{|D_j| + 1}</script><p>其中，$|D|$表示总的文档数目，$|D_j|$表示关键词$j$出现过的文档数目</p><p><code>scikit-learn</code>内为</p><script type="math/tex; mode=display">IDF_j = \log \frac{|D| + 1}{|D_j| + 1} + 1</script><p><img src="/2018/10/25/TF-IDF/sklearn.jpg" alt="sklearn_tfidf"></p><h2 id="词频-逆文档频率-TF-IDF"><a href="#词频-逆文档频率-TF-IDF" class="headerlink" title="词频-逆文档频率(TF-IDF)"></a>词频-逆文档频率(TF-IDF)</h2><script type="math/tex; mode=display">TF-IDF_{i} = TF_i × IDF</script><h1 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h1><p>例如有如下$3$个文本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：My dog ate my homework.</span><br><span class="line">文本2：My cat ate the sandwich.</span><br><span class="line">文本3：A dolphin ate the homework.</span><br></pre></td></tr></table></figure></p><p>提取字典，一般需要处理大小写、去除停用词<code>a</code>，处理结果为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ate, cat, dog, dolphin, homework, my, sandwich, the</span><br></pre></td></tr></table></figure></p><p>故各个文本的词数向量为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：[1, 0, 1, 0, 1, 2, 0, 0]</span><br><span class="line">文本2：[1, 1, 0, 0, 0, 1, 1, 1]</span><br><span class="line">文本3：[1, 0, 0, 1, 1, 0, 0, 1]</span><br></pre></td></tr></table></figure></p><p>各个文本的词频向量(TF)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ]</span><br><span class="line">文本2：[0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ]</span><br><span class="line">文本3：[0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]</span><br></pre></td></tr></table></figure></p><p>各词出现过的文档次数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3, 1, 1, 1, 2, 2, 1, 2]</span><br></pre></td></tr></table></figure></p><p>总文档数为$3$，各词的逆文档频率(IDF)向量</p><blockquote><p>这里使用<code>scikit-learn</code>内的方法求解</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207,  1.28768207, 1.69314718, 1.28768207]</span><br></pre></td></tr></table></figure><p>故各文档的TF-IDF向量为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文本1：</span><br><span class="line">[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ]</span><br><span class="line">文本2：</span><br><span class="line">[0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641]</span><br><span class="line">文本3：</span><br><span class="line">[0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]</span><br></pre></td></tr></table></figure></p><p>经单位化后，有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文本1：</span><br><span class="line">[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ]</span><br><span class="line">文本2：</span><br><span class="line">[0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178]</span><br><span class="line">文本3：</span><br><span class="line">[0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]</span><br></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_num = np.array([</span><br><span class="line">[1, 0, 1, 0, 1, 2, 0, 0],</span><br><span class="line">[1, 1, 0, 0, 0, 1, 1, 1],</span><br><span class="line">[1, 0, 0, 1, 1, 0, 0, 1]</span><br><span class="line">])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tf = vec_num / np.sum(vec_num, axis=1).reshape(-1, 1)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tf</span><br><span class="line">array([[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ],</span><br><span class="line">       [0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ],</span><br><span class="line">       [0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_num[vec_num&gt;0] = 1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; n_showup = np.sum(vec_num, axis=0)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; n_showup</span><br><span class="line">array([3, 1, 1, 1, 2, 2, 1, 2])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; d = 3</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_idf = np.log((d + 1) / (n_showup + 1)) + 1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_idf</span><br><span class="line">array([1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf = vec_tf * vec_idf</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf</span><br><span class="line">array([[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ],</span><br><span class="line">       [0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641],</span><br><span class="line">       [0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf = vec_tfidf / np.linalg.norm(vec_tfidf, axis=1).reshape((-1, 1))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf</span><br><span class="line">array([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805, 0.73861611, 0.        , 0.        ],</span><br><span class="line">       [0.31544415, 0.53409337, 0.        , 0.        , 0.        , 0.40619178, 0.53409337, 0.40619178],</span><br><span class="line">       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 , 0.        , 0.        , 0.4804584 ]])</span><br></pre></td></tr></table></figure><h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><p>使用<code>scikit-learn</code>机器学习包计算结果<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vectorizer = TfidfVectorizer()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; text = [</span><br><span class="line">"My dog ate my homework",</span><br><span class="line">"My cat ate the sandwich",</span><br><span class="line">"A dolphin ate the homework"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vectorizer.fit_transform(text).toarray()</span><br><span class="line">array([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ],</span><br><span class="line">       [0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178],</span><br><span class="line">       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vectorizer.get_feature_names()</span><br><span class="line">['ate', 'cat', 'dog', 'dolphin', 'homework', 'my', 'sandwich', 'the']</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Practice </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVD</title>
      <link href="/2018/10/23/SVD/"/>
      <url>/2018/10/23/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>奇异值分解<code>Singular Value Decomposition</code>是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="从特征值分解-EVD-讲起"><a href="#从特征值分解-EVD-讲起" class="headerlink" title="从特征值分解(EVD)讲起"></a>从特征值分解(EVD)讲起</h2><p>我们知道对于一个$n$阶方阵$A_{n×n}$，有</p><script type="math/tex; mode=display">A\alpha_i = \lambda_i \alpha_i　i = 1, ..., n</script><p>取</p><script type="math/tex; mode=display">P = \left[\alpha_1, \alpha_2, ..., \alpha_n\right]</script><p>有下式成立</p><script type="math/tex; mode=display">AP = P\Lambda</script><p>其中</p><script type="math/tex; mode=display">\Lambda = \left[        \begin{matrix}            \lambda_1 & & \\            & ... & \\            & & \lambda_n \\        \end{matrix}\right]</script><blockquote><p>特征值一般从大到小排列</p></blockquote><p>利用该式可将方阵$A_{n×n}$化作对角阵$\Lambda_{n×n}$</p><script type="math/tex; mode=display">\Lambda = P^{-1}AP</script><p>或者</p><script type="math/tex; mode=display">A = P \Lambda P^{-1} = \sum_{i=1}^n \lambda_i (P_{,i})(P_{,i})^{-1}</script><blockquote><p>“$_{i}$”表示第$i$行，“$_{,i}$”表示第$i$列</p></blockquote><p>这样我们就可以理解为，矩阵$A$是由$n$个$n$阶矩阵$P_{,i}P^{-1}_{i}$加权组成，特征值$\lambda_i$即为权重。</p><blockquote><p>以上为个人理解，不妥之处可以指出。</p></blockquote><h2 id="奇异值分解-SVD"><a href="#奇异值分解-SVD" class="headerlink" title="奇异值分解(SVD)"></a>奇异值分解(SVD)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>对于长方阵$A_{m×n}$，不能进行特征值分解，可进行如下分解</p><script type="math/tex; mode=display">A_{m×n} = U_{m×m} \Sigma_{m×n} V_{n×n}^T</script><p>其中$U \in \mathbb{R}^{m×m}, V \in \mathbb{R}^{n×n}$，均为正交矩阵。矩阵$\Sigma_{m×n}$如下</p><ul><li><p>对于$m&gt;n$</p><script type="math/tex; mode=display">  \Sigma_{m×n} = \left[          \begin{matrix}              S_{n×n} \\              --- \\              O_{(m-n)×n}          \end{matrix}  \right]</script></li><li><p>对于$m&lt;n$</p><script type="math/tex; mode=display">  \Sigma_{m×n} = \left[          \begin{matrix}              S_{m×m} & | & O_{m×(n-m)}          \end{matrix}  \right]</script></li></ul><p>矩阵$S_{n×n}$为对角阵，对角元素从大到小排列</p><script type="math/tex; mode=display">S_{n×n} = \left[    \begin{matrix}        \sigma_1 & & \\         & ... & \\         & & \sigma_n\\    \end{matrix}\right]</script><p>直观表示<code>SVD</code>分解如下<br><img src="/2018/10/23/SVD/直观表示SVD.jpg" alt="直观表示SVD"></p><p>当取$r&lt;n$时，有部分奇异值分解，可用于降维</p><script type="math/tex; mode=display">A_{m×n} = U_{m×r} \Sigma_{r×r} V_{r×n}^T</script><h3 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h3><blockquote><p>以下仅考虑$m&gt;n$的情况</p></blockquote><ol><li><p>令矩阵$A^T$与$A$相乘，有</p><script type="math/tex; mode=display"> A^TA = (U \Sigma V^T)^T (U \Sigma V^T)</script><script type="math/tex; mode=display"> = V \Sigma^T U^T U \Sigma V^T</script><script type="math/tex; mode=display"> A^TA = V \Sigma^T \Sigma V^T</script><blockquote><p>矩阵$U$为正交阵，即满足$U^TU=I$</p></blockquote><p> 其中</p><script type="math/tex; mode=display"> \Sigma^T \Sigma =          \left[             \begin{matrix}                 S^T_{n×n} & | & O^T_{n×(m-n)}             \end{matrix}         \right]         \left[             \begin{matrix}                 S_{n×n} \\                 --- \\                 O_{(m-n)×n}             \end{matrix}         \right]</script><script type="math/tex; mode=display"> = S_{n×n}^2  = \left[     \begin{matrix}         \sigma_1^2 & & \\         & ... & \\         & & \sigma_n^2\\     \end{matrix} \right]</script><p> 则</p><script type="math/tex; mode=display"> A^T A = V S^2  V^T</script><p> 即矩阵$A^T A$相似对角化为$S^2$，对角元素$\sigma_i^2$与矩阵$V$的列向量$v_i(i=1, …, n)$为矩阵$A^T A$的特征对。</p><p> 那么对矩阵$A^T A$进行特征值分解，有</p><script type="math/tex; mode=display"> (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i</script><p> 则</p><script type="math/tex; mode=display"> v_i = \alpha^{(1)}_i　\sigma_i = \sqrt{\lambda^{(1)}_i}</script><blockquote><p>注：对于二次型$x^T (A^T A) x$</p><script type="math/tex; mode=display">x^T (A^T A) x = (Ax)^T(Ax) \geq 0</script><p>故矩阵$A^T A$半正定，$\sigma_i = \sqrt{\lambda_i}$有解</p></blockquote></li></ol><ol><li><p>同理，令矩阵$A$与$A^T$相乘，可证得</p><script type="math/tex; mode=display"> A A^T = U \Sigma \Sigma^T U^T</script><p> 其中</p><script type="math/tex; mode=display"> \Sigma \Sigma^T =          \left[             \begin{matrix}                 S_{n×n} \\                 --- \\                 O_{(m-n)×n}             \end{matrix}         \right]         \left[             \begin{matrix}                 S^T_{n×n} & | & O^T_{n×(m-n)}             \end{matrix}         \right]</script><script type="math/tex; mode=display"> = \left[     \begin{matrix}         S^2_{n×n} & O_{n×(m-n)} \\         O_{(m-n)×n} & O_{(m-n)×(m-n)}     \end{matrix} \right]</script><p> 即矩阵$A A^T$相似对角化，对角元素$\sigma_i^2$与矩阵$U$的列向量$u_i(i=1, …, m)$为矩阵$A A^T$的特征对。</p><p> 对矩阵$A A^T$进行特征值分解，有</p><script type="math/tex; mode=display"> (A^T A) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i</script><p> 则</p><script type="math/tex; mode=display"> u_i = \alpha^{(2)}_i　\sigma_i = \sqrt{\lambda^{(2)}_i}</script><blockquote><p>同理可证得$A A^T$半正定，略。</p></blockquote></li></ol><p>一般来说，为减少计算量，计算奇异值分解只进行一次特征值分解，如对于矩阵$X_{m×n}(m&gt;n)$，选取$n$阶矩阵$X^T X$进行特征值分解计算$v_i$，计算$u_i$方法下面介绍。</p><p>根据前面推导，我们有特征值分解</p><script type="math/tex; mode=display">(A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i</script><script type="math/tex; mode=display">(A A^T) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i</script><p>其中$\lambda^{(1)}_i = \lambda^{(2)}_i = \sigma_i^2$，$v_i = \alpha^{(1)}_i$，$u_i = \alpha^{(2)}_i$，即</p><script type="math/tex; mode=display">A^T A v_i = \sigma_i^2 v_i \tag{1}</script><script type="math/tex; mode=display">A A^T u_i = \sigma_i^2 u_i \tag{2}</script><p>$(1)$式左右乘$A$，有</p><script type="math/tex; mode=display">A A^T A v_i = \sigma_i^2 A v_i</script><p>发现什么？这是另一个特征值分解的表达式！</p><script type="math/tex; mode=display">(A A^T) (A v_i) = \sigma_i^2 (A v_i)</script><p>故</p><script type="math/tex; mode=display">u_i \propto A v_i　或　u_i = k · A v_i \tag{3}</script><p>现在求解系数$k$，根据定义</p><script type="math/tex; mode=display">A = U \Sigma V^T　\Rightarrow　AV = U \Sigma</script><p>则</p><script type="math/tex; mode=display">A v_i = \sigma_i u_i　\Rightarrow　u_i = \frac{1}{\sigma_i} A v_i</script><p>或者</p><script type="math/tex; mode=display">U = A V \Sigma^{-1}</script><blockquote><p>注：只能求前$n$个$u_i$，之后的需要列写方程求解</p></blockquote><h1 id="举栗"><a href="#举栗" class="headerlink" title="举栗"></a>举栗</h1><p>将矩阵$A$进行分解</p><script type="math/tex; mode=display">A = \left[    \begin{matrix}        0 & 1 \\        1 & 1 \\        1 & 0    \end{matrix}\right]</script><p>为减少计算量，取$A^T A$计算</p><script type="math/tex; mode=display">A^T A = \left[    \begin{matrix}        2 & 1 \\        1 & 2     \end{matrix}\right]</script><p>特征值分解，有</p><script type="math/tex; mode=display">A\left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]= \left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]\left[    \begin{matrix}        3 &  \\          & 1    \end{matrix} \right]</script><p>故</p><script type="math/tex; mode=display">\Sigma = \left[    \begin{matrix}        \sqrt{3} &  \\          & 1    \end{matrix} \right]　V = \left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]</script><script type="math/tex; mode=display">U = A V \Sigma^{-1} = \left[    \begin{matrix}        \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\        \frac{2}{\sqrt{6}} & 0 \\        \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}}    \end{matrix} \right]</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; A = np.array([</span><br><span class="line">[0, 1], [1, 1], [1, 0]</span><br><span class="line">])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; ATA = A.T.dot(A)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; eigval, eigvec= np.linalg.eig(ATA)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; V = eigvec.copy()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; S = np.diag(np.sqrt(eigval))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; U = A.dot(V).dot(np.linalg.inv(S))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; U</span><br><span class="line">array([[ 0.40824829,  0.70710678],</span><br><span class="line">       [ 0.81649658,  0.        ],</span><br><span class="line">       [ 0.40824829, -0.70710678]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; S</span><br><span class="line">array([[1.73205081, 0.        ],</span><br><span class="line">       [0.        , 1.        ]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; V</span><br><span class="line">array([[ 0.70710678, -0.70710678],</span><br><span class="line">       [ 0.70710678,  0.70710678]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 验证</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; U.dot(S).dot(V.T)</span><br><span class="line">array([[-2.23711432e-17,  1.00000000e+00],</span><br><span class="line">       [ 1.00000000e+00,  1.00000000e+00],</span><br><span class="line">       [ 1.00000000e+00, -2.23711432e-17]])</span><br></pre></td></tr></table></figure><h1 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h1><p>展开表达式，取$r \leq n$时，</p><script type="math/tex; mode=display">A = U_{m×r} \Sigma_{r×r} V_{r×n}^T = \sum_{i=1}^r \sigma_i (U_{,i}) (V_{,i})^T</script><p>就得到与<code>PCA</code>相同的结论，矩阵$A$可由$r$个$m×n$的矩阵$(U_{,i}) (V_{,i})^T$加权组成。一般来说，前$10\%$甚至$1\%$的奇异值就占了全部奇异值之和的$99\%$，极大地保留了信息，而大大减少了存储空间。</p><blockquote><p>以图片为例，若原有<code>24bit</code>图片，其大小为<code>(1024, 768)</code>，则不计图片信息，仅仅数据共占<code>1024×768×3 B</code>，或<code>2.25 MB</code>。用奇异值分解进行压缩，保留$60\%$的奇异值，可达到几乎无损的程度，此时需要保存向量矩阵$U_{1024×60}$，$V_{60×768}$以及$60$个奇异值，以浮点数<code>float32</code>存储，一共占<code>420 KB</code>即可。</p><script type="math/tex; mode=display">(1024 × 60 + 60 × 768 + 60) × 4 / 2^{10} = 420.23</script><p>说句题外话，存储量的压缩必然以计算量的增大为代价，相反亦然，所以需要协调好<code>RAM</code>与<code>ROM</code>容量，考虑计算机的计算速度。换句话说，空间和时间上必然是互补的，哲学的味道hhhh。</p></blockquote><h1 id="分解结果的信息保留"><a href="#分解结果的信息保留" class="headerlink" title="分解结果的信息保留"></a>分解结果的信息保留</h1><p>分解后各样本间的欧式距离与角度信息应不变，给出证明如下<br>设有$m$组$n$维样本样本</p><script type="math/tex; mode=display">X_{n×m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]</script><p>经奇异值分解，有</p><script type="math/tex; mode=display">X_{n×m} = U_{n×r} \Sigma_{r×r} V_{r×m}^T</script><p>记</p><script type="math/tex; mode=display">Z_{r×m} = \Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]</script><p>有</p><script type="math/tex; mode=display">X = U Z</script><ul><li><p>欧式距离</p><script type="math/tex; mode=display">  || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2</script><script type="math/tex; mode=display">  = \left[ U (Z^{(i)} - Z^{(j)}) \right]^T \left[ U (Z^{(i)} - Z^{(j)}) \right]</script><script type="math/tex; mode=display">  = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)})</script><script type="math/tex; mode=display">  = || Z^{(i)} - Z^{(j)} ||_2^2</script><p>  即</p><script type="math/tex; mode=display">  || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2</script></li><li><p>角度信息</p><script type="math/tex; mode=display">  \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2}</script><script type="math/tex; mode=display">  = \frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2}</script><script type="math/tex; mode=display">  = \frac{(UZ^{(i)})^T(UZ^{(j)})}{\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \sqrt{(UZ^{(j)})^T(UZ^{(j)})}}</script><script type="math/tex; mode=display">  = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}</script><p>  即</p><script type="math/tex; mode=display">  \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} =   \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}</script></li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p15_svd.py" target="_blank" rel="noopener">@Github: Code of SVD</a><br>对图片进行了分解<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Singular Value Decomposition</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        m &#123;int&#125;</span></span><br><span class="line"><span class="string">        n &#123;int&#125;</span></span><br><span class="line"><span class="string">        r &#123;int&#125;: if r == -1, then r = n</span></span><br><span class="line"><span class="string">        isTrains &#123;bool&#125;: isTrains = True if input.shape[0] &lt; input.shape[1]</span></span><br><span class="line"><span class="string">        U &#123;ndarray(m, r)&#125;</span></span><br><span class="line"><span class="string">        S &#123;ndarray(r, )&#125;</span></span><br><span class="line"><span class="string">        V &#123;ndarray(n, r)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        - Transpose input matrix if m &lt; n, and m, n := n, m</span></span><br><span class="line"><span class="string">        - Reassign r if eigvals contains zero</span></span><br><span class="line"><span class="string">        - Singular values are stored in a 1-dim array `S`</span></span><br><span class="line"><span class="string">        - X' = U S V^T</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, r=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        self.m = <span class="keyword">None</span></span><br><span class="line">        self.n = <span class="keyword">None</span></span><br><span class="line">        self.r = r</span><br><span class="line">        self.isTrans = <span class="keyword">False</span></span><br><span class="line">        self.U = <span class="keyword">None</span></span><br><span class="line">        self.S = <span class="keyword">None</span></span><br><span class="line">        self.V = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" calculate components</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - Transpose input matrix if m &lt; n, and m, n := n, m</span></span><br><span class="line"><span class="string">            - reassign self.r if eigvals contains zero</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (self.m, self.n) = X.shape</span><br><span class="line">        <span class="keyword">if</span> self.m &lt; self.n:</span><br><span class="line">            X = X.T</span><br><span class="line">            self.m, self.n = self.n, self.m</span><br><span class="line">            self.isTrans = <span class="keyword">True</span></span><br><span class="line">        self.r = self.n <span class="keyword">if</span> (self.r == <span class="number">-1</span>) <span class="keyword">else</span> self.r</span><br><span class="line"></span><br><span class="line">        XTX = X.T.dot(X)</span><br><span class="line">        eigval, eigvec = np.linalg.eig(X.T.dot(X))</span><br><span class="line">        eigval, eigvec = np.real(eigval), np.real(eigvec)</span><br><span class="line">        </span><br><span class="line">        self.S = np.sqrt(np.clip(eigval, <span class="number">0</span>, float(<span class="string">'inf'</span>)))</span><br><span class="line">        self.S = self.S[self.S &gt; <span class="number">0</span>]</span><br><span class="line">        self.r = min(self.r, self.S.shape[<span class="number">0</span>])               <span class="comment"># reassign self.r</span></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>][: self.r]          <span class="comment"># sort eigval from large to small</span></span><br><span class="line">        eigval = eigval[order]; eigvec = eigvec[:, order]</span><br><span class="line">        self.V = eigvec.copy()</span><br><span class="line">        self.U = X.dot(self.V).dot(</span><br><span class="line">                    np.linalg.inv(np.diag(self.S)))</span><br><span class="line">        <span class="keyword">return</span> self.U, self.S, self.V</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compose</span><span class="params">(self, r=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        <span class="string">""" merge first r components</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            r &#123;int&#125;: if r==-1, merge all components</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(m, n)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> r == <span class="number">-1</span>:</span><br><span class="line">            X = self.U.dot(np.diag(self.S)).dot(self.V.T)</span><br><span class="line">            X = X.T <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            (m, n) = (self.n, self.m) <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> (self.m, self.n)</span><br><span class="line">            X = np.zeros(shape=(m, n))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(r):</span><br><span class="line">                X += self.__getitem__(i)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">""" get a component</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            index &#123;int&#125;: range from (0, self.r)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        u = self.U[:, idx]</span><br><span class="line">        v = self.V[:, idx]</span><br><span class="line">        s = self.S[idx]</span><br><span class="line">        x = s * u.reshape(self.m, <span class="number">1</span>).\</span><br><span class="line">                    dot(v.reshape(<span class="number">1</span>, self.n))</span><br><span class="line">        x = x.T <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showComponets</span><span class="params">(self, r=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        <span class="string">""" display components</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - Resize components' shape into (40, 30)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        m, n = self.m, self.n</span><br><span class="line">        r = self.r <span class="keyword">if</span> r==<span class="number">-1</span> <span class="keyword">else</span> r</span><br><span class="line">        n_images = <span class="number">10</span>; m_images = r // n_images + <span class="number">1</span></span><br><span class="line">        m_size, n_size = <span class="number">40</span>, <span class="number">30</span></span><br><span class="line">        showfig = np.zeros(shape=(m_images*m_size, n_images*n_size))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(r):</span><br><span class="line">            m_pos = i // n_images</span><br><span class="line">            n_pos = i %  n_images</span><br><span class="line">            component = self.__getitem__(i)</span><br><span class="line">            component = component.T <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> component</span><br><span class="line">            component = cv2.resize(component, (<span class="number">30</span>, <span class="number">40</span>))</span><br><span class="line">            showfig[m_pos*m_size: (m_pos+<span class="number">1</span>)*m_size, n_pos*n_size: (n_pos+<span class="number">1</span>)*n_size] = component</span><br><span class="line">        plt.figure(<span class="string">'components'</span>)</span><br><span class="line">        plt.imshow(showfig)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure></p><p>用上面的代码进行实验<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 读取一张图片</span><br><span class="line">X = load_images()[0].reshape((32, 32))</span><br><span class="line">showmat2d(X)</span><br><span class="line"><span class="meta">#</span> 对图片进行奇异值分解</span><br><span class="line">decomposer = SVD(r=-1)</span><br><span class="line">decomposer.fit(X)</span><br><span class="line"><span class="meta">#</span> 显示一下分量</span><br><span class="line">decomposer.showComponets(r=-1)</span><br><span class="line"><span class="meta">#</span> 将全部分量组合，并显示</span><br><span class="line">X_ = decomposer.compose(r=-1)</span><br><span class="line">showmat2d(X_)</span><br><span class="line"><span class="meta">#</span> 将前5个分量组合，并显示</span><br><span class="line">X_ = decomposer.compose(r=5)</span><br><span class="line">showmat2d(X_)</span><br></pre></td></tr></table></figure></p><ul><li><p>载入原图如下<br><img src="/2018/10/23/SVD/source.png" alt="source"></p></li><li><p>分量显示如下<br><img src="/2018/10/23/SVD/components.png" alt="components"></p></li><li><p>组合分量显示如下</p><ul><li>组合全部<br>  <img src="/2018/10/23/SVD/merge_all.png" alt="merge_all"></li><li>组合前5个分量<br>  <img src="/2018/10/23/SVD/merge_5.png" alt="merge_5"></li></ul></li></ul><h1 id="应用：推荐系统"><a href="#应用：推荐系统" class="headerlink" title="应用：推荐系统"></a>应用：推荐系统</h1><blockquote><p>详情查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/ZhaoHaitao%2C%20ECUST/recommend.py" target="_blank" rel="noopener">Basic-Machine-Learning-Algorithm/ZhaoHaitao, ECUST/recommend.py</a></p></blockquote><p>数据可从<a href="http://files.grouplens.org/datasets/movielens/" target="_blank" rel="noopener">MoiveLens</a>下载，实验使用<a href="http://files.grouplens.org/datasets/movielens/ml-100k.zip" target="_blank" rel="noopener">ml-100k.zip</a>。假设有用户$N_{user}$人，电影$N_{item}$部，先每个人对其看过的部分电影已进行评分，及其稀疏，现希望从这些数据预测出未知数据。</p><p>存储数据为矩阵</p><script type="math/tex; mode=display">M_{N_{user} \times N_{item}} = \left[\begin{matrix}    r_{ij}\end{matrix}\right]</script><p>现希望从用户相似度的角度，找出臭味相投的一些用户，用他们的评分均值作为该用户的评分，先对上述矩阵进行SVD分解</p><script type="math/tex; mode=display">M_{N_{user} \times N_{item}} = U_{N_{user} \times N_{user}} \cdot \Sigma_{N_{user} \times N_{item}} \cdot (V_{N_{item} \times N_{item}})^T \tag{4}</script><p>其中</p><script type="math/tex; mode=display">U_{N_{user} \times N_{user}} = \left[\begin{matrix}    \vec{u_{1}} & \vec{u_{2}} & \cdots & \vec{u_{N_{user}}}\end{matrix}\right]</script><script type="math/tex; mode=display">V_{N_{item} \times N_{item}} = \left[\begin{matrix}    \vec{v_{1}} & \vec{v_{2}} & \cdots & \vec{v_{N_{item}}}\end{matrix}\right]</script><p>利用矩阵$V$提取用户的特征，设特征维度为$D_u$</p><script type="math/tex; mode=display">M^u = M_{N_{user} \times N_{item}} \cdot V_{N_{item} \times D_u} \tag{5}</script><p>而</p><script type="math/tex; mode=display">V_{N_{item} \times N_{item}} = \left[\begin{matrix}    V_{N_{item} \times D_u} & | & V_{N_{item} \times (N_{item} - D_u)}\end{matrix}\right]</script><script type="math/tex; mode=display">(V_{N_{item} \times N_{item}})^T \cdot V_{N_{item} \times D_u} = \left[\begin{matrix}    I_{D_u \times D_u} \\ --- \\ O_{(N_{item} - D_u) \times D_u}\end{matrix}\right] \tag{6}</script><p>所以$(4)(6)$代入$(5)$，亦可化简为</p><script type="math/tex; mode=display">M^u = U_{N_{user} \times N_{user}} \cdot \Sigma_{N_{user} \times N_{item}} \cdot (V_{N_{item} \times N_{item}})^T \cdot V_{N_{item} \times D_u}</script><script type="math/tex; mode=display">= U_{N_{user} \times D_u} \cdot \Sigma_{D_u \times D_u} \tag{7}</script><p>现计算用户间的相似度矩阵，可用余弦度量，即</p><script type="math/tex; mode=display">s_{ij} = \frac{M^{u_iT} M^u_j}{||M^u_i|| ||M^u_j||}</script><p>或者</p><script type="math/tex; mode=display">M^u_i := \frac{M^u_i}{||M^u_i||}</script><script type="math/tex; mode=display">S_{N_{user} \times N_{user}} = M^u \cdot M^{uT} \tag{8}</script><p>对于某部电影$\text{item}^{(j)}$，先找到该用户$\text{user}^{(i)}$的最近似的几个用户$\text{user}^{(k)}, k \in \{1, \cdots, N_{user} \}, k \neq i$，取其均值作为该用户的评分。</p><p>若利用电影的相似度，只需</p><script type="math/tex; mode=display">M^i = (U_{N_{item} \times D_i})^T \cdot M_{N_{user} \times N_{item}} \tag{9}</script><p>以下同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">according_to_user</span><span class="params">(train_matrix, test_matrix, cols=<span class="number">80</span>, n_keep=<span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将矩阵SVD分解</span></span><br><span class="line">    _, _, vh = np.linalg.svd(train_matrix)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 压缩原矩阵，A' = A V[:, :k]</span></span><br><span class="line">    train_compressed_col = train_matrix.dot(vh[: cols].T)   <span class="comment"># N_USERS x cols</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算相似度矩阵</span></span><br><span class="line">    similarity_user = get_cosine_similarity_matrix(train_compressed_col)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    pred_matrix = np.zeros_like(test_matrix)        <span class="comment"># 保存预测结果</span></span><br><span class="line">    to_pred = np.array(np.where(test_matrix != <span class="number">0</span>))  <span class="comment"># 需要预测的数据位置, (2, n)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(to_pred.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        r, c = to_pred[:, i]                        <span class="comment"># r为用户索引，c为电影索引</span></span><br><span class="line"></span><br><span class="line">        id = np.argsort(similarity_user[r])[::<span class="number">-1</span>]   <span class="comment"># 将用户以相似度从大到小排序</span></span><br><span class="line">        id = id[<span class="number">1</span>: n_keep + <span class="number">1</span>]                      <span class="comment"># 获取相似度最大的几个用户，除自身</span></span><br><span class="line">        rates = train_matrix[id, c]                 <span class="comment"># 获取这几个用户对该电影的评分</span></span><br><span class="line">        rates = rates[rates!=<span class="number">0</span>]                     <span class="comment"># 已评价的数据</span></span><br><span class="line"></span><br><span class="line">        rate = np.mean(rates) <span class="keyword">if</span> rates.shape[<span class="number">0</span>] != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        pred_matrix[r, c] = rate</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred_matrix</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>删除停用词</title>
      <link href="/2018/10/23/%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D/"/>
      <url>/2018/10/23/%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.yiibai.com/python_text_processing/python_remove_stopwords.html" target="_blank" rel="noopener">删除停用词 - Python文本处理教程™</a></p></blockquote><p>停用词是对句子没有多大意义的英语单词。 在不牺牲句子含义的情况下，可以安全地忽略它们。 例如，the, he, have等等的单词已经在名为语料库的语料库中捕获了这些单词。</p><h1 id="下载语料库"><a href="#下载语料库" class="headerlink" title="下载语料库"></a>下载语料库</h1><ul><li><p>安装<code>nltk</code>模块</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install nltk</span><br></pre></td></tr></table></figure></li><li><p>下载语料库</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line">nltk.download(&apos;stopwords&apos;)</span><br></pre></td></tr></table></figure></li></ul><h1 id="使用库料库"><a href="#使用库料库" class="headerlink" title="使用库料库"></a>使用库料库</h1><ul><li><p>验证停用词</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; stopwords.words('english')</span><br><span class="line">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', </span><br><span class="line">'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', </span><br><span class="line">'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',</span><br><span class="line">"she's", 'her', 'hers', 'herself', 'it', "it's", 'its', </span><br><span class="line">'itself', 'they', 'them', 'their', 'theirs', 'themselves', </span><br><span class="line">'what', 'which', 'who', 'whom', 'this', 'that', "that'll", </span><br><span class="line">'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', </span><br><span class="line">'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', </span><br><span class="line">'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', </span><br><span class="line">'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', </span><br><span class="line">'with', 'about', 'against', 'between', 'into', 'through', </span><br><span class="line">'during', 'before', 'after', 'above', 'below', 'to', 'from', </span><br><span class="line">'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', </span><br><span class="line">'again', 'further', 'then', 'once', 'here', 'there', 'when',</span><br><span class="line">'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', </span><br><span class="line">'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', </span><br><span class="line">'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', </span><br><span class="line">'can', 'will', 'just', 'don', "don't", 'should', "should've", </span><br><span class="line">'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', </span><br><span class="line">"aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', </span><br><span class="line">"doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', </span><br><span class="line">"haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn',</span><br><span class="line">"mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', </span><br><span class="line">"shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', </span><br><span class="line">"won't", 'wouldn', "wouldn't"]</span><br></pre></td></tr></table></figure><p>  除了英语之外，具有这些停用词的各种语言如下。</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; stopwords.fileids()</span><br><span class="line">['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', </span><br><span class="line">'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', </span><br><span class="line">'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian',</span><br><span class="line">'spanish', 'swedish', 'turkish']</span><br></pre></td></tr></table></figure></li><li><p>示例<br>  从单词列表中删除停用词。</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; en_stops = set(stopwords.words('english'))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; all_words = ['There', 'is', 'a', 'tree','near','the','river']</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for word in all_words:</span><br><span class="line">if word not in en_stops:</span><br><span class="line">print(word)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">There</span><br><span class="line">tree</span><br><span class="line">near</span><br><span class="line">river</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PCA</title>
      <link href="/2018/10/22/PCA/"/>
      <url>/2018/10/22/PCA/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><code>PCA</code>全称<code>Principal Component Analysis</code>，即主成分分析，是一种常用的数据降维方法。它可以通过线性变换将原始数据变换为一组各维度线性无关的表示，以此来提取数据的主要线性分量。</p><h1 id="向量的投影"><a href="#向量的投影" class="headerlink" title="向量的投影"></a>向量的投影</h1><p>现有两个任意不共线向量$\vec{u}, \vec{v}$，将$\vec{u}$投射到$\vec{v}$上<br><img src="/2018/10/22/PCA/向量投影.jpg" alt="向量投影"></p><p>投影后，可以得到两个正交向量</p><script type="math/tex; mode=display">\vec{u}' · (\vec{u} - \vec{u}') = 0</script><p>我们设</p><script type="math/tex; mode=display">\vec{u}' = \mu \vec{v} \tag{1}</script><p>代入后有</p><script type="math/tex; mode=display">\mu \vec{v} · (\vec{u} - \mu \vec{v}) = 0</script><p>引入矩阵运算，即</p><script type="math/tex; mode=display">(\mu v)^T (u - \mu v) = 0</script><p>有</p><script type="math/tex; mode=display">v^T u = \mu v^T v</script><p>则得到$u’$以$v$为基向量的坐标</p><script type="math/tex; mode=display">\mu  = (v^T v)^{-1} v^T u \tag{2}</script><p>所以得到</p><script type="math/tex; mode=display">u' = v (v^T v)^{-1} v^T u \tag{*}</script><blockquote><ul><li><p>坐标变换求解投影向量：$u’$可视作$u$经坐标变换$u’ = P u$得到，所以</p><script type="math/tex; mode=display">P = v (v^T v)^{-1} v^T</script></li><li><p>推广至多个向量的投影，即得到</p><script type="math/tex; mode=display">P = X (X^T X)^{-1} X^T</script><p>这与<a href="https://louishsu.xyz/2018/10/18/Linear-Regression/" target="_blank" rel="noopener">线性回归</a>中得到的结论一致。</p></li></ul></blockquote><p>实际上</p><script type="math/tex; mode=display">u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T u</script><p>记单位向量$\frac{v}{||v||}$为$v_0$，得到</p><script type="math/tex; mode=display">u' = v_0 v_0^T u</script><p>由几何关系，可以计算得投影后的长度为</p><script type="math/tex; mode=display">d = ||u|| \cos \theta = ||u|| \frac{v^T u}{||u||||v||}= v_0^T u</script><p>所以在向量投影中，$u^T v_0$表示以$v_0$为基向量的坐标。</p><h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><p>现在有$N$维数据集$D=\{x^{(1)}, x^{(2)}, …, x^{(M)}\}$，其中$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, …, x^{(i)}_N\right]^T$，各维特征$D_{j}$间存在线性相关性，利用主成分分析可使</p><ul><li>数据维度降低；</li><li>提取主成分，且各成分间不相关。</li></ul><blockquote><p>说明</p><ul><li>由于选取的特征轴是正交的，所以计算结果线性无关；</li><li>提取了方差较大的几个特征，为主要线性分量。</li></ul></blockquote><p>以二维空间中的数据$x^{(i)} = \left[\begin{matrix}<br>    x^{(i)}_1 \\ x^{(i)}_2<br>\end{matrix}\right]$为例，维度可降至一维，如下图所示。<br><img src="/2018/10/22/PCA/PCA动态图.gif" alt="PCA动态图"></p><p>主轴可有无穷多种选择，那么问题就是<strong>如何选取最优的主轴</strong>。先给出<code>PCA</code>的计算步骤。</p><h2 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h2><p>输入的$M$个$N$维样本，有样本矩阵</p><script type="math/tex; mode=display">X_{N×M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right]= \left[    \begin{matrix}        x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\        x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\        ... \\        x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\    \end{matrix}\right]</script><h3 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h3><ol><li><p>对每个维度(行)进行去均值化</p><script type="math/tex; mode=display">X_j := X_j - \mu_j</script><p> 其中$\mu_j = \overline{X_j}$，$j = 1, 2, …, N$</p></li><li><p>求各维度间的协方差矩阵$\Sigma_{N×N}$</p><script type="math/tex; mode=display">\Sigma_{ij} = Cov(x_i, x_j)</script><p> 或</p><script type="math/tex; mode=display"> \Sigma = \frac{1}{M} X X^T</script></li></ol><blockquote><p>注：</p><ol><li><script type="math/tex; mode=display">X X^T = \left[           \begin{matrix}     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\     ... &     ... &     ... &     ... \\     \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_N x^{(i)}_N \end{matrix}\right]</script><script type="math/tex; mode=display">= \sum_{i=1}^M \left[           \begin{matrix}         x^{(i)}_1 x^{(i)}_1 &          x^{(i)}_1 x^{(i)}_2 &         ... &         x^{(i)}_1 x^{(i)}_N \\         x^{(i)}_2 x^{(i)}_1 &          x^{(i)}_2 x^{(i)}_2 &         ... &         x^{(i)}_2 x^{(i)}_N \\         ... &         ... &         ... &         ... \\         x^{(i)}_N x^{(i)}_1 &          x^{(i)}_N x^{(i)}_2 &         ... &         x^{(i)}_N x^{(i)}_N \end{matrix}\right]</script><script type="math/tex; mode=display">= \sum_{i=1}^M x^{(i)} x^{(i)T}</script></li><li><p>协方差定义式</p><script type="math/tex; mode=display">   Cov(x,y)≝\frac{1}{n-1} ∑_{i=1}^n (x_i−\overline{x})^T(y_i−\overline{y})</script><p>其中$x=[x_1, x_2, …, x_n]^T, y=[y_1, y_2, …, y_n]^T$</p></li></ol></blockquote><ol><li>求协方差矩阵$\Sigma$的特征值$λ_i$及其对应特征向量$α_i$，$i=1, …, N$；</li><li><p>按照特征值从大到小排列特征对$(λ_i,α_i)$，选取$K$个最大特征值对应的特征向量作为降维后的主轴$ \beta_1, \beta_2, …, \beta_K $，其中$\beta_k$为单位向量</p><script type="math/tex; mode=display">\beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^T</script><p>记</p><script type="math/tex; mode=display">B_{N×K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]</script><p>$K$的选取方法有如下两种：</p><ul><li>指定选取$K$个主轴</li><li>保留$99\%$的方差<script type="math/tex; mode=display">\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99</script></li></ul></li></ol><ol><li><p>将样本点投射到$K$维坐标系上<br> 样本$X^{(i)}$投射到主成分轴$\beta_k$上，其坐标表示为向量，为</p><script type="math/tex; mode=display"> S^{(i)}_k = X^{(i)T}\beta_k</script><blockquote><p>注意此时的基座标为$\beta_k$，或者说$X’^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$</p></blockquote><p> 所有样本在主轴$\beta_k$上的投影坐标即</p><script type="math/tex; mode=display"> S = B^T X</script><p> 其中$S_{K×M}$，$B_{N×K}$，$X_{N×M}$</p></li></ol><blockquote><p>注：若取$K=N$，可重建数据，如下<br><img src="/2018/10/22/PCA/pca_restructure1.png" alt="pca_restructure1"><br><img src="/2018/10/22/PCA/pca_restructure2.png" alt="pca_restructure2"></p></blockquote><h3 id="复原"><a href="#复原" class="headerlink" title="复原"></a>复原</h3><p>第$5$步中，样本点向量$X^{(i)}$的主要分量投射到$K$个$N$维向量上，投影坐标为$S^{(i)}_k$，即</p><script type="math/tex; mode=display">X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_k</script><p>以上就是样本点的复原公式，矩阵形式即</p><script type="math/tex; mode=display">\hat{X} = BS</script><p>其中$\hat{X}_{N×M}$，$B_{N×K}$，$S_{K×M}$</p><p>考虑到已去均值化，故</p><script type="math/tex; mode=display">\hat{X}_j \approx \hat{X}_j + \mu_j</script><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><blockquote><p>投影向量的$2$范数最大，或者说，投影后的坐标平方和最大</p></blockquote><p>当所有样本$X$投射到第一主轴$\beta_1$上，其坐标为</p><script type="math/tex; mode=display">S_1 = X^T \beta_1</script><p>所有元素的平方和，或向量$S_1$的$2$范数为</p><script type="math/tex; mode=display">||S_1||_2^2 = S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}</script><p>即优化目标为</p><script type="math/tex; mode=display">\max ||S_1||_2^2</script><script type="math/tex; mode=display">s.t.　||\beta_1||_2^2 = 1</script><p>矩阵$C=XX^T$为对称矩阵，故可单位正交化</p><script type="math/tex; mode=display">C = W \Lambda W^T</script><script type="math/tex; mode=display">W = \left[\begin{matrix}    | & & |\\    w_1 & ... & w_M\\    | & & |\\\end{matrix}\right]　\Lambda = \left[\begin{matrix}    \lambda_1 &  & \\     & ... & \\     &  & \lambda_M\\\end{matrix}\right]</script><p>其中$\lambda_1 &gt; …&gt; \lambda_M$，$w_i(i=1,…,M)$为矩阵$C$的特征向量(单位向量，互相正交)</p><blockquote><p>实际上$R(C) \leq (n-1)$，即最多有$(n-1)$个特征值大于$0$。</p></blockquote><script type="math/tex; mode=display">||S_1||_2^2= \beta_1^T W \Lambda W^T \beta_1 \tag{2}</script><p>令$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$，可得</p><script type="math/tex; mode=display">||S_1||_2^2= \alpha_1^T \Lambda \alpha_1 \tag{3}</script><p>即</p><script type="math/tex; mode=display">||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \tag{4}</script><p>进一步</p><script type="math/tex; mode=display">\sum_{i=1}^M \lambda_i \alpha_{1i}^2\leq \lambda_1 \sum_{i=1}^M \alpha_{1i}^2 \tag{5}</script><p>且由于$\beta_1^T\beta_1 = 1$，故</p><script type="math/tex; mode=display">1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = \sum_{i=1}^M \alpha_{1i}^2</script><p>可得</p><script type="math/tex; mode=display">||S_1||_2^2= \sum_{i=1}^M \lambda_i \alpha_{1i}^2\leq \lambda_1  \tag{6}</script><p>为使$(6)$取等号，即达最大值，可使</p><script type="math/tex; mode=display">\begin{cases}    \alpha_{11} = 1 \\    \alpha_{12} = ... = \alpha_{1M} = 0\end{cases}</script><p>即令</p><script type="math/tex; mode=display">\beta_1 = W \alpha_1 = w_1</script><blockquote><p>$\alpha_1 = [1, 0, …, 0]^T$</p></blockquote><p>所以$\beta_1$对应矩阵$C=XX^T$的特征向量$w_1$，且有</p><script type="math/tex; mode=display">||S_1||_2^2 = \lambda_1</script><blockquote><p>或者第一主成分的证明也可以这样，建立优化目标</p><script type="math/tex; mode=display">\beta_1 = \arg \max　||S_1||_2^2</script><script type="math/tex; mode=display">s.t.　||\beta_1||_2^2 = 1</script><p>构造拉格朗日函数</p><script type="math/tex; mode=display">L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)</script><p>也即</p><script type="math/tex; mode=display">L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)</script><p>求其极值点</p><script type="math/tex; mode=display">▽_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0</script><p>有</p><script type="math/tex; mode=display">X X^T \beta_1 = \lambda_1 \beta_1</script><p>可见$\beta_1$即方阵$X X^T$的特征向量</p></blockquote><p>当我们希望用更多的主成分刻画数据，如已经求得主成分$\beta_1, …, \beta_{r-1}$，先需求解$\beta_r$，引入正交约束$\beta_r^T \beta_i = 0$，即目标函数为</p><script type="math/tex; mode=display">||S_r||_2^2 = \beta_r^T C \beta_r</script><script type="math/tex; mode=display">s.t.　\beta_r^T \beta_i = 0, i = 1, ..., r-1</script><script type="math/tex; mode=display">||\beta_r||_2^2 = 1</script><p>令$\beta_r = W \alpha_r$，则</p><script type="math/tex; mode=display">||S_r||_2^2= \alpha_r^T \Lambda \alpha_r= \sum_i \lambda_i \alpha_{ri}^2</script><p>而根据正交约束</p><script type="math/tex; mode=display">0 = \beta_r^T \beta_i = \alpha_r^T W^T w_i = \alpha_{ri},　i = 1, ..., r-1</script><blockquote><p>$ W^T w_i = \left[0, …, 1_i, …, 0\right]^T$</p></blockquote><p>所以</p><script type="math/tex; mode=display">||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{5}</script><p>又因为$\beta_r^T \beta_r = 1$(单位向量)，故</p><script type="math/tex; mode=display">\beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1</script><p>于是类似的，为使$(5)$取最大，取</p><script type="math/tex; mode=display">\begin{cases}    \alpha_{rr} = 1\\    \alpha_{ri} = 0,　i = 1, ..., M, i \neq r\end{cases}</script><blockquote><p>$\alpha_r = [0, …, 1_r, …, 0]$</p></blockquote><p>则此时</p><script type="math/tex; mode=display">\beta_r = W \alpha_r = w_r</script><p>且有</p><script type="math/tex; mode=display">||S_r||_2^2 = \lambda_r</script><p>证毕。</p><h2 id="白化-whitening"><a href="#白化-whitening" class="headerlink" title="白化(whitening)"></a>白化(whitening)</h2><p><code>whitening</code>的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。</p><p>数据的<code>whitening</code>必须满足两个条件：</p><ol><li>不同特征间相关性最小，接近$0$；</li><li>所有特征的方差相等（不一定为$1$）。</li></ol><p>常见的白化操作有<code>PCA whitening</code>和<code>ZCA whitening</code>。</p><blockquote><p><a href="http://deeplearning.stanford.edu/wiki/index.php/Whitening" target="_blank" rel="noopener">Whitening - Ufldl</a></p></blockquote><ul><li>PCA whitening<br>  <code>PCA whitening</code>指将数据$X$经过<code>PCA</code>降维为$S$后，可以看出$S$中每一维是独立的，满足<code>whitening</code>的第一个条件，这是只需要将$S$中的每一维都除以标准差就得到了每一维的方差为$1$，也就是说方差相等。<script type="math/tex; mode=display">  X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}}</script></li></ul><ul><li>ZCA whitening<br>  <code>ZCA whitening</code>是指数据$X$先经过<code>PCA</code>变换为$S$，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足<code>whtienning</code>的第一个条件，特征间相互独立。然后同样进行方差为$1$的操作，最后将得到的矩阵左乘一个特征向量矩阵$U$即可。<script type="math/tex; mode=display">  X_{ZCAwhite} = U · X_{PCAwhite}</script></li></ul><h1 id="Kernel-PCA"><a href="#Kernel-PCA" class="headerlink" title="Kernel PCA"></a>Kernel PCA</h1><p><code>Kernel PCA</code>的思想是在高维的特征空间中求解协方差矩阵</p><script type="math/tex; mode=display">\Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><p>其中$\Phi(X^{(i)})$表示将样本$i$映射到高维空间后中的向量，即</p><script type="math/tex; mode=display">\Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^T</script><p>其中$N’ &gt; N$，由于$\Phi(X^{(i)})$为隐式的，故设置核函数求解，记</p><script type="math/tex; mode=display">\kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><blockquote><p>关于核技巧，移步<a href="">非线性支持向量机</a></p></blockquote><p><img src="/2018/10/22/PCA/kernel_pca.jpg" alt="kernel_pca"></p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>可利用<code>PCA</code>与线性回归求解$3$维空间中平面的法向量</p><ol><li>利用<code>PCA</code>重建数据(不降维，此时为$3$维)，此时第$1, 2$主成分轴可张成所求平面，即该平面可表示为<script type="math/tex; mode=display"> \Pi = span \{ \beta_1, \beta_2 \}</script></li></ol><blockquote><p>就是说，第一、二主成分是这些点“拉伸”最大的方向 :-)，好懂不？</p></blockquote><ol><li><p>由<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">正交投影</a>可知，平面外一点$y$可通过最小二乘(线性回归)的方法投射到平面上，向量运算，不考虑偏置项，即</p><script type="math/tex; mode=display"> \hat{y}  = \theta_1 x_1 + \theta_2 x_2 \tag{*}</script><p> 其中$x_1, x_2$表示第一、第二主成分$\beta_1, \beta_2$，为$3$维向量</p><script type="math/tex; mode=display"> \hat{y} = \left[     \begin{matrix}         \hat{y_1} \\         \hat{y_2} \\         \hat{y_3} \\     \end{matrix} \right]　 x_i = \left[     \begin{matrix}         x_{i1} \\         x_{i2} \\         x_{i3} \\     \end{matrix} \right]</script><p> 可利用公式求解回归参数$\theta$</p><script type="math/tex; mode=display"> \theta = (X^TX+\lambda I)^{-1} X^T y</script><blockquote><p>注意：$X(n_samples, n_features)$，这里把$(x_{1j}, x_{2j}, y_{j})作为一组样本$</p></blockquote><p> 此时该参数表示在主轴上的坐标$(\theta_1, \theta_2)$，带回$(*))$即可解得$\hat{y}$</p><script type="math/tex; mode=display"> \hat{y}  = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*}</script><p> 通俗理解，一掌把$y$拍平在了平面$\Pi$上，变成了$\hat{y}$，但是哪有这么好拍。。。这个时候刺在掌心里一定有一个垂直的向量分量，即为该平面的法向量</p><script type="math/tex; mode=display"> \vec{n} = y - \hat{y}</script><p> <strong>也可使用粗暴一点的方法，直接将第三主成分作为法向量。</strong></p><blockquote><p>或者直接上投影公式：</p><script type="math/tex; mode=display">\hat{y} = Py</script><script type="math/tex; mode=display">　P = X (X^TX+\lambda I)^{-1} X^T</script></blockquote></li></ol><pre><code>![projection](/PCA/projection.jpg)总体的运算流程如下- 利用所有样本点(近似平面)计算主成分，第一、二主成分张成平面$\Pi$；- 选出其中一个样本点，将平行于平面$\Pi$的成分投射到$\Pi$上；- 该样本点剩余分量即法向量；- 一般来说，取所有点法向量的均值。</code></pre><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-3-pca" target="_blank" rel="noopener">@Github: PCA</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PrincipalComponentAnalysis</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_component=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        self.n_component = n_component</span><br><span class="line">        self.meanVal = <span class="keyword">None</span></span><br><span class="line">        self.axis = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, prop=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        the parameter 'prop' is only for 'n_component = -1'</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 第一步: 归一化</span></span><br><span class="line">        self.meanVal = np.mean(X, axis=<span class="number">0</span>)                   <span class="comment"># 训练样本每个特征上的的均值</span></span><br><span class="line">        X_normalized = (X - self.meanVal)                   <span class="comment"># 归一化训练样本</span></span><br><span class="line">        <span class="comment"># 第二步：计算协方差矩阵</span></span><br><span class="line">        <span class="comment"># cov = X_normalized.T.dot(X_normalized)</span></span><br><span class="line">        cov = np.cov(X_normalized.T)                        <span class="comment"># 协方差矩阵</span></span><br><span class="line">        eigVal, eigVec = np.linalg.eig(cov)                 <span class="comment"># EVD</span></span><br><span class="line">        order = np.argsort(eigVal)[::<span class="number">-1</span>]                    <span class="comment"># 从大到小排序</span></span><br><span class="line">        eigVal = eigVal[order]</span><br><span class="line">        eigVec = eigVec.T[order].T</span><br><span class="line">        <span class="comment"># 选择主成分的数量</span></span><br><span class="line">        <span class="keyword">if</span> self.n_component == <span class="number">-1</span>:</span><br><span class="line">            sumOfEigVal = np.sum(eigVal)</span><br><span class="line">            sum_tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(eigVal.shape[<span class="number">0</span>]):</span><br><span class="line">                sum_tmp += eigVal[k]</span><br><span class="line">                <span class="keyword">if</span> sum_tmp &gt; prop * sumOfEigVal:            <span class="comment"># 平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值</span></span><br><span class="line">                    self.n_component = k + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 选择投影坐标轴</span></span><br><span class="line">        self.axis = eigVec[:, :self.n_component]            <span class="comment"># 选择前n_component个特征向量作为投影坐标轴</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 第一步：归一化</span></span><br><span class="line">        X_normalized = (X - self.meanVal)                   <span class="comment"># 归一化测试样本</span></span><br><span class="line">        <span class="comment"># 第二步：投影 X_nxk · V_kxk' = X'_nxk'</span></span><br><span class="line">        X_transformed = X_normalized.dot(self.axis)</span><br><span class="line">        <span class="keyword">return</span> X_transformed</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, prop=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        self.fit(X, prop=prop)</span><br><span class="line">        <span class="keyword">return</span> self.transform(X)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X_transformed)</span>:</span></span><br><span class="line">        <span class="comment"># 视投影向量长度为一个单位长度，投影结果为投影向量上的坐标</span></span><br><span class="line">        <span class="comment"># X'_nxk' · V_kxk'.T = X''_nxk</span></span><br><span class="line">        X_restructed = X_transformed.dot(self.axis.T)</span><br><span class="line">        <span class="comment"># 还原数据</span></span><br><span class="line">        X_restructed = X_restructed + self.meanVal</span><br><span class="line">        <span class="keyword">return</span> X_restructed</span><br></pre></td></tr></table></figure><p>实验结果</p><ul><li><p>Demo1: PCA applied on 2-d datasets<br>  <img src="/2018/10/22/PCA/2d_restructed.png" alt="2d_restructed"></p></li><li><p>Demo2: PCA applied on wild face</p><ul><li>origin<br><img src="/2018/10/22/PCA/face_origin.png" alt="origin"></li><li>reduced<br><img src="/2018/10/22/PCA/face_reduced.png" alt="reduced"></li><li>restructured<br><img src="/2018/10/22/PCA/face_restructed.png" alt="restructured"></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Activate Functions</title>
      <link href="/2018/10/20/Activate-Functions/"/>
      <url>/2018/10/20/Activate-Functions/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247483977&amp;idx=1&amp;sn=401b211bf72bc70f733d6ac90f7352cc&amp;chksm=fdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">SigAI 理解神经网络的激活函数</a><br><a href="https://www.cnblogs.com/silence-tommy/p/7113405.html" target="_blank" rel="noopener">机器学习笔记：形象的解释神经网络激活函数的作用是什么？ - 不说话的汤姆猫 - 博客园</a></p></blockquote><h1 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h1><h2 id="复合函数"><a href="#复合函数" class="headerlink" title="复合函数"></a>复合函数</h2><p>神经网络可以看作一个多层复合函数，以下图隐含层的激活函数为例，讲解其非线性作用。<br><img src="/2018/10/20/Activate-Functions/激活函数的非线性作用.png" alt="激活函数的非线性作用"></p><p>记激活函数为$\sigma(·)$，上图神经网络各层间具有如下关系</p><script type="math/tex; mode=display">a = \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1)</script><script type="math/tex; mode=display">b = \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2)</script><script type="math/tex; mode=display">c = \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3)</script><p>输出层采用线性单元</p><script type="math/tex; mode=display">A = w^{(2)}_{1}a + w^{(2)}_{2}b + w^{(2)}_{3}c + b^{(2)}</script><!-- 或者写作复合函数$$A = w^{(2)}_{1} \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1) +     w^{(2)}_{2} \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2) +     w^{(2)}_{3} \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3) +     b^{(2)}$$ --><p>为便于作图，固定参数</p><script type="math/tex; mode=display">W^{(1)} = \left[    \begin{matrix}        1   &  1 \\        0.1 & -1 \\        1   & -1    \end{matrix}\right],b^{(1)} = \left[    \begin{matrix}        -2  \\        1.5 \\        -1    \end{matrix}\right]W^{(2)} = \left[    \begin{matrix}        1 & 2 & 3    \end{matrix}\right],b^{(2)} = \left[    \begin{matrix}        -1    \end{matrix}\right]</script><ul><li><p>线性单元作为激活函数<br>  此时神经网络的输出为</p><script type="math/tex; mode=display">  A = (x + y - 2) +       2 (0.1x - y + 1.5) +       3 (x - y - 1)- 1</script><p>  可见仍为线性函数，做出图像如下所示<br>  <img src="/2018/10/20/Activate-Functions/Linear.png" alt="Linear"></p></li><li><p>非线性单元作为激活函数<br>  此时神经网络的输出为</p><script type="math/tex; mode=display">  A = \sigma(x + y - 2) +       2 \sigma(0.1x - y + 1.5) +       3 \sigma(x - y - 1)- 1</script><p>  激活函数选择<code>Sigmoid</code>，做出图像如下所示<br>  <img src="/2018/10/20/Activate-Functions/nonLinear.png" alt="nonLinear"></p></li></ul><h2 id="分割平面"><a href="#分割平面" class="headerlink" title="分割平面"></a>分割平面</h2><p>神经网络可实现逻辑运算，各个神经元视作分割超平面时，可分割出不同形状的平面，在线性和非线性激活函数时分割效果如图。当神经元组合的情况更复杂时，表达能力就会更强。<br><img src="/2018/10/20/Activate-Functions/激活函数的非线性作用.jpg" alt=""></p><h1 id="激活函数的性质"><a href="#激活函数的性质" class="headerlink" title="激活函数的性质"></a>激活函数的性质</h1><p>已经证明，只要激活函数选择得当，神经元个数足够多，使用3层即包含一个隐含层的神经网络就可以实现对任何一个从输入向量到输出向量的连续映射函数的逼近，这个结论称为万能逼近（universal approximation）定理。</p><blockquote><p>如果$\varphi(x)$是一个非常数、有界、单调递增的连续函数，$I_{m}$是$m$维的单位立方体，$I_{m}$中的连续函数空间为$C(I_{m})$。对于任意$\varepsilon&gt;0$以及函数$f\in C(I_{m})$，存在整数$N$，实数$v_{i},b_{i}$，实向量$w_{i}\in R^{m}$，通过它们构造函数$F(x)$作为函数$f$的逼近：</p><script type="math/tex; mode=display">F(x) = \sum_{i=1}^N v_i \varphi(w_i^T x + b_i)</script><p>对任意的$X\in I_{m}$满足：</p><script type="math/tex; mode=display">| F(x) - f(x) | < \varepsilon</script><p>Cybenko, G. Approximation by superpositions of a sigmoid function. Mathematics of Control, Signals, and Systems, 2, 303-314, 1989.</p></blockquote><p>这个定理对激活函数的要求是<strong>必须非常数、有界、单调递增，并且连续</strong>。</p><p>神经网络的训练使用梯度下降法进行求解，需要计算损失函数对参数的梯度值，涉及到计算激活函数的导数，因此激活函数必须是可导的。实际应用时并不要求它在定义域内处处可导，只要是几乎处处可导即可。</p><blockquote><p>定义$R$为一维欧氏空间，$E\subset R$是它的一个子集，$mE$为点集$E$的<strong>Lebesgue测度</strong>。如果$E$为$R$中的可测集，$f(x)$为定义在上$E$的实函数，如果存在$N\subset E$，满足：$mN=0$，对于任意的$x_{0}\in E/N$函数$f(x)$在$x_{0}$处都可导，则称$f(x)$在$E$上几乎处处可导。</p></blockquote><p>如果将激活函数输入值$x$看做是随机变量，则它落在这些不可导点处的概率是$0$。在计算机实现时，因此有一定的概率会落在不可导点处，但概率非常小。</p><blockquote><p>例如ReLU函数在$x=0$处不可导</p><script type="math/tex; mode=display">f(x) = \begin{cases}    x & x \geq 0 \\    0 & x < 0\end{cases}</script></blockquote><h1 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h1><p><img src="/2018/10/20/Activate-Functions/常用的激活函数.jpg" alt="常用的激活函数"></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Feedforward Neural Network</title>
      <link href="/2018/10/20/Feedforward-Neural-Network/"/>
      <url>/2018/10/20/Feedforward-Neural-Network/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一，既可以用于解决分类问题，也可以用于解决回归问题。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>前馈神经网络也叫作多层感知机，包含输入层，隐含层和输出层三个部分。它的目的是为了实现输入到输出的映射。</p><script type="math/tex; mode=display">y = f(x;W)</script><p>由于各层采用了非线性激活函数，神经网络具有良好的非线性特性，如下图所示。</p><ul><li>激活函数为线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/Linear.png" alt="Linear"></li><li>激活函数为非线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/nonLinear.png" alt="nonLinear"></li></ul><p>前馈神经网络可用于解决非线性的分类或回归问题，参数通过反向传播算法<code>(Back Propagation)</code>学习。</p><h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><h2 id="神经元与网络结构图"><a href="#神经元与网络结构图" class="headerlink" title="神经元与网络结构图"></a>神经元与网络结构图</h2><p>单个神经元的示意图如下，输入为前一层的输出参数$X^{(l-1)}$</p><script type="math/tex; mode=display">h_{w, b}(x) = \sigma (WX + b)</script><p>$\sigma(·)$表示激活函数。</p><p><img src="/2018/10/20/Feedforward-Neural-Network/单个神经元示意图.png" alt="单个神经元示意图"></p><p>以下为典型的神经网络结构图<br><img src="/2018/10/20/Feedforward-Neural-Network/前馈神经网络结构图.png" alt="前馈神经网络结构图"></p><ul><li>第一层为输入层<code>input layer</code>，一般不设置权值，预处理在输入网络前完成；</li><li>最后一层为输出层<code>output layer</code>；</li><li>其余层称为隐藏层<code>hidden layer</code>，隐藏层用于提取数据特征，隐藏层层数与各层神经元个数为超参数。</li></ul><blockquote><p>神经元权值取值不同，可实现不同的逻辑运算，单个超平面只能进行二元划分，利用逻辑运算可将多个超平面划分的区域拼接起来，如图<br><img src="/2018/10/20/Feedforward-Neural-Network/超平面划分区域的拼接.jpg" alt="超平面划分区域的拼接"></p><p>以下说明逻辑运算的实现方法<br><img src="/2018/10/20/Feedforward-Neural-Network/二元逻辑运算.png" alt="二元逻辑运算"><br>其中</p><script type="math/tex; mode=display">f(z) = \begin{cases}    1 & z \geq 0 \\    0 & otherwise\end{cases}</script><ul><li><p>与运算 $a ∧ b$</p><script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -30</script></li><li><p>或运算 $a ∧ b$</p><script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -10</script></li><li><p>非运算 $a = \overline{b}$</p><script type="math/tex; mode=display">w_1 = -20, w_2 = 0, b = 0</script></li><li><p>异或运算 $a \bigoplus b$，可通过组合运算实现</p><script type="math/tex; mode=display">a \bigoplus b = (\overline{a} ∧ b) ∨ (a ∧ \overline{b})</script></li></ul></blockquote><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul><li><p>隐藏层的激活函数，详情可查看<a href="https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">另一篇博文：神经网络的激活函数</a>；</p></li><li><p>输出层的激活函数</p><ul><li><p>回归问题时，采用线性单元即可</p><script type="math/tex; mode=display">  f(x) = x</script></li><li><p>分类问题时，一般有以下几种选择</p><ul><li><p>单类别概率输出<br>  即每个神经元的输出对应该类别的$0-1$分布输出，这就需要将输出值限制在$[0, 1]$内，例如</p><script type="math/tex; mode=display">P(y=1|x )= max\{0, min\{1, z\}\}</script><p>  <img src="/2018/10/20/Feedforward-Neural-Network/clf_linearout.png" alt="线性输出单元"></p><p>  但是可以看到，当$(w^Tx+b)$处于单位区间外时，模型的输出对它的参数的梯度都将为$0$ ，不利于网络的训练，故采用$S$形函数<code>Sigmoid</code>(<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)</p><script type="math/tex; mode=display">  P(y=1|x ) = \frac{1}{1+e^{-(w^Tx+b)}}</script><blockquote><p>$(1)$ <code>Sigmoid</code>函数定义域为$(-\infty, \infty)$，值域为$(0, 1)$，且在整个定义域上单调递增，即为单值函数，故可将线性输出单元的结果映射到$(0, 1)$范围内；<br>$(2)$ 在定义域上处处可导。</p></blockquote></li><li><p>多类别的概率输出<br>  即每个神经元的输出对应判别为该类别的概率，且有</p><script type="math/tex; mode=display">  \sum_{i=1}^C y_i = 1</script><p>  例如</p><script type="math/tex; mode=display">  y_i = \frac{z_i}{\sum_j z_j}</script><p>  但是分式求导异常麻烦，故采用<code>Softmax</code>函数(<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)作为输出结点的激活函数，该函数求导结果比较简洁，且可利用输出计算导数，计算量减少。</p><script type="math/tex; mode=display">  Softmax(x) = \frac              {1}              {\sum_{k=1}^K exp(x_k)}              \left[                  \begin{matrix}                      exp(x_1)\\                      exp(x_2)\\                      ...\\                      exp(x_K)                  \end{matrix}              \right]</script></li></ul></li></ul></li></ul><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><ul><li><p>回归问题<br>  常见的用于回归问题的损失函数为<code>MSE</code>，即</p><script type="math/tex; mode=display">  L(y, \hat{y}) = \frac{1}{2M} \sum_{i=1}^M (\hat{y}^{(i)} - y^{(i)})^2</script></li><li><p>分类问题<br>  一般采用交叉熵作为损失函数，如下</p><script type="math/tex; mode=display">  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M 1\{y^{(i)}_j=k\}   \log (\hat{y}^{(i)}_j)</script><script type="math/tex; mode=display">  1\{y^{(i)}_j=k\} =       \begin{cases}          1 & y^{(i)}_j = k \\          0 & y^{(i)}_j \neq k       \end{cases}　j = 1, ..., N</script><p>  或者</p><script type="math/tex; mode=display">  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M   y^{(i)T} \log (\hat{y}^{(i)})</script><p>  其中$y^{(i)}, \hat{y}^{(i)}$均表示向量，采用<code>one-hot</code>编码。</p></li></ul><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><p>以上内容网上资料一大堆，进入重点，反向传播时的梯度推导，给出网络结构如下。</p><ul><li>回归与分类在输出层有所区别；</li><li>各层激活函数的输入变量以$z^{(l)}$表示，输出变量均以$x^{(l)}$表示；</li><li>$W^{(l)}$表示从第$l$层到第$(l+1)$层的权值矩阵，则$w^{(l)}_{ij}$表示第$l$层第$j$个神经元到$(l+1)$层第$i$个神经元的连接权值；</li><li>$b^{(l)}$表示第$l$层到第$(l+1)$层的偏置，则$b^{(l)}_i$表示到第$(l+1)$层第$i$个神经元的偏置值；</li><li>各层变量维度推广为输入$d_{i}$，中间层$d_{h}$，输出层$d_{o}$；</li><li>全连接，部分线条已省略，激活函数已省略；</li></ul><p><img src="/2018/10/20/Feedforward-Neural-Network/fnn.jpg" alt="FNN"></p><p>则各层参数矩阵为</p><script type="math/tex; mode=display">W^{(1)} = \left[        \begin{matrix}            w^{(1)}_{11} & ... & w^{(1)}_{1d_i} \\            ... & ... & ... \\            w^{(1)}_{d_h1} & ... & w^{(1)}_{d_hd_i}        \end{matrix}\right]　b^{(1)} = \left[        \begin{matrix}            b^{(1)}_{1} \\            ... \\            b^{(1)}_{d_h}        \end{matrix}\right]</script><script type="math/tex; mode=display">W^{(2)} = \left[        \begin{matrix}            w^{(2)}_{11} & ... & w^{(2)}_{1d_h} \\            ... & ... & ... \\            w^{(2)}_{d_o1} & ... & w^{(2)}_{d_od_h}        \end{matrix}\right]　b^{(2)} = \left[        \begin{matrix}            b^{(2)}_{1} \\            ... \\            b^{(2)}_{d_o}        \end{matrix}\right]</script><p>有</p><script type="math/tex; mode=display">Z^{(2)} = W^{(1)} X^{(1)} + b^{(1)}</script><script type="math/tex; mode=display">X^{(2)} = \sigma_1 (Z^{(2)})</script><script type="math/tex; mode=display">Z^{(3)} = W^{(2)} X^{(2)} + b^{(2)}</script><script type="math/tex; mode=display">X^{(3)} = \sigma_2 (Z^{(3)})</script><script type="math/tex; mode=display">X^{(1)} = X　\hat{Y} = X^{(3)}</script><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>损失函数采用<code>MSE</code>，即</p><script type="math/tex; mode=display">L(Y, \hat{Y}) = \frac{1}{M} \sum_{i=1}^M L(Y^{(i)}, \hat{Y}^{(i)})</script><script type="math/tex; mode=display">L(Y^{(i)}, \hat{Y}^{(i)}) = \frac{1}{2} || \hat{Y}^{(i)} - Y^{(i)} ||_2^2= \frac{1}{2} \sum_{d_2=1}^{d_o}(\hat{y}^{(i)}_{d_2} - y^{(i)}_{d_2})^2</script><p>下面推导单个样本的损失函数的梯度，该批数据的梯度为均值。</p><blockquote><p>省略样本标记<code>$^{(i)}$</code></p></blockquote><ul><li><p>隐含层到输出层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = \frac{∂}{∂w^{(2)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2} \tag{1}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}  \end{cases}</script><p>  且</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)}) \frac{∂z_{d_2}^{(3)}}{∂w^{(2)}_{ij}} \tag{2}</script><script type="math/tex; mode=display">  \frac{∂}{∂w^{(2)}_{ij}} z_{d_2}^{(3)} =       \begin{cases}          x^{(2)}_{d_1} & d_1 = j, d_2 = i \\          0 & otherwise      \end{cases} \tag{3}</script><p>  $(3)$代入$(2)$，再代入$(1)$可得到</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) x^{(2)}_{d_1} | _{d_1=j, d_2=i}  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \tag{*1}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i}  = \frac{∂}{∂b^{(2)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(2)}_i} \hat{y}_{d_2} \tag{4}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}  \end{cases}</script><p>  有</p><script type="math/tex; mode=display">  \frac{∂}{∂b^{(2)}_i} z_{d_2}^{(3)} =       \begin{cases}          1 &  d_2 = i \\          0 & otherwise      \end{cases} \tag{5}</script><p>  所以</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) | _{d_2=i}  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \tag{*2}</script></li></ul></li><li><p>输入层到隐含层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = \frac{∂}{∂w^{(1)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2} \tag{6}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \\      x^{(2)}_{d_1} = \sigma_1 (z_{d_1}^{(2)}) \\      z_{d_1}^{(2)} = \sum_{d_0=1}^{d_i} w^{(1)}_{d_1 d_0} x^{(1)}_{d_0} + b^{(1)}_{d_1}  \end{cases}</script><p>  故</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}  = \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}}       \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \tag{7}</script><p>  其中</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}}   = \sigma_2' (z_{d_2}^{(3)})  \tag{8}</script><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}   = \sum_{d1=1}^{d_h} w^{(2)}_{d_2 d_1} \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} \tag{9}</script><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}}  = \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}}      \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}}  \tag{10}</script><p>  而其中</p><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}} = x^{(2)}_{d_1} = \sigma_1' (z_{d_1}^{(2)}) \tag{11}</script><script type="math/tex; mode=display">  \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}} =   \begin{cases}      x^{(1)}_{d_0} & d_1 = i, d_0 = j\\      0 & otherwise  \end{cases} \tag{12}</script><p>  $(11),(12)$代入$(10)$得到</p><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} =   \sigma_1' (z_{d_1}^{(2)})      x^{(1)}_{d_0} | _{d_1 = i, d_0 = j} \tag{13}</script><p>  $(13)$代回$(9)$，有</p><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}   = \sum_{d1=1}^{d_h}   \left[      w^{(2)}_{d_2 d_1}       \sigma_1' (z_{d_1}^{(2)})      x^{(1)}_{d_0}  \right] | _{d_1 = i, d_0 = j}</script><script type="math/tex; mode=display">  = w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{14}</script><p>  将$(8),(14)$代入$(7)$得到</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{15}</script><p>  $(15)$代入$(6)$有</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = \sum_{d_2=1}^{d_o}       (\hat{y}_{d_2} - y_{d_2})            \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{*3}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i}  = \frac{∂}{∂b^{(1)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2} \tag{16}</script><p>  同理可得</p><script type="math/tex; mode=display">  \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})  \tag{17}</script><p>  所以</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i} =  \sum_{d_2=1}^{d_o}       (\hat{y}_{d_2} - y_{d_2})            \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)}) \tag{*4}</script></li></ul></li></ul><p>综上所述</p><script type="math/tex; mode=display">\frac{∂L}{∂w^{(2)}_{ij}}= (\hat{y}_{i} - y_{i})     \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j}</script><script type="math/tex; mode=display">\frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{i} - y_{i})     \sigma_2' (z_i^{(3)})</script><script type="math/tex; mode=display">\frac{∂L}{∂w^{(1)}_{ij}}= \sum_{d_2=1}^{d_o}     (\hat{y}_{d_2} - y_{d_2})          \sigma_2' (z_{d_2}^{(3)})     w^{(2)}_{d_2 i}     \sigma_1' (z_i^{(2)})    x^{(1)}_j</script><script type="math/tex; mode=display">\frac{∂L}{∂b^{(1)}_i} = \sum_{d_2=1}^{d_o}     (\hat{y}_{d_2} - y_{d_2})          \sigma_2' (z_{d_2}^{(3)})     w^{(2)}_{d_2 i}     \sigma_1' (z_i^{(2)})</script><p>令</p><script type="math/tex; mode=display">\begin{cases}    \delta^{(2)}_i     = (\hat{y}_{i} - y_{i})         \sigma_2' (z_i^{(3)}) \\    \delta^{(1)}_i     = \sum_{d_2=1}^{d_o}         \delta^{(2)}_{d_2}         w^{(2)}_{d_2 i}         \sigma_1' (z_i^{(2)})\end{cases}</script><p>有</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂L}{∂w^{(2)}_{ij}} = \delta^{(2)}_i x^{(2)}_{j}\\    \frac{∂L}{∂b^{(2)}_i}    = \delta^{(2)}_i\\    \frac{∂L}{∂w^{(1)}_{ij}} = \delta^{(1)}_i x^{(1)}_j\\    \frac{∂L}{∂b^{(1)}_i}    = \delta^{(1)}_i\end{cases}</script><p>至此推导完毕。</p><blockquote><p>当隐藏层采用<code>Sigmoid</code>函数，输出层采用线性单元，可得到</p><script type="math/tex; mode=display">\sigma_1' (z_i^{(2)}) = \sigma_1 (z_i^{(2)})     \left[1 - \sigma_1 (z_i^{(2)}) \right]= x_i^{(2)} (1 - x_i^{(2)})</script><script type="math/tex; mode=display">\sigma_2' (z_i^{(3)}) = z_i^{(3)}</script><p>此时</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) z_i^{(3)} x^{(2)}_{j} \\    \frac{∂L}{∂b^{(2)}_i}    = (\hat{y}_{i} - y_{i}) z_i^{(3)} \\    \frac{∂L}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} x^{(1)}_j \\    \frac{∂L}{∂b^{(1)}_i}    = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)}\end{cases}</script><p>可以看到，计算梯度时使用的数据在上一次前向传播时已计算得，故可减少计算量。</p></blockquote><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>损失函数采用<code>Cross Entropy</code>，即</p><script type="math/tex; mode=display">L(\hat{y}, y) = \frac{1}{M} \sum_{i=1}^M L(\hat{y}^{(i)}, y^{(i)})</script><script type="math/tex; mode=display">L(\hat{y}^{(i)}, y^{(i)}) = - y^{(i)T} \log (\hat{y}^{(i)})</script><p>上式中，$y^{(i)}, \hat{y}^{(i)}$均为列向量，且$y^{(i)}$表示<code>one-hot</code>编码后的标签向量，也可写作</p><script type="math/tex; mode=display">L(\hat{y}^{(i)}, y^{(i)})= - \log \hat{y}^{(i)}_{y^{(i)}}</script><ul><li>由该式可以看出，若输出层激活函数采用<code>Sigmoid</code>作为激活函数，则隐藏层——输出层之间权值矩阵$W^{(2)}$只会更新$w^{(i)}_{y^{(i)} d_1}, d_1 = 1, …, d_h$；</li><li>一般采用<code>SoftMax</code>作为输出层激活函数，<code>Sigmoid</code>下面不作推导。</li></ul><blockquote><p>关于<code>SoftMax</code>的梯度，移步<a href="https://louishsu.xyz/2018/10/18/Softmax-Regression/" target="_blank" rel="noopener">SoftMax Regression</a>中查看详细推导过程，这里直接给出结论。<br>对于</p><script type="math/tex; mode=display">S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]</script><p>其梯度为</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i}_{K×1} =  \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] -  \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right]= \left( \left[ \begin{matrix}  0 \\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i</script><p>省略样本标记<code>$^{(i)}$</code></p></blockquote><ul><li><p>隐含层到输出层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = - \frac{∂}{∂w^{(2)}_{ij}} \log \hat{y}_{y}  = - \frac{1}{\hat{y}_y}       \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}} \tag{18}</script><p>  其中$\hat{y}_{y}$与$z^{(3)}_{d_2}(d_2 = 1, …, d_o) $均有联系，故</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}}  = \sum_{d2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}} \tag{19}</script><p>  而</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}  = \begin{cases}      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\      - \hat{y}_{y} \hat{y}_{d_2} & otherwise  \end{cases}</script><script type="math/tex; mode=display">  \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}}  = \begin{cases}      x^{(2)}_{d_1} & i = d_2, j = d_1 \\      0 & otherwise  \end{cases}</script><blockquote><p>$z^{(3)}_{d_2} = \sum_{d_1=0}^{d_h} w^{(2)}_{d_2d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}$</p></blockquote><p>  代回$(19)$，再带回$(18)$，有</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = - \frac{1}{\hat{y}_{y}}       \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       x^{(2)}_{d_1} | _{d_2=i, d_1=j}</script><script type="math/tex; mode=display">  = \begin{cases}      - \frac{1}{\hat{y}_{y}} \hat{y}_{y} (1 - \hat{y}_i) x^{(2)}_j & i = y \\      - \frac{1}{\hat{y}_{y}} (- \hat{y}_{y} \hat{y}_i) x^{(2)}_j & otherwise  \end{cases}</script><script type="math/tex; mode=display">  = \begin{cases}      (\hat{y}_i - 1) x^{(2)}_j & i = y \\      \hat{y}_i x^{(2)}_j & otherwise  \end{cases}</script><p>  即</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = (\hat{y}_i - y_i) x^{(2)}_j \tag{*5}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i}  = \hat{y}_i - y_i \tag{*6}</script></li></ul></li><li><p>输入层到隐含层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = - \frac{∂}{∂w^{(1)}_{ij}} \log \hat{y}_{y}  = - \frac{1}{\hat{y}_{y}}       \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}} \tag{20}</script><p>  其中</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}}  = \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}} \tag{21}</script><p>  $\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}$部分与回归相同，有</p><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}  = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><p>  由上面分析可得</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}  = \begin{cases}      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\      - \hat{y}_{y} \hat{y}_{d_2} & otherwise  \end{cases}</script><p>  故代回$(20)$可得到</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = - \frac{1}{\hat{y}_{y}}      \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}</script><script type="math/tex; mode=display">  = - \frac{1}{\hat{y}_{y}}      \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">  = \left[       \sum_{d_2=1, d_2 \neq y}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} +       (\hat{y}_y - 1) w^{(2)}_{y i}   \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">  = \left[       \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} -       w^{(2)}_{y i}  \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*7}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i}  = \left[       \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} -       w^{(2)}_{y i}  \right] \sigma_1' (z_i^{(2)}) \tag{*8}</script></li></ul></li></ul><p>至此推导完毕。</p><blockquote><p>这个推导，仅供参考</p></blockquote><h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>和其他算法一样，前馈神经网络也存在过拟合的问题，解决方法有以下几种</p><ul><li><p>正则化<br>  与线性回归类似，神经网络也可以加入范数惩罚项，以下$C$表示普通的损失函数，$\lambda$为惩罚系数，$n$为样本数目，$w$表示权值参数。</p><ul><li><code>L1</code>正则化<br>  惩罚项为网络所有权值的绝对值之和。<script type="math/tex; mode=display">  C = C_0 + \frac{\lambda}{n} \sum_w |w|</script></li><li><code>L2</code>正则化<br>  又称权值衰减<code>weights decay</code>，惩罚项为网络所有权值的平方和。<script type="math/tex; mode=display">  C = C_0 + \frac{\lambda}{2n} \sum_w w^2</script></li></ul></li><li><p>Dropout<br>  以概率大小为<code>p</code>使部分神经元输出值直接为0，如此可以使反向传播时相关权值系数不做更新，只有被保留下来的权值和偏置值会被更新。<br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_1.png" alt="dropout_1"><br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_2.png" alt="dropout_2"></p></li><li><p>增加训练数据大小<br>  可在原数据上加以变换或噪声，图像的扩增方法可查看<a href="https://louishsu.xyz/2018/11/02/%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%89%A9%E5%A2%9E-Augment-%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">图像数据集扩增</a>。</p></li></ul><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-PyTorch-Tutorial/blob/master/NeuralNetwork_ANN_MNIST.py" target="_blank" rel="noopener">@Github: Code of Neural Network</a></p><p>使用<code>PyTorch</code>实现神经网络，以下为模型定义<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnnNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AnnNet, self).__init__()</span><br><span class="line">        self.input_size = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">        self.hidden_size = <span class="number">100</span></span><br><span class="line">        self.output_size = <span class="number">10</span></span><br><span class="line">        self.fc1 = nn.Linear(self.input_size,  self.hidden_size)    <span class="comment"># input   - hidden</span></span><br><span class="line">        self.fc2 = nn.Linear(self.hidden_size, self.output_size )   <span class="comment"># hidden  - output</span></span><br><span class="line">        <span class="comment"># self.activate = nn.Sigmoid()  # 参数更新非常慢，特别是层数多时</span></span><br><span class="line">        self.activate = nn.ReLU()       <span class="comment"># 事实证明ReLU作为激活函数更加合适</span></span><br><span class="line">        self.softmax = nn.Softmax()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        h = self.activate(self.fc1(X))</span><br><span class="line">        y_pred = self.softmax(self.fc2(h))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>分类问题的决策平面</title>
      <link href="/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/"/>
      <url>/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>对于分类问题，计算结果一般为概率值，那么如何根据计算得的概率进行判别分类呢？</p><blockquote><p>这部分理解后，<a href="https://louishsu.xyz/2018/10/18/Logistic-Regression/" target="_blank" rel="noopener">Logistic回归</a>与<a href="https://louishsu.xyz/2018/10/18/Softmax-Regression/" target="_blank" rel="noopener">Softmax回归</a>的模型就很容易推得。</p></blockquote><h1 id="判别函数"><a href="#判别函数" class="headerlink" title="判别函数"></a>判别函数</h1><p>对于一个类别为$K$的分类问题，如果对于所有的$ i,j=1,…,K, j\neq i$，有</p><script type="math/tex; mode=display">g_i(x) > g_j(x)</script><p>则此分类器将这个样本对应的特征向量$x$判别为$w_i$，则此分类器的作用是，计算$K$个判别函数并选取与最大判别值最大对应的类别。</p><blockquote><p>判别函数的形式并不唯一，可以将所有的判别函数乘上相同的正常数或者加上一个相同的常量而不影响其判决结果。更一般的情况下，我们使用单调递增函数$f(·)$进行映射，将每一个$g_i(x)$替换成$f(g_i(x))$，分类结果不变。</p><div style="text-align: right"> ——《模式识别原理与应用课程笔记》</div></blockquote><p>例如<a href="https://louishsu.xyz/2018/10/18/Bayes-Decision/" target="_blank" rel="noopener">最小风险贝叶斯决策</a></p><h1 id="正态分布下的判别函数"><a href="#正态分布下的判别函数" class="headerlink" title="正态分布下的判别函数"></a>正态分布下的判别函数</h1><blockquote><p><a href="https://www.cnblogs.com/bingjianing/p/9117330.html" target="_blank" rel="noopener">多元高斯分布（The Multivariate normal distribution） - bingjianing - 博客园</a></p></blockquote><p>由大数定理可知，在样本足够的情况下，数据服从正态分布。多元正态分布形式如下</p><script type="math/tex; mode=display">f(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))</script><p>其中</p><script type="math/tex; mode=display">x = [x_1, ..., x_n]^T</script><script type="math/tex; mode=display">\mu = [\mu_1, ..., \mu_n]^T</script><script type="math/tex; mode=display">\Sigma_{ij} = cov(x_i, x_j)</script><!-- $$\Sigma =  \left[​    \begin{matrix}​        1 & 2 & 3 \\​        4 & 5 & 6 \\​        7 & 8 & 9​    \end{matrix}\right] \tag{3}$$ --><p>在<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">最小错误率判别</a>时</p><script type="math/tex; mode=display">g_i(x) = P(x|c_i)P(c_i)</script><p>即</p><script type="math/tex; mode=display">g_i(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}}exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i)) ·P(c_i)</script><p>取对数运算，并舍去常数项，展开整理得</p><script type="math/tex; mode=display">g_i(x) = -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x  -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i) \tag{0}</script><blockquote><p><code>注：</code> 协方差矩阵 $ \Sigma^T = \Sigma $</p></blockquote><h2 id="1-Sigma-i-sigma-2-I"><a href="#1-Sigma-i-sigma-2-I" class="headerlink" title="1. $\Sigma_i = \sigma^2 I$"></a>1. $\Sigma_i = \sigma^2 I$</h2><p>$\Sigma_i^{-1} = \frac{1}{\sigma^2} I$代入$(0)$，有</p><script type="math/tex; mode=display">g_i(x) = \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)\tag{1}</script><p>定义</p><script type="math/tex; mode=display">w_i = \frac{1}{\sigma^2}\mu_i^T</script><script type="math/tex; mode=display">w_0 =  - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)</script><p>有一般形式如下，表示取$c_i$的概率</p><script type="math/tex; mode=display">g_i(x) = w_i x + w_0\tag{2}</script><p>设决策平面为</p><script type="math/tex; mode=display">w^T (x−x_0)=0\tag{3}</script><p>决策平面上，取$c_i$和$c_j$的概率相等，即</p><script type="math/tex; mode=display">g_i(x) = g_j(x)</script><p>可得</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} \tag{4}</script><blockquote><p>推导过程如下，将$(1)$代入上式<br>$ \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i) = \frac{1}{\sigma^2}\mu_j^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_j ^T \mu_j) + ln P(c_j) $<br>$ \mu_i^T x - \frac{1}{2} \mu_i ^T \mu_i + ln P(c_i) = \mu_j^T x - \frac{1}{2} \mu_j ^T \mu_j + ln P(c_j) $<br>$ (\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} $</p></blockquote><p>由$(3)$$(4)$，利用待定系数法，可得</p><script type="math/tex; mode=display">w = \mu_i - \mu_j</script><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)}</script><p>特别地，当等先验概率时，即$P(c_i) = P(c_j)$时</p><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j)</script><p>故</p><script type="math/tex; mode=display">x_0 = \frac{1}{2}(\mu_i + \mu_j)</script><p>结论：等先验概率时超平面$ w^T (x−x_0)=0 $平分判别空间</p><blockquote><p>$\mu_i$与$\mu_j$分别表示两个类别的中心，由向量运算，$x_0$为两类中心的连线的中点。</p></blockquote><h2 id="2-Sigma-i-Sigma"><a href="#2-Sigma-i-Sigma" class="headerlink" title="2. $\Sigma_i = \Sigma$"></a>2. $\Sigma_i = \Sigma$</h2><p>代入$(0)$后可得</p><script type="math/tex; mode=display">g_i(x) =  \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) \tag{5}</script><p>定义</p><script type="math/tex; mode=display">w_i = \mu_i^T \Sigma ^{-1}</script><script type="math/tex; mode=display">w_0 = - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i)</script><p>有一般形式如下，表示取$c_i$的概率</p><script type="math/tex; mode=display">g_i(x) = w_i x + w_0\tag{6}</script><p>同样的，设决策平面为</p><script type="math/tex; mode=display">w^T (x−x_0)=0\tag{7}</script><p>决策平面上，取$c_i$和$c_j$的概率相等，即</p><script type="math/tex; mode=display">g_i(x) = g_j(x)</script><p>有</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i - \mu_j)^T \Sigma ^{-1} (\mu_i - \mu_j) - ln \frac{P(c_i)}{P(c_j)}</script><blockquote><p>$ \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $<br>$ \mu_i^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $<br>$ (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j) - ln \frac{P(c_i)}{P(c_j)} $</p></blockquote><p>特别的，当取等先验概率时</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)</script><p>由$(7)$$(8)$，利用待定系数法</p><script type="math/tex; mode=display">w^T = (\mu_i - \mu_j)^T \Sigma^{-1}</script><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)</script><blockquote><p><code>注：</code> 协方差矩阵 $ \Sigma^T = \Sigma $</p></blockquote><script type="math/tex; mode=display">w = \Sigma^{-1}(\mu_i - \mu_j)</script><script type="math/tex; mode=display">x_0 = \frac{1}{2} (\mu_i + \mu_j)</script><p>由于通常$w=Σ^{−1}(μ_i−μ_j)$并非朝着$(μ_i−μ_j)$的方向，因而通常分离两类的超平面也并非与均值的连线垂直正交。但是， 如果先验概率相等，其判定面确实是与均值连线交于中点$x_0$处的。如果先验概率不等，最优边界超平面将远离可能性较大的均值。同前，如果偏移量足够大，判定面可以不落在两个均值向量之间。</p><h2 id="3-Sigma-i-Sigma-i-∀"><a href="#3-Sigma-i-Sigma-i-∀" class="headerlink" title="3. $\Sigma_i = \Sigma_i(∀) $"></a>3. $\Sigma_i = \Sigma_i(∀) $</h2><script type="math/tex; mode=display">g_i(x) =  -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x  -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)</script><p>定义</p><script type="math/tex; mode=display">W_i = -\frac{1}{2} \Sigma_i ^{-1}</script><script type="math/tex; mode=display">w_i = \mu_i^T \Sigma_i ^{-1}</script><script type="math/tex; mode=display">w_0 = -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)</script><p>有</p><script type="math/tex; mode=display">g_i(x) = x^TW_ix + w_ix + w_0</script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Bayes Decision</title>
      <link href="/2018/10/18/Bayes-Decision/"/>
      <url>/2018/10/18/Bayes-Decision/</url>
      
        <content type="html"><![CDATA[<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>基于贝叶斯公式</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><script type="math/tex; mode=display">P(x)=\sum_j p(x|c_j)P(c_j)</script><h1 id="几种常用的贝叶斯决策"><a href="#几种常用的贝叶斯决策" class="headerlink" title="几种常用的贝叶斯决策"></a>几种常用的贝叶斯决策</h1><h2 id="最小错误率贝叶斯决策"><a href="#最小错误率贝叶斯决策" class="headerlink" title="最小错误率贝叶斯决策"></a>最小错误率贝叶斯决策</h2><p>在分类问题中，我们往往希望尽可能减少分类错误，即目标是追求最小错误率。假设有$K$分类问题，由贝叶斯公式</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><p>上式中$ k=1,…,K  $，各部分定义如下</p><blockquote><p>$P(c_k|x)$——<code>后验概率(posteriori probability)</code><br>$P(c_k)$——<code>先验概率(priori probability)</code>，<br>$p(x|c_k)$——$c_k$关于$x$的<code>似然函数(likelihood)</code>，<br>$p(x)$——<code>证据因子(evidence)</code>，</p></blockquote><p>证据因子由下式计算</p><script type="math/tex; mode=display">p(x)=\sum_{j=0}^K p(x|c_j)P(c_j)</script><p>以上就是从样本中训练的参数，在预测阶段，定义决策规则为</p><blockquote><p>$if$ $P(c_i|x)&gt;P(c_j|x)$, $then$ $ x \in c_i $</p></blockquote><p>由于分母为标量，对于任意输入的样本特征$x$，$P(x)$一定，故决策规则可简化为</p><blockquote><p>$if$ $P(x|c_i)P(c_i)&gt;P(x|c_j)P(c_j)$, $then$ $ x \in c_i $</p></blockquote><p>而对于分类错误的样本，如样本$x$属于分类$c_i$，但错误分类为$c_{err}, err \neq i$，样本的错误分类概率为</p><script type="math/tex; mode=display">P(error|x) = P(c_{err}|x)</script><p>上式被称作<code>误差概率</code>，某类后验概率越大，则相应的误差概率就越小，定义平均误差概率</p><script type="math/tex; mode=display">P_{mean} = \int P(error|x)P(x)dx</script><h2 id="带有拒绝域的最小错误率贝叶斯决策"><a href="#带有拒绝域的最小错误率贝叶斯决策" class="headerlink" title="带有拒绝域的最小错误率贝叶斯决策"></a>带有拒绝域的最小错误率贝叶斯决策</h2><p>一些情况下，某样本对应特征$x$计算结果中，属于各类别的概率没有显著比较大的数值，换句话说都比较小，那么对这次的判别就不太信任，选择拒绝决策结果。<br>将决策平面划分为两个区域</p><script type="math/tex; mode=display">Acquired = \{x|max_j P(c_j|x)\geq 1-t\}</script><script type="math/tex; mode=display">Rejected = \{x|max_j P(c_j|x) < 1-t\}</script><p>其中$t$为阈值，$t$越小时，拒绝域$Rejected$越大，当满足</p><script type="math/tex; mode=display">1-t \leq \frac{1}{K}</script><p>或者 </p><script type="math/tex; mode=display">t \geq \frac{K-1}{K}</script><p>此时拒绝域为</p><script type="math/tex; mode=display">Rejected = \{x|max_j P(c_j|x) < \frac{1}{K}\}</script><p>而当且仅当各分类概率相等时才有 $ max_j P(c_j|x) = \frac{1}{K} $，因此此时拒绝域为空，接受所有决策结果</p><h2 id="最小风险贝叶斯决策"><a href="#最小风险贝叶斯决策" class="headerlink" title="最小风险贝叶斯决策"></a>最小风险贝叶斯决策</h2><p>在决策过程中，不同类型的决策错误所产生的代价是不同的。引入风险函数</p><script type="math/tex; mode=display">\lambda_{i, j} = \lambda (\alpha_i|c_j)</script><p>表示实际类别为$c_j$时，采取错误判断为$c_i$的行为$\alpha_i$所产生的损失。该函数称为损失函数，通常它可以用表格的形式给出，叫做决策表，形如<br><img src="/2018/10/18/Bayes-Decision/decision_table.jpg" alt="决策表"><br>定义条件风险</p><script type="math/tex; mode=display">R(\alpha_i|c_j) = \sum_j \lambda (\alpha_i|c_j) P(c_j|x)</script><p>特别地，取$0-1$损失时，即最小错误率贝叶斯决策</p><script type="math/tex; mode=display">\lambda (\alpha_i|c_j) = \begin{cases}0 & i = j \\1 & i \neq j\end{cases}</script><!-- $$函数名 = \begin{cases}公式1 & 条件1 \\公式2 & 条件2 \\公式3 & 条件3 \end{cases}$$--><p>可能比较抽象，这里举了一个例子</p><div style="align: center"><img src="/2018/10/18/Bayes-Decision/最小风险贝叶斯决策例.png"></div><h1 id="关于判别函数"><a href="#关于判别函数" class="headerlink" title="关于判别函数"></a>关于判别函数</h1><p>可查看<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a></p><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><img src="/2018/10/18/Bayes-Decision/李航-例4.1.png" alt="例4.1"></p><p>为帮助理解，先手动计算一遍结果</p><blockquote><p>先验概率(<code>priori probability</code>):<br>$ P(Y = -1) = \frac{6}{15} $<br>$ P(Y = 1) = \frac{9}{15} $<br>似然函数(<code>likelihood</code>)<br>$ P(X^{(1)} = 1|Y=-1) = \frac{3}{6}$<br>$ P(X^{(1)} = 2|Y=-1) = \frac{2}{6}$<br>$ P(X^{(1)} = 3|Y=-1) = \frac{1}{6}$<br>$ P(X^{(2)} = S|Y=-1) = \frac{3}{6}$<br>$ P(X^{(2)} = M|Y=-1) = \frac{2}{6}$<br>$ P(X^{(2)} = L|Y=-1) = \frac{1}{6}$<br>$ P(X^{(1)} = 1|Y=1) = \frac{2}{9}$<br>$ P(X^{(1)} = 2|Y=1) = \frac{3}{9}$<br>$ P(X^{(1)} = 3|Y=1) = \frac{4}{9}$<br>$ P(X^{(2)} = S|Y=1) = \frac{1}{9}$<br>$ P(X^{(2)} = M|Y=1) = \frac{4}{9}$<br>$ P(X^{(2)} = L|Y=1) = \frac{4}{9}$</p></blockquote><p>注意：证据因子(<code>evidence</code>)不能用如下朴素贝叶斯求解</p><script type="math/tex; mode=display">P(X) = P(X^{(1)}) P(X^{(2)})</script><p>而是</p><script type="math/tex; mode=display">P(X) =  P(X^{(1)}|Y=-1)P(Y = -1) + P(X^{(2)}|Y=-1)P(Y = -1)</script><p>一般分子用朴素贝叶斯求解</p><script type="math/tex; mode=display">P(X|Y) = P(X^{(1)}|Y) P(X^{(2)}|Y)</script><p>将其加和作为分母</p><script type="math/tex; mode=display">c_k: P(X)_k = \sum_{k=0}^2 P(X^{(1)}|Y=k) P(X^{(2)}|Y=k)</script><script type="math/tex; mode=display">P(Y_k|X) = \frac{P(X|Y_k)P(Y_k)}{P(X)_k}</script><p>选取最大概率的$ k $类别作为判别类别</p><script type="math/tex; mode=display">k = argmax_k P(Y_k|X)</script><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Statistical%20Learning%20Method%2C%20Li%20Hang/naive_bayes_algorithm_demo.py" target="_blank" rel="noopener">@Github: Code for Naive Bayes Decision</a></p><h3 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    X_encoded = self.featureEncoder.fit_transform(X).toarray()</span><br><span class="line">    y_encoded = OneHotEncoder().fit_transform(y.reshape((<span class="number">-1</span>, <span class="number">1</span>))).toarray()</span><br><span class="line">    self.P_X = np.mean(X_encoded, axis=<span class="number">0</span>)                           <span class="comment"># one-hot编码下，各列的均值即各特征的概率</span></span><br><span class="line">    self.P_Y = np.mean(y_encoded, axis=<span class="number">0</span>)                           <span class="comment"># one-hot编码下，各列的均值即各了别的概率</span></span><br><span class="line">    self.n_labels, self.n_features = y_encoded.shape[<span class="number">1</span>], X_encoded.shape[<span class="number">1</span>]   </span><br><span class="line">    self.P_X_Y = np.zeros(shape=(self.n_labels, self.n_features))   <span class="comment"># 各个类别下，分别统计各特征的概率</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_labels):</span><br><span class="line">        X_encoded_of_yi = X_encoded[y_encoded[:, i]==<span class="number">1</span>]             <span class="comment"># 取出属于i类别的样本</span></span><br><span class="line">        self.P_X_Y[i] = np.mean(X_encoded_of_yi, axis=<span class="number">0</span>)            <span class="comment"># one-hot编码下，各列的均值即各特征的概率</span></span><br></pre></td></tr></table></figure><h3 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    X_encoded = self.featureEncoder.transform(X).toarray()</span><br><span class="line">    n_samples = X_encoded.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred_prob = np.zeros(shape=(n_samples, self.n_labels))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_labels):</span><br><span class="line">            P_Xi_encoded_Yj = X_encoded[i] * self.P_X_Y[j]          <span class="comment"># 在Yj类别下，选出输入样本Xi对应的条件概率</span></span><br><span class="line">            P_Xi_encoded_Yj[P_Xi_encoded_Yj==<span class="number">0.0</span>] = <span class="number">1.0</span>             <span class="comment"># 将为0值替换为1，便于求解ΠP(Xi|yc)，只要将各元素累乘即可</span></span><br><span class="line">            y_pred_prob[i, j] = self.P_Y[j] * P_Xi_encoded_Yj.prod()</span><br><span class="line">        y_pred_prob[i] /= np.sum(y_pred_prob[i])                    <span class="comment"># 分母一般是将分子加和，不能假定各特征独立并用朴素贝叶斯计算分母</span></span><br><span class="line">    <span class="keyword">return</span> np.argmax(y_pred_prob, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">]</span><br><span class="line">y = [<span class="number">0</span> ,<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">estimator = NaiveBayes()</span><br><span class="line">estimator.fit(X, y)</span><br><span class="line"></span><br><span class="line">X_test = np.array([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y_pred = estimator.predict(X_test)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Softmax Regression</title>
      <link href="/2018/10/18/Softmax-Regression/"/>
      <url>/2018/10/18/Softmax-Regression/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" target="_blank" rel="noopener">Unsupervised Feature Learning and Deep Learning Tutorial</a></p></blockquote><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><code>Logistic Regression</code>中采用的非线性函数为<code>Sigmoid</code>，将输出值映射到$(0, 1)$之间作为概率输出，处理的是二分类问题，那么对于多分类的问题怎么处理呢？</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><blockquote><p>由<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">Logistic回归</a>推广而来</p></blockquote><h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p><code>Softmax</code>在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类$(K&gt;2)$问题，分类器最后的输出单元需要<code>Softmax</code>函数进行数值处理。</p><script type="math/tex; mode=display">S(x) = \frac            {1}            {\sum_{k=1}^K exp(x_k)}            \left[                \begin{matrix}                    exp(x_1)\\                    exp(x_2)\\                    ...\\                    exp(x_K)                \end{matrix}            \right]</script><p>其中$x$为矩阵形式的向量，其维度为$(K×1)$，$K$为类别数目。<code>Softmax</code>的输出向量维度与$x$相同，各元素$x_i$加和为$1$，可用于表示取各个类别的概率。</p><p>注意到，对于函数$e^x$</p><script type="math/tex; mode=display">\lim_{x \rightarrow - \infty} e^x = 0</script><script type="math/tex; mode=display">\lim_{x \rightarrow + \infty} e^x = +\infty</script><blockquote><p>假设所有的$x_i$等于某常数$c$，理论上对所有$x_i$上式结果为$\frac{1}{n}$</p><ul><li>若$c$为很小的负数，$e^c$下溢，结果为$NaN$；</li><li>若$c$量级很大，$e^c$上溢，结果为$NaN$。</li></ul></blockquote><p>在数值计算时并不稳定，但是<code>Softmax</code>所有输入增加同一常数时，输出不变，得稳定版本：</p><script type="math/tex; mode=display">S(x) := S(x - max(x_i))</script><blockquote><script type="math/tex; mode=display">e^{x_{max} - max(x_i)} = 1</script><ul><li>减去最大值导致$e^x$最大为$1$，排除上溢；</li><li>分母中至少有一项为$1$，排除分母下溢导致处以$0$的情况。</li></ul><p>其对数</p><script type="math/tex; mode=display">log S(x)_i = x_i - log ({\sum_{k=1}^K exp(x_k)})</script><ul><li>注意到，第一项表示输入$x_i$总是对代价函数有直接的贡献。这一项不会饱和，所以即使$x_i$对上式的第二项的贡献很小，学习依然可以进行；</li><li>当最大化对数似然时，第一项鼓励$x_i$被推高，而第二项则鼓励所有的$x$被压低；</li><li>第二项$log ({\sum_{k=1}^K exp(x_k)})$可以大致近似为$max(x_k)$，这种近似是基于对任何明显小于$max(x_k)$的$x_k$都是不重要的，<strong>负对数似然代价函数总是强烈地惩罚最活跃的不正确预测</strong></li><li>除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习</li></ul><hr><p>作者：NirHeavenX<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/qsczse943062710/article/details/61912464" target="_blank" rel="noopener">https://blog.csdn.net/qsczse943062710/article/details/61912464</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p></blockquote><h2 id="Softmax解决多分类问题"><a href="#Softmax解决多分类问题" class="headerlink" title="Softmax解决多分类问题"></a>Softmax解决多分类问题</h2><p>对于具有$K$个分类的问题，每个类别训练一组参数$ w_k $</p><script type="math/tex; mode=display">z_k^{(i)} = w_k^Tx^{(i)}</script><p>或写作矩阵形式</p><script type="math/tex; mode=display">z^{(i)} = W^Tx^{(i)}</script><p>其中</p><script type="math/tex; mode=display">x^{(i)} =     \left[        \begin{matrix}            x_0^{(i)}\\            x_1^{(i)}\\            ...\\            x_n^{(i)}        \end{matrix}    \right]_{n×1},x_0^{(i)}=1</script><script type="math/tex; mode=display">W = [w_1, w_2, ..., w_K]_{(n+1)×K}</script><script type="math/tex; mode=display">w_i =     \left[        \begin{matrix}            w_{i0}\\            w_{i1}\\            ...\\            w_{in}        \end{matrix}    \right]_{n×1}</script><p>最终各类别输出概率为</p><script type="math/tex; mode=display">\hat{y}^{(i)} = Softmax(z^{(i)})</script><blockquote><p><strong>产生了一个奇怪的脑洞。。。</strong><br>二分类问题</p><script type="math/tex; mode=display">p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }</script><p>定义二分类线性单元输出的差值为</p><script type="math/tex; mode=display">z = x_1 - x_2</script><p>得到</p><script type="math/tex; mode=display">p(x_1) = \frac{1}{1 + e^{-z}}</script><p>以$x_1 = [x_{11}, x_{12}]^T$为例(二维特征)，取$w_1=1, w_2=2, b=3$</p><script type="math/tex; mode=display">p(x_1) = \frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}}</script><p><img src="/2018/10/18/Softmax-Regression/Sigmoid_2dim.png" alt="特征为2时的决策平面"></p><p>而多分类问题，以$3$分类为例</p><script type="math/tex; mode=display">p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }</script><p>定义线性单元输出的差值为</p><script type="math/tex; mode=display">z_{12} = x_1 - x_2</script><script type="math/tex; mode=display">z_{13} = x_1 - x_3</script><script type="math/tex; mode=display">p(x_1) = \frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }</script><p>做出图像为<br><img src="/2018/10/18/Softmax-Regression/3D_sigmoid_1.png" alt="3D_sigmoid_1"><br><img src="/2018/10/18/Softmax-Regression/3D_sigmoid_2.png" alt="3D_sigmoid_2"></p></blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="由交叉熵理解"><a href="#由交叉熵理解" class="headerlink" title="由交叉熵理解"></a>由交叉熵理解</h2><script type="math/tex; mode=display">CrossEnt = \sum_j p_j log \frac{1}{q_j}</script><p>而对于样本$ (X^{(i)}, y^{(i)}) $，为确定事件，故标签概率各元素的取值$p_j$为$ y^{(i)}_j ∈ \{0,1\}$，$ q_j即预测输出的概率值\hat{y}^{(i)}_j$</p><p>一般取各个样本损失的均值$(\frac{1}{N})$</p><script type="math/tex; mode=display">L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N 1\{y^{(i)}_j=k\} log (\hat{y}^{(i)}_j)</script><script type="math/tex; mode=display">1\{y^{(i)}_j=k\} =     \begin{cases}        1 & y^{(i)}_j = k \\        0 & y^{(i)}_j \neq k     \end{cases}</script><p>可对实际标签$y^{(i)}$采取<code>One-Hot</code>编码，便于计算</p><script type="math/tex; mode=display">y^{(i)} = \left[         \begin{matrix}            0, ..., 1_{y^{(i)}}, ..., 0        \end{matrix}     \right]^T</script><p>则</p><script type="math/tex; mode=display">L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N y^{(i)T}log (\hat{y}^{(i)})</script><blockquote><p>实际上，由熵定义</p><script type="math/tex; mode=display">H(p) = \sum_x p(x) \log \frac{1}{p(x)}</script><p>交叉熵为</p><script type="math/tex; mode=display">H(p, q) = \sum_x p(x) \log \frac{1}{q(x)}</script><p>K-L散度为</p><script type="math/tex; mode=display">D_{KL}(p || q) = \sum_x p(x) \log \frac{p(x)}{q(x)}</script><p>也即</p><script type="math/tex; mode=display">D_{KL}(p || q) = H(p, q) - H(p)</script><p>常常用于衡量两个概率分布$p(x), q(x)$之间的差异。而对于固定的数据集，$H(p)$为常熟，故最小化交叉熵$H(p, q)$实际上为最小化K-L散度$D_{KL}(p || q)$。</p></blockquote><h2 id="由决策平面理解"><a href="#由决策平面理解" class="headerlink" title="由决策平面理解"></a>由决策平面理解</h2><p>从<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>和<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a>可知，对于类别$c_i$，有</p><script type="math/tex; mode=display">P(c_i|x) = \frac{P(x|c_i)}{\sum_{j=0}^KP(x|c_j)}</script><blockquote><p>假设每个类别的样本服从正态分布，先验概率相等，各类别样本特征间协方差相等。证明略.</p></blockquote><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><h2 id="Softmax函数的导数"><a href="#Softmax函数的导数" class="headerlink" title="Softmax函数的导数"></a>Softmax函数的导数</h2><p>对于</p><script type="math/tex; mode=display">S(x) = \frac            {1}            {\sum_{k=1}^K exp(x_k)}            \left[                \begin{matrix}                    exp(x_1)\\                    exp(x_2)\\                    ...\\                    exp(x_K)                \end{matrix}            \right]</script><p>一般输出作为概率值，记</p><script type="math/tex; mode=display">P = S(x)</script><script type="math/tex; mode=display">p_i = S(x)_i</script><p>对向量$x$中某元素求导</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i} = \frac{∂}{∂x_i}                    \left[                        \begin{matrix}                            ...\\                            \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}\\                            ...\\                        \end{matrix}                    \right]</script><blockquote><p>$(1)$ $i=k$<br>$<br>\frac{∂}{∂x_i} \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)}$<br>$ = \frac{exp’(x_i)·\sum_{j=1}^K exp(x_j) - exp(x_i)·(\sum_{j=1}^K exp(x_j))’}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{exp(x_i)·\sum_{j=1}^K exp(x_j) - exp^2(x_i)}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)} -<br>(\frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)})^2<br>$<br>$ = p_i (1 - p_i)<br>$</p><p>$(2)$ $i\neq k$<br>$<br>\frac{∂}{∂x_i} \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}$<br>$ = \frac{exp’(x_k)·\sum_{j=1}^K exp(x_j) - exp(x_k)·(\sum_{j=1}^K exp(x_j))’}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{- exp(x_k)exp(x_i)}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$= - p_i p_k$</p><p>综上</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i}_{K×1} =  \left[                          \begin{matrix}                              0\\                              ...\\                              p_i\\                              ...\\                              0                           \end{matrix}                      \right] -                       \left[                          \begin{matrix}                                p_i p_1\\                                ...\\                                p_i^2\\                                ...\\                                p_i p_K                            \end{matrix}                      \right]=     \left(                      \left[                          \begin{matrix}                              0\\                              ...\\                              1\\                              ...\\                              0                           \end{matrix}                      \right] -                       p      \right)p_i</script></blockquote><h2 id="损失函数梯度"><a href="#损失函数梯度" class="headerlink" title="损失函数梯度"></a>损失函数梯度</h2><p>在<code>OneHot</code>编码下，损失函数形式为</p><script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{N} \sum_{i=1}^N L (y^{(i)}, \hat{y}^{(i)})</script><script type="math/tex; mode=display">L (y^{(i)}, \hat{y}^{(i)}) = - y^{(i)T}log \hat{y}^{(i)}</script><script type="math/tex; mode=display">\hat{y}^{(i)} = S(z^{(i)})</script><script type="math/tex; mode=display">z^{(i)} = W^T x^{(i)}</script><p>即只考虑实际分类对应的概率值</p><script type="math/tex; mode=display">L (y^{(i)}, \hat{y}^{(i)}) = - log \hat{y}^{(i)}_{y^{(i)}}</script><blockquote><p>由于 $S(z^{(i)})_{t^{(i)}}$与$z^{(i)}$向量各个元素都有关，由链式求导法则</p><script type="math/tex; mode=display">\frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } (\sum_{k=1}^K  \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}  \frac{∂z^{(i)}_k}{∂w_{pq}})</script><p>$1.$ 考察 $\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}$</p><script type="math/tex; mode=display">  \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} = ​      \begin{cases}​          \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_k) & k=y^{(i)} \\​          - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_k & k \neq y^{(i)} ​      \end{cases}</script><p>$2.$ 考察 $\frac{∂z^{(i)}_k}{∂w_{pq}}$</p><script type="math/tex; mode=display">  \frac{∂z^{(i)}_k}{∂w_{pq}} =       \begin{cases}            \frac{∂z^{(i)}_k}{∂w_{pq}} = x^{(i)}_p & k=q\\            \frac{∂z^{(i)}_k}{∂w_{pq}} = 0 & k \neq q        \end{cases}</script></blockquote><p>综上所述</p><script type="math/tex; mode=display">\frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} }   \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}  \frac{∂z^{(i)}_q}{∂w_{pq}}</script><p>其中</p><script type="math/tex; mode=display">\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}= \begin{cases}        \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_q) & q = y^{(i)}\\        - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_q & q \neq y^{(i)}    \end{cases}</script><script type="math/tex; mode=display">\frac{∂z^{(i)}_q}{∂w_{pq}} = x^{(i)}_p</script><p>故对于单个样本$(X^{(i)}, y^{(i)})$，当样本标签采用$OneHot$编码时</p><script type="math/tex; mode=display">\frac{∂L^{(i)}}{∂w_{pq}}  = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} }   \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}  x^{(i)}_p= \begin{cases}    (\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\    \hat{y}^{(i)}_qx^{(i)}_p & q \neq y^{(i)}\end{cases}</script><blockquote><p>注： 这里可以约分去掉$\hat{y}^{(i)}_{y^{(i)}}$</p></blockquote><script type="math/tex; mode=display">\frac{∂L^{(i)}}{∂w_{pq}} = ( \hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_p</script><p>更一般的，写成矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">∇_W L = X^T(\hat{Y} - Y)</script><blockquote><p><strong>用线性模型解决分类和回归问题时，形式竟如此统一!</strong></p></blockquote><p>至此为止，梯度推导结束，利用梯度下降法迭代求解参数矩阵$W$即可。</p><script type="math/tex; mode=display">W := W - \alpha ∇_W L</script><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex3-2-softmax_regression" target="_blank" rel="noopener">@GitHub: Code of Softmax Regression</a></p><h2 id="Softmax-1"><a href="#Softmax-1" class="headerlink" title="Softmax"></a>Softmax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">""" 数值计算稳定版本的softmax函数</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; X: shape(batch_size, n_labels)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X_max = np.max(X, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))  <span class="comment"># 每行的最大值</span></span><br><span class="line">    X = X - X_max                        <span class="comment"># 每行减去最大值</span></span><br><span class="line">    X = np.exp(X)</span><br><span class="line">    <span class="keyword">return</span> X / np.sum(X, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossEnt</span><span class="params">(self, y_label_true, y_prob_pred)</span>:</span></span><br><span class="line">    <span class="string">""" 计算交叉熵损失函数</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; y_label_true: 真实标签 shape(batch_size,)</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; y_prob_pred: 预测输出 shape(batch_size, n_labels)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mask = self.encoder.transform(y_label_true.reshape(<span class="number">-1</span>, <span class="number">1</span>)).toarray()  <span class="comment"># shape(batch_size, n_labels)</span></span><br><span class="line">    y_prob_masked = np.sum(mask * y_prob_pred, axis=<span class="number">1</span>)          <span class="comment"># 每行真实标签对应的预测输出值</span></span><br><span class="line">    y_prob_masked[y_prob_masked==<span class="number">0.</span>] = <span class="number">1.</span></span><br><span class="line">    y_loss = np.log(y_prob_masked)</span><br><span class="line">    loss = - np.mean(y_loss)                                    <span class="comment"># 求各样本损失的均值</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="gradient"><a href="#gradient" class="headerlink" title="gradient"></a>gradient</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span><span class="params">(self, X_train, y_train, y_prob_pred)</span>:</span></span><br><span class="line">    <span class="string">""" 计算梯度 \frac &#123;∂L&#125; &#123;∂W_&#123;pq&#125;&#125;</span></span><br><span class="line"><span class="string">    @param X_train: 训练集特征</span></span><br><span class="line"><span class="string">    @param y_train: 训练集标签</span></span><br><span class="line"><span class="string">    @param y_prob_pred:  训练集预测概率输出</span></span><br><span class="line"><span class="string">    @param y_label_pred: 训练集预测标签输出</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    y_train = self.encoder.transform(y_train)</span><br><span class="line">    dW = X_train.T.dot(y_prob_pred - y_train)</span><br><span class="line">    <span class="keyword">return</span> dW</span><br></pre></td></tr></table></figure><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><p>省略可视化和验证部分的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, X_valid, y_train, y_valid, min_acc=<span class="number">0.95</span>, max_epoch=<span class="number">20</span>, batch_size=<span class="number">20</span>)</span>:</span></span><br><span class="line">    <span class="string">""" 训练</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 添加首1列，输入到偏置w0</span></span><br><span class="line">    X_train = np.c_[np.ones(shape=(X_train.shape[<span class="number">0</span>],)), X_train]</span><br><span class="line">    X_valid = np.c_[np.ones(shape=(X_valid.shape[<span class="number">0</span>],)), X_valid]</span><br><span class="line">    X_train = self.scaler.fit_transform(X_train)    <span class="comment"># 尺度归一化</span></span><br><span class="line">    X_valid = self.scaler.transform(X_valid)        <span class="comment"># 尺度归一化</span></span><br><span class="line">    self.encoder.fit(y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    self.n_features = X_train.shape[<span class="number">1</span>]</span><br><span class="line">    self.n_labels = self.encoder.transform(y_train).shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    self.W = np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(self.n_features, self.n_labels))</span><br><span class="line">    n_batch = X_train.shape[<span class="number">0</span>] // batch_size</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 可视化相关</span></span><br><span class="line">    plt.ion()</span><br><span class="line">    plt.figure(<span class="string">'loss'</span>); plt.figure(<span class="string">'accuracy'</span>)</span><br><span class="line">    loss_train_epoch = []; loss_valid_epoch = []</span><br><span class="line">    acc_train_epoch = [];  acc_valid_epoch = []</span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> range(max_epoch):</span><br><span class="line">        <span class="keyword">for</span> i_batch <span class="keyword">in</span> range(n_batch):              <span class="comment"># 批处理梯度下降</span></span><br><span class="line">            n1, n2 = i_batch * batch_size, (i_batch + <span class="number">1</span>) * batch_size</span><br><span class="line">            X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2]</span><br><span class="line">            <span class="comment"># 预测</span></span><br><span class="line">            y_prob_train = self.predict(X_train_batch, preprocessed=<span class="keyword">True</span>)</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss_train_batch = self.crossEnt(y_train_batch, y_prob_train)</span><br><span class="line">            <span class="comment"># 计算准确率</span></span><br><span class="line">            y_label_train = np.argmax(y_prob_train, axis=<span class="number">1</span>)</span><br><span class="line">            a = y_train_batch.reshape((<span class="number">-1</span>,))</span><br><span class="line">            acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((<span class="number">-1</span>,))).astype(<span class="string">'float'</span>))</span><br><span class="line">            <span class="comment"># 计算梯度 dW</span></span><br><span class="line">            dW = self.grad(X_train_batch, y_train_batch, y_prob_train)</span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            self.W -= self.lr * dW</span><br></pre></td></tr></table></figure></p><h2 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, preprocessed=False)</span>:</span></span><br><span class="line">    <span class="string">""" 对输入的样本进行预测，输出标签</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; X: shape(batch_size, n_features)</span></span><br><span class="line"><span class="string">    @return &#123;ndarray&#125; y_prob: probability, shape(batch_size, n_labels)</span></span><br><span class="line"><span class="string">            &#123;ndarray&#125; y_label: labels, shape(batch_size,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> preprocessed:    <span class="comment"># 训练过程中调用此函数时，不用加首1列</span></span><br><span class="line">        X = np.c_[np.ones(shape=(X.shape[<span class="number">0</span>],)), X]              <span class="comment"># 添加首1项，输入到偏置w0</span></span><br><span class="line">    X = self.scaler.transform(X)</span><br><span class="line"></span><br><span class="line">    y_prob = softmax(X.dot(self.W))                             <span class="comment"># 预测概率值 shape(batch_size, n_labels)</span></span><br><span class="line">    <span class="keyword">return</span> y_prob</span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>以下蓝线为训练集参数，红线为验证集参数，若稳定训练(如<code>batch_size = 20</code>的结果)，最终准确率在$80\%$左右。</p><blockquote><ul><li>由于<code>随机梯度下降(SGD)</code>遍历次数太多，运行较慢，没有用<code>SGD</code>方法训练，就前几个<code>epoch</code>来看，效果没有<code>batch_size = 20</code>的好；</li><li>添加隐含层形成三层结构的<code>前馈神经网络</code>，可提高准确率；</li><li>还有一点，使用<code>批处理梯度下降(n_batch = 1)</code>训练时，可以看到损失值已经趋于$0$，但准确率却很低，说明已经陷入局部最优解。</li></ul></blockquote><ul><li><p>batch size = 20</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batchsize_20.png" alt="loss_batchsize_20"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batchsize_20.png" alt="accuracy_batchsize_20"></li></ul></li><li><p>batch_size = 200</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batchsize_200.png" alt="loss_batchsize_200"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batchsize_200.png" alt="accuracy_batchsize_200"></li></ul></li><li><p>n_batch = 1</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batch_1.png" alt="loss_batch_1"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batch_1.png" alt="accuracy_batch_1"></li></ul></li></ul><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>推公式要我老命。。。。</p><p><code>Softmax</code>回归可以视作<strong>不含隐含层的<a href="https://louishsu.xyz/2018/10/20/Feedforward-Neural-Network/" target="_blank" rel="noopener">前馈神经网络</a></strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression</title>
      <link href="/2018/10/18/Logistic-Regression/"/>
      <url>/2018/10/18/Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>先给出模型，推导过程稍后给出，逻辑回归包含<code>Sigmoid</code>函数</p><script type="math/tex; mode=display">f(z) = \frac{1}{1+e^{-z}}</script><p>其图像如下<br><img src="/2018/10/18/Logistic-Regression/Sigmoid.png" alt="`Sigmod函数`"></p><p>定义</p><script type="math/tex; mode=display">z = w^Tx</script><p>其中$x=[x_0, x_1, …, x_n]^T, x_0=1$</p><script type="math/tex; mode=display">h_w(x) = g(z) =  \frac{1}{1+e^{-z}}</script><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="由最大似然估计推导"><a href="#由最大似然估计推导" class="headerlink" title="由最大似然估计推导"></a>由最大似然估计推导</h2><p>对于二元分类问题，其取值作为随机变量，服从二项分布 $B(1, p)$，其中$p$即为预测输出概率$\hat{y}$</p><script type="math/tex; mode=display">P(y_i^{(i)}) = (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}</script><p>由极大似然估计</p><script type="math/tex; mode=display">L = \prod_{i=0}^N P(y_i^{(i)}) = \prod_{i=0}^N (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}</script><p>取对数似然函数</p><script type="math/tex; mode=display">logL = \sum_{i=0}^N [y_i^{(i)} log \hat{y}_i^{(i)} + (1-y_i^{(i)}) log (1-\hat{y}_i^{(i)})]</script><p>优化目标是</p><script type="math/tex; mode=display">w = argmax_w logL</script><p>优化问题一般表述成<code>minimize</code>问题，添加负号，构成<code>Neg Log Likelihood</code>损失</p><script type="math/tex; mode=display">w = argmin_w (-logL)</script><p>一般取均值</p><script type="math/tex; mode=display">L(\hat{y}, y)=- \frac{1}{N} \sum_i [y_i^{(i)} log(\hat{y}_i^{(i)})+(1 - y_i^{(i)})log(1-\hat{y}_i^{(i)})]</script><p>其中$y_i$表示真实值，$\hat{y}_i$表示预测值</p><h2 id="从交叉熵理解"><a href="#从交叉熵理解" class="headerlink" title="从交叉熵理解"></a>从交叉熵理解</h2><p>已知交叉熵<code>cross entropy</code>定义如下</p><script type="math/tex; mode=display">CrossEnt = \sum_i p_i log \frac{1}{q_i}</script><p>而对于样本$ (X_i, y_i) $，为确定事件，故标签概率的取值为$ p_i = y_i ∈ \{0,1\}$，$ q_i即预测输出的概率值\hat{y}_i $，可得到与上面相同的推导结论</p><h2 id="从决策平面和贝叶斯决策理解"><a href="#从决策平面和贝叶斯决策理解" class="headerlink" title="从决策平面和贝叶斯决策理解"></a>从决策平面和贝叶斯决策理解</h2><p>相关内容查看<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a>和<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>，逻辑回归考虑的一般是等先验概率问题，故决策函数定义为</p><blockquote><p>$if$ $P(c_i|x)&gt;P(c_j|x)$ $then$ $ x \in c_i $,  $ i, j = 1, 2 $</p></blockquote><p>从贝叶斯决策可知，对于类别$c_1$，有</p><script type="math/tex; mode=display">P(c_1|x) = \frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}</script><p>设在各个类别下，特征$x$服从正态分布</p><script type="math/tex; mode=display">P(x|c_i) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}}exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i))</script><p>则</p><script type="math/tex; mode=display">P(c_1|x) = \frac{1}{    1 + exp(-z)}</script><script type="math/tex; mode=display">P(c_2|x) = 1 - P(c_1|x) = \frac{exp(-z)}{1+exp(-z)}</script><blockquote><p>$<br>P(c_1|x) = \frac<br>{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}<br>{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}<br>$</p><p>$<br>P(c_1|x) = \frac<br>{1}<br>{1 + \frac{exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}<br>}<br>$</p><p>假定各分类的样本方差相等，$ \Sigma_1 = \Sigma_2 = \sigma^2 I $</p><p>$ P(c_1|x) = \frac {1}{1 + exp(- [ \frac{1}{\sigma^2} (\mu_1-\mu_2)^T x - \frac{1}{2 \sigma^2} (\mu_1^T\mu_1 - \mu_2^T\mu_2) ])}<br>$</p><p>令</p><script type="math/tex; mode=display">w = \frac{1}{\sigma^2} (\mu_1 -\mu_2)</script><script type="math/tex; mode=display">b = - \frac{1}{2\sigma^2}(\mu_1^T \mu_1 - \mu_2^T \mu_2)</script><p>即可得到</p><script type="math/tex; mode=display">P(c_1|x) = \frac{1}{    1 + exp(-z)}</script><p>其中</p><script type="math/tex; mode=display">z = w^T x + b</script></blockquote><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><p>先推导<code>Sigmoid</code>函数的导数</p><script type="math/tex; mode=display">f'(z) = (1 - f(z))f(z)</script><p>值得注意的是，从$f’(z)$的图像可以看到，在$ x=0 $处$f’(z)$取极大值，且</p><script type="math/tex; mode=display">f'(z)_{max} = f'(z)|_{z=0} = 0.25</script><script type="math/tex; mode=display">\lim_{z \rightarrow \infty} f'(z) = 0</script><p>在多层神经网络反向传播更新参数时，由于梯度多次累乘，<code>Sigmoid</code>作为<a href="https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">激活函数</a>会存在“梯度消失”的问题，使得参数更新非常缓慢。</p><p><img src="/2018/10/18/Logistic-Regression/Sigmoid_gradient.png" alt="`Sigmod导函数`"></p><blockquote><p>$ f’(z) $<br>$ = (\frac{1}{1+e^{-z}})’ $<br>$ = \frac<br>​          {-(1+e^{-z})’}<br>​          {(1+e^{-z})^2} $<br>$ = \frac<br>​          {e^{-z}}<br>​          {(1+e^{-z})^2} $<br>$ = \frac<br>​          {e^{-z}}<br>​          {1+e^{-z}}<br>​    \frac<br>​          {1}<br>​          {1+e^{-z}}$<br>$ = (1 - f(z))f(z)$</p></blockquote><p>利用链式求导法则可得</p><blockquote><p>$\frac{∂L}{∂w_j}$<br>$= -\frac{∂}{∂w_j} \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})]$<br>$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\frac{∂}{∂w_j}\hat{y}^{(i)}-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\frac{∂}{∂w_j}\hat{y}^{(i)}]$<br>$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j]$<br>$= - \frac{1}{N} \sum_i [y^{(i)} (1-\hat{y}^{(i)})w_j-(1-y^{(i)}) y^{(i)} w_j]$<br>$=  \frac{1}{N} \sum_i (\hat{y}^{(i)} - y^{(i)})w_j $</p></blockquote><p>写作矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">∇_w L = X^T (\hat{Y} - Y)</script><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>和线性回归一样，采用梯度下降法求解</p><script type="math/tex; mode=display">w := w - \alpha ∇_w L</script><h1 id="处理多分类问题"><a href="#处理多分类问题" class="headerlink" title="处理多分类问题"></a>处理多分类问题</h1><p>假设有$K$个类别，则依次以类别$c_i$为正样本训练模型，一共训练$K$个。测试样本在每个模型上计算，最终将概率最大的作为分类结果。</p><blockquote><p>这样划分数据集，会使训练集正负样本数目严重不对称，特别是类别很多的情况，对结果会产生影响。可推广至<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">softmax回归</a>解决这个问题。</p></blockquote><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex2-logisticregression/LogReg.py" target="_blank" rel="noopener">@Github: Code for Logistic Regression</a></p><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFunctionDerivative</span><span class="params">(self, X, theta, y_true)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算损失函数对参数theta的梯度</span></span><br><span class="line"><span class="string">    对theta[j]的梯度为：(y_pred - y_true)*x[j]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    err = self.predict_prob(X, theta) - y_true</span><br><span class="line">    <span class="keyword">return</span> X.T.dot(err)/y_true.shape[<span class="number">0</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFunction</span><span class="params">(self, y_pred_prob, y_true)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    未使用</span></span><br><span class="line"><span class="string">    计算损失值: Cross-Entropy</span></span><br><span class="line"><span class="string">    y_pred_prob, y_true: NumPy array, shape=(n,)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tmp = y_true*np.log(y_pred_prob) + (<span class="number">1</span> - y_true)*np.log(<span class="number">1</span> - y_pred_prob)</span><br><span class="line">    <span class="keyword">return</span> np.mean(-tmp)</span><br></pre></td></tr></table></figure><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradDescent</span><span class="params">(self, min_acc, learning_rate=<span class="number">0.01</span>, max_iter=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    acc = <span class="number">0</span>; n_iter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n_iter <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(self.n_batch):</span><br><span class="line">            X_batch = self.X[n*self.batch_size:(n+<span class="number">1</span>)*self.batch_size]</span><br><span class="line">            t_batch = self.t[n*self.batch_size:(n+<span class="number">1</span>)*self.batch_size]</span><br><span class="line">            grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch)</span><br><span class="line">            self.theta -= learning_rate * grad <span class="comment"># 梯度下降</span></span><br><span class="line">            acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t)</span><br><span class="line">            <span class="keyword">if</span> acc &gt; min_acc:</span><br><span class="line">                print(<span class="string">'第%d次迭代, 第%d批数据'</span> % (n_iter, n))</span><br><span class="line">                print(<span class="string">"当前总体样本准确率为: "</span>, acc)</span><br><span class="line">                print(<span class="string">"当前参数值为: "</span>, self.theta)</span><br><span class="line">                <span class="keyword">return</span> self.theta</span><br><span class="line">        <span class="keyword">if</span> n_iter%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'第%d次迭代'</span> % n_iter)</span><br><span class="line">            print(<span class="string">'准确率： '</span>, acc)</span><br><span class="line">    print(<span class="string">"超过迭代次数"</span>)</span><br><span class="line">    print(<span class="string">"当前总体样本准确率为: "</span>, acc)</span><br><span class="line">    print(<span class="string">"当前参数值为: "</span>, self.theta)</span><br><span class="line">    <span class="keyword">return</span> self.theta</span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/2018/10/18/Logistic-Regression/logistic_regression_result.png" alt="实验结果"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2018/10/18/Linear-Regression/"/>
      <url>/2018/10/18/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>线性回归可以说是机器学习最基础的算法</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><script type="math/tex; mode=display">\hat{y}^{(i)} = w^Tx^{(i)}</script><p>其中</p><script type="math/tex; mode=display">x^{(i)}=[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T, x_0^{(i)}=1</script><p>这里$x_0^{(i)}=1$表示偏置$b$，即$b=w_0$</p><script type="math/tex; mode=display">\hat{y}^{(i)} = w^Tx^{(i)} + b</script><blockquote><p><code>注</code>：对于非线性的数据，可构造高次特征。</p></blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="定义误差"><a href="#定义误差" class="headerlink" title="定义误差"></a>定义误差</h2><script type="math/tex; mode=display">e^{(i)} = \hat{y}^{(i)} - y^{(i)}</script><p>其中$y^{(i)}$表示真实值</p><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>单个样本的误差定义为</p><script type="math/tex; mode=display">L_{single}(\hat{y}^{(i)}, y^{(i)})=\frac{1}{2}||e^{(i)}||_2^2=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2</script><p>所有样本的误差定义为</p><script type="math/tex; mode=display">L(y, t)=\frac{1}{2N}\sum_i (\hat{y}^{(i)}-y^{(i)})^2</script><p>也可以定义为误差的和而不是均值，对结果无影响，可视作学习率$α$除去一个常数</p><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><blockquote><p>$\frac{∂L}{∂w_j}$<br>$= \frac{∂}{∂w_j}\frac{1}{2N}\sum_i(\hat{y}^{(i)}-y^{(i)})^2$<br>$= \frac{1}{2N} \sum_i \frac{∂}{∂w_j} (\hat{y}^{(i)}-y^{(i)})^2$<br>$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) \frac{∂t^{(i)}}{∂w_j}$<br>$=  \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)}$</p></blockquote><p>或者使用矩阵推导，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">L = \frac{1}{2}(Xw-Y)^T(Xw-Y)</script><script type="math/tex; mode=display">∇_w L = X^T(\hat{Y}-Y)</script><blockquote><p>$∇_w L$<br>$= \frac{1}{2} ∇_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY)$<br>$= \frac{1}{2} (2X^TXw - X^TY - X^TY)$<br>$= X^T(Xw-Y) $</p></blockquote><p>在梯度为$\vec{0}$的点，即$∇_w L = \vec{0}$时对应最优解</p><script type="math/tex; mode=display">X^T(Xw-Y) = 0</script><blockquote><p>令<script type="math/tex">X^T(Xw-Y) = 0</script></p><p>有<script type="math/tex">X^TXw = X^TY</script></p><script type="math/tex; mode=display">w^*=(X^TX+\lambda I)^{-1}X^TY</script></blockquote><p>其中$X^+=(X^TX+\lambda I)^{-1}X^T$，表示矩阵$X_{m×n}$的伪逆</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>采用梯度下降法求解</p><script type="math/tex; mode=display">w := w - \alpha ∇_w L</script><p>其中$w$表示参数向量</p><blockquote><p>进一步思考：为什么使用梯度下降可以求取最优解呢？</p><script type="math/tex; mode=display">∇_w^2 L = ∇_w X^T(Xw-Y) = X^TX</script><p>而对于矩阵 $ X^TX $</p><script type="math/tex; mode=display">u^T(X^TX)u = (Xu)^T(Xu) \geq 0</script><p>即损失函数的<code>Hessian</code>矩阵$∇_w^2 L$为正定矩阵，$L$为凸函数，存在全局最优解</p></blockquote><h1 id="从投影的角度理解线性回归"><a href="#从投影的角度理解线性回归" class="headerlink" title="从投影的角度理解线性回归"></a>从投影的角度理解线性回归</h1><p><img src="/2018/10/18/Linear-Regression/projection_linreg2.png" alt="投影理解"></p><p><img src="/2018/10/18/Linear-Regression/projection_linreg3.png" alt="用投影推导最优解"></p><h1 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h1><p>为克服过拟合问题，可加入正则化项$||w||_2^2$，此时损失函数定义为</p><script type="math/tex; mode=display">L(\hat{y}, y)=\frac{1}{2N} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_2^2</script><p>或者</p><script type="math/tex; mode=display">L(\hat{y}, y)=\frac{1}{2N} \sum_i (\hat{y}^{(i)}-y^{(i)})^2 +  \frac{\lambda}{2N}\sum_j w_j^2</script><p>其中$i = 1, …, N_{sample}; j = 1, …, N_{feature},j&gt;0 $</p><p>此时梯度为</p><script type="math/tex; mode=display">\frac{∂L}{∂w_j} = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \frac{\lambda}{N}w_j</script><p>其中$j = 1, …, N_{feature},j&gt;0 $</p><h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><p>目标函数定义为</p><script type="math/tex; mode=display">L(y, t)=\frac{1}{2N}\sum_i w^{(i)} (\hat{y}^{(i)}-y^{(i)})^2</script><p>其中</p><script type="math/tex; mode=display">w^{(i)} = e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}</script><p>$x$表示输入的预测样本，$x^{(i)}$表示训练样本</p><p><div style="align: center"><img src="/2018/10/18/Linear-Regression/w_i_x_i.png"></div><br>离很近的样本，权值接近于1，而对于离很远的样本，此时权值接近于0，这样就是在局部构成线性回归，它依赖的也只是周边的点。</p><p>对于线性回归算法，一旦拟合出适合训练数据的参数$w$，保存这些参数$w$，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。而对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数$w$），没有固定的参数$w$，所以是非参数算法。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex5-regularizedllinearregression" target="_blank" rel="noopener">@Github: Code for Linear Regression</a></p><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, learning_rate=<span class="number">0.01</span>, max_iter=<span class="number">5000</span>, min_loss=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># --------------- 数据预处理部分 ---------------</span></span><br><span class="line">    <span class="comment"># 加入全1列</span></span><br><span class="line">    X = np.c_[np.ones(shape=(X.shape[<span class="number">0</span>])), X]</span><br><span class="line">    <span class="comment"># 构造高次特征</span></span><br><span class="line">    <span class="keyword">if</span> self.n_ploy &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, self.n_ploy + <span class="number">1</span>):</span><br><span class="line">            X = np.c_[X, X[:, <span class="number">1</span>]**i]</span><br><span class="line">    <span class="comment"># ---------------- 参数迭代部分 ----------------</span></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    self.theta = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, size=(X.shape[<span class="number">1</span>],))</span><br><span class="line">    <span class="comment"># 数据批次</span></span><br><span class="line">    n_batch = X.shape[<span class="number">0</span>] <span class="keyword">if</span> self.n_batch==<span class="number">-1</span> <span class="keyword">else</span> self.n_batch</span><br><span class="line">    batch_size = X.shape[<span class="number">0</span>] // n_batch</span><br><span class="line">    <span class="comment"># 停止条件</span></span><br><span class="line">    n_iter = <span class="number">0</span>; loss = float(<span class="string">'inf'</span>)</span><br><span class="line">    <span class="comment"># 开始迭代</span></span><br><span class="line">    <span class="keyword">for</span> n_iter <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            n1, n2 = n*batch_size, (n+<span class="number">1</span>)*batch_size</span><br><span class="line">            X_batch = X[n1: n2]; y_batch = y[n1: n2]</span><br><span class="line">            </span><br><span class="line">            grad = self.lossFunctionDerivative(X_batch, y_batch)</span><br><span class="line">            self.theta -= learning_rate * grad</span><br><span class="line">            </span><br><span class="line">            loss = self.score(y_batch, self.predict(X_batch))</span><br><span class="line">            <span class="keyword">if</span> loss &lt; min_loss:</span><br><span class="line">                print(<span class="string">'第%d次迭代, 第%d批数据'</span> % (n_iter, n))</span><br><span class="line">                print(<span class="string">"当前总体样本损失为: "</span>, loss)</span><br><span class="line">                <span class="keyword">return</span> self.theta</span><br><span class="line">        <span class="keyword">if</span> n_iter%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'第%d次迭代'</span> % n_iter)</span><br><span class="line">            print(<span class="string">"当前总体样本损失为: "</span>, loss)</span><br><span class="line">    print(<span class="string">"超过迭代次数"</span>)</span><br><span class="line">    print(<span class="string">"当前总体样本损失为: "</span>, loss)</span><br><span class="line">    <span class="keyword">return</span> self.theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFunctionDerivative</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    y_pred = self.predict(X)</span><br><span class="line">    <span class="comment"># theta = self.theta;     # ！注意：theta = self.theta 不仅仅是赋值，类似引用，修改theta会影响self.theta</span></span><br><span class="line">    theta = self.theta.copy()</span><br><span class="line">    theta[<span class="number">0</span>] = <span class="number">0</span>            <span class="comment"># θ0不需要正则化</span></span><br><span class="line">    <span class="keyword">return</span> (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h2 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, preprocessed=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> preprocessed:</span><br><span class="line">        <span class="comment"># 加入全1列</span></span><br><span class="line">        X = np.c_[np.ones(shape=(X.shape[<span class="number">0</span>])), X]</span><br><span class="line">        <span class="comment"># 构造高次特征</span></span><br><span class="line">        <span class="keyword">if</span> self.n_ploy &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, self.n_ploy + <span class="number">1</span>):</span><br><span class="line">                X = np.c_[X, X[:, <span class="number">1</span>]**i]</span><br><span class="line">    <span class="keyword">return</span> X.dot(self.theta)</span><br></pre></td></tr></table></figure><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><ul><li><p>无正则化<br>  <img src="/2018/10/18/Linear-Regression/result_linreg_noreg.png" alt="无正则化的线性回归结果"></p></li><li><p>正则化<br>  <img src="/2018/10/18/Linear-Regression/result_linreg_reg.png" alt="正则化的线性回归结果"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
