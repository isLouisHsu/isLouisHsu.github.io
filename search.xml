<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ubuntuç¼–è¯‘å®‰è£…Tensorflow]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow%2F</url>
    <content type="text"><![CDATA[éžå¸¸é‡è¦å¦‚æžœä¸­é€”å‡ºçŽ°é”™è¯¯ï¼Œxxxxæ–‡ä»¶æ‰¾ä¸åˆ°ï¼Œä¸è¦æ€€ç–‘ï¼å°±æ˜¯å¤§å¤©æœçš„ç½‘ç»œé—®é¢˜ï¼æŽ¨èç§‘å­¦ä¸Šç½‘ï¼ å®‰è£…CUDAä¸ŽCUDNNé¦–å…ˆæŸ¥çœ‹æ˜¾å¡æ˜¯å¦æ”¯æŒCUDAåŠ é€Ÿï¼Œè¾“å…¥1$ nvidia-smi åœ¨Ubuntu16.04 LTSä¸‹ï¼ŒæŽ¨èå®‰è£…CUDA9.0å’ŒCUDNN 7ã€‚ CUDA CUDA Toolkit 9.0 Downloads | NVIDIA Developer https://developer.nvidia.com/cuda-90-download-archive ä¸‹è½½.runç‰ˆæœ¬ï¼Œå®‰è£…æ–¹æ³•å¦‚ä¸‹ 12$ sudo chmod +x cuda_9.0.176_384.81_linux.run $ sudo sh ./cuda_9.0.176_384.81_linux.run æœåŠ¡æ¡æ¬¾å¾ˆé•¿ã€‚ã€‚ã€‚ã€‚ CUDNN NVIDIA cuDNN | NVIDIA Developer https://developer.nvidia.com/cudnn 1234$ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* å®‰è£…åŽè¿›è¡ŒéªŒè¯ 1234$ cp -r /usr/src/cudnn_samples_v7/ $HOME$ cd $HOME/cudnn_samples_v7/mnistCUDNN$ make clean &amp;&amp; make$ ./mnistCUDNN ç¼–è¯‘Tensorflow(CPU version)ç”±äºŽè®­ç»ƒä»£ç ä½¿ç”¨Pythonå®žçŽ°ï¼Œæ•…C++ç‰ˆæœ¬çš„Tensorflowä¸ä½¿ç”¨GPUï¼Œä»…å®žçŽ°é¢„æµ‹ä»£ç å³å¯ã€‚ bazel Installing Bazel on Ubuntu - Bazel https://docs.bazel.build/versions/master/install-ubuntu.htmlä¸€å®šè¦ç”¨æºç å®‰è£…ï¼ï¼ï¼ download the Bazel binary installer named bazel-&lt;version&gt;-installer-linux-x86_64.sh from the Bazel releases page on GitHub. 123456$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python$ chmod +x bazel-&lt;version&gt;-installer-linux-x86_64.sh$ ./bazel-&lt;version&gt;-installer-linux-x86_64.sh --user$ sudo nano ~/.bashrc # export PATH=&quot;$PATH:$HOME/bin&quot;$ source ~/.bashrc $ bazel version ç¼–è¯‘CPUç‰ˆæœ¬çš„CPUæŸ¥çœ‹javaç‰ˆæœ¬1234$ java -versionopenjdk version &quot;1.8.0_191&quot;OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) å®‰è£…ä¾èµ–è½¯ä»¶åŒ…çŽ¯å¢ƒ1234$ sudo apt install python3-dev$ pip3 install six$ pip3 install numpy$ pip3 instal wheel ä¸‹è½½Tensorflowæºç 1$ git clone https://github.com/tensorflow/tensorflow ç¼–è¯‘ä¸Žå®‰è£…12$ cd tensorflow$ ./configure é…ç½®é€‰é¡¹å¦‚ä¸‹1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command &quot;bazel shutdown&quot;.INFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5You have bazel 0.20.0 installed.Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3Found possible Python library paths: /usr/local/lib/python3.5/dist-packages /usr/lib/python3/dist-packagesPlease input the desired Python library path to use. Default is [/usr/local/lib/python3.5/dist-packages]Do you wish to build TensorFlow with XLA JIT support? [Y/n]: nNo XLA JIT support will be enabled for TensorFlow.Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: nNo OpenCL SYCL support will be enabled for TensorFlow.Do you wish to build TensorFlow with ROCm support? [y/N]: nNo ROCm support will be enabled for TensorFlow.Do you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Do you wish to download a fresh release of clang? (Experimental) [y/N]: nClang will not be downloaded.Do you wish to build TensorFlow with MPI support? [y/N]: nNo MPI support will be enabled for TensorFlow.Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is -march=native -Wno-sign-compare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Preconfigured Bazel build configs. You can use any of the below by adding &quot;--config=&lt;&gt;&quot; to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=monolithic # Config for mostly static monolithic build. --config=gdr # Build with GDR support. --config=verbs # Build with libverbs support. --config=ngraph # Build with Intel nGraph support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.Preconfigured Bazel build configs to DISABLE default on features: --config=noaws # Disable AWS S3 filesystem support. --config=nogcp # Disable GCP support. --config=nohdfs # Disable HDFS support. --config=noignite # Disable Apacha Ignite support. --config=nokafka # Disable Apache Kafka support. --config=nonccl # Disable NVIDIA NCCL support.Configuration finished ä½¿ç”¨bazelç¼–è¯‘1$ bazel build --config=opt //tensorflow:libtensorflow_cc.so å‡ºçŽ°é”™è¯¯ TF failing to build on Bazel CI Â· Issue #19464 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/19464Failure to build TF 1.12 from source - multiple definitions in grpc Â· Issue #23402 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197Explicitly import tools/bazel.rc by meteorcloudy Â· Pull Request #23583 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583Explicitly import tools/bazel.rc by meteorcloudy Â· Pull Request #23583 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5 è§£å†³æ–¹æ³• æ–¹æ³•1 æ–¹æ³•2 å°†tools/bazel.rcä¸­å†…å®¹ç²˜åˆ°.tf_configure.bazelrcä¸­ï¼Œæ¯æ¬¡é‡æ–°é…ç½®åŽéœ€è¦é‡æ–°ç²˜è´´ä¸€æ¬¡ã€‚ æºç å®‰è£…protobuf3.6.0 https://github.com/protocolbuffers/protobuf 1234./autogen.sh./configuremakemake install ä¸‹è½½å…¶ä»–æ–‡ä»¶ 12$ ./tensorflow/contrib/makefile/download_dependencies.shmkdir /tmp/eigen å€¼å¾—æ³¨æ„ï¼Œdownload_dependencies.shä¸­ä¸‹è½½ä¾èµ–åŒ…æ—¶ï¼Œéœ€è¦ç”¨åˆ°curlï¼Œä½†æ˜¯é»˜è®¤æ–¹å¼å®‰è£… 1$ sudo apt install curl &gt; çŽ°åœ¨æ˜¯2018/12/19/02:48ï¼Œè¢«è¿™ä¸ªé—®é¢˜æŠ˜è…¾äº†3ä¸ªå°æ—¶ã€‚ æ—¶ä¸æ”¯æŒ`https`åè®®ï¼Œæ•…éœ€è¦å®‰è£…`OpenSSL`ï¼Œå¹¶æºç å®‰è£…ï¼Œè¯¦ç»†èµ„æ–™è§[curlæç¤ºä¸æ”¯æŒhttpsåè®®è§£å†³æ–¹æ³• - æ ‡é…çš„å°å· - åšå®¢å›­](https://www.cnblogs.com/biaopei/p/8669810.html) - æ‰§è¡Œ`./autogen.sh`æ—¶ï¼Œå‘ç”Ÿé”™è¯¯`autoreconf: not found`ï¼Œåˆ™å®‰è£… 12$ sudo apt install autoconf aotomake libtool$ sudo apt install libffi-dev æºç å®‰è£…Eigen 12345cd tensorflow/contrib/makefile/Downloads/eigenmkdir buildcd buildcmakemake install è°ƒç”¨C++ç‰ˆæœ¬çš„Tensorflowåˆ›å»ºæ–‡ä»¶ç›®å½•å¦‚ä¸‹1234|-- tf_test |-- build |-- main.cpp |-- CMakeLists.txt main.cppæ–‡ä»¶å†…å®¹å¦‚ä¸‹1234567891011121314151617181920212223#include &quot;tensorflow/cc/client/client_session.h&quot;#include &quot;tensorflow/cc/ops/standard_ops.h&quot;#include &quot;tensorflow/core/framework/tensor.h&quot;int main() &#123; using namespace tensorflow; using namespace tensorflow::ops; Scope root = Scope::NewRootScope(); // Matrix A = [3 2; -1 0] auto A = Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125;&#125;); // Vector b = [3 5] auto b = Const(root, &#123; &#123;3.f, 5.f&#125;&#125;); // v = Ab^T auto v = MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true)); std::vector&lt;Tensor&gt; outputs; ClientSession session(root); // Run and fetch v TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs)); // Expect outputs[0] == [19; -3] LOG(INFO) &lt;&lt; outputs[0].matrix&lt;float&gt;(); return 0;&#125; CMakeLists.txtå†…å®¹å¦‚ä¸‹12345678910111213141516171819202122232425262728293031cmake_minimum_required (VERSION 2.8.8)project (tf_example)set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -g -std=c++11 -W&quot;)set(EIGEN_DIR /usr/local/include/eigen3)set(PROTOBUF_DIR /usr/local/include/google/protobuf)set(TENSORFLOW_DIR /home/louishsu/install/tensorflow-1.12.0)include_directories( $&#123;EIGEN_DIR&#125; $&#123;PROTOBUF_DIR&#125; $&#123;TENSORFLOW_DIR&#125; $&#123;TENSORFLOW_DIR&#125;/bazel-genfiles $&#123;TENSORFLOW_DIR&#125;/tensorflow/contrib/makefile/downloads/absl)link_directories( /usr/local/lib)add_executable( tf_test main.cpp)target_link_libraries( tf_test tensorflow_cc tensorflow_framework) 123$ mkdir build &amp;&amp; cd build$ cmake .. &amp;&amp; make$ ./tf_test install tensorflow-gpu for pythonå¯ä½¿ç”¨pipæŒ‡ä»¤å®‰è£…ï¼ŒæŽ¨èä¸‹è½½å®‰è£…åŒ…ï¼Œ tensorflow Â· PyPI https://pypi.org/project/tensorflow/ 12$ cd ~/Downloads$ pip3 --default-timeout=1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user å®‰è£…åŽè¿›è¡ŒéªŒè¯123456789101112131415161718192021222324252627$ python3Python 3.5.2 (default, Nov 12 2018, 13:43:14) [GCC 5.4.0 20160609] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; sess = tf.Session()2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758pciBusID: 0000:04:00.0totalMemory: 983.44MiB freeMemory: 177.19MiB2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 02018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)&gt;&gt;&gt; a = tf.Variable([233])&gt;&gt;&gt; init = tf.initialize_all_variables()WARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.Instructions for updating:Use `tf.global_variables_initializer` instead.&gt;&gt;&gt; sess.run(init)&gt;&gt;&gt; sess.run(a)array([233], dtype=int32)&gt;&gt;&gt; sess.close() æ³¨æ„ï¼Œå¦‚æžœå¼‚å¸¸ä¸­æ–­ç¨‹åºï¼Œæ˜¾å­˜ä¸ä¼šè¢«é‡Šæ”¾ï¼Œéœ€è¦è‡ªè¡Œkill1$ nvidia-smi èŽ·å¾—PIDåºå·ï¼Œä½¿ç”¨æŒ‡ä»¤ç»“æŸè¿›ç¨‹1$ kill -9 pid Reference TensorFlow C++åŠ¨æ€åº“ç¼–è¯‘ - ç®€ä¹¦ https://www.jianshu.com/p/d46596558640Tensorflow C++ ä»Žè®­ç»ƒåˆ°éƒ¨ç½²(1)ï¼šçŽ¯å¢ƒæ­å»º | æŠ€æœ¯åˆ˜ http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/]]></content>
      <categories>
        <category>Linux</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntuç¼–è¯‘å®‰è£…OpenCV]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV%2F</url>
    <content type="text"><![CDATA[ä¸‹è½½æºç  OpenCV library https://opencv.org/ ç¼–è¯‘å®‰è£…ä¾èµ–è½¯ä»¶åŒ…12$ sudo apt install cmake$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev ç¼–è¯‘12345$ unzip opencv-3.4.4.zip$ cd opencv-3.4.4$ mkdir build &amp;&amp; cd build$ cmake ..$ make -j4 å®‰è£…123$ sudo make install$ sudo nano /etc/ld.so.conf.d/opencv.conf # add `/usr/local/lib`$ sudo ldconfig éªŒè¯OpenCVè‡ªå¸¦éªŒè¯ç¨‹åºï¼Œåœ¨opencv-3.4.4/samples/cpp/example_cmakeä¸­å¯ä»¥æ‰¾åˆ° 1234$ cd opencv-3.4.4/samples/cpp/example_cmake$ cmake .$ make$ ./opencv_example å¦‚æžœæ²¡é—®é¢˜ï¼Œå¯ä»¥çœ‹åˆ°ä½ çš„å¤§è„¸äº†~ Reference Ubuntu16.04å®‰è£…openCV3.4.4 - è¾£å±å°å¿ƒçš„å­¦ä¹ ç¬”è®° - CSDNåšå®¢ https://blog.csdn.net/weixin_39992397/article/details/84345197]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonè¯»å†™é…ç½®æ–‡ä»¶]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæœ‰è®¸å¤šè¿è¡Œå‚æ•°éœ€è¦æŒ‡å®šï¼Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥è§£å†³ å®šä¹‰.pyæ–‡ä»¶å­˜å‚¨å˜é‡ å®šä¹‰å‘½åå…ƒç»„collections.namedtuple() åˆ›å»º.configï¼Œ.iniç­‰é…ç½®æ–‡ä»¶ Python è¯»å–å†™å…¥é…ç½®æ–‡ä»¶å¾ˆæ–¹ä¾¿ï¼Œä½¿ç”¨å†…ç½®æ¨¡å—configparserå³å¯ è¯»å‡ºé¦–å…ˆåˆ›å»ºæ–‡ä»¶test.configæˆ–test.iniï¼Œå†™å…¥å¦‚ä¸‹å†…å®¹123456789[db]db_port = 3306db_user = rootdb_host = 127.0.0.1db_pass = test[concurrent]processor = 20thread = 10 è¯»å–æ“ä½œå¦‚ä¸‹1234567891011121314151617181920212223242526272829&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; configfile = &quot;./test.config&quot;&gt;&gt;&gt; inifile = &quot;./test.ini&quot;&gt;&gt;&gt; &gt;&gt;&gt; cf = configparser.ConfigParser()&gt;&gt;&gt; cf.read(configfile) # è¯»å–æ–‡ä»¶å†…å®¹&gt;&gt;&gt; &gt;&gt;&gt; sections = cf.sections() # æ‰€æœ‰çš„sectionï¼Œä»¥åˆ—è¡¨çš„å½¢å¼è¿”å›ž&gt;&gt;&gt; sections[&apos;db&apos;, &apos;concurrent&apos;]&gt;&gt;&gt; &gt;&gt;&gt; options = cf.options(&apos;db&apos;) # è¯¥sectionçš„æ‰€æœ‰option&gt;&gt;&gt; options[&apos;db_port&apos;, &apos;db_user&apos;, &apos;db_host&apos;, &apos;db_pass&apos;]&gt;&gt;&gt; &gt;&gt;&gt; items = cf.items(&apos;db&apos;) # è¯¥sectionçš„æ‰€æœ‰é”®å€¼å¯¹&gt;&gt;&gt; items[(&apos;db_port&apos;, &apos;3306&apos;), (&apos;db_user&apos;, &apos;root&apos;), (&apos;db_host&apos;, &apos;127.0.0.1&apos;), (&apos;db_pass&apos;, &apos;test&apos;)]&gt;&gt;&gt; &gt;&gt;&gt; db_user = cf.get(&apos;db&apos;, &apos;db_user&apos;) # sectionä¸­optionçš„å€¼ï¼Œè¿”å›žä¸ºstringç±»åž‹&gt;&gt;&gt; db_user&apos;root&apos;&gt;&gt;&gt; &gt;&gt;&gt; db_port = cf.getint(&apos;db&apos;, &apos;db_port&apos;) # å¾—åˆ°sectionä¸­optionçš„å€¼ï¼Œè¿”å›žä¸ºintç±»åž‹&gt;&gt;&gt; # ç±»ä¼¼çš„è¿˜æœ‰getboolean()ä¸Žgetfloat()&gt;&gt;&gt; db_port3306 å†™å…¥12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; cf = configparser.ConfigParser()&gt;&gt;&gt; cf.add_section(&apos;test1&apos;) # æ–°å¢žsection&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) # æ–°å¢žoptionï¼šé”™è¯¯ç¤ºèŒƒTraceback (most recent call last): File &quot;&lt;pyshell#7&gt;&quot;, line 1, in &lt;module&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1192, in set self._validate_value_types(option=option, value=value) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1177, in _validate_value_types raise TypeError(&quot;option values must be strings&quot;)TypeError: option values must be strings&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, &apos;1&apos;) # æ–°å¢žoption&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test1&quot;, &quot;opt1&quot;, &apos;ok&apos;) # æ–°å¢žoption&gt;&gt;&gt; cf.remove_option(&quot;test1&quot;, &quot;opt1&quot;) # åˆ é™¤optionTrue&gt;&gt;&gt; &gt;&gt;&gt; cf.add_section(&apos;test2&apos;) # æ–°å¢žsection&gt;&gt;&gt; cf.remove_section(&apos;test2&apos;) # åˆ é™¤sectionTrue&gt;&gt;&gt; &gt;&gt;&gt; with open(&quot;./test_wr.config&quot;, &apos;w+&apos;) as f: cf.write(f) # å†™å…¥æ–‡ä»¶test_wr.config &gt;&gt;&gt; çŽ°åœ¨ç›®å½•å·²åˆ›å»ºæ–‡ä»¶test_wr.configï¼Œæ‰“å¼€å¯ä»¥çœ‹åˆ°12[test1]count = 1]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pythonæ›´æ–°å®‰è£…çš„åŒ…]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%2F</url>
    <content type="text"><![CDATA[pipä¸æä¾›å‡çº§å…¨éƒ¨å·²å®‰è£…æ¨¡å—çš„æ–¹æ³•ï¼Œä»¥ä¸‹æŒ‡ä»¤å¯æŸ¥çœ‹æ›´æ–°ä¿¡æ¯1$ pip list --outdate å¾—åˆ°è¾“å‡ºä¿¡æ¯å¦‚ä¸‹123456789101112131415161718192021222324Package Version Latest Type----------------- --------- ---------- -----absl-py 0.3.0 0.6.1 sdistautopep8 1.3.5 1.4.2 sdistbleach 2.1.4 3.0.2 wheelcertifi 2018.8.24 2018.10.15 wheeldask 0.20.0 0.20.1 wheelgrpcio 1.14.1 1.16.0 wheelipykernel 5.0.0 5.1.0 wheelipython 7.0.1 7.1.1 wheeljedi 0.12.1 0.13.1 wheeljupyter-console 5.2.0 6.0.0 wheelMarkdown 2.6.11 3.0.1 wheelMarkupSafe 1.0 1.1.0 wheelmatplotlib 2.2.2 3.0.2 wheelmistune 0.8.3 0.8.4 wheelnumpy 1.14.5 1.15.4 wheelopencv-python 3.4.2.17 3.4.3.18 wheelPillow 5.2.0 5.3.0 wheelprometheus-client 0.3.1 0.4.2 sdistpyparsing 2.2.0 2.3.0 wheelpython-dateutil 2.7.3 2.7.5 wheelpytz 2018.5 2018.7 wheelurllib3 1.23 1.24.1 wheel ä»¥ä¸‹æä¾›ä¸€é”®å‡çº§çš„æ–¹æ³•ï¼Œå¯èƒ½æ¯”è¾ƒä¹…hhhh12345678from pip._internal.utils.misc import get_installed_distributionsfrom subprocess import call for dist in get_installed_distributions(): modulename = dist.project_name print(&apos;start processing module &apos; + modulename) call(&quot;pip install --upgrade &quot; + modulename, shell=True) print(&apos;module &apos; + modulename + &apos;done!&apos;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pythonè®°å½•æ—¥å¿—]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[å‰è¨€æ—¥å¿—å¯ä»¥ç”¨æ¥è®°å½•åº”ç”¨ç¨‹åºçš„çŠ¶æ€ã€é”™è¯¯å’Œä¿¡æ¯æ¶ˆæ¯ï¼Œä¹Ÿç»å¸¸ä½œä¸ºè°ƒè¯•ç¨‹åºçš„å·¥å…·ã€‚Pythonæä¾›äº†ä¸€ä¸ªæ ‡å‡†çš„æ—¥å¿—æŽ¥å£ï¼Œå°±æ˜¯loggingæ¨¡å—ã€‚æ—¥å¿—çº§åˆ«æœ‰DEBUGã€INFOã€WARNINGã€ERRORã€CRITICALäº”ç§ã€‚ logging â€” Logging facility for Python â€” Python 3.7.1 documentation ä½¿ç”¨æ–¹æ³•loggerå¯¹è±¡1234&gt;&gt;&gt; import logging&gt;&gt;&gt; logger = logging.getLogger(__name__)&gt;&gt;&gt; logger&lt;Logger __main__ (WARNING)&gt; æ—¥å¿—çº§åˆ«å¯è¾“å‡ºäº”ç§ä¸åŒçš„æ—¥å¿—çº§åˆ«ï¼Œåˆ†åˆ«ä¸ºæœ‰DEBUGã€INFOã€WARNINGã€ERRORã€CRITICAL12345678&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&gt;&gt;&gt; logger.info(&apos;test log&apos;)&gt;&gt;&gt; logger.warning(&apos;test log&apos;)test log&gt;&gt;&gt; logger.error(&apos;test log&apos;)test log&gt;&gt;&gt; logger.critical(&apos;test log&apos;)test log å¯ä»¥çœ‹åˆ°åªæœ‰WARNINGåŠä»¥ä¸Šçº§åˆ«æ—¥å¿—è¢«è¾“å‡ºï¼Œè¿™æ˜¯ç”±äºŽé»˜è®¤çš„æ—¥å¿—çº§åˆ«æ˜¯WARNING ï¼Œæ‰€ä»¥ä½ŽäºŽæ­¤çº§åˆ«çš„æ—¥å¿—ä¸ä¼šè®°å½•ã€‚ åŸºç¡€é…ç½®1logging.basicConfig(**kwarg) **kwargä¸­éƒ¨åˆ†å‚æ•°å¦‚ä¸‹ format 12345678910%(levelname)ï¼šæ—¥å¿—çº§åˆ«çš„åå­—æ ¼å¼%(levelno)sï¼šæ—¥å¿—çº§åˆ«çš„æ•°å­—è¡¨ç¤º%(name)sï¼šæ—¥å¿—åå­—%(funcName)sï¼šå‡½æ•°åå­—%(asctime)ï¼šæ—¥å¿—æ—¶é—´ï¼Œå¯ä»¥ä½¿ç”¨datefmtåŽ»å®šä¹‰æ—¶é—´æ ¼å¼ï¼Œå¦‚ä¸Šå›¾ã€‚%(pathname)ï¼šè„šæœ¬çš„ç»å¯¹è·¯å¾„%(filename)ï¼šè„šæœ¬çš„åå­—%(module)ï¼šæ¨¡å—çš„åå­—%(thread)ï¼šthread id%(threadName)ï¼šçº¿ç¨‹çš„åå­— datefmt 1&apos;%Y-%m-%d %H:%M:%S&apos; level é»˜è®¤ä¸ºERROR 12345logging.DEBUGlogging.INFOlogging.WARNINGlogging.ERRORlogging.CRITICAL ä¾‹å¦‚12345678910111213141516&gt;&gt;&gt; # æœªè¾“å‡ºdebug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&gt;&gt;&gt; &gt;&gt;&gt; # ä¿®æ”¹é…ç½®&gt;&gt;&gt; log_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;&gt;&gt;&gt; log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;&gt;&gt;&gt; log_level = logging.DEBUG&gt;&gt;&gt; logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level)&gt;&gt;&gt; &gt;&gt;&gt; # è¾“å‡ºdebug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&lt;pyshell#8&gt; [2018-11-13 11:59:52] [DEBUG] test log è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶ä¿å­˜ä»£ç ä¸ºæ–‡ä»¶log_test.py1234567891011121314151617181920import logginglog_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;log_level = logging.DEBUGlog_filename = &apos;./test.log&apos;log_filemode = &apos;a&apos; # ä¹Ÿå¯ä»¥ä¸º&apos;w&apos;, &apos;w+&apos;ç­‰logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level, filename=log_filename, filemode=log_filemode)logger = logging.getLogger(__name__)logger.debug(&apos;test log&apos;)logger.info(&apos;test log&apos;)logger.warning(&apos;test log&apos;)logger.error(&apos;test log&apos;)logger.critical(&apos;test log&apos;) è¿è¡Œå®Œæ¯•ï¼Œæ‰“å¼€log_test.logæ–‡ä»¶å¯ä»¥çœ‹åˆ°12345log_test.py [2018-11-13 12:11:04] [DEBUG] test loglog_test.py [2018-11-13 12:11:04] [INFO] test loglog_test.py [2018-11-13 12:11:04] [WARNING] test loglog_test.py [2018-11-13 12:11:04] [ERROR] test loglog_test.py [2018-11-13 12:11:04] [CRITICAL] test log]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Githubåšå®¢æ­å»º]]></title>
    <url>%2F2019%2F01%2F04%2FGithub-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[å‰è¨€é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼ŒçŽ°æœ‰çš„åšå®¢è¿˜æ˜¯çŽ°æœ‰çš„è¿™ç¯‡æ–‡ç« å‘¢ï¼Ÿ è½¯ä»¶å®‰è£…å®‰è£…node.js, git, hexo åšå®¢æ­å»ºåˆå§‹åŒ–æŽ¨èä½¿ç”¨gitå‘½ä»¤çª—å£ï¼Œæ‰§è¡Œå¦‚ä¸‹æŒ‡ä»¤12345678910111213141516171819202122232425262728293031$ mkdir Blog$ cd Blog$ hexo initINFO Cloning hexo-starter to ~\Desktop\BlogCloning into &apos;C:\Users\LouisHsu\Desktop\Blog&apos;...remote: Enumerating objects: 68, done.remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68Unpacking objects: 100% (68/68), done.Submodule &apos;themes/landscape&apos; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &apos;themes/landscape&apos;Cloning into &apos;C:/Users/LouisHsu/Desktop/Blog/themes/landscape&apos;...remote: Enumerating objects: 1, done.remote: Counting objects: 100% (1/1), done.remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866Receiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.Resolving deltas: 100% (459/459), done.Submodule path &apos;themes/landscape&apos;: checked out &apos;73a23c51f8487cfcd7c6deec96ccc7543960d350&apos;[32mINFO [39m Install dependenciesnpm WARN deprecated titlecase@1.1.2: no longer maintainednpm WARN deprecated postinstall-build@5.0.3: postinstall-build&apos;s behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.&gt; nunjucks@3.1.6 postinstall C:\Users\LouisHsu\Desktop\Blog\node_modules\nunjucks&gt; node postinstall-build.js srcnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)added 422 packages from 501 contributors and audited 4700 packages in 59.195sfound 0 vulnerabilitiesINFO Start blogging with Hexo! ç”Ÿæˆç›®å½•ç»“æž„å¦‚ä¸‹123456\-- scaffolds\-- source \-- _posts\-- themes|-- _config.yml|-- package.json ç»§ç»­123456$ npm installnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)audited 4700 packages in 5.99sfound 0 vulnerabilities çŽ°åœ¨è¯¥ç›®å½•æ‰§è¡ŒæŒ‡ä»¤ï¼Œå¼€å¯hexoæœåŠ¡å™¨123$ hexo sINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. ç”Ÿæˆç›®å½•å’Œæ ‡ç­¾1234$ hexo n page about$ hexo n page archives$ hexo n page categories$ hexo n page tags ä¿®æ”¹/source/tags/index.mdï¼Œå…¶ä»–åŒç†1234567891011121301| ---02| title: tags03| date: 2019-01-04 17:34:1504| ----&gt;01| ---02| title: tags03| date: 2019-01-04 17:34:1504| type: &quot;tags&quot;05| comments: false06| --- å…³è”Githubåœ¨Githubæ–°å»ºä¸€ä¸ªä»“åº“ï¼Œå‘½åä¸ºusername.github.ioï¼Œä¾‹å¦‚isLouisHsu.github.ioï¼Œæ–°å»ºæ—¶å‹¾é€‰Initialize this repository with a READMEï¼Œå› ä¸ºè¿™ä¸ªä»“åº“å¿…é¡»ä¸èƒ½ä¸ºç©ºã€‚ æ‰“å¼€åšå®¢ç›®å½•ä¸‹çš„_config.ymlé…ç½®æ–‡ä»¶ï¼Œå®šä½åˆ°æœ€åŽçš„deployé€‰é¡¹ï¼Œä¿®æ”¹å¦‚ä¸‹1234deploy: type: git repository: git@github.com:isLouisHsu/isLouisHsu.github.io.git branch: master å®‰è£…æ’ä»¶1$ npm install hexo-deployer-git --save çŽ°åœ¨å°±å¯ä»¥å°†è¯¥ç›®å½•å†…å®¹æŽ¨é€åˆ°Githubæ–°å»ºçš„ä»“åº“ä¸­äº†1$ hexo d ä½¿ç”¨ä¸ªäººåŸŸå åœ¨sourceç›®å½•ä¸‹æ–°å»ºæ–‡ä»¶CNAMEï¼Œè¾“å…¥è§£æžåŽçš„ä¸ªäººåŸŸå åœ¨Githubä¸»é¡µä¿®æ”¹åŸŸå å¤‡ä»½åšå®¢ æ²¡ã€‚æ²¡ä»€ä¹ˆç”¨æˆ‘ã€‚æˆ‘ä¸å¤‡ä»½äº†å¯ä»¥æ–°å»ºä¸€ä¸ªä»“åº“ä¸“é—¨ä¿å­˜æ–‡ä»¶è¯•è¯• çŽ°åœ¨åšå®¢çš„æºæ–‡ä»¶ä»…ä¿å­˜åœ¨PCä¸Šï¼Œ æˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œå¤‡ä»½ï¼Œå¹¶å°†ä»“åº“ä½œä¸ºåšå®¢æ–‡ä»¶å¤¹ åœ¨ä»“åº“æ–°å»ºåˆ†æ”¯hexoï¼Œè®¾ç½®ä¸ºé»˜è®¤åˆ†æ”¯ å°†ä»“åº“å…‹éš†è‡³æœ¬åœ° 1$ git clone https://github.com/isLouisHsu/isLouisHsu.github.io.git å…‹éš†æ–‡ä»¶ å°†ä¹‹å‰çš„Hexoæ–‡ä»¶å¤¹ä¸­çš„ 123456scffolds/source/themes/.gitignore_config.ymlpackage.json å¤åˆ¶åˆ°å…‹éš†ä¸‹æ¥çš„ä»“åº“æ–‡ä»¶å¤¹isLouisHsu.github.io å®‰è£…åŒ… 123$ npm install$ npm install hexo --save$ npm install hexo-deployer-git --save å¤‡ä»½åšå®¢ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤ 123$ git add .$ git commit -m &quot;backup&quot;$ git push origin hexo éƒ¨ç½²åšå®¢æŒ‡ä»¤ 1$ hexo g -d å•é”®æäº¤ ç¼–å†™è„šæœ¬commit.batï¼ŒåŒå‡»å³å¯ 1234git add .git commit -m &apos;backup&apos;git push origin hexohexo g -d ä½¿ç”¨æ–¹æ³• ç›®å½•ç»“æž„ public ç”Ÿæˆçš„ç½‘ç«™æ–‡ä»¶ï¼Œå‘å¸ƒçš„ç«™ç‚¹æ–‡ä»¶ã€‚ source èµ„æºæ–‡ä»¶å¤¹ï¼Œç”¨äºŽå­˜æ”¾å†…å®¹ã€‚ tag æ ‡ç­¾æ–‡ä»¶å¤¹ã€‚ archive å½’æ¡£æ–‡ä»¶å¤¹ã€‚ categoryåˆ†ç±»æ–‡ä»¶å¤¹ã€‚ downloads/code include codeæ–‡ä»¶å¤¹ã€‚ :lang i18n_dir å›½é™…åŒ–æ–‡ä»¶å¤¹ã€‚ _config.yml é…ç½®æ–‡ä»¶ æŒ‡ä»¤ 123456789101112131415161718192021222324252627$ hexo helpUsage: hexo &lt;command&gt;Commands: clean Remove generated files and cache. config Get or set configurations. deploy Deploy your website. generate Generate static files. help Get help on a command. init Create a new Hexo folder. list List the information of the site migrate Migrate your site from other system to Hexo. new Create a new post. publish Moves a draft post from _drafts to _posts folder. render Render files with renderer plugins. server Start the server. version Display version information.Global Options: --config Specify config file instead of using _config.yml --cwd Specify the CWD --debug Display all verbose messages in the terminal --draft Display draft posts --safe Disable all plugins and scripts --silent Hide output on consoleFor more help, you can use &apos;hexo help [command]&apos; for the detailed information or you can check the docs: http://hexo.io/docs/ æ‹“å±•åŠŸèƒ½æ”¯æŒæ’å…¥å›¾ç‰‡1$ npm install hexo-asset-image --save ä¿®æ”¹æ–‡ä»¶_config.yml1post_asset_folder: true åœ¨æ‰§è¡Œ$ hexo n [layout] &lt;title&gt;æ—¶ä¼šç”ŸæˆåŒåæ–‡ä»¶å¤¹ï¼ŒæŠŠå›¾ç‰‡æ”¾åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹å†…ï¼Œåœ¨.mdæ–‡ä»¶ä¸­æ’å…¥å›¾ç‰‡1![image_name](/title/image_name.png) æœç´¢åŠŸèƒ½12$ npm install hexo-generator-searchdb --save$ npm install hexo-generator-search --save ç«™ç‚¹é…ç½®æ–‡ä»¶_config.ymlä¸­æ·»åŠ 12345search: path: search.xml field: post format: html limit: 10000 ä¿®æ”¹ä¸»é¢˜é…ç½®æ–‡ä»¶/themes/xxx/_config.yml12local_search: enable: true å¸¦è¿‡æ»¤åŠŸèƒ½çš„é¦–é¡µæ’ä»¶åœ¨é¦–é¡µåªæ˜¾ç¤ºæŒ‡å®šåˆ†ç±»ä¸‹é¢çš„æ–‡ç« åˆ—è¡¨ã€‚12$ npm install hexo-generator-index2 --save$ npm uninstall hexo-generator-index --save ä¿®æ”¹_config.yml1234567index_generator: per_page: 10 order_by: -date include: - category Web # åªåŒ…å«Webåˆ†ç±»ä¸‹çš„æ–‡ç«  exclude: - tag Hexo # ä¸åŒ…å«æ ‡ç­¾ä¸ºHexoçš„æ–‡ç«  æ•°å­¦å…¬å¼æ”¯æŒhexoé»˜è®¤çš„æ¸²æŸ“å¼•æ“Žæ˜¯markedï¼Œä½†æ˜¯markedä¸æ”¯æŒmathjaxã€‚kramedæ˜¯åœ¨markedçš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹ã€‚1234$ npm uninstall hexo-math --save # åœæ­¢ä½¿ç”¨ hexo-math$ npm install hexo-renderer-mathjax --save # å®‰è£…hexo-renderer-mathjaxåŒ…ï¼š$ npm uninstall hexo-renderer-marked --save # å¸è½½åŽŸæ¥çš„æ¸²æŸ“å¼•æ“Ž$ npm install hexo-renderer-kramed --save # å®‰è£…æ–°çš„æ¸²æŸ“å¼•æ“Ž ä¿®æ”¹/node_modules/kramed/lib/rules/inline.js12345678911| escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,...20| em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,-&gt;11| escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,...20| em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, ä¿®æ”¹/node_modules/hexo-renderer-kramed/lib/renderer.js123456789101112131464| // Change inline math rule65| function formatText(text) &#123;66| // Fit kramed&apos;s rule: $$ + \1 + $$67| return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);68| &#125;-&gt;64| // Change inline math rule65| function formatText(text) &#123;66| // Fit kramed&apos;s rule: $$ + \1 + $$67| // return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);68| return text;69| &#125; åœ¨ä¸»é¢˜ä¸­å¼€å¯mathjaxå¼€å…³ï¼Œä¾‹å¦‚nextä¸»é¢˜ä¸­1234# MathJax Supportmathjax: enable: true per_page: true åœ¨æ–‡ç« ä¸­12345678---title: title.mddate: 2019-01-04 12:47:37categories:tags:mathjax: truetop:--- æµ‹è¯• A = \left[\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix}\right]Reference åŸºäºŽhexo+githubæ­å»ºä¸€ä¸ªç‹¬ç«‹åšå®¢ - ç‰§äº‘äº‘ - åšå®¢å›­ https://www.cnblogs.com/MuYunyun/p/5927491.htmlhexo+github pagesè½»æ¾æ­åšå®¢(1) | ex2tronâ€™s Blog http://ex2tron.wang/hexo-blog-with-github-pages-1/hexoä¸‹LaTeXæ— æ³•æ˜¾ç¤ºçš„è§£å†³æ–¹æ¡ˆ - crazy_scottçš„åšå®¢ - CSDNåšå®¢ https://blog.csdn.net/crazy_scott/article/details/79293576åœ¨Hexoä¸­æ¸²æŸ“MathJaxæ•°å­¦å…¬å¼ - ç®€ä¹¦ https://www.jianshu.com/p/7ab21c7f0674æ€Žä¹ˆåŽ»å¤‡ä»½ä½ çš„Hexoåšå®¢ - ç®€ä¹¦ https://www.jianshu.com/p/baab04284923Hexoä¸­æ·»åŠ æœ¬åœ°å›¾ç‰‡ - èœ•å˜C - åšå®¢å›­ https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referralhexo æœç´¢åŠŸèƒ½ - é˜¿ç”˜çš„åšå®¢ - CSDNåšå®¢ https://blog.csdn.net/ganzhilin520/article/details/79047983]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[EM & GMM]]></title>
    <url>%2F2018%2F11%2F12%2FEM-GMM%2F</url>
    <content type="text"><![CDATA[EMç®—æ³•Expectation Maximization Algorithmï¼Œæ˜¯ Dempster, Laind, Rubin äºŽ 1977 å¹´æå‡ºçš„æ±‚å‚æ•°æžå¤§ä¼¼ç„¶ä¼°è®¡çš„ä¸€ç§æ–¹æ³•ï¼Œå®ƒå¯ä»¥ä»Žéžå®Œæ•´æ•°æ®é›†ä¸­å¯¹å‚æ•°è¿›è¡Œ MLE ä¼°è®¡ï¼Œæ˜¯ä¸€ç§éžå¸¸ç®€å•å®žç”¨çš„å­¦ä¹ ç®—æ³•ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¹¿æ³›åœ°åº”ç”¨äºŽå¤„ç†ç¼ºæŸæ•°æ®ï¼Œæˆªå°¾æ•°æ®ï¼Œå¸¦æœ‰å™ªå£°ç­‰æ‰€è°“çš„ä¸å®Œå…¨æ•°æ®ã€‚ å¼•ä¾‹ï¼šå…ˆæŒ–ä¸ªå‘ç»™å‡ºæŽèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹çš„ä¸‰ç¡¬å¸æ¨¡åž‹ä¾‹å­ï¼Œå‡è®¾æœ‰$3$æžšç¡¬å¸$A, B, C$ï¼Œå„è‡ªå‡ºçŽ°æ­£é¢çš„æ¦‚çŽ‡åˆ†åˆ«ä¸º$\pi, p, q$ï¼Œå…ˆè¿›è¡Œå¦‚ä¸‹å®žéªŒï¼šå…ˆæŠ•æŽ·ç¡¬å¸$A$ï¼Œè‹¥ç»“æžœä¸ºæ­£é¢ï¼Œåˆ™é€‰æ‹©ç¡¬å¸$B$æŠ•æŽ·ä¸€æ¬¡ï¼Œå¦åˆ™é€‰æ‹©$C$ï¼Œè®°å½•æŠ•æŽ·ç»“æžœå¦‚ä¸‹ 1, 1, 0, 1, 0, 0, 1, 0, 1, 1åªèƒ½è§‚æµ‹åˆ°å®žéªŒç»“æžœï¼Œè€ŒæŠ•æŽ·è¿‡ç¨‹æœªçŸ¥ï¼Œå³ç¡¬å¸$A$çš„æŠ•æŽ·ç»“æžœæœªçŸ¥ï¼ŒçŽ°æ¬²ä¼°è®¡ä¸‰æžšç¡¬å¸çš„å‚æ•°$\pi, p, q$ã€‚ è§£ï¼šæ ¹æ®é¢˜æ„å¯ä»¥å¾—åˆ°ä¸‰ä¸ªéšæœºå˜é‡$X_1, X_2, X_3$çš„æ¦‚çŽ‡åˆ†å¸ƒå¦‚ä¸‹ P(X_1) = \pi ^ {X_1} (1 - \pi) ^ {1 - X_1} P(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2} P(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}å®šä¹‰éšæœºå˜é‡$X$è¡¨ç¤ºè§‚æµ‹ç»“æžœä¸ºæ­£é¢ï¼Œç”±å…¨æ¦‚çŽ‡å…¬å¼å¯ä»¥å¾—åˆ° P(X) = P(X|X_1)P(X_1) + P(X|\overline{X_1})P(\overline{X_1}) = \pi p + (1 - \pi) q P(\overline{X}) = P(\overline{X}|X_1)P(X_1) + P(\overline{X}|\overline{X_1})P(\overline{X_1}) = \pi (1 - p) + (1 - \pi) (1 - q)å³ P(X) = [\pi p + (1 - \pi) q] ^ {X} [\pi (1 - p) + (1 - \pi) (1 - q)] ^ {1 - X}åˆ©ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæœ‰ \log L(D | \theta) = 6 \log [\pi p + (1 - \pi) q] + 4 \log [\pi (1 - p) + (1 - \pi) (1 - q)]è‡³æ­¤ï¼Œæˆ‘ä»¬ä¸€å®šèƒ½æƒ³åˆ°é€šè¿‡æ±‚ä¼¼ç„¶å‡½æ•°æžå€¼æ¥æ±‚è§£å‚æ•° \frac{âˆ‚ }{âˆ‚ \pi} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{âˆ‚ }{âˆ‚ p} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{âˆ‚ }{âˆ‚ q} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0ä½†æ˜¯å¥½åƒå‡ºäº†é—®é¢˜ï¼Œå¹¶ä¸èƒ½æ±‚è§£ï¼Œæ‰€ä»¥æˆ‘ä»¬å¼•å…¥EMç®—æ³•è¿­ä»£æ±‚è§£ã€‚ æŽ¨å¯¼ä»¥$x^{(i)}$è¡¨ç¤ºè®­ç»ƒæ•°æ®ï¼Œ$w_k$è¡¨ç¤ºç±»åˆ«ï¼Œè®¾å½“å‰è¿­ä»£å‚æ•°ä¸º$\theta^{(t)}$ï¼Œåˆ™ä¸‹ä¸€æ¬¡è¿­ä»£åº”æœ‰ \theta^{(t+1)} = \arg \max \sum_i \log P(x^{(i)}|\theta) \tag{1}ç”±è¾¹ç¼˜æ¦‚çŽ‡å…¬å¼ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \tag{2} $P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)}|x^{(i)}, \theta)$è‡³æ­¤å·²å¾—å‡ºå¼•ä¾‹ä¸­çš„è¡¨è¾¾å¼ï¼Œå…¶ä¸­$P(w_k^{(i)}|x^{(i)}, \theta)$ä¸Ž$P(x^{(i)} | w_k^{(i)}, \theta)$å‡æœªçŸ¥ï¼Œè€Œé€šè¿‡æ±‚æžå€¼ä¸èƒ½è§£å¾—å‚æ•°ã€‚ æˆ‘ä»¬å¼•å…¥è¿­ä»£å‚æ•°$\theta^{(t)}$ï¼Œå³ç¬¬$t$æ¬¡è¿­ä»£æ—¶çš„å‚æ•°$\theta$ï¼Œè¯¥å‚æ•°ä¸ºå·²çŸ¥å˜é‡ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)} | \theta^{(t)})} {P(w_k^{(i)} | \theta^{(t)})} $P(w_k^{(i)}|\theta^{(t)})$è¡¨ç¤ºæ ·æœ¬$x^{(i)}$ç±»åˆ«ä¸º$w_k^{(i)}$çš„æ¦‚çŽ‡ï¼Œæ³¨æ„ä¸Šæ ‡ã€‚ å¼•å…¥Jensenä¸ç­‰å¼ï¼š For a real convex function $\varphi$, numbers $x_1, â€¦, x_n$ in its domain, and positive weights $a_i$, Jensenâ€™s inequality can be stated as: \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \leq \frac{\sum a_i \varphi(x_i)}{\sum a_i}and the inquality is reversed if $\varphi$ is concave, which is \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \geq \frac{\sum a_i \varphi(x_i)}{\sum a_i}Equality holds if and only if $x_1 = â€¦ = x_n$ or $\varphi$ is linear. $\log(Â·)$ä¸ºå‡¹å‡½æ•°(concave)ï¼Œä¸”æ»¡è¶³ \sum_k P(w_k^{(i)} | \theta^{(t)}) = 1æ‰€ä»¥æœ‰ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)}|\theta^{(t)})} {P(w_k^{(i)}|\theta^{(t)})} \geq \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{3}æ­¤æ—¶æˆ‘ä»¬å¾—åˆ°ä¼¼ç„¶å‡½æ•°$\sum_i \log P(x^{(i)}|\theta)$çš„ä¸€ä¸ªä¸‹ç•Œï¼Œä½†å¿…é¡»ä¿è¯è¿™ä¸ªä¸‹ç•Œæ˜¯ç´§çš„ï¼Œä¹Ÿå°±æ˜¯è‡³å°‘æœ‰ç‚¹èƒ½ä½¿ç­‰å·æˆç«‹ ç”±Jensenä¸ç­‰å¼ï¼Œå½“ä¸”ä»…å½“$ P(x^{(i)}, w_k^{(i)}|\theta)=C $æ—¶å–ç­‰å· å®šä¹‰ L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) - P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)})å…¶ä¸­ç¬¬ä¸€é¡¹å³æœŸæœ› E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) \tag{4}ç¬¬äºŒé¡¹ä¸º$P(w | X, \theta^{(t)})$çš„ä¿¡æ¯ç†µ H[P(w | X, \theta^{(t)})] = - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5}å³ L(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] + H[P(w | X, \theta^{(t)})] \tag{E-step} æ³¨æ„åˆ°$H[P(w | X, \theta^{(t)})]$é¡¹ä¸ºå¸¸æ•°ï¼Œæ•…ä¹Ÿå¯è®¾ Q(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] ä»£å›ž$(1)$ï¼Œå¾—åˆ°ä¼˜åŒ–ç›®æ ‡ \theta^{(t+1)} = \arg \max L(\theta|\theta^{(t)}) \tag{M-step}æˆ‘ä»¬éœ€è¦ä¸æ–­æœ€å¤§åŒ–$L(\theta | \theta^{(t)})$æ¥ä¸æ–­ä¼˜åŒ–ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„EMç®—æ³•ï¼ŒE-stepæ˜¯æŒ‡æ±‚å‡ºæœŸæœ›ï¼ŒM-stepæ˜¯æŒ‡è¿­ä»£æ›´æ–°å‚æ•° ä¼ªä»£ç å¦‚ä¸‹123456According to prior knowledge set $\theta$Repeat until convergence&#123; E-step: The expectation of hidden variables M-step: Finding the maximum of likelihood function&#125; å®žé™…ä¸Šï¼Œä»Žè¾¹ç¼˜æ¦‚çŽ‡ä¸Žæ¡ä»¶æ¦‚çŽ‡å…¥æ‰‹ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta) \geq \sum_i \sum_k P(w_k^{(i)} | \theta) \log P(x^{(i)} | w_k^{(i)}, \theta) \tag{Jensen inequality} = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)}è€Œç”±$(3)$ï¼Œå¼•å…¥è¿­ä»£å˜é‡å¯ä»¥å¾—åˆ° \sum_i \log P(x^{(i)}|\theta) \geq L(\theta|\theta^{(t)})å…¶ä¸­ L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}åˆ™ \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta)}è€Œç”±KLæ•£åº¦( Kullbackâ€“Leibler divergence)(åˆç§°ç›¸å¯¹ç†µ(relative entropy))å®šä¹‰ D(P||Q) = \sum P(x) \log \frac{P(x)}{Q(x)} å¯çŸ¥ \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = D\left[ P(w_k^{(i)}|\theta^{(t)}) || P(w_k^{(i)}|\theta) \right]å³è¿­ä»£çš„$P(w_k^{(i)}|\theta^{(t)})$ä¸ŽçœŸå®žçš„$P(w_k^{(i)}|\theta)$ä¹‹é—´çš„ç›¸å¯¹ç†µï¼ è¿™é‡Œå…³äºŽK-Læ•£åº¦çš„å›°æ‰°äº†$N$ä¹…ï¼Œç»ˆäºŽæžå‡ºæ¥äº†ã€‚ å¼•ä¾‹çš„æ±‚è§£ $Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$ æ­¤é¢˜ä¸­ P(w_k|\pi) = \pi^{w_k}(1-\pi)^{1-w_k} P(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}} P(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}} $E-step$ Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) \log P(x^{(i)}, w_k^{(i)} | \pi, p, q) å…ˆæ±‚$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$ï¼Œå³ç¬¬ä¸€æ¬¡æŠ•æŽ·ç»“æžœä¸º$w_k$çš„æ¦‚çŽ‡ P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_k}} {\sum_j \left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_j} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_j}} å³ \begin{cases} P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\ P(w_2^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \end{cases} è®° \mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})\mu_2^{(i)} = 1 - \mu_1^{(i)} æ³¨æ„$w^{(i)}_k$ä¸Šæ ‡^{(i)} å†æ±‚$P(x^{(i)}, w_k^{(i)} | \pi, p, q)$ï¼Œå·²çŸ¥ P(x^{(i)}, w_k^{(i)} | \pi, p, q) = P(x^{(i)} | w_k^{(i)}, \pi, p, q) P(w_k^{(i)} | \pi, p, q) æ‰€ä»¥ P(x^{(i)}, w_k^{(i)} | \pi, p,q) = \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} ç»¼ä¸Š Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) = \sum_i \sum_{k=1}^2 \mu^{(i)}_k \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} = \sum_i \mu_1^{(i)} \log \pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \mu_1^{(i)}) \log (1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}} $M-step$ $\frac{âˆ‚Q}{âˆ‚\pi} = 0$ \frac{âˆ‚Q}{âˆ‚\pi} = \sum_i \mu_1^{(i)} \frac {p^{x^{(i)}}(1-p)^{1-x^{(i)}}} {\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} + (1 - \mu_1^{(i)}) \frac {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}} {(1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}} = \sum_i \frac{\mu_1^{(i)}}{\pi} + \frac{\mu_1^{(i)} - 1}{1 - \pi} = \sum_i \frac{\mu_1^{(i)} - \pi}{\pi(1 - \pi)} = \frac{\sum_i \mu_1^{(i)} - n\pi}{\pi(1 - \pi)} = 0 \Rightarrow \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)} $\frac{âˆ‚Q}{âˆ‚p} = 0$ \frac{âˆ‚Q}{âˆ‚p} = \sum_i \mu_1^{(i)} \left[ \frac{x^{(i)}}{p} - \frac{1 - x^{(i)}}{1 - p} \right] = \frac{1}{p(1 - p)} \sum_i \mu_1^{(i)} (x^{(i)} - p) = \frac{1}{p(1 - p)} \left[ \sum_i \mu_1^{(i)} x^{(i)} - p \sum_i \mu_1^{(i)} \right] = 0 \Rightarrow p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}} $\frac{âˆ‚Q}{âˆ‚q} = 0$ \frac{âˆ‚Q}{âˆ‚q} = \sum_i (1 - \mu_1^{(i)}) \left[ \frac{x^{(i)}}{q} - \frac{1 - x^{(i)}}{1 - q} \right] = \frac{1}{q(1 - q)} \sum_i (1 - \mu_1^{(i)}) (x^{(i)} - q) = \frac{1}{q(1 - q)} \left[ \sum_i (1 - \mu_1^{(i)}) x^{(i)} - q \sum_i (1 - \mu_1^{(i)}) \right] = 0 \Rightarrow q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})} å¤šæ¬¡è¿­ä»£å³å¯æ±‚è§£ï¼Œç»ˆæ­¢æ¡ä»¶å¯è®¾ç½®ä¸º || \theta^{(t+1)} - \theta^{(t)} || < \epsilonæˆ– ||Q(\theta^{(t+1)} | \theta^{(t)}) - Q(\theta^{(t)} |\theta^{(t)})|| < \epsilonGMMæ¨¡åž‹Gaussian Mixture Modelï¼Œæ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œå¸¸ç”¨äºŽèšç±»ã€‚å½“èšç±»é—®é¢˜ä¸­å„ä¸ªç±»åˆ«çš„å°ºå¯¸ä¸åŒã€èšç±»é—´æœ‰ç›¸å…³å…³ç³»çš„æ—¶å€™ï¼Œå¾€å¾€ä½¿ç”¨GMMæ›´åˆé€‚ã€‚å¯¹ä¸€ä¸ªæ ·æœ¬æ¥è¯´ï¼ŒGMMå¾—åˆ°çš„æ˜¯å…¶å±žäºŽå„ä¸ªç±»çš„æ¦‚çŽ‡(é€šè¿‡è®¡ç®—åŽéªŒæ¦‚çŽ‡å¾—åˆ°)ï¼Œè€Œä¸æ˜¯å®Œå…¨çš„å±žäºŽæŸä¸ªç±»ï¼Œè¿™ç§èšç±»æ–¹æ³•è¢«æˆä¸ºè½¯èšç±»ã€‚ä¸€èˆ¬è¯´æ¥ï¼Œ ä»»æ„å½¢çŠ¶çš„æ¦‚çŽ‡åˆ†å¸ƒéƒ½å¯ä»¥ç”¨å¤šä¸ªé«˜æ–¯åˆ†å¸ƒå‡½æ•°åŽ»è¿‘ä¼¼ï¼Œå› è€Œï¼ŒGMMçš„åº”ç”¨ä¹Ÿæ¯”è¾ƒå¹¿æ³›ã€‚ é«˜æ–¯æ··åˆæ¨¡åž‹ï¼ŒæŒ‡å…·æœ‰å¦‚ä¸‹å½¢å¼çš„æ¦‚çŽ‡åˆ†å¸ƒæ¨¡åž‹ï¼š P(x|\mu_k, \Sigma_k) = \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)å…¶ä¸­ $\pi_k(0 \leq \pi_k \leq 1)$æ˜¯ç³»æ•°ï¼Œä¸”$\sum_k \pi_k = 1$ $N(x|\mu_k, \Sigma_k)$ä¸ºé«˜æ–¯å¯†åº¦å‡½æ•° N(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) \right] å³å¤šä¸ªé«˜æ–¯åˆ†å¸ƒå åŠ å‡ºæ¥çš„çŽ©æ„ï¼› çŽ°åœ¨æˆ‘ä»¬éœ€è¦æ±‚å–ç³»æ•°$\pi_k$åŠé«˜æ–¯æ¨¡åž‹çš„å‚æ•°$(\mu_k, \Sigma_k)$ï¼› ä¸ŽK-Meansç­‰èšç±»æ–¹æ³•åŒºåˆ«æ˜¯ï¼ŒGMMæ±‚å‡ºçš„æ˜¯è¿žç»­çš„åˆ†å¸ƒæ¨¡åž‹ï¼Œå¯è®¡ç®—å‡ºâ€œå½’å±žäºŽâ€å“ªä¸€ç±»çš„æ¦‚çŽ‡ã€‚ æŽ¨å¯¼ \log P(X|\pi, \mu, \Sigma) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \Sigma_k) s.t. \sum_k \pi_k = 1æš´åŠ›æ±‚è§£ä»¥$1$ç»´é«˜æ–¯åˆ†å¸ƒä¸ºä¾‹ N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}}æž„é€ æ‹‰æ ¼æœ—æ—¥(Lagrange)å‡½æ•° L(\pi, \mu, \sigma^2) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \sigma_k^2) + \lambda \left(\sum_k \pi_k - 1 \right) \tag{5} \begin{cases} \frac{âˆ‚}{âˆ‚\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{âˆ‚}{âˆ‚\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{âˆ‚}{âˆ‚\mu_k}N(x^{(i)}|\mu_k, \sigma_k^2) \\ \frac{âˆ‚}{âˆ‚\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{âˆ‚}{âˆ‚\sigma_k^2}N(x^{(i)}|\mu_k, \sigma_k^2) \end{cases} \tag{6}å…¶ä¸­ \frac{âˆ‚}{âˆ‚\mu_k} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{x-\mu_k}{\sigma_k^2} = N(x|\mu_k, \sigma_k^2) Â· \frac{x-\mu_k}{\sigma_k^2} \frac{âˆ‚}{âˆ‚\sigma_k^2} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) $\frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k}\right) = - \frac{\sigma_k^{-3}}{2}; \frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) = - \frac{1}{\sigma_k^4}$ = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(- \frac{\sigma_k^{-3}}{2}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \left(- \frac{1}{\sigma_k^4}\right) = N(x|\mu_k, \sigma_k^2) \left[ \frac{(x - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2}ä»£å›ž$(6)$å¯ä»¥å¾—åˆ° \begin{cases} \frac{âˆ‚}{âˆ‚\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{âˆ‚}{âˆ‚\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{x^{(i)}-\mu_k}{\sigma_k^2} \\ \frac{âˆ‚}{âˆ‚\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2} \end{cases} \tag{7}ä»¤ \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \tag{8} é€šä¿—ç†è§£ï¼š$\gamma^{(i)}_k$è¡¨ç¤ºæ ·æœ¬$x^{(i)}$ä¸­æ¥è‡ªç±»åˆ«$w_k$çš„â€œè´¡çŒ®ç™¾åˆ†æ¯”â€ ä»¤$\frac{âˆ‚}{âˆ‚\mu_k} \log P(X|\pi, \mu, \sigma^2) = 0$ï¼Œæ•´ç†å¾—åˆ° \sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) = 0 \Rightarrow \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} ä»¤$\frac{âˆ‚}{âˆ‚\sigma_k^2} \log P(X|\pi, \mu, \sigma^2) = 0$ï¼Œæ•´ç†å¾—åˆ° \sum_i \gamma^{(i)}_k \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] = 0 \Rightarrow \sigma_k^2 = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k)^2}{\sum_i \gamma^{(i)}_k} å¯¹äºŽ$\frac{âˆ‚}{âˆ‚\pi_k} \log P(X|\pi, \mu, \sigma^2) = 0$ï¼Œéœ€è¦åšä¸€ç‚¹å¤„ç† ä¸¤è¾¹åŒä¹˜$\pi_k$ï¼Œå¾—åˆ° \sum_i \gamma^{(i)}_k = - \lambda \pi_k \tag{9} ç„¶åŽä¸¤è¾¹å¯¹$k$ä½œç´¯åŠ  \sum_k \sum_i \gamma^{(i)}_k = - \lambda \sum_k \pi_k $\sum_k \sum_i \gamma^{(i)}_k = \sum_i \sum_k \gamma^{(i)}_k = N, \sum_k \pi_k = 1$ N = - \lambda æˆ– \lambda = -N \tag{10} ä»£å›ž$(9)$ï¼Œå¾—åˆ° \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N} ç»¼ä¸Šï¼Œæˆ‘ä»¬å¾—åˆ°$4$ä¸ªç”¨äºŽè¿­ä»£çš„è®¡ç®—å¼ï¼Œå°†å…¶æŽ¨å¹¿è‡³å¤šç»´å³ \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)} \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} \Sigma_k = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)}_k} \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}ç”¨EMç®—æ³•æ±‚è§£ $Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$ Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k) $ M-step $ P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)} = \gamma^{(i)}_k P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k) = P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k) P(w_k^{(i)}|\mu_k, \Sigma_k) = \pi_k N(x^{(i)}|\mu_k, \Sigma_k) æ•… Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k \gamma^{(i)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k) é€šè¿‡æ±‚è§£æžå€¼å¯å¾—åˆ°ä¸Ž$\underline{æš´åŠ›æ±‚è§£}$ä¸€æ ·çš„ç­‰å¼ï¼Œå³ \gamma^{(i)(t)}_k = \frac{\pi^{(t)}_k N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})} \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k} \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k} \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N} ä¼ªä»£ç ä¸º 123456789101112According to prior knowledge set \pi^&#123;(t)&#125;(n_clusters,) \mu^&#123;(t)&#125;(n_clusters, n_features) \Sigma^&#123;(t)&#125;(n_clusters, n_features, n_features)Repeat until convergence&#123; # E-step: calculate \gamma^&#123;(t)&#125; \gamma(n_samples, n_clusters) # M-step: update \pi, \mu, \Sigma \pi^&#123;(t+1)&#125;(n_clusters,) \mu^&#123;(t+1)&#125;(n_clusters, n_features) \Sigma^&#123;(t+1)&#125;(n_clusters, n_features, n_features)&#125; åˆå§‹ç‚¹çš„é€‰æ‹©å¯ä»¥éšæœºé€‰æ‹©ï¼Œä¹Ÿå¯ä½¿ç”¨K-Means GMMç®—æ³•æ”¶æ•›è¿‡ç¨‹å¦‚ä¸‹ ä»£ç @Github: GMM123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class GMM(): &quot;&quot;&quot; Gaussian Mixture Model Attributes: n_clusters &#123;int&#125; prior &#123;ndarray(n_clusters,)&#125; mu &#123;ndarray(n_clusters, n_features)&#125; sigma &#123;ndarray(n_clusters, n_features, n_features)&#125; &quot;&quot;&quot; def __init__(self, n_clusters): self.n_clusters = n_clusters self.prior = None self.mu = None self.sigma = None def fit(self, X, delta=0.01): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; delta &#123;float&#125; Notes: - Initialize with k-means &quot;&quot;&quot; (n_samples, n_features) = X.shape # initialize with k-means clf = KMeans(n_clusters=self.n_clusters) clf.fit(X) self.mu = clf.cluster_centers_ self.prior = np.zeros(self.n_clusters) self.sigma = np.zeros((self.n_clusters, n_features, n_features)) for k in range(self.n_clusters): X_ = X[clf.labels_==k] self.prior[k] = X_.shape[0] / X_.shape[0] self.sigma[k] = np.cov(X_.T) while True: mu_ = self.mu.copy() # E-step: updata gamma gamma = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): denominator = 0 for j in range(self.n_clusters): post = self.prior[k] *\ multiGaussian(X[i], self.mu[j], self.sigma[j]) denominator += post if j==k: numerator = post gamma[i, k] = numerator/denominator # M-step: updata prior, mu, sigma for k in range(self.n_clusters): sum1 = 0 sum2 = 0 sum3 = 0 for i in range(n_samples): sum1 += gamma[i, k] sum2 += gamma[i, k] * X[i] x_ = np.reshape(X[i] - self.mu[k], (n_features, 1)) sum3 += gamma[i, k] * x_.dot(x_.T) self.prior[k] = sum1 / n_samples self.mu[k] = sum2 / sum1 self.sigma[k] = sum3 / sum1 # to stop mu_delta = 0 for k in range(self.n_clusters): mu_delta += nl.norm(self.mu[k] - mu_[k]) print(mu_delta) if mu_delta &lt; delta: break return self.prior, self.mu, self.sigma def predict_proba(self, X): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples, n_clusters)&#125; &quot;&quot;&quot; (n_samples, n_features) = X.shape y_pred_proba = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): y_pred_proba[i, k] = self.prior[k] *\ multiGaussian(X[i], self.mu[k], self.sigma[k]) return y_pred_proba def predict(self, X): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples,)&#125; &quot;&quot;&quot; y_pred_proba = self.predict_proba(X) return np.argmax(y_pred_proba, axis=1)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[äºŒæ¬¡å…¥å‘raspberry-pi]]></title>
    <url>%2F2018%2F10%2F29%2F%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi%2F</url>
    <content type="text"><![CDATA[å‰è¨€è·ä¸Šä¸€æ¬¡æ­å»ºæ ‘èŽ“æ´¾å¹³å°å·²ç»ä¸¤å¹´äº†ï¼Œä¿å­˜çš„é•œåƒå‡ºäº†é—®é¢˜ï¼Œé‡æ–°æ­å»ºä¸€ä¸‹ã€‚ ç³»ç»Ÿä¸‹è½½ä»Žå®˜ç½‘ä¸‹è½½æ ‘èŽ“æ´¾ç³»ç»Ÿé•œåƒï¼Œæœ‰ä»¥ä¸‹å‡ ç§å¯é€‰ Raspberry Pi â€” Teach, Learn, and Make with Raspberry Pi Raspbian &amp; Raspbian Liteï¼ŒåŸºäºŽDebian Noobs &amp; Noobs Lite Ubuntu MATE Snappy Ubuntu Core Windows 10 IOT å…¶ä½™ä¸å¤ªäº†è§£ï¼Œä¹‹å‰å®‰è£…çš„æ˜¯Raspbianï¼Œå¯¹äºŽDebianå„ç§ä¸é€‚ï¼Œæ¢ä¸Šç•Œé¢ä¼˜é›…çš„Ubuntu MateçŽ©ä¸€ä¸‹è€è€å®žå®žçŽ©Raspbianï¼Œç¬‘è„¸:-) å®‰è£…æ¯”è¾ƒç®€å•ï¼Œå‡†å¤‡micro-SDå¡ï¼Œç”¨Win32 Disk Imagerçƒ§å†™é•œåƒ Win32 Disk Imager download | SourceForge.net å®‰è£…å®Œè½¯ä»¶åŽå¯ç‚¹å‡»Readå¤‡ä»½è‡ªå·±çš„é•œåƒã€‚ æ³¨æ„ç¬¬äºŒæ¬¡å¼€æœºå‰éœ€è¦é…ç½®config.txtæ–‡ä»¶ï¼Œå¦åˆ™hdmiæ— æ³•æ˜¾ç¤º æ ‘èŽ“æ´¾é…ç½®æ–‡æ¡£ config.txt è¯´æ˜Ž | æ ‘èŽ“æ´¾å®žéªŒå®¤ 123456disable_overscan=1 hdmi_force_hotplug=1hdmi_group=2 # DMThdmi_mode=32 # 1280x960hdmi_drive=2config_hdmi_boost=4 ä¿®æ”¹äº¤æ¢åˆ†åŒºUbuntu MateæŸ¥çœ‹äº¤æ¢åˆ†åŒº1$ free -m æœªè®¾ç½®æ—¶å¦‚ä¸‹1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 0 0 0 åˆ›å»ºå’ŒæŒ‚è½½12345678910111213141516# èŽ·å–æƒé™$ sudo -i# åˆ›å»ºç›®å½•$ mkdir /swap$ cd /swap# æŒ‡å®šä¸€ä¸ªå¤§å°ä¸º1Gçš„åä¸ºâ€œswapâ€çš„äº¤æ¢æ–‡ä»¶$ dd if=/dev/zero of=swap bs=1M count=1k# åˆ›å»ºäº¤æ¢æ–‡ä»¶$ mkswap swap# æŒ‚è½½äº¤æ¢åˆ†åŒº$ swapon swap# å¸è½½äº¤æ¢åˆ†åŒº# $ swapoff swap æŸ¥çœ‹äº¤æ¢åˆ†åŒº1$ free -m æœªè®¾ç½®æ—¶å¦‚ä¸‹1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 RaspbianWe will change the configuration in the file /etc/dphys-swapfile:1$ sudo nano /etc/dphys-swapfile The default value in Raspbian is:1CONF_SWAPSIZE=100 We will need to change this to:1CONF_SWAPSIZE=1024 Then you will need to stop and start the service that manages the swapfile own Rasbian:12$ sudo /etc/init.d/dphys-swapfile stop$ sudo /etc/init.d/dphys-swapfile start You can then verify the amount of memory + swap by issuing the following command:1$ free -m The output should look like:1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 è½¯ä»¶å®‰è£…æŒ‡ä»¤ apt-get å®‰è£…è½¯ä»¶apt-get install softname1 softname2 softname3 ... å¸è½½è½¯ä»¶apt-get remove softname1 softname2 softname3 ... å¸è½½å¹¶æ¸…é™¤é…ç½®apt-get remove --purge softname1 æ›´æ–°è½¯ä»¶ä¿¡æ¯æ•°æ®åº“apt-get update è¿›è¡Œç³»ç»Ÿå‡çº§apt-get upgrade æœç´¢è½¯ä»¶åŒ…apt-cache search softname1 softname2 softname3 ... ä¿®æ­£ï¼ˆä¾èµ–å…³ç³»ï¼‰å®‰è£…ï¼šapt-get -f insta dpkg å®‰è£….debè½¯ä»¶åŒ…dpkg -i xxx.deb åˆ é™¤è½¯ä»¶åŒ…dpkg -r xxx.deb è¿žåŒé…ç½®æ–‡ä»¶ä¸€èµ·åˆ é™¤dpkg -r --purge xxx.deb æŸ¥çœ‹è½¯ä»¶åŒ…ä¿¡æ¯dpkg -info xxx.deb æŸ¥çœ‹æ–‡ä»¶æ‹·è´è¯¦æƒ…dpkg -L xxx.deb æŸ¥çœ‹ç³»ç»Ÿä¸­å·²å®‰è£…è½¯ä»¶åŒ…ä¿¡æ¯dpkg -l é‡æ–°é…ç½®è½¯ä»¶åŒ…dpkg-reconfigure xx å¸è½½è½¯ä»¶åŒ…åŠå…¶é…ç½®æ–‡ä»¶ï¼Œä½†æ— æ³•è§£å†³ä¾èµ–å…³ç³»ï¼sudo dpkg -p package_name å¸è½½è½¯ä»¶åŒ…åŠå…¶é…ç½®æ–‡ä»¶ä¸Žä¾èµ–å…³ç³»åŒ…sudo aptitude purge pkgname æ¸…é™¤æ‰€æœ‰å·²åˆ é™¤åŒ…çš„æ®‹é¦€é…ç½®æ–‡ä»¶dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P è½¯ä»¶æº å¤‡ä»½åŽŸå§‹æ–‡ä»¶ 1$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup ä¿®æ”¹æ–‡ä»¶å¹¶æ·»åŠ å›½å†…æº 1$ vi /etc/apt/sources.list æ³¨é‡Šå…ƒæ–‡ä»¶å†…çš„æºå¹¶æ·»åŠ å¦‚ä¸‹åœ°å€ 123456789101112131415161718192021#Mirror.lupaworld.com æºæ›´æ–°æœåŠ¡å™¨ï¼ˆæµ™æ±Ÿçœæ­å·žå¸‚åŒçº¿æœåŠ¡å™¨ï¼Œç½‘é€šåŒç”µä¿¡éƒ½å¯ä»¥ç”¨ï¼Œäºšæ´²åœ°åŒºå®˜æ–¹æ›´æ–°æœåŠ¡å™¨ï¼‰ï¼šdeb http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse#Ubuntu å®˜æ–¹æº deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse æˆ–è€… 1234567891011121314151617181920212223#é˜¿é‡Œäº‘deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse#ç½‘æ˜“163deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse æ”¾ç½®éžå®˜æ–¹æºçš„åŒ…ä¸å®Œæ•´ï¼Œå¯åœ¨ä¸ºä¸æ·»åŠ å®˜æ–¹æº 1deb http://archive.ubuntu.org.cn/ubuntu-cn/ feisty main restricted universe multiverse æ›´æ–°æº 1$ sudo apt-get update æ›´æ–°è½¯ä»¶ 1$ sudo apt-get dist-upgrade å¸¸è§çš„ä¿®å¤å®‰è£…å‘½ä»¤ 1$ sudo apt-get -f install Pythonä¸»è¦æ˜¯Pythonå’Œç›¸å…³ä¾èµ–åŒ…çš„å®‰è£…ï¼Œä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å¯å¯¼å‡ºå·²å®‰è£…çš„ä¾èµ–åŒ…1$ pip freeze &gt; requirements.txt å¹¶ä½¿ç”¨æŒ‡ä»¤å®‰è£…åˆ°æ ‘èŽ“æ´¾1$ pip install -r requirements.txt æ³¨æ„pipæ›´æ–°1python -m pip install --upgrade pip æœ€æ–°ç‰ˆæœ¬ä¼šæŠ¥é”™1ImportError: cannot import name main ä¿®æ”¹æ–‡ä»¶/usr/bin/pip123from pip import mainif __name__ == &apos;__main__&apos;: sys.exit(main()) æ”¹ä¸º123from pip import __main__if __name__ == &apos;__main__&apos;: sys.exit(__main__._main()) æˆåŠŸ!!!å¤±è´¥äº†ï¼Œç¬‘è„¸:-)ï¼Œæ‰‹åŠ¨å®‰è£…å§ã€‚ã€‚ã€‚ éƒ¨åˆ†åŒ…å¯ä½¿ç”¨pip3 123$ pip3 install numpy$ pip3 install pandas$ pip3 install sklearn è‹¥éœ€è¦æƒé™ï¼ŒåŠ å…¥--user éƒ¨åˆ†åŒ…ç”¨apt-getï¼Œä½†æ˜¯ä¼˜å…ˆå®‰è£…åˆ°Python2.7ç‰ˆæœ¬ï¼Œç¬‘è„¸:-) 123$ sudo apt-get install python-scipy$ sudo apt-get install python-matplotlib$ sudo apt-get install python-opencv éƒ¨åˆ†ä»ŽPIPYä¸‹è½½.whlæˆ–.tar.gzæ–‡ä»¶ PyPI â€“ the Python Package Index Â· PyPI tensorboardX-1.4-py2.py3-none-any.whl visdom-0.1.8.5.tar.gz å®‰è£…æŒ‡ä»¤ä¸º 1$ pip3 install xxx.whl 12$ tar -zxvf xxx.tar.gz$ python setup.py install Pytorchæºç å®‰è£… pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration å®‰è£…æ–¹æ³•Installation - From Source éœ€è¦ç”¨åˆ°minicondaï¼Œå®‰è£…æ–¹æ³•å¦‚ä¸‹ï¼Œæ³¨æ„ä¸­é—´å›žè½¦æŒ‰æ…¢ä¸€ç‚¹ï¼Œæœ‰ä¸¤æ¬¡è¾“å…¥ã€‚ã€‚ã€‚ã€‚ã€‚(è¡Œæˆ‘æ…¢æ…¢çœ‹æ¡æ¬¾ä¸è¡Œä¹ˆã€‚ã€‚ç¬‘è„¸:-)) ç¬¬ä¸€æ¬¡æ˜¯æ˜¯å¦åŒæ„æ¡æ¬¾ï¼Œyes ç¬¬äºŒæ¬¡æ˜¯æ·»åŠ åˆ°çŽ¯å¢ƒå˜é‡ï¼Œyesï¼Œå¦åˆ™è‡ªå·±ä¿®æ”¹/home/pi/.bashrcæ·»åŠ åˆ°çŽ¯å¢ƒå˜é‡ 1234567891011$ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh$ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5$ sudo /bin/bash Miniconda3-latest-Linux-armv7l.sh # -&gt; change default directory to /home/pi/miniconda3$ sudo nano /home/pi/.bashrc # -&gt; add: export PATH=&quot;/home/pi/miniconda3/bin:$PATH&quot;$ sudo reboot -h now$ conda $ python --version$ sudo chown -R pi miniconda3 ç„¶åŽå°±å¯ä»¥å®‰è£…äº†æ²¡æœ‰å¯¹åº”ç‰ˆæœ¬çš„mklï¼Œç¬‘è„¸:-) 12345678910111213export CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; # [anaconda root directory]# Disable CUDAexport NO_CUDA=1# Install basic dependenciesconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typingconda install -c mingfeima mkldnn# Install Pytorchgit clone --recursive https://github.com/pytorch/pytorchcd pytorchpython setup.py install tensorflow å®‰è£…tensorflowéœ€è¦çš„ä¸€äº›ä¾èµ–å’Œå·¥å…· 1234567$ sudo apt-get update# For Python 2.7$ sudo apt-get install python-pip python-dev# For Python 3.3+$ sudo apt-get install python3-pip python3-dev å®‰è£…tensorflow è‹¥ä¸‹è½½å¤±è´¥ï¼Œæ‰‹åŠ¨æ‰“å¼€ä¸‹é¢ç½‘é¡µä¸‹è½½.whlåŒ… 1234567# For Python 2.7$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp27-none-linux_armv7l.whl$ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl# For Python 3.4$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl$ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl å¸è½½ï¼Œé‡è£…mock 1234567# For Python 2.7$ sudo pip uninstall mock$ sudo pip install mock# For Python 3.3+$ sudo pip3 uninstall mock$ sudo pip3 install mock å®‰è£…çš„ç‰ˆæœ¬tensorflow v1.1.0æ²¡æœ‰modelsï¼Œå› ä¸º1.0ç‰ˆæœ¬ä»¥åŽmodelså°±è¢«Sam Abrahamsç‹¬ç«‹å‡ºæ¥äº†ï¼Œä¾‹å¦‚classify_image.pyå°±åœ¨models/tutorials/image/imagenet/é‡Œ tensorflow/models å…¶ä½™ è¾“å…¥æ³• 12$ sudo apt-get install fcitx fcitx-googlepinyin $ fcitx-module-cloudpinyin fcitx-sunpinyin git 1$ sudo apt-get install git é…ç½®gitå’Œssh 12345$ git config --global user.name &quot;Louis Hsu&quot;$ git config --global user.email is.louishsu@foxmail.com$ ssh-keygen -t rsa -C &quot;is.louishsu@foxmail.com&quot;$ cat ~/.ssh/id_rsa.pub # æ·»åŠ åˆ°github]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2018%2F10%2F23%2FSVD%2F</url>
    <content type="text"><![CDATA[å¼•è¨€å¥‡å¼‚å€¼åˆ†è§£Singular Value Decompositionæ˜¯çº¿æ€§ä»£æ•°ä¸­ä¸€ç§é‡è¦çš„çŸ©é˜µåˆ†è§£ï¼Œå¥‡å¼‚å€¼åˆ†è§£åˆ™æ˜¯ç‰¹å¾åˆ†è§£åœ¨ä»»æ„çŸ©é˜µä¸Šçš„æŽ¨å¹¿ã€‚åœ¨ä¿¡å·å¤„ç†ã€ç»Ÿè®¡å­¦ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚ åŽŸç†ä»Žç‰¹å¾å€¼åˆ†è§£(EVD)è®²èµ·æˆ‘ä»¬çŸ¥é“å¯¹äºŽä¸€ä¸ª$n$é˜¶æ–¹é˜µ$A_{nÃ—n}$ï¼Œæœ‰ A\alpha_i = \lambda_i \alpha_i i = 1, ..., nå– P = \left[\alpha_1, \alpha_2, ..., \alpha_n\right]æœ‰ä¸‹å¼æˆç«‹ AP = P\Lambdaå…¶ä¸­ \Lambda = \left[ \begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_n \\ \end{matrix} \right] ç‰¹å¾å€¼ä¸€èˆ¬ä»Žå¤§åˆ°å°æŽ’åˆ— åˆ©ç”¨è¯¥å¼å¯å°†æ–¹é˜µ$A_{nÃ—n}$åŒ–ä½œå¯¹è§’é˜µ$\Lambda_{nÃ—n}$ \Lambda = P^{-1}APæˆ–è€… A = P \Lambda P^{-1} = \sum_{i=1}^n \lambda_i (P_{,i})(P_{,i})^{-1} â€œ$_{i}$â€è¡¨ç¤ºç¬¬$i$è¡Œï¼Œâ€œ$_{,i}$â€è¡¨ç¤ºç¬¬$i$åˆ— è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç†è§£ä¸ºï¼ŒçŸ©é˜µ$A$æ˜¯ç”±$n$ä¸ª$n$é˜¶çŸ©é˜µ$P_{,i}P^{-1}_{i}$åŠ æƒç»„æˆï¼Œç‰¹å¾å€¼$\lambda_i$å³ä¸ºæƒé‡ã€‚ ä»¥ä¸Šä¸ºä¸ªäººç†è§£ï¼Œä¸å¦¥ä¹‹å¤„å¯ä»¥æŒ‡å‡ºã€‚ å¥‡å¼‚å€¼åˆ†è§£(SVD)å®šä¹‰å¯¹äºŽé•¿æ–¹é˜µ$A_{mÃ—n}$ï¼Œä¸èƒ½è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå¯è¿›è¡Œå¦‚ä¸‹åˆ†è§£ A_{mÃ—n} = U_{mÃ—m} \Sigma_{mÃ—n} V_{nÃ—n}^Tå…¶ä¸­$U \in \mathbb{R}^{mÃ—m}, V \in \mathbb{R}^{nÃ—n}$ï¼Œå‡ä¸ºæ­£äº¤çŸ©é˜µã€‚çŸ©é˜µ$\Sigma_{mÃ—n}$å¦‚ä¸‹ å¯¹äºŽ$m&gt;n$ \Sigma_{mÃ—n} = \left[ \begin{matrix} S_{nÃ—n} \\ --- \\ O_{(m-n)Ã—n} \end{matrix} \right] å¯¹äºŽ$m&lt;n$ \Sigma_{mÃ—n} = \left[ \begin{matrix} S_{mÃ—m} & | & O_{mÃ—(n-m)} \end{matrix} \right] çŸ©é˜µ$S_{nÃ—n}$ä¸ºå¯¹è§’é˜µï¼Œå¯¹è§’å…ƒç´ ä»Žå¤§åˆ°å°æŽ’åˆ— S_{nÃ—n} = \left[ \begin{matrix} \sigma_1 & & \\ & ... & \\ & & \sigma_n\\ \end{matrix} \right]ç›´è§‚è¡¨ç¤ºSVDåˆ†è§£å¦‚ä¸‹ å½“å–$r&lt;n$æ—¶ï¼Œæœ‰éƒ¨åˆ†å¥‡å¼‚å€¼åˆ†è§£ï¼Œå¯ç”¨äºŽé™ç»´ A_{mÃ—n} = U_{mÃ—r} \Sigma_{rÃ—r} V_{rÃ—n}^Tè®¡ç®— ä»¥ä¸‹ä»…è€ƒè™‘$m&gt;n$çš„æƒ…å†µ ä»¤çŸ©é˜µ$A^T$ä¸Ž$A$ç›¸ä¹˜ï¼Œæœ‰ A^TA = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T A^TA = V \Sigma^T \Sigma V^T çŸ©é˜µ$U$ä¸ºæ­£äº¤é˜µï¼Œå³æ»¡è¶³$U^TU=I$ å…¶ä¸­ \Sigma^T \Sigma = \left[ \begin{matrix} S^T_{nÃ—n} & | & O^T_{nÃ—(m-n)} \end{matrix} \right] \left[ \begin{matrix} S_{nÃ—n} \\ --- \\ O_{(m-n)Ã—n} \end{matrix} \right] = S_{nÃ—n}^2 = \left[ \begin{matrix} \sigma_1^2 & & \\ & ... & \\ & & \sigma_n^2\\ \end{matrix} \right] åˆ™ A^T A = V S^2 V^T å³çŸ©é˜µ$A^T A$ç›¸ä¼¼å¯¹è§’åŒ–ä¸º$S^2$ï¼Œå¯¹è§’å…ƒç´ $\sigma_i^2$ä¸ŽçŸ©é˜µ$V$çš„åˆ—å‘é‡$v_i(i=1, â€¦, n)$ä¸ºçŸ©é˜µ$A^T A$çš„ç‰¹å¾å¯¹ã€‚ é‚£ä¹ˆå¯¹çŸ©é˜µ$A^T A$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œæœ‰ (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i åˆ™ v_i = \alpha^{(1)}_i \sigma_i = \sqrt{\lambda^{(1)}_i} æ³¨ï¼šå¯¹äºŽäºŒæ¬¡åž‹$x^T (A^T A) x$ x^T (A^T A) x = (Ax)^T(Ax) \geq 0æ•…çŸ©é˜µ$A^T A$åŠæ­£å®šï¼Œ$\sigma_i = \sqrt{\lambda_i}$æœ‰è§£ åŒç†ï¼Œä»¤çŸ©é˜µ$A$ä¸Ž$A^T$ç›¸ä¹˜ï¼Œå¯è¯å¾— A A^T = U \Sigma \Sigma^T U^T å…¶ä¸­ \Sigma \Sigma^T = \left[ \begin{matrix} S_{nÃ—n} \\ --- \\ O_{(m-n)Ã—n} \end{matrix} \right] \left[ \begin{matrix} S^T_{nÃ—n} & | & O^T_{nÃ—(m-n)} \end{matrix} \right] = \left[ \begin{matrix} S^2_{nÃ—n} & O_{nÃ—(m-n)} \\ O_{(m-n)Ã—n} & O_{(m-n)Ã—(m-n)} \end{matrix} \right] å³çŸ©é˜µ$A A^T$ç›¸ä¼¼å¯¹è§’åŒ–ï¼Œå¯¹è§’å…ƒç´ $\sigma_i^2$ä¸ŽçŸ©é˜µ$U$çš„åˆ—å‘é‡$u_i(i=1, â€¦, m)$ä¸ºçŸ©é˜µ$A A^T$çš„ç‰¹å¾å¯¹ã€‚ å¯¹çŸ©é˜µ$A A^T$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œæœ‰ (A^T A) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i åˆ™ u_i = \alpha^{(2)}_i \sigma_i = \sqrt{\lambda^{(2)}_i} åŒç†å¯è¯å¾—$A A^T$åŠæ­£å®šï¼Œç•¥ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œä¸ºå‡å°‘è®¡ç®—é‡ï¼Œè®¡ç®—å¥‡å¼‚å€¼åˆ†è§£åªè¿›è¡Œä¸€æ¬¡ç‰¹å¾å€¼åˆ†è§£ï¼Œå¦‚å¯¹äºŽçŸ©é˜µ$X_{mÃ—n}(m&gt;n)$ï¼Œé€‰å–$n$é˜¶çŸ©é˜µ$X^T X$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£è®¡ç®—$v_i$ï¼Œè®¡ç®—$u_i$æ–¹æ³•ä¸‹é¢ä»‹ç»ã€‚ æ ¹æ®å‰é¢æŽ¨å¯¼ï¼Œæˆ‘ä»¬æœ‰ç‰¹å¾å€¼åˆ†è§£ (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i (A A^T) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_iå…¶ä¸­$\lambda^{(1)}_i = \lambda^{(2)}_i = \sigma_i^2$ï¼Œ$v_i = \alpha^{(1)}_i$ï¼Œ$u_i = \alpha^{(2)}_i$ï¼Œå³ A^T A v_i = \sigma_i^2 v_i \tag{1} A A^T u_i = \sigma_i^2 u_i \tag{2}$(1)$å¼å·¦å³ä¹˜$A$ï¼Œæœ‰ A A^T A v_i = \sigma_i^2 A v_iå‘çŽ°ä»€ä¹ˆï¼Ÿè¿™æ˜¯å¦ä¸€ä¸ªç‰¹å¾å€¼åˆ†è§£çš„è¡¨è¾¾å¼ï¼ (A A^T) (A v_i) = \sigma_i^2 (A v_i)æ•… u_i \propto A v_i æˆ– u_i = k Â· A v_i \tag{3}çŽ°åœ¨æ±‚è§£ç³»æ•°$k$ï¼Œæ ¹æ®å®šä¹‰ A = U \Sigma V^T \Rightarrow AV = U \Sigmaåˆ™ A v_i = \sigma_i u_i \Rightarrow u_i = \frac{1}{\sigma_i} A v_iæˆ–è€… U = A V \Sigma^{-1} æ³¨ï¼šåªèƒ½æ±‚å‰$n$ä¸ª$u_i$ï¼Œä¹‹åŽçš„éœ€è¦åˆ—å†™æ–¹ç¨‹æ±‚è§£ ä¸¾æ —å°†çŸ©é˜µ$A$è¿›è¡Œåˆ†è§£ A = \left[ \begin{matrix} 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{matrix} \right]ä¸ºå‡å°‘è®¡ç®—é‡ï¼Œå–$A^T A$è®¡ç®— A^T A = \left[ \begin{matrix} 2 & 1 \\ 1 & 2 \end{matrix} \right]ç‰¹å¾å€¼åˆ†è§£ï¼Œæœ‰ A\left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] \left[ \begin{matrix} 3 & \\ & 1 \end{matrix} \right]æ•… \Sigma = \left[ \begin{matrix} \sqrt{3} & \\ & 1 \end{matrix} \right] V = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] U = A V \Sigma^{-1} = \left[ \begin{matrix} \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\ \frac{2}{\sqrt{6}} & 0 \\ \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \end{matrix} \right]123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; A = np.array([ [0, 1], [1, 1], [1, 0] ])&gt;&gt;&gt; ATA = A.T.dot(A)&gt;&gt;&gt; eigval, eigvec= np.linalg.eig(ATA)&gt;&gt;&gt; V = eigvec.copy()&gt;&gt;&gt; S = np.diag(np.sqrt(eigval))&gt;&gt;&gt; U = A.dot(V).dot(np.linalg.inv(S))&gt;&gt;&gt; Uarray([[ 0.40824829, 0.70710678], [ 0.81649658, 0. ], [ 0.40824829, -0.70710678]])&gt;&gt;&gt; Sarray([[1.73205081, 0. ], [0. , 1. ]])&gt;&gt;&gt; Varray([[ 0.70710678, -0.70710678], [ 0.70710678, 0.70710678]])&gt;&gt;&gt; # éªŒè¯&gt;&gt;&gt; U.dot(S).dot(V.T)array([[-2.23711432e-17, 1.00000000e+00], [ 1.00000000e+00, 1.00000000e+00], [ 1.00000000e+00, -2.23711432e-17]]) ç†è§£å±•å¼€è¡¨è¾¾å¼ï¼Œå–$r \leq n$æ—¶ï¼Œ A = U_{mÃ—r} \Sigma_{rÃ—r} V_{rÃ—n}^T = \sum_{i=1}^r \sigma_i (U_{,i}) (V_{,i})^Tå°±å¾—åˆ°ä¸ŽPCAç›¸åŒçš„ç»“è®ºï¼ŒçŸ©é˜µ$A$å¯ç”±$r$ä¸ª$mÃ—n$çš„çŸ©é˜µ$(U_{,i}) (V_{,i})^T$åŠ æƒç»„æˆã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå‰$10\%$ç”šè‡³$1\%$çš„å¥‡å¼‚å€¼å°±å äº†å…¨éƒ¨å¥‡å¼‚å€¼ä¹‹å’Œçš„$99\%$ï¼Œæžå¤§åœ°ä¿ç•™äº†ä¿¡æ¯ï¼Œè€Œå¤§å¤§å‡å°‘äº†å­˜å‚¨ç©ºé—´ã€‚ ä»¥å›¾ç‰‡ä¸ºä¾‹ï¼Œè‹¥åŽŸæœ‰24bitå›¾ç‰‡ï¼Œå…¶å¤§å°ä¸º(1024, 768)ï¼Œåˆ™ä¸è®¡å›¾ç‰‡ä¿¡æ¯ï¼Œä»…ä»…æ•°æ®å…±å 1024Ã—768Ã—3 Bï¼Œæˆ–2.25 MBã€‚ç”¨å¥‡å¼‚å€¼åˆ†è§£è¿›è¡ŒåŽ‹ç¼©ï¼Œä¿ç•™$60\%$çš„å¥‡å¼‚å€¼ï¼Œå¯è¾¾åˆ°å‡ ä¹Žæ— æŸçš„ç¨‹åº¦ï¼Œæ­¤æ—¶éœ€è¦ä¿å­˜å‘é‡çŸ©é˜µ$U_{1024Ã—60}$ï¼Œ$V_{60Ã—768}$ä»¥åŠ$60$ä¸ªå¥‡å¼‚å€¼ï¼Œä»¥æµ®ç‚¹æ•°float32å­˜å‚¨ï¼Œä¸€å…±å 420 KBå³å¯ã€‚ (1024 Ã— 60 + 60 Ã— 768 + 60) Ã— 4 / 2^{10} = 420.23è¯´å¥é¢˜å¤–è¯ï¼Œå­˜å‚¨é‡çš„åŽ‹ç¼©å¿…ç„¶ä»¥è®¡ç®—é‡çš„å¢žå¤§ä¸ºä»£ä»·ï¼Œç›¸åäº¦ç„¶ï¼Œæ‰€ä»¥éœ€è¦åè°ƒå¥½RAMä¸ŽROMå®¹é‡ï¼Œè€ƒè™‘è®¡ç®—æœºçš„è®¡ç®—é€Ÿåº¦ã€‚æ¢å¥è¯è¯´ï¼Œç©ºé—´å’Œæ—¶é—´ä¸Šå¿…ç„¶æ˜¯äº’è¡¥çš„ï¼Œå“²å­¦çš„å‘³é“hhhhã€‚ åˆ†è§£ç»“æžœçš„ä¿¡æ¯ä¿ç•™åˆ†è§£åŽå„æ ·æœ¬é—´çš„æ¬§å¼è·ç¦»ä¸Žè§’åº¦ä¿¡æ¯åº”ä¸å˜ï¼Œç»™å‡ºè¯æ˜Žå¦‚ä¸‹è®¾æœ‰$m$ç»„$n$ç»´æ ·æœ¬æ ·æœ¬ X_{nÃ—m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]ç»å¥‡å¼‚å€¼åˆ†è§£ï¼Œæœ‰ X_{nÃ—m} = U_{nÃ—r} \Sigma_{rÃ—r} V_{rÃ—m}^Tè®° Z_{rÃ—m} = \Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]æœ‰ X = U Z æ¬§å¼è·ç¦» || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2 = \left[ U (Z^{(i)} - Z^{(j)}) \right]^T \left[ U (Z^{(i)} - Z^{(j)}) \right] = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)}) = || Z^{(i)} - Z^{(j)} ||_2^2 å³ || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2 è§’åº¦ä¿¡æ¯ \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \sqrt{(UZ^{(j)})^T(UZ^{(j)})}} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} å³ \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} ä»£ç @Github: Code of SVDå¯¹å›¾ç‰‡è¿›è¡Œäº†åˆ†è§£1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798class SVD(): &quot;&quot;&quot; Singular Value Decomposition Attributes: m &#123;int&#125; n &#123;int&#125; r &#123;int&#125;: if r == -1, then r = n isTrains &#123;bool&#125;: isTrains = True if input.shape[0] &lt; input.shape[1] U &#123;ndarray(m, r)&#125; S &#123;ndarray(r, )&#125; V &#123;ndarray(n, r)&#125; Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - Reassign r if eigvals contains zero - Singular values are stored in a 1-dim array `S` - X&apos; = U S V^T &quot;&quot;&quot; def __init__(self, r=-1): self.m = None self.n = None self.r = r self.isTrans = False self.U = None self.S = None self.V = None def fit(self, X): &quot;&quot;&quot; calculate components Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - reassign self.r if eigvals contains zero &quot;&quot;&quot; (self.m, self.n) = X.shape if self.m &lt; self.n: X = X.T self.m, self.n = self.n, self.m self.isTrans = True self.r = self.n if (self.r == -1) else self.r XTX = X.T.dot(X) eigval, eigvec = np.linalg.eig(X.T.dot(X)) eigval, eigvec = np.real(eigval), np.real(eigvec) self.S = np.sqrt(np.clip(eigval, 0, float(&apos;inf&apos;))) self.S = self.S[self.S &gt; 0] self.r = min(self.r, self.S.shape[0]) # reassign self.r order = np.argsort(eigval)[::-1][: self.r] # sort eigval from large to small eigval = eigval[order]; eigvec = eigvec[:, order] self.V = eigvec.copy() self.U = X.dot(self.V).dot( np.linalg.inv(np.diag(self.S))) return self.U, self.S, self.V def compose(self, r=-1): &quot;&quot;&quot; merge first r components Parameters: r &#123;int&#125;: if r==-1, merge all components Returns: X &#123;ndarray(m, n)&#125; &quot;&quot;&quot; if r == -1: X = self.U.dot(np.diag(self.S)).dot(self.V.T) X = X.T if self.isTrans else X else: (m, n) = (self.n, self.m) if self.isTrans else (self.m, self.n) X = np.zeros(shape=(m, n)) for i in range(r): X += self.__getitem__(i) return X def __getitem__(self, idx): &quot;&quot;&quot; get a component Parameters: index &#123;int&#125;: range from (0, self.r) &quot;&quot;&quot; u = self.U[:, idx] v = self.V[:, idx] s = self.S[idx] x = s * u.reshape(self.m, 1).\ dot(v.reshape(1, self.n)) x = x.T if self.isTrans else x return x def showComponets(self, r=-1): &quot;&quot;&quot; display components Notes: - Resize components&apos; shape into (40, 30) &quot;&quot;&quot; m, n = self.m, self.n r = self.r if r==-1 else r n_images = 10; m_images = r // n_images + 1 m_size, n_size = 40, 30 showfig = np.zeros(shape=(m_images*m_size, n_images*n_size)) for i in range(r): m_pos = i // n_images n_pos = i % n_images component = self.__getitem__(i) component = component.T if self.isTrans else component component = cv2.resize(component, (30, 40)) showfig[m_pos*m_size: (m_pos+1)*m_size, n_pos*n_size: (n_pos+1)*n_size] = component plt.figure(&apos;components&apos;) plt.imshow(showfig) plt.show() ç”¨ä¸Šé¢çš„ä»£ç è¿›è¡Œå®žéªŒ1234567891011121314# è¯»å–ä¸€å¼ å›¾ç‰‡X = load_images()[0].reshape((32, 32))showmat2d(X)# å¯¹å›¾ç‰‡è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£decomposer = SVD(r=-1)decomposer.fit(X)# æ˜¾ç¤ºä¸€ä¸‹åˆ†é‡decomposer.showComponets(r=-1)# å°†å…¨éƒ¨åˆ†é‡ç»„åˆï¼Œå¹¶æ˜¾ç¤ºX_ = decomposer.compose(r=-1)showmat2d(X_)# å°†å‰5ä¸ªåˆ†é‡ç»„åˆï¼Œå¹¶æ˜¾ç¤ºX_ = decomposer.compose(r=5)showmat2d(X_) è½½å…¥åŽŸå›¾å¦‚ä¸‹ åˆ†é‡æ˜¾ç¤ºå¦‚ä¸‹ ç»„åˆåˆ†é‡æ˜¾ç¤ºå¦‚ä¸‹ ç»„åˆå…¨éƒ¨ ç»„åˆå‰5ä¸ªåˆ†é‡]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PCA]]></title>
    <url>%2F2018%2F10%2F22%2FPCA%2F</url>
    <content type="text"><![CDATA[å¼•è¨€PCAå…¨ç§°Principal Component Analysisï¼Œå³ä¸»æˆåˆ†åˆ†æžï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°æ®é™ç»´æ–¹æ³•ã€‚å®ƒå¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢å°†åŽŸå§‹æ•°æ®å˜æ¢ä¸ºä¸€ç»„å„ç»´åº¦çº¿æ€§æ— å…³çš„è¡¨ç¤ºï¼Œä»¥æ­¤æ¥æå–æ•°æ®çš„ä¸»è¦çº¿æ€§åˆ†é‡ã€‚ å‘é‡çš„æŠ•å½±çŽ°æœ‰ä¸¤ä¸ªä»»æ„ä¸å…±çº¿å‘é‡$\vec{u}, \vec{v}$ï¼Œå°†$\vec{u}$æŠ•å°„åˆ°$\vec{v}$ä¸Š æŠ•å½±åŽï¼Œå¯ä»¥å¾—åˆ°ä¸¤ä¸ªæ­£äº¤å‘é‡ \vec{u}' Â· (\vec{u} - \vec{u}') = 0æˆ‘ä»¬è®¾ \vec{u}' = \mu \vec{v} \tag{1}ä»£å…¥åŽæœ‰ \mu \vec{v} Â· (\vec{u} - \mu \vec{v}) = 0å¼•å…¥çŸ©é˜µè¿ç®—ï¼Œå³ (\mu v)^T (u - \mu v) = 0æœ‰ v^T u = \mu v^T våˆ™å¾—åˆ°$uâ€™$ä»¥$v$ä¸ºåŸºå‘é‡çš„åæ ‡ \mu = (v^T v)^{-1} v^T u \tag{2}æ‰€ä»¥å¾—åˆ° u' = v (v^T v)^{-1} v^T u \tag{*} åæ ‡å˜æ¢æ±‚è§£æŠ•å½±å‘é‡ï¼š$uâ€™$å¯è§†ä½œ$u$ç»åæ ‡å˜æ¢$uâ€™ = P u$å¾—åˆ°ï¼Œæ‰€ä»¥ P = v (v^T v)^{-1} v^T æŽ¨å¹¿è‡³å¤šä¸ªå‘é‡çš„æŠ•å½±ï¼Œå³å¾—åˆ° P = X (X^T X)^{-1} X^Tè¿™ä¸Žçº¿æ€§å›žå½’ä¸­å¾—åˆ°çš„ç»“è®ºä¸€è‡´ã€‚ å®žé™…ä¸Š u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T uè®°å•ä½å‘é‡$\frac{v}{||v||}$ä¸º$v_0$ï¼Œå¾—åˆ° u' = v_0 v_0^T uç”±å‡ ä½•å…³ç³»ï¼Œå¯ä»¥è®¡ç®—å¾—æŠ•å½±åŽçš„é•¿åº¦ä¸º d = ||u|| \cos \theta = ||u|| \frac{v^T u}{||u||||v||} = v_0^T uæ‰€ä»¥åœ¨å‘é‡æŠ•å½±ä¸­ï¼Œ$u^T v_0$è¡¨ç¤ºä»¥$v_0$ä¸ºåŸºå‘é‡çš„åæ ‡ã€‚ PCAçŽ°åœ¨æœ‰$N$ç»´æ•°æ®é›†$D=\{x^{(1)}, x^{(2)}, â€¦, x^{(M)}\}$ï¼Œå…¶ä¸­$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, â€¦, x^{(i)}_N\right]^T$ï¼Œå„ç»´ç‰¹å¾$D_{j}$é—´å­˜åœ¨çº¿æ€§ç›¸å…³æ€§ï¼Œåˆ©ç”¨ä¸»æˆåˆ†åˆ†æžå¯ä½¿ æ•°æ®ç»´åº¦é™ä½Žï¼› æå–ä¸»æˆåˆ†ï¼Œä¸”å„æˆåˆ†é—´ä¸ç›¸å…³ã€‚ è¯´æ˜Ž ç”±äºŽé€‰å–çš„ç‰¹å¾è½´æ˜¯æ­£äº¤çš„ï¼Œæ‰€ä»¥è®¡ç®—ç»“æžœçº¿æ€§æ— å…³ï¼› æå–äº†æ–¹å·®è¾ƒå¤§çš„å‡ ä¸ªç‰¹å¾ï¼Œä¸ºä¸»è¦çº¿æ€§åˆ†é‡ã€‚ ä»¥äºŒç»´ç©ºé—´ä¸­çš„æ•°æ®$x^{(i)} = \left[\begin{matrix} x^{(i)}_1 \\ x^{(i)}_2\end{matrix}\right]$ä¸ºä¾‹ï¼Œç»´åº¦å¯é™è‡³ä¸€ç»´ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ ä¸»è½´å¯æœ‰æ— ç©·å¤šç§é€‰æ‹©ï¼Œé‚£ä¹ˆé—®é¢˜å°±æ˜¯å¦‚ä½•é€‰å–æœ€ä¼˜çš„ä¸»è½´ã€‚å…ˆç»™å‡ºPCAçš„è®¡ç®—æ­¥éª¤ã€‚ è®¡ç®—æ­¥éª¤è¾“å…¥çš„$M$ä¸ª$N$ç»´æ ·æœ¬ï¼Œæœ‰æ ·æœ¬çŸ©é˜µ X_{NÃ—M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right] = \left[ \begin{matrix} x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\ x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\ ... \\ x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\ \end{matrix} \right]æŠ•å½± å¯¹æ¯ä¸ªç»´åº¦(è¡Œ)è¿›è¡ŒåŽ»å‡å€¼åŒ– X_j := X_j - \mu_j å…¶ä¸­$\mu_j = \overline{X_j}$ï¼Œ$j = 1, 2, â€¦, N$ æ±‚å„ç»´åº¦é—´çš„åæ–¹å·®çŸ©é˜µ$\Sigma_{NÃ—N}$ \Sigma_{ij} = Cov(x_i, x_j) æˆ– \Sigma = \frac{1}{M} X X^T æ³¨ï¼š X X^T = \left[ \begin{matrix} \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\ \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M \left[ \begin{matrix} x^{(i)}_1 x^{(i)}_1 & x^{(i)}_1 x^{(i)}_2 & ... & x^{(i)}_1 x^{(i)}_N \\ x^{(i)}_2 x^{(i)}_1 & x^{(i)}_2 x^{(i)}_2 & ... & x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ x^{(i)}_N x^{(i)}_1 & x^{(i)}_N x^{(i)}_2 & ... & x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M x^{(i)} x^{(i)T} åæ–¹å·®å®šä¹‰å¼ Cov(x,y)â‰\frac{1}{n-1} âˆ‘_{i=1}^n (x_iâˆ’\overline{x})^T(y_iâˆ’\overline{y})å…¶ä¸­$x=[x_1, x_2, â€¦, x_n]^T, y=[y_1, y_2, â€¦, y_n]^T$ æ±‚åæ–¹å·®çŸ©é˜µ$\Sigma$çš„ç‰¹å¾å€¼$Î»_i$åŠå…¶å¯¹åº”ç‰¹å¾å‘é‡$Î±_i$ï¼Œ$i=1, â€¦, N$ï¼› æŒ‰ç…§ç‰¹å¾å€¼ä»Žå¤§åˆ°å°æŽ’åˆ—ç‰¹å¾å¯¹$(Î»_i,Î±_i)$ï¼Œé€‰å–$K$ä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ä½œä¸ºé™ç»´åŽçš„ä¸»è½´$ \beta_1, \beta_2, â€¦, \beta_K $ï¼Œå…¶ä¸­$\beta_k$ä¸ºå•ä½å‘é‡ \beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^Tè®° B_{NÃ—K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]$K$çš„é€‰å–æ–¹æ³•æœ‰å¦‚ä¸‹ä¸¤ç§ï¼š æŒ‡å®šé€‰å–$K$ä¸ªä¸»è½´ ä¿ç•™$99\%$çš„æ–¹å·®\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99 å°†æ ·æœ¬ç‚¹æŠ•å°„åˆ°$K$ç»´åæ ‡ç³»ä¸Š æ ·æœ¬$X^{(i)}$æŠ•å°„åˆ°ä¸»æˆåˆ†è½´$\beta_k$ä¸Šï¼Œå…¶åæ ‡è¡¨ç¤ºä¸ºå‘é‡ï¼Œä¸º S^{(i)}_k = X^{(i)T}\beta_k æ³¨æ„æ­¤æ—¶çš„åŸºåº§æ ‡ä¸º$\beta_k$ï¼Œæˆ–è€…è¯´$Xâ€™^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$ æ‰€æœ‰æ ·æœ¬åœ¨ä¸»è½´$\beta_k$ä¸Šçš„æŠ•å½±åæ ‡å³ S = B^T X å…¶ä¸­$S_{KÃ—M}$ï¼Œ$B_{NÃ—K}$ï¼Œ$X_{NÃ—M}$ æ³¨ï¼šè‹¥å–$K=N$ï¼Œå¯é‡å»ºæ•°æ®ï¼Œå¦‚ä¸‹ å¤åŽŸç¬¬$5$æ­¥ä¸­ï¼Œæ ·æœ¬ç‚¹å‘é‡$X^{(i)}$çš„ä¸»è¦åˆ†é‡æŠ•å°„åˆ°$K$ä¸ª$N$ç»´å‘é‡ä¸Šï¼ŒæŠ•å½±åæ ‡ä¸º$S^{(i)}_k$ï¼Œå³ X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_kä»¥ä¸Šå°±æ˜¯æ ·æœ¬ç‚¹çš„å¤åŽŸå…¬å¼ï¼ŒçŸ©é˜µå½¢å¼å³ \hat{X} = BSå…¶ä¸­$\hat{X}_{NÃ—M}$ï¼Œ$B_{NÃ—K}$ï¼Œ$S_{KÃ—M}$ è€ƒè™‘åˆ°å·²åŽ»å‡å€¼åŒ–ï¼Œæ•… \hat{X}_j \approx \hat{X}_j + \mu_jè¯æ˜Ž æŠ•å½±å‘é‡çš„$2$èŒƒæ•°æœ€å¤§ï¼Œæˆ–è€…è¯´ï¼ŒæŠ•å½±åŽçš„åæ ‡å¹³æ–¹å’Œæœ€å¤§ å½“æ‰€æœ‰æ ·æœ¬$X$æŠ•å°„åˆ°ç¬¬ä¸€ä¸»è½´$\beta_1$ä¸Šï¼Œå…¶åæ ‡ä¸º S_1 = X^T \beta_1æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œï¼Œæˆ–å‘é‡$S_1$çš„$2$èŒƒæ•°ä¸º ||S_1||_2^2 = S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}å³ä¼˜åŒ–ç›®æ ‡ä¸º \max ||S_1||_2^2 s.t. ||\beta_1||_2^2 = 1çŸ©é˜µ$C=XX^T$ä¸ºå¯¹ç§°çŸ©é˜µï¼Œæ•…å¯å•ä½æ­£äº¤åŒ– C = W \Lambda W^T W = \left[\begin{matrix} | & & |\\ w_1 & ... & w_M\\ | & & |\\ \end{matrix}\right] \Lambda = \left[\begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_M\\ \end{matrix}\right]å…¶ä¸­$\lambda_1 &gt; â€¦&gt; \lambda_M$ï¼Œ$w_i(i=1,â€¦,M)$ä¸ºçŸ©é˜µ$C$çš„ç‰¹å¾å‘é‡(å•ä½å‘é‡ï¼Œäº’ç›¸æ­£äº¤) å®žé™…ä¸Š$R(C) \leq (n-1)$ï¼Œå³æœ€å¤šæœ‰$(n-1)$ä¸ªç‰¹å¾å€¼å¤§äºŽ$0$ã€‚ ||S_1||_2^2 = \beta_1^T W \Lambda W^T \beta_1 \tag{2}ä»¤$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$ï¼Œå¯å¾— ||S_1||_2^2 = \alpha_1^T \Lambda \alpha_1 \tag{3}å³ ||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \tag{4}è¿›ä¸€æ­¥ \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \leq \lambda_1 \sum_{i=1}^M \alpha_{1i}^2 \tag{5}ä¸”ç”±äºŽ$\beta_1^T\beta_1 = 1$ï¼Œæ•… 1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = \sum_{i=1}^M \alpha_{1i}^2å¯å¾— ||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \leq \lambda_1 \tag{6}ä¸ºä½¿$(6)$å–ç­‰å·ï¼Œå³è¾¾æœ€å¤§å€¼ï¼Œå¯ä½¿ \begin{cases} \alpha_{11} = 1 \\ \alpha_{12} = ... = \alpha_{1M} = 0 \end{cases}å³ä»¤ \beta_1 = W \alpha_1 = w_1 $\alpha_1 = [1, 0, â€¦, 0]^T$ æ‰€ä»¥$\beta_1$å¯¹åº”çŸ©é˜µ$C=XX^T$çš„ç‰¹å¾å‘é‡$w_1$ï¼Œä¸”æœ‰ ||S_1||_2^2 = \lambda_1 æˆ–è€…ç¬¬ä¸€ä¸»æˆåˆ†çš„è¯æ˜Žä¹Ÿå¯ä»¥è¿™æ ·ï¼Œå»ºç«‹ä¼˜åŒ–ç›®æ ‡ \beta_1 = \arg \max ||S_1||_2^2s.t. ||\beta_1||_2^2 = 1æž„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•° L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)ä¹Ÿå³ L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)æ±‚å…¶æžå€¼ç‚¹ â–½_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0æœ‰ X X^T \beta_1 = \lambda_1 \beta_1å¯è§$\beta_1$å³æ–¹é˜µ$X X^T$çš„ç‰¹å¾å‘é‡ å½“æˆ‘ä»¬å¸Œæœ›ç”¨æ›´å¤šçš„ä¸»æˆåˆ†åˆ»ç”»æ•°æ®ï¼Œå¦‚å·²ç»æ±‚å¾—ä¸»æˆåˆ†$\beta_1, â€¦, \beta_{r-1}$ï¼Œå…ˆéœ€æ±‚è§£$\beta_r$ï¼Œå¼•å…¥æ­£äº¤çº¦æŸ$\beta_r^T \beta_i = 0$ï¼Œå³ç›®æ ‡å‡½æ•°ä¸º ||S_r||_2^2 = \beta_r^T C \beta_r s.t. \beta_r^T \beta_i = 0, i = 1, ..., r-1 ||\beta_r||_2^2 = 1ä»¤$\beta_r = W \alpha_r$ï¼Œåˆ™ ||S_r||_2^2 = \alpha_r^T \Lambda \alpha_r = \sum_i \lambda_i \alpha_{ri}^2è€Œæ ¹æ®æ­£äº¤çº¦æŸ 0 = \beta_r^T \beta_i = \alpha_r^T W^T w_i = \alpha_{ri}, i = 1, ..., r-1 $ W^T w_i = \left[0, â€¦, 1_i, â€¦, 0\right]^T$ æ‰€ä»¥ ||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{5}åˆå› ä¸º$\beta_r^T \beta_r = 1$(å•ä½å‘é‡)ï¼Œæ•… \beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1äºŽæ˜¯ç±»ä¼¼çš„ï¼Œä¸ºä½¿$(5)$å–æœ€å¤§ï¼Œå– \begin{cases} \alpha_{rr} = 1\\ \alpha_{ri} = 0, i = 1, ..., M, i \neq r \end{cases} $\alpha_r = [0, â€¦, 1_r, â€¦, 0]$ åˆ™æ­¤æ—¶ \beta_r = W \alpha_r = w_rä¸”æœ‰ ||S_r||_2^2 = \lambda_rè¯æ¯•ã€‚ ç™½åŒ–(whitening)whiteningçš„ç›®çš„æ˜¯åŽ»æŽ‰æ•°æ®ä¹‹é—´çš„ç›¸å…³è”åº¦ï¼Œæ˜¯å¾ˆå¤šç®—æ³•è¿›è¡Œé¢„å¤„ç†çš„æ­¥éª¤ã€‚æ¯”å¦‚è¯´å½“è®­ç»ƒå›¾ç‰‡æ•°æ®æ—¶ï¼Œç”±äºŽå›¾ç‰‡ä¸­ç›¸é‚»åƒç´ å€¼æœ‰ä¸€å®šçš„å…³è”ï¼Œæ‰€ä»¥å¾ˆå¤šä¿¡æ¯æ˜¯å†—ä½™çš„ã€‚è¿™æ—¶å€™åŽ»ç›¸å…³çš„æ“ä½œå°±å¯ä»¥é‡‡ç”¨ç™½åŒ–æ“ä½œã€‚ æ•°æ®çš„whiteningå¿…é¡»æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š ä¸åŒç‰¹å¾é—´ç›¸å…³æ€§æœ€å°ï¼ŒæŽ¥è¿‘$0$ï¼› æ‰€æœ‰ç‰¹å¾çš„æ–¹å·®ç›¸ç­‰ï¼ˆä¸ä¸€å®šä¸º$1$ï¼‰ã€‚ å¸¸è§çš„ç™½åŒ–æ“ä½œæœ‰PCA whiteningå’ŒZCA whiteningã€‚ Whitening - Ufldl PCA whitening PCA whiteningæŒ‡å°†æ•°æ®$X$ç»è¿‡PCAé™ç»´ä¸º$S$åŽï¼Œå¯ä»¥çœ‹å‡º$S$ä¸­æ¯ä¸€ç»´æ˜¯ç‹¬ç«‹çš„ï¼Œæ»¡è¶³whiteningçš„ç¬¬ä¸€ä¸ªæ¡ä»¶ï¼Œè¿™æ˜¯åªéœ€è¦å°†$S$ä¸­çš„æ¯ä¸€ç»´éƒ½é™¤ä»¥æ ‡å‡†å·®å°±å¾—åˆ°äº†æ¯ä¸€ç»´çš„æ–¹å·®ä¸º$1$ï¼Œä¹Ÿå°±æ˜¯è¯´æ–¹å·®ç›¸ç­‰ã€‚ X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}} ZCA whitening ZCA whiteningæ˜¯æŒ‡æ•°æ®$X$å…ˆç»è¿‡PCAå˜æ¢ä¸º$S$ï¼Œä½†æ˜¯å¹¶ä¸é™ç»´ï¼Œå› ä¸ºè¿™é‡Œæ˜¯æŠŠæ‰€æœ‰çš„æˆåˆ†éƒ½é€‰è¿›åŽ»äº†ã€‚è¿™æ˜¯ä¹ŸåŒæ ·æ»¡è¶³whtienningçš„ç¬¬ä¸€ä¸ªæ¡ä»¶ï¼Œç‰¹å¾é—´ç›¸äº’ç‹¬ç«‹ã€‚ç„¶åŽåŒæ ·è¿›è¡Œæ–¹å·®ä¸º$1$çš„æ“ä½œï¼Œæœ€åŽå°†å¾—åˆ°çš„çŸ©é˜µå·¦ä¹˜ä¸€ä¸ªç‰¹å¾å‘é‡çŸ©é˜µ$U$å³å¯ã€‚ X_{ZCAwhite} = U Â· X_{PCAwhite} Kernel PCAKernel PCAçš„æ€æƒ³æ˜¯åœ¨é«˜ç»´çš„ç‰¹å¾ç©ºé—´ä¸­æ±‚è§£åæ–¹å·®çŸ©é˜µ \Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^Tå…¶ä¸­$\Phi(X^{(i)})$è¡¨ç¤ºå°†æ ·æœ¬$i$æ˜ å°„åˆ°é«˜ç»´ç©ºé—´åŽä¸­çš„å‘é‡ï¼Œå³ \Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^Tå…¶ä¸­$Nâ€™ &gt; N$ï¼Œç”±äºŽ$\Phi(X^{(i)})$ä¸ºéšå¼çš„ï¼Œæ•…è®¾ç½®æ ¸å‡½æ•°æ±‚è§£ï¼Œè®° \kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T å…³äºŽæ ¸æŠ€å·§ï¼Œç§»æ­¥éžçº¿æ€§æ”¯æŒå‘é‡æœº åº”ç”¨å¯åˆ©ç”¨PCAä¸Žçº¿æ€§å›žå½’æ±‚è§£$3$ç»´ç©ºé—´ä¸­å¹³é¢çš„æ³•å‘é‡ åˆ©ç”¨PCAé‡å»ºæ•°æ®(ä¸é™ç»´ï¼Œæ­¤æ—¶ä¸º$3$ç»´)ï¼Œæ­¤æ—¶ç¬¬$1, 2$ä¸»æˆåˆ†è½´å¯å¼ æˆæ‰€æ±‚å¹³é¢ï¼Œå³è¯¥å¹³é¢å¯è¡¨ç¤ºä¸º \Pi = span \{ \beta_1, \beta_2 \} å°±æ˜¯è¯´ï¼Œç¬¬ä¸€ã€äºŒä¸»æˆåˆ†æ˜¯è¿™äº›ç‚¹â€œæ‹‰ä¼¸â€æœ€å¤§çš„æ–¹å‘ :-)ï¼Œå¥½æ‡‚ä¸ï¼Ÿ ç”±æ­£äº¤æŠ•å½±å¯çŸ¥ï¼Œå¹³é¢å¤–ä¸€ç‚¹$y$å¯é€šè¿‡æœ€å°äºŒä¹˜(çº¿æ€§å›žå½’)çš„æ–¹æ³•æŠ•å°„åˆ°å¹³é¢ä¸Šï¼Œå‘é‡è¿ç®—ï¼Œä¸è€ƒè™‘åç½®é¡¹ï¼Œå³ \hat{y} = \theta_1 x_1 + \theta_2 x_2 \tag{*} å…¶ä¸­$x_1, x_2$è¡¨ç¤ºç¬¬ä¸€ã€ç¬¬äºŒä¸»æˆåˆ†$\beta_1, \beta_2$ï¼Œä¸º$3$ç»´å‘é‡ \hat{y} = \left[ \begin{matrix} \hat{y_1} \\ \hat{y_2} \\ \hat{y_3} \\ \end{matrix} \right] x_i = \left[ \begin{matrix} x_{i1} \\ x_{i2} \\ x_{i3} \\ \end{matrix} \right] å¯åˆ©ç”¨å…¬å¼æ±‚è§£å›žå½’å‚æ•°$\theta$ \theta = (X^TX+\lambda I)^{-1} X^T y æ³¨æ„ï¼š$X(n_samples, n_features)$ï¼Œè¿™é‡ŒæŠŠ$(x_{1j}, x_{2j}, y_{j})ä½œä¸ºä¸€ç»„æ ·æœ¬$ æ­¤æ—¶è¯¥å‚æ•°è¡¨ç¤ºåœ¨ä¸»è½´ä¸Šçš„åæ ‡$(\theta_1, \theta_2)$ï¼Œå¸¦å›ž$(*))$å³å¯è§£å¾—$\hat{y}$ \hat{y} = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*} é€šä¿—ç†è§£ï¼Œä¸€æŽŒæŠŠ$y$æ‹å¹³åœ¨äº†å¹³é¢$\Pi$ä¸Šï¼Œå˜æˆäº†$\hat{y}$ï¼Œä½†æ˜¯å“ªæœ‰è¿™ä¹ˆå¥½æ‹ã€‚ã€‚ã€‚è¿™ä¸ªæ—¶å€™åˆºåœ¨æŽŒå¿ƒé‡Œä¸€å®šæœ‰ä¸€ä¸ªåž‚ç›´çš„å‘é‡åˆ†é‡ï¼Œå³ä¸ºè¯¥å¹³é¢çš„æ³•å‘é‡ \vec{n} = y - \hat{y} ä¹Ÿå¯ä½¿ç”¨ç²—æš´ä¸€ç‚¹çš„æ–¹æ³•ï¼Œç›´æŽ¥å°†ç¬¬ä¸‰ä¸»æˆåˆ†ä½œä¸ºæ³•å‘é‡ã€‚ æˆ–è€…ç›´æŽ¥ä¸ŠæŠ•å½±å…¬å¼ï¼š \hat{y} = Py P = X (X^TX+\lambda I)^{-1} X^T ![projection](/PCA/projection.jpg) &gt; æ€»ä½“çš„è¿ç®—æµç¨‹å¦‚ä¸‹ &gt; - åˆ©ç”¨æ‰€æœ‰æ ·æœ¬ç‚¹(è¿‘ä¼¼å¹³é¢)è®¡ç®—ä¸»æˆåˆ†ï¼Œç¬¬ä¸€ã€äºŒä¸»æˆåˆ†å¼ æˆå¹³é¢$\Pi$ï¼› &gt; - é€‰å‡ºå…¶ä¸­ä¸€ä¸ªæ ·æœ¬ç‚¹ï¼Œå°†å¹³è¡ŒäºŽå¹³é¢$\Pi$çš„æˆåˆ†æŠ•å°„åˆ°$\Pi$ä¸Šï¼› &gt; - è¯¥æ ·æœ¬ç‚¹å‰©ä½™åˆ†é‡å³æ³•å‘é‡ï¼› &gt; - ä¸€èˆ¬æ¥è¯´ï¼Œå–æ‰€æœ‰ç‚¹æ³•å‘é‡çš„å‡å€¼ã€‚ ç¨‹åº@Github: PCA 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class PrincipalComponentAnalysis(): def __init__(self, n_component=-1): self.n_component = n_component self.meanVal = None self.axis = None def fit(self, X, prop=0.99): &apos;&apos;&apos; the parameter &apos;prop&apos; is only for &apos;n_component = -1&apos; &apos;&apos;&apos; # ç¬¬ä¸€æ­¥: å½’ä¸€åŒ– self.meanVal = np.mean(X, axis=0) # è®­ç»ƒæ ·æœ¬æ¯ä¸ªç‰¹å¾ä¸Šçš„çš„å‡å€¼ X_normalized = (X - self.meanVal) # å½’ä¸€åŒ–è®­ç»ƒæ ·æœ¬ # ç¬¬äºŒæ­¥ï¼šè®¡ç®—åæ–¹å·®çŸ©é˜µ # cov = X_normalized.T.dot(X_normalized) cov = np.cov(X_normalized.T) # åæ–¹å·®çŸ©é˜µ eigVal, eigVec = np.linalg.eig(cov) # EVD order = np.argsort(eigVal)[::-1] # ä»Žå¤§åˆ°å°æŽ’åº eigVal = eigVal[order] eigVec = eigVec.T[order].T # é€‰æ‹©ä¸»æˆåˆ†çš„æ•°é‡ if self.n_component == -1: sumOfEigVal = np.sum(eigVal) sum_tmp = 0 for k in range(eigVal.shape[0]): sum_tmp += eigVal[k] if sum_tmp &gt; prop * sumOfEigVal: # å¹³å‡å‡æ–¹è¯¯å·®ä¸Žè®­ç»ƒé›†æ–¹å·®çš„æ¯”ä¾‹å°½å¯èƒ½å°çš„æƒ…å†µä¸‹é€‰æ‹©å°½å¯èƒ½å°çš„ K å€¼ self.n_component = k + 1 break # é€‰æ‹©æŠ•å½±åæ ‡è½´ self.axis = eigVec[:, :self.n_component] # é€‰æ‹©å‰n_componentä¸ªç‰¹å¾å‘é‡ä½œä¸ºæŠ•å½±åæ ‡è½´ def transform(self, X): # ç¬¬ä¸€æ­¥ï¼šå½’ä¸€åŒ– X_normalized = (X - self.meanVal) # å½’ä¸€åŒ–æµ‹è¯•æ ·æœ¬ # ç¬¬äºŒæ­¥ï¼šæŠ•å½± X_nxk Â· V_kxk&apos; = X&apos;_nxk&apos; X_transformed = X_normalized.dot(self.axis) return X_transformed def fit_transform(self, X, prop=0.99): self.fit(X, prop=prop) return self.transform(X) def transform_inv(self, X_transformed): # è§†æŠ•å½±å‘é‡é•¿åº¦ä¸ºä¸€ä¸ªå•ä½é•¿åº¦ï¼ŒæŠ•å½±ç»“æžœä¸ºæŠ•å½±å‘é‡ä¸Šçš„åæ ‡ # X&apos;_nxk&apos; Â· V_kxk&apos;.T = X&apos;&apos;_nxk X_restructed = X_transformed.dot(self.axis.T) # è¿˜åŽŸæ•°æ® X_restructed = X_restructed + self.meanVal return X_restructed å®žéªŒç»“æžœ Demo1: PCA applied on 2-d datasets Demo2: PCA applied on wild face origin reduced restructured]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Softmax Regression]]></title>
    <url>%2F2018%2F10%2F18%2FSoftmax-Regression%2F</url>
    <content type="text"><![CDATA[Unsupervised Feature Learning and Deep Learning Tutorial å¼•è¨€Logistic Regressionä¸­é‡‡ç”¨çš„éžçº¿æ€§å‡½æ•°ä¸ºSigmoidï¼Œå°†è¾“å‡ºå€¼æ˜ å°„åˆ°$(0, 1)$ä¹‹é—´ä½œä¸ºæ¦‚çŽ‡è¾“å‡ºï¼Œå¤„ç†çš„æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œé‚£ä¹ˆå¯¹äºŽå¤šåˆ†ç±»çš„é—®é¢˜æ€Žä¹ˆå¤„ç†å‘¢ï¼Ÿ æ¨¡åž‹ ç”±Logisticå›žå½’æŽ¨å¹¿è€Œæ¥ SoftmaxSoftmaxåœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­æœ‰ç€éžå¸¸å¹¿æ³›çš„åº”ç”¨ã€‚å°¤å…¶åœ¨å¤„ç†å¤šåˆ†ç±»$(K&gt;2)$é—®é¢˜ï¼Œåˆ†ç±»å™¨æœ€åŽçš„è¾“å‡ºå•å…ƒéœ€è¦Softmaxå‡½æ•°è¿›è¡Œæ•°å€¼å¤„ç†ã€‚ S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]å…¶ä¸­$x$ä¸ºçŸ©é˜µå½¢å¼çš„å‘é‡ï¼Œå…¶ç»´åº¦ä¸º$(KÃ—1)$ï¼Œ$K$ä¸ºç±»åˆ«æ•°ç›®ã€‚Softmaxçš„è¾“å‡ºå‘é‡ç»´åº¦ä¸Ž$x$ç›¸åŒï¼Œå„å…ƒç´ $x_i$åŠ å’Œä¸º$1$ï¼Œå¯ç”¨äºŽè¡¨ç¤ºå–å„ä¸ªç±»åˆ«çš„æ¦‚çŽ‡ã€‚ æ³¨æ„åˆ°ï¼Œå¯¹äºŽå‡½æ•°$e^x$ \lim_{x \rightarrow - \infty} e^x = 0\lim_{x \rightarrow + \infty} e^x = +\infty å‡è®¾æ‰€æœ‰çš„$x_i$ç­‰äºŽæŸå¸¸æ•°$c$ï¼Œç†è®ºä¸Šå¯¹æ‰€æœ‰$x_i$ä¸Šå¼ç»“æžœä¸º$\frac{1}{n}$ è‹¥$c$ä¸ºå¾ˆå°çš„è´Ÿæ•°ï¼Œ$e^c$ä¸‹æº¢ï¼Œç»“æžœä¸º$NaN$ï¼› è‹¥$c$é‡çº§å¾ˆå¤§ï¼Œ$e^c$ä¸Šæº¢ï¼Œç»“æžœä¸º$NaN$ã€‚ åœ¨æ•°å€¼è®¡ç®—æ—¶å¹¶ä¸ç¨³å®šï¼Œä½†æ˜¯Softmaxæ‰€æœ‰è¾“å…¥å¢žåŠ åŒä¸€å¸¸æ•°æ—¶ï¼Œè¾“å‡ºä¸å˜ï¼Œå¾—ç¨³å®šç‰ˆæœ¬ï¼š S(x) := S(x - max(x_i)) e^{x_{max} - max(x_i)} = 1 å‡åŽ»æœ€å¤§å€¼å¯¼è‡´$e^x$æœ€å¤§ä¸º$1$ï¼ŒæŽ’é™¤ä¸Šæº¢ï¼› åˆ†æ¯ä¸­è‡³å°‘æœ‰ä¸€é¡¹ä¸º$1$ï¼ŒæŽ’é™¤åˆ†æ¯ä¸‹æº¢å¯¼è‡´å¤„ä»¥$0$çš„æƒ…å†µã€‚ å…¶å¯¹æ•° log S(x)_i = x_i - log ({\sum_{k=1}^K exp(x_k)}) æ³¨æ„åˆ°ï¼Œç¬¬ä¸€é¡¹è¡¨ç¤ºè¾“å…¥$x_i$æ€»æ˜¯å¯¹ä»£ä»·å‡½æ•°æœ‰ç›´æŽ¥çš„è´¡çŒ®ã€‚è¿™ä¸€é¡¹ä¸ä¼šé¥±å’Œï¼Œæ‰€ä»¥å³ä½¿$x_i$å¯¹ä¸Šå¼çš„ç¬¬äºŒé¡¹çš„è´¡çŒ®å¾ˆå°ï¼Œå­¦ä¹ ä¾ç„¶å¯ä»¥è¿›è¡Œï¼› å½“æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶æ—¶ï¼Œç¬¬ä¸€é¡¹é¼“åŠ±$x_i$è¢«æŽ¨é«˜ï¼Œè€Œç¬¬äºŒé¡¹åˆ™é¼“åŠ±æ‰€æœ‰çš„$x$è¢«åŽ‹ä½Žï¼› ç¬¬äºŒé¡¹$log ({\sum_{k=1}^K exp(x_k)})$å¯ä»¥å¤§è‡´è¿‘ä¼¼ä¸º$max(x_k)$ï¼Œè¿™ç§è¿‘ä¼¼æ˜¯åŸºäºŽå¯¹ä»»ä½•æ˜Žæ˜¾å°äºŽ$max(x_k)$çš„$x_k$éƒ½æ˜¯ä¸é‡è¦çš„ï¼Œè´Ÿå¯¹æ•°ä¼¼ç„¶ä»£ä»·å‡½æ•°æ€»æ˜¯å¼ºçƒˆåœ°æƒ©ç½šæœ€æ´»è·ƒçš„ä¸æ­£ç¡®é¢„æµ‹ é™¤äº†å¯¹æ•°ä¼¼ç„¶ä¹‹å¤–çš„è®¸å¤šç›®æ ‡å‡½æ•°å¯¹ softmax å‡½æ•°ä¸èµ·ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œé‚£äº›ä¸ä½¿ç”¨å¯¹æ•°æ¥æŠµæ¶ˆ softmax ä¸­çš„æŒ‡æ•°çš„ç›®æ ‡å‡½æ•°ï¼Œå½“æŒ‡æ•°å‡½æ•°çš„å˜é‡å–éžå¸¸å°çš„è´Ÿå€¼æ—¶ä¼šé€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œä»Žè€Œæ— æ³•å­¦ä¹  ä½œè€…ï¼šNirHeavenXæ¥æºï¼šCSDNåŽŸæ–‡ï¼šhttps://blog.csdn.net/qsczse943062710/article/details/61912464ç‰ˆæƒå£°æ˜Žï¼šæœ¬æ–‡ä¸ºåšä¸»åŽŸåˆ›æ–‡ç« ï¼Œè½¬è½½è¯·é™„ä¸Šåšæ–‡é“¾æŽ¥ï¼ Softmaxè§£å†³å¤šåˆ†ç±»é—®é¢˜å¯¹äºŽå…·æœ‰$K$ä¸ªåˆ†ç±»çš„é—®é¢˜ï¼Œæ¯ä¸ªç±»åˆ«è®­ç»ƒä¸€ç»„å‚æ•°$ w_k $ z_k^{(i)} = w_k^Tx^{(i)}æˆ–å†™ä½œçŸ©é˜µå½¢å¼ z^{(i)} = W^Tx^{(i)}å…¶ä¸­ x^{(i)} = \left[ \begin{matrix} x_0^{(i)}\\ x_1^{(i)}\\ ...\\ x_n^{(i)} \end{matrix} \right]_{nÃ—1}, x_0^{(i)}=1 W = [w_1, w_2, ..., w_K]_{(n+1)Ã—K} w_i = \left[ \begin{matrix} w_{i0}\\ w_{i1}\\ ...\\ w_{in} \end{matrix} \right]_{nÃ—1}æœ€ç»ˆå„ç±»åˆ«è¾“å‡ºæ¦‚çŽ‡ä¸º \hat{y}^{(i)} = Softmax(z^{(i)}) äº§ç”Ÿäº†ä¸€ä¸ªå¥‡æ€ªçš„è„‘æ´žã€‚ã€‚ã€‚äºŒåˆ†ç±»é—®é¢˜ p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }å®šä¹‰äºŒåˆ†ç±»çº¿æ€§å•å…ƒè¾“å‡ºçš„å·®å€¼ä¸º z = x_1 - x_2å¾—åˆ° p(x_1) = \frac{1}{1 + e^{-z}}ä»¥$x_1 = [x_{11}, x_{12}]^T$ä¸ºä¾‹(äºŒç»´ç‰¹å¾)ï¼Œå–$w_1=1, w_2=2, b=3$ p(x_1) = \frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}} è€Œå¤šåˆ†ç±»é—®é¢˜ï¼Œä»¥$3$åˆ†ç±»ä¸ºä¾‹ p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }å®šä¹‰çº¿æ€§å•å…ƒè¾“å‡ºçš„å·®å€¼ä¸º z_{12} = x_1 - x_2 z_{13} = x_1 - x_3 p(x_1) = \frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }åšå‡ºå›¾åƒä¸º æŸå¤±å‡½æ•°ç”±äº¤å‰ç†µç†è§£CrossEnt = \sum_j p_j log \frac{1}{q_j}è€Œå¯¹äºŽæ ·æœ¬$ (X^{(i)}, y^{(i)}) $ï¼Œä¸ºç¡®å®šäº‹ä»¶ï¼Œæ•…æ ‡ç­¾æ¦‚çŽ‡å„å…ƒç´ çš„å–å€¼$p_j$ä¸º$ y^{(i)}_j âˆˆ \{0,1\}$ï¼Œ$ q_jå³é¢„æµ‹è¾“å‡ºçš„æ¦‚çŽ‡å€¼\hat{y}^{(i)}_j$ ä¸€èˆ¬å–å„ä¸ªæ ·æœ¬æŸå¤±çš„å‡å€¼$(\frac{1}{N})$ L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N 1\{y^{(i)}_j=k\} log (\hat{y}^{(i)}_j) 1\{y^{(i)}_j=k\} = \begin{cases} 1 & y^{(i)}_j = k \\ 0 & y^{(i)}_j \neq k \end{cases}å¯å¯¹å®žé™…æ ‡ç­¾$y^{(i)}$é‡‡å–One-Hotç¼–ç ï¼Œä¾¿äºŽè®¡ç®— y^{(i)} = \left[ \begin{matrix} 0, ..., 1_{y^{(i)}}, ..., 0 \end{matrix} \right]^Tåˆ™ L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N y^{(i)T} log (\hat{y}^{(i)})ç”±å†³ç­–å¹³é¢ç†è§£ä»Žè´å¶æ–¯å†³ç­–å’Œåˆ†ç±»é—®é¢˜çš„å†³ç­–å¹³é¢å¯çŸ¥ï¼Œå¯¹äºŽç±»åˆ«$c_i$ï¼Œæœ‰ P(c_i|x) = \frac{P(x|c_i)}{\sum_{j=0}^KP(x|c_j)} å‡è®¾æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æœä»Žæ­£æ€åˆ†å¸ƒï¼Œå…ˆéªŒæ¦‚çŽ‡ç›¸ç­‰ï¼Œå„ç±»åˆ«æ ·æœ¬ç‰¹å¾é—´åæ–¹å·®ç›¸ç­‰ã€‚è¯æ˜Žç•¥. æ¢¯åº¦æŽ¨å¯¼Softmaxå‡½æ•°çš„å¯¼æ•°å¯¹äºŽ S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]ä¸€èˆ¬è¾“å‡ºä½œä¸ºæ¦‚çŽ‡å€¼ï¼Œè®° P = S(x)p_i = S(x)_iå¯¹å‘é‡$x$ä¸­æŸå…ƒç´ æ±‚å¯¼ \frac{âˆ‚S(x)}{âˆ‚x_i} = \frac{âˆ‚}{âˆ‚x_i} \left[ \begin{matrix} ...\\ \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}\\ ...\\ \end{matrix} \right] $(1)$ $i=k$$\frac{âˆ‚}{âˆ‚x_i} \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{expâ€™(x_i)Â·\sum_{j=1}^K exp(x_j) - exp(x_i)Â·(\sum_{j=1}^K exp(x_j))â€™}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)Â·\sum_{j=1}^K exp(x_j) - exp^2(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)} -(\frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)})^2$$ = p_i (1 - p_i)$ $(2)$ $i\neq k$$\frac{âˆ‚}{âˆ‚x_i} \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{expâ€™(x_k)Â·\sum_{j=1}^K exp(x_j) - exp(x_k)Â·(\sum_{j=1}^K exp(x_j))â€™}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{- exp(x_k)exp(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$= - p_i p_k$ ç»¼ä¸Š \frac{âˆ‚S(x)}{âˆ‚x_i}_{KÃ—1} = \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] - \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right] = \left( \left[ \begin{matrix} 0\\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i æŸå¤±å‡½æ•°æ¢¯åº¦åœ¨OneHotç¼–ç ä¸‹ï¼ŒæŸå¤±å‡½æ•°å½¢å¼ä¸º L(\hat{y},y) = \frac{1}{N} \sum_{i=1}^N L (y^{(i)}, \hat{y}^{(i)}) L (y^{(i)}, \hat{y}^{(i)}) = - y^{(i)T} log \hat{y}^{(i)} \hat{y}^{(i)} = S(z^{(i)}) z^{(i)} = W^T x^{(i)}å³åªè€ƒè™‘å®žé™…åˆ†ç±»å¯¹åº”çš„æ¦‚çŽ‡å€¼ L (y^{(i)}, \hat{y}^{(i)}) = - log \hat{y}^{(i)}_{y^{(i)}} ç”±äºŽ $S(z^{(i)})_{t^{(i)}}$ä¸Ž$z^{(i)}$å‘é‡å„ä¸ªå…ƒç´ éƒ½æœ‰å…³ï¼Œç”±é“¾å¼æ±‚å¯¼æ³•åˆ™ \frac{âˆ‚ L^{(i)} }{âˆ‚w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } ( \sum_{k=1}^K \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_k} \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} )$1.$ è€ƒå¯Ÿ $\frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_k}$ \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_k} = â€‹ \begin{cases} â€‹ \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_k) & k=y^{(i)} \\ â€‹ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_k & k \neq y^{(i)} â€‹ \end{cases}$2.$ è€ƒå¯Ÿ $\frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}}$ \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} = \begin{cases} \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} = x^{(i)}_p & k=q\\ \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} = 0 & k \neq q \end{cases} ç»¼ä¸Šæ‰€è¿° \frac{âˆ‚ L^{(i)} }{âˆ‚w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_q} \frac{âˆ‚z^{(i)}_q}{âˆ‚w_{pq}}å…¶ä¸­ \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_q} = \begin{cases} \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_q) & q = y^{(i)}\\ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_q & q \neq y^{(i)} \end{cases} \frac{âˆ‚z^{(i)}_q}{âˆ‚w_{pq}} = x^{(i)}_pæ•…å¯¹äºŽå•ä¸ªæ ·æœ¬$(X^{(i)}, y^{(i)})$ï¼Œå½“æ ·æœ¬æ ‡ç­¾é‡‡ç”¨$OneHot$ç¼–ç æ—¶ \frac{âˆ‚L^{(i)}}{âˆ‚w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_q} x^{(i)}_p = \begin{cases} (\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\ \hat{y}^{(i)}_qx^{(i)}_p & q \neq y^{(i)} \end{cases} æ³¨ï¼š è¿™é‡Œå¯ä»¥çº¦åˆ†åŽ»æŽ‰$\hat{y}^{(i)}_{y^{(i)}}$ \frac{âˆ‚L^{(i)}}{âˆ‚w_{pq}} = ( \hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_pæ›´ä¸€èˆ¬çš„ï¼Œå†™æˆçŸ©é˜µå½¢å¼ï¼Œè®°$X = [x_1, x_2, â€¦, x_m]^T$ï¼Œ$x_i$ä¸ºæ ·æœ¬ç‰¹å¾(åˆ—å‘é‡) âˆ‡_W L = X^T(\hat{Y} - Y) ç”¨çº¿æ€§æ¨¡åž‹è§£å†³åˆ†ç±»å’Œå›žå½’é—®é¢˜æ—¶ï¼Œå½¢å¼ç«Ÿå¦‚æ­¤ç»Ÿä¸€! è‡³æ­¤ä¸ºæ­¢ï¼Œæ¢¯åº¦æŽ¨å¯¼ç»“æŸï¼Œåˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿­ä»£æ±‚è§£å‚æ•°çŸ©é˜µ$W$å³å¯ã€‚ W := W - \alpha âˆ‡_W Lä»£ç @GitHub: Code of Softmax Regression Softmax12345678def softmax(X): &quot;&quot;&quot; æ•°å€¼è®¡ç®—ç¨³å®šç‰ˆæœ¬çš„softmaxå‡½æ•° @param &#123;ndarray&#125; X: shape(batch_size, n_labels) &quot;&quot;&quot; X_max = np.max(X, axis=1).reshape((-1, 1)) # æ¯è¡Œçš„æœ€å¤§å€¼ X = X - X_max # æ¯è¡Œå‡åŽ»æœ€å¤§å€¼ X = np.exp(X) return X / np.sum(X, axis=1).reshape((-1, 1)) cost function1234567891011def crossEnt(self, y_label_true, y_prob_pred): &quot;&quot;&quot; è®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•° @param &#123;ndarray&#125; y_label_true: çœŸå®žæ ‡ç­¾ shape(batch_size,) @param &#123;ndarray&#125; y_prob_pred: é¢„æµ‹è¾“å‡º shape(batch_size, n_labels) &quot;&quot;&quot; mask = self.encoder.transform(y_label_true.reshape(-1, 1)).toarray() # shape(batch_size, n_labels) y_prob_masked = np.sum(mask * y_prob_pred, axis=1) # æ¯è¡ŒçœŸå®žæ ‡ç­¾å¯¹åº”çš„é¢„æµ‹è¾“å‡ºå€¼ y_prob_masked[y_prob_masked==0.] = 1. y_loss = np.log(y_prob_masked) loss = - np.mean(y_loss) # æ±‚å„æ ·æœ¬æŸå¤±çš„å‡å€¼ return loss gradient12345678910def grad(self, X_train, y_train, y_prob_pred): &quot;&quot;&quot; è®¡ç®—æ¢¯åº¦ \frac &#123;âˆ‚L&#125; &#123;âˆ‚W_&#123;pq&#125;&#125; @param X_train: è®­ç»ƒé›†ç‰¹å¾ @param y_train: è®­ç»ƒé›†æ ‡ç­¾ @param y_prob_pred: è®­ç»ƒé›†é¢„æµ‹æ¦‚çŽ‡è¾“å‡º @param y_label_pred: è®­ç»ƒé›†é¢„æµ‹æ ‡ç­¾è¾“å‡º &quot;&quot;&quot; y_train = self.encoder.transform(y_train) dW = X_train.T.dot(y_prob_pred - y_train) return dW training stepçœç•¥å¯è§†åŒ–å’ŒéªŒè¯éƒ¨åˆ†çš„ä»£ç 123456789101112131415161718192021222324252627282930313233343536def fit(self, X_train, X_valid, y_train, y_valid, min_acc=0.95, max_epoch=20, batch_size=20): &quot;&quot;&quot; è®­ç»ƒ &quot;&quot;&quot; # æ·»åŠ é¦–1åˆ—ï¼Œè¾“å…¥åˆ°åç½®w0 X_train = np.c_[np.ones(shape=(X_train.shape[0],)), X_train] X_valid = np.c_[np.ones(shape=(X_valid.shape[0],)), X_valid] X_train = self.scaler.fit_transform(X_train) # å°ºåº¦å½’ä¸€åŒ– X_valid = self.scaler.transform(X_valid) # å°ºåº¦å½’ä¸€åŒ– self.encoder.fit(y_train.reshape(-1, 1)) self.n_features = X_train.shape[1] self.n_labels = self.encoder.transform(y_train).shape[1] # åˆå§‹åŒ–å‚æ•° self.W = np.random.normal(loc=0, scale=1.0, size=(self.n_features, self.n_labels)) n_batch = X_train.shape[0] // batch_size # å¯è§†åŒ–ç›¸å…³ plt.ion() plt.figure(&apos;loss&apos;); plt.figure(&apos;accuracy&apos;) loss_train_epoch = []; loss_valid_epoch = [] acc_train_epoch = []; acc_valid_epoch = [] for i_epoch in range(max_epoch): for i_batch in range(n_batch): # æ‰¹å¤„ç†æ¢¯åº¦ä¸‹é™ n1, n2 = i_batch * batch_size, (i_batch + 1) * batch_size X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2] # é¢„æµ‹ y_prob_train = self.predict(X_train_batch, preprocessed=True) # è®¡ç®—æŸå¤± loss_train_batch = self.crossEnt(y_train_batch, y_prob_train) # è®¡ç®—å‡†ç¡®çŽ‡ y_label_train = np.argmax(y_prob_train, axis=1) a = y_train_batch.reshape((-1,)) acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((-1,))).astype(&apos;float&apos;)) # è®¡ç®—æ¢¯åº¦ dW dW = self.grad(X_train_batch, y_train_batch, y_prob_train) # æ›´æ–°å‚æ•° self.W -= self.lr * dW predict step123456789101112def predict(self, X, preprocessed=False): &quot;&quot;&quot; å¯¹è¾“å…¥çš„æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å‡ºæ ‡ç­¾ @param &#123;ndarray&#125; X: shape(batch_size, n_features) @return &#123;ndarray&#125; y_prob: probability, shape(batch_size, n_labels) &#123;ndarray&#125; y_label: labels, shape(batch_size,) &quot;&quot;&quot; if not preprocessed: # è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒç”¨æ­¤å‡½æ•°æ—¶ï¼Œä¸ç”¨åŠ é¦–1åˆ— X = np.c_[np.ones(shape=(X.shape[0],)), X] # æ·»åŠ é¦–1é¡¹ï¼Œè¾“å…¥åˆ°åç½®w0 X = self.scaler.transform(X) y_prob = softmax(X.dot(self.W)) # é¢„æµ‹æ¦‚çŽ‡å€¼ shape(batch_size, n_labels) return y_prob å®žéªŒç»“æžœä»¥ä¸‹è“çº¿ä¸ºè®­ç»ƒé›†å‚æ•°ï¼Œçº¢çº¿ä¸ºéªŒè¯é›†å‚æ•°ï¼Œè‹¥ç¨³å®šè®­ç»ƒ(å¦‚batch_size = 20çš„ç»“æžœ)ï¼Œæœ€ç»ˆå‡†ç¡®çŽ‡åœ¨$80\%$å·¦å³ã€‚ ç”±äºŽéšæœºæ¢¯åº¦ä¸‹é™(SGD)éåŽ†æ¬¡æ•°å¤ªå¤šï¼Œè¿è¡Œè¾ƒæ…¢ï¼Œæ²¡æœ‰ç”¨SGDæ–¹æ³•è®­ç»ƒï¼Œå°±å‰å‡ ä¸ªepochæ¥çœ‹ï¼Œæ•ˆæžœæ²¡æœ‰batch_size = 20çš„å¥½ï¼› æ·»åŠ éšå«å±‚å½¢æˆä¸‰å±‚ç»“æž„çš„å‰é¦ˆç¥žç»ç½‘ç»œï¼Œå¯æé«˜å‡†ç¡®çŽ‡ï¼› è¿˜æœ‰ä¸€ç‚¹ï¼Œä½¿ç”¨æ‰¹å¤„ç†æ¢¯åº¦ä¸‹é™(n_batch = 1)è®­ç»ƒæ—¶ï¼Œå¯ä»¥çœ‹åˆ°æŸå¤±å€¼å·²ç»è¶‹äºŽ$0$ï¼Œä½†å‡†ç¡®çŽ‡å´å¾ˆä½Žï¼Œè¯´æ˜Žå·²ç»é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚ batch size = 20 æŸå¤± å‡†ç¡®çŽ‡ batch_size = 200 æŸå¤± å‡†ç¡®çŽ‡ n_batch = 1 æŸå¤± å‡†ç¡®çŽ‡ æ„Ÿæ‚ŸæŽ¨å…¬å¼è¦æˆ‘è€å‘½ã€‚ã€‚ã€‚ã€‚ Softmaxå›žå½’å¯ä»¥è§†ä½œä¸å«éšå«å±‚çš„å‰é¦ˆç¥žç»ç½‘ç»œã€‚]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[å¼•è¨€é€»è¾‘å›žå½’ï¼ˆLogistic Regressionï¼‰æ˜¯ç”¨äºŽå¤„ç†å› å˜é‡ä¸ºåˆ†ç±»å˜é‡çš„å›žå½’é—®é¢˜ï¼Œå¸¸è§çš„æ˜¯äºŒåˆ†ç±»æˆ–äºŒé¡¹åˆ†å¸ƒé—®é¢˜ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šåˆ†ç±»é—®é¢˜ï¼Œå®ƒå®žé™…ä¸Šæ˜¯å±žäºŽä¸€ç§åˆ†ç±»æ–¹æ³•ã€‚ æ¨¡åž‹å…ˆç»™å‡ºæ¨¡åž‹ï¼ŒæŽ¨å¯¼è¿‡ç¨‹ç¨åŽç»™å‡ºï¼Œé€»è¾‘å›žå½’åŒ…å«Sigmoidå‡½æ•° f(z) = \frac{1}{1+e^{-z}}å…¶å›¾åƒå¦‚ä¸‹ å®šä¹‰ z = w^Txå…¶ä¸­$x=[x_0, x_1, â€¦, x_n]^T, x_0=1$ h_w(x) = g(z) = \frac{1}{1+e^{-z}}æŸå¤±å‡½æ•°ç”±æœ€å¤§ä¼¼ç„¶ä¼°è®¡æŽ¨å¯¼å¯¹äºŽäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œå…¶å–å€¼ä½œä¸ºéšæœºå˜é‡ï¼Œæœä»ŽäºŒé¡¹åˆ†å¸ƒ $B(1, p)$ï¼Œå…¶ä¸­$p$å³ä¸ºé¢„æµ‹è¾“å‡ºæ¦‚çŽ‡$\hat{y}$ P(y_i^{(i)}) = (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}ç”±æžå¤§ä¼¼ç„¶ä¼°è®¡ L = \prod_{i=0}^N P(y_i^{(i)}) = \prod_{i=0}^N (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}å–å¯¹æ•°ä¼¼ç„¶å‡½æ•° logL = \sum_{i=0}^N [y_i^{(i)} log \hat{y}_i^{(i)} + (1-y_i^{(i)}) log (1-\hat{y}_i^{(i)})]ä¼˜åŒ–ç›®æ ‡æ˜¯ w = argmax_w logLä¼˜åŒ–é—®é¢˜ä¸€èˆ¬è¡¨è¿°æˆminimizeé—®é¢˜ï¼Œæ·»åŠ è´Ÿå·ï¼Œæž„æˆNeg Log LikelihoodæŸå¤± w = argmin_w (-logL)ä¸€èˆ¬å–å‡å€¼ L(\hat{y}, y)=- \frac{1}{N} \sum_i [y_i^{(i)} log(\hat{y}_i^{(i)})+(1 - y_i^{(i)})log(1-\hat{y}_i^{(i)})]å…¶ä¸­$y_i$è¡¨ç¤ºçœŸå®žå€¼ï¼Œ$\hat{y}_i$è¡¨ç¤ºé¢„æµ‹å€¼ ä»Žäº¤å‰ç†µç†è§£å·²çŸ¥äº¤å‰ç†µcross entropyå®šä¹‰å¦‚ä¸‹ CrossEnt = \sum_i p_i log \frac{1}{q_i}è€Œå¯¹äºŽæ ·æœ¬$ (X_i, y_i) $ï¼Œä¸ºç¡®å®šäº‹ä»¶ï¼Œæ•…æ ‡ç­¾æ¦‚çŽ‡çš„å–å€¼ä¸º$ p_i = y_i âˆˆ \{0,1\}$ï¼Œ$ q_iå³é¢„æµ‹è¾“å‡ºçš„æ¦‚çŽ‡å€¼\hat{y}_i $ï¼Œå¯å¾—åˆ°ä¸Žä¸Šé¢ç›¸åŒçš„æŽ¨å¯¼ç»“è®º ä»Žå†³ç­–å¹³é¢å’Œè´å¶æ–¯å†³ç­–ç†è§£ç›¸å…³å†…å®¹æŸ¥çœ‹åˆ†ç±»é—®é¢˜çš„å†³ç­–å¹³é¢å’Œè´å¶æ–¯å†³ç­–ï¼Œé€»è¾‘å›žå½’è€ƒè™‘çš„ä¸€èˆ¬æ˜¯ç­‰å…ˆéªŒæ¦‚çŽ‡é—®é¢˜ï¼Œæ•…å†³ç­–å‡½æ•°å®šä¹‰ä¸º $if$ $P(c_i|x)&gt;P(c_j|x)$ $then$ $ x \in c_i $, $ i, j = 1, 2 $ ä»Žè´å¶æ–¯å†³ç­–å¯çŸ¥ï¼Œå¯¹äºŽç±»åˆ«$c_1$ï¼Œæœ‰ P(c_1|x) = \frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}è®¾åœ¨å„ä¸ªç±»åˆ«ä¸‹ï¼Œç‰¹å¾$x$æœä»Žæ­£æ€åˆ†å¸ƒ P(x|c_i) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i))åˆ™ P(c_1|x) = \frac {1} { 1 + exp(-z) } P(c_2|x) = 1 - P(c_1|x) = \frac{exp(-z)}{1+exp(-z)} $P(c_1|x) = \frac{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}$ $P(c_1|x) = \frac{1}{1 + \frac{exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}}$ å‡å®šå„åˆ†ç±»çš„æ ·æœ¬æ–¹å·®ç›¸ç­‰ï¼Œ$ \Sigma_1 = \Sigma_2 = \sigma^2 I $ $ P(c_1|x) = \frac {1}{1 + exp(- [ \frac{1}{\sigma^2} (\mu_1-\mu_2)^T x - \frac{1}{2 \sigma^2} (\mu_1^T\mu_1 - \mu_2^T\mu_2) ])}$ ä»¤ w = \frac{1}{\sigma^2} (\mu_1 -\mu_2)b = - \frac{1}{2\sigma^2}(\mu_1^T \mu_1 - \mu_2^T \mu_2)å³å¯å¾—åˆ° P(c_1|x) = \frac {1} { 1 + exp(-z) }å…¶ä¸­ z = w^T x + b æ¢¯åº¦æŽ¨å¯¼å…ˆæŽ¨å¯¼Sigmoidå‡½æ•°çš„å¯¼æ•° f'(z) = (1 - f(z))f(z)å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»Ž$fâ€™(z)$çš„å›¾åƒå¯ä»¥çœ‹åˆ°ï¼Œåœ¨$ x=0 $å¤„$fâ€™(z)$å–æžå¤§å€¼ï¼Œä¸” f'(z)_{max} = f'(z)|_{z=0} = 0.25 \lim_{z \rightarrow \infty} f'(z) = 0åœ¨å¤šå±‚ç¥žç»ç½‘ç»œåå‘ä¼ æ’­æ›´æ–°å‚æ•°æ—¶ï¼Œç”±äºŽæ¢¯åº¦å¤šæ¬¡ç´¯ä¹˜ï¼ŒSigmoidä½œä¸ºæ¿€æ´»å‡½æ•°ä¼šå­˜åœ¨â€œæ¢¯åº¦æ¶ˆå¤±â€çš„é—®é¢˜ï¼Œä½¿å¾—å‚æ•°æ›´æ–°éžå¸¸ç¼“æ…¢ã€‚ $ fâ€™(z) $$ = (\frac{1}{1+e^{-z}})â€™ $$ = \fracâ€‹ {-(1+e^{-z})â€™}â€‹ {(1+e^{-z})^2} $$ = \fracâ€‹ {e^{-z}}â€‹ {(1+e^{-z})^2} $$ = \fracâ€‹ {e^{-z}}â€‹ {1+e^{-z}}â€‹ \fracâ€‹ {1}â€‹ {1+e^{-z}}$$ = (1 - f(z))f(z)$ åˆ©ç”¨é“¾å¼æ±‚å¯¼æ³•åˆ™å¯å¾— $\frac{âˆ‚L}{âˆ‚w_j}$$= -\frac{âˆ‚}{âˆ‚w_j} \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})]$$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\frac{âˆ‚}{âˆ‚w_j}\hat{y}^{(i)}-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\frac{âˆ‚}{âˆ‚w_j}\hat{y}^{(i)}]$$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j]$$= - \frac{1}{N} \sum_i [y^{(i)} (1-\hat{y}^{(i)})w_j-(1-y^{(i)}) y^{(i)} w_j]$$= \frac{1}{N} \sum_i (\hat{y}^{(i)} - y^{(i)})w_j $ å†™ä½œçŸ©é˜µå½¢å¼ï¼Œè®°$X = [x_1, x_2, â€¦, x_m]^T$ï¼Œ$x_i$ä¸ºæ ·æœ¬ç‰¹å¾(åˆ—å‘é‡) âˆ‡_w L = X^T (\hat{Y} - Y)è®­ç»ƒå’Œçº¿æ€§å›žå½’ä¸€æ ·ï¼Œé‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£ w := w - \alpha âˆ‡_w Lå¤„ç†å¤šåˆ†ç±»é—®é¢˜å‡è®¾æœ‰$K$ä¸ªç±»åˆ«ï¼Œåˆ™ä¾æ¬¡ä»¥ç±»åˆ«$c_i$ä¸ºæ­£æ ·æœ¬è®­ç»ƒæ¨¡åž‹ï¼Œä¸€å…±è®­ç»ƒ$K$ä¸ªã€‚æµ‹è¯•æ ·æœ¬åœ¨æ¯ä¸ªæ¨¡åž‹ä¸Šè®¡ç®—ï¼Œæœ€ç»ˆå°†æ¦‚çŽ‡æœ€å¤§çš„ä½œä¸ºåˆ†ç±»ç»“æžœã€‚ è¿™æ ·åˆ’åˆ†æ•°æ®é›†ï¼Œä¼šä½¿è®­ç»ƒé›†æ­£è´Ÿæ ·æœ¬æ•°ç›®ä¸¥é‡ä¸å¯¹ç§°ï¼Œç‰¹åˆ«æ˜¯ç±»åˆ«å¾ˆå¤šçš„æƒ…å†µï¼Œå¯¹ç»“æžœä¼šäº§ç”Ÿå½±å“ã€‚å¯æŽ¨å¹¿è‡³softmaxå›žå½’è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ ç¨‹åºä»£ç @Github: Code for Logistic Regression cost function123456789101112131415def lossFunctionDerivative(self, X, theta, y_true): &apos;&apos;&apos; è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°thetaçš„æ¢¯åº¦ å¯¹theta[j]çš„æ¢¯åº¦ä¸ºï¼š(y_pred - y_true)*x[j] &apos;&apos;&apos; err = self.predict_prob(X, theta) - y_true return X.T.dot(err)/y_true.shape[0]def lossFunction(self, y_pred_prob, y_true): &apos;&apos;&apos; æœªä½¿ç”¨ è®¡ç®—æŸå¤±å€¼: Cross-Entropy y_pred_prob, y_true: NumPy array, shape=(n,) &apos;&apos;&apos; tmp = y_true*np.log(y_pred_prob) + (1 - y_true)*np.log(1 - y_pred_prob) return np.mean(-tmp) training step123456789101112131415161718192021def gradDescent(self, min_acc, learning_rate=0.01, max_iter=10000): acc = 0; n_iter = 0 for n_iter in range(max_iter): for n in range(self.n_batch): X_batch = self.X[n*self.batch_size:(n+1)*self.batch_size] t_batch = self.t[n*self.batch_size:(n+1)*self.batch_size] grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch) self.theta -= learning_rate * grad # æ¢¯åº¦ä¸‹é™ acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t) if acc &gt; min_acc: print(&apos;ç¬¬%dæ¬¡è¿­ä»£, ç¬¬%dæ‰¹æ•°æ®&apos; % (n_iter, n)) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬å‡†ç¡®çŽ‡ä¸º: &quot;, acc) print(&quot;å½“å‰å‚æ•°å€¼ä¸º: &quot;, self.theta) return self.theta if n_iter%100 == 0: print(&apos;ç¬¬%dæ¬¡è¿­ä»£&apos; % n_iter) print(&apos;å‡†ç¡®çŽ‡ï¼š &apos;, acc) print(&quot;è¶…è¿‡è¿­ä»£æ¬¡æ•°&quot;) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬å‡†ç¡®çŽ‡ä¸º: &quot;, acc) print(&quot;å½“å‰å‚æ•°å€¼ä¸º: &quot;, self.theta) return self.theta å®žéªŒç»“æžœ]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLinear-Regression%2F</url>
    <content type="text"><![CDATA[å¼•è¨€çº¿æ€§å›žå½’å¯ä»¥è¯´æ˜¯æœºå™¨å­¦ä¹ æœ€åŸºç¡€çš„ç®—æ³• æ¨¡åž‹\hat{y}^{(i)} = w^Tx^{(i)}å…¶ä¸­ x^{(i)}=[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T, x_0^{(i)}=1è¿™é‡Œ$x_0^{(i)}=1$è¡¨ç¤ºåç½®$b$ï¼Œå³$b=w_0$ \hat{y}^{(i)} = w^Tx^{(i)} + b æ³¨ï¼šå¯¹äºŽéžçº¿æ€§çš„æ•°æ®ï¼Œå¯æž„é€ é«˜æ¬¡ç‰¹å¾ã€‚ æŸå¤±å‡½æ•°å®šä¹‰è¯¯å·®e^{(i)} = \hat{y}^{(i)} - y^{(i)}å…¶ä¸­$y^{(i)}$è¡¨ç¤ºçœŸå®žå€¼ å®šä¹‰æŸå¤±å‡½æ•°å•ä¸ªæ ·æœ¬çš„è¯¯å·®å®šä¹‰ä¸º L_{single}(\hat{y}^{(i)}, y^{(i)})=\frac{1}{2}||e^{(i)}||_2^2=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2æ‰€æœ‰æ ·æœ¬çš„è¯¯å·®å®šä¹‰ä¸º L(y, t)=\frac{1}{2N}\sum_i (\hat{y}^{(i)}-y^{(i)})^2ä¹Ÿå¯ä»¥å®šä¹‰ä¸ºè¯¯å·®çš„å’Œè€Œä¸æ˜¯å‡å€¼ï¼Œå¯¹ç»“æžœæ— å½±å“ï¼Œå¯è§†ä½œå­¦ä¹ çŽ‡$Î±$é™¤åŽ»ä¸€ä¸ªå¸¸æ•° æ¢¯åº¦æŽ¨å¯¼ $\frac{âˆ‚L}{âˆ‚w_j}$$= \frac{âˆ‚}{âˆ‚w_j}\frac{1}{2N}\sum_i(\hat{y}^{(i)}-y^{(i)})^2$$= \frac{1}{2N} \sum_i \frac{âˆ‚}{âˆ‚w_j} (\hat{y}^{(i)}-y^{(i)})^2$$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) \frac{âˆ‚t^{(i)}}{âˆ‚w_j}$$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)}$ æˆ–è€…ä½¿ç”¨çŸ©é˜µæŽ¨å¯¼ï¼Œè®°$X = [x_1, x_2, â€¦, x_m]^T$ï¼Œ$x_i$ä¸ºæ ·æœ¬ç‰¹å¾(åˆ—å‘é‡) L = \frac{1}{2}(Xw-Y)^T(Xw-Y) âˆ‡_w L = X^T(\hat{Y}-Y) $âˆ‡_w L$$= \frac{1}{2} âˆ‡_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY)$$= \frac{1}{2} (2X^TXw - X^TY - X^TY)$$= X^T(Xw-Y) $ åœ¨æ¢¯åº¦ä¸º$\vec{0}$çš„ç‚¹ï¼Œå³$âˆ‡_w L = \vec{0}$æ—¶å¯¹åº”æœ€ä¼˜è§£ X^T(Xw-Y) = 0 ä»¤X^T(Xw-Y) = 0 æœ‰X^TXw = X^TY w^*=(X^TX+\lambda I)^{-1}X^TY å…¶ä¸­$X^+=(X^TX+\lambda I)^{-1}X^T$ï¼Œè¡¨ç¤ºçŸ©é˜µ$X_{mÃ—n}$çš„ä¼ªé€† è®­ç»ƒé‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£ w := w - \alpha âˆ‡_w Lå…¶ä¸­$w$è¡¨ç¤ºå‚æ•°å‘é‡ è¿›ä¸€æ­¥æ€è€ƒï¼šä¸ºä»€ä¹ˆä½¿ç”¨æ¢¯åº¦ä¸‹é™å¯ä»¥æ±‚å–æœ€ä¼˜è§£å‘¢ï¼Ÿ âˆ‡_w^2 L = âˆ‡_w X^T(Xw-Y) = X^TXè€Œå¯¹äºŽçŸ©é˜µ $ X^TX $ u^T(X^TX)u = (Xu)^T(Xu) \geq 0å³æŸå¤±å‡½æ•°çš„HessiançŸ©é˜µ$âˆ‡_w^2 L$ä¸ºæ­£å®šçŸ©é˜µï¼Œ$L$ä¸ºå‡¸å‡½æ•°ï¼Œå­˜åœ¨å…¨å±€æœ€ä¼˜è§£ ä»ŽæŠ•å½±çš„è§’åº¦ç†è§£çº¿æ€§å›žå½’ çº¿æ€§å›žå½’çš„æ­£åˆ™åŒ–ä¸ºå…‹æœè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¯åŠ å…¥æ­£åˆ™åŒ–é¡¹$||w||_2^2$ï¼Œæ­¤æ—¶æŸå¤±å‡½æ•°å®šä¹‰ä¸º L(\hat{y}, y)=\frac{1}{2N} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_2^2æˆ–è€… L(\hat{y}, y)=\frac{1}{2N} \sum_i (\hat{y}^{(i)}-y^{(i)})^2 + \frac{\lambda}{2N}\sum_j w_j^2å…¶ä¸­$i = 1, â€¦, N_{sample}; j = 1, â€¦, N_{feature},j&gt;0 $ æ­¤æ—¶æ¢¯åº¦ä¸º \frac{âˆ‚L}{âˆ‚w_j} = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \frac{\lambda}{N}w_jå…¶ä¸­$j = 1, â€¦, N_{feature},j&gt;0 $ å±€éƒ¨åŠ æƒçº¿æ€§å›žå½’ç›®æ ‡å‡½æ•°å®šä¹‰ä¸º L(y, t)=\frac{1}{2N}\sum_i w^{(i)} (\hat{y}^{(i)}-y^{(i)})^2å…¶ä¸­ w^{(i)} = e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}$x$è¡¨ç¤ºè¾“å…¥çš„é¢„æµ‹æ ·æœ¬ï¼Œ$x^{(i)}$è¡¨ç¤ºè®­ç»ƒæ ·æœ¬ ç¦»å¾ˆè¿‘çš„æ ·æœ¬ï¼Œæƒå€¼æŽ¥è¿‘äºŽ1ï¼Œè€Œå¯¹äºŽç¦»å¾ˆè¿œçš„æ ·æœ¬ï¼Œæ­¤æ—¶æƒå€¼æŽ¥è¿‘äºŽ0ï¼Œè¿™æ ·å°±æ˜¯åœ¨å±€éƒ¨æž„æˆçº¿æ€§å›žå½’ï¼Œå®ƒä¾èµ–çš„ä¹Ÿåªæ˜¯å‘¨è¾¹çš„ç‚¹ã€‚ å¯¹äºŽçº¿æ€§å›žå½’ç®—æ³•ï¼Œä¸€æ—¦æ‹Ÿåˆå‡ºé€‚åˆè®­ç»ƒæ•°æ®çš„å‚æ•°$w$ï¼Œä¿å­˜è¿™äº›å‚æ•°$w$ï¼Œå¯¹äºŽä¹‹åŽçš„é¢„æµ‹ï¼Œä¸éœ€è¦å†ä½¿ç”¨åŽŸå§‹è®­ç»ƒæ•°æ®é›†ï¼Œæ‰€ä»¥æ˜¯å‚æ•°å­¦ä¹ ç®—æ³•ã€‚è€Œå¯¹äºŽå±€éƒ¨åŠ æƒçº¿æ€§å›žå½’ç®—æ³•ï¼Œæ¯æ¬¡è¿›è¡Œé¢„æµ‹éƒ½éœ€è¦å…¨éƒ¨çš„è®­ç»ƒæ•°æ®ï¼ˆæ¯æ¬¡è¿›è¡Œçš„é¢„æµ‹å¾—åˆ°ä¸åŒçš„å‚æ•°$w$ï¼‰ï¼Œæ²¡æœ‰å›ºå®šçš„å‚æ•°$w$ï¼Œæ‰€ä»¥æ˜¯éžå‚æ•°ç®—æ³•ã€‚ ä»£ç @Github: Code for Linear Regression training step12345678910111213141516171819202122232425262728293031323334353637383940414243def fit(self, X, y, learning_rate=0.01, max_iter=5000, min_loss=10): # --------------- æ•°æ®é¢„å¤„ç†éƒ¨åˆ† --------------- # åŠ å…¥å…¨1åˆ— X = np.c_[np.ones(shape=(X.shape[0])), X] # æž„é€ é«˜æ¬¡ç‰¹å¾ if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] # ---------------- å‚æ•°è¿­ä»£éƒ¨åˆ† ---------------- # åˆå§‹åŒ–å‚æ•° self.theta = np.random.uniform(-1, 1, size=(X.shape[1],)) # æ•°æ®æ‰¹æ¬¡ n_batch = X.shape[0] if self.n_batch==-1 else self.n_batch batch_size = X.shape[0] // n_batch # åœæ­¢æ¡ä»¶ n_iter = 0; loss = float(&apos;inf&apos;) # å¼€å§‹è¿­ä»£ for n_iter in range(max_iter): for n in range(n_batch): n1, n2 = n*batch_size, (n+1)*batch_size X_batch = X[n1: n2]; y_batch = y[n1: n2] grad = self.lossFunctionDerivative(X_batch, y_batch) self.theta -= learning_rate * grad loss = self.score(y_batch, self.predict(X_batch)) if loss &lt; min_loss: print(&apos;ç¬¬%dæ¬¡è¿­ä»£, ç¬¬%dæ‰¹æ•°æ®&apos; % (n_iter, n)) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬æŸå¤±ä¸º: &quot;, loss) return self.theta if n_iter%100 == 0: print(&apos;ç¬¬%dæ¬¡è¿­ä»£&apos; % n_iter) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬æŸå¤±ä¸º: &quot;, loss) print(&quot;è¶…è¿‡è¿­ä»£æ¬¡æ•°&quot;) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬æŸå¤±ä¸º: &quot;, loss) return self.thetadef lossFunctionDerivative(self, X, y): y_pred = self.predict(X) # theta = self.theta; # ï¼æ³¨æ„ï¼štheta = self.theta ä¸ä»…ä»…æ˜¯èµ‹å€¼ï¼Œç±»ä¼¼å¼•ç”¨ï¼Œä¿®æ”¹thetaä¼šå½±å“self.theta theta = self.theta.copy() theta[0] = 0 # Î¸0ä¸éœ€è¦æ­£åˆ™åŒ– return (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[0] predict step123456789def predict(self, X, preprocessed=False): if preprocessed: # åŠ å…¥å…¨1åˆ— X = np.c_[np.ones(shape=(X.shape[0])), X] # æž„é€ é«˜æ¬¡ç‰¹å¾ if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] return X.dot(self.theta) è¿è¡Œç»“æžœ æ— æ­£åˆ™åŒ– æ­£åˆ™åŒ–]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
</search>
