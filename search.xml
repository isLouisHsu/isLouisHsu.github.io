<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ubuntuç¼–è¯‘å®‰è£…Tensorflow]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow%2F</url>
    <content type="text"><![CDATA[éå¸¸é‡è¦å¦‚æœä¸­é€”å‡ºç°é”™è¯¯ï¼Œxxxxæ–‡ä»¶æ‰¾ä¸åˆ°ï¼Œä¸è¦æ€€ç–‘ï¼å°±æ˜¯å¤§å¤©æœçš„ç½‘ç»œé—®é¢˜ï¼æ¨èç§‘å­¦ä¸Šç½‘ï¼ å®‰è£…CUDAä¸CUDNNé¦–å…ˆæŸ¥çœ‹æ˜¾å¡æ˜¯å¦æ”¯æŒCUDAåŠ é€Ÿï¼Œè¾“å…¥1$ nvidia-smi åœ¨Ubuntu16.04 LTSä¸‹ï¼Œæ¨èå®‰è£…CUDA9.0å’ŒCUDNN 7ã€‚ CUDA CUDA Toolkit 9.0 Downloads | NVIDIA Developer https://developer.nvidia.com/cuda-90-download-archive ä¸‹è½½.runç‰ˆæœ¬ï¼Œå®‰è£…æ–¹æ³•å¦‚ä¸‹ 12$ sudo chmod +x cuda_9.0.176_384.81_linux.run $ sudo sh ./cuda_9.0.176_384.81_linux.run æœåŠ¡æ¡æ¬¾å¾ˆé•¿ã€‚ã€‚ã€‚ã€‚ CUDNN NVIDIA cuDNN | NVIDIA Developer https://developer.nvidia.com/cudnn 1234$ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* å®‰è£…åè¿›è¡ŒéªŒè¯ 1234$ cp -r /usr/src/cudnn_samples_v7/ $HOME$ cd $HOME/cudnn_samples_v7/mnistCUDNN$ make clean &amp;&amp; make$ ./mnistCUDNN ç¼–è¯‘Tensorflow(CPU version)ç”±äºè®­ç»ƒä»£ç ä½¿ç”¨Pythonå®ç°ï¼Œæ•…C++ç‰ˆæœ¬çš„Tensorflowä¸ä½¿ç”¨GPUï¼Œä»…å®ç°é¢„æµ‹ä»£ç å³å¯ã€‚ bazel Installing Bazel on Ubuntu - Bazel https://docs.bazel.build/versions/master/install-ubuntu.htmlä¸€å®šè¦ç”¨æºç å®‰è£…ï¼ï¼ï¼ download the Bazel binary installer named bazel-&lt;version&gt;-installer-linux-x86_64.sh from the Bazel releases page on GitHub. 123456$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python$ chmod +x bazel-&lt;version&gt;-installer-linux-x86_64.sh$ ./bazel-&lt;version&gt;-installer-linux-x86_64.sh --user$ sudo nano ~/.bashrc # export PATH=&quot;$PATH:$HOME/bin&quot;$ source ~/.bashrc $ bazel version ç¼–è¯‘CPUç‰ˆæœ¬çš„CPUæŸ¥çœ‹javaç‰ˆæœ¬1234$ java -versionopenjdk version &quot;1.8.0_191&quot;OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) å®‰è£…ä¾èµ–è½¯ä»¶åŒ…ç¯å¢ƒ1234$ sudo apt install python3-dev$ pip3 install six$ pip3 install numpy$ pip3 instal wheel ä¸‹è½½Tensorflowæºç 1$ git clone https://github.com/tensorflow/tensorflow ç¼–è¯‘ä¸å®‰è£…12$ cd tensorflow$ ./configure é…ç½®é€‰é¡¹å¦‚ä¸‹1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command &quot;bazel shutdown&quot;.INFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5You have bazel 0.20.0 installed.Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3Found possible Python library paths: /usr/local/lib/python3.5/dist-packages /usr/lib/python3/dist-packagesPlease input the desired Python library path to use. Default is [/usr/local/lib/python3.5/dist-packages]Do you wish to build TensorFlow with XLA JIT support? [Y/n]: nNo XLA JIT support will be enabled for TensorFlow.Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: nNo OpenCL SYCL support will be enabled for TensorFlow.Do you wish to build TensorFlow with ROCm support? [y/N]: nNo ROCm support will be enabled for TensorFlow.Do you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Do you wish to download a fresh release of clang? (Experimental) [y/N]: nClang will not be downloaded.Do you wish to build TensorFlow with MPI support? [y/N]: nNo MPI support will be enabled for TensorFlow.Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is -march=native -Wno-sign-compare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Preconfigured Bazel build configs. You can use any of the below by adding &quot;--config=&lt;&gt;&quot; to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=monolithic # Config for mostly static monolithic build. --config=gdr # Build with GDR support. --config=verbs # Build with libverbs support. --config=ngraph # Build with Intel nGraph support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.Preconfigured Bazel build configs to DISABLE default on features: --config=noaws # Disable AWS S3 filesystem support. --config=nogcp # Disable GCP support. --config=nohdfs # Disable HDFS support. --config=noignite # Disable Apacha Ignite support. --config=nokafka # Disable Apache Kafka support. --config=nonccl # Disable NVIDIA NCCL support.Configuration finished ä½¿ç”¨bazelç¼–è¯‘1$ bazel build --config=opt //tensorflow:libtensorflow_cc.so å‡ºç°é”™è¯¯ TF failing to build on Bazel CI Â· Issue #19464 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/19464Failure to build TF 1.12 from source - multiple definitions in grpc Â· Issue #23402 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197Explicitly import tools/bazel.rc by meteorcloudy Â· Pull Request #23583 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583Explicitly import tools/bazel.rc by meteorcloudy Â· Pull Request #23583 Â· tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5 è§£å†³æ–¹æ³• æ–¹æ³•1 æ–¹æ³•2 å°†tools/bazel.rcä¸­å†…å®¹ç²˜åˆ°.tf_configure.bazelrcä¸­ï¼Œæ¯æ¬¡é‡æ–°é…ç½®åéœ€è¦é‡æ–°ç²˜è´´ä¸€æ¬¡ã€‚ æºç å®‰è£…protobuf3.6.0 https://github.com/protocolbuffers/protobuf 1234./autogen.sh./configuremakemake install ä¸‹è½½å…¶ä»–æ–‡ä»¶ 12$ ./tensorflow/contrib/makefile/download_dependencies.shmkdir /tmp/eigen å€¼å¾—æ³¨æ„ï¼Œdownload_dependencies.shä¸­ä¸‹è½½ä¾èµ–åŒ…æ—¶ï¼Œéœ€è¦ç”¨åˆ°curlï¼Œä½†æ˜¯é»˜è®¤æ–¹å¼å®‰è£… 1$ sudo apt install curl &gt; ç°åœ¨æ˜¯2018/12/19/02:48ï¼Œè¢«è¿™ä¸ªé—®é¢˜æŠ˜è…¾äº†3ä¸ªå°æ—¶ã€‚ æ—¶ä¸æ”¯æŒ`https`åè®®ï¼Œæ•…éœ€è¦å®‰è£…`OpenSSL`ï¼Œå¹¶æºç å®‰è£…ï¼Œè¯¦ç»†èµ„æ–™è§[curlæç¤ºä¸æ”¯æŒhttpsåè®®è§£å†³æ–¹æ³• - æ ‡é…çš„å°å· - åšå®¢å›­](https://www.cnblogs.com/biaopei/p/8669810.html) - æ‰§è¡Œ`./autogen.sh`æ—¶ï¼Œå‘ç”Ÿé”™è¯¯`autoreconf: not found`ï¼Œåˆ™å®‰è£… 12$ sudo apt install autoconf aotomake libtool$ sudo apt install libffi-dev æºç å®‰è£…Eigen 12345cd tensorflow/contrib/makefile/Downloads/eigenmkdir buildcd buildcmakemake install è°ƒç”¨C++ç‰ˆæœ¬çš„Tensorflowåˆ›å»ºæ–‡ä»¶ç›®å½•å¦‚ä¸‹1234|-- tf_test |-- build |-- main.cpp |-- CMakeLists.txt main.cppæ–‡ä»¶å†…å®¹å¦‚ä¸‹1234567891011121314151617181920212223#include &quot;tensorflow/cc/client/client_session.h&quot;#include &quot;tensorflow/cc/ops/standard_ops.h&quot;#include &quot;tensorflow/core/framework/tensor.h&quot;int main() &#123; using namespace tensorflow; using namespace tensorflow::ops; Scope root = Scope::NewRootScope(); // Matrix A = [3 2; -1 0] auto A = Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125;&#125;); // Vector b = [3 5] auto b = Const(root, &#123; &#123;3.f, 5.f&#125;&#125;); // v = Ab^T auto v = MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true)); std::vector&lt;Tensor&gt; outputs; ClientSession session(root); // Run and fetch v TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs)); // Expect outputs[0] == [19; -3] LOG(INFO) &lt;&lt; outputs[0].matrix&lt;float&gt;(); return 0;&#125; CMakeLists.txtå†…å®¹å¦‚ä¸‹12345678910111213141516171819202122232425262728293031cmake_minimum_required (VERSION 2.8.8)project (tf_example)set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -g -std=c++11 -W&quot;)set(EIGEN_DIR /usr/local/include/eigen3)set(PROTOBUF_DIR /usr/local/include/google/protobuf)set(TENSORFLOW_DIR /home/louishsu/install/tensorflow-1.12.0)include_directories( $&#123;EIGEN_DIR&#125; $&#123;PROTOBUF_DIR&#125; $&#123;TENSORFLOW_DIR&#125; $&#123;TENSORFLOW_DIR&#125;/bazel-genfiles $&#123;TENSORFLOW_DIR&#125;/tensorflow/contrib/makefile/downloads/absl)link_directories( /usr/local/lib)add_executable( tf_test main.cpp)target_link_libraries( tf_test tensorflow_cc tensorflow_framework) 123$ mkdir build &amp;&amp; cd build$ cmake .. &amp;&amp; make$ ./tf_test install tensorflow-gpu for pythonå¯ä½¿ç”¨pipæŒ‡ä»¤å®‰è£…ï¼Œæ¨èä¸‹è½½å®‰è£…åŒ…ï¼Œ tensorflow Â· PyPI https://pypi.org/project/tensorflow/ 12$ cd ~/Downloads$ pip3 --default-timeout=1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user å®‰è£…åè¿›è¡ŒéªŒè¯123456789101112131415161718192021222324252627$ python3Python 3.5.2 (default, Nov 12 2018, 13:43:14) [GCC 5.4.0 20160609] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; sess = tf.Session()2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758pciBusID: 0000:04:00.0totalMemory: 983.44MiB freeMemory: 177.19MiB2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 02018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)&gt;&gt;&gt; a = tf.Variable([233])&gt;&gt;&gt; init = tf.initialize_all_variables()WARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.Instructions for updating:Use `tf.global_variables_initializer` instead.&gt;&gt;&gt; sess.run(init)&gt;&gt;&gt; sess.run(a)array([233], dtype=int32)&gt;&gt;&gt; sess.close() æ³¨æ„ï¼Œå¦‚æœå¼‚å¸¸ä¸­æ–­ç¨‹åºï¼Œæ˜¾å­˜ä¸ä¼šè¢«é‡Šæ”¾ï¼Œéœ€è¦è‡ªè¡Œkill1$ nvidia-smi è·å¾—PIDåºå·ï¼Œä½¿ç”¨æŒ‡ä»¤ç»“æŸè¿›ç¨‹1$ kill -9 pid Reference TensorFlow C++åŠ¨æ€åº“ç¼–è¯‘ - ç®€ä¹¦ https://www.jianshu.com/p/d46596558640Tensorflow C++ ä»è®­ç»ƒåˆ°éƒ¨ç½²(1)ï¼šç¯å¢ƒæ­å»º | æŠ€æœ¯åˆ˜ http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/]]></content>
      <categories>
        <category>Linux</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntuç¼–è¯‘å®‰è£…OpenCV]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV%2F</url>
    <content type="text"><![CDATA[ä¸‹è½½æºç  OpenCV library https://opencv.org/ ç¼–è¯‘å®‰è£…ä¾èµ–è½¯ä»¶åŒ…12$ sudo apt install cmake$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev ç¼–è¯‘12345$ unzip opencv-3.4.4.zip$ cd opencv-3.4.4$ mkdir build &amp;&amp; cd build$ cmake ..$ make -j4 å®‰è£…123$ sudo make install$ sudo nano /etc/ld.so.conf.d/opencv.conf # add `/usr/local/lib`$ sudo ldconfig éªŒè¯OpenCVè‡ªå¸¦éªŒè¯ç¨‹åºï¼Œåœ¨opencv-3.4.4/samples/cpp/example_cmakeä¸­å¯ä»¥æ‰¾åˆ° 1234$ cd opencv-3.4.4/samples/cpp/example_cmake$ cmake .$ make$ ./opencv_example å¦‚æœæ²¡é—®é¢˜ï¼Œå¯ä»¥çœ‹åˆ°ä½ çš„å¤§è„¸äº†~ Reference Ubuntu16.04å®‰è£…openCV3.4.4 - è¾£å±å°å¿ƒçš„å­¦ä¹ ç¬”è®° - CSDNåšå®¢ https://blog.csdn.net/weixin_39992397/article/details/84345197]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pythonè¯»å†™é…ç½®æ–‡ä»¶]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæœ‰è®¸å¤šè¿è¡Œå‚æ•°éœ€è¦æŒ‡å®šï¼Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥è§£å†³ å®šä¹‰.pyæ–‡ä»¶å­˜å‚¨å˜é‡ å®šä¹‰å‘½åå…ƒç»„collections.namedtuple() åˆ›å»º.configï¼Œ.iniç­‰é…ç½®æ–‡ä»¶ Python è¯»å–å†™å…¥é…ç½®æ–‡ä»¶å¾ˆæ–¹ä¾¿ï¼Œä½¿ç”¨å†…ç½®æ¨¡å—configparserå³å¯ è¯»å‡ºé¦–å…ˆåˆ›å»ºæ–‡ä»¶test.configæˆ–test.iniï¼Œå†™å…¥å¦‚ä¸‹å†…å®¹123456789[db]db_port = 3306db_user = rootdb_host = 127.0.0.1db_pass = test[concurrent]processor = 20thread = 10 è¯»å–æ“ä½œå¦‚ä¸‹1234567891011121314151617181920212223242526272829&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; configfile = &quot;./test.config&quot;&gt;&gt;&gt; inifile = &quot;./test.ini&quot;&gt;&gt;&gt; &gt;&gt;&gt; cf = configparser.ConfigParser()&gt;&gt;&gt; cf.read(configfile) # è¯»å–æ–‡ä»¶å†…å®¹&gt;&gt;&gt; &gt;&gt;&gt; sections = cf.sections() # æ‰€æœ‰çš„sectionï¼Œä»¥åˆ—è¡¨çš„å½¢å¼è¿”å›&gt;&gt;&gt; sections[&apos;db&apos;, &apos;concurrent&apos;]&gt;&gt;&gt; &gt;&gt;&gt; options = cf.options(&apos;db&apos;) # è¯¥sectionçš„æ‰€æœ‰option&gt;&gt;&gt; options[&apos;db_port&apos;, &apos;db_user&apos;, &apos;db_host&apos;, &apos;db_pass&apos;]&gt;&gt;&gt; &gt;&gt;&gt; items = cf.items(&apos;db&apos;) # è¯¥sectionçš„æ‰€æœ‰é”®å€¼å¯¹&gt;&gt;&gt; items[(&apos;db_port&apos;, &apos;3306&apos;), (&apos;db_user&apos;, &apos;root&apos;), (&apos;db_host&apos;, &apos;127.0.0.1&apos;), (&apos;db_pass&apos;, &apos;test&apos;)]&gt;&gt;&gt; &gt;&gt;&gt; db_user = cf.get(&apos;db&apos;, &apos;db_user&apos;) # sectionä¸­optionçš„å€¼ï¼Œè¿”å›ä¸ºstringç±»å‹&gt;&gt;&gt; db_user&apos;root&apos;&gt;&gt;&gt; &gt;&gt;&gt; db_port = cf.getint(&apos;db&apos;, &apos;db_port&apos;) # å¾—åˆ°sectionä¸­optionçš„å€¼ï¼Œè¿”å›ä¸ºintç±»å‹&gt;&gt;&gt; # ç±»ä¼¼çš„è¿˜æœ‰getboolean()ä¸getfloat()&gt;&gt;&gt; db_port3306 å†™å…¥12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; cf = configparser.ConfigParser()&gt;&gt;&gt; cf.add_section(&apos;test1&apos;) # æ–°å¢section&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) # æ–°å¢optionï¼šé”™è¯¯ç¤ºèŒƒTraceback (most recent call last): File &quot;&lt;pyshell#7&gt;&quot;, line 1, in &lt;module&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1192, in set self._validate_value_types(option=option, value=value) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1177, in _validate_value_types raise TypeError(&quot;option values must be strings&quot;)TypeError: option values must be strings&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, &apos;1&apos;) # æ–°å¢option&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test1&quot;, &quot;opt1&quot;, &apos;ok&apos;) # æ–°å¢option&gt;&gt;&gt; cf.remove_option(&quot;test1&quot;, &quot;opt1&quot;) # åˆ é™¤optionTrue&gt;&gt;&gt; &gt;&gt;&gt; cf.add_section(&apos;test2&apos;) # æ–°å¢section&gt;&gt;&gt; cf.remove_section(&apos;test2&apos;) # åˆ é™¤sectionTrue&gt;&gt;&gt; &gt;&gt;&gt; with open(&quot;./test_wr.config&quot;, &apos;w+&apos;) as f: cf.write(f) # å†™å…¥æ–‡ä»¶test_wr.config &gt;&gt;&gt; ç°åœ¨ç›®å½•å·²åˆ›å»ºæ–‡ä»¶test_wr.configï¼Œæ‰“å¼€å¯ä»¥çœ‹åˆ°12[test1]count = 1]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pythonæ›´æ–°å®‰è£…çš„åŒ…]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%2F</url>
    <content type="text"><![CDATA[pipä¸æä¾›å‡çº§å…¨éƒ¨å·²å®‰è£…æ¨¡å—çš„æ–¹æ³•ï¼Œä»¥ä¸‹æŒ‡ä»¤å¯æŸ¥çœ‹æ›´æ–°ä¿¡æ¯1$ pip list --outdate å¾—åˆ°è¾“å‡ºä¿¡æ¯å¦‚ä¸‹123456789101112131415161718192021222324Package Version Latest Type----------------- --------- ---------- -----absl-py 0.3.0 0.6.1 sdistautopep8 1.3.5 1.4.2 sdistbleach 2.1.4 3.0.2 wheelcertifi 2018.8.24 2018.10.15 wheeldask 0.20.0 0.20.1 wheelgrpcio 1.14.1 1.16.0 wheelipykernel 5.0.0 5.1.0 wheelipython 7.0.1 7.1.1 wheeljedi 0.12.1 0.13.1 wheeljupyter-console 5.2.0 6.0.0 wheelMarkdown 2.6.11 3.0.1 wheelMarkupSafe 1.0 1.1.0 wheelmatplotlib 2.2.2 3.0.2 wheelmistune 0.8.3 0.8.4 wheelnumpy 1.14.5 1.15.4 wheelopencv-python 3.4.2.17 3.4.3.18 wheelPillow 5.2.0 5.3.0 wheelprometheus-client 0.3.1 0.4.2 sdistpyparsing 2.2.0 2.3.0 wheelpython-dateutil 2.7.3 2.7.5 wheelpytz 2018.5 2018.7 wheelurllib3 1.23 1.24.1 wheel ä»¥ä¸‹æä¾›ä¸€é”®å‡çº§çš„æ–¹æ³•ï¼Œå¯èƒ½æ¯”è¾ƒä¹…hhhh12345678from pip._internal.utils.misc import get_installed_distributionsfrom subprocess import call for dist in get_installed_distributions(): modulename = dist.project_name print(&apos;start processing module &apos; + modulename) call(&quot;pip install --upgrade &quot; + modulename, shell=True) print(&apos;module &apos; + modulename + &apos;done!&apos;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pythonè®°å½•æ—¥å¿—]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[å‰è¨€æ—¥å¿—å¯ä»¥ç”¨æ¥è®°å½•åº”ç”¨ç¨‹åºçš„çŠ¶æ€ã€é”™è¯¯å’Œä¿¡æ¯æ¶ˆæ¯ï¼Œä¹Ÿç»å¸¸ä½œä¸ºè°ƒè¯•ç¨‹åºçš„å·¥å…·ã€‚Pythonæä¾›äº†ä¸€ä¸ªæ ‡å‡†çš„æ—¥å¿—æ¥å£ï¼Œå°±æ˜¯loggingæ¨¡å—ã€‚æ—¥å¿—çº§åˆ«æœ‰DEBUGã€INFOã€WARNINGã€ERRORã€CRITICALäº”ç§ã€‚ logging â€” Logging facility for Python â€” Python 3.7.1 documentation ä½¿ç”¨æ–¹æ³•loggerå¯¹è±¡1234&gt;&gt;&gt; import logging&gt;&gt;&gt; logger = logging.getLogger(__name__)&gt;&gt;&gt; logger&lt;Logger __main__ (WARNING)&gt; æ—¥å¿—çº§åˆ«å¯è¾“å‡ºäº”ç§ä¸åŒçš„æ—¥å¿—çº§åˆ«ï¼Œåˆ†åˆ«ä¸ºæœ‰DEBUGã€INFOã€WARNINGã€ERRORã€CRITICAL12345678&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&gt;&gt;&gt; logger.info(&apos;test log&apos;)&gt;&gt;&gt; logger.warning(&apos;test log&apos;)test log&gt;&gt;&gt; logger.error(&apos;test log&apos;)test log&gt;&gt;&gt; logger.critical(&apos;test log&apos;)test log å¯ä»¥çœ‹åˆ°åªæœ‰WARNINGåŠä»¥ä¸Šçº§åˆ«æ—¥å¿—è¢«è¾“å‡ºï¼Œè¿™æ˜¯ç”±äºé»˜è®¤çš„æ—¥å¿—çº§åˆ«æ˜¯WARNING ï¼Œæ‰€ä»¥ä½äºæ­¤çº§åˆ«çš„æ—¥å¿—ä¸ä¼šè®°å½•ã€‚ åŸºç¡€é…ç½®1logging.basicConfig(**kwarg) **kwargä¸­éƒ¨åˆ†å‚æ•°å¦‚ä¸‹ format 12345678910%(levelname)ï¼šæ—¥å¿—çº§åˆ«çš„åå­—æ ¼å¼%(levelno)sï¼šæ—¥å¿—çº§åˆ«çš„æ•°å­—è¡¨ç¤º%(name)sï¼šæ—¥å¿—åå­—%(funcName)sï¼šå‡½æ•°åå­—%(asctime)ï¼šæ—¥å¿—æ—¶é—´ï¼Œå¯ä»¥ä½¿ç”¨datefmtå»å®šä¹‰æ—¶é—´æ ¼å¼ï¼Œå¦‚ä¸Šå›¾ã€‚%(pathname)ï¼šè„šæœ¬çš„ç»å¯¹è·¯å¾„%(filename)ï¼šè„šæœ¬çš„åå­—%(module)ï¼šæ¨¡å—çš„åå­—%(thread)ï¼šthread id%(threadName)ï¼šçº¿ç¨‹çš„åå­— datefmt 1&apos;%Y-%m-%d %H:%M:%S&apos; level é»˜è®¤ä¸ºERROR 12345logging.DEBUGlogging.INFOlogging.WARNINGlogging.ERRORlogging.CRITICAL ä¾‹å¦‚12345678910111213141516&gt;&gt;&gt; # æœªè¾“å‡ºdebug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&gt;&gt;&gt; &gt;&gt;&gt; # ä¿®æ”¹é…ç½®&gt;&gt;&gt; log_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;&gt;&gt;&gt; log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;&gt;&gt;&gt; log_level = logging.DEBUG&gt;&gt;&gt; logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level)&gt;&gt;&gt; &gt;&gt;&gt; # è¾“å‡ºdebug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&lt;pyshell#8&gt; [2018-11-13 11:59:52] [DEBUG] test log è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶ä¿å­˜ä»£ç ä¸ºæ–‡ä»¶log_test.py1234567891011121314151617181920import logginglog_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;log_level = logging.DEBUGlog_filename = &apos;./test.log&apos;log_filemode = &apos;a&apos; # ä¹Ÿå¯ä»¥ä¸º&apos;w&apos;, &apos;w+&apos;ç­‰logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level, filename=log_filename, filemode=log_filemode)logger = logging.getLogger(__name__)logger.debug(&apos;test log&apos;)logger.info(&apos;test log&apos;)logger.warning(&apos;test log&apos;)logger.error(&apos;test log&apos;)logger.critical(&apos;test log&apos;) è¿è¡Œå®Œæ¯•ï¼Œæ‰“å¼€log_test.logæ–‡ä»¶å¯ä»¥çœ‹åˆ°12345log_test.py [2018-11-13 12:11:04] [DEBUG] test loglog_test.py [2018-11-13 12:11:04] [INFO] test loglog_test.py [2018-11-13 12:11:04] [WARNING] test loglog_test.py [2018-11-13 12:11:04] [ERROR] test loglog_test.py [2018-11-13 12:11:04] [CRITICAL] test log]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Githubåšå®¢æ­å»º]]></title>
    <url>%2F2019%2F01%2F04%2FGithub-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[å‰è¨€é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œç°æœ‰çš„åšå®¢è¿˜æ˜¯ç°æœ‰çš„è¿™ç¯‡æ–‡ç« å‘¢ï¼Ÿ è½¯ä»¶å®‰è£…å®‰è£…node.js, git, hexo åšå®¢æ­å»ºåˆå§‹åŒ–æ¨èä½¿ç”¨gitå‘½ä»¤çª—å£ï¼Œæ‰§è¡Œå¦‚ä¸‹æŒ‡ä»¤12345678910111213141516171819202122232425262728293031$ mkdir Blog$ cd Blog$ hexo initINFO Cloning hexo-starter to ~\Desktop\BlogCloning into &apos;C:\Users\LouisHsu\Desktop\Blog&apos;...remote: Enumerating objects: 68, done.remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68Unpacking objects: 100% (68/68), done.Submodule &apos;themes/landscape&apos; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &apos;themes/landscape&apos;Cloning into &apos;C:/Users/LouisHsu/Desktop/Blog/themes/landscape&apos;...remote: Enumerating objects: 1, done.remote: Counting objects: 100% (1/1), done.remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866Receiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.Resolving deltas: 100% (459/459), done.Submodule path &apos;themes/landscape&apos;: checked out &apos;73a23c51f8487cfcd7c6deec96ccc7543960d350&apos;[32mINFO [39m Install dependenciesnpm WARN deprecated titlecase@1.1.2: no longer maintainednpm WARN deprecated postinstall-build@5.0.3: postinstall-build&apos;s behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.&gt; nunjucks@3.1.6 postinstall C:\Users\LouisHsu\Desktop\Blog\node_modules\nunjucks&gt; node postinstall-build.js srcnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)added 422 packages from 501 contributors and audited 4700 packages in 59.195sfound 0 vulnerabilitiesINFO Start blogging with Hexo! ç”Ÿæˆç›®å½•ç»“æ„å¦‚ä¸‹123456\-- scaffolds\-- source \-- _posts\-- themes|-- _config.yml|-- package.json ç»§ç»­123456$ npm installnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)audited 4700 packages in 5.99sfound 0 vulnerabilities ç°åœ¨è¯¥ç›®å½•æ‰§è¡ŒæŒ‡ä»¤ï¼Œå¼€å¯hexoæœåŠ¡å™¨123$ hexo sINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. ç”Ÿæˆç›®å½•å’Œæ ‡ç­¾1234$ hexo n page about$ hexo n page archives$ hexo n page categories$ hexo n page tags ä¿®æ”¹/source/tags/index.mdï¼Œå…¶ä»–åŒç†1234567891011121301| ---02| title: tags03| date: 2019-01-04 17:34:1504| ----&gt;01| ---02| title: tags03| date: 2019-01-04 17:34:1504| type: &quot;tags&quot;05| comments: false06| --- å…³è”Githubåœ¨Githubæ–°å»ºä¸€ä¸ªä»“åº“ï¼Œå‘½åä¸ºusername.github.ioï¼Œä¾‹å¦‚isLouisHsu.github.ioï¼Œæ–°å»ºæ—¶å‹¾é€‰Initialize this repository with a READMEï¼Œå› ä¸ºè¿™ä¸ªä»“åº“å¿…é¡»ä¸èƒ½ä¸ºç©ºã€‚ æ‰“å¼€åšå®¢ç›®å½•ä¸‹çš„_config.ymlé…ç½®æ–‡ä»¶ï¼Œå®šä½åˆ°æœ€åçš„deployé€‰é¡¹ï¼Œä¿®æ”¹å¦‚ä¸‹1234deploy: type: git repository: git@github.com:isLouisHsu/isLouisHsu.github.io.git branch: master å®‰è£…æ’ä»¶1$ npm install hexo-deployer-git --save ç°åœ¨å°±å¯ä»¥å°†è¯¥ç›®å½•å†…å®¹æ¨é€åˆ°Githubæ–°å»ºçš„ä»“åº“ä¸­äº†1$ hexo d ä½¿ç”¨ä¸ªäººåŸŸå åœ¨sourceç›®å½•ä¸‹æ–°å»ºæ–‡ä»¶CNAMEï¼Œè¾“å…¥è§£æåçš„ä¸ªäººåŸŸå åœ¨Githubä¸»é¡µä¿®æ”¹åŸŸå å¤‡ä»½åšå®¢ æ²¡ã€‚æ²¡ä»€ä¹ˆç”¨æˆ‘ã€‚æˆ‘ä¸å¤‡ä»½äº†å¯ä»¥æ–°å»ºä¸€ä¸ªä»“åº“ä¸“é—¨ä¿å­˜æ–‡ä»¶è¯•è¯• ç°åœ¨åšå®¢çš„æºæ–‡ä»¶ä»…ä¿å­˜åœ¨PCä¸Šï¼Œ æˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œå¤‡ä»½ï¼Œå¹¶å°†ä»“åº“ä½œä¸ºåšå®¢æ–‡ä»¶å¤¹ åœ¨ä»“åº“æ–°å»ºåˆ†æ”¯hexoï¼Œè®¾ç½®ä¸ºé»˜è®¤åˆ†æ”¯ å°†ä»“åº“å…‹éš†è‡³æœ¬åœ° 1$ git clone https://github.com/isLouisHsu/isLouisHsu.github.io.git å…‹éš†æ–‡ä»¶ å°†ä¹‹å‰çš„Hexoæ–‡ä»¶å¤¹ä¸­çš„ 123456scffolds/source/themes/.gitignore_config.ymlpackage.json å¤åˆ¶åˆ°å…‹éš†ä¸‹æ¥çš„ä»“åº“æ–‡ä»¶å¤¹isLouisHsu.github.io å®‰è£…åŒ… 123$ npm install$ npm install hexo --save$ npm install hexo-deployer-git --save å¤‡ä»½åšå®¢ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤ 123$ git add .$ git commit -m &quot;backup&quot;$ git push origin hexo éƒ¨ç½²åšå®¢æŒ‡ä»¤ 1$ hexo g -d å•é”®æäº¤ ç¼–å†™è„šæœ¬commit.batï¼ŒåŒå‡»å³å¯ 1234git add .git commit -m &apos;backup&apos;git push origin hexohexo g -d ä½¿ç”¨æ–¹æ³• ç›®å½•ç»“æ„ public ç”Ÿæˆçš„ç½‘ç«™æ–‡ä»¶ï¼Œå‘å¸ƒçš„ç«™ç‚¹æ–‡ä»¶ã€‚ source èµ„æºæ–‡ä»¶å¤¹ï¼Œç”¨äºå­˜æ”¾å†…å®¹ã€‚ tag æ ‡ç­¾æ–‡ä»¶å¤¹ã€‚ archive å½’æ¡£æ–‡ä»¶å¤¹ã€‚ categoryåˆ†ç±»æ–‡ä»¶å¤¹ã€‚ downloads/code include codeæ–‡ä»¶å¤¹ã€‚ :lang i18n_dir å›½é™…åŒ–æ–‡ä»¶å¤¹ã€‚ _config.yml é…ç½®æ–‡ä»¶ æŒ‡ä»¤ 123456789101112131415161718192021222324252627$ hexo helpUsage: hexo &lt;command&gt;Commands: clean Remove generated files and cache. config Get or set configurations. deploy Deploy your website. generate Generate static files. help Get help on a command. init Create a new Hexo folder. list List the information of the site migrate Migrate your site from other system to Hexo. new Create a new post. publish Moves a draft post from _drafts to _posts folder. render Render files with renderer plugins. server Start the server. version Display version information.Global Options: --config Specify config file instead of using _config.yml --cwd Specify the CWD --debug Display all verbose messages in the terminal --draft Display draft posts --safe Disable all plugins and scripts --silent Hide output on consoleFor more help, you can use &apos;hexo help [command]&apos; for the detailed information or you can check the docs: http://hexo.io/docs/ æ‹“å±•åŠŸèƒ½æ”¯æŒæ’å…¥å›¾ç‰‡1$ npm install hexo-asset-image --save ä¿®æ”¹æ–‡ä»¶_config.yml1post_asset_folder: true åœ¨æ‰§è¡Œ$ hexo n [layout] &lt;title&gt;æ—¶ä¼šç”ŸæˆåŒåæ–‡ä»¶å¤¹ï¼ŒæŠŠå›¾ç‰‡æ”¾åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹å†…ï¼Œåœ¨.mdæ–‡ä»¶ä¸­æ’å…¥å›¾ç‰‡1![image_name](/title/image_name.png) æœç´¢åŠŸèƒ½12$ npm install hexo-generator-searchdb --save$ npm install hexo-generator-search --save ç«™ç‚¹é…ç½®æ–‡ä»¶_config.ymlä¸­æ·»åŠ 12345search: path: search.xml field: post format: html limit: 10000 ä¿®æ”¹ä¸»é¢˜é…ç½®æ–‡ä»¶/themes/xxx/_config.yml12local_search: enable: true å¸¦è¿‡æ»¤åŠŸèƒ½çš„é¦–é¡µæ’ä»¶åœ¨é¦–é¡µåªæ˜¾ç¤ºæŒ‡å®šåˆ†ç±»ä¸‹é¢çš„æ–‡ç« åˆ—è¡¨ã€‚12$ npm install hexo-generator-index2 --save$ npm uninstall hexo-generator-index --save ä¿®æ”¹_config.yml1234567index_generator: per_page: 10 order_by: -date include: - category Web # åªåŒ…å«Webåˆ†ç±»ä¸‹çš„æ–‡ç«  exclude: - tag Hexo # ä¸åŒ…å«æ ‡ç­¾ä¸ºHexoçš„æ–‡ç«  æ•°å­¦å…¬å¼æ”¯æŒhexoé»˜è®¤çš„æ¸²æŸ“å¼•æ“æ˜¯markedï¼Œä½†æ˜¯markedä¸æ”¯æŒmathjaxã€‚kramedæ˜¯åœ¨markedçš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹ã€‚1234$ npm uninstall hexo-math --save # åœæ­¢ä½¿ç”¨ hexo-math$ npm install hexo-renderer-mathjax --save # å®‰è£…hexo-renderer-mathjaxåŒ…ï¼š$ npm uninstall hexo-renderer-marked --save # å¸è½½åŸæ¥çš„æ¸²æŸ“å¼•æ“$ npm install hexo-renderer-kramed --save # å®‰è£…æ–°çš„æ¸²æŸ“å¼•æ“ ä¿®æ”¹/node_modules/kramed/lib/rules/inline.js12345678911| escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,...20| em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,-&gt;11| escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,...20| em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, ä¿®æ”¹/node_modules/hexo-renderer-kramed/lib/renderer.js123456789101112131464| // Change inline math rule65| function formatText(text) &#123;66| // Fit kramed&apos;s rule: $$ + \1 + $$67| return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);68| &#125;-&gt;64| // Change inline math rule65| function formatText(text) &#123;66| // Fit kramed&apos;s rule: $$ + \1 + $$67| // return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);68| return text;69| &#125; åœ¨ä¸»é¢˜ä¸­å¼€å¯mathjaxå¼€å…³ï¼Œä¾‹å¦‚nextä¸»é¢˜ä¸­1234# MathJax Supportmathjax: enable: true per_page: true åœ¨æ–‡ç« ä¸­12345678---title: title.mddate: 2019-01-04 12:47:37categories:tags:mathjax: truetop:--- æµ‹è¯• A = \left[\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix}\right]Reference åŸºäºhexo+githubæ­å»ºä¸€ä¸ªç‹¬ç«‹åšå®¢ - ç‰§äº‘äº‘ - åšå®¢å›­ https://www.cnblogs.com/MuYunyun/p/5927491.htmlhexo+github pagesè½»æ¾æ­åšå®¢(1) | ex2tronâ€™s Blog http://ex2tron.wang/hexo-blog-with-github-pages-1/hexoä¸‹LaTeXæ— æ³•æ˜¾ç¤ºçš„è§£å†³æ–¹æ¡ˆ - crazy_scottçš„åšå®¢ - CSDNåšå®¢ https://blog.csdn.net/crazy_scott/article/details/79293576åœ¨Hexoä¸­æ¸²æŸ“MathJaxæ•°å­¦å…¬å¼ - ç®€ä¹¦ https://www.jianshu.com/p/7ab21c7f0674æ€ä¹ˆå»å¤‡ä»½ä½ çš„Hexoåšå®¢ - ç®€ä¹¦ https://www.jianshu.com/p/baab04284923Hexoä¸­æ·»åŠ æœ¬åœ°å›¾ç‰‡ - èœ•å˜C - åšå®¢å›­ https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referralhexo æœç´¢åŠŸèƒ½ - é˜¿ç”˜çš„åšå®¢ - CSDNåšå®¢ https://blog.csdn.net/ganzhilin520/article/details/79047983]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Metrics]]></title>
    <url>%2F2018%2F11%2F21%2FMetrics%2F</url>
    <content type="text"><![CDATA[å›å½’(regression)è¯„ä¼°æŒ‡æ ‡è§£é‡Šæ–¹å·®(Explained Variance) EV(\hat{y}, y) = 1 - \frac{Var(y-\hat{y})}{Var(y)}è§£é‡Šæ–¹å·®è¶Šæ¥è¿‘$1$è¡¨ç¤ºå›å½’æ•ˆæœè¶Šå¥½ã€‚ å¹³å‡ç»å¯¹è¯¯å·®(Mean Absolute Error - MAE) MAE(\hat{y}, y) = E(||\hat{y} - y||_1) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} |\hat{y}^{(i)} - y^{(i)}|$MAE$è¶Šå°è¡¨ç¤ºå›å½’æ•ˆæœè¶Šå¥½ã€‚ å¹³å‡å¹³æ–¹è¯¯å·®(Mean Squared Error - MSE)åœ¨çº¿æ€§å›å½’ä¸€èŠ‚ï¼Œä½¿ç”¨çš„æŸå¤±å‡½æ•°å³$MSE$ MSE(\hat{y}, y) = E(||\hat{y} - y||_2^2) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2å…¶ä¸­$y$ä¸$\hat{y}$å‡ä¸º$1$ç»´å‘é‡ï¼Œ$MSE$è¶Šå°è¡¨ç¤ºå›å½’æ•ˆæœè¶Šå¥½ã€‚ å…¶å«ä¹‰æ¯”è¾ƒç›´è§‚ï¼Œå³åå·®çš„å¹³æ–¹å’Œã€‚ä¹Ÿå¯ä»¥ä»æœ€å°åŒ–æ–¹å·®çš„è§’åº¦è§£é‡Šï¼Œå®šä¹‰è¯¯å·®å‘é‡ e = \hat{y} - yæˆ‘ä»¬å‡å®šå…¶æœŸæœ›ä¸º$0$ï¼Œå³ E(e) = 0 æˆ– \overline{e} = 0é‚£ä¹ˆè¯¯å·®çš„æ–¹å·®ä¸º Var(e) = E[(e - \overline{e})^T (e - \overline{e})] = E(||e||_2^2)ä¹Ÿå³$MSE$ã€‚ å‡æ–¹æ ¹è¯¯å·®(Root Mean Squared Error - RMSE) RMSE(\hat{y}, y) = \sqrt{\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2}å®è´¨ä¸$MSE$æ˜¯ä¸€æ ·çš„ã€‚åªä¸è¿‡ç”¨äºæ•°æ®æ›´å¥½çš„æè¿°ï¼Œä½¿è®¡ç®—å¾—æŸå¤±çš„å€¼è¾ƒå°ã€‚$RMSE$è¶Šå°è¡¨ç¤ºå›å½’æ•ˆæœè¶Šå¥½ã€‚ å‡æ–¹å¯¹æ•°è¯¯å·®(Mean Squard Logarithmic Error - MSLE) MSLE(\hat{y}, y) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} \left[\log (1+y^{(i)}) - \log (1+\hat{y}^{(i)})\right]^2é€šå¸¸ç”¨äºè¾“å‡ºæŒ‡æ•°å¢é•¿çš„æ¨¡å‹ï¼Œå¦‚ï¼Œäººå£ç»Ÿè®¡ï¼Œå•†å“çš„å¹³å‡é”€å”®é‡ï¼Œä»¥åŠä¸€æ®µæ—¶é—´å†…çš„å¹³å‡é”€å”®é‡ç­‰ã€‚æ³¨æ„ï¼Œç”±å¯¹æ•°æ€§è´¨ï¼Œè¿™ä¸€æŒ‡æ ‡å¯¹è¿‡å°çš„é¢„æµ‹çš„æƒ©ç½šå¤§äºé¢„æµ‹è¿‡å¤§çš„é¢„æµ‹çš„æƒ©ç½šã€‚ ä¸­å€¼ç»å¯¹è¯¯å·®(Median Absolute Error - MedAE) MedAE(\hat{y}, y) = median(|y - \hat{y}|)Rå†³å®šç³»æ•°(R2)åˆç§°æ‹Ÿåˆä¼˜åº¦ï¼Œæä¾›äº†ä¸€ä¸ªè¡¡é‡æœªæ¥æ ·æœ¬æœ‰å¤šå¥½çš„é¢„æµ‹æ¨¡å‹ã€‚æœ€ä½³å¯èƒ½çš„åˆ†æ•°æ˜¯$1.0$ï¼Œå®ƒå¯ä»¥æ˜¯è´Ÿçš„(å› ä¸ºæ¨¡å‹å¯ä»¥ä»»æ„æ¶åŒ–)ã€‚ä¸€ä¸ªå¸¸æ•°æ¨¡å‹æ€»æ˜¯é¢„æµ‹$y$çš„æœŸæœ›å€¼ï¼Œè€Œä¸è€ƒè™‘è¾“å…¥ç‰¹æ€§ï¼Œåˆ™å¾—åˆ°$R^2$åˆ†æ•°ä¸º$0.0$ã€‚ R^2(\hat{y}, y) = 1 - \frac{\sum_{i=1}^{n_{samples}} (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{n_{samples}} (y^{(i)} - \overline{y})^2}å…¶ä¸­ \overline{y} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} y^{(i)}åˆ†ç±»(classification)è¯„ä¼°æŒ‡æ ‡å…ˆä½œå¦‚ä¸‹å®šä¹‰ å‡†ç¡®ç‡(Accuracy) Accuracy(y, \hat{y}) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} 1(y^{(i)}=\hat{y}^{(i)})ä¹Ÿå³ Accuracy = \frac{TN+TP}{TN+TP+FN+FP}ç²¾åº¦åªæ˜¯ç®€å•åœ°è®¡ç®—å‡ºæ¯”ä¾‹ï¼Œä½†æ˜¯æ²¡æœ‰å¯¹ä¸åŒç±»åˆ«è¿›è¡ŒåŒºåˆ†ã€‚å› ä¸ºä¸åŒç±»åˆ«é”™è¯¯ä»£ä»·å¯èƒ½ä¸åŒã€‚ä¾‹å¦‚ï¼šåˆ¤æ–­è¿™ä¸ªç—…äººæ˜¯ä¸æ˜¯ç—…å±ï¼Œå¦‚æœä¸æ˜¯ç—…å±é”™è¯¯åˆ¤æ–­ä¸ºç—…å±ï¼Œé‚£åªæ˜¯æŸå¤±ä¸€ç‚¹åŒ»åŠ¡äººå‘˜çš„æ—¶é—´å’Œç²¾åŠ›ï¼Œå¦‚æœæ˜¯æŠŠç—…å±çš„äººåˆ¤æ–­ä¸ºéç—…å±çŠ¶æ€ï¼Œé‚£æŸå¤±çš„å°±æ˜¯ä¸€æ¡äººå‘½ã€‚ä»–ä»¬ä¹‹é—´å­˜åœ¨é‡è¦æ€§å·®å¼‚ï¼Œè¿™æ—¶å€™å°±ä¸èƒ½ç”¨ç²¾åº¦ã€‚å¯¹äºæ ·æœ¬ä¸å‡è¡¡çš„æƒ…å†µï¼Œä¹Ÿä¸æ˜¯ç”¨ç²¾åº¦æ¥è¡¡é‡ã€‚ä¾‹å¦‚ï¼šæœ‰Aç±»1000ä¸ªï¼ŒBç±»5ä¸ªï¼Œå¦‚æœæˆ‘æŠŠè¿™1005ä¸ªæ ·æœ¬éƒ½é¢„æµ‹æˆAç±»ï¼Œæ­£ç¡®ç‡=1000/1005=99.5%ã€‚ ç²¾ç¡®ç‡(Precision)ä¸å¬å›ç‡(Recall) ç²¾ç¡®ç‡(Precision) å³é¢„æµ‹æ­£æ ·æœ¬ä¸­ï¼Œå®é™…ä¸ºæ­£æ ·æœ¬çš„ç™¾åˆ†æ¯”ï¼Œåº¦é‡äº†åˆ†ç±»å™¨ä¸ä¼šå°†çœŸæ­£çš„è´Ÿæ ·æœ¬é”™è¯¯åœ°åˆ†ä¸ºæ­£æ ·æœ¬çš„èƒ½åŠ›ã€‚ Precision = \frac{TP}{TP+FP} å¬å›ç‡(Recall) åˆç§°æŸ¥å…¨ç‡ï¼Œå³å®é™…æ­£æ ·æœ¬ä¸­ï¼Œè¢«é¢„æµ‹ä¸ºæ­£æ ·æœ¬çš„ç™¾åˆ†æ¯”ï¼Œåº¦é‡äº†åˆ†ç±»å™¨æ‰¾åˆ°æ‰€æœ‰æ­£æ ·æœ¬çš„èƒ½åŠ›ã€‚ Recall = \frac{TP}{TP + FN} Fåº¦é‡ F1 score - Wikipedia $F_1$ ä¸ºç²¾ç¡®ç‡(Precision)ä¸å¬å›ç‡(Recall)çš„è°ƒå’Œå‡å€¼(harmonic mean)ã€‚ \frac{1}{F_1} = \frac{1}{2} (\frac{1}{Precision} + \frac{1}{Recall}) ä¹Ÿå³ F_1 = 2 Â· \frac{PrecisionÂ·Recall}{Precision + Recall} $F_{\beta}$ åœ¨$F_1$åº¦é‡çš„åŸºç¡€ä¸Šå¢åŠ æƒå€¼$\beta$ï¼Œ$\beta$è¶Šå¤§ï¼Œ$Recall$çš„æƒé‡è¶Šå¤§ï¼Œå¦åˆ™$Precision$çš„æƒé‡è¶Šå¤§ã€‚ \frac{1}{F_{\beta}} = \frac{1}{1+\beta^2} \frac{1}{Precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{Recall} ä¹Ÿå³ F_{\beta} = (1+\beta^2)Â·\frac{PrecisionÂ·Recall}{(\beta^2Â·Precision) + Recall} æ··æ·†çŸ©é˜µConfusion matrixï¼Œä¹Ÿè¢«ç§°ä½œé”™è¯¯çŸ©é˜µ(Error matrix)ï¼Œæ˜¯ä¸€ä¸ªç‰¹åˆ«çš„è¡¨ã€‚æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œé€šå¸¸ç§°ä½œåŒ¹é…çŸ©é˜µ(Matching matrix)ã€‚æ¯ä¸€åˆ—è¡¨è¾¾äº†åˆ†ç±»å™¨å¯¹æ ·æœ¬çš„ç±»åˆ«é¢„æµ‹ï¼Œæ¯ä¸€è¡Œè¡¨è¾¾äº†æ ·æœ¬æ‰€å±çš„çœŸå®ç±»åˆ«ã€‚ ä¾‹å¦‚æˆ‘ä»¬æœ‰$27$ä¸ªå¾…åˆ†ç±»æ ·æœ¬ï¼Œå°†å…¶åˆ’åˆ†ä¸ºCatï¼ŒDogï¼ŒRabbitï¼Œè®²å®é™…æ ‡ç­¾ä¸é¢„æµ‹æ ‡ç­¾æ•°ç›®ç»Ÿè®¡åå¡«å…¥æ··æ·†çŸ©é˜µã€‚ ä¾‹å¦‚å®é™…ä¸Šæœ‰$8$ä¸ªæ ·æœ¬ä¸ºCatï¼Œè€Œè¯¥åˆ†ç±»å™¨å°†å…¶ä¸­$3$ä¸ªåˆ’åˆ†ä¸ºDogï¼Œå°†$2$ä¸ªä¸ºDogçš„æ ·æœ¬åˆ’åˆ†ä¸ºCatã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®ä¸Šè¿°æ··æ·†çŸ©é˜µå¾—å‡ºç»“è®ºï¼Œè¯¥åˆ†ç±»å™¨å¯¹Dogå’ŒCatåˆ†ç±»èƒ½åŠ›è¾ƒå¼±ï¼Œè€Œå¯¹Rabbitåˆ†ç±»èƒ½åŠ›è¾ƒå¼ºã€‚è€Œä¸”æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°ç›®éƒ½åœ¨å¯¹è§’çº¿ä¸Šï¼Œå¾ˆå®¹æ˜“ç›´è§‚åœ°æ£€æŸ¥è¡¨ä¸­çš„é¢„æµ‹é”™è¯¯ã€‚ ä»¥ä¸‹ä¸ºscikit-learnä¸­æ··æ·†çŸ©é˜µçš„API1234567891011121314151617181920&gt;&gt;&gt; from sklearn.metrics import confusion_matrix&gt;&gt;&gt; &gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]&gt;&gt;&gt; confusion_matrix(y_true, y_pred)array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])&gt;&gt;&gt; &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]&gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]&gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])&gt;&gt;&gt; &gt;&gt;&gt; # In the binary case, we can extract true positives, etc as follows:&gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()&gt;&gt;&gt; (tn, fp, fn, tp)(0, 2, 1, 1) ROCæ›²çº¿Receiver Operating Characteristicï¼Œæ˜¯æ ¹æ®ä¸€ç³»åˆ—ä¸åŒçš„äºŒåˆ†ç±»æ–¹å¼(åˆ†ç•Œå€¼æˆ–å†³å®šé˜ˆ)ï¼Œä»¥å¬å›ç‡(çœŸæ­£ç‡TPRã€çµæ•åº¦)ä¸ºçºµåæ ‡ï¼Œfall-out(å‡æ­£ç‡FPRã€$1$-ç‰¹å¼‚åº¦)ä¸ºæ¨ªåæ ‡ç»˜åˆ¶çš„æ›²çº¿ã€‚ true positive rate - TPR æ‰€æœ‰é˜³æ€§æ ·æœ¬ä¸­æœ‰å¤šå°‘æ­£ç¡®çš„é˜³æ€§ç»“æœã€‚ TPR = \frac{TP}{P} = \frac{TP}{TP + FN} false positive rate - FPR æ‰€æœ‰é˜´æ€§æ ·æœ¬ä¸­æœ‰å¤šå°‘ä¸æ­£ç¡®çš„é˜³æ€§ç»“æœã€‚ FPR = \frac{FP}{N} = \frac{FP}{FP + TN} ROC space åˆ†åˆ«ä»¥FPRä¸TPRä½œä¸ºæ¨ªçºµè½´(åˆç§°çµæ•åº¦-$1$ç‰¹å¼‚åº¦æ›²çº¿sensitivity vs (1 âˆ’ specificity) plot)ï¼› æ¯æ¬¡é¢„æµ‹ç»“æœæˆ–æ··æ·†çŸ©é˜µçš„å®ä¾‹ä»£è¡¨äº†ROCç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼› ä¾‹å¦‚ä¸Šå›¾ä¸­$A, B, C, Câ€™$æ˜¯ä»¥ä¸‹è¡¨æ•°æ®è®¡ç®—å¾—åˆ°çš„ç‚¹ã€‚ åœ¨ROCç©ºé—´ä¸­æœ€å·¦ä¸Šæ–¹çš„ç‚¹$(0, 1)$ç§°ä½œå®Œç¾åˆ†ç±»å™¨(perfect classification)ï¼› éšæœºåˆ†ç±»å™¨çš„ç»“æœåˆ†å¸ƒåœ¨ROC spaceå¯¹è§’çº¿$(0, 0)-(1, 1)$ä¸Šï¼Œå½“å®éªŒæ¬¡æ•°è¶³å¤Ÿå¤šï¼Œå…¶åˆ†åŒºè¶‹å‘$(0.5, 0.5)$; å¯¹è§’çº¿ä»¥ä¸Šçš„ç‚¹ä»£è¡¨å¥½çš„åˆ†ç±»ç»“æœ(æ¯”éšæœºçš„å¥½)ï¼›çº¿ä¸‹çš„ç‚¹ä»£è¡¨åçš„ç»“æœ(æ¯”éšæœºçš„å·®)ï¼› æ³¨æ„ï¼ŒæŒç»­ä¸è‰¯åˆ†ç±»å™¨çš„è¾“å‡ºå¯ä»¥ç®€å•åœ°åè½¬ä»¥è·å¾—ä¸€ä¸ªå¥½çš„åˆ†ç±»å™¨ï¼Œåè½¬åçš„åˆ†ç±»å™¨ä¸åŸåˆ†ç±»å™¨åœ¨å¹³é¢ä¸Šå…³äºå¯¹è§’çº¿å¯¹ç§°ï¼Œä¾‹å¦‚ç‚¹$Câ€™$ã€‚ ROCæ›²çº¿çš„ç»˜åˆ¶è‹¥è®­ç»ƒé›†æ ·æœ¬ä¸­ï¼Œæ­£æ ·æœ¬ä¸è´Ÿæ ·æœ¬ä»¥æ­£æ€åˆ†å¸ƒçš„å½¢å¼åˆ†å¸ƒåœ¨æ ·æœ¬å¹³é¢ä¸Šï¼Œå¦‚ä¸‹å›¾ï¼Œå·¦å³°ä¸ºè´Ÿæ ·æœ¬ï¼Œå³å³°ä¸ºæ­£æ ·æœ¬ï¼Œå­˜åœ¨éƒ¨åˆ†é‡å (ä¸ç„¶å°±ä¸ç”¨æè¿™ä¹ˆå¤šåˆ†ç±»ç®—æ³•äº†)ã€‚ è‹¥å‡è®¾æ­£æ ·æœ¬æ¦‚ç‡å¯†åº¦ä¸º$f_1(x)$ï¼Œè´Ÿæ ·æœ¬çš„æ¦‚ç‡å¯†åº¦ä¸º$f_0(x)$ï¼Œç»™å®šé˜ˆå€¼$T$ï¼Œåˆ™å³ TPR(T) = \int_T^{\infty} f_1(x) dx FPR(T) = \int_T^{\infty} f_0(x) dxé€‰å–ä¸åŒçš„é˜ˆå€¼åˆ’åˆ†åˆ†ç±»å™¨è¾“å‡ºï¼Œå°±èƒ½å¾—åˆ°ROCæ›²çº¿ã€‚ åœ¨åŸºäºæœ‰é™æ ·æœ¬ä½œROCå›¾æ—¶ï¼Œå¯ä»¥çœ‹åˆ°æ›²çº¿æ¯æ¬¡éƒ½æ˜¯ä¸€ä¸ªâ€œçˆ¬å¡â€ï¼Œé‡åˆ°æ­£ä¾‹å¾€ä¸Šçˆ¬ä¸€æ ¼$(1/m+)$ï¼Œé”™äº†å¾€å³çˆ¬ä¸€æ ¼$(1/m-)$ï¼Œæ˜¾ç„¶å¾€ä¸Šçˆ¬å¯¹äºç®—æ³•æ€§èƒ½æ¥è¯´æ˜¯æœ€å¥½çš„ã€‚ Area Under the Curve - AUCROCæ›²çº¿ä¸‹çš„é¢ç§¯AUCç‰©ç†æ„ä¹‰ä¸ºï¼Œä»»å–ä¸€å¯¹æ­£è´Ÿæ ·æœ¬ï¼Œæ­£æ ·æœ¬çš„é¢„æµ‹å€¼å¤§äºè´Ÿæ ·æœ¬çš„é¢„æµ‹å€¼çš„æ¦‚ç‡ã€‚ A = \int_{-\infty}^{\infty} TPR(T) dFPR(T) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} I(T'> T) f_1(T') f_0(T) dT' dT = P(X_1 > X_0)åŒæ ·çš„ï¼Œåœ¨æœ‰é™ä¸ªæ ·æœ¬ä¸‹ï¼Œå…¶é¢ç§¯ç”¨ç´¯åŠ çš„æ–¹æ³•è®¡ç®—(æ¢¯å½¢é¢ç§¯) AUC = \sum_{i=1}^{m-1} \frac{1}{2} (y_{i+1} + y_i)(x_{i+1} - x_i) $AUC = 1$ï¼Œæ˜¯å®Œç¾åˆ†ç±»å™¨ï¼Œé‡‡ç”¨è¿™ä¸ªé¢„æµ‹æ¨¡å‹æ—¶ï¼Œå­˜åœ¨è‡³å°‘ä¸€ä¸ªé˜ˆå€¼èƒ½å¾—å‡ºå®Œç¾é¢„æµ‹ã€‚ç»å¤§å¤šæ•°é¢„æµ‹çš„åœºåˆï¼Œä¸å­˜åœ¨å®Œç¾åˆ†ç±»å™¨ã€‚ $0.5 &lt; AUC &lt; 1$ï¼Œä¼˜äºéšæœºçŒœæµ‹ã€‚è¿™ä¸ªåˆ†ç±»å™¨ï¼ˆæ¨¡å‹ï¼‰å¦¥å–„è®¾å®šé˜ˆå€¼çš„è¯ï¼Œèƒ½æœ‰é¢„æµ‹ä»·å€¼ã€‚ $AUC = 0.5$ï¼Œè·ŸéšæœºçŒœæµ‹ä¸€æ ·ï¼ˆä¾‹ï¼šä¸¢é“œæ¿ï¼‰ï¼Œæ¨¡å‹æ²¡æœ‰é¢„æµ‹ä»·å€¼ã€‚ $AUC &lt; 0.5$ï¼Œæ¯”éšæœºçŒœæµ‹è¿˜å·®ï¼›ä½†åªè¦æ€»æ˜¯åé¢„æµ‹è€Œè¡Œï¼Œå°±ä¼˜äºéšæœºçŒœæµ‹ã€‚ sklearnä»¥ä¸‹ä¸ºscikit-learnä¸­æ··æ·†çŸ©é˜µçš„ROCæ›²çº¿APIã€‚12345678910111213141516&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn import metrics&gt;&gt;&gt; &gt;&gt;&gt; y = np.array([1, 1, 2, 2])&gt;&gt;&gt; scores = np.array([0.1, 0.4, 0.35, 0.8])&gt;&gt;&gt; &gt;&gt;&gt; fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)&gt;&gt;&gt; fprarray([ 0. , 0.5, 0.5, 1. ])&gt;&gt;&gt; tprarray([ 0.5, 0.5, 1. , 1. ])&gt;&gt;&gt; thresholdsarray([ 0.8 , 0.4 , 0.35, 0.1 ])&gt;&gt;&gt; &gt;&gt;&gt; metrics.auc(fpr, tpr)0.75 èšç±»(clustering)è¯„ä¼°æŒ‡æ ‡ AIï¼ˆ005ï¼‰ - ç¬”è®° - èšç±»æ€§èƒ½è¯„ä¼°ï¼ˆClustering Evaluationï¼‰ - DarkRabbitçš„ä¸“æ  - CSDNåšå®¢ Wikipedia, the free encyclopedia è¯´æ˜èšç±»æ€§èƒ½æ¯”è¾ƒå¥½ï¼Œå°±æ˜¯èšç±»ç»“æœç°‡å†…ç›¸ä¼¼åº¦(intra-cluster similarity)é«˜ï¼Œè€Œç°‡é—´ç›¸ä¼¼åº¦(inter-cluster similarity)ä½ï¼Œå³åŒä¸€ç°‡çš„æ ·æœ¬å°½å¯èƒ½çš„ç›¸ä¼¼ï¼Œä¸åŒç°‡çš„æ ·æœ¬å°½å¯èƒ½ä¸åŒã€‚ èšç±»æ€§èƒ½çš„è¯„ä¼°ï¼ˆåº¦é‡ï¼‰åˆ†ä¸ºä¸¤å¤§ç±»ï¼š å¤–éƒ¨è¯„ä¼°(external evaluation)ï¼šå°†ç»“æœä¸æŸä¸ªå‚è€ƒæ¨¡å‹(reference model)è¿›è¡Œæ¯”è¾ƒï¼› å†…éƒ¨è¯„ä¼°(internal evaluation)ï¼šç›´æ¥è€ƒè™‘èšç±»ç»“æœè€Œä¸åˆ©ç”¨ä»»ä½•å‚è€ƒæ¨¡å‹ã€‚ å°†$n_{samples}$ä¸ªæ ·æœ¬$\{x^{(1)}, â€¦, x^{(n_{samples})}\}$ç”¨å¾…è¯„ä¼°èšç±»ç®—æ³•åˆ’åˆ†ä¸º$K$ä¸ªç±»$\{X_1, â€¦, X_K\}$ï¼Œå‡å®šå‚è€ƒæ¨¡å‹å°†å…¶åˆ’åˆ†ä¸º$L$ç±»$\{Y_1, â€¦, Y_L\}$ï¼Œå°†æ ·æœ¬ä¸¤è¾†åŒ¹é… \begin{cases} a = |SS| & SS = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)}, x^{(j)} \in Y_l\} \\ b = |SD| & SD = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \\ c = |DS| & DS = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)}, x^{(j)} \in Y_l\} \\ d = |DD| & DD = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \end{cases}å…¶ä¸­$k = 1, â€¦, K; l = 1, â€¦, L$ a + b + c + d = \left( \begin{matrix} n \\ 2 \end{matrix} \right) = \frac{n(n-1)}{2} $SS$åŒ…å«ä¸¤ç§åˆ’åˆ†ä¸­å‡å±äºåŒä¸€ç±»çš„æ ·æœ¬å¯¹ï¼› $SD$åŒ…å«ç”¨å¾…è¯„ä¼°èšç±»ç®—æ³•åˆ’åˆ†ä¸­å±äºåŒä¸€ç±»ï¼Œè€Œåœ¨å‚è€ƒæ¨¡å‹ä¸­å±äºä¸åŒç±»çš„æ ·æœ¬å¯¹ï¼› $DS$åŒ…å«ç”¨å¾…è¯„ä¼°èšç±»ç®—æ³•åˆ’åˆ†ä¸­å±äºä¸åŒç±»ï¼Œè€Œåœ¨å‚è€ƒæ¨¡å‹ä¸­å±äºåŒä¸€ç±»çš„æ ·æœ¬å¯¹ï¼› $DD$åŒ…å«ä¸¤ç§åˆ’åˆ†ä¸­å‡ä¸å±äºåŒä¸€ç±»çš„æ ·æœ¬å¯¹ã€‚ å¸¸ç”¨å¤–éƒ¨è¯„ä¼°(external evaluation)Rand Index(RI) Rand index - Wikipedia RI = \frac{a+d}{a + b + c + d} = \frac{a+d}{\left(\begin{matrix} n \\ 2 \end{matrix}\right)}æ˜¾ç„¶ï¼Œç»“æœå€¼åœ¨$[0,1]$ä¹‹é—´ï¼Œä¸”å€¼è¶Šå¤§è¶Šå¥½ã€‚ å½“ä¸º$0$æ—¶ï¼Œä¸¤ä¸ªèšç±»æ— é‡å ï¼› å½“ä¸º$1$æ—¶ï¼Œä¸¤ä¸ªèšç±»å®Œå…¨é‡å ã€‚ Adjust Rand Index(ARI)è®©$RI$æœ‰äº†ä¿®æ­£æœºä¼š(corrected-for-chance)ï¼Œåœ¨å–å€¼ä¸Šä»$[0,1]$å˜æˆ$[-1, 1]$ å¯¹äº$X$ä¸$Y$çš„é‡å å¯ä»¥ç”¨ä¸€ä¸ªåˆ—è”è¡¨(contingency table)è¡¨ç¤ºï¼Œè®°ä½œ$[n_{ij}]$ï¼Œ$n_{ij} = |X_i \bigcap Y_j|$ åˆ™å®šä¹‰$ARI$å¦‚ä¸‹ äº’ä¿¡æ¯ä¸è°ƒæ•´äº’ä¿¡æ¯(Adjusted Mutual Information - AMI) å…³äºäº’ä¿¡æ¯å¯æŸ¥çœ‹ç†µä¸€èŠ‚è¯´æ˜ã€‚ $X_i$ç±»åˆ«çš„æ¦‚ç‡å®šä¹‰ä¸º P(k) = \frac{|X_k|}{N}åˆ™åˆ’åˆ†ç»“æœçš„ç†µå®šä¹‰ä¸º H(X) = - \sum_k P(k) \log P(k)ç±»ä¼¼çš„ P'(l) = \frac{|Y_l|}{N} H(Y) = - \sum_j P'(l) \log P'(l)å¦å¤– P(k, l) = \frac{|X_k, Y_l|}{N}é‚£ä¹ˆä¸¤ç§åˆ’åˆ†çš„äº’ä¿¡æ¯å®šä¹‰ä¸º MI(X, Y) = \sum_{k, l} P(k, l) \log \frac{P(k, l)}{P(k) P'(l)}å’Œ$ARI$ä¸€æ ·ï¼Œæˆ‘ä»¬å¯¹å®ƒè¿›è¡Œè°ƒæ•´ã€‚ E[MI(X, Y)] = \sum_k \sum_l \sum_{n_{kl} = \max\{1, a_k + b_l - N\}}^{\min \{a_k, b_l\}} \frac{n_{kl}}{N} \log \left( \frac{NÂ·n_{kl}}{a_k b_l} \right) Ã— \frac {a_k!b_l!(N-a_k)!(N-b_l)!} {N!n_{kl}!(a_k-n_{kl})!(b_l-n_{kl})!(N-a_k-b_l+n_{kl})!}æœ€ç»ˆ$AMI$è¡¨è¾¾å¼ä¸º AMI(X, Y) = \frac{MI(X, Y) - E[MI(X, Y)]}{\max \{H(X), H(Y)\} - E[MI(X, Y)]}åŒè´¨æ€§(Homogeneity)ä¸å®Œæ•´æ€§(Completeness)è¿™ä¸¤ä¸ªç±»ä¼¼åˆ†ç±»ç§çš„çš„å‡†ç¡®ç‡(accuracy)ä¸å¬å›ç‡(recall)ã€‚ åŒè´¨æ€§(Homogeneity) å³ä¸€ä¸ªç°‡ä»…åŒ…å«ä¸€ä¸ªç±»åˆ«çš„æ ·æœ¬ H = 1 - \frac{H(X|Y)}{H(X)} å…¶ä¸­$H(X|Y)$ä¸ºæ¡ä»¶ç†µ H(X|Y) = \sum_k \sum_l P(X_k, Y_l) \log \frac{P(Y_l)}{P(X_k, Y_l)} = \sum_k \sum_l \frac{n_{kl}}{N} \log \frac{n_{kl}}{N} å®Œæ•´æ€§(Completeness) åŒç±»åˆ«æ ·æœ¬è¢«å½’ç±»åˆ°ç›¸åŒç°‡ä¸­ C = 1 - \frac{H(Y|X)}{H(Y)} $V-measure$ Homogeneityå’ŒCompletenessçš„è°ƒå’Œå¹³å‡ V = \frac{1}{\frac{1}{2} \left(\frac{1}{H} + \frac{1}{C}\right)} = \frac{2HC}{H + C} Fowlkes-Mallows index(FMI)æˆå¯¹ç²¾åº¦å’Œå¬å›ç‡çš„å‡ ä½•å‡å€¼ Fowlkesâ€“Mallows index - Wikipedia å®šä¹‰ $TP$ as the number of points that are present in the same cluster in both $A_1$ and $A_2$. $FP$ as the number of points that are present in the same cluster in $A_1$ but not in $A_2$. $FN$ as the number of points that are present in the same cluster in $A_2$ but not in $A_1$. $TN$ as the number of points that are in different clusters in both $A_1$ and $A_2$. åˆ™ TP + FP + TN + FN = \frac{n(n-1)}{2}å®šä¹‰ FMI = \sqrt{\frac{TP}{TP + FP} Â· \frac{TP}{TP + FN}}æ°å¡å¾·ç³»æ•°(Jaccard Coefficient - JC) Jaccard index - Wikipedia ç»™å®šä¸¤ä¸ªå…·æœ‰$n$ä¸ªå…ƒç´ çš„é›†åˆ$A, B$ï¼Œå®šä¹‰ $M_{11}$ represents the total number of attributes where $A$ and $B$ both have a value of $1$. $M_{01}$ represents the total number of attributes where the attribute of $A$ is $0$ and the attribute of $B$ is $1$. $M_{10}$ represents the total number of attributes where the attribute of $A$ is $1$ and the attribute of $B$ is $0$. $M_{00}$ represents the total number of attributes where $A$ and $B$ both have a value of $0$. åˆ™æœ‰ M_{11} + M_{01} + M_{10} + M_{00} = n Jaccardç›¸ä¼¼åº¦ç³»æ•° J = \frac{M_{11}}{M_{01} + M_{10} + M_{11}} ä¹Ÿå³$J=\frac{A \cap B}{A \cup B}$ Jaccardè·ç¦» D_J = 1 - J å¸¸ç”¨å†…éƒ¨è¯„ä¼°(internal evaluation)è½®å»“ç³»æ•°(Silhouette coefficient)åˆç§°ä¾§å½±æ³•ï¼Œé€‚ç”¨äºå®é™…ç±»åˆ«ä¿¡æ¯æœªçŸ¥çš„æƒ…å†µï¼Œå¯¹å…¶ä¸­ä¸€ä¸ªæ ·æœ¬ç‚¹$x^{(i)}$ï¼Œè®° $a(i)$ï¼šåˆ°æœ¬ç°‡å…¶ä»–æ ·æœ¬ç‚¹çš„è·ç¦»çš„å¹³å‡å€¼ $b(i)$ï¼šè¯¥ç‚¹åˆ°å…¶ä»–å„ä¸ªç°‡çš„æ ·æœ¬ç‚¹çš„å¹³å‡è·ç¦»çš„æœ€å°å€¼ å®šä¹‰è½®å»“ç³»æ•° S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}æˆ–è€… S(i) = \begin{cases} 1 - \frac{a(i)}{b(i)} & a(i) < b(i) \\ 0 & a(i) = b(i) \\ \frac{b(i)}{a(i)} - 1 & a(i) > b(i) \end{cases}å…¶å«ä¹‰å¦‚ä¸‹ å½“$a(i) \ll b(i)$æ—¶ï¼Œæ— é™æ¥è¿‘äº$1$ï¼Œåˆ™æ„å‘³ç€èšç±»åˆé€‚ï¼› å½“$a(i) \gg b(i)$æ—¶ï¼Œæ— é™æ¥è¿‘äº$-1$ï¼Œåˆ™æ„å‘³ç€æŠŠæ ·æœ¬ièšç±»åˆ°ç›¸é‚»ç°‡ä¸­æ›´åˆé€‚ï¼› å½“$a(i)\approxeq b(i)$æ—¶ï¼Œæ— é™æ¥è¿‘äº$0$ï¼Œåˆ™æ„å‘³ç€æ ·æœ¬åœ¨ä¸¤ä¸ªç°‡äº¤é›†å¤„ã€‚ ä¸€èˆ¬å†å¯¹å„ä¸ªç‚¹çš„è½®å»“ç³»æ•°æ±‚å‡å€¼ \overline{S} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} S(i) å½“$\overline{S} &gt; 0.5$ï¼Œè¡¨ç¤ºèšç±»åˆé€‚ï¼› å½“$\overline{S} &lt; 0.2$ï¼Œè¡¨ç¤ºè¡¨æ˜æ•°æ®ä¸å­˜åœ¨èšç±»ç‰¹å¾ Calinski-Harabaz(CH)ä¹Ÿé€‚ç”¨äºå®é™…ç±»åˆ«ä¿¡æ¯æœªçŸ¥çš„æƒ…å†µï¼Œä»¥$K$åˆ†ç±»ä¸ºä¾‹ ç±»å†…æ•£åº¦$W$ W(K) = \sum_k \sum_{C(j)=k} ||x_j - \overline{x_k}||^2 ç±»é—´æ•£åº¦$B$ B(K) = \sum_k a_k ||\overline{x_k} - \overline{x}||^2 $CH$ CH(K) = \frac{B(K)(N-K)}{W(K)(K-1)} Davies-Bouldin Index(DBI)å®šä¹‰ $c_k$ï¼šç°‡$C_k$çš„ä¸­å¿ƒç‚¹ $\sigma_k$ï¼šç°‡$C_k$ä¸­æ‰€æœ‰å…ƒç´ åˆ°$c_k$çš„è·ç¦»çš„å‡å€¼ $d(c_i, c_j)$ï¼šç°‡ä¸­å¿ƒ$c_i$ä¸$c_j$ä¹‹é—´çš„è·ç¦» åˆ™ DBI = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)$DBI$è¶Šå°è¶Šå¥½ Dunn index(DI)å®šä¹‰ $d(i,j)$ï¼šä¸¤ç±»ç°‡çš„è·ç¦»ï¼Œå®šä¹‰æ–¹æ³•å¤šæ ·ï¼Œä¾‹å¦‚ä¸¤ç±»ç°‡ä¸­å¿ƒçš„è·ç¦»ï¼› $dâ€™(k)$ï¼šç°‡$C_k$çš„ç±»å†…è·ç¦»ï¼ŒåŒæ ·çš„ï¼Œå¯å®šä¹‰å¤šç§ï¼Œä¾‹å¦‚ç°‡$C_k$ä¸­ä»»æ„ä¸¤ç‚¹è·ç¦»çš„æœ€å¤§å€¼ã€‚ åˆ™ DI = \frac{\min_{1 \leq i < j \leq K} d(i, j)}{\max_{1 \leq k \leq K} d'(k)}sklearnä¸­çš„è¯„ä»·æŒ‡æ ‡ 3.3. Model evaluation: quantifying the quality of predictions â€” scikit-learn 0.19.0 documentation - ApacheCN 1234567891011121314151617181920212223242526&gt;&gt;&gt; from sklearn import metrics&gt;&gt;&gt; dir(metrics)[&apos;SCORERS&apos;, &apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;, &apos;accuracy_score&apos;, &apos;adjusted_mutual_info_score&apos;, &apos;adjusted_rand_score&apos;, &apos;auc&apos;, &apos;average_precision_score&apos;, &apos;balanced_accuracy_score&apos;, &apos;base&apos;, &apos;brier_score_loss&apos;, &apos;calinski_harabaz_score&apos;, &apos;check_scoring&apos;, &apos;classification&apos;, &apos;classification_report&apos;, &apos;cluster&apos;, &apos;cohen_kappa_score&apos;, &apos;completeness_score&apos;, &apos;confusion_matrix&apos;, &apos;consensus_score&apos;, &apos;coverage_error&apos;, &apos;davies_bouldin_score&apos;, &apos;euclidean_distances&apos;, &apos;explained_variance_score&apos;, &apos;f1_score&apos;, &apos;fbeta_score&apos;, &apos;fowlkes_mallows_score&apos;, &apos;get_scorer&apos;, &apos;hamming_loss&apos;, &apos;hinge_loss&apos;, &apos;homogeneity_completeness_v_measure&apos;, &apos;homogeneity_score&apos;, &apos;jaccard_similarity_score&apos;, &apos;label_ranking_average_precision_score&apos;, &apos;label_ranking_loss&apos;, &apos;log_loss&apos;, &apos;make_scorer&apos;, &apos;matthews_corrcoef&apos;, &apos;mean_absolute_error&apos;, &apos;mean_squared_error&apos;, &apos;mean_squared_log_error&apos;, &apos;median_absolute_error&apos;, &apos;mutual_info_score&apos;, &apos;normalized_mutual_info_score&apos;, &apos;pairwise&apos;, &apos;pairwise_distances&apos;, &apos;pairwise_distances_argmin&apos;, &apos;pairwise_distances_argmin_min&apos;, &apos;pairwise_distances_chunked&apos;, &apos;pairwise_fast&apos;, &apos;pairwise_kernels&apos;, &apos;precision_recall_curve&apos;, &apos;precision_recall_fscore_support&apos;, &apos;precision_score&apos;, &apos;r2_score&apos;, &apos;ranking&apos;, &apos;recall_score&apos;, &apos;regression&apos;, &apos;roc_auc_score&apos;, &apos;roc_curve&apos;, &apos;scorer&apos;, &apos;silhouette_samples&apos;, &apos;silhouette_score&apos;, &apos;v_measure_score&apos;, &apos;zero_one_loss&apos;]]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Entropy]]></title>
    <url>%2F2018%2F11%2F21%2FEntropy%2F</url>
    <content type="text"><![CDATA[ä¿¡æ¯é‡æ¦‚ç‡$p$æ˜¯å¯¹ç¡®å®šæ€§çš„åº¦é‡ï¼Œé‚£ä¹ˆä¿¡æ¯é‡å°±æ˜¯å¯¹ä¸ç¡®å®šæ€§çš„åº¦é‡ï¼Œå…¬å¼å®šä¹‰ä¸º I(x) = - \log p(x) \tag{1}ä¿¡æ¯é‡ä¹Ÿè¢«ç§°ä¸ºéšæœºå˜é‡$x$çš„è‡ªä¿¡æ¯(self-information) åº•æ•°ä¸º$2$æ—¶ï¼Œå•ä½ä¸ºbitï¼Œåº•æ•°ä¸º$e$æ—¶ï¼Œå•ä½ä¸ºnat ä¿¡æ¯ç†µä¿¡æ¯ç†µ(information entropy)å®šä¹‰ä¸º H(X) = - \sum_{x} p(x) \log p(x) \tag{2}å¯çœ‹ä½œä¿¡æ¯é‡çš„æœŸæœ›,åœ¨$0-1$åˆ†å¸ƒçš„ä¿¡æ¯ç†µä¸º H(p) = - p \log p - (1 - p) \log (1 - p)å›¾åƒå¦‚ä¸‹ï¼Œå¯è§åœ¨$p=0.5$æ—¶ï¼Œç†µæœ€å¤§ã€‚ å‡½æ•°$y=x \log x$çš„å›¾åƒæœ‰ \lim_{x \rightarrow 0} y = \lim_{x \rightarrow 1} y = 0 è”åˆç†µæ ¹æ®ä¿¡æ¯ç†µçš„å®šä¹‰ï¼Œæ¨å¹¿åˆ°å¤šç»´éšæœºå˜é‡ï¼Œå°±å¾—åˆ°è”åˆç†µçš„å®šä¹‰å¼ï¼Œä»¥$2$ç»´éšæœºå˜é‡ä¸ºä¾‹ H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y) \tag{3}å¯æ¨å¹¿è‡³å¤šç»´ã€‚ äº¤å‰ç†µç°åœ¨æœ‰å…³äºæ ·æœ¬é›†çš„ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ$p(x)$å’Œ$q(x)$ï¼Œå…¶ä¸­$p(x)$ä¸ºçœŸå®åˆ†å¸ƒï¼Œ$q(x)$éçœŸå®åˆ†å¸ƒã€‚ å¦‚æœç”¨çœŸå®åˆ†å¸ƒ$p(x)$æ¥è¡¡é‡è¯†åˆ«åˆ«ä¸€ä¸ªæ ·æœ¬æ‰€éœ€è¦ç¼–ç é•¿åº¦çš„æœŸæœ›ï¼ˆå¹³å‡ç¼–ç é•¿åº¦ï¼‰ä¸º: H(p) = - \sum_x p(x) \log p(x)å¦‚æœç”¨éçœŸå®åˆ†å¸ƒ$q(x)$æ¥è¡¡é‡è¯†åˆ«åˆ«ä¸€ä¸ªæ ·æœ¬æ‰€éœ€è¦ç¼–ç é•¿åº¦çš„æœŸæœ›ï¼ˆå¹³å‡ç¼–ç é•¿åº¦ï¼‰ä¸º: H(p, q) = - \sum_x p(x) \log q(x) \tag{4}æ³¨æ„ H(p, q) - H(p) = \sum_x p(x) \log \frac{p(x)}{q(x)} = D_{KL}(p||q)å½“ç”¨éçœŸå®åˆ†å¸ƒ$q(x)$å¾—åˆ°çš„å¹³å‡ç é•¿æ¯”çœŸå®åˆ†å¸ƒ$p(x)$å¾—åˆ°çš„å¹³å‡ç é•¿å¤šå‡ºçš„æ¯”ç‰¹æ•°å°±æ˜¯ç›¸å¯¹ç†µã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æœ€å°åŒ–ç›¸å¯¹ç†µ$D_{KL}(p||q)$ä½¿$q(x)$å°½é‡è¶‹è¿‘$p(x)$ï¼Œå³ q(x) = \arg \min_{q(x)} D_{KL} (p||q)è€Œ$H(p)$æ˜¯æ ·æœ¬é›†çš„ç†µï¼Œä¸ºå›ºå®šçš„å€¼ï¼Œæ•… q(x) = \arg \min_{q(x)} H(p, q)å³ç­‰ä»·äºæœ€å°åŒ–äº¤å‰ç†µã€‚ æ¡ä»¶ç†µæ¡ä»¶ç†µ$H(Y|X)$è¡¨ç¤ºåœ¨å·²çŸ¥éšæœºå˜é‡$X$çš„æ¡ä»¶ä¸‹éšæœºå˜é‡$Y$çš„ä¸ç¡®å®šæ€§ã€‚å®šä¹‰ä¸ºåœ¨ç»™å®š$X$ä¸‹$Y$çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„ç†µå¯¹$X$çš„æœŸæœ›ï¼Œå³ H(Y|X) = E_{p(x)} H(Y|X=x) = \sum_x p(x) H(Y|X=x) \tag{5}å…¶ä¸­ H(Y|X=x) = - \sum_y p(y|x) \log p(y|x)æ•… H(Y|X) = \sum_x p(x) \left[- \sum_y p(y|x) \log p(y|x)\right] = - \sum_x \sum_y p(x, y) \log p(y|x)å³ H(Y|X) = - \sum_{x, y} p(x, y) \log p(y|x) \tag{6}å®é™…ä¸Šï¼Œæ¡ä»¶ç†µæ»¡è¶³ H(Y|X) = H(X, Y) - H(X) \tag{7} è¯æ˜ï¼šå·²çŸ¥ H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y) H(X) = - \sum_{x} p(x) \log p(x)åˆ™ H(X, Y) - H(X) = - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x} p(x) \log p(x) = - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x, y} p(x, y) \log p(x) = \sum_{x, y} p(x, y) \log \frac{p(x)}{p(x, y)} = \sum_{x, y} p(x, y) \log p(y|x) = H(Y|X) ç›¸å¯¹ç†µç›¸å¯¹ç†µ(relative entropy)ï¼Œåˆç§°KLæ•£åº¦(Kullbackâ€“Leibler divergence)ã€‚å¯ä»¥ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå°±æ˜¯æ±‚$p(x)$ä¸$q(x)$ä¹‹é—´çš„å¯¹æ•°å·®åœ¨ pp ä¸Šçš„æœŸæœ›å€¼ã€‚ D_{KL} (p||q) = E_{p(x)} \log \frac{p(x)}{q(x)} = \sum_x p(x) \log \frac{p(x)}{q(x)} \tag{8}æ³¨æ„ ç›¸å¯¹ç†µä¸å…·æœ‰å¯¹ç§°æ€§ï¼Œå³ D_{KL} (p||q) \neq D_{KL} (q||p) $D_{KL} (p||q) \geq 0$ è¯æ˜ï¼š D_{KL} (p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = - \sum_x p(x) \log \frac{q(x)}{p(x)}ç”±Jensen inequality \sum_x p(x) \log \frac{q(x)}{p(x)} \leq \log \sum_x p(x) \frac{q(x)}{p(x)} = \log \sum_x q(x)æ‰€ä»¥ D_{KL} (p||q) \geq - \log \sum_x q(x)è€Œ$0 \leq q(x) \leq 1$ï¼Œæ•… D_{KL} (p||q) \geq 0]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Deep Learing</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Non-parameter Estimation]]></title>
    <url>%2F2018%2F11%2F19%2FNon-parameter-Estimation%2F</url>
    <content type="text"><![CDATA[å‰è¨€è‹¥å‚æ•°ä¼°è®¡æ—¶æˆ‘ä»¬ä¸çŸ¥é“æ ·æœ¬çš„åˆ†å¸ƒå½¢å¼ï¼Œé‚£ä¹ˆå°±æ— æ³•ç¡®å®šéœ€è¦ä¼°è®¡çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œæ— æ³•ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€è´å¶æ–¯ä¼°è®¡ç­‰å‚æ•°ä¼°è®¡æ–¹æ³•ï¼Œåº”è¯¥ç”¨éå‚æ•°ä¼°è®¡æ–¹æ³•ã€‚ éœ€è¦çŸ¥é“çš„æ˜¯ï¼Œä½œä¸ºéå‚æ•°æ–¹æ³•çš„å…±åŒé—®é¢˜æ˜¯å¯¹æ ·æœ¬æ•°é‡éœ€æ±‚è¾ƒå¤§ï¼Œåªè¦æ ·æœ¬æ•°ç›®è¶³å¤Ÿå¤§ä¼—å¯ä»¥ä¿è¯æ”¶æ•›äºä»»ä½•å¤æ‚çš„ä½ç½®å¯†åº¦ï¼Œä½†æ˜¯è®¡ç®—é‡å’Œå­˜å‚¨é‡éƒ½æ¯”è¾ƒå¤§ã€‚å½“æ ·æœ¬æ•°å¾ˆå°‘æ—¶ï¼Œå¦‚æœèƒ½å¤Ÿå¯¹å¯†åº¦å‡½æ•°æœ‰å…ˆéªŒè®¤è¯†ï¼Œåˆ™å‚æ•°ä¼°è®¡èƒ½å–å¾—æ›´å¥½çš„ä¼°è®¡æ•ˆæœã€‚ åŸºæœ¬åŸç†è‹¥æœ‰$M$ä¸ªæ ·æœ¬$x^{(1)}, â€¦, x^{(M)}$ï¼Œä¾æ¦‚ç‡å¯†åº¦å‡½æ•°$p(x)$ç‹¬ç«‹åŒåˆ†å¸ƒæŠ½æ ·å¾—åˆ°ã€‚ ä¸€ä¸ªæ ·æœ¬$x$è½åœ¨åŒºåŸŸ$R$ä¸­çš„æ¦‚ç‡$P$å¯è¡¨ç¤ºä¸º P = \int_R p(x) dx \tag{1}æˆ‘ä»¬é€šè¿‡è®¡ç®—$P$æ¥ä¼°è®¡æ¦‚ç‡å¯†åº¦$p(x)$ã€‚ $K$ä¸ªæ ·æœ¬è½å…¥åŒºåŸŸ$R$çš„æ¦‚ç‡$P_K$ä¸ºäºŒé¡¹åˆ†å¸ƒï¼Œå³$K \sim B(M, P)$ P_K = \left(\begin{matrix} M\\K \end{matrix}\right) P^K (1-P)^{M-K} \tag{2}åˆ™$K$çš„æœŸæœ›ä¸æ–¹å·®åˆ†åˆ«ä¸º E(K) = MP; D(K) = MP(1-P)æ ·æœ¬ä¸ªæ•°$M$è¶Šå¤šï¼Œ$D(K)$è¶Šå¤§ï¼Œå³$K$åœ¨æœŸæœ›é™„è¿‘çš„æ³¢å³°è¶Šæ˜æ˜¾ï¼Œå› æ­¤æ ·æœ¬è¶³å¤Ÿå¤šæ—¶ï¼Œç”¨$K/M$ä½œä¸º$P$çš„ä¸€ä¸ªä¼°è®¡éå¸¸å‡†ç¡®ï¼Œå³ P \approx \frac{K}{M} \tag{3}è‹¥æˆ‘ä»¬å‡è®¾$p(x)$æ˜¯è¿ç»­çš„ï¼Œä¸”åŒºåŸŸ$R$è¶³å¤Ÿå°ï¼Œè®°å…¶ä½“ç§¯ä¸º$V$ï¼Œé‚£ä¹ˆæœ‰ P = \int_R p(x)dx \approx p(x) V \tag{4}æ‰€ä»¥æ ¹æ®$(3)(4)$ï¼Œå¾—åˆ° p(x) \approx \frac{K/M}{V} \tag{*}ä½†æ˜¯æˆ‘ä»¬è·å¾—çš„å…¶å®ä¸ºå¹³æ»‘åçš„æ¦‚ç‡å¯†åº¦å‡½æ•° \frac{P}{V} = \frac{\int_R p(x)dx}{\int_R dx}æˆ‘ä»¬å¸Œæœ›å…¶å°½å¯èƒ½åœ°è¶‹è¿‘$p(x)$ï¼Œé‚£ä¹ˆå¿…é¡»è¦æ±‚$V \rightarrow 0$ï¼Œä½†æ˜¯è¿™æ ·å°±å¯èƒ½ä¸åŒ…å«ä»»ä½•æ ·æœ¬ï¼Œé‚£ä¹ˆ$p(x)\approx 0$ï¼Œè¿™æ ·ä¼°è®¡çš„ç»“æœæ¯«æ— æ„ä¹‰ã€‚ æ‰€ä»¥åœ¨å®é™…ä¸­ï¼Œä¸€èˆ¬æ„é€ å¤šä¸ªåŒ…å«æ ·æœ¬$x$çš„åŒºåŸŸ$R_1, â€¦, R_i, â€¦, R_n$ï¼Œç¬¬$i$ä¸ªåŒºåŸŸä½¿ç”¨$i$ä¸ªæ ·æœ¬ï¼Œè®°$V_i$ä¸º$R_i$çš„ä½“ç§¯ï¼Œ$M_i$ä¸ºè½åœ¨$R_i$ä¸­çš„æ ·æœ¬ä¸ªæ•°ï¼Œåˆ™å¯¹$p(x)$ç¬¬$i$æ¬¡ä¼°è®¡$p_i(x)$è¡¨ç¤ºä¸º p_i(x) \approx \frac{M_i / M}{V_i} \tag{5}è‹¥è¦æ±‚$p_i(x)$æ”¶æ•›åˆ°$p(x)$ï¼Œåˆ™å¿…é¡»æ»¡è¶³ $\lim_{i\rightarrow \infty} V_i = 0$ $\lim_{i\rightarrow \infty} M_i = 0$ $\lim_{i\rightarrow \infty} \frac{M_i}{M} = 0$ ç›´æ–¹å›¾æ³•è®°ä¸è®°å¾—å°å­¦æ—¶çš„ç›´æ–¹å›¾ç»Ÿè®¡ï¼Œç›´æ–¹å›¾æ–¹æ³•çš„æ€æƒ³å°±æ˜¯è¿™æ ·ï¼Œä»¥$1$ç»´æ ·æœ¬ä¸ºä¾‹ï¼Œæˆ‘ä»¬å°†$x$çš„å–å€¼èŒƒå›´å¹³å‡ç­‰åˆ†ä¸º$K$ä¸ªåŒºé—´ï¼Œç»Ÿè®¡æ¯ä¸ªåŒºé—´å†…æ ·æœ¬çš„ä¸ªæ•°ï¼Œç”±æ­¤è®¡ç®—åŒºé—´çš„æ¦‚ç‡å¯†åº¦ã€‚ åŸç†è‹¥å…±æœ‰$N$ç»´æ ·æœ¬$M$ç»„ï¼Œåœ¨æ¯ä¸ªç»´åº¦ä¸Š$K$ç­‰åˆ†ï¼Œå°±æœ‰$K^N$ä¸ªå°ç©ºé—´ï¼Œæ¯ä¸ªå°ç©ºé—´çš„ä½“ç§¯$V_i$å¯ä»¥å®šä¹‰ä¸º V_i = \prod_{n=1}^N d_n, i=1,...,K^Nå…¶ä¸­ d_n = \frac{\max x_n - \min x_n}{K}å‡è®¾æ ·æœ¬è½åˆ°å„ä¸ªå°ç©ºé—´çš„æ¦‚ç‡ç›¸åŒï¼Œè‹¥ç¬¬$i$ä¸ªå°ç©ºé—´åŒ…å«$M_i$ä¸ªæ ·æœ¬ï¼Œåˆ™è¯¥ç©ºé—´çš„æ¦‚ç‡å¯†åº¦$\hat{p_i}$ä¸º \hat{p_i} = \frac{M_i / M}{V_i} \tag{6}ä¼°è®¡çš„æ•ˆæœä¸å°åŒºé—´çš„å¤§å°å¯†åˆ‡ç›¸è¿ï¼Œå¦‚æœåŒºåŸŸé€‰æ‹©è¿‡å¤§ï¼Œä¼šå¯¼è‡´æœ€ç»ˆä¼°è®¡å‡ºæ¥çš„æ¦‚ç‡å¯†åº¦å‡½æ•°éå¸¸ç²—ç³™ï¼›å¦‚æœåŒºåŸŸçš„é€‰æ‹©è¿‡å°ï¼Œå¯èƒ½ä¼šå¯¼è‡´æœ‰äº›åŒºåŸŸå†…æ ¹æœ¬æ²¡æœ‰æ ·æœ¬æˆ–è€…æ ·æœ¬éå¸¸å°‘ï¼Œè¿™æ ·ä¼šå¯¼è‡´ä¼°è®¡å‡ºæ¥çš„æ¦‚ç‡å¯†åº¦å‡½æ•°å¾ˆä¸è¿ç»­ã€‚ ä»£ç @Github: Non-parametric Estmation æˆ‘ä»¬å¯ä»¥ç”¨matplotlib.pyplot.hist()æˆ–numpy.histogram()å®ç° matplotlib 1n, bins, patches = plt.hist(arr, bins=10, normed=0, facecolor=&apos;black&apos;, edgecolor=&apos;black&apos;,alpha=1ï¼Œhisttype=&apos;bar&apos;) Args å‚æ•°å¾ˆå¤šï¼Œé€‰å‡ ä¸ªå¸¸ç”¨çš„è®²è§£ arr: éœ€è¦è®¡ç®—ç›´æ–¹å›¾çš„ä¸€ç»´æ•°ç»„ bins: ç›´æ–¹å›¾çš„æŸ±æ•°ï¼Œå¯é€‰é¡¹ï¼Œé»˜è®¤ä¸º10 normed: æ˜¯å¦å°†å¾—åˆ°çš„ç›´æ–¹å›¾å‘é‡å½’ä¸€åŒ–ã€‚é»˜è®¤ä¸º0 facecolor: ç›´æ–¹å›¾é¢œè‰² edgecolor: ç›´æ–¹å›¾è¾¹æ¡†é¢œè‰² alpha: é€æ˜åº¦ histtype: ç›´æ–¹å›¾ç±»å‹ï¼Œâ€˜barâ€™, â€˜barstackedâ€™, â€˜stepâ€™, â€˜stepfilledâ€™ Returns n: ç›´æ–¹å›¾å‘é‡ï¼Œæ˜¯å¦å½’ä¸€åŒ–ç”±å‚æ•°normedè®¾å®š bins: è¿”å›å„ä¸ªbinçš„åŒºé—´èŒƒå›´ patches: è¿”å›æ¯ä¸ªbiné‡Œé¢åŒ…å«çš„æ•°æ®ï¼Œæ˜¯ä¸€ä¸ªlist numpy 1hist, bin_edges = histogram(a, bins=10, range=None, normed=None, weights=None, density=None) 12345678910def histEstimate(X, n_bins, showfig=False): &quot;&quot;&quot; ç›´æ–¹å›¾å¯†åº¦ä¼°è®¡ Args: n_bins: &#123;int&#125; ç›´æ–¹å›¾çš„æ¡æ•° Returns: hist: &#123;ndarray(n_bins,)&#125; &quot;&quot;&quot; n, bins, patches = plt.hist(X, bins=n_bins, normed=1, facecolor=&apos;lightblue&apos;, edgecolor=&apos;white&apos;) if showfig: plt.show() return n, bins, patches matplotlibç›´æ–¹å›¾æ˜¾ç¤ºå¦‚ä¸‹ æ‹Ÿåˆå„ä¸­å¿ƒç‚¹æ˜¾ç¤ºå¦‚ä¸‹ $K_n$è¿‘é‚»ä¼°è®¡æ³•éšç€æ ·æœ¬æ•°çš„å¢åŠ ï¼ŒåŒºåŸŸçš„ä½“ç§¯åº”è¯¥å°½å¯èƒ½å°ï¼ŒåŒæ—¶åˆå¿…é¡»ä¿è¯åŒºåŸŸå†…æœ‰å……åˆ†å¤šçš„æ ·æœ¬ï¼Œä½†æ˜¯æ¯ä¸ªåŒºåŸŸçš„æ ·æœ¬æ•°æœ‰å¿…é¡»æ˜¯æ€»æ ·æœ¬æ•°çš„å¾ˆå°çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ä¸ç›´æ–¹å›¾ä¼°è®¡é‚£æ ·ä½“ç§¯ä¸å˜ã€‚ é‚£ä¹ˆæˆ‘ä»¬æƒ³ï¼Œèƒ½å¦æ ¹æ®æ ·æœ¬çš„åˆ†å¸ƒè°ƒæ•´åˆ†åŒºå¤§å°å‘¢ï¼Œ$K$è¿‘é‚»ä¼°è®¡æ³•å°±æ˜¯ä¸€ç§é‡‡ç”¨å¯å˜å¤§å°åŒºé—´çš„å¯†åº¦ä¼°è®¡æ–¹æ³•ã€‚ åŸç†æ ¹æ®æ€»æ ·æœ¬ç¡®å®šå‚æ•°$K_n$ï¼Œåœ¨æ±‚æ ·æœ¬$x$å¤„çš„å¯†åº¦ä¼°è®¡$\hat{p}(x)$æ—¶ï¼Œè°ƒæ•´åŒºåŸŸä½“ç§¯$V(x)$ï¼Œç›´åˆ°åŒºåŸŸå†…æ°å¥½è½å…¥$K_n$ä¸ªæ ·æœ¬ï¼Œä¼°è®¡å…¬å¼ä¸º \hat{p}(x) = \frac{K_n/M}{V(x)} \tag{7}ä¸€èˆ¬æŒ‡å®šè¶…å‚æ•°$a$ï¼Œå– K_n = a Ã— \sqrt{M} \tag{8} \hat{p}(x) = \frac{a Ã— \sqrt{M} /M}{V(x)} = \frac{K_n'/M}{V'(x)}å…¶ä¸­$K_nâ€™ = a,Vâ€™(x) = V(x)Ã—\frac{1}{\sqrt{M}}$ åœ¨æ ·æœ¬å¯†åº¦æ¯”è¾ƒé«˜çš„åŒºåŸŸçš„ä½“ç§¯å°±ä¼šæ¯”è¾ƒå°ï¼Œè€Œåœ¨å¯†åº¦ä½çš„åŒºåŸŸçš„ä½“ç§¯åˆ™ä¼šè‡ªåŠ¨å¢å¤§ï¼Œè¿™æ ·å°±èƒ½å¤Ÿè¾ƒå¥½çš„å…¼é¡¾åœ¨é«˜å¯†åº¦åŒºåŸŸä¼°è®¡çš„åˆ†è¾¨ç‡å’Œåœ¨ä½å¯†åº¦åŒºåŸŸä¼°è®¡çš„è¿ç»­æ€§ã€‚ Parzençª—æ³•åˆç§°æ ¸å¯†åº¦ä¼°è®¡ã€‚ åŸç†æˆ‘ä»¬æš‚æ—¶å‡è®¾å¾…ä¼°è®¡ç‚¹$x$çš„é™„è¿‘åŒºé—´$R$ä¸ºä¸€ä¸ª$N$ç»´çš„è¶…ç«‹æ–¹ä½“ï¼Œç”¨$h$è¡¨ç¤ºè¾¹çš„é•¿åº¦ï¼Œé‚£ä¹ˆ V_i = h^Nå³å®šä¹‰çª—å‡½æ•°$\varphi(Â·)$ï¼Œè¡¨ç¤ºè½å…¥ä»¥$x$ä¸ºä¸­å¿ƒçš„è¶…ç«‹æ–¹ä½“çš„åŒºåŸŸçš„ç‚¹ \varphi \left(\frac{x_i-x}{h}\right) = \begin{cases} 1 & \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2}, n=1,...,N \\ 0 & otherwise \end{cases} \tag{9} \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2} å³ (x_i-x)_n \leq \frac{h}{2}è¿™é‡Œçš„$h$èµ·åˆ°å•ä½åŒ–çš„ä½œç”¨ï¼Œä¾¿äºæ¨å¹¿ é‚£ä¹ˆè½å…¥ä»¥$x$ä¸ºä¸­å¿ƒçš„è¶…ç«‹æ–¹ä½“çš„åŒºåŸŸçš„ç‚¹çš„ä¸ªæ•°ä¸º M_i = \sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right) \tag{10}ä»£å…¥$p(x) \approx \frac{M_i/M}{V_i}$ï¼Œæˆ‘ä»¬å¾—åˆ° p(x) \approx \frac{\sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right)/M}{V_i} = \frac{1}{M} \sum_{i=1}^M \frac{1}{V_i} \varphi \left(\frac{x_i-x}{h}\right) \tag{11}æˆ‘ä»¬å®šä¹‰æ ¸å‡½æ•°(æˆ–ç§°â€œçª—å‡½æ•°â€) \kappa(z) = \frac{1}{V_i} \varphi(z) \tag{12}æ ¸å‡½æ•°ååº”äº†ä¸€ä¸ªè§‚æµ‹æ ·æœ¬$x_i$å¯¹åœ¨$x$å¤„çš„æ¦‚ç‡å¯†åº¦ä¼°è®¡çš„è´¡çŒ®ï¼Œä¸æ ·æœ¬$x_i$å’Œ$x$çš„è·ç¦»æœ‰å…³ã€‚è€Œæ¦‚ç‡å¯†åº¦ä¼°è®¡å°±æ˜¯åœ¨è¿™ä¸€ç‚¹ä¸ŠæŠŠæ‰€æœ‰è§‚æµ‹æ ·æœ¬çš„è´¡çŒ®è¿›è¡Œå¹³å‡ p(x) \approx \frac{1}{M} \sum_{i=1}^M \kappa\left(\frac{x_i-x}{h}\right) \tag{13}æ ¸å‡½æ•°æ ¸å‡½æ•°åº”æ»¡è¶³æ¦‚ç‡å¯†åº¦çš„è¦æ±‚ï¼Œå³ \kappa(z) \geq 0 \And \int \kappa(z)dz = 1é€šå¸¸æœ‰ä»¥ä¸‹å‡ ç§æ ¸å‡½æ•° å‡åŒ€æ ¸ \kappa(z) = \begin{cases} 1 & |z_n| \leq \frac{1}{2}, n=1,...,N \\ 0 & otherwise \end{cases} é«˜æ–¯æ ¸(æ­£æ€æ ¸) é«˜æ–¯æ ¸æ˜¯å°†çª—æ”¾å¤§åˆ°æ•´ä¸ªç©ºé—´ï¼Œå„ä¸ªè§‚æµ‹æ ·æœ¬$x_i$å¯¹å¾…è§‚æµ‹ç‚¹$x$çš„åŠ æƒå’Œ(è¶Šè¿œæƒå€¼è¶Šå°)ã€‚ \kappa(z) = \frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}} \exp \left(-\frac{1}{2} (z - \mu)^T \Sigma^{-1} (z - \mu)\right) è¶…çƒçª— \kappa(z) = \begin{cases} V^{-1} & ||z|| \leq 1 \\ 0 & otherwise \end{cases} $z=\frac{x_i-x}{h}$ï¼Œæ•…$||z||\leq 1$å³$||x_i-x||^2\leq h^2$æ­¤æ—¶$h$è¡¨ç¤ºè¶…çƒä½“çš„åŠå¾„ sklearnsklearn.neighbors.KernelDensity â€” scikit-learn 0.19.0 documentation - ApacheCN123456789101112131415161718&gt;&gt;&gt; from sklearn.neighbors import KernelDensity&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])&gt;&gt;&gt; kde = KernelDensity(kernel=&apos;gaussian&apos;, bandwidth=0.2).fit(X)&gt;&gt;&gt; kde.score_samples(X)array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698, -0.41076071])&gt;&gt;&gt; kde.sample(10)array([[ 1.80042291, 1.1030739 ], [ 0.87299669, 1.0762352 ], [-2.40180586, -1.19554374], [-1.97985919, -1.19361193], [-2.95866231, -2.1972637 ], [-1.12739556, -0.80851063], [ 1.03756706, 1.24855099], [ 1.21729703, 1.02345815], [-2.11816867, -1.0486257 ], [-1.04875537, -0.89928711]]) ä»£ç å…·ä½“ä»£ç è§@Github: Non-parametric Estmation å®šä¹‰æ ¸å‡½æ•°å¦‚ä¸‹1234# é«˜æ–¯æ ¸gaussian = lambda z: np.exp(-0.5*(np.linalg.norm(z)**2)) / np.sqrt(2*np.pi)# å‡åŒ€æ ¸square = lambda z: 1 if (np.linalg.norm(z) &lt;= 0.5) else 0 å¯†åº¦ä¼°è®¡å‡½æ•°å¦‚ä¸‹ï¼Œéœ€è¦å¯¹è¿ç»­èŒƒå›´å†…çš„å„ä¸ªç‚¹ï¼Œå³$x \in [min(X), max(X)]$è¿›è¡Œä¼°è®¡è·å¾—pï¼Œä½œå›¾æ˜¾ç¤º$x-p$å³å¯123456789101112131415161718192021def parzenEstimate(X, kernel, h, n_num=50): &quot;&quot;&quot; æ ¸å‚æ•°ä¼°è®¡ Args: X: &#123;ndarray(n_samples,)&#125; kernel: &#123;function&#125; å¯è°ƒç”¨çš„æ ¸å‡½æ•° h: &#123;float&#125; æ ¸å‡½æ•°çš„å‚æ•° Returns: p: &#123;ndarray(n_num,)&#125; Notes: - ä¸€ç»´ï¼Œæ•…`V_i = h` - p(x) = \frac&#123;1&#125;&#123;M&#125; \sum_&#123;i=1&#125;^M \kappa \left( \frac&#123;x_i - x&#125;&#123;h&#125; \right) &quot;&quot;&quot; x = np.linspace(np.min(X), np.max(X), num=n_num) p = np.zeros(shape=(x.shape[0],)) z = lambda x, x_i, h: (x - x_i) / h V_i = h; n_samples = X.shape[0] for idx in range(x.shape[0]): for i in range(X.shape[0]): p[idx] += kernel(z(x[idx], X[i], h)) / V_i p[idx] /= n_samples return p å‡åŒ€æ ¸ $h=0.5$ $h=0.8$ $h=1.0$ $h=2.0$ é«˜æ–¯æ ¸ $h=0.5$ $h=0.8$ $h=1.0$ $h=2.0$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Parameter Estimation]]></title>
    <url>%2F2018%2F11%2F19%2FParameter-Estimation%2F</url>
    <content type="text"><![CDATA[è´å¶æ–¯å­¦æ´¾ä¸é¢‘ç‡å­¦æ´¾æœ‰ä½•ä¸åŒï¼Ÿ - ä»»å¤çš„å›ç­” - çŸ¥ä¹ å¼•è¨€å‚æ•°ä¼°è®¡(parameter estimation)ï¼Œç»Ÿè®¡æ¨æ–­çš„ä¸€ç§ã€‚æ ¹æ®ä»æ€»ä½“ä¸­æŠ½å–çš„éšæœºæ ·ä¹¦ï¼æ¥ä¼°è®¡æ€»ä½“åˆ†å¸ƒä¸­æœªçŸ¥å‚æ•°çš„è¿‡ç¨‹ã€‚ä¸»è¦ä»‹ç»æœ€å¤§ä¼¼ç„¶ä¼°è®¡(MLE: Maximum Likelihood Estimation)ï¼Œæœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡(MAP: Maximum A Posteriori Estimation)ï¼Œè´å¶æ–¯ä¼°è®¡(Bayesian Estimation)ã€‚ è§£é‡Šä¸€ä¸‹â€œä¼¼ç„¶å‡½æ•°â€å’Œâ€œåéªŒæ¦‚ç‡â€ï¼Œåœ¨è´å¶æ–¯å†³ç­–ä¸€èŠ‚ï¼Œç»™å‡ºå®šä¹‰å¦‚ä¸‹ P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}ä¸Šå¼ä¸­$ k=1,â€¦,K $ï¼Œå„éƒ¨åˆ†å®šä¹‰å¦‚ä¸‹$P(c_k|x)$â€”â€”åéªŒæ¦‚ç‡(posteriori probability)$P(c_k)$â€”â€”å…ˆéªŒæ¦‚ç‡(priori probability)$p(x|c_k)$â€”â€”$c_k$å…³äº$x$çš„ä¼¼ç„¶å‡½æ•°(likelihood)$p(x)$â€”â€”è¯æ®å› å­(evidence) å¼•ä¾‹ä»¥æœ€ç»å…¸çš„æ·ç¡¬å¸å®éªŒä¸ºä¾‹ï¼Œå‡è®¾æœ‰ä¸€æšç¡¬å¸ï¼ŒæŠ•æ·ä¸€æ¬¡å‡ºç°æ­£é¢è®°$â€1â€$ï¼ŒæŠ•æ·$10$æ¬¡çš„å®éªŒç»“æœå¦‚ä¸‹ \{ 0ï¼Œ 1ï¼Œ 1ï¼Œ 1ï¼Œ 1ï¼Œ 0ï¼Œ 1ï¼Œ 1ï¼Œ 1ï¼Œ0 \}è®°ç¡¬å¸æŠ•æ·ç»“æœä¸ºéšæœºå˜é‡$X$ï¼Œä¸”$ x \in \{0, 1\}$ï¼Œç¡¬å¸æŠ•æ·ä¸€æ¬¡æœä»äºŒé¡¹åˆ†å¸ƒï¼Œä¼°è®¡äºŒé¡¹åˆ†å¸ƒçš„å‚æ•°$\theta$ æœ€å¤§ä¼¼ç„¶ä¼°è®¡(MLE)ä¼¼ç„¶å‡½æ•° Likelihood function - Wikipedia ç¦»æ•£å‹ L(x | \theta) = p_{\theta}(x)=P_{\theta}(X = x) è¿ç»­å‹ L(x | \theta) = f_{\theta}(x) å¾ˆå¤šäººèƒ½è®²å‡ºä¸€å¤§å †å“²å­¦ç†è®ºæ¥é˜æ˜è¿™ä¸€å¯¹åŒºåˆ«ã€‚ä½†æˆ‘è§‰å¾—ï¼Œä»å·¥ç¨‹å¸ˆè§’åº¦æ¥è®²ï¼Œè¿™æ ·ç†è§£å°±å¤Ÿäº†:é¢‘ç‡ $vs$ è´å¶æ–¯ = $P(X; w)$ $vs$ $P(X|w)$ æˆ– $P(X,w)$ ä½œè€…ï¼šè®¸é“-å·¡æ´‹èˆ°ç§‘æŠ€é“¾æ¥ï¼šhttps://www.zhihu.com/question/20587681/answer/122348889æ¥æºï¼šçŸ¥ä¹è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚ æ¨¡å‹æœ‰æ•°æ®é›†$D = \{x_1, x_2, â€¦, x_N\}$ï¼ŒæŒ‰$c$ä¸ªç±»åˆ«åˆ†æˆ$\{D_1, D_2, â€¦, D_C\}$ï¼Œå„ä¸ªç±»åˆ«æœä»çš„æ¦‚ç‡åˆ†å¸ƒå¯†åº¦å‡½æ•°æ¨¡å‹å·²ç»™å‡ºï¼Œä¼°è®¡å‚æ•°$\hat{\Theta} = \{\hat{\theta}_{c_1}, \hat{\theta}_{c_2}, â€¦, \hat{\theta}_{c_C}\} $ å‡å®š ç±»åˆ«é—´ç‹¬ç«‹ï¼Œä¸”å„è‡ªæœä»æ¦‚ç‡åˆ†å¸ƒå¯†åº¦å‡½æ•°ä¸º$p(x|c_j)$ å„ç±»åˆ«çš„æ¦‚ç‡å¯†åº¦$p(x|c_j)$ä»¥å‚æ•°$\theta_{c_j}$ç¡®å®šï¼Œå³$p(x|c_j; \theta_{c_j})$ æ•…ä¼¼ç„¶å‡½æ•°ä¸º L(D | \Theta) = P(x_1, x_2, ..., x_N | \Theta) = \prod_{i=1}^N p(x_i | \theta_{x_i \in c_j}) ç†è§£ä¸ºï¼Œåœ¨å‚æ•°$\Theta$ä¸ºä½•å€¼çš„æ¡ä»¶ä¸‹ï¼Œå®éªŒç»“æœå‡ºç°æ•°æ®é›†$D$çš„æ¦‚ç‡æœ€å¤§ æ±‚å–å…¶æå¤§å€¼å¯¹åº”çš„å‚æ•°å³å¯ ä¸€èˆ¬å–å¯¹æ•°ä¼¼ç„¶å‡½æ•°\log L(D | \Theta) = \sum_{i=1}^N \log p(x_i | \theta_{x_i \in c_j}) æå¤§å€¼å³å¯¹åº”æ¢¯åº¦ä¸º$\vec{0}$çš„ä½ç½®ï¼Œå³ âˆ‡_\Theta \log L(D | \Theta) = \vec{0} \Rightarrow \hat{\Theta} Some comments about ML ML estimation is usually simpler than alternative methods. Has good convergence properties as the number of training samples increases. If the model chosen for p(x|Î¸) is correct, and independence assumptions among variables are true, ML will give very good results. If the model is wrong, ML will give poor results. â€”â€” Zhao Haitao. Maximum Likelihood and Bayes Estimation ä¾‹ï¼šæ­£æ€åˆ†å¸ƒçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ•°æ®é›†(å•ç±»åˆ«)æœä»é«˜æ–¯åˆ†å¸ƒ$N(\mu, \sigma^2)$æ—¶çš„çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ P(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} L(D | \mu, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} =\left( \frac{1}{\sqrt{2\pi} \sigma } \right)^N \prod_{i=1}^N e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}å–å¯¹æ•°ä¼¼ç„¶ \log L(D | \mu, \sigma^2) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^21. å‚æ•°$\mu$çš„ä¼°è®¡ \frac{âˆ‚}{âˆ‚\mu} L(D | \mu, \sigma^2) = \frac{1}{\sigma^2} (\sum_{i=1}^N x_i - N\mu) = 0 \Rightarrow \hat{\mu} = \frac{1}{N} \sum_{i=1}^N x_i2. å‚æ•°$\sigma^2$çš„ä¼°è®¡ \frac{âˆ‚}{âˆ‚\sigma^2} \log L(D | \mu, \sigma^2) = - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 = 0 \Rightarrow \hat{\sigma^2} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2 å‚æ•°$\hat{\mu}, \hat{\sigma}^2$çš„å€¼ä¸æ ·æœ¬å‡å€¼å’Œæ ·æœ¬æ–¹å·®ç›¸ç­‰ æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡(MAP) æ¨¡å‹æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯æ±‚å‚æ•°$\theta$, ä½¿ä¼¼ç„¶å‡½æ•°$P(D | \theta)$æœ€å¤§ï¼Œæœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡åˆ™æ˜¯æ±‚$\theta$ä½¿$P(\theta | D)$æœ€å¤§ ç†è§£ä¸ºï¼Œåœ¨å·²å‡ºç°çš„å®éªŒæ ·æœ¬$D$ä¸Šï¼Œå‚æ•°$\theta$å–ä½•å€¼çš„æ¦‚ç‡æœ€å¤§ ä¸”æ³¨æ„åˆ° P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}æ•…$MAP$ä¸ä»…ä»…ä½¿ä¼¼ç„¶å‡½æ•°$P(D | \theta)$æœ€å¤§ï¼Œè€Œä¸”ä½¿$P(\theta)$æœ€å¤§ï¼Œå³ \theta = argmax L(\theta | D) L(\theta | D) = P(\theta) P(D | \theta) = P(\theta) \prod_{i=1}^N p(x_i | \theta) æ¯”$ML$å¤šäº†ä¸€é¡¹$P(\theta)$ å–å¯¹æ•°å \log L(\theta | D) = \sum_{i=1}^N \log p(x_i | \theta) + \log P(\theta) æ±‚å–æå¤§å€¼ âˆ‡_\theta L(\theta | D) = 0 \Rightarrow \hat{\theta} $MAP$å’Œ$MLE$çš„åŒºåˆ«ï¼š$MAP$å…è®¸æˆ‘ä»¬æŠŠå…ˆéªŒçŸ¥è¯†åŠ å…¥åˆ°ä¼°è®¡æ¨¡å‹ä¸­ï¼Œè¿™åœ¨æ ·æœ¬å¾ˆå°‘çš„æ—¶å€™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œå› ä¸ºæ ·æœ¬å¾ˆå°‘çš„æ—¶å€™æˆ‘ä»¬çš„è§‚æµ‹ç»“æœå¾ˆå¯èƒ½å‡ºç°åå·®ï¼Œæ­¤æ—¶å…ˆéªŒçŸ¥è¯†ä¼šæŠŠä¼°è®¡çš„ç»“æœâ€œæ‹‰â€å‘å…ˆéªŒï¼Œå®é™…çš„é¢„ä¼°ç»“æœå°†ä¼šåœ¨å…ˆéªŒç»“æœçš„ä¸¤ä¾§å½¢æˆä¸€ä¸ªé¡¶å³°ã€‚é€šè¿‡è°ƒèŠ‚å…ˆéªŒåˆ†å¸ƒçš„å‚æ•°ï¼Œæ¯”å¦‚betaåˆ†å¸ƒçš„$\alpha, \beta$ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è°ƒèŠ‚æŠŠä¼°è®¡çš„ç»“æœâ€œæ‹‰â€å‘å…ˆéªŒçš„å¹…åº¦ï¼Œ$\alpha, \beta$è¶Šå¤§ï¼Œè¿™ä¸ªé¡¶å³°è¶Šå°–é”ã€‚è¿™æ ·çš„å‚æ•°ï¼Œæˆ‘ä»¬å«åšé¢„ä¼°æ¨¡å‹çš„â€œè¶…å‚æ•°â€ã€‚æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡(MAP)ï¼Œè´å¶æ–¯ä¼°è®¡ - æé‘«o_O - CSDNåšå®¢ ä¾‹ï¼šæ­£æ€åˆ†å¸ƒçš„æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡æ•°æ®é›†(å•ç±»åˆ«)æœä»é«˜æ–¯åˆ†å¸ƒ$N(\mu, \sigma^2)$æ—¶çš„æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡ p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} \log p(x_i | \mu, \sigma^2) = - \frac{1}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (x_i - \mu)^2 1. å‚æ•°$\mu$çš„ä¼°è®¡ç»™å®šå…ˆéªŒæ¡ä»¶ï¼š$\mu$æœä»æ­£æ€åˆ†å¸ƒ$N(\mu_0, \sigma_{\mu_0}^2)$ï¼Œå³ p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}} \log p(\mu) = - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2 åˆ™ \log L(\mu, \sigma^2 | D) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2åˆ™ \frac{âˆ‚}{âˆ‚\mu} \log L(\mu, \sigma^2 | D) = \frac{1}{\sigma^2} \sum_{i=0}^N (x_i - \mu) - \frac{1}{\sigma_{\mu_0}^2} (\mu - \mu_0) = 0 \Rightarrow \hat{\mu} = \frac{\mu_0 \sigma^2 + \sigma_{\mu_0}^2 \sum_{i=0}^N x_i} {\sigma^2 + N \sigma_{\mu_0}^2} = \frac{\mu_0 + \frac{\sigma_{\mu_0}^2}{\sigma^2} \sum_{i=0}^N x_i} {1 + \frac{\sigma_{\mu_0}^2}{\sigma^2} N }2. å‚æ•°$\sigma^2$çš„ä¼°è®¡ç»™å®šå…ˆéªŒæ¡ä»¶ï¼š$\sigma^2$æœä»æ­£æ€åˆ†å¸ƒ$N(\sigma_0^2, \sigma_{\sigma_0^2}^2)$ï¼Œå³ p(\sigma^2) = \frac{1}{\sqrt{2\pi} \sigma_{\sigma_0^2}} e^ {-\frac{(\sigma^2- \sigma_0^2)^2}{2 \sigma_{\sigma_0^2} ^2}} \log p(\sigma^2) = - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2 åˆ™ \log L(\mu, \sigma^2 | D) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2åˆ™ \frac{âˆ‚}{âˆ‚\sigma^2} \log L(\mu, \sigma^2 | D) = - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2\sigma_{\sigma_0}^2} \frac{\sigma - \sigma_0}{\sigma} \Rightarrow \hat{\sigma^2}(ç•¥) \frac{âˆ‚}{âˆ‚\sigma^2}(\sigma - \sigma_0)^2 = 2(\sigma - \sigma_0) \frac{âˆ‚}{âˆ‚\sigma^2} (\sigma - \sigma_0) = \frac{\sigma - \sigma_0}{\sigma} è´å¶æ–¯ä¼°è®¡æ¨¡å‹ p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)} = a Â· p(\theta) \prod_{i=1}^N p(x_i | \theta)å…¶ä¸­$a$æ˜¯ä½¿ \int p(\theta | D) = 1åˆ©ç”¨â€œè´¨å¿ƒå…¬å¼â€æ±‚è§£è´å¶æ–¯çš„ç‚¹ä¼°è®¡ Î¸_{Bayes} = \int Î¸Â·p(Î¸|D) d Î¸ä¾‹ï¼šæ­£æ€åˆ†å¸ƒçš„è´å¶æ–¯ä¼°è®¡æ•°æ®é›†(å•ç±»åˆ«)æœä»é«˜æ–¯åˆ†å¸ƒ$N(\mu, \sigma^2)$æ—¶çš„è´å¶æ–¯ä¼°è®¡ p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}å‚æ•°$\mu$çš„ä¼°è®¡ç»™å®šå…ˆéªŒæ¡ä»¶ï¼š$\mu$æœä»æ­£æ€åˆ†å¸ƒ$N(\mu_0, \sigma_{\mu_0}^2)$ï¼Œå³ p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}åˆ™ P(\mu | D) = a Â· p(\mu) \prod_{i=1}^N p(x_i | \mu) = a Â· \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}} \prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} = a Â· \left( \frac{1}{\sqrt{2\pi}} \right)^{N + 1} \frac{1}{\sigma_{\mu_0} \sigma^N} e^ { -\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2} -\sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2} }æ˜“è¯ æˆ‘å·²ç»æƒ³åˆ°äº†ä¸€ä¸ªç»å¦™çš„è¯æ˜,ä½†æ˜¯è¿™å°ç”µè„‘çš„ç¡¬ç›˜å¤ªå°äº†,å†™ä¸ä¸‹ã€‚ p(\mu | D) = \frac{1}{\sqrt{2\pi}\sigma_N} e^ {-\frac{(\mu - \mu_N)^2}{2\sigma_N^2}}å…¶ä¸­ \mu_N = \frac{N \sigma_0^2} {N \sigma_0^2 + \sigma^2} \frac{1}{N} \sum_{i=1}^N x_i +\frac{\sigma^2}{N \sigma_0^2 + \sigma^2} \mu_0 \sigma_N^2 = \frac{\sigma_0^2 \sigma^2} {N \sigma_0^2 + \sigma^2} ä¸$MLE$ï¼Œ$MAP$çš„åŒºåˆ« ç›¸æ¯”è¾ƒ$MLE$ä¸$MAP$çš„ç‚¹ä¼°è®¡ï¼Œè´å¶æ–¯ä¼°è®¡å¾—åˆ°çš„ç»“æœæ˜¯å‚æ•°$\theta$çš„å¯†åº¦å‡½æ•°$p(\theta | D)$ æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡ä¸ºæ±‚å–å¯¹åº”æœ€å¤§åéªŒæ¦‚ç‡çš„ç‚¹ \theta = argmax_\theta p(\theta | D) è´å¶æ–¯ä¼°è®¡ä¸ºæ±‚å–æ•´ä¸ªå–å€¼èŒƒå›´çš„æ¦‚ç‡å¯†åº¦$p(\theta | D)$ï¼Œæ—¢ç„¶å¦‚æ­¤ï¼Œå¿…æœ‰ \int p(\theta | D) d\theta = 1 ç»Ÿè®¡å­¦ä¹ æ–¹æ³•å­¦ä¹ ç¬”è®°ï¼ˆä¸€ï¼‰â€”æå¤§ä¼¼ç„¶ä¼°è®¡ä¸è´å¶æ–¯ä¼°è®¡åŸç†åŠåŒºåˆ« - YJ-20 - åšå®¢å›­ p(\theta | D) = \frac {p(\theta) \prod_{i=1}^N p(x_i | \theta)} {\int_\theta p(\theta) \prod_{i=1}^N p(x_i | \theta) d\theta}ç”±äº$\theta$æ˜¯æ»¡è¶³ä¸€å®šæ¦‚ç‡åˆ†å¸ƒçš„å˜é‡ï¼Œæ‰€ä»¥åœ¨è®¡ç®—çš„æ—¶å€™éœ€è¦å°†è€ƒè™‘æ‰€æœ‰$\theta$å–å€¼çš„æƒ…å†µï¼Œåœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸å¯é¿å…åœ°é«˜å¤æ‚åº¦ã€‚æ‰€ä»¥è®¡ç®—æ—¶å¹¶ä¸æŠŠæ‰€æœ‰åœ°åéªŒæ¦‚ç‡$p(\theta | D)$éƒ½æ‰¾å‡ºæ¥ï¼Œè€Œæ˜¯é‡‡ç”¨ç±»ä¼¼äºæå¤§ä¼¼ç„¶ä¼°è®¡åœ°æ€æƒ³ï¼Œæ¥æå¤§åŒ–åéªŒæ¦‚ç‡ï¼Œå¾—åˆ°è¿™ç§æœ‰æ•ˆçš„å«åš$MAP$ å¼•ä¾‹çš„æ±‚è§£å·²çŸ¥ç¡¬å¸æŠ•æ·ç»“æœæœä»$Bernoulli$åˆ†å¸ƒ X 0 1 P 1-Î¸ Î¸ æˆ–è€… P(X_i) = \theta ^{X_i} (1 - \theta) ^{1 - X_i}æœ€å¤§ä¼¼ç„¶ä¼°è®¡å®éªŒç»“æœä¸­æ­£é¢å‡ºç°$7$æ¬¡ï¼Œåé¢å‡ºç°$3$æ¬¡ï¼Œä¼¼ç„¶å‡½æ•°ä¸º L(\theta) = \prod_{i=1}^{10} \theta ^{X_i} (1 - \theta) ^{1 - X_i} = \theta ^7 (1 - \theta) ^3å–å¯¹æ•°ä¼¼ç„¶å‡½æ•°å¹¶æ±‚æå¤§å€¼ \log L(\theta) = 7 \log \theta + 3 \log (1 - \theta)ä»¤ \frac{âˆ‚}{âˆ‚ \theta} \log L(\theta) = \frac{7}{\theta} - \frac{3}{1-\theta} = 0è§£å¾— \theta = 0.7å³ç¡¬å¸æœä»$B(1, 0.7)$çš„æ¦‚ç‡åˆ†å¸ƒ åšå‡º$L(\theta)$å›¾åƒéªŒè¯ï¼Œå¦‚ä¸‹ æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡ç»™å®šå…ˆéªŒæ¡ä»¶ \theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)åˆ™æœ€å¤§åŒ– L(\theta | D) = \theta ^7 (1 - \theta) ^3 Â· \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}}å–å¯¹æ•° \log L(\theta | D) = 7 \log \theta + 3 \log (1 - \theta) - \frac{1}{2} \log(2\pi \sigma_{\theta_0}^2) - \frac{1}{2\sigma_{\theta_0}^2} (\theta - \theta_0)^2æ±‚å–æå¤§å€¼ç‚¹ \frac{âˆ‚}{âˆ‚\theta} \log L(\theta | D) = \frac {7}{\theta} - \frac{3}{1-\theta} - \frac{\theta - \theta_0}{\sigma_{\theta_0}^2} = 0å¾—åˆ° \theta^3 - (\theta_0 + 1) \theta^2 + (\theta_0 - 10\sigma_{\theta_0}^2) \theta + 7\sigma_{\theta_0}^2 = 0ä»¥ä¸‹ä¸ºé€‰å–ä¸åŒå…ˆéªŒæ¡ä»¶æ—¶çš„$L(\theta | D)$å›¾åƒï¼Œç”¨äºå¯¹æ¯” ç¬¬ä¸€å¼ å›¾ä¸ºæå¤§ä¼¼ç„¶ä¼°è®¡$L(D|\theta)$ ç¬¬äºŒå¼ å›¾ä¸ºå…ˆéªŒæ¦‚ç‡å¯†åº¦å‡½æ•°$P(\theta)$ ç¬¬ä¸‰å¼ å›¾ä¸ºæœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡$L(\theta | D)$ï¼Œ$\hat{\theta}$ç”±æŸ¥è¡¨æ³•æ±‚è§£ä»£ç è§ä»“åº“ $\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.42$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.56$ $\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.70$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\Rightarrow$ $\hat{\theta} = 0.50$ $\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$ $\Rightarrow$ $\hat{\theta} = 0.70$ ç»“è®º ç”±å›¾$1, 2, 3$ï¼Œå¯ä»¥çœ‹åˆ°å½“$\theta_0$åç§»$0.7$æ—¶ï¼Œ$MAP$ç»“æœä¹Ÿç›¸åº”åç§»ï¼› ç”±å›¾$2, 4, 5$ï¼Œå¯ä»¥çœ‹åˆ°å½“$\sigma_{\theta_0}^2$è¶Šå°ï¼Œå³è¶Šç¡®å®šå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒæ—¶ï¼Œ$MAP$ç»“æœä¹Ÿè¶Šè¶‹å‘äºå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒã€‚ è´å¶æ–¯ä¼°è®¡å…ˆéªŒæ¡ä»¶ä¸ºæ­£æ€åˆ†å¸ƒ \theta \thicksim N(\theta_0, \sigma_{\theta_0}^2) p(\theta | D) = a Â· p(\theta) \prod_{i=1}^N p(x_i | \theta) = a Â· \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}} Â· \theta ^7 (1 - \theta) ^3 å‚æ•°$a$ä½¿ç”¨scipy.integrate.quadæ±‚è§£ é€‰å–ä¸åŒå…ˆéªŒæ¡ä»¶æ—¶çš„$L(\theta | D)$å›¾åƒï¼Œç”¨äºå¯¹æ¯” $\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Clustering]]></title>
    <url>%2F2018%2F11%2F16%2FClustering%2F</url>
    <content type="text"><![CDATA[å‰è¨€è¿™æ˜¯ç¬¬ä¸€ç¯‡å…³äºæ— ç›‘ç£å­¦ä¹ çš„åšæ–‡ï¼Œæ— ç›‘ç£çš„å­¦ä¹ åˆ™ä¸æ˜¯å°è¯•é¢„æµ‹ä»»ä½•ä¸œè¥¿ï¼Œè€Œæ˜¯å¯»æ‰¾æ•°æ®ä¸­çš„ç‰¹å¾ï¼Œåœ¨æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œæœ‰ä¸€ä¸ªé‡è¦çš„æ–¹æ³•ç§°ä¸ºèšç±»ï¼Œæ˜¯æŠŠå…·æœ‰ç›¸åŒç‰¹å¾çš„æ•°æ®èšé›†åœ¨ä¸€ç»„ã€‚ åŸºç¡€çŸ¥è¯†è·ç¦»åº¦é‡æ–¹æ³•æœºå™¨å­¦ä¹ ä¸­è·ç¦»åº¦é‡æ–¹æ³•æœ‰å¾ˆå¤šï¼Œä»¥ä¸‹ç®€å•ä»‹ç»å‡ ç§ã€‚ æœºå™¨å­¦ä¹ å¸¸ç”¨çš„è·ç¦»åº¦é‡æ–¹æ³• - taotiezhengfengçš„åšå®¢ - CSDNåšå®¢ç®—æ³•ä¸­çš„å„ç§è·ç¦»ï¼ˆæ¬§å¼è·ç¦»ï¼Œé©¬æ°è·ç¦»ï¼Œé—µå¯å¤«æ–¯åŸºè·ç¦»â€¦â€¦ï¼‰ - å•Šå“¦123çš„åšå®¢ - CSDNåšå®¢ å®šä¹‰ä¸¤ä¸ª$n$ç»´å‘é‡ x = [x_1, x_2, ..., x_n]^T y = [y_1, y_2, ..., y_n]^T æ›¼å“ˆé¡¿è·ç¦»(Manhattan Distance) d = || x - y ||_1 = \sum_i |x_i - y_i| æ¬§æ°è·ç¦»(Euclidean Distance) d = || x - y ||_2 = \sqrt{\sum_i (x_i - y_i)^2} é—½å¯å¤«æ–¯åŸºè·ç¦»(Minkowski Distance) d = || x - y ||_p = \left(\sum_i | x_i - y_i |^{p} \right)^{\frac{1}{p}} å½“$p$å–$1$æ—¶ä¸ºæ›¼å“ˆé¡¿è·ç¦»ï¼Œå–$2$æ—¶ä¸ºæ¬§å¼è·ç¦»ã€‚ ä½™å¼¦è·ç¦»(Cosine) d = \frac{x^T y}{||x||_2 ||y||_2} = \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}} çªç„¶æƒ³åˆ°ä¸ºä»€ä¹ˆå‘é‡çš„å¤¹è§’ä½™å¼¦æ˜¯æ€ä¹ˆæ¥çš„ï¼Œé«˜ä¸­å­¦ä¹ ä¸€ç›´èƒŒçš„å…¬å¼ï¼Œç°åœ¨ç»™ä¸€ä¸‹è¯æ˜ã€‚è¯æ˜ï¼šå‘é‡çš„å¤¹è§’å…¬å¼ ä»ä½™å¼¦å®šç†(ä½™å¼¦å®šç†ç”¨å‡ ä½•å³å¯)å‡ºå‘ï¼Œæœ‰ \cos \theta = \frac{a^2+b^2-c^2}{2ab}å…¶ä¸­ ||\vec{a}|| = \sqrt{x_1^2 + y_1^2} ||\vec{b}|| = \sqrt{x_2^2 + y_2^2} ||\vec{c}|| = \sqrt{(x_1 - x_2)^2 + (x_2 - y_2)^2}æ•… \cos \theta = \frac {(\sqrt{x_1^2 + y_1^2})^2 + (\sqrt{x_2^2 + y_2^2})^2 - (\sqrt{(x_1 - x_2)^2 + (x_2 - y_2))^2}} {2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}} = \frac {x_1 x_2 + y_1 y_2} {\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}} = \frac{a^T b}{||a||Â·||b||} hard vs. soft clustering ç¡¬èšç±»(hard clustering) è®¡ç®—çš„æ˜¯ä¸€ä¸ªç¡¬åˆ†é…(hard ssignment)è¿‡ç¨‹,å³æ¯ä¸ªæ ·æœ¬ä»…ä»…å±äºä¸€ä¸ªç°‡ã€‚ è½¯èšç±»(soft clustering) åˆ†é…è¿‡ç¨‹æ˜¯è½¯çš„ï¼Œå³ä¸€ä¸ªæ ·æœ¬çš„åˆ†é…ç»“æœæ˜¯åœ¨æ‰€æœ‰ç°‡ä¸Šçš„ä¸€ä¸ªåˆ†å¸ƒï¼Œåœ¨è½¯åˆ†é…ç»“æœä¸­ï¼Œä¸€ä¸ªæ ·æœ¬å¯èƒ½å¯¹å¤šä¸ªç°‡éƒ½å…·æœ‰éš¶å±åº¦ã€‚ èšç±»æ–¹æ³•çš„åˆ†ç±» åˆ’åˆ†æ–¹æ³• K-meansï¼ŒK-medoidsï¼ŒGMMç­‰ã€‚ å±‚æ¬¡æ–¹æ³• AGNESï¼ŒDIANAï¼ŒBIRCHï¼ŒCUREå’ŒCURE-NSç­‰ã€‚ åŸºäºå¯†åº¦çš„æ–¹æ³• DBSCANï¼ŒOPTICSï¼ŒDENCLUEç­‰ã€‚ å…¶ä»– å¦‚STINGç­‰ã€‚ å¸¸ç”¨èšç±»æ–¹æ³•Kå‡å€¼(K-means)æ˜¯æœ€ä¸ºç»å…¸çš„åŸºäºåˆ’åˆ†çš„èšç±»æ–¹æ³•ï¼Œæ˜¯åå¤§ç»å…¸æ•°æ®æŒ–æ˜ç®—æ³•ä¹‹ä¸€ï¼Œé€šå¸¸ç”¨äºå¯»æ‰¾æ¬¡ä¼˜è§£ï¼Œå†é€šè¿‡å…¶ä»–ç®—æ³•(å¦‚GMM)å¯»æ‰¾æ›´ä¼˜çš„èšç±»ç»“æœã€‚ åŸç†ç»™å®š$N$ç»´æ•°æ®é›† X = [x^{(1)}, x^{(2)}, ..., x^{(M)}]æŒ‡å®šç±»åˆ«æ•°$K$ä¸åˆå§‹ä¸­å¿ƒç‚¹$\mu^{(0)}$ï¼Œå°†æ ·æœ¬åˆ’åˆ†åˆ°ä¸­å¿ƒç‚¹è·ç¦»å…¶æœ€è¿‘çš„ç°‡ä¸­ï¼Œå†æ ¹æ®æœ¬æ¬¡åˆ’åˆ†æ›´æ–°å„ç°‡çš„ä¸­å¿ƒ$\mu^{(t)}$ï¼Œå¦‚æ­¤è¿­ä»£ç›´è‡³å¾—åˆ°æœ€å¥½çš„èšç±»ç»“æœã€‚é¢„æµ‹æµ‹è¯•æ ·æœ¬æ—¶ï¼Œå°†å…¶åˆ’åˆ†åˆ°ä¸­å¿ƒç‚¹è·å…¶æœ€è¿‘çš„ç°‡ï¼Œä¹Ÿå¯é€šè¿‡KNNç­‰æ–¹æ³•ã€‚ ä¸€èˆ¬ä½¿ç”¨æ¬§å¼è·ç¦»åº¦é‡æ ·æœ¬åˆ°å„ä¸­å¿ƒç‚¹çš„è·ç¦»ï¼Œä¹Ÿå¯é€‰æ‹©ä½™å¼¦è·ç¦»ç­‰ï¼Œè¿™ä¹Ÿæ˜¯K-meansç®—æ³•çš„å…³é”® D(x^{(i)}, \mu_k) = || x^{(i)} - \mu_k ||_2^2å®šä¹‰æŸå¤±å‡½æ•°ä¸º J(\Omega) = \sum_i \sum_k r^{(i)}_k D(x^{(i)}, \mu_k)å…¶ä¸­ r^{(i)}_k = \begin{cases} 1 & x^{(i)} \in C_k \\ 0 & otherwise \end{cases}æˆ–è¡¨ç¤ºä¸º r^{(i)} = [0, ..., 1_k, ..., 0]^Tåœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼ŒæŸå¤±å‡½æ•°çš„å€¼ä¸æ–­ä¸‹é™ï¼Œä¼˜åŒ–ç›®æ ‡ä¸º \min J(\Omega)è®¡ç®—æ­¥éª¤ éšæœºé€‰å–$K$ä¸ªä¸­å¿ƒç‚¹ï¼› éå†æ‰€æœ‰æ•°æ®ï¼Œè®¡ç®—æ¯ä¸ªç‚¹åˆ°å„ä¸­å¿ƒç‚¹çš„è·ç¦»ï¼› å°†æ¯ä¸ªæ•°æ®åˆ’åˆ†åˆ°æœ€è¿‘çš„ä¸­å¿ƒç‚¹ä¸­ï¼› è®¡ç®—æ¯ä¸ªèšç±»çš„å¹³å‡å€¼ï¼Œä½œä¸ºæ–°çš„ä¸­å¿ƒç‚¹ï¼› é‡å¤æ­¥éª¤2-æ­¥éª¤4ï¼Œç›´åˆ°è¿™kä¸ªä¸­çº¿ç‚¹ä¸å†å˜åŒ–(æ”¶æ•›)ï¼Œæˆ–æ‰§è¡Œäº†è¶³å¤Ÿå¤šçš„è¿­ä»£ï¼› K-meansæ›´æ–°è¿­ä»£è¿‡ç¨‹å¦‚ä¸‹å›¾ ç¼ºç‚¹ä¸éƒ¨åˆ†è§£å†³æ–¹æ³• å±€éƒ¨æœ€ä¼˜ åˆå€¼æ•æ„Ÿ åˆå§‹ç‚¹çš„é€‰æ‹©ä¼šå½±å“K-meansèšç±»çš„ç»“æœï¼Œå³å¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜è§£ï¼Œå¦‚ä¸‹å›¾ å¯é€šè¿‡å¦‚ä¸‹æ–¹æ³•è§£å†³ å¤šæ¬¡é€‰æ‹©åˆå§‹ç‚¹è¿è¡ŒK-meansç®—æ³•ï¼Œé€‰æ‹©æœ€ä¼˜çš„ä½œä¸ºè¾“å‡ºç»“æœï¼› K-means++ éœ€è¦å®šä¹‰meanï¼Œå¯¹äºæ ‡ç§°å‹(categorical)æ•°æ®ä¸é€‚ç”¨ éœ€è¦ç»™å®šèšç±»ç°‡æ•°ç›®$K$ è¿™é‡Œç»™å‡ºä¸€ç§é€‰æ‹©ç°‡æ•°ç›®çš„æ–¹æ³•ï¼Œé€‰æ‹©å¤šä¸ª$K$å€¼è¿›è¡Œèšç±»ï¼Œè®¡ç®—ä»£ä»·å‡½æ•°ï¼ŒåšæˆæŠ˜çº¿å›¾åå¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°åœ¨$K=3$å¤„æŸå¤±å€¼çš„å˜åŒ–ç‡å‡ºç°è¾ƒå¤§å˜åŒ–ï¼Œåˆ™å¯é€‰æ‹©ç°‡çš„æ•°ç›®ä¸º$3$ã€‚ å™ªå£°æ•°æ®å¹²æ‰°å¤§ å¯¹äºéå‡¸é›†(non-convex)æ•°æ®æ— èƒ½ä¸ºåŠ› è°±èšç±»å¯è§£å†³éå‡¸é›†æ•°æ®çš„èšç±»é—®é¢˜ã€‚ æ”¹è¿› K-means++ æ”¹è¿›åˆå§‹ç‚¹é€‰æ‹©æ–¹æ³•ï¼Œç¬¬$1$ä¸ªä¸­å¿ƒç‚¹éšæœºé€‰æ‹©ï¼›ä¹‹åçš„åˆå§‹ä¸­å¿ƒç‚¹æ ¹æ®å‰é¢é€‰æ‹©çš„ä¸­å¿ƒç‚¹å†³å®šï¼Œè‹¥å·²é€‰å–$n$ä¸ªåˆå§‹èšç±»ä¸­å¿ƒ$(0&lt;n&lt;K)$ï¼Œé€‰å–ç¬¬$n+1$ä¸ªèšç±»ä¸­å¿ƒæ—¶ï¼Œè·ç¦»å½“å‰$n$ä¸ªèšç±»ä¸­å¿ƒè¶Šè¿œçš„ç‚¹ä¼šæœ‰æ›´é«˜çš„æ¦‚ç‡è¢«é€‰ä¸ºç¬¬$n+1$ä¸ªèšç±»ä¸­å¿ƒã€‚ ISODATA æ€æƒ³ï¼šå½“å±äºæŸä¸ªç±»åˆ«çš„æ ·æœ¬æ•°è¿‡å°‘æ—¶æŠŠè¿™ä¸ªç±»åˆ«å»é™¤ï¼Œå½“å±äºæŸä¸ªç±»åˆ«çš„æ ·æœ¬æ•°è¿‡å¤šã€åˆ†æ•£ç¨‹åº¦è¾ƒå¤§æ—¶æŠŠè¿™ä¸ªç±»åˆ«åˆ†ä¸ºä¸¤ä¸ªå­ç±»åˆ«. Kernel K-means å‚ç…§æ”¯æŒå‘é‡æœºä¸­æ ¸å‡½æ•°çš„æ€æƒ³ï¼Œå°†æ‰€æœ‰æ ·æœ¬æ˜ å°„åˆ°å¦å¤–ä¸€ä¸ªç‰¹å¾ç©ºé—´ä¸­å†è¿›è¡Œèšç±»ã€‚ ç±»ä¼¼çš„ç®—æ³•ä¸K-meansç±»ä¼¼çš„ç®—æ³•æœ‰å¾ˆå¤šï¼Œä¾‹å¦‚ K-medoids K-meansçš„å–å€¼èŒƒå›´å¯ä»¥æ˜¯è¿ç»­ç©ºé—´ä¸­çš„ä»»æ„å€¼ï¼Œè¦æ±‚æ‰€æœ‰æ•°æ®æ ·æœ¬å¤„åœ¨ä¸€ä¸ªæ¬§å¼ç©ºé—´ä¸­ï¼Œå¯¹äºæœ‰å¾ˆå¤šå™ªå£°çš„æ•°æ®å°±ä¼šé€ æˆæå¤§çš„è¯¯å·®ã€‚K-medoidsçš„å–å€¼æ˜¯æ•°æ®æ ·æœ¬èŒƒå›´ä¸­çš„æ ·æœ¬ï¼Œä¸”å¯åº”ç”¨åœ¨éæ•°å€¼å‹æ•°æ®æ ·æœ¬ä¸Šã€‚ k-medians $K$ä¸­å€¼ï¼Œé€‰æ‹©ä¸­ä½æ•°æ›´æ–°å„ç°‡çš„ä¸­å¿ƒç‚¹ã€‚ K-centers æ··åˆç±»å‹æ•°æ®çš„K-Centersèšç±»ç®—æ³•/The K-Centers Clustering Algorithm for Categorical and Mixe ä»£ç @Github: K-Means12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697class KMeans(): def __init__(self, n_cluster, mode): self.n_cluster = n_cluster # ç°‡çš„ä¸ªæ•° self.mode = mode # è·ç¦»åº¦é‡æ–¹å¼ self.centroids = None # ç°‡çš„ä¸­å¿ƒ self.loss = float(&apos;inf&apos;) # ä¼˜åŒ–ç›®æ ‡å€¼ plt.ion() def fit(self, X, max_iter=5, min_move=0.1, display=False): def initializeCentroids(): &apos;&apos;&apos; é€‰æ‹©åˆå§‹ç‚¹ &apos;&apos;&apos; centroid = np.zeros(shape=(self.n_cluster, X.shape[1])) # ä¿å­˜é€‰å‡ºçš„ç‚¹ pointIdx = [] # ä¿å­˜å·²é€‰å‡ºçš„ç‚¹çš„ç´¢å¼• for n in range(self.n_cluster): idx = np.random.randint(0, X.shape[0]) # éšæœºé€‰æ‹©ä¸€ä¸ªç‚¹ while idx in pointIdx: # è‹¥è¯¥ç‚¹å·²é€‰å‡ºï¼Œåˆ™ä¸¢å¼ƒé‡æ–°é€‰æ‹© idx = np.random.randint(0, X.shape[0]) pointIdx.append(idx) centroid[n] = X[idx] return centroid def dist2Centroids(x, centroids, mode): &apos;&apos;&apos; è¿”å›å‘é‡xåˆ°kä¸ªä¸­å¿ƒç‚¹çš„è·ç¦»å€¼ &apos;&apos;&apos; d = np.zeros(shape=(self.n_cluster,)) for n in range(self.n_cluster): d[n] = mathFunc.distance(x, centroids[n], mode) return d def nearestInfo(centroids, mode): &apos;&apos;&apos; æ¯ä¸ªç‚¹æœ€è¿‘çš„ç°‡ä¸­å¿ƒç´¢å¼•ã€è·ç¦» &apos;&apos;&apos; ctIdx = -np.ones(shape=(X.shape[0],), dtype=np.int8) # æ¯ä¸ªç‚¹æœ€è¿‘çš„ç°‡ä¸­å¿ƒç´¢å¼•ï¼Œåˆå§‹åŒ–ä¸º-1ï¼Œå¯ä½œä¸ºå¼‚å¸¸æ¡ä»¶ ctDist = np.ones(shape=(X.shape[0],), dtype=np.float) # æ¯ä¸ªç‚¹åˆ°æœ€è¿‘ç°‡ä¸­å¿ƒçš„è·ç¦» for i in range(X.shape[0]): dists = dist2Centroids(X[i], centroids, mode) if mode == &apos;Euclidean&apos;: ctIdx[i] = np.argmin(dists) elif mode == &apos;Cosine&apos;: ctIdx[i] = np.argmax(dists) ctDist[i] = dists[ctIdx[i]] # ä¿å­˜æœ€ç›¸ä¼¼çš„è·ç¦»åº¦é‡ï¼Œç”¨äºè®¡ç®—loss return ctIdx, ctDist def updateCentroids(ctIdx): &apos;&apos;&apos; æ›´æ–°ç°‡ä¸­å¿ƒ &apos;&apos;&apos; centroids = np.zeros(shape=(self.n_cluster, X.shape[1])) for n in range(self.n_cluster): X_ = X[ctIdx == n] # ç­›é€‰å‡ºç¦»ç°‡ä¸­å¿ƒCnæœ€è¿‘çš„æ ·æœ¬ç‚¹ centroids[n] = np.mean(X_, axis=0) # æ ¹æ®ç­›é€‰å‡ºçš„æ ·æœ¬ç‚¹æ›´æ–°ä¸­å¿ƒå€¼ return centroids def loss(dist): return np.mean(dist**2) # ----------------------------------------- loss_min = float(&apos;inf&apos;) # æœ€ä¼˜åˆ†ç±»æ—¶çš„æŸå¤±å€¼ï¼Œæœ€å° n_iter = 0 while n_iter &lt; max_iter: # æ¯æ¬¡è¿­ä»£é€‰æ‹©ä¸åŒçš„åˆå§‹ç‚¹ n_iter += 1; isDone = False # è¡¨ç¤ºæœ¬æ¬¡è¿­ä»£æ˜¯å¦å·²æ”¶æ•› centroids_tmp = initializeCentroids() # é€‰æ‹©æœ¬æ¬¡è¿­ä»£çš„åˆå§‹ç‚¹ loss_last = float(&apos;inf&apos;) # æœ¬æ¬¡è¿­ä»£ä¸­ï¼Œä¸­å¿ƒç‚¹æ›´æ–°å‰çš„æŸå¤±å€¼ n_update = 0 # æœ¬æ¬¡è¿­ä»£çš„æ›´æ–°æ¬¡æ•°è®¡æ•° while not isDone: n_update += 1 ctIdx, ctDist = nearestInfo(centroids_tmp, mode=self.mode) centroids_tmp = updateCentroids(ctIdx) # æ›´æ–°ç°‡ä¸­å¿ƒ # --- å¯è§†åŒ– --- if (display==True) and (X.shape[1] == 2): plt.ion() plt.figure(n_iter); plt.cla() plt.scatter(X[:, 0], X[:, 1], c=ctIdx) plt.scatter(centroids_tmp[:, 0], centroids_tmp[:, 1], c=&apos;r&apos;) plt.pause(0.5) # ------------- loss_now = loss(ctDist); moved = np.abs(loss_last - loss_now) if moved &lt; min_move: # è‹¥ç§»åŠ¨è¿‡å°ï¼Œåˆ™æœ¬æ¬¡è¿­ä»£æ”¶æ•› isDone = True print(&apos;ç¬¬%dæ¬¡è¿­ä»£ç»“æŸï¼Œä¸­å¿ƒç‚¹æ›´æ–°%dæ¬¡&apos; % (n_iter, n_update)) else: loss_last = loss_now if loss_now &lt; loss_min: self.centroids = centroids_tmp # ä¿å­˜æŸå¤±æœ€å°çš„æ¨¡å‹(æœ€ä¼˜) loss_min = loss_now # print(&apos;èšç±»ç»“æœå·²æ›´æ–°&apos;) self.loss = loss_min print(&apos;=========== è¿­ä»£ç»“æŸ ===========&apos;) def predict(self, X): &apos;&apos;&apos; å„ä¸ªæ ·æœ¬çš„æœ€è¿‘ç°‡ä¸­å¿ƒç´¢å¼• &apos;&apos;&apos; labels = -np.ones(shape=(X.shape[0],), dtype=np.int) # åˆå§‹åŒ–ä¸º-1ï¼Œå¯ç”¨ä½œå¼‚å¸¸æ¡ä»¶ for i in range(X.shape[0]): dists_i = np.zeros(shape=(self.n_cluster,)) # ä¿å­˜X[i]åˆ°ä¸­å¿ƒç‚¹Cnçš„è·ç¦» for n in range(self.n_cluster): dists_i[n] = mathFunc.distance(X[i], self.centroids[n], mode=self.mode) if self.mode == &apos;Euclidean&apos;: labels[i] = np.argmin(dists_i) elif self.mode == &apos;Cosine&apos;: labels[i] = np.argmax(dists_i) return labels ç°‡æ•°çš„é€‰æ‹©ä»£ç å¦‚ä¸‹123456789101112def chooseBestK(X, start, stop, step=1, mode=&apos;Euclidean&apos;): Ks = np.arange(start, stop + 1, step, dtype=np.int) # å¾…é€‰æ‹©çš„K Losses = np.zeros(shape=Ks.shape) # ä¿å­˜ä¸åŒKå€¼æ—¶çš„æœ€å°æŸå¤±å€¼ for k in range(1, Ks.shape[0] + 1): # å¯¹äºä¸åŒçš„Kï¼Œè®­ç»ƒæ¨¡å‹ï¼Œè®¡ç®—æŸå¤± print(&apos;K = %d&apos;, k) estimator = KMeans(n_cluster=k, mode=mode) estimator.fit(X, max_iter=10, min_move=0.01, display=False) Losses[k - 1] = estimator.loss plt.ioff() plt.figure(); plt.xlabel(&apos;n_clusters&apos;); plt.ylabel(&apos;loss&apos;) plt.plot(Ks, Losses) # åšå‡ºloss-Kæ›²çº¿ plt.show() å‡å€¼æ¼‚ç§»(Meanshift)æœ¬è´¨æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨ä¸€ç»„æ•°æ®çš„å¯†åº¦åˆ†å¸ƒä¸­å¯»æ‰¾åˆ°å±€éƒ¨æå€¼ï¼Œæ¯”è¾ƒç¨³å®šï¼Œè€Œä¸”æ˜¯æ— å‚å¯†åº¦ä¼°è®¡(ä¸éœ€è¦äº‹å…ˆçŸ¥é“æ ·æœ¬æ•°æ®çš„æ¦‚ç‡å¯†åº¦åˆ†å¸ƒå‡½æ•°ï¼Œå®Œå…¨ä¾é å¯¹æ ·æœ¬ç‚¹çš„è®¡ç®—)ï¼Œè€Œä¸”åœ¨é‡‡æ ·å……åˆ†çš„æƒ…å†µä¸‹ï¼Œä¸€å®šä¼šæ”¶æ•›ï¼Œå³å¯ä»¥å¯¹æœä»ä»»æ„åˆ†å¸ƒçš„æ•°æ®è¿›è¡Œå¯†åº¦ä¼°è®¡ã€‚ åŸç†æœ‰ä¸€ä¸ªæ»‘åŠ¨çª—å£çš„æ€æƒ³ï¼Œå³åˆ©ç”¨å½“å‰ä¸­å¿ƒç‚¹ä¸€å®šèŒƒå›´å†…(é€šå¸¸ä¸ºçƒåŸŸ)çš„ç‚¹è¿­ä»£æ›´æ–°ä¸­å¿ƒç‚¹ï¼Œé‡å¤ç§»åŠ¨çª—å£ï¼Œç›´åˆ°æ»¡è¶³æ”¶æ•›æ¡ä»¶ã€‚ç®€å•çš„è¯´ï¼ŒMeanshiftå°±æ˜¯æ²¿ç€å¯†åº¦ä¸Šå‡çš„æ–¹å‘å¯»æ‰¾åŒå±ä¸€ä¸ªç°‡çš„æ•°æ®ç‚¹ã€‚ å®šä¹‰ç‚¹$x_0$çš„$\epsilon$çƒåŸŸå¦‚ä¸‹ S_h(x_0) = \{ x | (x - x_0)^T (x - x_0) \leq \epsilon \}è‹¥æœ‰$n$ä¸ªç‚¹$(x_1, â€¦, x_n)$è½åœ¨ä¸­å¿ƒç‚¹$ptCentroid$çš„é‚»åŸŸå†…ï¼Œå…¶åˆ†å¸ƒå¦‚å›¾ åˆ™åç§»å‘é‡è®¡ç®—æ–¹å¼ä¸º vecShift = \frac{1}{n} \sum_{i=1}^n (x_i - ptCentroid)ä¸­å¿ƒç‚¹æ›´æ–°å…¬å¼ä¸º ptCentroid := ptCentroid + vecShift å±•å¼€åå¯å‘ç°ï¼Œå…¶æ›´æ–°å…¬å¼å³ vecShift = \frac{1}{n} \sum_{i=1}^n x_i - ptCentroid ptCentroid := \frac{1}{n} \sum_{i=1}^n x_i ä¸€ä¸ªæ»‘åŠ¨çª—å£çš„åŠ¨æ€æ›´æ–°è¿‡ç¨‹å¦‚ä¸‹å›¾åˆå§‹åŒ–å¤šä¸ªæ»‘åŠ¨çª—å£è¿›è¡ŒMeanShiftç®—æ³•ï¼Œå…¶æ›´æ–°è¿‡ç¨‹å¦‚ä¸‹ï¼Œå…¶ä¸­æ¯ä¸ªé»‘ç‚¹ä»£è¡¨æ»‘åŠ¨çª—å£çš„è´¨å¿ƒï¼Œæ¯ä¸ªç°ç‚¹ä»£è¡¨ä¸€ä¸ªæ•°æ®ç‚¹ é«˜æ–¯æƒé‡åŸºæœ¬æ€æƒ³æ˜¯ï¼Œè·ç¦»å½“å‰ä¸­å¿ƒç‚¹è¿‘çš„å‘é‡å¯¹æ›´æ–°ç»“æœæƒé‡å¤§ï¼Œè€Œè¿œçš„æƒé‡å°ï¼Œå¯å‡å°è¿œç‚¹çš„å¹²æ‰°ï¼Œå¦‚ä¸‹å›¾ï¼Œ$vecShift_2$ä¸ºé«˜æ–¯æƒé‡ä¸‹çš„åç§»å‘é‡ å…¶åç§»å‘é‡è®¡ç®—æ–¹å¼ä¸º vecShift = \frac{1}{n} \sum_{i=1}^n w_i Â· (x_i - ptCentroid) w_i = \frac{\kappa(x_i - ptCentroid)}{\sum_j \kappa(x_j - ptCentroid)}å…¶ä¸­ \kappa(z) = \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{||z||^2}{2\sigma^2} \right)ä¸­å¿ƒç‚¹æ›´æ–°å…¬å¼ä»ç„¶ä¸º ptCentroid := ptCentroid + vecShift å±•å¼€ä¹Ÿå¯å¾—åˆ° ptCentroid := \frac{\sum_{i=1}^n w_i x_i}{\sum_j w_j} è®¡ç®—æ­¥éª¤å¯¹äºç»™å®šçš„$N$ç»´æ•°æ®é›†$X = (x^{(1)}, x^{(2)}, â€¦, x^{(M)})$ï¼ŒæŒ‡å®šé‚»åŸŸå‚æ•°$\epsilon_0$ï¼Œç»ˆæ­¢æ¡ä»¶å‚æ•°$\epsilon_1$ï¼Œç°‡åˆå¹¶å‚æ•°$\epsilon_2$ï¼Œå¹¶æŒ‡å®šæ ·æœ¬è·ç¦»åº¦é‡æ–¹å¼ï¼Œç›®æ ‡ä¸ºå°†å…¶åˆ’åˆ†ä¸º$K$ä¸ªç°‡ã€‚ åˆå§‹åŒ–ï¼š åœ¨æ ·æœ¬é›†ä¸­éšæœºé€‰æ‹©$K_0(K_0 \gg K)$ä¸ªæ ·æœ¬ä½œä¸ºåˆå§‹ä¸­å¿ƒç‚¹ï¼Œä»¥é‚»åŸŸå¤§å°ä¸º$\epsilon_0$å»ºç«‹æ»‘åŠ¨çª—å£ï¼› å„ä¸ªæ ·æœ¬åˆå§‹åŒ–ä¸€ä¸ªæ ‡è®°å‘é‡ï¼Œç”¨äºè®°å½•è¢«å„ç±»åˆ«è®¿é—®çš„æ¬¡æ•°ï¼› ä»¥å•ä¸ªæ»‘åŠ¨çª—å£åˆ†æï¼Œè®°å…¶ä¸­å¿ƒç‚¹ä¸º$ptCentroid$ï¼Œæ‰¾åˆ°æ»‘åŠ¨çª—å£å†…çš„æ‰€æœ‰ç‚¹ï¼Œè®°ä½œé›†åˆ$M$ï¼Œè®¤ä¸ºè¿™äº›ç‚¹å±äºè¯¥æ»‘åŠ¨çª—å£æ‰€å±çš„ç°‡ç±»åˆ«ï¼ŒåŒæ—¶ï¼Œè¿™äº›ç‚¹è¢«è¯¥ç°‡è®¿é—®çš„æ¬¡æ•°$+1$ï¼› ä»¥$ptCentroid$ä¸ºä¸­å¿ƒï¼Œè®¡ç®—å…¶åˆ°é›†åˆ$M$ä¸­å„ä¸ªå…ƒç´ çš„å‘é‡ï¼Œä»¥è¿™äº›å‘é‡è®¡ç®—å¾—åˆ°åç§»å‘é‡$vecShift$ï¼› æ›´æ–°ä¸­å¿ƒç‚¹ï¼š$ptCentroid = ptCentroid + vecShift$ï¼Œå³æ»‘åŠ¨çª—å£æ²¿ç€$vecShift$æ–¹å‘ç§»åŠ¨ï¼Œè·ç¦»ä¸º$||vecShift||$ï¼› é‡å¤æ­¥éª¤$2-4$ï¼Œç›´åˆ°$||vecShift||&lt;\epsilon_1$ï¼Œä¿å­˜å½“å‰ä¸­å¿ƒç‚¹ï¼› å¦‚æœæ”¶æ•›æ—¶å½“å‰ç°‡$ptCentroid$ä¸å…¶å®ƒå·²ç»å­˜åœ¨çš„ç°‡çš„ä¸­å¿ƒçš„è·ç¦»å°äºé˜ˆå€¼$\epsilon_2$ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªç°‡åˆå¹¶ã€‚å¦åˆ™ï¼ŒæŠŠå½“å‰ç°‡ä½œä¸ºæ–°çš„ç°‡ç±»ï¼Œå¢åŠ $1$ç±»ï¼› é‡å¤è¿­ä»£ç›´åˆ°æ‰€æœ‰çš„ç‚¹éƒ½è¢«æ ‡è®°è®¿é—®ï¼› æ ¹æ®æ¯ä¸ªæ ·æœ¬è¢«å„ç°‡çš„è®¿é—®é¢‘ç‡ï¼Œå–è®¿é—®é¢‘ç‡æœ€å¤§çš„é‚£ä¸ªç°‡ç±»åˆ«ä½œä¸ºå½“å‰ç‚¹é›†çš„æ‰€å±ç±»ã€‚ å³ä¸åŒç±»å‹çš„æ»‘çª—æ²¿ç€å¯†åº¦ä¸Šå‡çš„æ–¹å‘è¿›è¡Œç§»åŠ¨ï¼Œå¯¹å„æ ·æœ¬ç‚¹è¿›è¡Œæ ‡è®°ï¼Œæœ€åå°†æ ·æœ¬åˆ’åˆ†ä¸ºæ ‡è®°æœ€å¤šçš„ç±»åˆ«ï¼›å½“ä¸¤ç±»éå¸¸æ¥è¿‘æ—¶ï¼Œåˆå¹¶ä¸ºä¸€ç±»ã€‚ ä»£ç @Github: MeanShift å…ˆå®šä¹‰äº†çª—æ ¼å¯¹è±¡1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class SlidingWindow(): &quot;&quot;&quot; Attributes: centroid: &#123;ndarray(n_features,)&#125; epsilon: &#123;float&#125; æ»‘åŠ¨çª—æ ¼å¤§å°ï¼Œä¸ºåŠå¾„çš„å¹³æ–¹ sigma: &#123;float&#125; é«˜æ–¯æ ¸å‡½æ•°çš„å‚æ•° label: &#123;int&#125; è¯¥çª—æ ¼çš„æ ‡è®° X: &#123;ndarray(n_samples, n_features)&#125; containIdx: &#123;ndarray(n_contain,)&#125; çª—æ ¼å†…åŒ…å«ç‚¹çš„ç´¢å¼• &quot;&quot;&quot; def __init__(self, centroid, epsilon, sigma, label, X): self.centroid = centroid self.epsilon = epsilon self.sigma = sigma self.label = label self.containIdx = self.updateContain(X) def k(self, z): &quot;&quot;&quot; é«˜æ–¯æ ¸å‡½æ•° Args: z: &#123;ndarray(n_features,)&#125; Notes: - \kappa(z) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \exp \left( - \frac&#123;||z||^2&#125;&#123;2\sigma^2&#125; \right) &quot;&quot;&quot; norm = np.linalg.norm(z) return np.exp(- 0.5 * (norm / self.sigma)**2) / np.sqrt(2*np.pi) def step(self, X): &quot;&quot;&quot; æ›´æ–°æ»‘åŠ¨çª—æ ¼çš„ä¸­å¿ƒç‚¹å’Œæ‰€åŒ…å«ç‚¹ Returns: &#123;float&#125; &quot;&quot;&quot; dshift = self.shift(X) self.containIdx = self.updateContain(X) return dshift def shift(self, X): &quot;&quot;&quot; ç§»åŠ¨çª—æ ¼ Args: vecShift: &#123;ndarray(n_features,)&#125; Returns: dshift: &#123;float&#125; ç§»åŠ¨çš„è·ç¦» &quot;&quot;&quot; (n_samples, n_features) = X.shape n_contain = self.containIdx.shape[0] contain_weighted_sum = np.zeros(shape=(n_features, )) weight_sum = 0 # æŒ‰åŒ…å«çš„ç‚¹è¿›è¡Œç§»åŠ¨ for i_contain in range(n_contain): vector = X[self.containIdx[i_contain]] - self.centroid weight = self.k(vector) contain_weighted_sum += weight*X[self.containIdx[i_contain]] weight_sum += weight centroid = contain_weighted_sum / weight_sum # è®¡ç®—ç§»åŠ¨çš„è·ç¦» dshift = np.linalg.norm(self.centroid - centroid) self.centroid = centroid return dshift def updateContain(self, X): &quot;&quot;&quot; æ›´æ–°çª—æ ¼å†…çš„ç‚¹ç´¢å¼• Args: X: &#123;ndarray(n_samples, n_features)&#125; Notes: - ç”¨æ¬§å¼è·ç¦»ä½œä¸ºåº¦é‡ &quot;&quot;&quot; d = lambda x_i, x_j: np.linalg.norm(x_i - x_j) n_samples = X.shape[0] containIdx = np.array([], dtype=&apos;int&apos;) for i_samples in range(n_samples): if d(X[i_samples], self.centroid) &lt; self.epsilon: containIdx = np.r_[containIdx, i_samples] return containIdx èšç±»ç®—æ³•å¦‚ä¸‹123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109class MeanShift(): &quot;&quot;&quot; Attributes: n_clusters: &#123;int&#125; åˆ’åˆ†ç°‡çš„ä¸ªæ•° n_windows: &#123;int&#125; æ»‘åŠ¨çª—æ ¼çš„ä¸ªæ•° epsilon: &#123;float&#125; æ»‘åŠ¨çª—æ ¼çš„å¤§å° sigma: &#123;float&#125; &#123;float&#125; é«˜æ–¯æ ¸å‚æ•° thresh: &#123;float&#125; è‹¥ä¸¤ä¸ªçª—æ ¼ä¸­å¿ƒè·ç¦»å°äºthreshï¼Œåˆ™åˆå¹¶ä¸¤ç±»ç°‡ min_move: &#123;float&#125; ç»ˆæ­¢æ¡ä»¶ windows: &#123;list[class SlidingWindow()]&#125; Note: - å‡è®¾æ‰€æœ‰ç‚¹å‡è¢«çª—æ ¼åˆ’è¿‡ &quot;&quot;&quot; def __init__(self, n_clusters, n_windows=-1, epsilon=0.5, sigma=2, thresh=1e-2, min_move=1e-3): self.n_clusters = n_clusters self.n_windows = 5*n_clusters if (n_windows == -1) else n_windows self.epsilon = epsilon self.sigma = sigma self.thresh = thresh self.min_move = min_move self.windows = [] self.centroids = None def fit(self, X): (n_samples, n_features) = X.shape # åˆ›å»ºçª—æ ¼ for i_windows in range(self.n_windows): idx = np.random.randint(n_samples) window = SlidingWindow(X[idx], self.epsilon, self.sigma, i_windows, X) # å°†å„çª—æ ¼åŒ…å«çš„ç‚¹æ ‡è®° n_contain = window.containIdx.shape[0] self.windows.append(window) dshift = float(&apos;inf&apos;) # åˆå§‹åŒ–ä¸ºæ— ç©·å¤§ plt.figure(); plt.ion() while dshift &gt; self.min_move: # ------ åšå›¾æ˜¾ç¤º ------ plt.cla() plt.scatter(X[:, 0], X[:, 1], c=&apos;b&apos;) for i_windows in range(self.n_windows): centroid = self.windows[i_windows].centroid plt.scatter(centroid[0], centroid[1], c=&apos;r&apos;) plt.pause(0.5) # --------------------- dshift = self.step(X) plt.ioff() # åˆå¹¶çª—æ ¼ dists = np.zeros(shape=(self.n_windows, self.n_windows)) for i_windows in range(self.n_windows): for j_windows in range(i_windows): centroid_i = self.windows[i_windows].centroid centroid_j = self.windows[j_windows].centroid dists[i_windows, j_windows] = np.linalg.norm(centroid_i-centroid_j) dists[j_windows, i_windows] = dists[i_windows, j_windows] # è·å¾—è·ç¦»ç›¸è¿‘ç´¢å¼• index = np.where(dists&lt;self.thresh) # ç”¨äºæ ‡è®°ç±»åˆ« winlabel = np.zeros(shape=(self.n_windows,), dtype=&apos;int&apos;) label = 1; winlabel[0] = label for i_windows in range(self.n_windows): idx_row = index[0][i_windows] idx_col = index[1][i_windows] # è‹¥å…¶ä¸­ä¸€ä¸ªç‚¹è¢«æ ‡è®°ï¼Œåˆ™å°†ä»¤ä¸€ä¸ªç‚¹å¹¶å…¥è¯¥ç±» if winlabel[idx_row]!=0: winlabel[idx_col] = winlabel[idx_row] elif winlabel[idx_col]!=0: winlabel[idx_row] = winlabel[idx_col] # å¦åˆ™æ–°åˆ›å»ºç±»åˆ« else: label += 1 winlabel[idx_row] = label winlabel[idx_col] = label # å°†æ ‡ç­¾ä¸€æ ·çš„çª—æ ¼åˆå¹¶ labels = list(set(winlabel)) # å»é‡åçš„æ ‡ç­¾ n_labels = len(labels) # æ ‡ç­¾ç§ç±»æ•° self.centroids = np.zeros(shape=(n_labels, n_features)) # è®°å½•æœ€ç»ˆèšç±»ä¸­å¿ƒ for i_labels in range(n_labels): cnt = 0 for i_windows in range(self.n_windows): if winlabel[i_windows] == labels[i_labels]: self.centroids[i_labels] += self.windows[i_windows].centroid cnt += 1 self.centroids[i_labels] /= cnt # å–åŒç±»çª—æ ¼ä¸­å¿ƒç‚¹çš„å‡å€¼ return self.centroids def step(self, X): &quot;&quot;&quot; update all sliding windows Returns: dshift: \sum_i^&#123;n_windows&#125; dshift_&#123;i&#125; &quot;&quot;&quot; dshift = 0 for i_windows in range(self.n_windows): dshift += self.windows[i_windows].step(X) # label the points n_contain = self.windows[i_windows].containIdx.shape[0] return dshift def predict(self, X): &quot;&quot;&quot; ç®€å•çš„ç”¨è¿‘é‚»çš„æ–¹æ³•æ±‚ &quot;&quot;&quot; (n_samples, n_features) = X.shape dists = np.zeros(shape=(n_samples, self.n_clusters)) for i_samples in range(n_samples): for i_clusters in range(self.n_clusters): dists[i_samples, i_clusters] = np.linalg.norm(X[i_samples]-self.centroids[i_clusters]) return np.argmin(dists, axis=1) è°±èšç±»(Spectral Clustering)è°±èšç±»æ˜¯ä»å›¾è®ºä¸­æ¼”åŒ–å‡ºæ¥çš„ç®—æ³•ï¼Œåæ¥åœ¨èšç±»ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œæ¯”èµ·ä¼ ç»Ÿçš„K-Meansç®—æ³•ï¼Œè°±èšç±»å¯¹æ•°æ®åˆ†å¸ƒçš„é€‚åº”æ€§æ›´å¼ºï¼Œèšç±»æ•ˆæœä¹Ÿå¾ˆä¼˜ç§€ï¼ŒåŒæ—¶èšç±»çš„è®¡ç®—é‡ä¹Ÿå°å¾ˆå¤šã€‚ åŸç† è°±èšç±»ï¼ˆspectral clusteringï¼‰åŸç†æ€»ç»“ - åˆ˜å»ºå¹³Pinard - åšå®¢å›­ æ— å‘æƒé‡å›¾æˆ‘ä»¬ç”¨ç‚¹çš„é›†åˆ$V$å’Œè¾¹çš„é›†åˆ$E$æè¿°ä¸€ä¸ªå›¾ï¼Œå³$G(V, E)$ï¼Œå…¶ä¸­$V$å³æ•°æ®é›†ä¸­çš„ç‚¹ V = [v_1, v_2, ..., v_n]è€Œç‚¹$v_i, v_j$é—´è¿æ¥æƒå€¼$w_{ij}$ç»„æˆé‚»æ¥çŸ©é˜µ$W$ï¼Œç”±äºä¸ºæ— å‘å›¾ï¼Œæ•…æ»¡è¶³$w_{ij}=w_{ji}$ W = \left[ \begin{matrix} w_{11} & ... & w_{1n} \\ ... & ... & ... \\ w_{n1} & ... & w_{nn} \\ \end{matrix} \right]å¯¹äºå›¾ä¸­çš„ä»»æ„ä¸€ä¸ªç‚¹$v_i$ï¼Œå®šä¹‰å…¶åº¦$d_i$ä¸º d_i = \sum_{j=1}^n w_{ij}åˆ™æˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªåº¦çŸ©é˜µ$D=diag(d_1, â€¦, d_n)$ D = \left[ \begin{matrix} d_1 & & \\ & ... & \\ & & d_n\\ \end{matrix} \right]é™¤æ­¤ä¹‹å¤–ï¼Œå¯¹äº$V$ä¸­å­é›†$V_{sub} \subset V$ï¼Œå®šä¹‰å­é›†$V_{sub}$ç‚¹çš„ä¸ªæ•°ä¸º |V_{sub}| := n_{sub}å¦å¤–ï¼Œå®šä¹‰è¯¥å­é›†ä¸­ç‚¹çš„åº¦ä¹‹å’Œä¸º vol(V_{sub}) = \sum_{i \in V_{sub}} d_iç›¸ä¼¼çŸ©é˜µä¸Šé¢è®²åˆ°çš„é‚»æ¥çŸ©é˜µ$W$å¯ä»¥æŒ‡å®šæƒå€¼ï¼Œä½†å¯¹äºæ•°æ®é‡åºå¤§çš„æ•°æ®é›†ï¼Œè¿™æ˜¾ç„¶ä¸æ˜¯ä¸€ä¸ª$wise$çš„é€‰æ‹©ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ç›¸ä¼¼çŸ©é˜µ$S$æ¥è·å¾—é‚»æ¥çŸ©é˜µ$W$ï¼ŒåŸºæœ¬æ€æƒ³æ˜¯ï¼Œè·ç¦»è¾ƒè¿œçš„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è¾¹æƒé‡å€¼è¾ƒä½ï¼Œè€Œè·ç¦»è¾ƒè¿‘çš„ä¸¤ä¸ªç‚¹ä¹‹é—´çš„è¾¹æƒé‡å€¼è¾ƒé«˜ã€‚ æ„å»ºé‚»æ¥çŸ©é˜µ$W$çš„æ–¹æ³•æœ‰ä¸‰ç±»ï¼š$\epsilon$-é‚»è¿‘æ³•ï¼Œ$K$é‚»è¿‘æ³•å’Œå…¨è¿æ¥æ³•ï¼Œå®šä¹‰è·ç¦» d_{ij} = ||x^{(i)} - x^{(j)}||_2^2 $\epsilon$-é‚»è¿‘æ³• è®¾ç½®è·ç¦»é˜ˆå€¼$\epsilon$ï¼Œç”¨æ¬§å¼è·ç¦»åº¦é‡ä¸¤ç‚¹çš„è·ç¦»$d_{ij}$ï¼Œç„¶åé€šè¿‡ä¸‹å¼ç¡®å®šé‚»æ¥æƒå€¼$w_{ij}$ w_{ij} = \begin{cases} 0 & d_{ij} > \epsilon \\ \epsilon & otherwise \end{cases} ä¸¤ç‚¹é—´çš„æƒé‡è¦ä¸å°±æ˜¯$\epsilon$ï¼Œè¦ä¸å°±æ˜¯0ï¼Œè·ç¦»è¿œè¿‘åº¦é‡å¾ˆä¸ç²¾ç¡®ï¼Œå› æ­¤åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¾ˆå°‘ä½¿ç”¨$\epsilon$-é‚»è¿‘æ³•ã€‚ $K$é‚»è¿‘æ³• ç¬¬ä¸€ç§ åªè¦ä¸€ä¸ªç‚¹åœ¨å¦ä¸€ä¸ªç‚¹çš„$K$è¿‘é‚»ä¸­ï¼Œåˆ™ä¿ç•™$d_{ij}$ w_{ij} = \begin{cases} \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)}) or x^{(j)} \in KNN(x^{(i)}) \\ 0 & otherwise \end{cases} ç¬¬äºŒç§ äº’ä¸º$K$è¿‘é‚»æ—¶ä¿ç•™$d_{ij}$ w_{ij} = \begin{cases} \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)}) and x^{(j)} \in KNN(x^{(i)}) \\ 0 & otherwise \end{cases} å…¨è¿æ¥æ³• å¯ä»¥é€‰æ‹©ä¸åŒçš„æ ¸å‡½æ•°æ¥å®šä¹‰è¾¹æƒé‡ï¼Œå¸¸ç”¨çš„æœ‰å¤šé¡¹å¼æ ¸å‡½æ•°ï¼Œé«˜æ–¯æ ¸å‡½æ•°å’ŒSigmoidæ ¸å‡½æ•°ã€‚æœ€å¸¸ç”¨çš„æ˜¯é«˜æ–¯æ ¸å‡½æ•°RBFï¼Œæ­¤æ—¶ç›¸ä¼¼çŸ©é˜µå’Œé‚»æ¥çŸ©é˜µç›¸åŒ w_{ij} = \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ(Graph Laplacians)å®šä¹‰ L = D - Wæ­£åˆ™åŒ–çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µä¸º L = D^{-1} (D - W)å…·æœ‰çš„æ€§è´¨å¦‚ä¸‹ $L^T = L$ å…¶ç‰¹å¾å€¼å‡ä¸ºå®æ•°ï¼Œå³$\lambda_i \in \mathbb{R}$ æ­£å®šæ€§ï¼š$\lambda_i \geq 0$ å¯¹äºä»»æ„å‘é‡$x$ï¼Œéƒ½æœ‰ x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2 è¯æ˜ï¼š x^T L x = x^T D x - x^T W x = \sum_i d_i > x_i^2 - \sum_{ij} w_{ij} x_i x_j = \frac{1}{2} \left[ \sum_i d_i x_i^2 - > 2\sum_{ij} w_{ij} x_i x_j + \sum_j d_j x_j^2 \right]å…¶ä¸­$ d_i = \sum_j w_{ij} $ï¼Œæ‰€ä»¥ x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2 æ— å‘å›¾çš„åˆ‡å›¾cutæˆ‘ä»¬å¸Œæœ›æŠŠä¸€å¼ æ— å‘å›¾$G(V, E)$æŒ‰ä¸€å®šæ–¹æ³•åˆ‡æˆå¤šä¸ªå­å›¾ï¼Œå„ä¸ªå­å›¾é—´æ— è¿æ¥ï¼Œæ¯ä¸ªå­å›¾çš„ç‚¹é›†ä¸º$V_1, â€¦, V_K$ï¼Œæ»¡è¶³ $\bigcup_{k=1}^K V_k = V$ $V_i \cap V_j = \emptyset$ å®šä¹‰ä¸¤ä¸ªå­å›¾ç‚¹é›†åˆ$A, B$ä¹‹é—´çš„åˆ‡å›¾æƒé‡ä¸º W(A, B) = \sum_{i \in A, j \in B} w_{ij} å…±æœ‰$n_A Ã— n_B$ä¸ªæƒå€¼ä½œç´¯åŠ  é‚£ä¹ˆå¯¹äº$K$ä¸ªå­å›¾ç‚¹çš„é›†åˆ$V_1, â€¦, V_K$ï¼Œå®šä¹‰åˆ‡å›¾ä¸º cut(V_1, ..., V_K) = \frac{1}{2} \sum_{i=1}^K cut(V_i, \overline{V_i}) cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})å…¶ä¸­$\overline{V_i}$è¡¨ç¤º$V_i$çš„è¡¥é›†ï¼Œæˆ–è€… \overline{V_i} = \bigcup_{k \neq i} V_ké€šè¿‡æœ€å°åŒ–$cut(V_1, â€¦, V_K)$ä½¿å­å›¾å†…æƒé‡å’Œå¤§ï¼Œè€Œå­å›¾é—´æƒé‡å’Œå°ã€‚ä½†æ˜¯è¿™ç§æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼Œå¦‚ä¸‹å›¾ é€‰æ‹©ä¸€ä¸ªæƒé‡æœ€å°çš„è¾¹ç¼˜çš„ç‚¹ï¼Œæ¯”å¦‚$C$å’Œ$H$ä¹‹é—´è¿›è¡Œ$cut$ï¼Œè¿™æ ·å¯ä»¥æœ€å°åŒ–$cut(V_1, â€¦, V_K)$ï¼Œä½†æ˜¯å´ä¸æ˜¯æœ€ä¼˜çš„åˆ‡å›¾ã€‚ ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œéœ€è¦å¯¹æ¯ä¸ªå­å›¾çš„è§„æ¨¡åšå‡ºé™å®šï¼Œä»¥ä¸‹ä»‹ç»ä¸¤ç§åˆ‡å›¾æ–¹å¼ã€‚ Ratio Cutä¸ä»…è€ƒè™‘æœ€å°åŒ–$cut(V_1, â€¦, V_K)$ï¼Œè€Œä¸”æœ€å¤§åŒ–æ¯ä¸ªå­å›¾çš„ç‚¹ä¸ªæ•°ï¼Œå³ RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{|V_k|} cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) $W(V_k, \overline{V_k}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}$ $|V_k| = n_k$ å¦‚æœæŒ‰ç…§éå†çš„æ–¹æ³•æ±‚è§£ï¼Œç”±å‰é¢åˆ†æï¼Œ$W(V_k, \overline{V_k})$éœ€è®¡ç®—$n_{V_k} Ã— n_{\overline{V_k}}$æ¬¡ç´¯åŠ ï¼Œè®¡ç®—é‡åºå¤§ï¼Œé‚£ä¹ˆå¦‚ä½•æ±‚è§£å‘¢ï¼Ÿ å®šä¹‰æŒ‡ç¤ºå‘é‡$h_k$ï¼Œå…¶æ„æˆçŸ©é˜µ$H$ H = [ h_1, ..., h_k, ..., h_K]å…¶ä¸­ h_k = \left[h_{k1}, h_{k2}, , ..., h_{kM} \right]^T h_{ki} = \begin{cases} \frac{1}{\sqrt{|V_k|}} & x^{(i)}\in V_k \\ 0 & otherwise \end{cases} $h_k$ä¸ºå•ä½å‘é‡ï¼Œä¸”ä¸¤ä¸¤æ­£äº¤ h_i^T h_j = \begin{cases} \sum_{|V_i|} \frac{1}{|V_i|} = |V_i| Â· \frac{1}{|V_i|} = 1 & i = j \\ 0 & i \neq j \end{cases} é‚£ä¹ˆç”±æ‹‰å¼çŸ©é˜µæ€§è´¨$4$ h_k^T L h_k = \frac{1}{2} \sum_{i,j} w_{ij} (h_{ki} - h_{kj})^2 = \frac{1}{2} [ \sum_{i \in V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 + \sum_{i \notin V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 + \sum_{i \in V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 + \sum_{i \notin V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 ] = \frac{1}{2} [ \sum_{i \in V_k, j \in V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - \frac{1}{\sqrt{|V_k|}})^2 + \sum_{i \notin V_k, j \in V_k} w_{ij} (0 - \frac{1}{\sqrt{|V_k|}})^2 + \sum_{i \in V_k, j \notin V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - 0)^2 + \sum_{i \notin V_k, j \notin V_k} w_{ij} (0 - 0)^2 ] = \frac{1}{2} [ \sum_{i \notin V_k, j \in V_k} w_{ij} \frac{1}{|V_k|} + \sum_{i \in V_k, j \notin V_k} w_{ij} \frac{1}{|V_k|} ] cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij} h_k^T L h_k = \frac{1}{2} [\frac{1}{|V_k|} cut(V_k, \overline{V_k}) + \frac{1}{|V_k|} cut(V_k, \overline{V_k})] = \frac{1}{|V_k|} cut(V_k, \overline{V_k})æ¨åˆ°è¿™é‡Œå°±èƒ½ç†è§£ä¸ºä»€ä¹ˆè¦å®šä¹‰$h_k$äº† RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k h_k^T L h_kå¹¶ä¸” h_k^T L h_k = tr(H^T L H) H^T L H = \left[ \begin{matrix} â€” & h_1^T & â€” \\ & ... & \\ â€” & h_K^T & â€” \\ \end{matrix} \right] L \left[ \begin{matrix} | & & | \\ h_1 & ... & h_K \\ | & & | \end{matrix} \right] = \left[ \begin{matrix} h_1^T L h_1 & ... & h_1^T L h_K \\ ... & ... & ... \\ h_K^T L h_K & ... & h_K^T L h_K \\ \end{matrix} \right] æ‰€ä»¥æœ€ç»ˆä¼˜åŒ–ç›®æ ‡ä¸º \min_H tr(H^T L H) s.t. H^T H = I H^T H = \left[ \begin{matrix} h_1^T h_1 & ... & h_1^T h_K \\ ... & ... & ... \\ h_K^T h_K & ... & h_K^T h_K \\ \end{matrix} \right] è€ŒçŸ©é˜µçš„æ­£äº¤ç›¸ä¼¼å˜æ¢$A = P \Lambda P^{-1}$æ»¡è¶³ tr(A) = tr(\Lambda) = \sum_i \lambda_iæ•… tr(H^T L H) = tr(L) = \sum_{i=1}^M \lambda_i$\lambda_i$ä¸ºçŸ©é˜µ$L$çš„ç‰¹å¾å€¼ã€‚ æˆ‘ä»¬å†è¿›è¡Œç»´åº¦è§„çº¦ï¼Œå°†ç»´åº¦ä»$M$é™åˆ°$k_1$ï¼Œå³æ‰¾åˆ°$k_1$ä¸ªæœ€å°çš„ç‰¹å¾å€¼ä¹‹å’Œã€‚ N Cutæ¨å¯¼è¿‡ç¨‹ä¸RatioCutå®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯å°†åˆ†æ¯$|V_i|$æ¢æˆ$vol(V_i)$ NCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{vol(V_i)} cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) vol(V_{sub}) = \sum_{i \in V_{sub}} d_i è®¡ç®—æ­¥éª¤å¯¹äºç»™å®šçš„$N$ç»´æ•°æ®é›†$X = (x^{(1)}, x^{(2)}, â€¦, x^{(M)})$ï¼Œå°†å…¶åˆ’åˆ†ä¸º$K$ç±»$(C_1, â€¦, C_K)$ æ ¹æ®è¾“å…¥çš„ç›¸ä¼¼çŸ©é˜µçš„ç”Ÿæˆæ–¹å¼æ„å»ºæ ·æœ¬çš„ç›¸ä¼¼çŸ©é˜µ$S_{MÃ—M}$ï¼› æ ¹æ®ç›¸ä¼¼çŸ©é˜µ$S$æ„å»ºé‚»æ¥çŸ©é˜µ$W_{MÃ—M}$ï¼› æ„å»ºåº¦çŸ©é˜µ$D_{MÃ—M}$ï¼› è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ$L_{MÃ—M}$ï¼Œå¯è¿›è¡Œè§„èŒƒåŒ–$ L := D^{-1}L $ï¼› å¯¹$L$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£(EVD)ï¼Œå¾—åˆ°ç‰¹å¾å¯¹$ (\lambda_i, \alpha_i), i=1,â€¦,M $ï¼› æŒ‡å®šè¶…å‚æ•°$K_1$ï¼Œé€‰å–$K_1$ä¸ªæœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ç»„æˆçŸ©é˜µ$F_{MÃ—K_1}$ï¼Œå¹¶å°†å…¶æŒ‰è¡Œæ ‡å‡†åŒ–ï¼› ä»¥$F$çš„è¡Œå‘é‡ä½œä¸ºæ–°çš„æ ·æœ¬æ•°($k_1$ç»´ï¼Œè¿™é‡Œä¹Ÿæœ‰é™ç»´æ“ä½œ)è¿›è¡Œèšç±»ï¼Œåˆ’åˆ†ä¸º$K$ç±»ï¼Œå¯ä½¿ç”¨K-meansï¼› èšç±»ç»“æœå³ä¸ºè¾“å‡ºç»“æœ æ³¨æ„æ˜¯$K_1$ä¸ªæœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œåˆ«é—®æˆ‘ä¸ºä»€ä¹ˆçŸ¥é“ã€‚ã€‚ã€‚ ä»£ç @Github: Spectral Clustering1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class SpectralClustering(): &quot;&quot;&quot; Attributes: k: &#123;int&#125;, k &lt; n_samples sigma: &#123;float&#125; Notes: Steps: - similarity matrix [W_&#123;nÃ—n&#125;] - diagonal matrix [D_&#123;nÃ—n&#125;] is defined as D_&#123;ii&#125; = \begin&#123;cases&#125; \sum_j W_&#123;ij&#125; &amp; i \neq j \\ 0 &amp; i = j \end&#123;cases&#125; - Laplacian matrix [L_&#123;nÃ—n&#125;], Laplacian matrix is defined as L = D - W or L = D^&#123;-1&#125; (D - W) - EVD: L \alpha_i = \lambda_i \alpha_i - takes the eigenvector corresponding to the largest eigenvalue as B_&#123;nÃ—k&#125; = [\beta_1, \beta_2, ..., \beta_k] - apply K-Means to the row vectors of matrix B &quot;&quot;&quot; def __init__(self, k, n_clusters=2, sigma=1.0): self.kmeans = KMeans(n_clusters=n_clusters) self.k = k self.sigma = sigma def predict(self, X): n_samples = X.shape[0] # step 1 kernelGaussian = lambda z, sigma: np.exp(-0.5 * np.square(z/sigma)) W = np.zeros((n_samples, n_samples)) for i in range(n_samples): for j in range(i): W[i, j] = kernelGaussian(np.linalg.norm(X[i] - X[j]), self.sigma) W[j, i] = W[i, j] # step 2 D = np.diag(np.sum(W, axis=1)) # step 3 L = D - W L = np.linalg.inv(D).dot(L) # step 4 eigval, eigvec = np.linalg.eig(L) # step 5 order = np.argsort(eigval) eigvec = eigvec[:, order] beta = eigvec[:, :self.k] # step 6 self.kmeans.fit(beta) return self.kmeans.labels_ DBSCANDBSCAN(Density-Based Spatial Clustering of Applications with Noise)ï¼Œå…·æœ‰å™ªå£°çš„åŸºäºå¯†åº¦çš„èšç±»æ–¹æ³•ã€‚å‡å®šç±»åˆ«å¯ä»¥é€šè¿‡æ ·æœ¬åˆ†å¸ƒçš„ç´§å¯†ç¨‹åº¦å†³å®šã€‚åŒä¸€ç±»åˆ«çš„æ ·æœ¬ï¼Œä»–ä»¬ä¹‹é—´çš„ç´§å¯†ç›¸è¿çš„ã€‚ DBSCANå¯†åº¦èšç±»ç®—æ³• - åˆ˜å»ºå¹³Pinard - åšå®¢å›­ åŸç†å…ˆä»‹ç»å‡ ä¸ªå…³äºå¯†åº¦çš„æ¦‚å¿µ $\epsilon$-é‚»åŸŸ å¯¹äºæ ·æœ¬$x^{(i)}$ï¼Œå…¶$\epsilon$-é‚»åŸŸåŒ…å«æ ·æœ¬é›†ä¸­ä¸$x^{(i)}$è·ç¦»ä¸å¤§äº$\epsilon$çš„å­æ ·æœ¬é›†ï¼Œå…¶æ ·æœ¬ä¸ªæ•°è®°ä½œ$|N_{\epsilon}(x^{(i)})|$ã€‚ N_{\epsilon}(x^{(i)}) = \{ x^{(j)} | d_{ij} \leq \epsilon \} æ ¸å¿ƒå¯¹è±¡ å¯¹äºä»»ä¸€æ ·æœ¬$x^{(i)}$ï¼Œè‹¥å…¶$\epsilon$-é‚»åŸŸ$N_{\epsilon}(x^{(i)})$è‡³å°‘åŒ…å«$minPts$ä¸ªæ ·æœ¬ï¼Œåˆ™è¯¥æ ·æœ¬ä¸ºæ ¸å¿ƒå¯¹è±¡ã€‚å¦‚å›¾ï¼Œé€‰æ‹©è‹¥é€‰å–$\epsilon=5$ï¼Œåˆ™çº¢ç‚¹å‡ä¸ºæ ¸å¿ƒå¯¹è±¡ å¯†åº¦ç›´è¾¾ è‹¥æ ·æœ¬$x^{(j)} \in N_{\epsilon}(x^{(i)})$ï¼Œä¸”$x^{(i)}$ä¸ºæ ¸å¿ƒå¯¹è±¡ï¼Œåˆ™ç§°$x^{(j)}$ç”±$x^{(i)}$å¯†åº¦ç›´è¾¾ã€‚ä¸æ»¡è¶³å¯¹ç§°æ€§ï¼Œå³åä¹‹ä¸ä¸€å®šæˆç«‹ï¼Œé™¤é$x^{(j)}$ä¹Ÿä¸ºæ ¸å¿ƒå¯¹è±¡ã€‚å¦‚å›¾ï¼Œ$x^{(8)}$å¯ç”±$x^{(6)}$å¯†åº¦ç›´è¾¾ï¼Œè€Œåä¹‹$x^{(6)}$ä¸å¯ç”±$x^{(8)}$å¯†åº¦ç›´è¾¾ï¼Œå› ä¸º$x^{(8)}$ä¸ä¸ºæ ¸å¿ƒå¯¹è±¡ã€‚ å¯†åº¦å¯è¾¾ è‹¥å­˜åœ¨æ ·æœ¬åºåˆ—$p_1, p_2, â€¦, p_T$ï¼Œæ»¡è¶³$p_1 = x^{(i)}, p_T = x^{(j)}$ï¼Œä¸”$p_{t+1}$å¯ç”±$p_t$å¯†åº¦ç›´è¾¾ï¼Œä¹Ÿå°±æ˜¯è¯´$p_1, p_2, â€¦, p_{T-1}$å‡ä¸ºæ ¸å¿ƒå¯¹è±¡ï¼Œåˆ™ç§°$x^{(j)}$ç”±$x^{(i)}$å¯†åº¦å¯è¾¾ã€‚ä¹Ÿä¸æ»¡è¶³å¯¹ç§°æ€§ã€‚å¦‚å›¾ï¼Œ$x^{(4)}$å¯ç”±$x^{(1)}$å¯†åº¦å¯è¾¾ï¼Œè€Œ$x^{(2)}$ä¸å¯ç”±$x^{(4)}$å¯†åº¦å¯è¾¾ï¼Œå› ä¸º$x^{(4)}$ä¸ä¸ºæ ¸å¿ƒå¯¹è±¡ã€‚ å¯†åº¦ç›¸è¿ å­˜åœ¨æ ¸å¿ƒå¯¹è±¡$x^{(k)}$ï¼Œä½¿å¾—$x^{(i)}$ä¸$x^{(j)}$å‡ç”±$x^{(k)}$å¯†åº¦å¯è¾¾ï¼Œåˆ™ç§°$x^{(i)}$ä¸$x^{(j)}$å¯†åº¦ç›¸è¿ã€‚æ³¨æ„å¯†åº¦ç›¸è¿æ»¡è¶³å¯¹ç§°æ€§ã€‚å¦‚å›¾ï¼Œ$x^{(8)}$ä¸$x^{(4)}$å‡å¯ç”±$x^{(1)}$å¯†åº¦å¯è¾¾ï¼Œåˆ™$x^{(8)}$ä¸$x^{(4)}$å¯†åº¦ç›¸è¿ã€‚ è®¡ç®—æ€æƒ³DBSCANçš„èšç±»æ€æƒ³æ˜¯ï¼Œç”±å¯†åº¦å¯è¾¾å…³ç³»å¯¼å‡ºçš„æœ€å¤§å¯†åº¦ç›¸è¿çš„æ ·æœ¬é›†åˆï¼Œå³ä¸ºæˆ‘ä»¬æœ€ç»ˆèšç±»çš„ä¸€ä¸ªç°‡ï¼Œè¿™ä¸ªç°‡é‡Œå¯èƒ½åªæœ‰ä¸€ä¸ªæ ¸å¿ƒå¯¹è±¡ï¼Œä¹Ÿå¯èƒ½æœ‰å¤šä¸ªæ ¸å¿ƒå¯¹è±¡ï¼Œè‹¥æœ‰å¤šä¸ªï¼Œåˆ™ç°‡é‡Œçš„ä»»æ„ä¸€ä¸ªæ ¸å¿ƒå¯¹è±¡çš„$\epsilon$-é‚»åŸŸä¸­ä¸€å®šæœ‰ä¸€ä¸ªå…¶ä»–çš„æ ¸å¿ƒå¯¹è±¡ï¼Œå¦åˆ™è¿™ä¸¤ä¸ªæ ¸å¿ƒå¯¹è±¡æ— æ³•å¯†åº¦å¯è¾¾ã€‚ å¦å¤–ï¼Œè€ƒè™‘ä»¥ä¸‹ä¸‰ä¸ªé—®é¢˜ å™ªéŸ³ç‚¹ ä¸€äº›å¼‚å¸¸æ ·æœ¬ç‚¹æˆ–è€…è¯´å°‘é‡æ¸¸ç¦»äºç°‡å¤–çš„æ ·æœ¬ç‚¹ï¼Œè¿™äº›ç‚¹ä¸åœ¨ä»»ä½•ä¸€ä¸ªæ ¸å¿ƒå¯¹è±¡åœ¨å‘¨å›´ï¼Œè¿™äº›æ ·æœ¬ç‚¹æ ‡è®°ä¸ºå™ªéŸ³ç‚¹ï¼Œwith Noiseå°±æ˜¯è¿™ä¸ªæ„æ€ã€‚ è·ç¦»çš„åº¦é‡ ä¸€èˆ¬é‡‡ç”¨æœ€è¿‘é‚»æ€æƒ³ï¼Œé‡‡ç”¨æŸä¸€ç§è·ç¦»åº¦é‡æ¥è¡¡é‡æ ·æœ¬è·ç¦»ï¼Œæ¯”å¦‚æ¬§å¼è·ç¦»ã€‚è¿™å’ŒKNNç®—æ³•çš„æœ€è¿‘é‚»æ€æƒ³å®Œå…¨ç›¸åŒã€‚å¯¹åº”å°‘é‡çš„æ ·æœ¬ï¼Œå¯»æ‰¾æœ€è¿‘é‚»å¯ä»¥ç›´æ¥å»è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„è·ç¦»ï¼Œå¦‚æœæ ·æœ¬é‡è¾ƒå¤§ï¼Œåˆ™ä¸€èˆ¬é‡‡ç”¨KDTreeæˆ–è€…çƒæ ‘æ¥å¿«é€Ÿçš„æœç´¢æœ€è¿‘é‚»ã€‚ ç±»åˆ«é‡å¤æ—¶çš„åˆ¤åˆ« æŸäº›æ ·æœ¬å¯èƒ½åˆ°ä¸¤ä¸ªæ ¸å¿ƒå¯¹è±¡çš„è·ç¦»éƒ½å°äº$\epsilon$ï¼Œä½†æ˜¯è¿™ä¸¤ä¸ªæ ¸å¿ƒå¯¹è±¡å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸æ˜¯å¯†åº¦ç›´è¾¾ï¼Œåˆä¸å±äºåŒä¸€ä¸ªèšç±»ç°‡ï¼Œé‚£ä¹ˆå¦‚æœç•Œå®šè¿™ä¸ªæ ·æœ¬çš„ç±»åˆ«å‘¢ï¼Ÿ ä¸€èˆ¬æ¥è¯´ï¼Œæ­¤æ—¶DBSCANé‡‡ç”¨å…ˆæ¥ååˆ°ï¼Œå…ˆè¿›è¡Œèšç±»çš„ç±»åˆ«ç°‡ä¼šæ ‡è®°è¿™ä¸ªæ ·æœ¬ä¸ºå®ƒçš„ç±»åˆ«ã€‚ä¹Ÿå°±æ˜¯è¯´BDSCANä¸æ˜¯å®Œå…¨ç¨³å®šçš„ç®—æ³•ã€‚ ç®—æ³•æ­¥éª¤å¯¹äºç»™å®šçš„$N$ç»´æ•°æ®é›†$X = (x^{(1)}, x^{(2)}, â€¦, x^{(M)})$ï¼ŒæŒ‡å®šé‚»åŸŸå‚æ•°$(\epsilon, minPts)$ä¸æ ·æœ¬è·ç¦»åº¦é‡æ–¹å¼ï¼Œå°†å…¶åˆ’åˆ†ä¸º$K$ç±»ã€‚ æ£€æµ‹æ•°æ®åº“ä¸­å°šæœªæ£€æŸ¥è¿‡çš„å¯¹è±¡$p$ï¼Œå¦‚æœ$p$æœªè¢«å¤„ç†(å½’ä¸ºæŸä¸ªç°‡æˆ–è€…æ ‡è®°ä¸ºå™ªå£°)ï¼Œåˆ™æ£€æŸ¥å…¶é‚»åŸŸï¼š è‹¥åŒ…å«çš„å¯¹è±¡æ•°ä¸å°äº$minPts$ï¼Œå»ºç«‹æ–°ç°‡$C$ï¼Œå°†å…¶ä¸­çš„æ‰€æœ‰ç‚¹åŠ å…¥å€™é€‰é›†$N$ï¼› å¯¹å€™é€‰é›†$N$ä¸­æ‰€æœ‰å°šæœªè¢«å¤„ç†çš„å¯¹è±¡$q$ï¼Œæ£€æŸ¥å…¶é‚»åŸŸï¼š è‹¥è‡³å°‘åŒ…å«$minPts$ä¸ªå¯¹è±¡ï¼Œåˆ™å°†è¿™äº›å¯¹è±¡åŠ å…¥$N$ï¼› å¦‚æœ$q$æœªå½’å…¥ä»»ä½•ä¸€ä¸ªç°‡ï¼Œåˆ™å°†$q$åŠ å…¥$C$ï¼› é‡å¤æ­¥éª¤$2$ï¼Œç»§ç»­æ£€æŸ¥$N$ä¸­æœªå¤„ç†çš„å¯¹è±¡ï¼Œç›´åˆ°å½“å‰å€™é€‰é›†$N$ä¸ºç©ºï¼› é‡å¤æ­¥éª¤$1$-$3$ï¼Œç›´åˆ°æ‰€æœ‰å¯¹è±¡éƒ½å½’å…¥äº†æŸä¸ªç°‡æˆ–æ ‡è®°ä¸ºå™ªå£°ã€‚ é«˜æ–¯æ··åˆæ¨¡å‹(GMM)è¯¦æƒ…æŸ¥çœ‹EMç®—æ³• &amp; GMMæ¨¡å‹ã€‚ å±‚æ¬¡èšç±»(Hierarchical Clustering)å±‚æ¬¡èšç±»æ›´å¤šçš„æ˜¯ä¸€ç§æ€æƒ³ï¼Œè€Œä¸æ˜¯æ–¹æ³•ï¼Œé€šè¿‡ä»ä¸‹å¾€ä¸Šä¸æ–­åˆå¹¶ç°‡ï¼Œæˆ–è€…ä»ä¸Šå¾€ä¸‹ä¸æ–­åˆ†ç¦»ç°‡å½¢æˆåµŒå¥—çš„ç°‡ã€‚ä¾‹å¦‚ä¸Šé¢è®²åˆ°çš„DBSCANæœ€åç°‡çš„åˆå¹¶å°±æœ‰è¿™ç§æ€æƒ³ã€‚ å±‚æ¬¡çš„ç±»é€šè¿‡â€œæ ‘çŠ¶å›¾â€æ¥è¡¨ç¤ºï¼Œå¦‚ä¸‹ ä¸»è¦çš„æ€æƒ³æˆ–æ–¹æ³•æœ‰ä¸¤ç§ è‡ªåº•å‘ä¸Šçš„å‡èšæ–¹æ³•(agglomerative hierarchical clustering) å¦‚AGNESã€‚ è‡ªä¸Šå‘ä¸‹çš„åˆ†è£‚æ–¹æ³•(divisive hierarchical clustering) å¦‚DIANAã€‚ å›¾å›¢ä½“æ£€æµ‹(Graph Community Detection)ç•¥]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[EM & GMM]]></title>
    <url>%2F2018%2F11%2F12%2FEM-GMM%2F</url>
    <content type="text"><![CDATA[EMç®—æ³•Expectation Maximization Algorithmï¼Œæ˜¯ Dempster, Laind, Rubin äº 1977 å¹´æå‡ºçš„æ±‚å‚æ•°æå¤§ä¼¼ç„¶ä¼°è®¡çš„ä¸€ç§æ–¹æ³•ï¼Œå®ƒå¯ä»¥ä»éå®Œæ•´æ•°æ®é›†ä¸­å¯¹å‚æ•°è¿›è¡Œ MLE ä¼°è®¡ï¼Œæ˜¯ä¸€ç§éå¸¸ç®€å•å®ç”¨çš„å­¦ä¹ ç®—æ³•ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¹¿æ³›åœ°åº”ç”¨äºå¤„ç†ç¼ºæŸæ•°æ®ï¼Œæˆªå°¾æ•°æ®ï¼Œå¸¦æœ‰å™ªå£°ç­‰æ‰€è°“çš„ä¸å®Œå…¨æ•°æ®ã€‚ å¼•ä¾‹ï¼šå…ˆæŒ–ä¸ªå‘ç»™å‡ºæèˆªã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹çš„ä¸‰ç¡¬å¸æ¨¡å‹ä¾‹å­ï¼Œå‡è®¾æœ‰$3$æšç¡¬å¸$A, B, C$ï¼Œå„è‡ªå‡ºç°æ­£é¢çš„æ¦‚ç‡åˆ†åˆ«ä¸º$\pi, p, q$ï¼Œå…ˆè¿›è¡Œå¦‚ä¸‹å®éªŒï¼šå…ˆæŠ•æ·ç¡¬å¸$A$ï¼Œè‹¥ç»“æœä¸ºæ­£é¢ï¼Œåˆ™é€‰æ‹©ç¡¬å¸$B$æŠ•æ·ä¸€æ¬¡ï¼Œå¦åˆ™é€‰æ‹©$C$ï¼Œè®°å½•æŠ•æ·ç»“æœå¦‚ä¸‹ 1, 1, 0, 1, 0, 0, 1, 0, 1, 1åªèƒ½è§‚æµ‹åˆ°å®éªŒç»“æœï¼Œè€ŒæŠ•æ·è¿‡ç¨‹æœªçŸ¥ï¼Œå³ç¡¬å¸$A$çš„æŠ•æ·ç»“æœæœªçŸ¥ï¼Œç°æ¬²ä¼°è®¡ä¸‰æšç¡¬å¸çš„å‚æ•°$\pi, p, q$ã€‚ è§£ï¼šæ ¹æ®é¢˜æ„å¯ä»¥å¾—åˆ°ä¸‰ä¸ªéšæœºå˜é‡$X_1, X_2, X_3$çš„æ¦‚ç‡åˆ†å¸ƒå¦‚ä¸‹ P(X_1) = \pi ^ {X_1} (1 - \pi) ^ {1 - X_1} P(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2} P(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}å®šä¹‰éšæœºå˜é‡$X$è¡¨ç¤ºè§‚æµ‹ç»“æœä¸ºæ­£é¢ï¼Œç”±å…¨æ¦‚ç‡å…¬å¼å¯ä»¥å¾—åˆ° P(X) = P(X|X_1)P(X_1) + P(X|\overline{X_1})P(\overline{X_1}) = \pi p + (1 - \pi) q P(\overline{X}) = P(\overline{X}|X_1)P(X_1) + P(\overline{X}|\overline{X_1})P(\overline{X_1}) = \pi (1 - p) + (1 - \pi) (1 - q)å³ P(X) = [\pi p + (1 - \pi) q] ^ {X} [\pi (1 - p) + (1 - \pi) (1 - q)] ^ {1 - X}åˆ©ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæœ‰ \log L(D | \theta) = 6 \log [\pi p + (1 - \pi) q] + 4 \log [\pi (1 - p) + (1 - \pi) (1 - q)]è‡³æ­¤ï¼Œæˆ‘ä»¬ä¸€å®šèƒ½æƒ³åˆ°é€šè¿‡æ±‚ä¼¼ç„¶å‡½æ•°æå€¼æ¥æ±‚è§£å‚æ•° \frac{âˆ‚ }{âˆ‚ \pi} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{âˆ‚ }{âˆ‚ p} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{âˆ‚ }{âˆ‚ q} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0ä½†æ˜¯å¥½åƒå‡ºäº†é—®é¢˜ï¼Œå¹¶ä¸èƒ½æ±‚è§£ï¼Œæ‰€ä»¥æˆ‘ä»¬å¼•å…¥EMç®—æ³•è¿­ä»£æ±‚è§£ã€‚ æ¨å¯¼ä»¥$x^{(i)}$è¡¨ç¤ºè®­ç»ƒæ•°æ®ï¼Œ$w_k$è¡¨ç¤ºç±»åˆ«ï¼Œè®¾å½“å‰è¿­ä»£å‚æ•°ä¸º$\theta^{(t)}$ï¼Œåˆ™ä¸‹ä¸€æ¬¡è¿­ä»£åº”æœ‰ \theta^{(t+1)} = \arg \max \sum_i \log P(x^{(i)}|\theta) \tag{1}ç”±è¾¹ç¼˜æ¦‚ç‡å…¬å¼ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \tag{2} $P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)}|x^{(i)}, \theta)$è‡³æ­¤å·²å¾—å‡ºå¼•ä¾‹ä¸­çš„è¡¨è¾¾å¼ï¼Œå…¶ä¸­$P(w_k^{(i)}|x^{(i)}, \theta)$ä¸$P(x^{(i)} | w_k^{(i)}, \theta)$å‡æœªçŸ¥ï¼Œè€Œé€šè¿‡æ±‚æå€¼ä¸èƒ½è§£å¾—å‚æ•°ã€‚ æˆ‘ä»¬å¼•å…¥è¿­ä»£å‚æ•°$\theta^{(t)}$ï¼Œå³ç¬¬$t$æ¬¡è¿­ä»£æ—¶çš„å‚æ•°$\theta$ï¼Œè¯¥å‚æ•°ä¸ºå·²çŸ¥å˜é‡ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)} | \theta^{(t)})} {P(w_k^{(i)} | \theta^{(t)})} $P(w_k^{(i)}|\theta^{(t)})$è¡¨ç¤ºæ ·æœ¬$x^{(i)}$ç±»åˆ«ä¸º$w_k^{(i)}$çš„æ¦‚ç‡ï¼Œæ³¨æ„ä¸Šæ ‡ã€‚ å¼•å…¥Jensenä¸ç­‰å¼ï¼š For a real convex function $\varphi$, numbers $x_1, â€¦, x_n$ in its domain, and positive weights $a_i$, Jensenâ€™s inequality can be stated as: \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \leq \frac{\sum a_i \varphi(x_i)}{\sum a_i}and the inquality is reversed if $\varphi$ is concave, which is \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \geq \frac{\sum a_i \varphi(x_i)}{\sum a_i}Equality holds if and only if $x_1 = â€¦ = x_n$ or $\varphi$ is linear. $\log(Â·)$ä¸ºå‡¹å‡½æ•°(concave)ï¼Œä¸”æ»¡è¶³ \sum_k P(w_k^{(i)} | \theta^{(t)}) = 1æ‰€ä»¥æœ‰ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)}|\theta^{(t)})} {P(w_k^{(i)}|\theta^{(t)})} \geq \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{3}æ­¤æ—¶æˆ‘ä»¬å¾—åˆ°ä¼¼ç„¶å‡½æ•°$\sum_i \log P(x^{(i)}|\theta)$çš„ä¸€ä¸ªä¸‹ç•Œï¼Œä½†å¿…é¡»ä¿è¯è¿™ä¸ªä¸‹ç•Œæ˜¯ç´§çš„ï¼Œä¹Ÿå°±æ˜¯è‡³å°‘æœ‰ç‚¹èƒ½ä½¿ç­‰å·æˆç«‹ ç”±Jensenä¸ç­‰å¼ï¼Œå½“ä¸”ä»…å½“$ P(x^{(i)}, w_k^{(i)}|\theta)=C $æ—¶å–ç­‰å· å®šä¹‰ L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) - P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)})å…¶ä¸­ç¬¬ä¸€é¡¹å³æœŸæœ› E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) \tag{4}ç¬¬äºŒé¡¹ä¸º$P(w | X, \theta^{(t)})$çš„ä¿¡æ¯ç†µ H[P(w | X, \theta^{(t)})] = - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5}å³ L(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] + H[P(w | X, \theta^{(t)})] \tag{E-step} æ³¨æ„åˆ°$H[P(w | X, \theta^{(t)})]$é¡¹ä¸ºå¸¸æ•°ï¼Œæ•…ä¹Ÿå¯è®¾ Q(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] ä»£å›$(1)$ï¼Œå¾—åˆ°ä¼˜åŒ–ç›®æ ‡ \theta^{(t+1)} = \arg \max L(\theta|\theta^{(t)}) \tag{M-step}æˆ‘ä»¬éœ€è¦ä¸æ–­æœ€å¤§åŒ–$L(\theta | \theta^{(t)})$æ¥ä¸æ–­ä¼˜åŒ–ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„EMç®—æ³•ï¼ŒE-stepæ˜¯æŒ‡æ±‚å‡ºæœŸæœ›ï¼ŒM-stepæ˜¯æŒ‡è¿­ä»£æ›´æ–°å‚æ•° ä¼ªä»£ç å¦‚ä¸‹123456According to prior knowledge set $\theta$Repeat until convergence&#123; E-step: The expectation of hidden variables M-step: Finding the maximum of likelihood function&#125; å®é™…ä¸Šï¼Œä»è¾¹ç¼˜æ¦‚ç‡ä¸æ¡ä»¶æ¦‚ç‡å…¥æ‰‹ \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta) \geq \sum_i \sum_k P(w_k^{(i)} | \theta) \log P(x^{(i)} | w_k^{(i)}, \theta) \tag{Jensen inequality} = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)}è€Œç”±$(3)$ï¼Œå¼•å…¥è¿­ä»£å˜é‡å¯ä»¥å¾—åˆ° \sum_i \log P(x^{(i)}|\theta) \geq L(\theta|\theta^{(t)})å…¶ä¸­ L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}åˆ™ \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta)}è€Œç”±KLæ•£åº¦( Kullbackâ€“Leibler divergence)(åˆç§°ç›¸å¯¹ç†µ(relative entropy))å®šä¹‰ D(P||Q) = \sum P(x) \log \frac{P(x)}{Q(x)} å¯çŸ¥ \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = D\left[ P(w_k^{(i)}|\theta^{(t)}) || P(w_k^{(i)}|\theta) \right]å³è¿­ä»£çš„$P(w_k^{(i)}|\theta^{(t)})$ä¸çœŸå®çš„$P(w_k^{(i)}|\theta)$ä¹‹é—´çš„ç›¸å¯¹ç†µï¼ è¿™é‡Œå…³äºK-Læ•£åº¦çš„å›°æ‰°äº†$N$ä¹…ï¼Œç»ˆäºæå‡ºæ¥äº†ã€‚ å¼•ä¾‹çš„æ±‚è§£ $Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$ æ­¤é¢˜ä¸­ P(w_k|\pi) = \pi^{w_k}(1-\pi)^{1-w_k} P(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}} P(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}} $E-step$ Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) \log P(x^{(i)}, w_k^{(i)} | \pi, p, q) å…ˆæ±‚$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$ï¼Œå³ç¬¬ä¸€æ¬¡æŠ•æ·ç»“æœä¸º$w_k$çš„æ¦‚ç‡ P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_k}} {\sum_j \left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_j} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_j}} å³ \begin{cases} P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\ P(w_2^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \end{cases} è®° \mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})\mu_2^{(i)} = 1 - \mu_1^{(i)} æ³¨æ„$w^{(i)}_k$ä¸Šæ ‡^{(i)} å†æ±‚$P(x^{(i)}, w_k^{(i)} | \pi, p, q)$ï¼Œå·²çŸ¥ P(x^{(i)}, w_k^{(i)} | \pi, p, q) = P(x^{(i)} | w_k^{(i)}, \pi, p, q) P(w_k^{(i)} | \pi, p, q) æ‰€ä»¥ P(x^{(i)}, w_k^{(i)} | \pi, p,q) = \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} ç»¼ä¸Š Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) = \sum_i \sum_{k=1}^2 \mu^{(i)}_k \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} = \sum_i \mu_1^{(i)} \log \pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \mu_1^{(i)}) \log (1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}} $M-step$ $\frac{âˆ‚Q}{âˆ‚\pi} = 0$ \frac{âˆ‚Q}{âˆ‚\pi} = \sum_i \mu_1^{(i)} \frac {p^{x^{(i)}}(1-p)^{1-x^{(i)}}} {\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} + (1 - \mu_1^{(i)}) \frac {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}} {(1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}} = \sum_i \frac{\mu_1^{(i)}}{\pi} + \frac{\mu_1^{(i)} - 1}{1 - \pi} = \sum_i \frac{\mu_1^{(i)} - \pi}{\pi(1 - \pi)} = \frac{\sum_i \mu_1^{(i)} - n\pi}{\pi(1 - \pi)} = 0 \Rightarrow \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)} $\frac{âˆ‚Q}{âˆ‚p} = 0$ \frac{âˆ‚Q}{âˆ‚p} = \sum_i \mu_1^{(i)} \left[ \frac{x^{(i)}}{p} - \frac{1 - x^{(i)}}{1 - p} \right] = \frac{1}{p(1 - p)} \sum_i \mu_1^{(i)} (x^{(i)} - p) = \frac{1}{p(1 - p)} \left[ \sum_i \mu_1^{(i)} x^{(i)} - p \sum_i \mu_1^{(i)} \right] = 0 \Rightarrow p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}} $\frac{âˆ‚Q}{âˆ‚q} = 0$ \frac{âˆ‚Q}{âˆ‚q} = \sum_i (1 - \mu_1^{(i)}) \left[ \frac{x^{(i)}}{q} - \frac{1 - x^{(i)}}{1 - q} \right] = \frac{1}{q(1 - q)} \sum_i (1 - \mu_1^{(i)}) (x^{(i)} - q) = \frac{1}{q(1 - q)} \left[ \sum_i (1 - \mu_1^{(i)}) x^{(i)} - q \sum_i (1 - \mu_1^{(i)}) \right] = 0 \Rightarrow q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})} å¤šæ¬¡è¿­ä»£å³å¯æ±‚è§£ï¼Œç»ˆæ­¢æ¡ä»¶å¯è®¾ç½®ä¸º || \theta^{(t+1)} - \theta^{(t)} || < \epsilonæˆ– ||Q(\theta^{(t+1)} | \theta^{(t)}) - Q(\theta^{(t)} |\theta^{(t)})|| < \epsilonGMMæ¨¡å‹Gaussian Mixture Modelï¼Œæ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œå¸¸ç”¨äºèšç±»ã€‚å½“èšç±»é—®é¢˜ä¸­å„ä¸ªç±»åˆ«çš„å°ºå¯¸ä¸åŒã€èšç±»é—´æœ‰ç›¸å…³å…³ç³»çš„æ—¶å€™ï¼Œå¾€å¾€ä½¿ç”¨GMMæ›´åˆé€‚ã€‚å¯¹ä¸€ä¸ªæ ·æœ¬æ¥è¯´ï¼ŒGMMå¾—åˆ°çš„æ˜¯å…¶å±äºå„ä¸ªç±»çš„æ¦‚ç‡(é€šè¿‡è®¡ç®—åéªŒæ¦‚ç‡å¾—åˆ°)ï¼Œè€Œä¸æ˜¯å®Œå…¨çš„å±äºæŸä¸ªç±»ï¼Œè¿™ç§èšç±»æ–¹æ³•è¢«æˆä¸ºè½¯èšç±»ã€‚ä¸€èˆ¬è¯´æ¥ï¼Œ ä»»æ„å½¢çŠ¶çš„æ¦‚ç‡åˆ†å¸ƒéƒ½å¯ä»¥ç”¨å¤šä¸ªé«˜æ–¯åˆ†å¸ƒå‡½æ•°å»è¿‘ä¼¼ï¼Œå› è€Œï¼ŒGMMçš„åº”ç”¨ä¹Ÿæ¯”è¾ƒå¹¿æ³›ã€‚ é«˜æ–¯æ··åˆæ¨¡å‹ï¼ŒæŒ‡å…·æœ‰å¦‚ä¸‹å½¢å¼çš„æ¦‚ç‡åˆ†å¸ƒæ¨¡å‹ï¼š P(x|\mu_k, \Sigma_k) = \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)å…¶ä¸­ $\pi_k(0 \leq \pi_k \leq 1)$æ˜¯ç³»æ•°ï¼Œä¸”$\sum_k \pi_k = 1$ $N(x|\mu_k, \Sigma_k)$ä¸ºé«˜æ–¯å¯†åº¦å‡½æ•° N(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) \right] å³å¤šä¸ªé«˜æ–¯åˆ†å¸ƒå åŠ å‡ºæ¥çš„ç©æ„ï¼› ç°åœ¨æˆ‘ä»¬éœ€è¦æ±‚å–ç³»æ•°$\pi_k$åŠé«˜æ–¯æ¨¡å‹çš„å‚æ•°$(\mu_k, \Sigma_k)$ï¼› ä¸K-Meansç­‰èšç±»æ–¹æ³•åŒºåˆ«æ˜¯ï¼ŒGMMæ±‚å‡ºçš„æ˜¯è¿ç»­çš„åˆ†å¸ƒæ¨¡å‹ï¼Œå¯è®¡ç®—å‡ºâ€œå½’å±äºâ€å“ªä¸€ç±»çš„æ¦‚ç‡ã€‚ æ¨å¯¼ \log P(X|\pi, \mu, \Sigma) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \Sigma_k) s.t. \sum_k \pi_k = 1æš´åŠ›æ±‚è§£ä»¥$1$ç»´é«˜æ–¯åˆ†å¸ƒä¸ºä¾‹ N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}}æ„é€ æ‹‰æ ¼æœ—æ—¥(Lagrange)å‡½æ•° L(\pi, \mu, \sigma^2) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \sigma_k^2) + \lambda \left(\sum_k \pi_k - 1 \right) \tag{5} \begin{cases} \frac{âˆ‚}{âˆ‚\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{âˆ‚}{âˆ‚\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{âˆ‚}{âˆ‚\mu_k}N(x^{(i)}|\mu_k, \sigma_k^2) \\ \frac{âˆ‚}{âˆ‚\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{âˆ‚}{âˆ‚\sigma_k^2}N(x^{(i)}|\mu_k, \sigma_k^2) \end{cases} \tag{6}å…¶ä¸­ \frac{âˆ‚}{âˆ‚\mu_k} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{x-\mu_k}{\sigma_k^2} = N(x|\mu_k, \sigma_k^2) Â· \frac{x-\mu_k}{\sigma_k^2} \frac{âˆ‚}{âˆ‚\sigma_k^2} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) $\frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k}\right) = - \frac{\sigma_k^{-3}}{2}; \frac{âˆ‚}{âˆ‚\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) = - \frac{1}{\sigma_k^4}$ = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(- \frac{\sigma_k^{-3}}{2}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \left(- \frac{1}{\sigma_k^4}\right) = N(x|\mu_k, \sigma_k^2) \left[ \frac{(x - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2}ä»£å›$(6)$å¯ä»¥å¾—åˆ° \begin{cases} \frac{âˆ‚}{âˆ‚\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{âˆ‚}{âˆ‚\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{x^{(i)}-\mu_k}{\sigma_k^2} \\ \frac{âˆ‚}{âˆ‚\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2} \end{cases} \tag{7}ä»¤ \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \tag{8} é€šä¿—ç†è§£ï¼š$\gamma^{(i)}_k$è¡¨ç¤ºæ ·æœ¬$x^{(i)}$ä¸­æ¥è‡ªç±»åˆ«$w_k$çš„â€œè´¡çŒ®ç™¾åˆ†æ¯”â€ ä»¤$\frac{âˆ‚}{âˆ‚\mu_k} \log P(X|\pi, \mu, \sigma^2) = 0$ï¼Œæ•´ç†å¾—åˆ° \sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) = 0 \Rightarrow \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} ä»¤$\frac{âˆ‚}{âˆ‚\sigma_k^2} \log P(X|\pi, \mu, \sigma^2) = 0$ï¼Œæ•´ç†å¾—åˆ° \sum_i \gamma^{(i)}_k \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] = 0 \Rightarrow \sigma_k^2 = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k)^2}{\sum_i \gamma^{(i)}_k} å¯¹äº$\frac{âˆ‚}{âˆ‚\pi_k} \log P(X|\pi, \mu, \sigma^2) = 0$ï¼Œéœ€è¦åšä¸€ç‚¹å¤„ç† ä¸¤è¾¹åŒä¹˜$\pi_k$ï¼Œå¾—åˆ° \sum_i \gamma^{(i)}_k = - \lambda \pi_k \tag{9} ç„¶åä¸¤è¾¹å¯¹$k$ä½œç´¯åŠ  \sum_k \sum_i \gamma^{(i)}_k = - \lambda \sum_k \pi_k $\sum_k \sum_i \gamma^{(i)}_k = \sum_i \sum_k \gamma^{(i)}_k = N, \sum_k \pi_k = 1$ N = - \lambda æˆ– \lambda = -N \tag{10} ä»£å›$(9)$ï¼Œå¾—åˆ° \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N} ç»¼ä¸Šï¼Œæˆ‘ä»¬å¾—åˆ°$4$ä¸ªç”¨äºè¿­ä»£çš„è®¡ç®—å¼ï¼Œå°†å…¶æ¨å¹¿è‡³å¤šç»´å³ \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)} \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} \Sigma_k = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)}_k} \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}ç”¨EMç®—æ³•æ±‚è§£ $Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$ Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k) $ M-step $ P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)} = \gamma^{(i)}_k P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k) = P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k) P(w_k^{(i)}|\mu_k, \Sigma_k) = \pi_k N(x^{(i)}|\mu_k, \Sigma_k) æ•… Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k \gamma^{(i)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k) é€šè¿‡æ±‚è§£æå€¼å¯å¾—åˆ°ä¸$\underline{æš´åŠ›æ±‚è§£}$ä¸€æ ·çš„ç­‰å¼ï¼Œå³ \gamma^{(i)(t)}_k = \frac{\pi^{(t)}_k N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})} \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k} \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k} \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N} ä¼ªä»£ç ä¸º 123456789101112According to prior knowledge set \pi^&#123;(t)&#125;(n_clusters,) \mu^&#123;(t)&#125;(n_clusters, n_features) \Sigma^&#123;(t)&#125;(n_clusters, n_features, n_features)Repeat until convergence&#123; # E-step: calculate \gamma^&#123;(t)&#125; \gamma(n_samples, n_clusters) # M-step: update \pi, \mu, \Sigma \pi^&#123;(t+1)&#125;(n_clusters,) \mu^&#123;(t+1)&#125;(n_clusters, n_features) \Sigma^&#123;(t+1)&#125;(n_clusters, n_features, n_features)&#125; åˆå§‹ç‚¹çš„é€‰æ‹©å¯ä»¥éšæœºé€‰æ‹©ï¼Œä¹Ÿå¯ä½¿ç”¨K-Means GMMç®—æ³•æ”¶æ•›è¿‡ç¨‹å¦‚ä¸‹ ä»£ç @Github: GMM123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class GMM(): &quot;&quot;&quot; Gaussian Mixture Model Attributes: n_clusters &#123;int&#125; prior &#123;ndarray(n_clusters,)&#125; mu &#123;ndarray(n_clusters, n_features)&#125; sigma &#123;ndarray(n_clusters, n_features, n_features)&#125; &quot;&quot;&quot; def __init__(self, n_clusters): self.n_clusters = n_clusters self.prior = None self.mu = None self.sigma = None def fit(self, X, delta=0.01): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; delta &#123;float&#125; Notes: - Initialize with k-means &quot;&quot;&quot; (n_samples, n_features) = X.shape # initialize with k-means clf = KMeans(n_clusters=self.n_clusters) clf.fit(X) self.mu = clf.cluster_centers_ self.prior = np.zeros(self.n_clusters) self.sigma = np.zeros((self.n_clusters, n_features, n_features)) for k in range(self.n_clusters): X_ = X[clf.labels_==k] self.prior[k] = X_.shape[0] / X_.shape[0] self.sigma[k] = np.cov(X_.T) while True: mu_ = self.mu.copy() # E-step: updata gamma gamma = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): denominator = 0 for j in range(self.n_clusters): post = self.prior[k] *\ multiGaussian(X[i], self.mu[j], self.sigma[j]) denominator += post if j==k: numerator = post gamma[i, k] = numerator/denominator # M-step: updata prior, mu, sigma for k in range(self.n_clusters): sum1 = 0 sum2 = 0 sum3 = 0 for i in range(n_samples): sum1 += gamma[i, k] sum2 += gamma[i, k] * X[i] x_ = np.reshape(X[i] - self.mu[k], (n_features, 1)) sum3 += gamma[i, k] * x_.dot(x_.T) self.prior[k] = sum1 / n_samples self.mu[k] = sum2 / sum1 self.sigma[k] = sum3 / sum1 # to stop mu_delta = 0 for k in range(self.n_clusters): mu_delta += nl.norm(self.mu[k] - mu_[k]) print(mu_delta) if mu_delta &lt; delta: break return self.prior, self.mu, self.sigma def predict_proba(self, X): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples, n_clusters)&#125; &quot;&quot;&quot; (n_samples, n_features) = X.shape y_pred_proba = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): y_pred_proba[i, k] = self.prior[k] *\ multiGaussian(X[i], self.mu[k], self.sigma[k]) return y_pred_proba def predict(self, X): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples,)&#125; &quot;&quot;&quot; y_pred_proba = self.predict_proba(X) return np.argmax(y_pred_proba, axis=1)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Augmentation]]></title>
    <url>%2F2018%2F11%2F02%2FData-Augmentation%2F</url>
    <content type="text"><![CDATA[â€œæœ‰æ—¶å€™ä¸æ˜¯ç”±äºç®—æ³•å¥½èµ¢äº†ã€‚è€Œæ˜¯ç”±äºæ‹¥æœ‰å¾ˆå¤šå…¶å®ƒçš„æ•°æ®æ‰èµ¢äº†ã€‚â€ æ•°æ®é›†æ‰©å¢åœ¨æ·±åº¦å­¦ä¹ ä¸­,å¾ˆå¤šè®­ç»ƒæ•°æ®æ„å‘³ç€èƒ½å¤Ÿç”¨æ›´æ·±çš„ç½‘ç»œï¼Œè®­ç»ƒå‡ºæ›´å¥½çš„æ¨¡å‹ã€‚æ—¢ç„¶è¿™æ ·ï¼Œæ”¶é›†å¾ˆå¤šå…¶å®ƒçš„æ•°æ®ä¸å³å¯å•¦ï¼Ÿå‡è®¾èƒ½å¤Ÿæ”¶é›†å¾ˆå¤šå…¶å®ƒèƒ½å¤Ÿç”¨çš„æ•°æ®å½“ç„¶å¥½ï¼Œæ¯”å¦‚ImageNetä¸Šå›¾åƒæ•°æ®é‡å·²è¾¾åˆ°$1400$ä¸‡å¼ ï¼Œå¯æ˜¯éå¸¸å¤šæ—¶å€™ï¼Œæ”¶é›†å¾ˆå¤šå…¶å®ƒçš„æ•°æ®æ„å‘³ç€é¡»è¦è€—è´¹å¾ˆå¤šå…¶å®ƒçš„äººåŠ›ç‰©åŠ›ï¼Œè¿™å°±éœ€è¦ä½¿ç”¨ä¸€å®šçš„æ–¹æ³•æ‰©å¢æ•°æ®é›†ã€‚ å›¾åƒæ‰©å¢å¤§éƒ¨åˆ†å€ŸåŠ©OpenCVåº“ï¼Œè¿™é‡Œæ¨èä¸€ä½å­¦é•¿çš„åšå®¢ï¼Œæ•´ç†äº†å¤§é‡çš„OpenCVä½¿ç”¨æ–¹æ³•. Ex2tronâ€™s Blog TensorFlowä¹Ÿæä¾›ç›¸åº”å›¾åƒå¤„ç†æ–¹æ³•Module: tf.image | TensorFlow éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ‰©å¢è¿‡ç¨‹ä¸­ï¼Œéœ€æ³¨æ„å›¾åƒæ•°æ®ç±»å‹ï¼Œå¯ä»¥å°†æ•°æ®å½’ä¸€åŒ–åˆ°$(0, 1)$é—´å†è¿›è¡Œå¤„ç† ç¿»è½¬12345678def flip(image): &quot;&quot;&quot; Parameters: image &#123;ndarray(H, W, C)&#125; &quot;&quot;&quot; rand_var = np.random.random() image = image[:, ::-1, :] if rand_var &gt; 0.5 else image return image æ—‹è½¬123456789101112def rotate(image, degree): &quot;&quot;&quot; Parameters: image &#123;ndarray(H, W, C)&#125; degree &#123;float&#125; &quot;&quot;&quot; (h, w) = image.shape[:2] center = (w // 2, h // 2) random_angel = np.random.randint(-degree, degree) M = cv2.getRotationMatrix2D(center, random_angel, 1.0) image = cv2.warpAffine(image, M, (w, h)) return image å™ªå£°å¯æ‰‹åŠ¨å®ç°ï¼Œå¦‚æ¤’ç›å™ªå£°ä»£ç å¦‚ä¸‹12345678910111213141516def saltnoise(image, salt=0.0): &quot;&quot;&quot; add salt &amp; pepper and gaussian noise Parameters: image &#123;ndarray(H, W, C)&#125; salt &#123;float(0, 1)&#125; number of salt pixel = salt*h*w Notes: TODO: gaussain noise &quot;&quot;&quot; (h, w) = image.shape[:2] n_salt = int(salt * h * w) for n in range(n_salt): hr = np.random.randint(0, h) wr = np.random.randint(0, w) issalt = (np.random.rand(1) &gt; 0.5) image[hr, wr] = 255 if issalt else 0 return image ä¹Ÿå¯è°ƒç”¨scikit-imageåº“ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œskimage.util.random_noise()ä¼šå°†åŸå›¾æ•°æ®è½¬æ¢ä¸º$(0, 1)$é—´çš„æµ®ç‚¹æ•°1234567891011121314151617def noise(image, gaussian, salt, seed=None): &quot;&quot;&quot; add noise to image TODO Parameters: image &#123;ndarray(H, W, C)&#125; gaussian &#123;bool&#125;: salt &#123;bool&#125;: Notes: Function to add random noise of various types to a floating-point image. &quot;&quot;&quot; dtype = image.dtype if gaussian: image = skimage.util.random_noise(image, mode=&apos;gaussian&apos;, seed=seed) if salt: image = skimage.util.random_noise(image, mode=&apos;s&amp;p&apos;, seed=seed) image = (image * 255).astype(dtype) return image äº®åº¦ä¸å¯¹æ¯”åº¦è°ƒæ•´è€ƒè™‘åˆ°æ•°æ®æº¢å‡ºï¼Œå…ˆè½¬æ¢ä¸ºæ•´å½¢æ•°æ®ï¼Œå†é™åˆ¶å…¶å€¼åˆ°$[0, 255]$ æ³¨æ„æ•°æ®ç±»å‹ 1234567891011def brightcontrast(image, brtadj=0, cstadj=1.0): &quot;&quot;&quot; adjust bright and contrast value Parameters: image &#123;ndarray(H, W, C)&#125; brtadj &#123;int&#125; if true, adjust bright cstadj &#123;float&#125; if true, adjust contrast &quot;&quot;&quot; dtype = image.dtype image = image.astype(&apos;int&apos;)*cstadj + brtadj image = np.clip(image, 0, 255).astype(dtype) return image æŠ•å°„å˜æ¢1234567891011121314151617181920212223242526def perspective(image, prop): &quot;&quot;&quot; é€å°„å˜æ¢ Parameters: image &#123;ndarray(H, W, C)&#125; prop &#123;float&#125;: åœ¨å››ä¸ªé¡¶ç‚¹å¤šå¤§çš„æ–¹æ ¼å†…é€‰å–æ–°é¡¶ç‚¹ï¼Œæ–¹æ ¼å¤§å°ä¸º(H*prop, W*prop) Notes: åœ¨å››ä¸ªé¡¶ç‚¹å‘¨å›´éšæœºé€‰å–æ–°çš„ç‚¹è¿›è¡Œä»¿å°„å˜æ¢ï¼Œå››ä¸ªç‚¹å¯¹åº”å·¦ä¸Šã€å³ä¸Šã€å·¦ä¸‹ã€å³ä¸‹ &quot;&quot;&quot; (h, w) = image.shape[:2] ptsrc = np.zeros(shape=(4, 2)) ptdst = np.array([[0, 0], [0, w], [h, 0], [h, w]]) for i in range(4): hr = np.random.randint(0, int(h*prop)) wr = np.random.randint(0, int(w*prop)) if i == 0: ptsrc[i] = np.array([hr, wr]) elif i == 1: ptsrc[i] = np.array([hr, w - wr]) elif i == 2: ptsrc[i] = np.array([h - hr, wr]) elif i == 3: ptsrc[i] = np.array([h - hr, w - wr]) M = cv2.getPerspectiveTransform(ptsrc.astype(&apos;float32&apos;), ptdst.astype(&apos;float32&apos;)) image = cv2.warpPerspective(image, M, (w, h)) return image]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[äºŒæ¬¡å…¥å‘raspberry-pi]]></title>
    <url>%2F2018%2F10%2F29%2F%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi%2F</url>
    <content type="text"><![CDATA[å‰è¨€è·ä¸Šä¸€æ¬¡æ­å»ºæ ‘è“æ´¾å¹³å°å·²ç»ä¸¤å¹´äº†ï¼Œä¿å­˜çš„é•œåƒå‡ºäº†é—®é¢˜ï¼Œé‡æ–°æ­å»ºä¸€ä¸‹ã€‚ ç³»ç»Ÿä¸‹è½½ä»å®˜ç½‘ä¸‹è½½æ ‘è“æ´¾ç³»ç»Ÿé•œåƒï¼Œæœ‰ä»¥ä¸‹å‡ ç§å¯é€‰ Raspberry Pi â€” Teach, Learn, and Make with Raspberry Pi Raspbian &amp; Raspbian Liteï¼ŒåŸºäºDebian Noobs &amp; Noobs Lite Ubuntu MATE Snappy Ubuntu Core Windows 10 IOT å…¶ä½™ä¸å¤ªäº†è§£ï¼Œä¹‹å‰å®‰è£…çš„æ˜¯Raspbianï¼Œå¯¹äºDebianå„ç§ä¸é€‚ï¼Œæ¢ä¸Šç•Œé¢ä¼˜é›…çš„Ubuntu Mateç©ä¸€ä¸‹è€è€å®å®ç©Raspbianï¼Œç¬‘è„¸:-) å®‰è£…æ¯”è¾ƒç®€å•ï¼Œå‡†å¤‡micro-SDå¡ï¼Œç”¨Win32 Disk Imagerçƒ§å†™é•œåƒ Win32 Disk Imager download | SourceForge.net å®‰è£…å®Œè½¯ä»¶åå¯ç‚¹å‡»Readå¤‡ä»½è‡ªå·±çš„é•œåƒã€‚ æ³¨æ„ç¬¬äºŒæ¬¡å¼€æœºå‰éœ€è¦é…ç½®config.txtæ–‡ä»¶ï¼Œå¦åˆ™hdmiæ— æ³•æ˜¾ç¤º æ ‘è“æ´¾é…ç½®æ–‡æ¡£ config.txt è¯´æ˜ | æ ‘è“æ´¾å®éªŒå®¤ 123456disable_overscan=1 hdmi_force_hotplug=1hdmi_group=2 # DMThdmi_mode=32 # 1280x960hdmi_drive=2config_hdmi_boost=4 ä¿®æ”¹äº¤æ¢åˆ†åŒºUbuntu MateæŸ¥çœ‹äº¤æ¢åˆ†åŒº1$ free -m æœªè®¾ç½®æ—¶å¦‚ä¸‹1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 0 0 0 åˆ›å»ºå’ŒæŒ‚è½½12345678910111213141516# è·å–æƒé™$ sudo -i# åˆ›å»ºç›®å½•$ mkdir /swap$ cd /swap# æŒ‡å®šä¸€ä¸ªå¤§å°ä¸º1Gçš„åä¸ºâ€œswapâ€çš„äº¤æ¢æ–‡ä»¶$ dd if=/dev/zero of=swap bs=1M count=1k# åˆ›å»ºäº¤æ¢æ–‡ä»¶$ mkswap swap# æŒ‚è½½äº¤æ¢åˆ†åŒº$ swapon swap# å¸è½½äº¤æ¢åˆ†åŒº# $ swapoff swap æŸ¥çœ‹äº¤æ¢åˆ†åŒº1$ free -m æœªè®¾ç½®æ—¶å¦‚ä¸‹1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 RaspbianWe will change the configuration in the file /etc/dphys-swapfile:1$ sudo nano /etc/dphys-swapfile The default value in Raspbian is:1CONF_SWAPSIZE=100 We will need to change this to:1CONF_SWAPSIZE=1024 Then you will need to stop and start the service that manages the swapfile own Rasbian:12$ sudo /etc/init.d/dphys-swapfile stop$ sudo /etc/init.d/dphys-swapfile start You can then verify the amount of memory + swap by issuing the following command:1$ free -m The output should look like:1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 è½¯ä»¶å®‰è£…æŒ‡ä»¤ apt-get å®‰è£…è½¯ä»¶apt-get install softname1 softname2 softname3 ... å¸è½½è½¯ä»¶apt-get remove softname1 softname2 softname3 ... å¸è½½å¹¶æ¸…é™¤é…ç½®apt-get remove --purge softname1 æ›´æ–°è½¯ä»¶ä¿¡æ¯æ•°æ®åº“apt-get update è¿›è¡Œç³»ç»Ÿå‡çº§apt-get upgrade æœç´¢è½¯ä»¶åŒ…apt-cache search softname1 softname2 softname3 ... ä¿®æ­£ï¼ˆä¾èµ–å…³ç³»ï¼‰å®‰è£…ï¼šapt-get -f insta dpkg å®‰è£….debè½¯ä»¶åŒ…dpkg -i xxx.deb åˆ é™¤è½¯ä»¶åŒ…dpkg -r xxx.deb è¿åŒé…ç½®æ–‡ä»¶ä¸€èµ·åˆ é™¤dpkg -r --purge xxx.deb æŸ¥çœ‹è½¯ä»¶åŒ…ä¿¡æ¯dpkg -info xxx.deb æŸ¥çœ‹æ–‡ä»¶æ‹·è´è¯¦æƒ…dpkg -L xxx.deb æŸ¥çœ‹ç³»ç»Ÿä¸­å·²å®‰è£…è½¯ä»¶åŒ…ä¿¡æ¯dpkg -l é‡æ–°é…ç½®è½¯ä»¶åŒ…dpkg-reconfigure xx å¸è½½è½¯ä»¶åŒ…åŠå…¶é…ç½®æ–‡ä»¶ï¼Œä½†æ— æ³•è§£å†³ä¾èµ–å…³ç³»ï¼sudo dpkg -p package_name å¸è½½è½¯ä»¶åŒ…åŠå…¶é…ç½®æ–‡ä»¶ä¸ä¾èµ–å…³ç³»åŒ…sudo aptitude purge pkgname æ¸…é™¤æ‰€æœ‰å·²åˆ é™¤åŒ…çš„æ®‹é¦€é…ç½®æ–‡ä»¶dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P è½¯ä»¶æº å¤‡ä»½åŸå§‹æ–‡ä»¶ 1$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup ä¿®æ”¹æ–‡ä»¶å¹¶æ·»åŠ å›½å†…æº 1$ vi /etc/apt/sources.list æ³¨é‡Šå…ƒæ–‡ä»¶å†…çš„æºå¹¶æ·»åŠ å¦‚ä¸‹åœ°å€ 123456789101112131415161718192021#Mirror.lupaworld.com æºæ›´æ–°æœåŠ¡å™¨ï¼ˆæµ™æ±Ÿçœæ­å·å¸‚åŒçº¿æœåŠ¡å™¨ï¼Œç½‘é€šåŒç”µä¿¡éƒ½å¯ä»¥ç”¨ï¼Œäºšæ´²åœ°åŒºå®˜æ–¹æ›´æ–°æœåŠ¡å™¨ï¼‰ï¼šdeb http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse#Ubuntu å®˜æ–¹æº deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse æˆ–è€… 1234567891011121314151617181920212223#é˜¿é‡Œäº‘deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse#ç½‘æ˜“163deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse æ”¾ç½®éå®˜æ–¹æºçš„åŒ…ä¸å®Œæ•´ï¼Œå¯åœ¨ä¸ºä¸æ·»åŠ å®˜æ–¹æº 1deb http://archive.ubuntu.org.cn/ubuntu-cn/ feisty main restricted universe multiverse æ›´æ–°æº 1$ sudo apt-get update æ›´æ–°è½¯ä»¶ 1$ sudo apt-get dist-upgrade å¸¸è§çš„ä¿®å¤å®‰è£…å‘½ä»¤ 1$ sudo apt-get -f install Pythonä¸»è¦æ˜¯Pythonå’Œç›¸å…³ä¾èµ–åŒ…çš„å®‰è£…ï¼Œä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å¯å¯¼å‡ºå·²å®‰è£…çš„ä¾èµ–åŒ…1$ pip freeze &gt; requirements.txt å¹¶ä½¿ç”¨æŒ‡ä»¤å®‰è£…åˆ°æ ‘è“æ´¾1$ pip install -r requirements.txt æ³¨æ„pipæ›´æ–°1python -m pip install --upgrade pip æœ€æ–°ç‰ˆæœ¬ä¼šæŠ¥é”™1ImportError: cannot import name main ä¿®æ”¹æ–‡ä»¶/usr/bin/pip123from pip import mainif __name__ == &apos;__main__&apos;: sys.exit(main()) æ”¹ä¸º123from pip import __main__if __name__ == &apos;__main__&apos;: sys.exit(__main__._main()) æˆåŠŸ!!!å¤±è´¥äº†ï¼Œç¬‘è„¸:-)ï¼Œæ‰‹åŠ¨å®‰è£…å§ã€‚ã€‚ã€‚ éƒ¨åˆ†åŒ…å¯ä½¿ç”¨pip3 123$ pip3 install numpy$ pip3 install pandas$ pip3 install sklearn è‹¥éœ€è¦æƒé™ï¼ŒåŠ å…¥--user éƒ¨åˆ†åŒ…ç”¨apt-getï¼Œä½†æ˜¯ä¼˜å…ˆå®‰è£…åˆ°Python2.7ç‰ˆæœ¬ï¼Œç¬‘è„¸:-) 123$ sudo apt-get install python-scipy$ sudo apt-get install python-matplotlib$ sudo apt-get install python-opencv éƒ¨åˆ†ä»PIPYä¸‹è½½.whlæˆ–.tar.gzæ–‡ä»¶ PyPI â€“ the Python Package Index Â· PyPI tensorboardX-1.4-py2.py3-none-any.whl visdom-0.1.8.5.tar.gz å®‰è£…æŒ‡ä»¤ä¸º 1$ pip3 install xxx.whl 12$ tar -zxvf xxx.tar.gz$ python setup.py install Pytorchæºç å®‰è£… pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration å®‰è£…æ–¹æ³•Installation - From Source éœ€è¦ç”¨åˆ°minicondaï¼Œå®‰è£…æ–¹æ³•å¦‚ä¸‹ï¼Œæ³¨æ„ä¸­é—´å›è½¦æŒ‰æ…¢ä¸€ç‚¹ï¼Œæœ‰ä¸¤æ¬¡è¾“å…¥ã€‚ã€‚ã€‚ã€‚ã€‚(è¡Œæˆ‘æ…¢æ…¢çœ‹æ¡æ¬¾ä¸è¡Œä¹ˆã€‚ã€‚ç¬‘è„¸:-)) ç¬¬ä¸€æ¬¡æ˜¯æ˜¯å¦åŒæ„æ¡æ¬¾ï¼Œyes ç¬¬äºŒæ¬¡æ˜¯æ·»åŠ åˆ°ç¯å¢ƒå˜é‡ï¼Œyesï¼Œå¦åˆ™è‡ªå·±ä¿®æ”¹/home/pi/.bashrcæ·»åŠ åˆ°ç¯å¢ƒå˜é‡ 1234567891011$ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh$ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5$ sudo /bin/bash Miniconda3-latest-Linux-armv7l.sh # -&gt; change default directory to /home/pi/miniconda3$ sudo nano /home/pi/.bashrc # -&gt; add: export PATH=&quot;/home/pi/miniconda3/bin:$PATH&quot;$ sudo reboot -h now$ conda $ python --version$ sudo chown -R pi miniconda3 ç„¶åå°±å¯ä»¥å®‰è£…äº†æ²¡æœ‰å¯¹åº”ç‰ˆæœ¬çš„mklï¼Œç¬‘è„¸:-) 12345678910111213export CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; # [anaconda root directory]# Disable CUDAexport NO_CUDA=1# Install basic dependenciesconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typingconda install -c mingfeima mkldnn# Install Pytorchgit clone --recursive https://github.com/pytorch/pytorchcd pytorchpython setup.py install tensorflow å®‰è£…tensorflowéœ€è¦çš„ä¸€äº›ä¾èµ–å’Œå·¥å…· 1234567$ sudo apt-get update# For Python 2.7$ sudo apt-get install python-pip python-dev# For Python 3.3+$ sudo apt-get install python3-pip python3-dev å®‰è£…tensorflow è‹¥ä¸‹è½½å¤±è´¥ï¼Œæ‰‹åŠ¨æ‰“å¼€ä¸‹é¢ç½‘é¡µä¸‹è½½.whlåŒ… 1234567# For Python 2.7$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp27-none-linux_armv7l.whl$ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl# For Python 3.4$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl$ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl å¸è½½ï¼Œé‡è£…mock 1234567# For Python 2.7$ sudo pip uninstall mock$ sudo pip install mock# For Python 3.3+$ sudo pip3 uninstall mock$ sudo pip3 install mock å®‰è£…çš„ç‰ˆæœ¬tensorflow v1.1.0æ²¡æœ‰modelsï¼Œå› ä¸º1.0ç‰ˆæœ¬ä»¥åmodelså°±è¢«Sam Abrahamsç‹¬ç«‹å‡ºæ¥äº†ï¼Œä¾‹å¦‚classify_image.pyå°±åœ¨models/tutorials/image/imagenet/é‡Œ tensorflow/models å…¶ä½™ è¾“å…¥æ³• 12$ sudo apt-get install fcitx fcitx-googlepinyin $ fcitx-module-cloudpinyin fcitx-sunpinyin git 1$ sudo apt-get install git é…ç½®gitå’Œssh 12345$ git config --global user.name &quot;Louis Hsu&quot;$ git config --global user.email is.louishsu@foxmail.com$ ssh-keygen -t rsa -C &quot;is.louishsu@foxmail.com&quot;$ cat ~/.ssh/id_rsa.pub # æ·»åŠ åˆ°github]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Underfitting & Overfitting]]></title>
    <url>%2F2018%2F10%2F26%2FUnderfitting-Overfitting%2F</url>
    <content type="text"><![CDATA[åŸå› åˆ†ææ”¾ä¸Šä¸€å¼ éå¸¸ç»å…¸çš„å›¾ï¼Œä»¥ä¸‹åˆ†åˆ«è¡¨ç¤ºäºŒåˆ†ç±»æ¨¡å‹ä¸­çš„æ¬ æ‹Ÿåˆ(underfit)ã€æ°å¥½(just right)ã€è¿‡æ‹Ÿåˆ(overfit)ï¼Œæ¥è‡ªå´æ©è¾¾è¯¾ç¨‹ç¬”è®°ã€‚ æ¬ æ‹Ÿåˆçš„æˆå› å¤§å¤šæ˜¯æ¨¡å‹ä¸å¤Ÿå¤æ‚ã€æ‹Ÿåˆå‡½æ•°çš„èƒ½åŠ›ä¸å¤Ÿï¼› è¿‡æ‹Ÿåˆæˆå› æ˜¯ç»™å®šçš„æ•°æ®é›†ç›¸å¯¹è¿‡äºç®€å•ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ‹Ÿåˆå‡½æ•°æ—¶è¿‡åˆ†åœ°è€ƒè™‘äº†å™ªå£°ç­‰ä¸å¿…è¦çš„æ•°æ®é—´çš„å…³è”ï¼Œæˆ–è€…è¯´ç›¸å¯¹äºç»™å®šæ•°æ®é›†ï¼Œæ¨¡å‹è¿‡äºå¤æ‚ã€æ‹Ÿåˆèƒ½åŠ›è¿‡å¼ºã€‚ åˆ¤åˆ«æ–¹æ³•å­¦ä¹ æ›²çº¿å¯é€šè¿‡å­¦ä¹ æ›²çº¿(Learning curve)è¿›è¡Œæ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆçš„åˆ¤åˆ«ã€‚ å­¦ä¹ æ›²çº¿å°±æ˜¯é€šè¿‡ç”»å‡ºä¸åŒè®­ç»ƒé›†å¤§å°æ—¶è®­ç»ƒé›†å’Œäº¤å‰éªŒè¯çš„å‡†ç¡®ç‡ï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œè¿›è€Œæ¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ–¹å·®åé«˜æˆ–åå·®è¿‡é«˜ï¼Œä»¥åŠå¢å¤§è®­ç»ƒé›†æ˜¯å¦å¯ä»¥å‡å°è¿‡æ‹Ÿåˆã€‚ ç»˜åˆ¶æ¨ªè½´ä¸ºè®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œçºµè½´ä¸ºæŸå¤±æˆ–å…¶ä»–è¯„ä¼°å‡†åˆ™ã€‚sklearnä¸­å­¦ä¹ æ›²çº¿ç»˜åˆ¶ä¾‹ç¨‹å¦‚ä¸‹1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltfrom sklearn.naive_bayes import GaussianNBfrom sklearn.datasets import load_digitsfrom sklearn.model_selection import learning_curvefrom sklearn.model_selection import ShuffleSplitdigits = load_digits(); X, y = digits.data, digits.targetcv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)estimator = GaussianNB()train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=cv, n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5))plt.figure()plt.title(&quot;Learning Curves (Naive Bayes)&quot;)plt.xlabel(&quot;Training examples&quot;)plt.ylabel(&quot;Score&quot;)train_scores_mean = np.mean(train_scores, axis=1)train_scores_std = np.std(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1)test_scores_std = np.std(test_scores, axis=1)plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;)plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;)plt.plot(train_sizes, train_scores_mean, &apos;o-&apos;, color=&quot;r&quot;, label=&quot;Training score&quot;)plt.plot(train_sizes, test_scores_mean, &apos;o-&apos;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;)plt.grid(); plt.legend(loc=&quot;best&quot;)plt.show() åˆ¤åˆ« æ¬ æ‹Ÿåˆï¼Œå³é«˜åå·®(high bias)ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¯¯å·®æ”¶æ•›ä½†å´å¾ˆé«˜ï¼› è¿‡æ‹Ÿåˆï¼Œå³é«˜æ–¹å·®(high variance)ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¯¯å·®ä¹‹é—´æœ‰å¤§çš„å·®è·ã€‚ æ¬ æ‹Ÿåˆè§£å†³æ–¹æ³• å¢åŠ è¿­ä»£æ¬¡æ•°ç»§ç»­è®­ç»ƒ å¢åŠ æ¨¡å‹å¤æ‚åº¦ å¢åŠ ç‰¹å¾ å‡å°‘æ­£åˆ™åŒ–ç¨‹åº¦ é‡‡ç”¨Boostingç­‰é›†æˆæ–¹æ³• æ­¤æ—¶å¢åŠ æ•°æ®é›†å¹¶ä¸èƒ½æ”¹å–„æ¬ æ‹Ÿåˆé—®é¢˜ã€‚ è¿‡æ‹Ÿåˆè§£å†³æ–¹æ³• æå‰åœæ­¢è®­ç»ƒ è·å–æ›´å¤šæ ·æœ¬æˆ–æ•°æ®æ‰©å¢ é‡é‡‡æ · ä¸Šé‡‡æ · å¢åŠ éšæœºå™ªå£° GAN å›¾åƒæ•°æ®çš„ç©ºé—´å˜æ¢ï¼ˆå¹³ç§»æ—‹è½¬é•œåƒï¼‰ å°ºåº¦å˜æ¢ï¼ˆç¼©æ”¾è£å‰ªï¼‰ é¢œè‰²å˜æ¢ æ”¹å˜åˆ†è¾¨ç‡ å¯¹æ¯”åº¦ äº®åº¦ é™ä½æ¨¡å‹å¤æ‚åº¦ å‡å°‘ç‰¹å¾ å¢åŠ æ­£åˆ™åŒ–ç¨‹åº¦ ç¥ç»ç½‘ç»œå¯é‡‡ç”¨Dropout å¤šæ¨¡å‹æŠ•ç¥¨æ–¹æ³•]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Cross Validation & Hyperparameter]]></title>
    <url>%2F2018%2F10%2F26%2FCross-Validation-Hyperparameter%2F</url>
    <content type="text"><![CDATA[äº¤å‰éªŒè¯ä¸è¶…å‚æ•°é€‰æ‹©äº¤å‰éªŒè¯ä»¥ä¸‹ç®€ç§°äº¤å‰éªŒè¯(Cross Validation)ä¸ºCV.CVæ˜¯ç”¨æ¥éªŒè¯åˆ†ç±»å™¨çš„æ€§èƒ½ä¸€ç§ç»Ÿè®¡åˆ†ææ–¹æ³•,åŸºæœ¬æ€æƒ³æ˜¯æŠŠåœ¨æŸç§æ„ä¹‰ä¸‹å°†åŸå§‹æ•°æ®(dataset)è¿›è¡Œåˆ†ç»„,ä¸€éƒ¨åˆ†åšä¸ºè®­ç»ƒé›†(train set),å¦ä¸€éƒ¨åˆ†åšä¸ºéªŒè¯é›†(validation set),é¦–å…ˆç”¨è®­ç»ƒé›†å¯¹åˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒ,åœ¨åˆ©ç”¨éªŒè¯é›†æ¥æµ‹è¯•è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹(model),ä»¥æ­¤æ¥åšä¸ºè¯„ä»·åˆ†ç±»å™¨çš„æ€§èƒ½æŒ‡æ ‡ã€‚ äº¤å‰éªŒè¯çš„å‡ ç§æ–¹æ³• kæŠ˜äº¤å‰éªŒè¯(K-fold) å°†å…¨éƒ¨è®­ç»ƒé›†$S$åˆ†æˆ$k$ä¸ªä¸ç›¸äº¤çš„å­é›†ï¼Œå‡è®¾$S$ä¸­çš„è®­ç»ƒæ ·ä¾‹ä¸ªæ•°ä¸º$m$ï¼Œåˆ™æ¯ä¸ªå­é›†ä¸­æœ‰$(\frac{m}{k})$ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œç›¸åº”å­é›†ç§°ä½œ$\{s_1, s_2, â€¦, s_k\}$ï¼› æ¯æ¬¡ä»åˆ†å¥½çš„å­é›†ä¸­ï¼Œæ‹¿å‡º$1$ä¸ªä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä»–$k-1$ä¸ªä½œä¸ºè®­ç»ƒé›†ï¼› åœ¨$k-1$ä¸ªè®­ç»ƒé›†ä¸Šè®­ç»ƒå‡ºå­¦ä¹ å™¨æ¨¡å‹ï¼Œå°†æ¨¡å‹æ”¾åˆ°æµ‹è¯•é›†ä¸Šï¼Œå¾—åˆ°åˆ†ç±»ç‡ï¼› è®¡ç®—kæ¬¡æ±‚å¾—çš„åˆ†ç±»ç‡å¹³å‡å€¼ï¼Œä½œä¸ºè¯¥æ¨¡å‹æˆ–è€…å‡è®¾å‡½æ•°çš„çœŸå®åˆ†ç±»ç‡ ç•™ä¸€æ³•äº¤å‰éªŒè¯(Leave One Out - LOO) å‡è®¾æœ‰$N$ä¸ªæ ·æœ¬ï¼Œå°†æ¯ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•æ ·æœ¬ï¼Œå…¶ä»–$(N-1)$ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒæ ·æœ¬ã€‚è¿™æ ·å¾—åˆ°$N$ä¸ªåˆ†ç±»å™¨ï¼Œ$N$ä¸ªæµ‹è¯•ç»“æœã€‚ç”¨è¿™$N$ä¸ªç»“æœçš„å¹³å‡å€¼è¡¡é‡æ¨¡å‹çš„æ€§èƒ½ã€‚ ç•™Pæ³•äº¤å‰éªŒè¯(Leave P Out - LPO) å°†$P$ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•æ ·æœ¬ï¼Œå…¶ä»–$(N-P)$ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒæ ·æœ¬ã€‚è¿™æ ·å¾—åˆ°$\left(\begin{matrix} P \\ N \end{matrix}\right)$ä¸ªè®­ç»ƒæµ‹è¯•å¯¹ã€‚å½“$Pï¼1$æ—¶ï¼Œæµ‹è¯•é›†ä¼šå‘ç”Ÿé‡å ã€‚å½“$P=1$æ—¶ï¼Œå˜æˆ$LOO$ã€‚ scikit-learnä¸­çš„äº¤å‰éªŒè¯ K-fold 12345678&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn.model_selection import KFold&gt;&gt;&gt; X = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]&gt;&gt;&gt; kf = KFold(n_splits=2)&gt;&gt;&gt; for train, test in kf.split(X):... print(&quot;%s %s&quot; % (train, test))[2 3] [0 1][0 1] [2 3] Leave One Out (LOO) 123456789&gt;&gt;&gt; from sklearn.model_selection import LeaveOneOut&gt;&gt;&gt; X = [1, 2, 3, 4]&gt;&gt;&gt; loo = LeaveOneOut()&gt;&gt;&gt; for train, test in loo.split(X):... print(&quot;%s %s&quot; % (train, test))[1 2 3] [0][0 2 3] [1][0 1 3] [2][0 1 2] [3] Leave P Out (LPO) 1234567891011&gt;&gt;&gt; from sklearn.model_selection import LeavePOut&gt;&gt;&gt; X = np.ones(4)&gt;&gt;&gt; lpo = LeavePOut(p=2)&gt;&gt;&gt; for train, test in lpo.split(X):... print(&quot;%s %s&quot; % (train, test))[2 3] [0 1][1 3] [0 2][1 2] [0 3][0 3] [1 2][0 2] [1 3][0 1] [2 3] ä½¿ç”¨äº¤å‰éªŒè¯è°ƒæ•´è¶…å‚æ•°è¶…å‚æ•°çš„å®šä¹‰ï¼šåœ¨æœºå™¨å­¦ä¹ çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¶…å‚æ•°æ˜¯åœ¨å¼€å§‹å­¦ä¹ è¿‡ç¨‹ä¹‹å‰è®¾ç½®å€¼çš„å‚æ•°ï¼Œè€Œä¸æ˜¯é€šè¿‡è®­ç»ƒå¾—åˆ°çš„å‚æ•°æ•°æ®ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œéœ€è¦å¯¹è¶…å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œç»™å­¦ä¹ æœºé€‰æ‹©ä¸€ç»„æœ€ä¼˜è¶…å‚æ•°ï¼Œä»¥æé«˜å­¦ä¹ çš„æ€§èƒ½å’Œæ•ˆæœã€‚è¶…å‚æ•°ä¾‹å¦‚ æ¨¡å‹ï¼ˆSVMï¼ŒSoftmaxï¼ŒMulti-layer Neural Network,â€¦)ï¼› è¿­ä»£ç®—æ³•ï¼ˆAdam, SGD, â€¦)(ä¸åŒçš„è¿­ä»£ç®—æ³•è¿˜æœ‰å„ç§ä¸åŒçš„è¶…å‚æ•°ï¼Œå¦‚beta1,beta2ç­‰ç­‰ï¼Œä½†å¸¸è§çš„åšæ³•æ˜¯ä½¿ç”¨é»˜è®¤å€¼ï¼Œä¸è¿›è¡Œè°ƒå‚ï¼‰ï¼› å­¦ä¹ ç‡ï¼ˆlearning rate)ï¼› æ­£åˆ™åŒ–æ–¹ç¨‹çš„é€‰æ‹©(L0,L1,L2)ï¼Œæ­£åˆ™åŒ–ç³»æ•°ï¼› dropoutçš„æ¦‚ç‡ â€¦ ç¡®å®šè°ƒèŠ‚èŒƒå›´è¶…å‚æ•°çš„ç§ç±»å¤šï¼Œè°ƒèŠ‚èŒƒå›´å¤§ï¼Œéœ€è¦å…ˆè¿›è¡Œç®€å•çš„æµ‹è¯•ç¡®å®šè°ƒå‚èŒƒå›´ã€‚ æ¨¡å‹é€‰æ‹© æ¨¡å‹çš„é€‰æ‹©å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå…·ä½“çš„å®é™…é—®é¢˜ï¼Œä½†å¿…é¡»é€šè¿‡å‡ é¡¹åŸºæœ¬æµ‹è¯•ã€‚ å¯ä»¥é€šè¿‡ç¬¬ä¸€ä¸ªepochçš„lossï¼Œè§‚å¯Ÿæ¨¡å‹èƒ½å¦æ— BUGè¿è¡Œï¼Œæ³¨æ„æ­¤è¿‡ç¨‹éœ€è¦è®¾ç½®æ­£åˆ™é¡¹ç³»æ•°ä¸º0ï¼Œå› ä¸ºæ­£åˆ™é¡¹å¼•å…¥çš„losséš¾ä»¥ä¼°ç®—ã€‚ æ¨¡å‹å¿…é¡»å¯ä»¥å¯¹äºå°æ•°æ®é›†è¿‡æ‹Ÿåˆï¼Œå¦åˆ™åº”è¯¥å°è¯•å…¶ä»–æˆ–è€…æ›´å¤æ‚çš„æ¨¡å‹ã€‚ è‹¥è®­ç»ƒé›†ä¸éªŒè¯é›†losså‡è¾ƒå¤§ï¼Œåˆ™åº”è¯¥å°è¯•å…¶ä»–æˆ–è€…æ›´å¤æ‚çš„æ¨¡å‹ã€‚ æ¨¡å‹é€‰æ‹©çš„æ–¹æ³•ä¸ºï¼š ä½¿ç”¨è®­ç»ƒé›†è®­ç»ƒå‡º 10 ä¸ªæ¨¡å‹ ç”¨ 10 ä¸ªæ¨¡å‹åˆ†åˆ«å¯¹äº¤å‰éªŒè¯é›†è®¡ç®—å¾—å‡ºäº¤å‰éªŒè¯è¯¯å·®ï¼ˆä»£ä»·å‡½æ•°çš„å€¼ï¼‰ é€‰å–ä»£ä»·å‡½æ•°å€¼æœ€å°çš„æ¨¡å‹ ç”¨æ­¥éª¤ 3 ä¸­é€‰å‡ºçš„æ¨¡å‹å¯¹æµ‹è¯•é›†è®¡ç®—å¾—å‡ºæ¨å¹¿è¯¯å·®ï¼ˆä»£ä»·å‡½æ•°çš„å€¼ï¼‰ â€”â€” Andrew Ng, Stanford University å­¦ä¹ ç‡ lossåŸºæœ¬ä¸å˜ï¼šå­¦ä¹ ç‡è¿‡ä½ lossæ³¢åŠ¨æ˜æ˜¾æˆ–è€…æº¢å‡ºï¼šå­¦ä¹ ç‡è¿‡é«˜ æ­£åˆ™é¡¹ç³»æ•° val_accä¸accç›¸å·®è¾ƒå¤§ï¼šæ­£åˆ™é¡¹ç³»æ•°è¿‡å° lossé€æ¸å¢å¤§ï¼šæ­£åˆ™é¡¹ç³»æ•°è¿‡å¤§ è¶…å‚æ•°çš„ç¡®å®š å…ˆç²—è°ƒï¼Œå†ç»†è°ƒ å…ˆé€šè¿‡æ•°é‡å°‘ï¼Œé—´è·å¤§çš„ç²—è°ƒç¡®å®šç»†è°ƒçš„å¤§è‡´èŒƒå›´ã€‚ç„¶ååœ¨å°èŒƒå›´å†…éƒ¨è¿›è¡Œé—´è·å°ï¼Œæ•°é‡å¤§çš„ç»†è°ƒã€‚ å°è¯•åœ¨å¯¹æ•°ç©ºé—´å†…è¿›è¡Œè°ƒèŠ‚ å³åœ¨å¯¹æ•°ç©ºé—´å†…éƒ¨éšæœºç”Ÿæˆæµ‹è¯•å‚æ•°ï¼Œè€Œä¸æ˜¯åœ¨åŸç©ºé—´ç”Ÿæˆï¼Œé€šå¸¸ç”¨äºå­¦ä¹ ç‡ä»¥åŠæ­£åˆ™é¡¹ç³»æ•°ç­‰çš„è°ƒèŠ‚ã€‚å‡ºå‘ç‚¹æ˜¯è¯¥è¶…å‚æ•°çš„æŒ‡æ•°é¡¹å¯¹äºæ¨¡å‹çš„ç»“æœå½±å“æ›´æ˜¾è‘—ï¼›è€ŒåŒé˜¶çš„æ•°æ®ä¹‹é—´å³ä¾¿åŸåŸŸç›¸å·®è¾ƒå¤§ï¼Œå¯¹äºæ¨¡å‹ç»“æœçš„å½±å“åè€Œä¸å¦‚ä¸åŒé˜¶çš„æ•°æ®å·®è·å¤§ã€‚ è¶…å‚æ•°æœç´¢ éšæœºæœç´¢å‚æ•°å€¼ï¼Œè€Œä¸æ˜¯ç½‘æ ¼æœç´¢ã€‚ è¶…å‚æ•°æœç´¢scikit-learnæä¾›è¶…å‚æ•°æœç´¢æ–¹æ³•ï¼Œå¯å‚è€ƒå®˜æ–¹æ–‡æ¡£ ç½‘æ ¼æœç´¢ 3.2.1. Exhaustive Grid Search è°ƒç”¨ä¾‹ç¨‹å¦‚ä¸‹ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results[&apos;rank_test_score&apos;] == i) for candidate in candidates: print(&quot;Model with rank: &#123;0&#125;&quot;.format(i)) print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format( results[&apos;mean_test_score&apos;][candidate], results[&apos;std_test_score&apos;][candidate])) print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&apos;params&apos;][candidate])) print(&quot;&quot;)# use a full grid over all parametersparam_grid = &#123;&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: [1, 3, 10], &quot;min_samples_split&quot;: [2, 3, 10], &quot;min_samples_leaf&quot;: [1, 3, 10], &quot;bootstrap&quot;: [True, False], &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid)start = time()grid_search.fit(X, y)print(&quot;GridSearchCV took %.2f seconds for %d candidate parameter settings.&quot; % (time() - start, len(grid_search.cv_results_[&apos;params&apos;])))report(grid_search.cv_results_) éšæœºæœç´¢ 3.2.2. Randomized Parameter Optimization è°ƒç”¨ä¾‹ç¨‹å¦‚ä¸‹ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results[&apos;rank_test_score&apos;] == i) for candidate in candidates: print(&quot;Model with rank: &#123;0&#125;&quot;.format(i)) print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format( results[&apos;mean_test_score&apos;][candidate], results[&apos;std_test_score&apos;][candidate])) print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&apos;params&apos;][candidate])) print(&quot;&quot;)# specify parameters and distributions to sample fromparam_dist = &#123;&quot;max_depth&quot;: [3, None], &quot;max_features&quot;: sp_randint(1, 11), &quot;min_samples_split&quot;: sp_randint(2, 11), &quot;min_samples_leaf&quot;: sp_randint(1, 11), &quot;bootstrap&quot;: [True, False], &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)start = time()random_search.fit(X, y)print(&quot;RandomizedSearchCV took %.2f seconds for %d candidates&quot; &quot; parameter settings.&quot; % ((time() - start), n_iter_search))report(random_search.cv_results_)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spam Classification]]></title>
    <url>%2F2018%2F10%2F26%2FSpam-Classification%2F</url>
    <content type="text"><![CDATA[è¸©å‘ï¼Ÿï¼Ÿï¼Ÿå…¨éƒ¨ç»™æˆ‘è¸©å¹³ï¼ï¼ï¼ æ¥è‡ªLintCodeåƒåœ¾çŸ­ä¿¡åˆ†ç±»@Github: spam or ham å’ä»£ç é¢„å¤„ç†åŠå‘é‡åŒ–è§‚å¯Ÿå„æ–‡æœ¬åï¼Œå‘ç°å„æ–‡æœ¬ä¸­åŒ…å«çš„å•è¯å¤šç§å¤šæ ·ï¼ŒåŒ…å«æ ‡ç‚¹ã€æ•°å­—ç­‰ï¼Œä¾‹å¦‚123- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...- XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http://wap. - 07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. ä¸”æŒ‰ç©ºæ ¼åˆ†è¯åï¼Œéƒ¨åˆ†å•è¯ä¸­ä»åŒ…å«whitespaceï¼Œæ•…é€‰æ‹©çš„é¢„å¤„ç†æ–¹æ¡ˆæ˜¯ï¼Œå»é™¤åˆ†è¯åæ–‡æœ¬ä¸­çš„æ ‡ç‚¹ã€æ•°å­—ã€ç©ºæ ¼ç­‰ï¼Œå¹¶å°†å•è¯ä¸­å­—æ¯å…¨éƒ¨è½¬ä¸ºå°å†™ã€‚ ä¸­æ–‡åˆ†è¯å¯é‡‡ç”¨jieba(è¡—éœ¸ï¼Ÿ) é¢„å¤„ç†åï¼ŒæŒ‰å½“å‰çš„æ–‡æœ¬å†…å®¹å»ºç«‹å­—å…¸ï¼Œå¹¶ç»Ÿè®¡å„æ ·æœ¬çš„è¯æ•°å‘é‡ï¼Œè¯¦ç»†ä»£ç å¦‚ä¸‹12345678910111213141516171819202122232425262728293031323334353637383940414243class Words2Vector(): &apos;&apos;&apos; å»ºç«‹å­—å…¸ï¼Œå°†è¾“å…¥çš„è¯åˆ—è¡¨è½¬æ¢ä¸ºå‘é‡ï¼Œè¡¨ç¤ºå„è¯å‡ºç°çš„æ¬¡æ•° &apos;&apos;&apos; def __init__(self): self.dict = None self.n_word = None def fit_transform(self, words): self.fit(words) return self.transform(words) def fit(self, words): &quot;&quot;&quot; @param &#123;list[list[str]]&#125; words &quot;&quot;&quot; words = _flatten(words) # å±•å¼€ä¸º1ç»´åˆ—è¡¨ words = self.filt(words) # æ»¤é™¤ç©ºæ ¼ã€æ•°å­—ã€æ ‡ç‚¹ self.word = list(set(words)) # å»é‡ self.n_word = len(set(words)) # ç»Ÿè®¡è¯çš„ä¸ªæ•° self.dict = dict(zip(self.word, [_ for _ in range(self.n_word)])) # å„è¯åœ¨å­—å…¸ä¸­çš„ä½ç½® def transform(self, words): &quot;&quot;&quot; @param &#123;list[list[str]]&#125; words @return &#123;ndarray&#125; retarray: vector &quot;&quot;&quot; retarray = np.zeros(shape=(len(words), self.n_word)) # è¿”å›çš„è¯æ•°å‘é‡ for i in range(len(words)): words[i] = self.filt(words[i]) # æ»¤é™¤ç©ºæ ¼ã€æ•°å­—ã€æ ‡ç‚¹ for i in range(len(words)): for w in words[i]: if w in self.word: # æ˜¯å¦åœ¨è®­ç»ƒé›†ç”Ÿæˆçš„å­—å…¸ä¸­ retarray[i, self.dict[w]] += 1 # æŸ¥è¯¢å­—å…¸ï¼Œæ‰¾åˆ°å¯¹åº”ç‰¹å¾çš„ä¸‹æ ‡ return retarray def filt(self, flattenWords): retWords = [] en_stops = set(stopwords.words(&apos;english&apos;)) # åœç”¨è¯åˆ—è¡¨ for word in flattenWords: word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.whitespace)) # å»é™¤ç©ºç™½ word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation)) # å»é™¤æ ‡ç‚¹ word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.digits)) # å»é™¤æ•°å­— if word not in en_stops and (len(word) &gt; 1): # åˆ é™¤åœç”¨è¯ï¼Œå¹¶é™¤å»é•¿åº¦å°äºç­‰äº2çš„è¯ retWords.append(word.lower()) return retWords TF-IDFæ–¹æ³•ç”±è¯æ•°å‘é‡å¯è®¡ç®—è¯é¢‘ï¼Œä½†åªç”¨è¯é¢‘å¿½ç•¥äº†å„æ–‡æœ¬åœ¨ä¸åŒæ–‡æ¡£ä¸­çš„é‡è¦ç¨‹åº¦ï¼Œå…³äºTF-IDFï¼Œåœ¨å¦ä¸€ç¯‡åšæ–‡ä¸­è¯¦ç»†è¯´æ˜ã€‚ ç”±äºå‰”é™¤äº†åœç”¨è¯ç­‰ï¼Œéƒ¨åˆ†å‘é‡ä¸åŒ…å«ä»»ä½•å†…å®¹ï¼Œå³è¯æ•°å‘é‡ä¸º$\vec{0}$ï¼Œè¿™æ—¶è®¡ç®—è¯é¢‘å’Œå•ä½åŒ–æ—¶ï¼Œä¼šå‡ºç°nançš„è¿ç®—ç»“æœï¼Œæ•…åªå¯¹éç©ºå‘é‡è¿›è¡Œè®¡ç®—ã€‚ è®­ç»ƒåéœ€è¦ä¿å­˜çš„æ˜¯IDFå‘é‡ï¼ŒTFå‘é‡åœ¨æ–°æ ·æœ¬è¾“å…¥åé‡æ–°è®¡ç®—ï¼Œæ•…æ— éœ€ä¿å­˜ã€‚ 12345678910111213141516171819202122232425262728293031class TfidfVectorizer(): def __init__(self): self.idf = None def fit_transform(self, num_vec): self.fit(num_vec) return self.transform(num_vec) def fit(self, num_vec): &quot;&quot;&quot; @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature) &quot;&quot;&quot; num_vec[num_vec&gt;0] = 1 n_doc = num_vec.shape[0] n_term = np.sum(num_vec, axis=0) # å„è¯å‡ºç°è¿‡çš„æ–‡æ¡£æ¬¡æ•° self.idf = np.log((n_doc + 1) / (n_term + 1)) + 1 return self.idf def transform(self, num_vec): &quot;&quot;&quot; @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature) &quot;&quot;&quot; # æ±‚è§£è¯é¢‘å‘é‡ï¼Œç”±äºéƒ¨åˆ†å‘é‡ä¸ºç©ºï¼Œæ•…ä¸‹å¥ä¼šå‡ºç°é—®é¢˜ # tf = num_vec / np.sum(num_vec, axis=1).reshape(-1, 1) =&gt; nan # è§£å†³æ–¹æ³•ï¼šåªå¯¹éç©ºå‘é‡è¿›è¡Œè¯é¢‘è®¡ç®— tf = np.zeros(shape=num_vec.shape) n_terms = np.sum(num_vec, axis=1); idx = (n_terms!=0) tf[idx] = num_vec[idx] / n_terms[idx].reshape(-1, 1) # è®¡ç®—è¯é¢‘ï¼Œåªå¯¹éç©ºå‘é‡è¿›è¡Œ tfidf = tf * self.idf tfidf[idx] /= np.linalg.norm(tfidf, axis=1)[idx].reshape(-1, 1) # å•ä½åŒ–ï¼Œåªå¯¹éç©ºå‘é‡è¿›è¡Œ return tfidf è´å¶æ–¯å†³ç­–å„æ–‡æœ¬å‘é‡åŒ–åï¼Œå°±å¯é€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œè¿™é‡Œé‡‡ç”¨çš„æ˜¯è´å¶æ–¯å†³ç­–çš„æ–¹æ³•ï¼Œéœ€è¦æ³¨æ„çš„æœ‰ä»¥ä¸‹å‡ ç‚¹ ä¼¼ç„¶å‡½æ•°$p(x|c_k)$ä¸è´å¶æ–¯å†³ç­–æ–‡ä¸­ä¾‹ä¸åŒï¼Œè¿™é‡Œå®œé‡‡ç”¨é«˜æ–¯åˆ†å¸ƒä½œä¸ºåˆ†å¸ƒæ¨¡å‹ï¼› æŒ‰æœ´ç´ è´å¶æ–¯è®¡ç®—$p(x|c_k)$ï¼Œä½†æ³¨æ„æ­¤å¤„ä¸èƒ½å°†å„ç»´ç‰¹å¾å•ç‹¬è®­ç»ƒ$1$ç»´é«˜æ–¯åˆ†å¸ƒæ¨¡å‹ï¼Œç„¶åè®¡ç®—é¢„æµ‹æ ·æœ¬ä¼¼ç„¶å‡½æ•°å€¼æ—¶è¿›è¡Œç´¯ä¹˜ï¼Œå¦‚ä¸‹ p(x|c_k) = \prod_{j=1}^{N_feature} p(x_j|c_k)å› ä¸ºç‰¹å¾ç»´åº¦ç‰¹åˆ«é«˜ï¼Œå„ä¸ªç‰¹å¾å•ç‹¬ç”¨$1$ç»´é«˜æ–¯åˆ†å¸ƒæè¿°ï¼Œç´¯ä¹˜è®¡ç®—ä¼šä¸‹æº¢ï¼Œæ•…è¿™é‡Œé‡‡ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ p(x|c_k) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}} Â· e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)} ä¸”ç»ä¸»æˆåˆ†åˆ†æåï¼Œå„ç»´åº¦é—´çº¿æ€§ç›¸å…³æ€§é™ä½ï¼Œæ•…å‡å®š \Sigma_k = diag\{\sigma_{k1}, ..., \sigma_{kn}\} ä½†åˆ†æ¯$(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}$åœ¨è®¡ç®—æ—¶ä¸ç¨³å®šï¼Œä¸”å„ç‰¹å¾æ ‡å‡†å·®å¤§å°ç›¸å·®æ— å‡ ï¼Œæ•…è¿™é‡Œå‡å®š \Sigma_k = I æœ€ç»ˆç®€åŒ–åçš„ä¼¼ç„¶å‡½æ•°è®¡ç®—æ–¹æ³•ä¸º p(x|c_k) = e^{-\frac{1}{2} (x - \mu_k)^T (x - \mu_k)} è´å¶æ–¯å†³ç­–æ¨¡å‹è®­ç»ƒåŸºäºä¸Šè¿°å‡è®¾ï¼Œåªéœ€è®­ç»ƒå¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å„ç»´å‡å€¼$\mu_j$ 1234567891011121314151617181920212223def fit(self, labels, text): &quot;&quot;&quot; @param &#123;ndarray&#125; labels: shape(N_samples, ), labels[i] \in &#123;0, 1&#125; @param &#123;list[list[str]]&#125; words &quot;&quot;&quot; labels = self.encodeLabel(labels); words = self.text2words(self.clean(text)) vecwords = self.numvectorizer.fit_transform(words) # å‘é‡åŒ– vecwords = self.tfidfvectorizer.fit_transform(vecwords) # tfidf, shape(N_samples, N_features) isnotEmpty = (np.sum(vecwords, axis=1)!=0) # å»æ‰ç©ºçš„æ ·æœ¬ vecwords = vecwords[isnotEmpty]; labels = labels[isnotEmpty] # vecwords = self.reduce_dim.fit_transform(vecwords) # é™ç»´ï¼Œè®¡ç®—é‡å¤ªå¤§ self.n_features = vecwords.shape[1] labels = OneHotEncoder().fit_transform(labels.reshape((-1, 1))).toarray() self.priori = np.mean(labels, axis=0) # å…ˆéªŒæ¦‚ç‡ self.likelihood_mu = np.zeros(shape=(2, vecwords.shape[1])) # è®¾ä¼¼ç„¶å‡½æ•°p(x|c)ä¸ºé«˜æ–¯åˆ†å¸ƒ for i in range(2): vec = vecwords[labels[:, i]==1] self.likelihood_mu[i] = np.mean(vec, axis=0) è´å¶æ–¯å†³ç­–æ¨¡å‹é¢„æµ‹å†³ç­–å‡½æ•°ä¸º if p(x|c_i)P(c_i) > p(x|c_j)P(c_j), then x \in c_iä½†å®é™…æ•ˆæœæ˜¾ç¤ºï¼Œç­‰å…ˆéªŒæ¦‚ç‡$P(c_j)$ç»“æœæ›´å¥½$(???)$ 123456789101112131415161718192021222324252627def multigaussian(self, x, mu): &quot;&quot;&quot; ç®€åŒ– &quot;&quot;&quot; x = x - mu a = np.exp(-0.5 * x.T.dot(x)) return adef predict(self, text): &quot;&quot;&quot; @param &#123;list[list[str]]&#125; words @note: p(x|c)P(c) P(c|x) = ------------ p(x) &quot;&quot;&quot; pred_porba = np.ones(shape=(len(self.clean(text)), 2)) words = self.text2words(text) vecwords = self.tfidfvectorizer.transform( self.numvectorizer.transform(words)) # å‘é‡åŒ– for i in range(vecwords.shape[0]): for c in range(2): # pred_porba[i, c] = self.priori[c] * self.multigaussian(vecwords[i], self.likelihood_mu[c]) pred_porba[i, c] = self.multigaussian(vecwords[i], self.likelihood_mu[c]) pred = np.argmax(pred_porba, axis=1) return self.decodeLabel(pred) è°ƒåŒ…ä¸»è¦ç”¨åˆ°äº†scikit-learnæœºå™¨å­¦ä¹ åŒ…ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½ sklearn.feature_extraction.text.TfidfVectorizer() sklearn.decomposition.PCA() sklearn.naive_bayes.BernoulliNB() æœ€ç»ˆå‡†ç¡®ç‡åœ¨$97\%$å·¦å³ï¼Œä»£ç æ¯”è¾ƒç®€å•ï¼Œä¸è¿›è¡Œè¯´æ˜ã€‚ é‡‡ç”¨sklearn.linear_model import.LogisticRegressionCV()æ•ˆæœæ›´ä½³ 123456789101112131415161718192021222324252627282930def main(): trainfile = &quot;./data/train.csv&quot; testfile = &quot;./data/test.csv&quot; # è¯»å–åŸå§‹æ•°æ® data_train = pd.read_csv(trainfile, names=[&apos;Label&apos;, &apos;Text&apos;]) txt_train = list(data_train[&apos;Text&apos;])[1: ]; label_train = list(data_train[&apos;Label&apos;])[1: ] drop(txt_train) # åˆ é™¤æ•°å­—å’Œæ ‡ç‚¹ txt_test = list(pd.read_csv(testfile, names=[&apos;Text&apos;])[&apos;Text&apos;])[1: ] drop(txt_test) # åˆ é™¤æ•°å­—å’Œæ ‡ç‚¹ # è®­ç»ƒ vectorizer = TfidfVectorizer(stop_words=&apos;english&apos;) # åˆ é™¤è‹±æ–‡åœç”¨è¯ vec_train = vectorizer.fit_transform(txt_train).toarray() # æå–æ–‡æœ¬ç‰¹å¾å‘é‡ # reduce_dim = PCA(n_components = 4096) # vec_train = reduce_dim.fit_transform(vec_train) estimator = BernoulliNB() estimator.fit(vec_train, label_train) # è®­ç»ƒæœ´ç´ è´å¶æ–¯æ¨¡å‹ # æµ‹è¯• label_train_pred = estimator.predict(vec_train) acc = np.mean((label_train_pred==label_train).astype(&apos;float&apos;)) # é¢„æµ‹ vec_test = vectorizer.transform(txt_test).toarray() # vec_test = reduce_dim.transform(vec_test) label_test_pred = estimator.predict(vec_test) with open(&apos;./data/sampleSubmission.txt&apos;, &apos;w&apos;) as f: for i in range(label_test_pred.shape[0]): f.write(label_test_pred[i] + &apos;\n&apos;)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TF-IDF]]></title>
    <url>%2F2018%2F10%2F25%2FTF-IDF%2F</url>
    <content type="text"><![CDATA[å¼•è¨€æ­£åœ¨åšLintCodeä¸Šçš„åƒåœ¾é‚®ä»¶åˆ†ç±»ï¼Œä½¿ç”¨æœ´ç´ è´å¶æ–¯æ–¹æ³•è§£å†³ï¼Œæ¶‰åŠåˆ°æ–‡æœ¬ç‰¹å¾çš„æå–ã€‚TF-IDFï¼ˆè¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼‰ç®—æ³•æ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚å­—è¯çš„é‡è¦æ€§éšç€å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”å¢åŠ ï¼Œä½†åŒæ—¶ä¼šéšç€å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”ä¸‹é™ã€‚ è®¡ç®—æ­¥éª¤è¯é¢‘(TF)Term Frequencyï¼Œå°±æ˜¯æŸä¸ªå…³é”®å­—å‡ºç°çš„é¢‘ç‡ï¼Œå…·ä½“æ¥è®²ï¼Œå°±æ˜¯è¯åº“ä¸­çš„æŸä¸ªè¯åœ¨å½“å‰æ–‡ç« ä¸­å‡ºç°çš„é¢‘ç‡ã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥å†™å‡ºå®ƒçš„è®¡ç®—å…¬å¼ï¼š TF_{ij} = \frac{n_{ij}}{\sum_k n_{i, k}}å…¶ä¸­ï¼Œ$n_{ij}$è¡¨ç¤ºå…³é”®è¯$j$åœ¨æ–‡æ¡£$i$ä¸­çš„å‡ºç°æ¬¡æ•°ã€‚ å•çº¯ä½¿ç”¨TFæ¥è¯„ä¼°å…³é”®è¯çš„é‡è¦æ€§å¿½ç•¥äº†å¸¸ç”¨è¯çš„å¹²æ‰°ã€‚å¸¸ç”¨è¯å°±æ˜¯æŒ‡é‚£äº›æ–‡ç« ä¸­å¤§é‡ç”¨åˆ°çš„ï¼Œä½†æ˜¯ä¸èƒ½åæ˜ æ–‡ç« æ€§è´¨çš„é‚£ç§è¯ï¼Œæ¯”å¦‚ï¼šå› ä¸ºã€æ‰€ä»¥ã€å› æ­¤ç­‰ç­‰çš„è¿è¯ï¼Œåœ¨è‹±æ–‡æ–‡ç« é‡Œå°±ä½“ç°ä¸ºandã€theã€ofç­‰ç­‰çš„è¯ã€‚è¿™äº›è¯å¾€å¾€æ‹¥æœ‰è¾ƒé«˜çš„TFï¼Œæ‰€ä»¥ä»…ä»…ä½¿ç”¨TFæ¥è€ƒå¯Ÿä¸€ä¸ªè¯çš„å…³é”®æ€§ï¼Œæ˜¯ä¸å¤Ÿçš„ã€‚ é€†æ–‡æ¡£é¢‘ç‡(IDF)Inverse Document Frequencyï¼Œæ–‡æ¡£é¢‘ç‡å°±æ˜¯ä¸€ä¸ªè¯åœ¨æ•´ä¸ªæ–‡åº“è¯å…¸ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œé€†æ–‡æ¡£é¢‘ç‡ç”¨ä¸‹å¼è®¡ç®— IDF_j = \log \frac{|D|}{|D_j| + 1}å…¶ä¸­ï¼Œ$|D|$è¡¨ç¤ºæ€»çš„æ–‡æ¡£æ•°ç›®ï¼Œ$|D_j|$è¡¨ç¤ºå…³é”®è¯$j$å‡ºç°è¿‡çš„æ–‡æ¡£æ•°ç›® scikit-learnå†…ä¸º IDF_j = \log \frac{|D| + 1}{|D_j| + 1} + 1 è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡(TF-IDF) TF-IDF_{i} = TF_i Ã— IDFä¸¾ä¾‹ä¾‹å¦‚æœ‰å¦‚ä¸‹$3$ä¸ªæ–‡æœ¬123æ–‡æœ¬1ï¼šMy dog ate my homework.æ–‡æœ¬2ï¼šMy cat ate the sandwich.æ–‡æœ¬3ï¼šA dolphin ate the homework. æå–å­—å…¸ï¼Œä¸€èˆ¬éœ€è¦å¤„ç†å¤§å°å†™ã€å»é™¤åœç”¨è¯aï¼Œå¤„ç†ç»“æœä¸º1ate, cat, dog, dolphin, homework, my, sandwich, the æ•…å„ä¸ªæ–‡æœ¬çš„è¯æ•°å‘é‡ä¸º123æ–‡æœ¬1ï¼š[1, 0, 1, 0, 1, 2, 0, 0]æ–‡æœ¬2ï¼š[1, 1, 0, 0, 0, 1, 1, 1]æ–‡æœ¬3ï¼š[1, 0, 0, 1, 1, 0, 0, 1] å„ä¸ªæ–‡æœ¬çš„è¯é¢‘å‘é‡(TF)123æ–‡æœ¬1ï¼š[0.2 , 0. , 0.2 , 0. , 0.2 , 0.4 , 0. , 0. ]æ–‡æœ¬2ï¼š[0.2 , 0.2 , 0. , 0. , 0. , 0.2 , 0.2 , 0.2 ]æ–‡æœ¬3ï¼š[0.25, 0. , 0. , 0.25, 0.25, 0. , 0. , 0.25] å„è¯å‡ºç°è¿‡çš„æ–‡æ¡£æ¬¡æ•°1[3, 1, 1, 1, 2, 2, 1, 2] æ€»æ–‡æ¡£æ•°ä¸º$3$ï¼Œå„è¯çš„é€†æ–‡æ¡£é¢‘ç‡(IDF)å‘é‡ è¿™é‡Œä½¿ç”¨scikit-learnå†…çš„æ–¹æ³•æ±‚è§£ 1[1. , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207] æ•…å„æ–‡æ¡£çš„TF-IDFå‘é‡ä¸º123456æ–‡æœ¬1ï¼š[0.2 , 0. , 0.33862944, 0. , 0.25753641, 0.51507283, 0. , 0. ]æ–‡æœ¬2ï¼š[0.2 , 0.33862944, 0. , 0. , 0. , 0.25753641, 0.33862944, 0.25753641]æ–‡æœ¬3ï¼š[0.25 , 0. , 0. , 0.4232868 , 0.32192052, 0. , 0. , 0.32192052] ç»å•ä½åŒ–åï¼Œæœ‰123456æ–‡æœ¬1ï¼š[0.28680065, 0. , 0.48559571, 0. , 0.36930805, 0.73861611, 0. , 0. ]æ–‡æœ¬2ï¼š[0.31544415, 0.53409337, 0. , 0. , 0. , 0.40619178, 0.53409337, 0.40619178]æ–‡æœ¬3ï¼š[0.37311881, 0. , 0. , 0.63174505, 0.4804584 , 0. , 0. , 0.4804584 ] 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; vec_num = np.array([ [1, 0, 1, 0, 1, 2, 0, 0], [1, 1, 0, 0, 0, 1, 1, 1], [1, 0, 0, 1, 1, 0, 0, 1] ])&gt;&gt;&gt; vec_tf = vec_num / np.sum(vec_num, axis=1).reshape(-1, 1)&gt;&gt;&gt; vec_tfarray([[0.2 , 0. , 0.2 , 0. , 0.2 , 0.4 , 0. , 0. ], [0.2 , 0.2 , 0. , 0. , 0. , 0.2 , 0.2 , 0.2 ], [0.25, 0. , 0. , 0.25, 0.25, 0. , 0. , 0.25]])&gt;&gt;&gt; vec_num[vec_num&gt;0] = 1&gt;&gt;&gt; n_showup = np.sum(vec_num, axis=0)&gt;&gt;&gt; n_showuparray([3, 1, 1, 1, 2, 2, 1, 2])&gt;&gt;&gt; d = 3&gt;&gt;&gt; vec_idf = np.log((d + 1) / (n_showup + 1)) + 1&gt;&gt;&gt; vec_idfarray([1. , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207])&gt;&gt;&gt; vec_tfidf = vec_tf * vec_idf&gt;&gt;&gt; vec_tfidfarray([[0.2 , 0. , 0.33862944, 0. , 0.25753641, 0.51507283, 0. , 0. ], [0.2 , 0.33862944, 0. , 0. , 0. , 0.25753641, 0.33862944, 0.25753641], [0.25 , 0. , 0. , 0.4232868 , 0.32192052, 0. , 0. , 0.32192052]])&gt;&gt;&gt; vec_tfidf = vec_tfidf / np.linalg.norm(vec_tfidf, axis=1).reshape((-1, 1))&gt;&gt;&gt; vec_tfidfarray([[0.28680065, 0. , 0.48559571, 0. , 0.36930805, 0.73861611, 0. , 0. ], [0.31544415, 0.53409337, 0. , 0. , 0. , 0.40619178, 0.53409337, 0.40619178], [0.37311881, 0. , 0. , 0.63174505, 0.4804584 , 0. , 0. , 0.4804584 ]]) éªŒè¯ä½¿ç”¨scikit-learnæœºå™¨å­¦ä¹ åŒ…è®¡ç®—ç»“æœ123456789101112&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer&gt;&gt;&gt; vectorizer = TfidfVectorizer()&gt;&gt;&gt; text = [ &quot;My dog ate my homework&quot;, &quot;My cat ate the sandwich&quot;, &quot;A dolphin ate the homework&quot;]&gt;&gt;&gt; vectorizer.fit_transform(text).toarray()array([[0.28680065, 0. , 0.48559571, 0. , 0.36930805, 0.73861611, 0. , 0. ], [0.31544415, 0.53409337, 0. , 0. , 0. , 0.40619178, 0.53409337, 0.40619178], [0.37311881, 0. , 0. , 0.63174505, 0.4804584 , 0. , 0. , 0.4804584 ]])&gt;&gt;&gt; vectorizer.get_feature_names()[&apos;ate&apos;, &apos;cat&apos;, &apos;dog&apos;, &apos;dolphin&apos;, &apos;homework&apos;, &apos;my&apos;, &apos;sandwich&apos;, &apos;the&apos;]]]></content>
      <categories>
        <category>Practice</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2018%2F10%2F23%2FSVD%2F</url>
    <content type="text"><![CDATA[å¼•è¨€å¥‡å¼‚å€¼åˆ†è§£Singular Value Decompositionæ˜¯çº¿æ€§ä»£æ•°ä¸­ä¸€ç§é‡è¦çš„çŸ©é˜µåˆ†è§£ï¼Œå¥‡å¼‚å€¼åˆ†è§£åˆ™æ˜¯ç‰¹å¾åˆ†è§£åœ¨ä»»æ„çŸ©é˜µä¸Šçš„æ¨å¹¿ã€‚åœ¨ä¿¡å·å¤„ç†ã€ç»Ÿè®¡å­¦ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚ åŸç†ä»ç‰¹å¾å€¼åˆ†è§£(EVD)è®²èµ·æˆ‘ä»¬çŸ¥é“å¯¹äºä¸€ä¸ª$n$é˜¶æ–¹é˜µ$A_{nÃ—n}$ï¼Œæœ‰ A\alpha_i = \lambda_i \alpha_i i = 1, ..., nå– P = \left[\alpha_1, \alpha_2, ..., \alpha_n\right]æœ‰ä¸‹å¼æˆç«‹ AP = P\Lambdaå…¶ä¸­ \Lambda = \left[ \begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_n \\ \end{matrix} \right] ç‰¹å¾å€¼ä¸€èˆ¬ä»å¤§åˆ°å°æ’åˆ— åˆ©ç”¨è¯¥å¼å¯å°†æ–¹é˜µ$A_{nÃ—n}$åŒ–ä½œå¯¹è§’é˜µ$\Lambda_{nÃ—n}$ \Lambda = P^{-1}APæˆ–è€… A = P \Lambda P^{-1} = \sum_{i=1}^n \lambda_i (P_{,i})(P_{,i})^{-1} â€œ$_{i}$â€è¡¨ç¤ºç¬¬$i$è¡Œï¼Œâ€œ$_{,i}$â€è¡¨ç¤ºç¬¬$i$åˆ— è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç†è§£ä¸ºï¼ŒçŸ©é˜µ$A$æ˜¯ç”±$n$ä¸ª$n$é˜¶çŸ©é˜µ$P_{,i}P^{-1}_{i}$åŠ æƒç»„æˆï¼Œç‰¹å¾å€¼$\lambda_i$å³ä¸ºæƒé‡ã€‚ ä»¥ä¸Šä¸ºä¸ªäººç†è§£ï¼Œä¸å¦¥ä¹‹å¤„å¯ä»¥æŒ‡å‡ºã€‚ å¥‡å¼‚å€¼åˆ†è§£(SVD)å®šä¹‰å¯¹äºé•¿æ–¹é˜µ$A_{mÃ—n}$ï¼Œä¸èƒ½è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå¯è¿›è¡Œå¦‚ä¸‹åˆ†è§£ A_{mÃ—n} = U_{mÃ—m} \Sigma_{mÃ—n} V_{nÃ—n}^Tå…¶ä¸­$U \in \mathbb{R}^{mÃ—m}, V \in \mathbb{R}^{nÃ—n}$ï¼Œå‡ä¸ºæ­£äº¤çŸ©é˜µã€‚çŸ©é˜µ$\Sigma_{mÃ—n}$å¦‚ä¸‹ å¯¹äº$m&gt;n$ \Sigma_{mÃ—n} = \left[ \begin{matrix} S_{nÃ—n} \\ --- \\ O_{(m-n)Ã—n} \end{matrix} \right] å¯¹äº$m&lt;n$ \Sigma_{mÃ—n} = \left[ \begin{matrix} S_{mÃ—m} & | & O_{mÃ—(n-m)} \end{matrix} \right] çŸ©é˜µ$S_{nÃ—n}$ä¸ºå¯¹è§’é˜µï¼Œå¯¹è§’å…ƒç´ ä»å¤§åˆ°å°æ’åˆ— S_{nÃ—n} = \left[ \begin{matrix} \sigma_1 & & \\ & ... & \\ & & \sigma_n\\ \end{matrix} \right]ç›´è§‚è¡¨ç¤ºSVDåˆ†è§£å¦‚ä¸‹ å½“å–$r&lt;n$æ—¶ï¼Œæœ‰éƒ¨åˆ†å¥‡å¼‚å€¼åˆ†è§£ï¼Œå¯ç”¨äºé™ç»´ A_{mÃ—n} = U_{mÃ—r} \Sigma_{rÃ—r} V_{rÃ—n}^Tè®¡ç®— ä»¥ä¸‹ä»…è€ƒè™‘$m&gt;n$çš„æƒ…å†µ ä»¤çŸ©é˜µ$A^T$ä¸$A$ç›¸ä¹˜ï¼Œæœ‰ A^TA = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T A^TA = V \Sigma^T \Sigma V^T çŸ©é˜µ$U$ä¸ºæ­£äº¤é˜µï¼Œå³æ»¡è¶³$U^TU=I$ å…¶ä¸­ \Sigma^T \Sigma = \left[ \begin{matrix} S^T_{nÃ—n} & | & O^T_{nÃ—(m-n)} \end{matrix} \right] \left[ \begin{matrix} S_{nÃ—n} \\ --- \\ O_{(m-n)Ã—n} \end{matrix} \right] = S_{nÃ—n}^2 = \left[ \begin{matrix} \sigma_1^2 & & \\ & ... & \\ & & \sigma_n^2\\ \end{matrix} \right] åˆ™ A^T A = V S^2 V^T å³çŸ©é˜µ$A^T A$ç›¸ä¼¼å¯¹è§’åŒ–ä¸º$S^2$ï¼Œå¯¹è§’å…ƒç´ $\sigma_i^2$ä¸çŸ©é˜µ$V$çš„åˆ—å‘é‡$v_i(i=1, â€¦, n)$ä¸ºçŸ©é˜µ$A^T A$çš„ç‰¹å¾å¯¹ã€‚ é‚£ä¹ˆå¯¹çŸ©é˜µ$A^T A$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œæœ‰ (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i åˆ™ v_i = \alpha^{(1)}_i \sigma_i = \sqrt{\lambda^{(1)}_i} æ³¨ï¼šå¯¹äºäºŒæ¬¡å‹$x^T (A^T A) x$ x^T (A^T A) x = (Ax)^T(Ax) \geq 0æ•…çŸ©é˜µ$A^T A$åŠæ­£å®šï¼Œ$\sigma_i = \sqrt{\lambda_i}$æœ‰è§£ åŒç†ï¼Œä»¤çŸ©é˜µ$A$ä¸$A^T$ç›¸ä¹˜ï¼Œå¯è¯å¾— A A^T = U \Sigma \Sigma^T U^T å…¶ä¸­ \Sigma \Sigma^T = \left[ \begin{matrix} S_{nÃ—n} \\ --- \\ O_{(m-n)Ã—n} \end{matrix} \right] \left[ \begin{matrix} S^T_{nÃ—n} & | & O^T_{nÃ—(m-n)} \end{matrix} \right] = \left[ \begin{matrix} S^2_{nÃ—n} & O_{nÃ—(m-n)} \\ O_{(m-n)Ã—n} & O_{(m-n)Ã—(m-n)} \end{matrix} \right] å³çŸ©é˜µ$A A^T$ç›¸ä¼¼å¯¹è§’åŒ–ï¼Œå¯¹è§’å…ƒç´ $\sigma_i^2$ä¸çŸ©é˜µ$U$çš„åˆ—å‘é‡$u_i(i=1, â€¦, m)$ä¸ºçŸ©é˜µ$A A^T$çš„ç‰¹å¾å¯¹ã€‚ å¯¹çŸ©é˜µ$A A^T$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œæœ‰ (A^T A) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i åˆ™ u_i = \alpha^{(2)}_i \sigma_i = \sqrt{\lambda^{(2)}_i} åŒç†å¯è¯å¾—$A A^T$åŠæ­£å®šï¼Œç•¥ã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œä¸ºå‡å°‘è®¡ç®—é‡ï¼Œè®¡ç®—å¥‡å¼‚å€¼åˆ†è§£åªè¿›è¡Œä¸€æ¬¡ç‰¹å¾å€¼åˆ†è§£ï¼Œå¦‚å¯¹äºçŸ©é˜µ$X_{mÃ—n}(m&gt;n)$ï¼Œé€‰å–$n$é˜¶çŸ©é˜µ$X^T X$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£è®¡ç®—$v_i$ï¼Œè®¡ç®—$u_i$æ–¹æ³•ä¸‹é¢ä»‹ç»ã€‚ æ ¹æ®å‰é¢æ¨å¯¼ï¼Œæˆ‘ä»¬æœ‰ç‰¹å¾å€¼åˆ†è§£ (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i (A A^T) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_iå…¶ä¸­$\lambda^{(1)}_i = \lambda^{(2)}_i = \sigma_i^2$ï¼Œ$v_i = \alpha^{(1)}_i$ï¼Œ$u_i = \alpha^{(2)}_i$ï¼Œå³ A^T A v_i = \sigma_i^2 v_i \tag{1} A A^T u_i = \sigma_i^2 u_i \tag{2}$(1)$å¼å·¦å³ä¹˜$A$ï¼Œæœ‰ A A^T A v_i = \sigma_i^2 A v_iå‘ç°ä»€ä¹ˆï¼Ÿè¿™æ˜¯å¦ä¸€ä¸ªç‰¹å¾å€¼åˆ†è§£çš„è¡¨è¾¾å¼ï¼ (A A^T) (A v_i) = \sigma_i^2 (A v_i)æ•… u_i \propto A v_i æˆ– u_i = k Â· A v_i \tag{3}ç°åœ¨æ±‚è§£ç³»æ•°$k$ï¼Œæ ¹æ®å®šä¹‰ A = U \Sigma V^T \Rightarrow AV = U \Sigmaåˆ™ A v_i = \sigma_i u_i \Rightarrow u_i = \frac{1}{\sigma_i} A v_iæˆ–è€… U = A V \Sigma^{-1} æ³¨ï¼šåªèƒ½æ±‚å‰$n$ä¸ª$u_i$ï¼Œä¹‹åçš„éœ€è¦åˆ—å†™æ–¹ç¨‹æ±‚è§£ ä¸¾æ —å°†çŸ©é˜µ$A$è¿›è¡Œåˆ†è§£ A = \left[ \begin{matrix} 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{matrix} \right]ä¸ºå‡å°‘è®¡ç®—é‡ï¼Œå–$A^T A$è®¡ç®— A^T A = \left[ \begin{matrix} 2 & 1 \\ 1 & 2 \end{matrix} \right]ç‰¹å¾å€¼åˆ†è§£ï¼Œæœ‰ A\left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] \left[ \begin{matrix} 3 & \\ & 1 \end{matrix} \right]æ•… \Sigma = \left[ \begin{matrix} \sqrt{3} & \\ & 1 \end{matrix} \right] V = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] U = A V \Sigma^{-1} = \left[ \begin{matrix} \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\ \frac{2}{\sqrt{6}} & 0 \\ \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \end{matrix} \right]123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; A = np.array([ [0, 1], [1, 1], [1, 0] ])&gt;&gt;&gt; ATA = A.T.dot(A)&gt;&gt;&gt; eigval, eigvec= np.linalg.eig(ATA)&gt;&gt;&gt; V = eigvec.copy()&gt;&gt;&gt; S = np.diag(np.sqrt(eigval))&gt;&gt;&gt; U = A.dot(V).dot(np.linalg.inv(S))&gt;&gt;&gt; Uarray([[ 0.40824829, 0.70710678], [ 0.81649658, 0. ], [ 0.40824829, -0.70710678]])&gt;&gt;&gt; Sarray([[1.73205081, 0. ], [0. , 1. ]])&gt;&gt;&gt; Varray([[ 0.70710678, -0.70710678], [ 0.70710678, 0.70710678]])&gt;&gt;&gt; # éªŒè¯&gt;&gt;&gt; U.dot(S).dot(V.T)array([[-2.23711432e-17, 1.00000000e+00], [ 1.00000000e+00, 1.00000000e+00], [ 1.00000000e+00, -2.23711432e-17]]) ç†è§£å±•å¼€è¡¨è¾¾å¼ï¼Œå–$r \leq n$æ—¶ï¼Œ A = U_{mÃ—r} \Sigma_{rÃ—r} V_{rÃ—n}^T = \sum_{i=1}^r \sigma_i (U_{,i}) (V_{,i})^Tå°±å¾—åˆ°ä¸PCAç›¸åŒçš„ç»“è®ºï¼ŒçŸ©é˜µ$A$å¯ç”±$r$ä¸ª$mÃ—n$çš„çŸ©é˜µ$(U_{,i}) (V_{,i})^T$åŠ æƒç»„æˆã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå‰$10\%$ç”šè‡³$1\%$çš„å¥‡å¼‚å€¼å°±å äº†å…¨éƒ¨å¥‡å¼‚å€¼ä¹‹å’Œçš„$99\%$ï¼Œæå¤§åœ°ä¿ç•™äº†ä¿¡æ¯ï¼Œè€Œå¤§å¤§å‡å°‘äº†å­˜å‚¨ç©ºé—´ã€‚ ä»¥å›¾ç‰‡ä¸ºä¾‹ï¼Œè‹¥åŸæœ‰24bitå›¾ç‰‡ï¼Œå…¶å¤§å°ä¸º(1024, 768)ï¼Œåˆ™ä¸è®¡å›¾ç‰‡ä¿¡æ¯ï¼Œä»…ä»…æ•°æ®å…±å 1024Ã—768Ã—3 Bï¼Œæˆ–2.25 MBã€‚ç”¨å¥‡å¼‚å€¼åˆ†è§£è¿›è¡Œå‹ç¼©ï¼Œä¿ç•™$60\%$çš„å¥‡å¼‚å€¼ï¼Œå¯è¾¾åˆ°å‡ ä¹æ— æŸçš„ç¨‹åº¦ï¼Œæ­¤æ—¶éœ€è¦ä¿å­˜å‘é‡çŸ©é˜µ$U_{1024Ã—60}$ï¼Œ$V_{60Ã—768}$ä»¥åŠ$60$ä¸ªå¥‡å¼‚å€¼ï¼Œä»¥æµ®ç‚¹æ•°float32å­˜å‚¨ï¼Œä¸€å…±å 420 KBå³å¯ã€‚ (1024 Ã— 60 + 60 Ã— 768 + 60) Ã— 4 / 2^{10} = 420.23è¯´å¥é¢˜å¤–è¯ï¼Œå­˜å‚¨é‡çš„å‹ç¼©å¿…ç„¶ä»¥è®¡ç®—é‡çš„å¢å¤§ä¸ºä»£ä»·ï¼Œç›¸åäº¦ç„¶ï¼Œæ‰€ä»¥éœ€è¦åè°ƒå¥½RAMä¸ROMå®¹é‡ï¼Œè€ƒè™‘è®¡ç®—æœºçš„è®¡ç®—é€Ÿåº¦ã€‚æ¢å¥è¯è¯´ï¼Œç©ºé—´å’Œæ—¶é—´ä¸Šå¿…ç„¶æ˜¯äº’è¡¥çš„ï¼Œå“²å­¦çš„å‘³é“hhhhã€‚ åˆ†è§£ç»“æœçš„ä¿¡æ¯ä¿ç•™åˆ†è§£åå„æ ·æœ¬é—´çš„æ¬§å¼è·ç¦»ä¸è§’åº¦ä¿¡æ¯åº”ä¸å˜ï¼Œç»™å‡ºè¯æ˜å¦‚ä¸‹è®¾æœ‰$m$ç»„$n$ç»´æ ·æœ¬æ ·æœ¬ X_{nÃ—m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]ç»å¥‡å¼‚å€¼åˆ†è§£ï¼Œæœ‰ X_{nÃ—m} = U_{nÃ—r} \Sigma_{rÃ—r} V_{rÃ—m}^Tè®° Z_{rÃ—m} = \Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]æœ‰ X = U Z æ¬§å¼è·ç¦» || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2 = \left[ U (Z^{(i)} - Z^{(j)}) \right]^T \left[ U (Z^{(i)} - Z^{(j)}) \right] = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)}) = || Z^{(i)} - Z^{(j)} ||_2^2 å³ || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2 è§’åº¦ä¿¡æ¯ \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \sqrt{(UZ^{(j)})^T(UZ^{(j)})}} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} å³ \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} ä»£ç @Github: Code of SVDå¯¹å›¾ç‰‡è¿›è¡Œäº†åˆ†è§£1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798class SVD(): &quot;&quot;&quot; Singular Value Decomposition Attributes: m &#123;int&#125; n &#123;int&#125; r &#123;int&#125;: if r == -1, then r = n isTrains &#123;bool&#125;: isTrains = True if input.shape[0] &lt; input.shape[1] U &#123;ndarray(m, r)&#125; S &#123;ndarray(r, )&#125; V &#123;ndarray(n, r)&#125; Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - Reassign r if eigvals contains zero - Singular values are stored in a 1-dim array `S` - X&apos; = U S V^T &quot;&quot;&quot; def __init__(self, r=-1): self.m = None self.n = None self.r = r self.isTrans = False self.U = None self.S = None self.V = None def fit(self, X): &quot;&quot;&quot; calculate components Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - reassign self.r if eigvals contains zero &quot;&quot;&quot; (self.m, self.n) = X.shape if self.m &lt; self.n: X = X.T self.m, self.n = self.n, self.m self.isTrans = True self.r = self.n if (self.r == -1) else self.r XTX = X.T.dot(X) eigval, eigvec = np.linalg.eig(X.T.dot(X)) eigval, eigvec = np.real(eigval), np.real(eigvec) self.S = np.sqrt(np.clip(eigval, 0, float(&apos;inf&apos;))) self.S = self.S[self.S &gt; 0] self.r = min(self.r, self.S.shape[0]) # reassign self.r order = np.argsort(eigval)[::-1][: self.r] # sort eigval from large to small eigval = eigval[order]; eigvec = eigvec[:, order] self.V = eigvec.copy() self.U = X.dot(self.V).dot( np.linalg.inv(np.diag(self.S))) return self.U, self.S, self.V def compose(self, r=-1): &quot;&quot;&quot; merge first r components Parameters: r &#123;int&#125;: if r==-1, merge all components Returns: X &#123;ndarray(m, n)&#125; &quot;&quot;&quot; if r == -1: X = self.U.dot(np.diag(self.S)).dot(self.V.T) X = X.T if self.isTrans else X else: (m, n) = (self.n, self.m) if self.isTrans else (self.m, self.n) X = np.zeros(shape=(m, n)) for i in range(r): X += self.__getitem__(i) return X def __getitem__(self, idx): &quot;&quot;&quot; get a component Parameters: index &#123;int&#125;: range from (0, self.r) &quot;&quot;&quot; u = self.U[:, idx] v = self.V[:, idx] s = self.S[idx] x = s * u.reshape(self.m, 1).\ dot(v.reshape(1, self.n)) x = x.T if self.isTrans else x return x def showComponets(self, r=-1): &quot;&quot;&quot; display components Notes: - Resize components&apos; shape into (40, 30) &quot;&quot;&quot; m, n = self.m, self.n r = self.r if r==-1 else r n_images = 10; m_images = r // n_images + 1 m_size, n_size = 40, 30 showfig = np.zeros(shape=(m_images*m_size, n_images*n_size)) for i in range(r): m_pos = i // n_images n_pos = i % n_images component = self.__getitem__(i) component = component.T if self.isTrans else component component = cv2.resize(component, (30, 40)) showfig[m_pos*m_size: (m_pos+1)*m_size, n_pos*n_size: (n_pos+1)*n_size] = component plt.figure(&apos;components&apos;) plt.imshow(showfig) plt.show() ç”¨ä¸Šé¢çš„ä»£ç è¿›è¡Œå®éªŒ1234567891011121314# è¯»å–ä¸€å¼ å›¾ç‰‡X = load_images()[0].reshape((32, 32))showmat2d(X)# å¯¹å›¾ç‰‡è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£decomposer = SVD(r=-1)decomposer.fit(X)# æ˜¾ç¤ºä¸€ä¸‹åˆ†é‡decomposer.showComponets(r=-1)# å°†å…¨éƒ¨åˆ†é‡ç»„åˆï¼Œå¹¶æ˜¾ç¤ºX_ = decomposer.compose(r=-1)showmat2d(X_)# å°†å‰5ä¸ªåˆ†é‡ç»„åˆï¼Œå¹¶æ˜¾ç¤ºX_ = decomposer.compose(r=5)showmat2d(X_) è½½å…¥åŸå›¾å¦‚ä¸‹ åˆ†é‡æ˜¾ç¤ºå¦‚ä¸‹ ç»„åˆåˆ†é‡æ˜¾ç¤ºå¦‚ä¸‹ ç»„åˆå…¨éƒ¨ ç»„åˆå‰5ä¸ªåˆ†é‡]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[åˆ é™¤åœç”¨è¯]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[åˆ é™¤åœç”¨è¯ - Pythonæ–‡æœ¬å¤„ç†æ•™ç¨‹â„¢ åœç”¨è¯æ˜¯å¯¹å¥å­æ²¡æœ‰å¤šå¤§æ„ä¹‰çš„è‹±è¯­å•è¯ã€‚ åœ¨ä¸ç‰ºç‰²å¥å­å«ä¹‰çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å®‰å…¨åœ°å¿½ç•¥å®ƒä»¬ã€‚ ä¾‹å¦‚ï¼Œthe, he, haveç­‰ç­‰çš„å•è¯å·²ç»åœ¨åä¸ºè¯­æ–™åº“çš„è¯­æ–™åº“ä¸­æ•è·äº†è¿™äº›å•è¯ã€‚ ä¸‹è½½è¯­æ–™åº“ å®‰è£…nltkæ¨¡å— 1pip install nltk ä¸‹è½½è¯­æ–™åº“ 12import nltknltk.download(&apos;stopwords&apos;) ä½¿ç”¨åº“æ–™åº“ éªŒè¯åœç”¨è¯ 123456789101112131415161718192021222324252627&gt;&gt;&gt; from nltk.corpus import stopwords&gt;&gt;&gt; stopwords.words(&apos;english&apos;)[&apos;i&apos;, &apos;me&apos;, &apos;my&apos;, &apos;myself&apos;, &apos;we&apos;, &apos;our&apos;, &apos;ours&apos;, &apos;ourselves&apos;, &apos;you&apos;, &quot;you&apos;re&quot;, &quot;you&apos;ve&quot;, &quot;you&apos;ll&quot;, &quot;you&apos;d&quot;, &apos;your&apos;, &apos;yours&apos;, &apos;yourself&apos;, &apos;yourselves&apos;, &apos;he&apos;, &apos;him&apos;, &apos;his&apos;, &apos;himself&apos;, &apos;she&apos;,&quot;she&apos;s&quot;, &apos;her&apos;, &apos;hers&apos;, &apos;herself&apos;, &apos;it&apos;, &quot;it&apos;s&quot;, &apos;its&apos;, &apos;itself&apos;, &apos;they&apos;, &apos;them&apos;, &apos;their&apos;, &apos;theirs&apos;, &apos;themselves&apos;, &apos;what&apos;, &apos;which&apos;, &apos;who&apos;, &apos;whom&apos;, &apos;this&apos;, &apos;that&apos;, &quot;that&apos;ll&quot;, &apos;these&apos;, &apos;those&apos;, &apos;am&apos;, &apos;is&apos;, &apos;are&apos;, &apos;was&apos;, &apos;were&apos;, &apos;be&apos;, &apos;been&apos;, &apos;being&apos;, &apos;have&apos;, &apos;has&apos;, &apos;had&apos;, &apos;having&apos;, &apos;do&apos;, &apos;does&apos;, &apos;did&apos;, &apos;doing&apos;, &apos;a&apos;, &apos;an&apos;, &apos;the&apos;, &apos;and&apos;, &apos;but&apos;, &apos;if&apos;, &apos;or&apos;, &apos;because&apos;, &apos;as&apos;, &apos;until&apos;, &apos;while&apos;, &apos;of&apos;, &apos;at&apos;, &apos;by&apos;, &apos;for&apos;, &apos;with&apos;, &apos;about&apos;, &apos;against&apos;, &apos;between&apos;, &apos;into&apos;, &apos;through&apos;, &apos;during&apos;, &apos;before&apos;, &apos;after&apos;, &apos;above&apos;, &apos;below&apos;, &apos;to&apos;, &apos;from&apos;, &apos;up&apos;, &apos;down&apos;, &apos;in&apos;, &apos;out&apos;, &apos;on&apos;, &apos;off&apos;, &apos;over&apos;, &apos;under&apos;, &apos;again&apos;, &apos;further&apos;, &apos;then&apos;, &apos;once&apos;, &apos;here&apos;, &apos;there&apos;, &apos;when&apos;,&apos;where&apos;, &apos;why&apos;, &apos;how&apos;, &apos;all&apos;, &apos;any&apos;, &apos;both&apos;, &apos;each&apos;, &apos;few&apos;, &apos;more&apos;, &apos;most&apos;, &apos;other&apos;, &apos;some&apos;, &apos;such&apos;, &apos;no&apos;, &apos;nor&apos;, &apos;not&apos;, &apos;only&apos;, &apos;own&apos;, &apos;same&apos;, &apos;so&apos;, &apos;than&apos;, &apos;too&apos;, &apos;very&apos;, &apos;s&apos;, &apos;t&apos;, &apos;can&apos;, &apos;will&apos;, &apos;just&apos;, &apos;don&apos;, &quot;don&apos;t&quot;, &apos;should&apos;, &quot;should&apos;ve&quot;, &apos;now&apos;, &apos;d&apos;, &apos;ll&apos;, &apos;m&apos;, &apos;o&apos;, &apos;re&apos;, &apos;ve&apos;, &apos;y&apos;, &apos;ain&apos;, &apos;aren&apos;, &quot;aren&apos;t&quot;, &apos;couldn&apos;, &quot;couldn&apos;t&quot;, &apos;didn&apos;, &quot;didn&apos;t&quot;, &apos;doesn&apos;, &quot;doesn&apos;t&quot;, &apos;hadn&apos;, &quot;hadn&apos;t&quot;, &apos;hasn&apos;, &quot;hasn&apos;t&quot;, &apos;haven&apos;, &quot;haven&apos;t&quot;, &apos;isn&apos;, &quot;isn&apos;t&quot;, &apos;ma&apos;, &apos;mightn&apos;, &quot;mightn&apos;t&quot;, &apos;mustn&apos;,&quot;mustn&apos;t&quot;, &apos;needn&apos;, &quot;needn&apos;t&quot;, &apos;shan&apos;, &quot;shan&apos;t&quot;, &apos;shouldn&apos;, &quot;shouldn&apos;t&quot;, &apos;wasn&apos;, &quot;wasn&apos;t&quot;, &apos;weren&apos;, &quot;weren&apos;t&quot;, &apos;won&apos;, &quot;won&apos;t&quot;, &apos;wouldn&apos;, &quot;wouldn&apos;t&quot;] é™¤äº†è‹±è¯­ä¹‹å¤–ï¼Œå…·æœ‰è¿™äº›åœç”¨è¯çš„å„ç§è¯­è¨€å¦‚ä¸‹ã€‚ 12345&gt;&gt;&gt; stopwords.fileids()[&apos;arabic&apos;, &apos;azerbaijani&apos;, &apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, &apos;french&apos;, &apos;german&apos;, &apos;greek&apos;, &apos;hungarian&apos;, &apos;indonesian&apos;, &apos;italian&apos;, &apos;kazakh&apos;, &apos;nepali&apos;, &apos;norwegian&apos;, &apos;portuguese&apos;, &apos;romanian&apos;, &apos;russian&apos;,&apos;spanish&apos;, &apos;swedish&apos;, &apos;turkish&apos;] ç¤ºä¾‹ ä»å•è¯åˆ—è¡¨ä¸­åˆ é™¤åœç”¨è¯ã€‚ 12345678910111213&gt;&gt;&gt; from nltk.corpus import stopwords&gt;&gt;&gt; en_stops = set(stopwords.words(&apos;english&apos;))&gt;&gt;&gt; &gt;&gt;&gt; all_words = [&apos;There&apos;, &apos;is&apos;, &apos;a&apos;, &apos;tree&apos;,&apos;near&apos;,&apos;the&apos;,&apos;river&apos;]&gt;&gt;&gt; for word in all_words: if word not in en_stops: print(word) Theretreenearriver]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PCA]]></title>
    <url>%2F2018%2F10%2F22%2FPCA%2F</url>
    <content type="text"><![CDATA[å¼•è¨€PCAå…¨ç§°Principal Component Analysisï¼Œå³ä¸»æˆåˆ†åˆ†æï¼Œæ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°æ®é™ç»´æ–¹æ³•ã€‚å®ƒå¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢å°†åŸå§‹æ•°æ®å˜æ¢ä¸ºä¸€ç»„å„ç»´åº¦çº¿æ€§æ— å…³çš„è¡¨ç¤ºï¼Œä»¥æ­¤æ¥æå–æ•°æ®çš„ä¸»è¦çº¿æ€§åˆ†é‡ã€‚ å‘é‡çš„æŠ•å½±ç°æœ‰ä¸¤ä¸ªä»»æ„ä¸å…±çº¿å‘é‡$\vec{u}, \vec{v}$ï¼Œå°†$\vec{u}$æŠ•å°„åˆ°$\vec{v}$ä¸Š æŠ•å½±åï¼Œå¯ä»¥å¾—åˆ°ä¸¤ä¸ªæ­£äº¤å‘é‡ \vec{u}' Â· (\vec{u} - \vec{u}') = 0æˆ‘ä»¬è®¾ \vec{u}' = \mu \vec{v} \tag{1}ä»£å…¥åæœ‰ \mu \vec{v} Â· (\vec{u} - \mu \vec{v}) = 0å¼•å…¥çŸ©é˜µè¿ç®—ï¼Œå³ (\mu v)^T (u - \mu v) = 0æœ‰ v^T u = \mu v^T våˆ™å¾—åˆ°$uâ€™$ä»¥$v$ä¸ºåŸºå‘é‡çš„åæ ‡ \mu = (v^T v)^{-1} v^T u \tag{2}æ‰€ä»¥å¾—åˆ° u' = v (v^T v)^{-1} v^T u \tag{*} åæ ‡å˜æ¢æ±‚è§£æŠ•å½±å‘é‡ï¼š$uâ€™$å¯è§†ä½œ$u$ç»åæ ‡å˜æ¢$uâ€™ = P u$å¾—åˆ°ï¼Œæ‰€ä»¥ P = v (v^T v)^{-1} v^T æ¨å¹¿è‡³å¤šä¸ªå‘é‡çš„æŠ•å½±ï¼Œå³å¾—åˆ° P = X (X^T X)^{-1} X^Tè¿™ä¸çº¿æ€§å›å½’ä¸­å¾—åˆ°çš„ç»“è®ºä¸€è‡´ã€‚ å®é™…ä¸Š u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T uè®°å•ä½å‘é‡$\frac{v}{||v||}$ä¸º$v_0$ï¼Œå¾—åˆ° u' = v_0 v_0^T uç”±å‡ ä½•å…³ç³»ï¼Œå¯ä»¥è®¡ç®—å¾—æŠ•å½±åçš„é•¿åº¦ä¸º d = ||u|| \cos \theta = ||u|| \frac{v^T u}{||u||||v||} = v_0^T uæ‰€ä»¥åœ¨å‘é‡æŠ•å½±ä¸­ï¼Œ$u^T v_0$è¡¨ç¤ºä»¥$v_0$ä¸ºåŸºå‘é‡çš„åæ ‡ã€‚ PCAç°åœ¨æœ‰$N$ç»´æ•°æ®é›†$D=\{x^{(1)}, x^{(2)}, â€¦, x^{(M)}\}$ï¼Œå…¶ä¸­$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, â€¦, x^{(i)}_N\right]^T$ï¼Œå„ç»´ç‰¹å¾$D_{j}$é—´å­˜åœ¨çº¿æ€§ç›¸å…³æ€§ï¼Œåˆ©ç”¨ä¸»æˆåˆ†åˆ†æå¯ä½¿ æ•°æ®ç»´åº¦é™ä½ï¼› æå–ä¸»æˆåˆ†ï¼Œä¸”å„æˆåˆ†é—´ä¸ç›¸å…³ã€‚ è¯´æ˜ ç”±äºé€‰å–çš„ç‰¹å¾è½´æ˜¯æ­£äº¤çš„ï¼Œæ‰€ä»¥è®¡ç®—ç»“æœçº¿æ€§æ— å…³ï¼› æå–äº†æ–¹å·®è¾ƒå¤§çš„å‡ ä¸ªç‰¹å¾ï¼Œä¸ºä¸»è¦çº¿æ€§åˆ†é‡ã€‚ ä»¥äºŒç»´ç©ºé—´ä¸­çš„æ•°æ®$x^{(i)} = \left[\begin{matrix} x^{(i)}_1 \\ x^{(i)}_2\end{matrix}\right]$ä¸ºä¾‹ï¼Œç»´åº¦å¯é™è‡³ä¸€ç»´ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ ä¸»è½´å¯æœ‰æ— ç©·å¤šç§é€‰æ‹©ï¼Œé‚£ä¹ˆé—®é¢˜å°±æ˜¯å¦‚ä½•é€‰å–æœ€ä¼˜çš„ä¸»è½´ã€‚å…ˆç»™å‡ºPCAçš„è®¡ç®—æ­¥éª¤ã€‚ è®¡ç®—æ­¥éª¤è¾“å…¥çš„$M$ä¸ª$N$ç»´æ ·æœ¬ï¼Œæœ‰æ ·æœ¬çŸ©é˜µ X_{NÃ—M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right] = \left[ \begin{matrix} x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\ x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\ ... \\ x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\ \end{matrix} \right]æŠ•å½± å¯¹æ¯ä¸ªç»´åº¦(è¡Œ)è¿›è¡Œå»å‡å€¼åŒ– X_j := X_j - \mu_j å…¶ä¸­$\mu_j = \overline{X_j}$ï¼Œ$j = 1, 2, â€¦, N$ æ±‚å„ç»´åº¦é—´çš„åæ–¹å·®çŸ©é˜µ$\Sigma_{NÃ—N}$ \Sigma_{ij} = Cov(x_i, x_j) æˆ– \Sigma = \frac{1}{M} X X^T æ³¨ï¼š X X^T = \left[ \begin{matrix} \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\ \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M \left[ \begin{matrix} x^{(i)}_1 x^{(i)}_1 & x^{(i)}_1 x^{(i)}_2 & ... & x^{(i)}_1 x^{(i)}_N \\ x^{(i)}_2 x^{(i)}_1 & x^{(i)}_2 x^{(i)}_2 & ... & x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ x^{(i)}_N x^{(i)}_1 & x^{(i)}_N x^{(i)}_2 & ... & x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M x^{(i)} x^{(i)T} åæ–¹å·®å®šä¹‰å¼ Cov(x,y)â‰\frac{1}{n-1} âˆ‘_{i=1}^n (x_iâˆ’\overline{x})^T(y_iâˆ’\overline{y})å…¶ä¸­$x=[x_1, x_2, â€¦, x_n]^T, y=[y_1, y_2, â€¦, y_n]^T$ æ±‚åæ–¹å·®çŸ©é˜µ$\Sigma$çš„ç‰¹å¾å€¼$Î»_i$åŠå…¶å¯¹åº”ç‰¹å¾å‘é‡$Î±_i$ï¼Œ$i=1, â€¦, N$ï¼› æŒ‰ç…§ç‰¹å¾å€¼ä»å¤§åˆ°å°æ’åˆ—ç‰¹å¾å¯¹$(Î»_i,Î±_i)$ï¼Œé€‰å–$K$ä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ä½œä¸ºé™ç»´åçš„ä¸»è½´$ \beta_1, \beta_2, â€¦, \beta_K $ï¼Œå…¶ä¸­$\beta_k$ä¸ºå•ä½å‘é‡ \beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^Tè®° B_{NÃ—K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]$K$çš„é€‰å–æ–¹æ³•æœ‰å¦‚ä¸‹ä¸¤ç§ï¼š æŒ‡å®šé€‰å–$K$ä¸ªä¸»è½´ ä¿ç•™$99\%$çš„æ–¹å·®\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99 å°†æ ·æœ¬ç‚¹æŠ•å°„åˆ°$K$ç»´åæ ‡ç³»ä¸Š æ ·æœ¬$X^{(i)}$æŠ•å°„åˆ°ä¸»æˆåˆ†è½´$\beta_k$ä¸Šï¼Œå…¶åæ ‡è¡¨ç¤ºä¸ºå‘é‡ï¼Œä¸º S^{(i)}_k = X^{(i)T}\beta_k æ³¨æ„æ­¤æ—¶çš„åŸºåº§æ ‡ä¸º$\beta_k$ï¼Œæˆ–è€…è¯´$Xâ€™^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$ æ‰€æœ‰æ ·æœ¬åœ¨ä¸»è½´$\beta_k$ä¸Šçš„æŠ•å½±åæ ‡å³ S = B^T X å…¶ä¸­$S_{KÃ—M}$ï¼Œ$B_{NÃ—K}$ï¼Œ$X_{NÃ—M}$ æ³¨ï¼šè‹¥å–$K=N$ï¼Œå¯é‡å»ºæ•°æ®ï¼Œå¦‚ä¸‹ å¤åŸç¬¬$5$æ­¥ä¸­ï¼Œæ ·æœ¬ç‚¹å‘é‡$X^{(i)}$çš„ä¸»è¦åˆ†é‡æŠ•å°„åˆ°$K$ä¸ª$N$ç»´å‘é‡ä¸Šï¼ŒæŠ•å½±åæ ‡ä¸º$S^{(i)}_k$ï¼Œå³ X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_kä»¥ä¸Šå°±æ˜¯æ ·æœ¬ç‚¹çš„å¤åŸå…¬å¼ï¼ŒçŸ©é˜µå½¢å¼å³ \hat{X} = BSå…¶ä¸­$\hat{X}_{NÃ—M}$ï¼Œ$B_{NÃ—K}$ï¼Œ$S_{KÃ—M}$ è€ƒè™‘åˆ°å·²å»å‡å€¼åŒ–ï¼Œæ•… \hat{X}_j \approx \hat{X}_j + \mu_jè¯æ˜ æŠ•å½±å‘é‡çš„$2$èŒƒæ•°æœ€å¤§ï¼Œæˆ–è€…è¯´ï¼ŒæŠ•å½±åçš„åæ ‡å¹³æ–¹å’Œæœ€å¤§ å½“æ‰€æœ‰æ ·æœ¬$X$æŠ•å°„åˆ°ç¬¬ä¸€ä¸»è½´$\beta_1$ä¸Šï¼Œå…¶åæ ‡ä¸º S_1 = X^T \beta_1æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œï¼Œæˆ–å‘é‡$S_1$çš„$2$èŒƒæ•°ä¸º ||S_1||_2^2 = S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}å³ä¼˜åŒ–ç›®æ ‡ä¸º \max ||S_1||_2^2 s.t. ||\beta_1||_2^2 = 1çŸ©é˜µ$C=XX^T$ä¸ºå¯¹ç§°çŸ©é˜µï¼Œæ•…å¯å•ä½æ­£äº¤åŒ– C = W \Lambda W^T W = \left[\begin{matrix} | & & |\\ w_1 & ... & w_M\\ | & & |\\ \end{matrix}\right] \Lambda = \left[\begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_M\\ \end{matrix}\right]å…¶ä¸­$\lambda_1 &gt; â€¦&gt; \lambda_M$ï¼Œ$w_i(i=1,â€¦,M)$ä¸ºçŸ©é˜µ$C$çš„ç‰¹å¾å‘é‡(å•ä½å‘é‡ï¼Œäº’ç›¸æ­£äº¤) å®é™…ä¸Š$R(C) \leq (n-1)$ï¼Œå³æœ€å¤šæœ‰$(n-1)$ä¸ªç‰¹å¾å€¼å¤§äº$0$ã€‚ ||S_1||_2^2 = \beta_1^T W \Lambda W^T \beta_1 \tag{2}ä»¤$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$ï¼Œå¯å¾— ||S_1||_2^2 = \alpha_1^T \Lambda \alpha_1 \tag{3}å³ ||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \tag{4}è¿›ä¸€æ­¥ \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \leq \lambda_1 \sum_{i=1}^M \alpha_{1i}^2 \tag{5}ä¸”ç”±äº$\beta_1^T\beta_1 = 1$ï¼Œæ•… 1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = \sum_{i=1}^M \alpha_{1i}^2å¯å¾— ||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \leq \lambda_1 \tag{6}ä¸ºä½¿$(6)$å–ç­‰å·ï¼Œå³è¾¾æœ€å¤§å€¼ï¼Œå¯ä½¿ \begin{cases} \alpha_{11} = 1 \\ \alpha_{12} = ... = \alpha_{1M} = 0 \end{cases}å³ä»¤ \beta_1 = W \alpha_1 = w_1 $\alpha_1 = [1, 0, â€¦, 0]^T$ æ‰€ä»¥$\beta_1$å¯¹åº”çŸ©é˜µ$C=XX^T$çš„ç‰¹å¾å‘é‡$w_1$ï¼Œä¸”æœ‰ ||S_1||_2^2 = \lambda_1 æˆ–è€…ç¬¬ä¸€ä¸»æˆåˆ†çš„è¯æ˜ä¹Ÿå¯ä»¥è¿™æ ·ï¼Œå»ºç«‹ä¼˜åŒ–ç›®æ ‡ \beta_1 = \arg \max ||S_1||_2^2s.t. ||\beta_1||_2^2 = 1æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•° L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)ä¹Ÿå³ L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)æ±‚å…¶æå€¼ç‚¹ â–½_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0æœ‰ X X^T \beta_1 = \lambda_1 \beta_1å¯è§$\beta_1$å³æ–¹é˜µ$X X^T$çš„ç‰¹å¾å‘é‡ å½“æˆ‘ä»¬å¸Œæœ›ç”¨æ›´å¤šçš„ä¸»æˆåˆ†åˆ»ç”»æ•°æ®ï¼Œå¦‚å·²ç»æ±‚å¾—ä¸»æˆåˆ†$\beta_1, â€¦, \beta_{r-1}$ï¼Œå…ˆéœ€æ±‚è§£$\beta_r$ï¼Œå¼•å…¥æ­£äº¤çº¦æŸ$\beta_r^T \beta_i = 0$ï¼Œå³ç›®æ ‡å‡½æ•°ä¸º ||S_r||_2^2 = \beta_r^T C \beta_r s.t. \beta_r^T \beta_i = 0, i = 1, ..., r-1 ||\beta_r||_2^2 = 1ä»¤$\beta_r = W \alpha_r$ï¼Œåˆ™ ||S_r||_2^2 = \alpha_r^T \Lambda \alpha_r = \sum_i \lambda_i \alpha_{ri}^2è€Œæ ¹æ®æ­£äº¤çº¦æŸ 0 = \beta_r^T \beta_i = \alpha_r^T W^T w_i = \alpha_{ri}, i = 1, ..., r-1 $ W^T w_i = \left[0, â€¦, 1_i, â€¦, 0\right]^T$ æ‰€ä»¥ ||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{5}åˆå› ä¸º$\beta_r^T \beta_r = 1$(å•ä½å‘é‡)ï¼Œæ•… \beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1äºæ˜¯ç±»ä¼¼çš„ï¼Œä¸ºä½¿$(5)$å–æœ€å¤§ï¼Œå– \begin{cases} \alpha_{rr} = 1\\ \alpha_{ri} = 0, i = 1, ..., M, i \neq r \end{cases} $\alpha_r = [0, â€¦, 1_r, â€¦, 0]$ åˆ™æ­¤æ—¶ \beta_r = W \alpha_r = w_rä¸”æœ‰ ||S_r||_2^2 = \lambda_rè¯æ¯•ã€‚ ç™½åŒ–(whitening)whiteningçš„ç›®çš„æ˜¯å»æ‰æ•°æ®ä¹‹é—´çš„ç›¸å…³è”åº¦ï¼Œæ˜¯å¾ˆå¤šç®—æ³•è¿›è¡Œé¢„å¤„ç†çš„æ­¥éª¤ã€‚æ¯”å¦‚è¯´å½“è®­ç»ƒå›¾ç‰‡æ•°æ®æ—¶ï¼Œç”±äºå›¾ç‰‡ä¸­ç›¸é‚»åƒç´ å€¼æœ‰ä¸€å®šçš„å…³è”ï¼Œæ‰€ä»¥å¾ˆå¤šä¿¡æ¯æ˜¯å†—ä½™çš„ã€‚è¿™æ—¶å€™å»ç›¸å…³çš„æ“ä½œå°±å¯ä»¥é‡‡ç”¨ç™½åŒ–æ“ä½œã€‚ æ•°æ®çš„whiteningå¿…é¡»æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š ä¸åŒç‰¹å¾é—´ç›¸å…³æ€§æœ€å°ï¼Œæ¥è¿‘$0$ï¼› æ‰€æœ‰ç‰¹å¾çš„æ–¹å·®ç›¸ç­‰ï¼ˆä¸ä¸€å®šä¸º$1$ï¼‰ã€‚ å¸¸è§çš„ç™½åŒ–æ“ä½œæœ‰PCA whiteningå’ŒZCA whiteningã€‚ Whitening - Ufldl PCA whitening PCA whiteningæŒ‡å°†æ•°æ®$X$ç»è¿‡PCAé™ç»´ä¸º$S$åï¼Œå¯ä»¥çœ‹å‡º$S$ä¸­æ¯ä¸€ç»´æ˜¯ç‹¬ç«‹çš„ï¼Œæ»¡è¶³whiteningçš„ç¬¬ä¸€ä¸ªæ¡ä»¶ï¼Œè¿™æ˜¯åªéœ€è¦å°†$S$ä¸­çš„æ¯ä¸€ç»´éƒ½é™¤ä»¥æ ‡å‡†å·®å°±å¾—åˆ°äº†æ¯ä¸€ç»´çš„æ–¹å·®ä¸º$1$ï¼Œä¹Ÿå°±æ˜¯è¯´æ–¹å·®ç›¸ç­‰ã€‚ X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}} ZCA whitening ZCA whiteningæ˜¯æŒ‡æ•°æ®$X$å…ˆç»è¿‡PCAå˜æ¢ä¸º$S$ï¼Œä½†æ˜¯å¹¶ä¸é™ç»´ï¼Œå› ä¸ºè¿™é‡Œæ˜¯æŠŠæ‰€æœ‰çš„æˆåˆ†éƒ½é€‰è¿›å»äº†ã€‚è¿™æ˜¯ä¹ŸåŒæ ·æ»¡è¶³whtienningçš„ç¬¬ä¸€ä¸ªæ¡ä»¶ï¼Œç‰¹å¾é—´ç›¸äº’ç‹¬ç«‹ã€‚ç„¶ååŒæ ·è¿›è¡Œæ–¹å·®ä¸º$1$çš„æ“ä½œï¼Œæœ€åå°†å¾—åˆ°çš„çŸ©é˜µå·¦ä¹˜ä¸€ä¸ªç‰¹å¾å‘é‡çŸ©é˜µ$U$å³å¯ã€‚ X_{ZCAwhite} = U Â· X_{PCAwhite} Kernel PCAKernel PCAçš„æ€æƒ³æ˜¯åœ¨é«˜ç»´çš„ç‰¹å¾ç©ºé—´ä¸­æ±‚è§£åæ–¹å·®çŸ©é˜µ \Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^Tå…¶ä¸­$\Phi(X^{(i)})$è¡¨ç¤ºå°†æ ·æœ¬$i$æ˜ å°„åˆ°é«˜ç»´ç©ºé—´åä¸­çš„å‘é‡ï¼Œå³ \Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^Tå…¶ä¸­$Nâ€™ &gt; N$ï¼Œç”±äº$\Phi(X^{(i)})$ä¸ºéšå¼çš„ï¼Œæ•…è®¾ç½®æ ¸å‡½æ•°æ±‚è§£ï¼Œè®° \kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T å…³äºæ ¸æŠ€å·§ï¼Œç§»æ­¥éçº¿æ€§æ”¯æŒå‘é‡æœº åº”ç”¨å¯åˆ©ç”¨PCAä¸çº¿æ€§å›å½’æ±‚è§£$3$ç»´ç©ºé—´ä¸­å¹³é¢çš„æ³•å‘é‡ åˆ©ç”¨PCAé‡å»ºæ•°æ®(ä¸é™ç»´ï¼Œæ­¤æ—¶ä¸º$3$ç»´)ï¼Œæ­¤æ—¶ç¬¬$1, 2$ä¸»æˆåˆ†è½´å¯å¼ æˆæ‰€æ±‚å¹³é¢ï¼Œå³è¯¥å¹³é¢å¯è¡¨ç¤ºä¸º \Pi = span \{ \beta_1, \beta_2 \} å°±æ˜¯è¯´ï¼Œç¬¬ä¸€ã€äºŒä¸»æˆåˆ†æ˜¯è¿™äº›ç‚¹â€œæ‹‰ä¼¸â€æœ€å¤§çš„æ–¹å‘ :-)ï¼Œå¥½æ‡‚ä¸ï¼Ÿ ç”±æ­£äº¤æŠ•å½±å¯çŸ¥ï¼Œå¹³é¢å¤–ä¸€ç‚¹$y$å¯é€šè¿‡æœ€å°äºŒä¹˜(çº¿æ€§å›å½’)çš„æ–¹æ³•æŠ•å°„åˆ°å¹³é¢ä¸Šï¼Œå‘é‡è¿ç®—ï¼Œä¸è€ƒè™‘åç½®é¡¹ï¼Œå³ \hat{y} = \theta_1 x_1 + \theta_2 x_2 \tag{*} å…¶ä¸­$x_1, x_2$è¡¨ç¤ºç¬¬ä¸€ã€ç¬¬äºŒä¸»æˆåˆ†$\beta_1, \beta_2$ï¼Œä¸º$3$ç»´å‘é‡ \hat{y} = \left[ \begin{matrix} \hat{y_1} \\ \hat{y_2} \\ \hat{y_3} \\ \end{matrix} \right] x_i = \left[ \begin{matrix} x_{i1} \\ x_{i2} \\ x_{i3} \\ \end{matrix} \right] å¯åˆ©ç”¨å…¬å¼æ±‚è§£å›å½’å‚æ•°$\theta$ \theta = (X^TX+\lambda I)^{-1} X^T y æ³¨æ„ï¼š$X(n_samples, n_features)$ï¼Œè¿™é‡ŒæŠŠ$(x_{1j}, x_{2j}, y_{j})ä½œä¸ºä¸€ç»„æ ·æœ¬$ æ­¤æ—¶è¯¥å‚æ•°è¡¨ç¤ºåœ¨ä¸»è½´ä¸Šçš„åæ ‡$(\theta_1, \theta_2)$ï¼Œå¸¦å›$(*))$å³å¯è§£å¾—$\hat{y}$ \hat{y} = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*} é€šä¿—ç†è§£ï¼Œä¸€æŒæŠŠ$y$æ‹å¹³åœ¨äº†å¹³é¢$\Pi$ä¸Šï¼Œå˜æˆäº†$\hat{y}$ï¼Œä½†æ˜¯å“ªæœ‰è¿™ä¹ˆå¥½æ‹ã€‚ã€‚ã€‚è¿™ä¸ªæ—¶å€™åˆºåœ¨æŒå¿ƒé‡Œä¸€å®šæœ‰ä¸€ä¸ªå‚ç›´çš„å‘é‡åˆ†é‡ï¼Œå³ä¸ºè¯¥å¹³é¢çš„æ³•å‘é‡ \vec{n} = y - \hat{y} ä¹Ÿå¯ä½¿ç”¨ç²—æš´ä¸€ç‚¹çš„æ–¹æ³•ï¼Œç›´æ¥å°†ç¬¬ä¸‰ä¸»æˆåˆ†ä½œä¸ºæ³•å‘é‡ã€‚ æˆ–è€…ç›´æ¥ä¸ŠæŠ•å½±å…¬å¼ï¼š \hat{y} = Py P = X (X^TX+\lambda I)^{-1} X^T ![projection](/PCA/projection.jpg) &gt; æ€»ä½“çš„è¿ç®—æµç¨‹å¦‚ä¸‹ &gt; - åˆ©ç”¨æ‰€æœ‰æ ·æœ¬ç‚¹(è¿‘ä¼¼å¹³é¢)è®¡ç®—ä¸»æˆåˆ†ï¼Œç¬¬ä¸€ã€äºŒä¸»æˆåˆ†å¼ æˆå¹³é¢$\Pi$ï¼› &gt; - é€‰å‡ºå…¶ä¸­ä¸€ä¸ªæ ·æœ¬ç‚¹ï¼Œå°†å¹³è¡Œäºå¹³é¢$\Pi$çš„æˆåˆ†æŠ•å°„åˆ°$\Pi$ä¸Šï¼› &gt; - è¯¥æ ·æœ¬ç‚¹å‰©ä½™åˆ†é‡å³æ³•å‘é‡ï¼› &gt; - ä¸€èˆ¬æ¥è¯´ï¼Œå–æ‰€æœ‰ç‚¹æ³•å‘é‡çš„å‡å€¼ã€‚ ç¨‹åº@Github: PCA 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class PrincipalComponentAnalysis(): def __init__(self, n_component=-1): self.n_component = n_component self.meanVal = None self.axis = None def fit(self, X, prop=0.99): &apos;&apos;&apos; the parameter &apos;prop&apos; is only for &apos;n_component = -1&apos; &apos;&apos;&apos; # ç¬¬ä¸€æ­¥: å½’ä¸€åŒ– self.meanVal = np.mean(X, axis=0) # è®­ç»ƒæ ·æœ¬æ¯ä¸ªç‰¹å¾ä¸Šçš„çš„å‡å€¼ X_normalized = (X - self.meanVal) # å½’ä¸€åŒ–è®­ç»ƒæ ·æœ¬ # ç¬¬äºŒæ­¥ï¼šè®¡ç®—åæ–¹å·®çŸ©é˜µ # cov = X_normalized.T.dot(X_normalized) cov = np.cov(X_normalized.T) # åæ–¹å·®çŸ©é˜µ eigVal, eigVec = np.linalg.eig(cov) # EVD order = np.argsort(eigVal)[::-1] # ä»å¤§åˆ°å°æ’åº eigVal = eigVal[order] eigVec = eigVec.T[order].T # é€‰æ‹©ä¸»æˆåˆ†çš„æ•°é‡ if self.n_component == -1: sumOfEigVal = np.sum(eigVal) sum_tmp = 0 for k in range(eigVal.shape[0]): sum_tmp += eigVal[k] if sum_tmp &gt; prop * sumOfEigVal: # å¹³å‡å‡æ–¹è¯¯å·®ä¸è®­ç»ƒé›†æ–¹å·®çš„æ¯”ä¾‹å°½å¯èƒ½å°çš„æƒ…å†µä¸‹é€‰æ‹©å°½å¯èƒ½å°çš„ K å€¼ self.n_component = k + 1 break # é€‰æ‹©æŠ•å½±åæ ‡è½´ self.axis = eigVec[:, :self.n_component] # é€‰æ‹©å‰n_componentä¸ªç‰¹å¾å‘é‡ä½œä¸ºæŠ•å½±åæ ‡è½´ def transform(self, X): # ç¬¬ä¸€æ­¥ï¼šå½’ä¸€åŒ– X_normalized = (X - self.meanVal) # å½’ä¸€åŒ–æµ‹è¯•æ ·æœ¬ # ç¬¬äºŒæ­¥ï¼šæŠ•å½± X_nxk Â· V_kxk&apos; = X&apos;_nxk&apos; X_transformed = X_normalized.dot(self.axis) return X_transformed def fit_transform(self, X, prop=0.99): self.fit(X, prop=prop) return self.transform(X) def transform_inv(self, X_transformed): # è§†æŠ•å½±å‘é‡é•¿åº¦ä¸ºä¸€ä¸ªå•ä½é•¿åº¦ï¼ŒæŠ•å½±ç»“æœä¸ºæŠ•å½±å‘é‡ä¸Šçš„åæ ‡ # X&apos;_nxk&apos; Â· V_kxk&apos;.T = X&apos;&apos;_nxk X_restructed = X_transformed.dot(self.axis.T) # è¿˜åŸæ•°æ® X_restructed = X_restructed + self.meanVal return X_restructed å®éªŒç»“æœ Demo1: PCA applied on 2-d datasets Demo2: PCA applied on wild face origin reduced restructured]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Activate Functions]]></title>
    <url>%2F2018%2F10%2F20%2FActivate-Functions%2F</url>
    <content type="text"><![CDATA[SigAI ç†è§£ç¥ç»ç½‘ç»œçš„æ¿€æ´»å‡½æ•°æœºå™¨å­¦ä¹ ç¬”è®°ï¼šå½¢è±¡çš„è§£é‡Šç¥ç»ç½‘ç»œæ¿€æ´»å‡½æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ - ä¸è¯´è¯çš„æ±¤å§†çŒ« - åšå®¢å›­ æ¿€æ´»å‡½æ•°çš„ä½œç”¨å¤åˆå‡½æ•°ç¥ç»ç½‘ç»œå¯ä»¥çœ‹ä½œä¸€ä¸ªå¤šå±‚å¤åˆå‡½æ•°ï¼Œä»¥ä¸‹å›¾éšå«å±‚çš„æ¿€æ´»å‡½æ•°ä¸ºä¾‹ï¼Œè®²è§£å…¶éçº¿æ€§ä½œç”¨ã€‚ è®°æ¿€æ´»å‡½æ•°ä¸º$\sigma(Â·)$ï¼Œä¸Šå›¾ç¥ç»ç½‘ç»œå„å±‚é—´å…·æœ‰å¦‚ä¸‹å…³ç³» a = \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1)b = \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2)c = \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3)è¾“å‡ºå±‚é‡‡ç”¨çº¿æ€§å•å…ƒ A = w^{(2)}_{1}a + w^{(2)}_{2}b + w^{(2)}_{3}c + b^{(2)} ä¸ºä¾¿äºä½œå›¾ï¼Œå›ºå®šå‚æ•° W^{(1)} = \left[ \begin{matrix} 1 & 1 \\ 0.1 & -1 \\ 1 & -1 \end{matrix} \right], b^{(1)} = \left[ \begin{matrix} -2 \\ 1.5 \\ -1 \end{matrix} \right] W^{(2)} = \left[ \begin{matrix} 1 & 2 & 3 \end{matrix} \right], b^{(2)} = \left[ \begin{matrix} -1 \end{matrix} \right] çº¿æ€§å•å…ƒä½œä¸ºæ¿€æ´»å‡½æ•° æ­¤æ—¶ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸º A = (x + y - 2) + 2 (0.1x - y + 1.5) + 3 (x - y - 1)- 1 å¯è§ä»ä¸ºçº¿æ€§å‡½æ•°ï¼Œåšå‡ºå›¾åƒå¦‚ä¸‹æ‰€ç¤º éçº¿æ€§å•å…ƒä½œä¸ºæ¿€æ´»å‡½æ•° æ­¤æ—¶ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸º A = \sigma(x + y - 2) + 2 \sigma(0.1x - y + 1.5) + 3 \sigma(x - y - 1)- 1 æ¿€æ´»å‡½æ•°é€‰æ‹©Sigmoidï¼Œåšå‡ºå›¾åƒå¦‚ä¸‹æ‰€ç¤º åˆ†å‰²å¹³é¢ç¥ç»ç½‘ç»œå¯å®ç°é€»è¾‘è¿ç®—ï¼Œå„ä¸ªç¥ç»å…ƒè§†ä½œåˆ†å‰²è¶…å¹³é¢æ—¶ï¼Œå¯åˆ†å‰²å‡ºä¸åŒå½¢çŠ¶çš„å¹³é¢ï¼Œåœ¨çº¿æ€§å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°æ—¶åˆ†å‰²æ•ˆæœå¦‚å›¾ã€‚å½“ç¥ç»å…ƒç»„åˆçš„æƒ…å†µæ›´å¤æ‚æ—¶ï¼Œè¡¨è¾¾èƒ½åŠ›å°±ä¼šæ›´å¼ºã€‚ æ¿€æ´»å‡½æ•°çš„æ€§è´¨å·²ç»è¯æ˜ï¼Œåªè¦æ¿€æ´»å‡½æ•°é€‰æ‹©å¾—å½“ï¼Œç¥ç»å…ƒä¸ªæ•°è¶³å¤Ÿå¤šï¼Œä½¿ç”¨3å±‚å³åŒ…å«ä¸€ä¸ªéšå«å±‚çš„ç¥ç»ç½‘ç»œå°±å¯ä»¥å®ç°å¯¹ä»»ä½•ä¸€ä¸ªä»è¾“å…¥å‘é‡åˆ°è¾“å‡ºå‘é‡çš„è¿ç»­æ˜ å°„å‡½æ•°çš„é€¼è¿‘ï¼Œè¿™ä¸ªç»“è®ºç§°ä¸ºä¸‡èƒ½é€¼è¿‘ï¼ˆuniversal approximationï¼‰å®šç†ã€‚ å¦‚æœ$\varphi(x)$æ˜¯ä¸€ä¸ªéå¸¸æ•°ã€æœ‰ç•Œã€å•è°ƒé€’å¢çš„è¿ç»­å‡½æ•°ï¼Œ$I_{m}$æ˜¯$m$ç»´çš„å•ä½ç«‹æ–¹ä½“ï¼Œ$I_{m}$ä¸­çš„è¿ç»­å‡½æ•°ç©ºé—´ä¸º$C(I_{m})$ã€‚å¯¹äºä»»æ„$\varepsilon&gt;0$ä»¥åŠå‡½æ•°$f\in C(I_{m})$ï¼Œå­˜åœ¨æ•´æ•°$N$ï¼Œå®æ•°$v_{i},b_{i}$ï¼Œå®å‘é‡$w_{i}\in R^{m}$ï¼Œé€šè¿‡å®ƒä»¬æ„é€ å‡½æ•°$F(x)$ä½œä¸ºå‡½æ•°$f$çš„é€¼è¿‘ï¼š F(x) = \sum_{i=1}^N v_i \varphi(w_i^T x + b_i)å¯¹ä»»æ„çš„$X\in I_{m}$æ»¡è¶³ï¼š | F(x) - f(x) | < \varepsilonCybenko, G. Approximation by superpositions of a sigmoid function. Mathematics of Control, Signals, and Systems, 2, 303-314, 1989. è¿™ä¸ªå®šç†å¯¹æ¿€æ´»å‡½æ•°çš„è¦æ±‚æ˜¯å¿…é¡»éå¸¸æ•°ã€æœ‰ç•Œã€å•è°ƒé€’å¢ï¼Œå¹¶ä¸”è¿ç»­ã€‚ ç¥ç»ç½‘ç»œçš„è®­ç»ƒä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œæ±‚è§£ï¼Œéœ€è¦è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦å€¼ï¼Œæ¶‰åŠåˆ°è®¡ç®—æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ï¼Œå› æ­¤æ¿€æ´»å‡½æ•°å¿…é¡»æ˜¯å¯å¯¼çš„ã€‚å®é™…åº”ç”¨æ—¶å¹¶ä¸è¦æ±‚å®ƒåœ¨å®šä¹‰åŸŸå†…å¤„å¤„å¯å¯¼ï¼Œåªè¦æ˜¯å‡ ä¹å¤„å¤„å¯å¯¼å³å¯ã€‚ å®šä¹‰$R$ä¸ºä¸€ç»´æ¬§æ°ç©ºé—´ï¼Œ$E\subset R$æ˜¯å®ƒçš„ä¸€ä¸ªå­é›†ï¼Œ$mE$ä¸ºç‚¹é›†$E$çš„Lebesgueæµ‹åº¦ã€‚å¦‚æœ$E$ä¸º$R$ä¸­çš„å¯æµ‹é›†ï¼Œ$f(x)$ä¸ºå®šä¹‰åœ¨ä¸Š$E$çš„å®å‡½æ•°ï¼Œå¦‚æœå­˜åœ¨$N\subset E$ï¼Œæ»¡è¶³ï¼š$mN=0$ï¼Œå¯¹äºä»»æ„çš„$x_{0}\in E/N$å‡½æ•°$f(x)$åœ¨$x_{0}$å¤„éƒ½å¯å¯¼ï¼Œåˆ™ç§°$f(x)$åœ¨$E$ä¸Šå‡ ä¹å¤„å¤„å¯å¯¼ã€‚ å¦‚æœå°†æ¿€æ´»å‡½æ•°è¾“å…¥å€¼$x$çœ‹åšæ˜¯éšæœºå˜é‡ï¼Œåˆ™å®ƒè½åœ¨è¿™äº›ä¸å¯å¯¼ç‚¹å¤„çš„æ¦‚ç‡æ˜¯$0$ã€‚åœ¨è®¡ç®—æœºå®ç°æ—¶ï¼Œå› æ­¤æœ‰ä¸€å®šçš„æ¦‚ç‡ä¼šè½åœ¨ä¸å¯å¯¼ç‚¹å¤„ï¼Œä½†æ¦‚ç‡éå¸¸å°ã€‚ ä¾‹å¦‚ReLUå‡½æ•°åœ¨$x=0$å¤„ä¸å¯å¯¼ f(x) = \begin{cases} x & x \geq 0 \\ 0 & x < 0 \end{cases} å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Feedforward Neural Network]]></title>
    <url>%2F2018%2F10%2F20%2FFeedforward-Neural-Network%2F</url>
    <content type="text"><![CDATA[å‰è¨€å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æœ€ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œå„ç¥ç»å…ƒåˆ†å±‚æ’åˆ—ã€‚æ¯ä¸ªç¥ç»å…ƒåªä¸å‰ä¸€å±‚çš„ç¥ç»å…ƒç›¸è¿ã€‚æ¥æ”¶å‰ä¸€å±‚çš„è¾“å‡ºï¼Œå¹¶è¾“å‡ºç»™ä¸‹ä¸€å±‚ï¼å„å±‚é—´æ²¡æœ‰åé¦ˆã€‚æ˜¯ç›®å‰åº”ç”¨æœ€å¹¿æ³›ã€å‘å±•æœ€è¿…é€Ÿçš„äººå·¥ç¥ç»ç½‘ç»œä¹‹ä¸€ï¼Œæ—¢å¯ä»¥ç”¨äºè§£å†³åˆ†ç±»é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ç”¨äºè§£å†³å›å½’é—®é¢˜ã€‚ ç®€ä»‹å‰é¦ˆç¥ç»ç½‘ç»œä¹Ÿå«ä½œå¤šå±‚æ„ŸçŸ¥æœºï¼ŒåŒ…å«è¾“å…¥å±‚ï¼Œéšå«å±‚å’Œè¾“å‡ºå±‚ä¸‰ä¸ªéƒ¨åˆ†ã€‚å®ƒçš„ç›®çš„æ˜¯ä¸ºäº†å®ç°è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ã€‚ y = f(x;W)ç”±äºå„å±‚é‡‡ç”¨äº†éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œç¥ç»ç½‘ç»œå…·æœ‰è‰¯å¥½çš„éçº¿æ€§ç‰¹æ€§ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ æ¿€æ´»å‡½æ•°ä¸ºçº¿æ€§å•å…ƒ æ¿€æ´»å‡½æ•°ä¸ºéçº¿æ€§å•å…ƒ å‰é¦ˆç¥ç»ç½‘ç»œå¯ç”¨äºè§£å†³éçº¿æ€§çš„åˆ†ç±»æˆ–å›å½’é—®é¢˜ï¼Œå‚æ•°é€šè¿‡åå‘ä¼ æ’­ç®—æ³•(Back Propagation)å­¦ä¹ ã€‚ ç»“æ„ç¥ç»å…ƒä¸ç½‘ç»œç»“æ„å›¾å•ä¸ªç¥ç»å…ƒçš„ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œè¾“å…¥ä¸ºå‰ä¸€å±‚çš„è¾“å‡ºå‚æ•°$X^{(l-1)}$ h_{w, b}(x) = \sigma (WX + b)$\sigma(Â·)$è¡¨ç¤ºæ¿€æ´»å‡½æ•°ã€‚ ä»¥ä¸‹ä¸ºå…¸å‹çš„ç¥ç»ç½‘ç»œç»“æ„å›¾ ç¬¬ä¸€å±‚ä¸ºè¾“å…¥å±‚input layerï¼Œä¸€èˆ¬ä¸è®¾ç½®æƒå€¼ï¼Œé¢„å¤„ç†åœ¨è¾“å…¥ç½‘ç»œå‰å®Œæˆï¼› æœ€åä¸€å±‚ä¸ºè¾“å‡ºå±‚output layerï¼› å…¶ä½™å±‚ç§°ä¸ºéšè—å±‚hidden layerï¼Œéšè—å±‚ç”¨äºæå–æ•°æ®ç‰¹å¾ï¼Œéšè—å±‚å±‚æ•°ä¸å„å±‚ç¥ç»å…ƒä¸ªæ•°ä¸ºè¶…å‚æ•°ã€‚ ç¥ç»å…ƒæƒå€¼å–å€¼ä¸åŒï¼Œå¯å®ç°ä¸åŒçš„é€»è¾‘è¿ç®—ï¼Œå•ä¸ªè¶…å¹³é¢åªèƒ½è¿›è¡ŒäºŒå…ƒåˆ’åˆ†ï¼Œåˆ©ç”¨é€»è¾‘è¿ç®—å¯å°†å¤šä¸ªè¶…å¹³é¢åˆ’åˆ†çš„åŒºåŸŸæ‹¼æ¥èµ·æ¥ï¼Œå¦‚å›¾ ä»¥ä¸‹è¯´æ˜é€»è¾‘è¿ç®—çš„å®ç°æ–¹æ³•å…¶ä¸­ f(z) = \begin{cases} 1 & z \geq 0 \\ 0 & otherwise \end{cases} ä¸è¿ç®— $a âˆ§ b$ w_1 = 20, w_2 = 20, b = -30 æˆ–è¿ç®— $a âˆ§ b$ w_1 = 20, w_2 = 20, b = -10 éè¿ç®— $a = \overline{b}$ w_1 = -20, w_2 = 0, b = 0 å¼‚æˆ–è¿ç®— $a \bigoplus b$ï¼Œå¯é€šè¿‡ç»„åˆè¿ç®—å®ç° a \bigoplus b = (\overline{a} âˆ§ b) âˆ¨ (a âˆ§ \overline{b}) æ¿€æ´»å‡½æ•° éšè—å±‚çš„æ¿€æ´»å‡½æ•°ï¼Œè¯¦æƒ…å¯æŸ¥çœ‹å¦ä¸€ç¯‡åšæ–‡ï¼šç¥ç»ç½‘ç»œçš„æ¿€æ´»å‡½æ•°ï¼› è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•° å›å½’é—®é¢˜æ—¶ï¼Œé‡‡ç”¨çº¿æ€§å•å…ƒå³å¯ f(x) = x åˆ†ç±»é—®é¢˜æ—¶ï¼Œä¸€èˆ¬æœ‰ä»¥ä¸‹å‡ ç§é€‰æ‹© å•ç±»åˆ«æ¦‚ç‡è¾“å‡º å³æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå¯¹åº”è¯¥ç±»åˆ«çš„$0-1$åˆ†å¸ƒè¾“å‡ºï¼Œè¿™å°±éœ€è¦å°†è¾“å‡ºå€¼é™åˆ¶åœ¨$[0, 1]$å†…ï¼Œä¾‹å¦‚ P(y=1|x )= max\{0, min\{1, z\}\} ä½†æ˜¯å¯ä»¥çœ‹åˆ°ï¼Œå½“$(w^Tx+b)$å¤„äºå•ä½åŒºé—´å¤–æ—¶ï¼Œæ¨¡å‹çš„è¾“å‡ºå¯¹å®ƒçš„å‚æ•°çš„æ¢¯åº¦éƒ½å°†ä¸º$0$ ï¼Œä¸åˆ©äºç½‘ç»œçš„è®­ç»ƒï¼Œæ•…é‡‡ç”¨$S$å½¢å‡½æ•°Sigmoid(è¯¦æƒ…) P(y=1|x ) = \frac{1}{1+e^{-(w^Tx+b)}} $(1)$ Sigmoidå‡½æ•°å®šä¹‰åŸŸä¸º$(-\infty, \infty)$ï¼Œå€¼åŸŸä¸º$(0, 1)$ï¼Œä¸”åœ¨æ•´ä¸ªå®šä¹‰åŸŸä¸Šå•è°ƒé€’å¢ï¼Œå³ä¸ºå•å€¼å‡½æ•°ï¼Œæ•…å¯å°†çº¿æ€§è¾“å‡ºå•å…ƒçš„ç»“æœæ˜ å°„åˆ°$(0, 1)$èŒƒå›´å†…ï¼›$(2)$ åœ¨å®šä¹‰åŸŸä¸Šå¤„å¤„å¯å¯¼ã€‚ å¤šç±»åˆ«çš„æ¦‚ç‡è¾“å‡º å³æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå¯¹åº”åˆ¤åˆ«ä¸ºè¯¥ç±»åˆ«çš„æ¦‚ç‡ï¼Œä¸”æœ‰ \sum_{i=1}^C y_i = 1 ä¾‹å¦‚ y_i = \frac{z_i}{\sum_j z_j} ä½†æ˜¯åˆ†å¼æ±‚å¯¼å¼‚å¸¸éº»çƒ¦ï¼Œæ•…é‡‡ç”¨Softmaxå‡½æ•°(è¯¦æƒ…)ä½œä¸ºè¾“å‡ºç»“ç‚¹çš„æ¿€æ´»å‡½æ•°ï¼Œè¯¥å‡½æ•°æ±‚å¯¼ç»“æœæ¯”è¾ƒç®€æ´ï¼Œä¸”å¯åˆ©ç”¨è¾“å‡ºè®¡ç®—å¯¼æ•°ï¼Œè®¡ç®—é‡å‡å°‘ã€‚ Softmax(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right] æŸå¤±å‡½æ•° å›å½’é—®é¢˜ å¸¸è§çš„ç”¨äºå›å½’é—®é¢˜çš„æŸå¤±å‡½æ•°ä¸ºMSEï¼Œå³ L(y, \hat{y}) = \frac{1}{2M} \sum_{i=1}^M (\hat{y}^{(i)} - y^{(i)})^2 åˆ†ç±»é—®é¢˜ ä¸€èˆ¬é‡‡ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¦‚ä¸‹ L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M 1\{y^{(i)}_j=k\} \log (\hat{y}^{(i)}_j) 1\{y^{(i)}_j=k\} = \begin{cases} 1 & y^{(i)}_j = k \\ 0 & y^{(i)}_j \neq k \end{cases} j = 1, ..., N æˆ–è€… L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M y^{(i)T} \log (\hat{y}^{(i)}) å…¶ä¸­$y^{(i)}, \hat{y}^{(i)}$å‡è¡¨ç¤ºå‘é‡ï¼Œé‡‡ç”¨one-hotç¼–ç ã€‚ æ¢¯åº¦æ¨å¯¼ä»¥ä¸Šå†…å®¹ç½‘ä¸Šèµ„æ–™ä¸€å¤§å †ï¼Œè¿›å…¥é‡ç‚¹ï¼Œåå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦æ¨å¯¼ï¼Œç»™å‡ºç½‘ç»œç»“æ„å¦‚ä¸‹ã€‚ å›å½’ä¸åˆ†ç±»åœ¨è¾“å‡ºå±‚æœ‰æ‰€åŒºåˆ«ï¼› å„å±‚æ¿€æ´»å‡½æ•°çš„è¾“å…¥å˜é‡ä»¥$z^{(l)}$è¡¨ç¤ºï¼Œè¾“å‡ºå˜é‡å‡ä»¥$x^{(l)}$è¡¨ç¤ºï¼› $W^{(l)}$è¡¨ç¤ºä»ç¬¬$l$å±‚åˆ°ç¬¬$(l+1)$å±‚çš„æƒå€¼çŸ©é˜µï¼Œåˆ™$w^{(l)}_{ij}$è¡¨ç¤ºç¬¬$l$å±‚ç¬¬$j$ä¸ªç¥ç»å…ƒåˆ°$(l+1)$å±‚ç¬¬$i$ä¸ªç¥ç»å…ƒçš„è¿æ¥æƒå€¼ï¼› $b^{(l)}$è¡¨ç¤ºç¬¬$l$å±‚åˆ°ç¬¬$(l+1)$å±‚çš„åç½®ï¼Œåˆ™$b^{(l)}_i$è¡¨ç¤ºåˆ°ç¬¬$(l+1)$å±‚ç¬¬$i$ä¸ªç¥ç»å…ƒçš„åç½®å€¼ï¼› å„å±‚å˜é‡ç»´åº¦æ¨å¹¿ä¸ºè¾“å…¥$d_{i}$ï¼Œä¸­é—´å±‚$d_{h}$ï¼Œè¾“å‡ºå±‚$d_{o}$ï¼› å…¨è¿æ¥ï¼Œéƒ¨åˆ†çº¿æ¡å·²çœç•¥ï¼Œæ¿€æ´»å‡½æ•°å·²çœç•¥ï¼› åˆ™å„å±‚å‚æ•°çŸ©é˜µä¸º W^{(1)} = \left[ \begin{matrix} w^{(1)}_{11} & ... & w^{(1)}_{1d_i} \\ ... & ... & ... \\ w^{(1)}_{d_h1} & ... & w^{(1)}_{d_hd_i} \end{matrix} \right] b^{(1)} = \left[ \begin{matrix} b^{(1)}_{1} \\ ... \\ b^{(1)}_{d_h} \end{matrix} \right] W^{(2)} = \left[ \begin{matrix} w^{(2)}_{11} & ... & w^{(2)}_{1d_h} \\ ... & ... & ... \\ w^{(2)}_{d_o1} & ... & w^{(2)}_{d_od_h} \end{matrix} \right] b^{(2)} = \left[ \begin{matrix} b^{(2)}_{1} \\ ... \\ b^{(2)}_{d_o} \end{matrix} \right]æœ‰ Z^{(2)} = W^{(1)} X^{(1)} + b^{(1)} X^{(2)} = \sigma_1 (Z^{(2)}) Z^{(3)} = W^{(2)} X^{(2)} + b^{(2)} X^{(3)} = \sigma_2 (Z^{(3)}) X^{(1)} = X \hat{Y} = X^{(3)}å›å½’é—®é¢˜æŸå¤±å‡½æ•°é‡‡ç”¨MSEï¼Œå³ L(Y, \hat{Y}) = \frac{1}{M} \sum_{i=1}^M L(Y^{(i)}, \hat{Y}^{(i)}) L(Y^{(i)}, \hat{Y}^{(i)}) = \frac{1}{2} || \hat{Y}^{(i)} - Y^{(i)} ||_2^2 = \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}^{(i)}_{d_2} - y^{(i)}_{d_2})^2ä¸‹é¢æ¨å¯¼å•ä¸ªæ ·æœ¬çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œè¯¥æ‰¹æ•°æ®çš„æ¢¯åº¦ä¸ºå‡å€¼ã€‚ çœç•¥æ ·æœ¬æ ‡è®°$^{(i)}$ éšå«å±‚åˆ°è¾“å‡ºå±‚ å¯¹æƒå€¼çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = \frac{âˆ‚}{âˆ‚w^{(2)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{âˆ‚}{âˆ‚w^{(2)}_{ij}} \hat{y}_{d_2} \tag{1} å…¶ä¸­ \begin{cases} \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\ z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \end{cases} ä¸” \frac{âˆ‚}{âˆ‚w^{(2)}_{ij}} \hat{y}_{d_2} = \sigma_2' (z_{d_2}^{(3)}) \frac{âˆ‚z_{d_2}^{(3)}}{âˆ‚w^{(2)}_{ij}} \tag{2} \frac{âˆ‚}{âˆ‚w^{(2)}_{ij}} z_{d_2}^{(3)} = \begin{cases} x^{(2)}_{d_1} & d_1 = j, d_2 = i \\ 0 & otherwise \end{cases} \tag{3} $(3)$ä»£å…¥$(2)$ï¼Œå†ä»£å…¥$(1)$å¯å¾—åˆ° \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) x^{(2)}_{d_1} | _{d_1=j, d_2=i} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \tag{*1} å¯¹åç½®çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚b^{(2)}_i} = \frac{âˆ‚}{âˆ‚b^{(2)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{âˆ‚}{âˆ‚b^{(2)}_i} \hat{y}_{d_2} \tag{4} å…¶ä¸­ \begin{cases} \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\ z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \end{cases} æœ‰ \frac{âˆ‚}{âˆ‚b^{(2)}_i} z_{d_2}^{(3)} = \begin{cases} 1 & d_2 = i \\ 0 & otherwise \end{cases} \tag{5} æ‰€ä»¥ \frac{âˆ‚L}{âˆ‚b^{(2)}_i} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) | _{d_2=i} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \tag{*2} è¾“å…¥å±‚åˆ°éšå«å±‚ å¯¹æƒå€¼çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚w^{(1)}_{ij}} = \frac{âˆ‚}{âˆ‚w^{(1)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{âˆ‚}{âˆ‚w^{(1)}_{ij}} \hat{y}_{d_2} \tag{6} å…¶ä¸­ \begin{cases} \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\ z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \\ x^{(2)}_{d_1} = \sigma_1 (z_{d_1}^{(2)}) \\ z_{d_1}^{(2)} = \sum_{d_0=1}^{d_i} w^{(1)}_{d_1 d_0} x^{(1)}_{d_0} + b^{(1)}_{d_1} \end{cases} æ•… \frac{âˆ‚}{âˆ‚w^{(1)}_{ij}} \hat{y}_{d_2} = \frac{âˆ‚\hat{y}_{d_2}}{âˆ‚z_{d_2}^{(3)}} \frac{âˆ‚z_{d_2}^{(3)}}{âˆ‚w^{(1)}_{ij}} \tag{7} å…¶ä¸­ \frac{âˆ‚\hat{y}_{d_2}}{âˆ‚z_{d_2}^{(3)}} = \sigma_2' (z_{d_2}^{(3)}) \tag{8} \frac{âˆ‚z_{d_2}^{(3)}}{âˆ‚w^{(1)}_{ij}} = \sum_{d1=1}^{d_h} w^{(2)}_{d_2 d_1} \frac{âˆ‚x^{(2)}_{d_1}}{âˆ‚w^{(1)}_{ij}} \tag{9} \frac{âˆ‚x^{(2)}_{d_1}}{âˆ‚w^{(1)}_{ij}} = \frac{âˆ‚x^{(2)}_{d_1}}{âˆ‚z_{d_1}^{(2)}} \frac{âˆ‚z_{d_1}^{(2)}}{âˆ‚w^{(1)}_{ij}} \tag{10} è€Œå…¶ä¸­ \frac{âˆ‚x^{(2)}_{d_1}}{âˆ‚z_{d_1}^{(2)}} = x^{(2)}_{d_1} = \sigma_1' (z_{d_1}^{(2)}) \tag{11} \frac{âˆ‚z_{d_1}^{(2)}}{âˆ‚w^{(1)}_{ij}} = \begin{cases} x^{(1)}_{d_0} & d_1 = i, d_0 = j\\ 0 & otherwise \end{cases} \tag{12} $(11),(12)$ä»£å…¥$(10)$å¾—åˆ° \frac{âˆ‚x^{(2)}_{d_1}}{âˆ‚w^{(1)}_{ij}} = \sigma_1' (z_{d_1}^{(2)}) x^{(1)}_{d_0} | _{d_1 = i, d_0 = j} \tag{13} $(13)$ä»£å›$(9)$ï¼Œæœ‰ \frac{âˆ‚z_{d_2}^{(3)}}{âˆ‚w^{(1)}_{ij}} = \sum_{d1=1}^{d_h} \left[ w^{(2)}_{d_2 d_1} \sigma_1' (z_{d_1}^{(2)}) x^{(1)}_{d_0} \right] | _{d_1 = i, d_0 = j} = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{14} å°†$(8),(14)$ä»£å…¥$(7)$å¾—åˆ° \frac{âˆ‚}{âˆ‚w^{(1)}_{ij}} \hat{y}_{d_2} = \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{15} $(15)$ä»£å…¥$(6)$æœ‰ \frac{âˆ‚L}{âˆ‚w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*3} å¯¹åç½®çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚b^{(1)}_i} = \frac{âˆ‚}{âˆ‚b^{(1)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{âˆ‚}{âˆ‚b^{(1)}_i} \hat{y}_{d_2} \tag{16} åŒç†å¯å¾— \frac{âˆ‚}{âˆ‚b^{(1)}_i} \hat{y}_{d_2} = \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) \tag{17} æ‰€ä»¥ \frac{âˆ‚L}{âˆ‚b^{(1)}_i} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) \tag{*4} ç»¼ä¸Šæ‰€è¿° \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \frac{âˆ‚L}{âˆ‚b^{(2)}_i} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \frac{âˆ‚L}{âˆ‚w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \frac{âˆ‚L}{âˆ‚b^{(1)}_i} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)})ä»¤ \begin{cases} \delta^{(2)}_i = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \\ \delta^{(1)}_i = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) \end{cases}æœ‰ \begin{cases} \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = \delta^{(2)}_i x^{(2)}_{j}\\ \frac{âˆ‚L}{âˆ‚b^{(2)}_i} = \delta^{(2)}_i\\ \frac{âˆ‚L}{âˆ‚w^{(1)}_{ij}} = \delta^{(1)}_i x^{(1)}_j\\ \frac{âˆ‚L}{âˆ‚b^{(1)}_i} = \delta^{(1)}_i \end{cases}è‡³æ­¤æ¨å¯¼å®Œæ¯•ã€‚ å½“éšè—å±‚é‡‡ç”¨Sigmoidå‡½æ•°ï¼Œè¾“å‡ºå±‚é‡‡ç”¨çº¿æ€§å•å…ƒï¼Œå¯å¾—åˆ° \sigma_1' (z_i^{(2)}) = \sigma_1 (z_i^{(2)}) \left[1 - \sigma_1 (z_i^{(2)}) \right] = x_i^{(2)} (1 - x_i^{(2)}) \sigma_2' (z_i^{(3)}) = z_i^{(3)}æ­¤æ—¶ \begin{cases} \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) z_i^{(3)} x^{(2)}_{j} \\ \frac{âˆ‚L}{âˆ‚b^{(2)}_i} = (\hat{y}_{i} - y_{i}) z_i^{(3)} \\ \frac{âˆ‚L}{âˆ‚w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} x^{(1)}_j \\ \frac{âˆ‚L}{âˆ‚b^{(1)}_i} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} \end{cases}å¯ä»¥çœ‹åˆ°ï¼Œè®¡ç®—æ¢¯åº¦æ—¶ä½¿ç”¨çš„æ•°æ®åœ¨ä¸Šä¸€æ¬¡å‰å‘ä¼ æ’­æ—¶å·²è®¡ç®—å¾—ï¼Œæ•…å¯å‡å°‘è®¡ç®—é‡ã€‚ åˆ†ç±»é—®é¢˜æŸå¤±å‡½æ•°é‡‡ç”¨Cross Entropyï¼Œå³ L(\hat{y}, y) = \frac{1}{M} \sum_{i=1}^M L(\hat{y}^{(i)}, y^{(i)}) L(\hat{y}^{(i)}, y^{(i)}) = - y^{(i)T} \log (\hat{y}^{(i)})ä¸Šå¼ä¸­ï¼Œ$y^{(i)}, \hat{y}^{(i)}$å‡ä¸ºåˆ—å‘é‡ï¼Œä¸”$y^{(i)}$è¡¨ç¤ºone-hotç¼–ç åçš„æ ‡ç­¾å‘é‡ï¼Œä¹Ÿå¯å†™ä½œ L(\hat{y}^{(i)}, y^{(i)}) = - \log \hat{y}^{(i)}_{y^{(i)}} ç”±è¯¥å¼å¯ä»¥çœ‹å‡ºï¼Œè‹¥è¾“å‡ºå±‚æ¿€æ´»å‡½æ•°é‡‡ç”¨Sigmoidä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œåˆ™éšè—å±‚â€”â€”è¾“å‡ºå±‚ä¹‹é—´æƒå€¼çŸ©é˜µ$W^{(2)}$åªä¼šæ›´æ–°$w^{(i)}_{y^{(i)} d_1}, d_1 = 1, â€¦, d_h$ï¼› ä¸€èˆ¬é‡‡ç”¨SoftMaxä½œä¸ºè¾“å‡ºå±‚æ¿€æ´»å‡½æ•°ï¼ŒSigmoidä¸‹é¢ä¸ä½œæ¨å¯¼ã€‚ å…³äºSoftMaxçš„æ¢¯åº¦ï¼Œç§»æ­¥SoftMax Regressionä¸­æŸ¥çœ‹è¯¦ç»†æ¨å¯¼è¿‡ç¨‹ï¼Œè¿™é‡Œç›´æ¥ç»™å‡ºç»“è®ºã€‚å¯¹äº S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]å…¶æ¢¯åº¦ä¸º \frac{âˆ‚S(x)}{âˆ‚x_i}_{KÃ—1} = \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] - \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right] = \left( \left[ \begin{matrix} 0 \\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_içœç•¥æ ·æœ¬æ ‡è®°$^{(i)}$ éšå«å±‚åˆ°è¾“å‡ºå±‚ å¯¹æƒå€¼çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = - \frac{âˆ‚}{âˆ‚w^{(2)}_{ij}} \log \hat{y}_{y} = - \frac{1}{\hat{y}_y} \frac{âˆ‚\hat{y}_{y}}{âˆ‚w^{(2)}_{ij}} \tag{18} å…¶ä¸­$\hat{y}_{y}$ä¸$z^{(3)}_{d_2}(d_2 = 1, â€¦, d_o) $å‡æœ‰è”ç³»ï¼Œæ•… \frac{âˆ‚\hat{y}_{y}}{âˆ‚w^{(2)}_{ij}} = \sum_{d2=1}^{d_o} \frac{âˆ‚\hat{y}_{y}}{âˆ‚z^{(3)}_{d_2}} \frac{âˆ‚z^{(3)}_{d_2}}{âˆ‚w^{(2)}_{ij}} \tag{19} è€Œ \frac{âˆ‚\hat{y}_{y}}{âˆ‚z^{(3)}_{d_2}} = \begin{cases} \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\ - \hat{y}_{y} \hat{y}_{d_2} & otherwise \end{cases} \frac{âˆ‚z^{(3)}_{d_2}}{âˆ‚w^{(2)}_{ij}} = \begin{cases} x^{(2)}_{d_1} & i = d_2, j = d_1 \\ 0 & otherwise \end{cases} $z^{(3)}_{d_2} = \sum_{d_1=0}^{d_h} w^{(2)}_{d_2d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}$ ä»£å›$(19)$ï¼Œå†å¸¦å›$(18)$ï¼Œæœ‰ \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = - \frac{1}{\hat{y}_{y}} \sum_{d_2=1}^{d_o} \frac{âˆ‚\hat{y}_{y}}{âˆ‚z^{(3)}_{d_2}} x^{(2)}_{d_1} | _{d_2=i, d_1=j} = \begin{cases} - \frac{1}{\hat{y}_{y}} \hat{y}_{y} (1 - \hat{y}_i) x^{(2)}_j & i = y \\ - \frac{1}{\hat{y}_{y}} (- \hat{y}_{y} \hat{y}_i) x^{(2)}_j & otherwise \end{cases} = \begin{cases} (\hat{y}_i - 1) x^{(2)}_j & i = y \\ \hat{y}_i x^{(2)}_j & otherwise \end{cases} å³ \frac{âˆ‚L}{âˆ‚w^{(2)}_{ij}} = (\hat{y}_i - y_i) x^{(2)}_j \tag{*5} å¯¹åç½®çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚b^{(2)}_i} = \hat{y}_i - y_i \tag{*6} è¾“å…¥å±‚åˆ°éšå«å±‚ å¯¹æƒå€¼çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚w^{(1)}_{ij}} = - \frac{âˆ‚}{âˆ‚w^{(1)}_{ij}} \log \hat{y}_{y} = - \frac{1}{\hat{y}_{y}} \frac{âˆ‚\hat{y}_{y}}{âˆ‚w^{(1)}_{ij}} \tag{20} å…¶ä¸­ \frac{âˆ‚\hat{y}_{y}}{âˆ‚w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \frac{âˆ‚\hat{y}_{y}}{âˆ‚z^{(3)}_{d_2}} \frac{âˆ‚z^{(3)}_{d_2}}{âˆ‚w^{(1)}_{ij}} \tag{21} $\frac{âˆ‚z^{(3)}_{d_2}}{âˆ‚w^{(1)}_{ij}}$éƒ¨åˆ†ä¸å›å½’ç›¸åŒï¼Œæœ‰ \frac{âˆ‚z_{d_2}^{(3)}}{âˆ‚w^{(1)}_{ij}} = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j ç”±ä¸Šé¢åˆ†æå¯å¾— \frac{âˆ‚\hat{y}_{y}}{âˆ‚z^{(3)}_{d_2}} = \begin{cases} \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\ - \hat{y}_{y} \hat{y}_{d_2} & otherwise \end{cases} æ•…ä»£å›$(20)$å¯å¾—åˆ° \frac{âˆ‚L}{âˆ‚w^{(1)}_{ij}} = - \frac{1}{\hat{y}_{y}} \sum_{d_2=1}^{d_o} \frac{âˆ‚\hat{y}_{y}}{âˆ‚z^{(3)}_{d_2}} \frac{âˆ‚z^{(3)}_{d_2}}{âˆ‚w^{(1)}_{ij}} = - \frac{1}{\hat{y}_{y}} \sum_{d_2=1}^{d_o} \frac{âˆ‚\hat{y}_{y}}{âˆ‚z^{(3)}_{d_2}} w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j = \left[ \sum_{d_2=1, d_2 \neq y}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} + (\hat{y}_y - 1) w^{(2)}_{y i} \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j = \left[ \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} - w^{(2)}_{y i} \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*7} å¯¹åç½®çŸ©é˜µçš„æ¢¯åº¦ \frac{âˆ‚L}{âˆ‚b^{(1)}_i} = \left[ \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} - w^{(2)}_{y i} \right] \sigma_1' (z_i^{(2)}) \tag{*8} è‡³æ­¤æ¨å¯¼å®Œæ¯•ã€‚ è¿™ä¸ªæ¨å¯¼ï¼Œä»…ä¾›å‚è€ƒ è¿‡æ‹Ÿåˆé—®é¢˜å’Œå…¶ä»–ç®—æ³•ä¸€æ ·ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œä¹Ÿå­˜åœ¨è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œè§£å†³æ–¹æ³•æœ‰ä»¥ä¸‹å‡ ç§ æ­£åˆ™åŒ– ä¸çº¿æ€§å›å½’ç±»ä¼¼ï¼Œç¥ç»ç½‘ç»œä¹Ÿå¯ä»¥åŠ å…¥èŒƒæ•°æƒ©ç½šé¡¹ï¼Œä»¥ä¸‹$C$è¡¨ç¤ºæ™®é€šçš„æŸå¤±å‡½æ•°ï¼Œ$\lambda$ä¸ºæƒ©ç½šç³»æ•°ï¼Œ$n$ä¸ºæ ·æœ¬æ•°ç›®ï¼Œ$w$è¡¨ç¤ºæƒå€¼å‚æ•°ã€‚ L1æ­£åˆ™åŒ– æƒ©ç½šé¡¹ä¸ºç½‘ç»œæ‰€æœ‰æƒå€¼çš„ç»å¯¹å€¼ä¹‹å’Œã€‚ C = C_0 + \frac{\lambda}{n} \sum_w |w| L2æ­£åˆ™åŒ– åˆç§°æƒå€¼è¡°å‡weights decayï¼Œæƒ©ç½šé¡¹ä¸ºç½‘ç»œæ‰€æœ‰æƒå€¼çš„å¹³æ–¹å’Œã€‚ C = C_0 + \frac{\lambda}{2n} \sum_w w^2 Dropout ä»¥æ¦‚ç‡å¤§å°ä¸ºpä½¿éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºå€¼ç›´æ¥ä¸º0ï¼Œå¦‚æ­¤å¯ä»¥ä½¿åå‘ä¼ æ’­æ—¶ç›¸å…³æƒå€¼ç³»æ•°ä¸åšæ›´æ–°ï¼Œåªæœ‰è¢«ä¿ç•™ä¸‹æ¥çš„æƒå€¼å’Œåç½®å€¼ä¼šè¢«æ›´æ–°ã€‚ å¢åŠ è®­ç»ƒæ•°æ®å¤§å° å¯åœ¨åŸæ•°æ®ä¸ŠåŠ ä»¥å˜æ¢æˆ–å™ªå£°ï¼Œå›¾åƒçš„æ‰©å¢æ–¹æ³•å¯æŸ¥çœ‹å›¾åƒæ•°æ®é›†æ‰©å¢ã€‚ ç¨‹åº@Github: Code of Neural Network ä½¿ç”¨PyTorchå®ç°ç¥ç»ç½‘ç»œï¼Œä»¥ä¸‹ä¸ºæ¨¡å‹å®šä¹‰123456789101112131415class AnnNet(nn.Module): def __init__(self): super(AnnNet, self).__init__() self.input_size = 28 * 28 self.hidden_size = 100 self.output_size = 10 self.fc1 = nn.Linear(self.input_size, self.hidden_size) # input - hidden self.fc2 = nn.Linear(self.hidden_size, self.output_size ) # hidden - output # self.activate = nn.Sigmoid() # å‚æ•°æ›´æ–°éå¸¸æ…¢ï¼Œç‰¹åˆ«æ˜¯å±‚æ•°å¤šæ—¶ self.activate = nn.ReLU() # äº‹å®è¯æ˜ReLUä½œä¸ºæ¿€æ´»å‡½æ•°æ›´åŠ åˆé€‚ self.softmax = nn.Softmax() def forward(self, X): h = self.activate(self.fc1(X)) y_pred = self.softmax(self.fc2(h)) return y_pred]]></content>
  </entry>
  <entry>
    <title><![CDATA[åˆ†ç±»é—®é¢˜çš„å†³ç­–å¹³é¢]]></title>
    <url>%2F2018%2F10%2F19%2F%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[å¼•è¨€å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œè®¡ç®—ç»“æœä¸€èˆ¬ä¸ºæ¦‚ç‡å€¼ï¼Œé‚£ä¹ˆå¦‚ä½•æ ¹æ®è®¡ç®—å¾—çš„æ¦‚ç‡è¿›è¡Œåˆ¤åˆ«åˆ†ç±»å‘¢ï¼Ÿ è¿™éƒ¨åˆ†ç†è§£åï¼ŒLogisticå›å½’ä¸Softmaxå›å½’çš„æ¨¡å‹å°±å¾ˆå®¹æ˜“æ¨å¾—ã€‚ åˆ¤åˆ«å‡½æ•°å¯¹äºä¸€ä¸ªç±»åˆ«ä¸º$K$çš„åˆ†ç±»é—®é¢˜ï¼Œå¦‚æœå¯¹äºæ‰€æœ‰çš„$ i,j=1,â€¦,K, j\neq i$ï¼Œæœ‰ g_i(x) > g_j(x)åˆ™æ­¤åˆ†ç±»å™¨å°†è¿™ä¸ªæ ·æœ¬å¯¹åº”çš„ç‰¹å¾å‘é‡$x$åˆ¤åˆ«ä¸º$w_i$ï¼Œåˆ™æ­¤åˆ†ç±»å™¨çš„ä½œç”¨æ˜¯ï¼Œè®¡ç®—$K$ä¸ªåˆ¤åˆ«å‡½æ•°å¹¶é€‰å–ä¸æœ€å¤§åˆ¤åˆ«å€¼æœ€å¤§å¯¹åº”çš„ç±»åˆ«ã€‚ åˆ¤åˆ«å‡½æ•°çš„å½¢å¼å¹¶ä¸å”¯ä¸€ï¼Œå¯ä»¥å°†æ‰€æœ‰çš„åˆ¤åˆ«å‡½æ•°ä¹˜ä¸Šç›¸åŒçš„æ­£å¸¸æ•°æˆ–è€…åŠ ä¸Šä¸€ä¸ªç›¸åŒçš„å¸¸é‡è€Œä¸å½±å“å…¶åˆ¤å†³ç»“æœã€‚æ›´ä¸€èˆ¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å•è°ƒé€’å¢å‡½æ•°$f(Â·)$è¿›è¡Œæ˜ å°„ï¼Œå°†æ¯ä¸€ä¸ª$g_i(x)$æ›¿æ¢æˆ$f(g_i(x))$ï¼Œåˆ†ç±»ç»“æœä¸å˜ã€‚ â€”â€”ã€Šæ¨¡å¼è¯†åˆ«åŸç†ä¸åº”ç”¨è¯¾ç¨‹ç¬”è®°ã€‹ ä¾‹å¦‚æœ€å°é£é™©è´å¶æ–¯å†³ç­– æ­£æ€åˆ†å¸ƒä¸‹çš„åˆ¤åˆ«å‡½æ•° å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼ˆThe Multivariate normal distributionï¼‰ - bingjianing - åšå®¢å›­ ç”±å¤§æ•°å®šç†å¯çŸ¥ï¼Œåœ¨æ ·æœ¬è¶³å¤Ÿçš„æƒ…å†µä¸‹ï¼Œæ•°æ®æœä»æ­£æ€åˆ†å¸ƒã€‚å¤šå…ƒæ­£æ€åˆ†å¸ƒå½¢å¼å¦‚ä¸‹ f(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))å…¶ä¸­ x = [x_1, ..., x_n]^T\mu = [\mu_1, ..., \mu_n]^T\Sigma_{ij} = cov(x_i, x_j) åœ¨æœ€å°é”™è¯¯ç‡åˆ¤åˆ«æ—¶ g_i(x) = P(x|c_i)P(c_i)å³ g_i(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i)) Â· P(c_i)å–å¯¹æ•°è¿ç®—ï¼Œå¹¶èˆå»å¸¸æ•°é¡¹ï¼Œå±•å¼€æ•´ç†å¾— g_i(x) = -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i) \tag{0} æ³¨ï¼š åæ–¹å·®çŸ©é˜µ $ \Sigma^T = \Sigma $ 1. $\Sigma_i = \sigma^2 I$$\Sigma_i^{-1} = \frac{1}{\sigma^2} I$ä»£å…¥$(0)$ï¼Œæœ‰ g_i(x) = \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)\tag{1}å®šä¹‰ w_i = \frac{1}{\sigma^2}\mu_i^Tw_0 = - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)æœ‰ä¸€èˆ¬å½¢å¼å¦‚ä¸‹ï¼Œè¡¨ç¤ºå–$c_i$çš„æ¦‚ç‡ g_i(x) = w_i x + w_0\tag{2}è®¾å†³ç­–å¹³é¢ä¸º w^T (xâˆ’x_0)=0\tag{3}å†³ç­–å¹³é¢ä¸Šï¼Œå–$c_i$å’Œ$c_j$çš„æ¦‚ç‡ç›¸ç­‰ï¼Œå³ g_i(x) = g_j(x)å¯å¾— (\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} \tag{4} æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼Œå°†$(1)$ä»£å…¥ä¸Šå¼$ \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i) = \frac{1}{\sigma^2}\mu_j^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_j ^T \mu_j) + ln P(c_j) $$ \mu_i^T x - \frac{1}{2} \mu_i ^T \mu_i + ln P(c_i) = \mu_j^T x - \frac{1}{2} \mu_j ^T \mu_j + ln P(c_j) $$ (\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} $ ç”±$(3)$$(4)$ï¼Œåˆ©ç”¨å¾…å®šç³»æ•°æ³•ï¼Œå¯å¾— w = \mu_i - \mu_j w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)}ç‰¹åˆ«åœ°ï¼Œå½“ç­‰å…ˆéªŒæ¦‚ç‡æ—¶ï¼Œå³$P(c_i) = P(c_j)$æ—¶ w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j)æ•… x_0 = \frac{1}{2}(\mu_i + \mu_j)ç»“è®ºï¼šç­‰å…ˆéªŒæ¦‚ç‡æ—¶è¶…å¹³é¢$ w^T (xâˆ’x_0)=0 $å¹³åˆ†åˆ¤åˆ«ç©ºé—´ $\mu_i$ä¸$\mu_j$åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªç±»åˆ«çš„ä¸­å¿ƒï¼Œç”±å‘é‡è¿ç®—ï¼Œ$x_0$ä¸ºä¸¤ç±»ä¸­å¿ƒçš„è¿çº¿çš„ä¸­ç‚¹ã€‚ 2. $\Sigma_i = \Sigma$ä»£å…¥$(0)$åå¯å¾— g_i(x) = \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) \tag{5}å®šä¹‰ w_i = \mu_i^T \Sigma ^{-1}w_0 = - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i)æœ‰ä¸€èˆ¬å½¢å¼å¦‚ä¸‹ï¼Œè¡¨ç¤ºå–$c_i$çš„æ¦‚ç‡ g_i(x) = w_i x + w_0\tag{6}åŒæ ·çš„ï¼Œè®¾å†³ç­–å¹³é¢ä¸º w^T (xâˆ’x_0)=0\tag{7}å†³ç­–å¹³é¢ä¸Šï¼Œå–$c_i$å’Œ$c_j$çš„æ¦‚ç‡ç›¸ç­‰ï¼Œå³ g_i(x) = g_j(x)æœ‰ (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i - \mu_j)^T \Sigma ^{-1} (\mu_i - \mu_j) - ln \frac{P(c_i)}{P(c_j)} $ \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $$ \mu_i^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $$ (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j) - ln \frac{P(c_i)}{P(c_j)} $ ç‰¹åˆ«çš„ï¼Œå½“å–ç­‰å…ˆéªŒæ¦‚ç‡æ—¶ (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)ç”±$(7)$$(8)$ï¼Œåˆ©ç”¨å¾…å®šç³»æ•°æ³• w^T = (\mu_i - \mu_j)^T \Sigma^{-1}w^T x_0 = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j) æ³¨ï¼š åæ–¹å·®çŸ©é˜µ $ \Sigma^T = \Sigma $ w = \Sigma^{-1}(\mu_i - \mu_j)x_0 = \frac{1}{2} (\mu_i + \mu_j)ç”±äºé€šå¸¸$w=Î£^{âˆ’1}(Î¼_iâˆ’Î¼_j)$å¹¶éæœç€$(Î¼_iâˆ’Î¼_j)$çš„æ–¹å‘ï¼Œå› è€Œé€šå¸¸åˆ†ç¦»ä¸¤ç±»çš„è¶…å¹³é¢ä¹Ÿå¹¶éä¸å‡å€¼çš„è¿çº¿å‚ç›´æ­£äº¤ã€‚ä½†æ˜¯ï¼Œ å¦‚æœå…ˆéªŒæ¦‚ç‡ç›¸ç­‰ï¼Œå…¶åˆ¤å®šé¢ç¡®å®æ˜¯ä¸å‡å€¼è¿çº¿äº¤äºä¸­ç‚¹$x_0$å¤„çš„ã€‚å¦‚æœå…ˆéªŒæ¦‚ç‡ä¸ç­‰ï¼Œæœ€ä¼˜è¾¹ç•Œè¶…å¹³é¢å°†è¿œç¦»å¯èƒ½æ€§è¾ƒå¤§çš„å‡å€¼ã€‚åŒå‰ï¼Œå¦‚æœåç§»é‡è¶³å¤Ÿå¤§ï¼Œåˆ¤å®šé¢å¯ä»¥ä¸è½åœ¨ä¸¤ä¸ªå‡å€¼å‘é‡ä¹‹é—´ã€‚ 3. $\Sigma_i = \Sigma_i(âˆ€) $g_i(x) = -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)å®šä¹‰ W_i = -\frac{1}{2} \Sigma_i ^{-1}w_i = \mu_i^T \Sigma_i ^{-1}w_0 = -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)æœ‰ g_i(x) = x^TW_ix + w_ix + w_0]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Bayes Decision]]></title>
    <url>%2F2018%2F10%2F18%2FBayes-Decision%2F</url>
    <content type="text"><![CDATA[åŸç†åŸºäºè´å¶æ–¯å…¬å¼ P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}P(x)=\sum_j p(x|c_j)P(c_j)å‡ ç§å¸¸ç”¨çš„è´å¶æ–¯å†³ç­–æœ€å°é”™è¯¯ç‡è´å¶æ–¯å†³ç­–åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€å¸Œæœ›å°½å¯èƒ½å‡å°‘åˆ†ç±»é”™è¯¯ï¼Œå³ç›®æ ‡æ˜¯è¿½æ±‚æœ€å°é”™è¯¯ç‡ã€‚å‡è®¾æœ‰$K$åˆ†ç±»é—®é¢˜ï¼Œç”±è´å¶æ–¯å…¬å¼ P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}ä¸Šå¼ä¸­$ k=1,â€¦,K $ï¼Œå„éƒ¨åˆ†å®šä¹‰å¦‚ä¸‹ $P(c_k|x)$â€”â€”åéªŒæ¦‚ç‡(posteriori probability)$P(c_k)$â€”â€”å…ˆéªŒæ¦‚ç‡(priori probability)ï¼Œ$p(x|c_k)$â€”â€”$c_k$å…³äº$x$çš„ä¼¼ç„¶å‡½æ•°(likelihood)ï¼Œ$p(x)$â€”â€”è¯æ®å› å­(evidence)ï¼Œ è¯æ®å› å­ç”±ä¸‹å¼è®¡ç®— p(x)=\sum_{j=0}^K p(x|c_j)P(c_j)ä»¥ä¸Šå°±æ˜¯ä»æ ·æœ¬ä¸­è®­ç»ƒçš„å‚æ•°ï¼Œåœ¨é¢„æµ‹é˜¶æ®µï¼Œå®šä¹‰å†³ç­–è§„åˆ™ä¸º $if$ $P(c_i|x)&gt;P(c_j|x)$, $then$ $ x \in c_i $ ç”±äºåˆ†æ¯ä¸ºæ ‡é‡ï¼Œå¯¹äºä»»æ„è¾“å…¥çš„æ ·æœ¬ç‰¹å¾$x$ï¼Œ$P(x)$ä¸€å®šï¼Œæ•…å†³ç­–è§„åˆ™å¯ç®€åŒ–ä¸º $if$ $P(x|c_i)P(c_i)&gt;P(x|c_j)P(c_j)$, $then$ $ x \in c_i $ è€Œå¯¹äºåˆ†ç±»é”™è¯¯çš„æ ·æœ¬ï¼Œå¦‚æ ·æœ¬$x$å±äºåˆ†ç±»$c_i$ï¼Œä½†é”™è¯¯åˆ†ç±»ä¸º$c_{err}, err \neq i$ï¼Œæ ·æœ¬çš„é”™è¯¯åˆ†ç±»æ¦‚ç‡ä¸º P(error|x) = P(c_{err}|x)ä¸Šå¼è¢«ç§°ä½œè¯¯å·®æ¦‚ç‡ï¼ŒæŸç±»åéªŒæ¦‚ç‡è¶Šå¤§ï¼Œåˆ™ç›¸åº”çš„è¯¯å·®æ¦‚ç‡å°±è¶Šå°ï¼Œå®šä¹‰å¹³å‡è¯¯å·®æ¦‚ç‡ P_{mean} = \int P(error|x)P(x)dxå¸¦æœ‰æ‹’ç»åŸŸçš„æœ€å°é”™è¯¯ç‡è´å¶æ–¯å†³ç­–ä¸€äº›æƒ…å†µä¸‹ï¼ŒæŸæ ·æœ¬å¯¹åº”ç‰¹å¾$x$è®¡ç®—ç»“æœä¸­ï¼Œå±äºå„ç±»åˆ«çš„æ¦‚ç‡æ²¡æœ‰æ˜¾è‘—æ¯”è¾ƒå¤§çš„æ•°å€¼ï¼Œæ¢å¥è¯è¯´éƒ½æ¯”è¾ƒå°ï¼Œé‚£ä¹ˆå¯¹è¿™æ¬¡çš„åˆ¤åˆ«å°±ä¸å¤ªä¿¡ä»»ï¼Œé€‰æ‹©æ‹’ç»å†³ç­–ç»“æœã€‚å°†å†³ç­–å¹³é¢åˆ’åˆ†ä¸ºä¸¤ä¸ªåŒºåŸŸ Acquired = \{x|max_j P(c_j|x)\geq 1-t\}Rejected = \{x|max_j P(c_j|x) < 1-t\}å…¶ä¸­$t$ä¸ºé˜ˆå€¼ï¼Œ$t$è¶Šå°æ—¶ï¼Œæ‹’ç»åŸŸ$Rejected$è¶Šå¤§ï¼Œå½“æ»¡è¶³ 1-t \leq \frac{1}{K}æˆ–è€… t \geq \frac{K-1}{K}æ­¤æ—¶æ‹’ç»åŸŸä¸º Rejected = \{x|max_j P(c_j|x) < \frac{1}{K}\}è€Œå½“ä¸”ä»…å½“å„åˆ†ç±»æ¦‚ç‡ç›¸ç­‰æ—¶æ‰æœ‰ $ max_j P(c_j|x) = \frac{1}{K} $ï¼Œå› æ­¤æ­¤æ—¶æ‹’ç»åŸŸä¸ºç©ºï¼Œæ¥å—æ‰€æœ‰å†³ç­–ç»“æœ æœ€å°é£é™©è´å¶æ–¯å†³ç­–åœ¨å†³ç­–è¿‡ç¨‹ä¸­ï¼Œä¸åŒç±»å‹çš„å†³ç­–é”™è¯¯æ‰€äº§ç”Ÿçš„ä»£ä»·æ˜¯ä¸åŒçš„ã€‚å¼•å…¥é£é™©å‡½æ•° \lambda_{i, j} = \lambda (\alpha_i|c_j)è¡¨ç¤ºå®é™…ç±»åˆ«ä¸º$c_j$æ—¶ï¼Œé‡‡å–é”™è¯¯åˆ¤æ–­ä¸º$c_i$çš„è¡Œä¸º$\alpha_i$æ‰€äº§ç”Ÿçš„æŸå¤±ã€‚è¯¥å‡½æ•°ç§°ä¸ºæŸå¤±å‡½æ•°ï¼Œé€šå¸¸å®ƒå¯ä»¥ç”¨è¡¨æ ¼çš„å½¢å¼ç»™å‡ºï¼Œå«åšå†³ç­–è¡¨ï¼Œå½¢å¦‚å®šä¹‰æ¡ä»¶é£é™© R(\alpha_i|c_j) = \sum_j \lambda (\alpha_i|c_j) P(c_j|x)ç‰¹åˆ«åœ°ï¼Œå–$0-1$æŸå¤±æ—¶ï¼Œå³æœ€å°é”™è¯¯ç‡è´å¶æ–¯å†³ç­– \lambda (\alpha_i|c_j) = \begin{cases} 0 & i = j \\ 1 & i \neq j \end{cases} å¯èƒ½æ¯”è¾ƒæŠ½è±¡ï¼Œè¿™é‡Œä¸¾äº†ä¸€ä¸ªä¾‹å­ å…³äºåˆ¤åˆ«å‡½æ•°å¯æŸ¥çœ‹åˆ†ç±»é—®é¢˜çš„å†³ç­–å¹³é¢ ç¨‹åº ä¸ºå¸®åŠ©ç†è§£ï¼Œå…ˆæ‰‹åŠ¨è®¡ç®—ä¸€éç»“æœ å…ˆéªŒæ¦‚ç‡(priori probability):$ P(Y = -1) = \frac{6}{15} $$ P(Y = 1) = \frac{9}{15} $ä¼¼ç„¶å‡½æ•°(likelihood)$ P(X^{(1)} = 1|Y=-1) = \frac{3}{6}$$ P(X^{(1)} = 2|Y=-1) = \frac{2}{6}$$ P(X^{(1)} = 3|Y=-1) = \frac{1}{6}$$ P(X^{(2)} = S|Y=-1) = \frac{3}{6}$$ P(X^{(2)} = M|Y=-1) = \frac{2}{6}$$ P(X^{(2)} = L|Y=-1) = \frac{1}{6}$$ P(X^{(1)} = 1|Y=1) = \frac{2}{9}$$ P(X^{(1)} = 2|Y=1) = \frac{3}{9}$$ P(X^{(1)} = 3|Y=1) = \frac{4}{9}$$ P(X^{(2)} = S|Y=1) = \frac{1}{9}$$ P(X^{(2)} = M|Y=1) = \frac{4}{9}$$ P(X^{(2)} = L|Y=1) = \frac{4}{9}$ æ³¨æ„ï¼šè¯æ®å› å­(evidence)ä¸èƒ½ç”¨å¦‚ä¸‹æœ´ç´ è´å¶æ–¯æ±‚è§£ P(X) = P(X^{(1)}) P(X^{(2)})è€Œæ˜¯ P(X) = P(X^{(1)}|Y=-1)P(Y = -1) + P(X^{(2)}|Y=-1)P(Y = -1)ä¸€èˆ¬åˆ†å­ç”¨æœ´ç´ è´å¶æ–¯æ±‚è§£ P(X|Y) = P(X^{(1)}|Y) P(X^{(2)}|Y)å°†å…¶åŠ å’Œä½œä¸ºåˆ†æ¯ c_k: P(X)_k = \sum_{k=0}^2 P(X^{(1)}|Y=k) P(X^{(2)}|Y=k)P(Y_k|X) = \frac{P(X|Y_k)P(Y_k)}{P(X)_k}é€‰å–æœ€å¤§æ¦‚ç‡çš„$ k $ç±»åˆ«ä½œä¸ºåˆ¤åˆ«ç±»åˆ« k = argmax_k P(Y_k|X)ä»£ç @Github: Code for Naive Bayes Decision training step12345678910def fit(self, X, y): X_encoded = self.featureEncoder.fit_transform(X).toarray() y_encoded = OneHotEncoder().fit_transform(y.reshape((-1, 1))).toarray() self.P_X = np.mean(X_encoded, axis=0) # one-hotç¼–ç ä¸‹ï¼Œå„åˆ—çš„å‡å€¼å³å„ç‰¹å¾çš„æ¦‚ç‡ self.P_Y = np.mean(y_encoded, axis=0) # one-hotç¼–ç ä¸‹ï¼Œå„åˆ—çš„å‡å€¼å³å„äº†åˆ«çš„æ¦‚ç‡ self.n_labels, self.n_features = y_encoded.shape[1], X_encoded.shape[1] self.P_X_Y = np.zeros(shape=(self.n_labels, self.n_features)) # å„ä¸ªç±»åˆ«ä¸‹ï¼Œåˆ†åˆ«ç»Ÿè®¡å„ç‰¹å¾çš„æ¦‚ç‡ for i in range(self.n_labels): X_encoded_of_yi = X_encoded[y_encoded[:, i]==1] # å–å‡ºå±äºiç±»åˆ«çš„æ ·æœ¬ self.P_X_Y[i] = np.mean(X_encoded_of_yi, axis=0) # one-hotç¼–ç ä¸‹ï¼Œå„åˆ—çš„å‡å€¼å³å„ç‰¹å¾çš„æ¦‚ç‡ predict step1234567891011def predict(self, X): X_encoded = self.featureEncoder.transform(X).toarray() n_samples = X_encoded.shape[0] y_pred_prob = np.zeros(shape=(n_samples, self.n_labels)) for i in range(n_samples): for j in range(self.n_labels): P_Xi_encoded_Yj = X_encoded[i] * self.P_X_Y[j] # åœ¨Yjç±»åˆ«ä¸‹ï¼Œé€‰å‡ºè¾“å…¥æ ·æœ¬Xiå¯¹åº”çš„æ¡ä»¶æ¦‚ç‡ P_Xi_encoded_Yj[P_Xi_encoded_Yj==0.0] = 1.0 # å°†ä¸º0å€¼æ›¿æ¢ä¸º1ï¼Œä¾¿äºæ±‚è§£Î P(Xi|yc)ï¼Œåªè¦å°†å„å…ƒç´ ç´¯ä¹˜å³å¯ y_pred_prob[i, j] = self.P_Y[j] * P_Xi_encoded_Yj.prod() y_pred_prob[i] /= np.sum(y_pred_prob[i]) # åˆ†æ¯ä¸€èˆ¬æ˜¯å°†åˆ†å­åŠ å’Œï¼Œä¸èƒ½å‡å®šå„ç‰¹å¾ç‹¬ç«‹å¹¶ç”¨æœ´ç´ è´å¶æ–¯è®¡ç®—åˆ†æ¯ return np.argmax(y_pred_prob, axis=1) main123456789101112X = [ [1, 0], [1, 1], [1, 1], [1, 0], [1, 0], [2, 0], [2, 1], [2, 1], [2, 2], [2, 2], [3, 2], [3, 1], [3, 2], [3, 2], [3, 2]]y = [0 ,0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]estimator = NaiveBayes()estimator.fit(X, y)X_test = np.array([[2, 0], [1, 1]])y_pred = estimator.predict(X_test)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Softmax Regression]]></title>
    <url>%2F2018%2F10%2F18%2FSoftmax-Regression%2F</url>
    <content type="text"><![CDATA[Unsupervised Feature Learning and Deep Learning Tutorial å¼•è¨€Logistic Regressionä¸­é‡‡ç”¨çš„éçº¿æ€§å‡½æ•°ä¸ºSigmoidï¼Œå°†è¾“å‡ºå€¼æ˜ å°„åˆ°$(0, 1)$ä¹‹é—´ä½œä¸ºæ¦‚ç‡è¾“å‡ºï¼Œå¤„ç†çš„æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œé‚£ä¹ˆå¯¹äºå¤šåˆ†ç±»çš„é—®é¢˜æ€ä¹ˆå¤„ç†å‘¢ï¼Ÿ æ¨¡å‹ ç”±Logisticå›å½’æ¨å¹¿è€Œæ¥ SoftmaxSoftmaxåœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä¸­æœ‰ç€éå¸¸å¹¿æ³›çš„åº”ç”¨ã€‚å°¤å…¶åœ¨å¤„ç†å¤šåˆ†ç±»$(K&gt;2)$é—®é¢˜ï¼Œåˆ†ç±»å™¨æœ€åçš„è¾“å‡ºå•å…ƒéœ€è¦Softmaxå‡½æ•°è¿›è¡Œæ•°å€¼å¤„ç†ã€‚ S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]å…¶ä¸­$x$ä¸ºçŸ©é˜µå½¢å¼çš„å‘é‡ï¼Œå…¶ç»´åº¦ä¸º$(KÃ—1)$ï¼Œ$K$ä¸ºç±»åˆ«æ•°ç›®ã€‚Softmaxçš„è¾“å‡ºå‘é‡ç»´åº¦ä¸$x$ç›¸åŒï¼Œå„å…ƒç´ $x_i$åŠ å’Œä¸º$1$ï¼Œå¯ç”¨äºè¡¨ç¤ºå–å„ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚ æ³¨æ„åˆ°ï¼Œå¯¹äºå‡½æ•°$e^x$ \lim_{x \rightarrow - \infty} e^x = 0\lim_{x \rightarrow + \infty} e^x = +\infty å‡è®¾æ‰€æœ‰çš„$x_i$ç­‰äºæŸå¸¸æ•°$c$ï¼Œç†è®ºä¸Šå¯¹æ‰€æœ‰$x_i$ä¸Šå¼ç»“æœä¸º$\frac{1}{n}$ è‹¥$c$ä¸ºå¾ˆå°çš„è´Ÿæ•°ï¼Œ$e^c$ä¸‹æº¢ï¼Œç»“æœä¸º$NaN$ï¼› è‹¥$c$é‡çº§å¾ˆå¤§ï¼Œ$e^c$ä¸Šæº¢ï¼Œç»“æœä¸º$NaN$ã€‚ åœ¨æ•°å€¼è®¡ç®—æ—¶å¹¶ä¸ç¨³å®šï¼Œä½†æ˜¯Softmaxæ‰€æœ‰è¾“å…¥å¢åŠ åŒä¸€å¸¸æ•°æ—¶ï¼Œè¾“å‡ºä¸å˜ï¼Œå¾—ç¨³å®šç‰ˆæœ¬ï¼š S(x) := S(x - max(x_i)) e^{x_{max} - max(x_i)} = 1 å‡å»æœ€å¤§å€¼å¯¼è‡´$e^x$æœ€å¤§ä¸º$1$ï¼Œæ’é™¤ä¸Šæº¢ï¼› åˆ†æ¯ä¸­è‡³å°‘æœ‰ä¸€é¡¹ä¸º$1$ï¼Œæ’é™¤åˆ†æ¯ä¸‹æº¢å¯¼è‡´å¤„ä»¥$0$çš„æƒ…å†µã€‚ å…¶å¯¹æ•° log S(x)_i = x_i - log ({\sum_{k=1}^K exp(x_k)}) æ³¨æ„åˆ°ï¼Œç¬¬ä¸€é¡¹è¡¨ç¤ºè¾“å…¥$x_i$æ€»æ˜¯å¯¹ä»£ä»·å‡½æ•°æœ‰ç›´æ¥çš„è´¡çŒ®ã€‚è¿™ä¸€é¡¹ä¸ä¼šé¥±å’Œï¼Œæ‰€ä»¥å³ä½¿$x_i$å¯¹ä¸Šå¼çš„ç¬¬äºŒé¡¹çš„è´¡çŒ®å¾ˆå°ï¼Œå­¦ä¹ ä¾ç„¶å¯ä»¥è¿›è¡Œï¼› å½“æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶æ—¶ï¼Œç¬¬ä¸€é¡¹é¼“åŠ±$x_i$è¢«æ¨é«˜ï¼Œè€Œç¬¬äºŒé¡¹åˆ™é¼“åŠ±æ‰€æœ‰çš„$x$è¢«å‹ä½ï¼› ç¬¬äºŒé¡¹$log ({\sum_{k=1}^K exp(x_k)})$å¯ä»¥å¤§è‡´è¿‘ä¼¼ä¸º$max(x_k)$ï¼Œè¿™ç§è¿‘ä¼¼æ˜¯åŸºäºå¯¹ä»»ä½•æ˜æ˜¾å°äº$max(x_k)$çš„$x_k$éƒ½æ˜¯ä¸é‡è¦çš„ï¼Œè´Ÿå¯¹æ•°ä¼¼ç„¶ä»£ä»·å‡½æ•°æ€»æ˜¯å¼ºçƒˆåœ°æƒ©ç½šæœ€æ´»è·ƒçš„ä¸æ­£ç¡®é¢„æµ‹ é™¤äº†å¯¹æ•°ä¼¼ç„¶ä¹‹å¤–çš„è®¸å¤šç›®æ ‡å‡½æ•°å¯¹ softmax å‡½æ•°ä¸èµ·ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œé‚£äº›ä¸ä½¿ç”¨å¯¹æ•°æ¥æŠµæ¶ˆ softmax ä¸­çš„æŒ‡æ•°çš„ç›®æ ‡å‡½æ•°ï¼Œå½“æŒ‡æ•°å‡½æ•°çš„å˜é‡å–éå¸¸å°çš„è´Ÿå€¼æ—¶ä¼šé€ æˆæ¢¯åº¦æ¶ˆå¤±ï¼Œä»è€Œæ— æ³•å­¦ä¹  ä½œè€…ï¼šNirHeavenXæ¥æºï¼šCSDNåŸæ–‡ï¼šhttps://blog.csdn.net/qsczse943062710/article/details/61912464ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºåšä¸»åŸåˆ›æ–‡ç« ï¼Œè½¬è½½è¯·é™„ä¸Šåšæ–‡é“¾æ¥ï¼ Softmaxè§£å†³å¤šåˆ†ç±»é—®é¢˜å¯¹äºå…·æœ‰$K$ä¸ªåˆ†ç±»çš„é—®é¢˜ï¼Œæ¯ä¸ªç±»åˆ«è®­ç»ƒä¸€ç»„å‚æ•°$ w_k $ z_k^{(i)} = w_k^Tx^{(i)}æˆ–å†™ä½œçŸ©é˜µå½¢å¼ z^{(i)} = W^Tx^{(i)}å…¶ä¸­ x^{(i)} = \left[ \begin{matrix} x_0^{(i)}\\ x_1^{(i)}\\ ...\\ x_n^{(i)} \end{matrix} \right]_{nÃ—1}, x_0^{(i)}=1 W = [w_1, w_2, ..., w_K]_{(n+1)Ã—K} w_i = \left[ \begin{matrix} w_{i0}\\ w_{i1}\\ ...\\ w_{in} \end{matrix} \right]_{nÃ—1}æœ€ç»ˆå„ç±»åˆ«è¾“å‡ºæ¦‚ç‡ä¸º \hat{y}^{(i)} = Softmax(z^{(i)}) äº§ç”Ÿäº†ä¸€ä¸ªå¥‡æ€ªçš„è„‘æ´ã€‚ã€‚ã€‚äºŒåˆ†ç±»é—®é¢˜ p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }å®šä¹‰äºŒåˆ†ç±»çº¿æ€§å•å…ƒè¾“å‡ºçš„å·®å€¼ä¸º z = x_1 - x_2å¾—åˆ° p(x_1) = \frac{1}{1 + e^{-z}}ä»¥$x_1 = [x_{11}, x_{12}]^T$ä¸ºä¾‹(äºŒç»´ç‰¹å¾)ï¼Œå–$w_1=1, w_2=2, b=3$ p(x_1) = \frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}} è€Œå¤šåˆ†ç±»é—®é¢˜ï¼Œä»¥$3$åˆ†ç±»ä¸ºä¾‹ p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }å®šä¹‰çº¿æ€§å•å…ƒè¾“å‡ºçš„å·®å€¼ä¸º z_{12} = x_1 - x_2 z_{13} = x_1 - x_3 p(x_1) = \frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }åšå‡ºå›¾åƒä¸º æŸå¤±å‡½æ•°ç”±äº¤å‰ç†µç†è§£CrossEnt = \sum_j p_j log \frac{1}{q_j}è€Œå¯¹äºæ ·æœ¬$ (X^{(i)}, y^{(i)}) $ï¼Œä¸ºç¡®å®šäº‹ä»¶ï¼Œæ•…æ ‡ç­¾æ¦‚ç‡å„å…ƒç´ çš„å–å€¼$p_j$ä¸º$ y^{(i)}_j âˆˆ \{0,1\}$ï¼Œ$ q_jå³é¢„æµ‹è¾“å‡ºçš„æ¦‚ç‡å€¼\hat{y}^{(i)}_j$ ä¸€èˆ¬å–å„ä¸ªæ ·æœ¬æŸå¤±çš„å‡å€¼$(\frac{1}{N})$ L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N 1\{y^{(i)}_j=k\} log (\hat{y}^{(i)}_j) 1\{y^{(i)}_j=k\} = \begin{cases} 1 & y^{(i)}_j = k \\ 0 & y^{(i)}_j \neq k \end{cases}å¯å¯¹å®é™…æ ‡ç­¾$y^{(i)}$é‡‡å–One-Hotç¼–ç ï¼Œä¾¿äºè®¡ç®— y^{(i)} = \left[ \begin{matrix} 0, ..., 1_{y^{(i)}}, ..., 0 \end{matrix} \right]^Tåˆ™ L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N y^{(i)T} log (\hat{y}^{(i)})ç”±å†³ç­–å¹³é¢ç†è§£ä»è´å¶æ–¯å†³ç­–å’Œåˆ†ç±»é—®é¢˜çš„å†³ç­–å¹³é¢å¯çŸ¥ï¼Œå¯¹äºç±»åˆ«$c_i$ï¼Œæœ‰ P(c_i|x) = \frac{P(x|c_i)}{\sum_{j=0}^KP(x|c_j)} å‡è®¾æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æœä»æ­£æ€åˆ†å¸ƒï¼Œå…ˆéªŒæ¦‚ç‡ç›¸ç­‰ï¼Œå„ç±»åˆ«æ ·æœ¬ç‰¹å¾é—´åæ–¹å·®ç›¸ç­‰ã€‚è¯æ˜ç•¥. æ¢¯åº¦æ¨å¯¼Softmaxå‡½æ•°çš„å¯¼æ•°å¯¹äº S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]ä¸€èˆ¬è¾“å‡ºä½œä¸ºæ¦‚ç‡å€¼ï¼Œè®° P = S(x)p_i = S(x)_iå¯¹å‘é‡$x$ä¸­æŸå…ƒç´ æ±‚å¯¼ \frac{âˆ‚S(x)}{âˆ‚x_i} = \frac{âˆ‚}{âˆ‚x_i} \left[ \begin{matrix} ...\\ \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}\\ ...\\ \end{matrix} \right] $(1)$ $i=k$$\frac{âˆ‚}{âˆ‚x_i} \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{expâ€™(x_i)Â·\sum_{j=1}^K exp(x_j) - exp(x_i)Â·(\sum_{j=1}^K exp(x_j))â€™}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)Â·\sum_{j=1}^K exp(x_j) - exp^2(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)} -(\frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)})^2$$ = p_i (1 - p_i)$ $(2)$ $i\neq k$$\frac{âˆ‚}{âˆ‚x_i} \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{expâ€™(x_k)Â·\sum_{j=1}^K exp(x_j) - exp(x_k)Â·(\sum_{j=1}^K exp(x_j))â€™}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{- exp(x_k)exp(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$= - p_i p_k$ ç»¼ä¸Š \frac{âˆ‚S(x)}{âˆ‚x_i}_{KÃ—1} = \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] - \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right] = \left( \left[ \begin{matrix} 0\\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i æŸå¤±å‡½æ•°æ¢¯åº¦åœ¨OneHotç¼–ç ä¸‹ï¼ŒæŸå¤±å‡½æ•°å½¢å¼ä¸º L(\hat{y},y) = \frac{1}{N} \sum_{i=1}^N L (y^{(i)}, \hat{y}^{(i)}) L (y^{(i)}, \hat{y}^{(i)}) = - y^{(i)T} log \hat{y}^{(i)} \hat{y}^{(i)} = S(z^{(i)}) z^{(i)} = W^T x^{(i)}å³åªè€ƒè™‘å®é™…åˆ†ç±»å¯¹åº”çš„æ¦‚ç‡å€¼ L (y^{(i)}, \hat{y}^{(i)}) = - log \hat{y}^{(i)}_{y^{(i)}} ç”±äº $S(z^{(i)})_{t^{(i)}}$ä¸$z^{(i)}$å‘é‡å„ä¸ªå…ƒç´ éƒ½æœ‰å…³ï¼Œç”±é“¾å¼æ±‚å¯¼æ³•åˆ™ \frac{âˆ‚ L^{(i)} }{âˆ‚w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } ( \sum_{k=1}^K \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_k} \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} )$1.$ è€ƒå¯Ÿ $\frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_k}$ \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_k} = â€‹ \begin{cases} â€‹ \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_k) & k=y^{(i)} \\ â€‹ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_k & k \neq y^{(i)} â€‹ \end{cases}$2.$ è€ƒå¯Ÿ $\frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}}$ \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} = \begin{cases} \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} = x^{(i)}_p & k=q\\ \frac{âˆ‚z^{(i)}_k}{âˆ‚w_{pq}} = 0 & k \neq q \end{cases} ç»¼ä¸Šæ‰€è¿° \frac{âˆ‚ L^{(i)} }{âˆ‚w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_q} \frac{âˆ‚z^{(i)}_q}{âˆ‚w_{pq}}å…¶ä¸­ \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_q} = \begin{cases} \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_q) & q = y^{(i)}\\ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_q & q \neq y^{(i)} \end{cases} \frac{âˆ‚z^{(i)}_q}{âˆ‚w_{pq}} = x^{(i)}_pæ•…å¯¹äºå•ä¸ªæ ·æœ¬$(X^{(i)}, y^{(i)})$ï¼Œå½“æ ·æœ¬æ ‡ç­¾é‡‡ç”¨$OneHot$ç¼–ç æ—¶ \frac{âˆ‚L^{(i)}}{âˆ‚w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{âˆ‚ \hat{y}^{(i)}_{y^{(i)}} }{âˆ‚z^{(i)}_q} x^{(i)}_p = \begin{cases} (\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\ \hat{y}^{(i)}_qx^{(i)}_p & q \neq y^{(i)} \end{cases} æ³¨ï¼š è¿™é‡Œå¯ä»¥çº¦åˆ†å»æ‰$\hat{y}^{(i)}_{y^{(i)}}$ \frac{âˆ‚L^{(i)}}{âˆ‚w_{pq}} = ( \hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_pæ›´ä¸€èˆ¬çš„ï¼Œå†™æˆçŸ©é˜µå½¢å¼ï¼Œè®°$X = [x_1, x_2, â€¦, x_m]^T$ï¼Œ$x_i$ä¸ºæ ·æœ¬ç‰¹å¾(åˆ—å‘é‡) âˆ‡_W L = X^T(\hat{Y} - Y) ç”¨çº¿æ€§æ¨¡å‹è§£å†³åˆ†ç±»å’Œå›å½’é—®é¢˜æ—¶ï¼Œå½¢å¼ç«Ÿå¦‚æ­¤ç»Ÿä¸€! è‡³æ­¤ä¸ºæ­¢ï¼Œæ¢¯åº¦æ¨å¯¼ç»“æŸï¼Œåˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿­ä»£æ±‚è§£å‚æ•°çŸ©é˜µ$W$å³å¯ã€‚ W := W - \alpha âˆ‡_W Lä»£ç @GitHub: Code of Softmax Regression Softmax12345678def softmax(X): &quot;&quot;&quot; æ•°å€¼è®¡ç®—ç¨³å®šç‰ˆæœ¬çš„softmaxå‡½æ•° @param &#123;ndarray&#125; X: shape(batch_size, n_labels) &quot;&quot;&quot; X_max = np.max(X, axis=1).reshape((-1, 1)) # æ¯è¡Œçš„æœ€å¤§å€¼ X = X - X_max # æ¯è¡Œå‡å»æœ€å¤§å€¼ X = np.exp(X) return X / np.sum(X, axis=1).reshape((-1, 1)) cost function1234567891011def crossEnt(self, y_label_true, y_prob_pred): &quot;&quot;&quot; è®¡ç®—äº¤å‰ç†µæŸå¤±å‡½æ•° @param &#123;ndarray&#125; y_label_true: çœŸå®æ ‡ç­¾ shape(batch_size,) @param &#123;ndarray&#125; y_prob_pred: é¢„æµ‹è¾“å‡º shape(batch_size, n_labels) &quot;&quot;&quot; mask = self.encoder.transform(y_label_true.reshape(-1, 1)).toarray() # shape(batch_size, n_labels) y_prob_masked = np.sum(mask * y_prob_pred, axis=1) # æ¯è¡ŒçœŸå®æ ‡ç­¾å¯¹åº”çš„é¢„æµ‹è¾“å‡ºå€¼ y_prob_masked[y_prob_masked==0.] = 1. y_loss = np.log(y_prob_masked) loss = - np.mean(y_loss) # æ±‚å„æ ·æœ¬æŸå¤±çš„å‡å€¼ return loss gradient12345678910def grad(self, X_train, y_train, y_prob_pred): &quot;&quot;&quot; è®¡ç®—æ¢¯åº¦ \frac &#123;âˆ‚L&#125; &#123;âˆ‚W_&#123;pq&#125;&#125; @param X_train: è®­ç»ƒé›†ç‰¹å¾ @param y_train: è®­ç»ƒé›†æ ‡ç­¾ @param y_prob_pred: è®­ç»ƒé›†é¢„æµ‹æ¦‚ç‡è¾“å‡º @param y_label_pred: è®­ç»ƒé›†é¢„æµ‹æ ‡ç­¾è¾“å‡º &quot;&quot;&quot; y_train = self.encoder.transform(y_train) dW = X_train.T.dot(y_prob_pred - y_train) return dW training stepçœç•¥å¯è§†åŒ–å’ŒéªŒè¯éƒ¨åˆ†çš„ä»£ç 123456789101112131415161718192021222324252627282930313233343536def fit(self, X_train, X_valid, y_train, y_valid, min_acc=0.95, max_epoch=20, batch_size=20): &quot;&quot;&quot; è®­ç»ƒ &quot;&quot;&quot; # æ·»åŠ é¦–1åˆ—ï¼Œè¾“å…¥åˆ°åç½®w0 X_train = np.c_[np.ones(shape=(X_train.shape[0],)), X_train] X_valid = np.c_[np.ones(shape=(X_valid.shape[0],)), X_valid] X_train = self.scaler.fit_transform(X_train) # å°ºåº¦å½’ä¸€åŒ– X_valid = self.scaler.transform(X_valid) # å°ºåº¦å½’ä¸€åŒ– self.encoder.fit(y_train.reshape(-1, 1)) self.n_features = X_train.shape[1] self.n_labels = self.encoder.transform(y_train).shape[1] # åˆå§‹åŒ–å‚æ•° self.W = np.random.normal(loc=0, scale=1.0, size=(self.n_features, self.n_labels)) n_batch = X_train.shape[0] // batch_size # å¯è§†åŒ–ç›¸å…³ plt.ion() plt.figure(&apos;loss&apos;); plt.figure(&apos;accuracy&apos;) loss_train_epoch = []; loss_valid_epoch = [] acc_train_epoch = []; acc_valid_epoch = [] for i_epoch in range(max_epoch): for i_batch in range(n_batch): # æ‰¹å¤„ç†æ¢¯åº¦ä¸‹é™ n1, n2 = i_batch * batch_size, (i_batch + 1) * batch_size X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2] # é¢„æµ‹ y_prob_train = self.predict(X_train_batch, preprocessed=True) # è®¡ç®—æŸå¤± loss_train_batch = self.crossEnt(y_train_batch, y_prob_train) # è®¡ç®—å‡†ç¡®ç‡ y_label_train = np.argmax(y_prob_train, axis=1) a = y_train_batch.reshape((-1,)) acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((-1,))).astype(&apos;float&apos;)) # è®¡ç®—æ¢¯åº¦ dW dW = self.grad(X_train_batch, y_train_batch, y_prob_train) # æ›´æ–°å‚æ•° self.W -= self.lr * dW predict step123456789101112def predict(self, X, preprocessed=False): &quot;&quot;&quot; å¯¹è¾“å…¥çš„æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å‡ºæ ‡ç­¾ @param &#123;ndarray&#125; X: shape(batch_size, n_features) @return &#123;ndarray&#125; y_prob: probability, shape(batch_size, n_labels) &#123;ndarray&#125; y_label: labels, shape(batch_size,) &quot;&quot;&quot; if not preprocessed: # è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒç”¨æ­¤å‡½æ•°æ—¶ï¼Œä¸ç”¨åŠ é¦–1åˆ— X = np.c_[np.ones(shape=(X.shape[0],)), X] # æ·»åŠ é¦–1é¡¹ï¼Œè¾“å…¥åˆ°åç½®w0 X = self.scaler.transform(X) y_prob = softmax(X.dot(self.W)) # é¢„æµ‹æ¦‚ç‡å€¼ shape(batch_size, n_labels) return y_prob å®éªŒç»“æœä»¥ä¸‹è“çº¿ä¸ºè®­ç»ƒé›†å‚æ•°ï¼Œçº¢çº¿ä¸ºéªŒè¯é›†å‚æ•°ï¼Œè‹¥ç¨³å®šè®­ç»ƒ(å¦‚batch_size = 20çš„ç»“æœ)ï¼Œæœ€ç»ˆå‡†ç¡®ç‡åœ¨$80\%$å·¦å³ã€‚ ç”±äºéšæœºæ¢¯åº¦ä¸‹é™(SGD)éå†æ¬¡æ•°å¤ªå¤šï¼Œè¿è¡Œè¾ƒæ…¢ï¼Œæ²¡æœ‰ç”¨SGDæ–¹æ³•è®­ç»ƒï¼Œå°±å‰å‡ ä¸ªepochæ¥çœ‹ï¼Œæ•ˆæœæ²¡æœ‰batch_size = 20çš„å¥½ï¼› æ·»åŠ éšå«å±‚å½¢æˆä¸‰å±‚ç»“æ„çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¯æé«˜å‡†ç¡®ç‡ï¼› è¿˜æœ‰ä¸€ç‚¹ï¼Œä½¿ç”¨æ‰¹å¤„ç†æ¢¯åº¦ä¸‹é™(n_batch = 1)è®­ç»ƒæ—¶ï¼Œå¯ä»¥çœ‹åˆ°æŸå¤±å€¼å·²ç»è¶‹äº$0$ï¼Œä½†å‡†ç¡®ç‡å´å¾ˆä½ï¼Œè¯´æ˜å·²ç»é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚ batch size = 20 æŸå¤± å‡†ç¡®ç‡ batch_size = 200 æŸå¤± å‡†ç¡®ç‡ n_batch = 1 æŸå¤± å‡†ç¡®ç‡ æ„Ÿæ‚Ÿæ¨å…¬å¼è¦æˆ‘è€å‘½ã€‚ã€‚ã€‚ã€‚ Softmaxå›å½’å¯ä»¥è§†ä½œä¸å«éšå«å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[å¼•è¨€é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰æ˜¯ç”¨äºå¤„ç†å› å˜é‡ä¸ºåˆ†ç±»å˜é‡çš„å›å½’é—®é¢˜ï¼Œå¸¸è§çš„æ˜¯äºŒåˆ†ç±»æˆ–äºŒé¡¹åˆ†å¸ƒé—®é¢˜ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šåˆ†ç±»é—®é¢˜ï¼Œå®ƒå®é™…ä¸Šæ˜¯å±äºä¸€ç§åˆ†ç±»æ–¹æ³•ã€‚ æ¨¡å‹å…ˆç»™å‡ºæ¨¡å‹ï¼Œæ¨å¯¼è¿‡ç¨‹ç¨åç»™å‡ºï¼Œé€»è¾‘å›å½’åŒ…å«Sigmoidå‡½æ•° f(z) = \frac{1}{1+e^{-z}}å…¶å›¾åƒå¦‚ä¸‹ å®šä¹‰ z = w^Txå…¶ä¸­$x=[x_0, x_1, â€¦, x_n]^T, x_0=1$ h_w(x) = g(z) = \frac{1}{1+e^{-z}}æŸå¤±å‡½æ•°ç”±æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¨å¯¼å¯¹äºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œå…¶å–å€¼ä½œä¸ºéšæœºå˜é‡ï¼Œæœä»äºŒé¡¹åˆ†å¸ƒ $B(1, p)$ï¼Œå…¶ä¸­$p$å³ä¸ºé¢„æµ‹è¾“å‡ºæ¦‚ç‡$\hat{y}$ P(y_i^{(i)}) = (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}ç”±æå¤§ä¼¼ç„¶ä¼°è®¡ L = \prod_{i=0}^N P(y_i^{(i)}) = \prod_{i=0}^N (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}å–å¯¹æ•°ä¼¼ç„¶å‡½æ•° logL = \sum_{i=0}^N [y_i^{(i)} log \hat{y}_i^{(i)} + (1-y_i^{(i)}) log (1-\hat{y}_i^{(i)})]ä¼˜åŒ–ç›®æ ‡æ˜¯ w = argmax_w logLä¼˜åŒ–é—®é¢˜ä¸€èˆ¬è¡¨è¿°æˆminimizeé—®é¢˜ï¼Œæ·»åŠ è´Ÿå·ï¼Œæ„æˆNeg Log LikelihoodæŸå¤± w = argmin_w (-logL)ä¸€èˆ¬å–å‡å€¼ L(\hat{y}, y)=- \frac{1}{N} \sum_i [y_i^{(i)} log(\hat{y}_i^{(i)})+(1 - y_i^{(i)})log(1-\hat{y}_i^{(i)})]å…¶ä¸­$y_i$è¡¨ç¤ºçœŸå®å€¼ï¼Œ$\hat{y}_i$è¡¨ç¤ºé¢„æµ‹å€¼ ä»äº¤å‰ç†µç†è§£å·²çŸ¥äº¤å‰ç†µcross entropyå®šä¹‰å¦‚ä¸‹ CrossEnt = \sum_i p_i log \frac{1}{q_i}è€Œå¯¹äºæ ·æœ¬$ (X_i, y_i) $ï¼Œä¸ºç¡®å®šäº‹ä»¶ï¼Œæ•…æ ‡ç­¾æ¦‚ç‡çš„å–å€¼ä¸º$ p_i = y_i âˆˆ \{0,1\}$ï¼Œ$ q_iå³é¢„æµ‹è¾“å‡ºçš„æ¦‚ç‡å€¼\hat{y}_i $ï¼Œå¯å¾—åˆ°ä¸ä¸Šé¢ç›¸åŒçš„æ¨å¯¼ç»“è®º ä»å†³ç­–å¹³é¢å’Œè´å¶æ–¯å†³ç­–ç†è§£ç›¸å…³å†…å®¹æŸ¥çœ‹åˆ†ç±»é—®é¢˜çš„å†³ç­–å¹³é¢å’Œè´å¶æ–¯å†³ç­–ï¼Œé€»è¾‘å›å½’è€ƒè™‘çš„ä¸€èˆ¬æ˜¯ç­‰å…ˆéªŒæ¦‚ç‡é—®é¢˜ï¼Œæ•…å†³ç­–å‡½æ•°å®šä¹‰ä¸º $if$ $P(c_i|x)&gt;P(c_j|x)$ $then$ $ x \in c_i $, $ i, j = 1, 2 $ ä»è´å¶æ–¯å†³ç­–å¯çŸ¥ï¼Œå¯¹äºç±»åˆ«$c_1$ï¼Œæœ‰ P(c_1|x) = \frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}è®¾åœ¨å„ä¸ªç±»åˆ«ä¸‹ï¼Œç‰¹å¾$x$æœä»æ­£æ€åˆ†å¸ƒ P(x|c_i) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i))åˆ™ P(c_1|x) = \frac {1} { 1 + exp(-z) } P(c_2|x) = 1 - P(c_1|x) = \frac{exp(-z)}{1+exp(-z)} $P(c_1|x) = \frac{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}$ $P(c_1|x) = \frac{1}{1 + \frac{exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}}$ å‡å®šå„åˆ†ç±»çš„æ ·æœ¬æ–¹å·®ç›¸ç­‰ï¼Œ$ \Sigma_1 = \Sigma_2 = \sigma^2 I $ $ P(c_1|x) = \frac {1}{1 + exp(- [ \frac{1}{\sigma^2} (\mu_1-\mu_2)^T x - \frac{1}{2 \sigma^2} (\mu_1^T\mu_1 - \mu_2^T\mu_2) ])}$ ä»¤ w = \frac{1}{\sigma^2} (\mu_1 -\mu_2)b = - \frac{1}{2\sigma^2}(\mu_1^T \mu_1 - \mu_2^T \mu_2)å³å¯å¾—åˆ° P(c_1|x) = \frac {1} { 1 + exp(-z) }å…¶ä¸­ z = w^T x + b æ¢¯åº¦æ¨å¯¼å…ˆæ¨å¯¼Sigmoidå‡½æ•°çš„å¯¼æ•° f'(z) = (1 - f(z))f(z)å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»$fâ€™(z)$çš„å›¾åƒå¯ä»¥çœ‹åˆ°ï¼Œåœ¨$ x=0 $å¤„$fâ€™(z)$å–æå¤§å€¼ï¼Œä¸” f'(z)_{max} = f'(z)|_{z=0} = 0.25 \lim_{z \rightarrow \infty} f'(z) = 0åœ¨å¤šå±‚ç¥ç»ç½‘ç»œåå‘ä¼ æ’­æ›´æ–°å‚æ•°æ—¶ï¼Œç”±äºæ¢¯åº¦å¤šæ¬¡ç´¯ä¹˜ï¼ŒSigmoidä½œä¸ºæ¿€æ´»å‡½æ•°ä¼šå­˜åœ¨â€œæ¢¯åº¦æ¶ˆå¤±â€çš„é—®é¢˜ï¼Œä½¿å¾—å‚æ•°æ›´æ–°éå¸¸ç¼“æ…¢ã€‚ $ fâ€™(z) $$ = (\frac{1}{1+e^{-z}})â€™ $$ = \fracâ€‹ {-(1+e^{-z})â€™}â€‹ {(1+e^{-z})^2} $$ = \fracâ€‹ {e^{-z}}â€‹ {(1+e^{-z})^2} $$ = \fracâ€‹ {e^{-z}}â€‹ {1+e^{-z}}â€‹ \fracâ€‹ {1}â€‹ {1+e^{-z}}$$ = (1 - f(z))f(z)$ åˆ©ç”¨é“¾å¼æ±‚å¯¼æ³•åˆ™å¯å¾— $\frac{âˆ‚L}{âˆ‚w_j}$$= -\frac{âˆ‚}{âˆ‚w_j} \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})]$$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\frac{âˆ‚}{âˆ‚w_j}\hat{y}^{(i)}-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\frac{âˆ‚}{âˆ‚w_j}\hat{y}^{(i)}]$$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j]$$= - \frac{1}{N} \sum_i [y^{(i)} (1-\hat{y}^{(i)})w_j-(1-y^{(i)}) y^{(i)} w_j]$$= \frac{1}{N} \sum_i (\hat{y}^{(i)} - y^{(i)})w_j $ å†™ä½œçŸ©é˜µå½¢å¼ï¼Œè®°$X = [x_1, x_2, â€¦, x_m]^T$ï¼Œ$x_i$ä¸ºæ ·æœ¬ç‰¹å¾(åˆ—å‘é‡) âˆ‡_w L = X^T (\hat{Y} - Y)è®­ç»ƒå’Œçº¿æ€§å›å½’ä¸€æ ·ï¼Œé‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£ w := w - \alpha âˆ‡_w Lå¤„ç†å¤šåˆ†ç±»é—®é¢˜å‡è®¾æœ‰$K$ä¸ªç±»åˆ«ï¼Œåˆ™ä¾æ¬¡ä»¥ç±»åˆ«$c_i$ä¸ºæ­£æ ·æœ¬è®­ç»ƒæ¨¡å‹ï¼Œä¸€å…±è®­ç»ƒ$K$ä¸ªã€‚æµ‹è¯•æ ·æœ¬åœ¨æ¯ä¸ªæ¨¡å‹ä¸Šè®¡ç®—ï¼Œæœ€ç»ˆå°†æ¦‚ç‡æœ€å¤§çš„ä½œä¸ºåˆ†ç±»ç»“æœã€‚ è¿™æ ·åˆ’åˆ†æ•°æ®é›†ï¼Œä¼šä½¿è®­ç»ƒé›†æ­£è´Ÿæ ·æœ¬æ•°ç›®ä¸¥é‡ä¸å¯¹ç§°ï¼Œç‰¹åˆ«æ˜¯ç±»åˆ«å¾ˆå¤šçš„æƒ…å†µï¼Œå¯¹ç»“æœä¼šäº§ç”Ÿå½±å“ã€‚å¯æ¨å¹¿è‡³softmaxå›å½’è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ ç¨‹åºä»£ç @Github: Code for Logistic Regression cost function123456789101112131415def lossFunctionDerivative(self, X, theta, y_true): &apos;&apos;&apos; è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°thetaçš„æ¢¯åº¦ å¯¹theta[j]çš„æ¢¯åº¦ä¸ºï¼š(y_pred - y_true)*x[j] &apos;&apos;&apos; err = self.predict_prob(X, theta) - y_true return X.T.dot(err)/y_true.shape[0]def lossFunction(self, y_pred_prob, y_true): &apos;&apos;&apos; æœªä½¿ç”¨ è®¡ç®—æŸå¤±å€¼: Cross-Entropy y_pred_prob, y_true: NumPy array, shape=(n,) &apos;&apos;&apos; tmp = y_true*np.log(y_pred_prob) + (1 - y_true)*np.log(1 - y_pred_prob) return np.mean(-tmp) training step123456789101112131415161718192021def gradDescent(self, min_acc, learning_rate=0.01, max_iter=10000): acc = 0; n_iter = 0 for n_iter in range(max_iter): for n in range(self.n_batch): X_batch = self.X[n*self.batch_size:(n+1)*self.batch_size] t_batch = self.t[n*self.batch_size:(n+1)*self.batch_size] grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch) self.theta -= learning_rate * grad # æ¢¯åº¦ä¸‹é™ acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t) if acc &gt; min_acc: print(&apos;ç¬¬%dæ¬¡è¿­ä»£, ç¬¬%dæ‰¹æ•°æ®&apos; % (n_iter, n)) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬å‡†ç¡®ç‡ä¸º: &quot;, acc) print(&quot;å½“å‰å‚æ•°å€¼ä¸º: &quot;, self.theta) return self.theta if n_iter%100 == 0: print(&apos;ç¬¬%dæ¬¡è¿­ä»£&apos; % n_iter) print(&apos;å‡†ç¡®ç‡ï¼š &apos;, acc) print(&quot;è¶…è¿‡è¿­ä»£æ¬¡æ•°&quot;) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬å‡†ç¡®ç‡ä¸º: &quot;, acc) print(&quot;å½“å‰å‚æ•°å€¼ä¸º: &quot;, self.theta) return self.theta å®éªŒç»“æœ]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLinear-Regression%2F</url>
    <content type="text"><![CDATA[å¼•è¨€çº¿æ€§å›å½’å¯ä»¥è¯´æ˜¯æœºå™¨å­¦ä¹ æœ€åŸºç¡€çš„ç®—æ³• æ¨¡å‹\hat{y}^{(i)} = w^Tx^{(i)}å…¶ä¸­ x^{(i)}=[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T, x_0^{(i)}=1è¿™é‡Œ$x_0^{(i)}=1$è¡¨ç¤ºåç½®$b$ï¼Œå³$b=w_0$ \hat{y}^{(i)} = w^Tx^{(i)} + b æ³¨ï¼šå¯¹äºéçº¿æ€§çš„æ•°æ®ï¼Œå¯æ„é€ é«˜æ¬¡ç‰¹å¾ã€‚ æŸå¤±å‡½æ•°å®šä¹‰è¯¯å·®e^{(i)} = \hat{y}^{(i)} - y^{(i)}å…¶ä¸­$y^{(i)}$è¡¨ç¤ºçœŸå®å€¼ å®šä¹‰æŸå¤±å‡½æ•°å•ä¸ªæ ·æœ¬çš„è¯¯å·®å®šä¹‰ä¸º L_{single}(\hat{y}^{(i)}, y^{(i)})=\frac{1}{2}||e^{(i)}||_2^2=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2æ‰€æœ‰æ ·æœ¬çš„è¯¯å·®å®šä¹‰ä¸º L(y, t)=\frac{1}{2N}\sum_i (\hat{y}^{(i)}-y^{(i)})^2ä¹Ÿå¯ä»¥å®šä¹‰ä¸ºè¯¯å·®çš„å’Œè€Œä¸æ˜¯å‡å€¼ï¼Œå¯¹ç»“æœæ— å½±å“ï¼Œå¯è§†ä½œå­¦ä¹ ç‡$Î±$é™¤å»ä¸€ä¸ªå¸¸æ•° æ¢¯åº¦æ¨å¯¼ $\frac{âˆ‚L}{âˆ‚w_j}$$= \frac{âˆ‚}{âˆ‚w_j}\frac{1}{2N}\sum_i(\hat{y}^{(i)}-y^{(i)})^2$$= \frac{1}{2N} \sum_i \frac{âˆ‚}{âˆ‚w_j} (\hat{y}^{(i)}-y^{(i)})^2$$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) \frac{âˆ‚t^{(i)}}{âˆ‚w_j}$$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)}$ æˆ–è€…ä½¿ç”¨çŸ©é˜µæ¨å¯¼ï¼Œè®°$X = [x_1, x_2, â€¦, x_m]^T$ï¼Œ$x_i$ä¸ºæ ·æœ¬ç‰¹å¾(åˆ—å‘é‡) L = \frac{1}{2}(Xw-Y)^T(Xw-Y) âˆ‡_w L = X^T(\hat{Y}-Y) $âˆ‡_w L$$= \frac{1}{2} âˆ‡_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY)$$= \frac{1}{2} (2X^TXw - X^TY - X^TY)$$= X^T(Xw-Y) $ åœ¨æ¢¯åº¦ä¸º$\vec{0}$çš„ç‚¹ï¼Œå³$âˆ‡_w L = \vec{0}$æ—¶å¯¹åº”æœ€ä¼˜è§£ X^T(Xw-Y) = 0 ä»¤X^T(Xw-Y) = 0 æœ‰X^TXw = X^TY w^*=(X^TX+\lambda I)^{-1}X^TY å…¶ä¸­$X^+=(X^TX+\lambda I)^{-1}X^T$ï¼Œè¡¨ç¤ºçŸ©é˜µ$X_{mÃ—n}$çš„ä¼ªé€† è®­ç»ƒé‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£ w := w - \alpha âˆ‡_w Lå…¶ä¸­$w$è¡¨ç¤ºå‚æ•°å‘é‡ è¿›ä¸€æ­¥æ€è€ƒï¼šä¸ºä»€ä¹ˆä½¿ç”¨æ¢¯åº¦ä¸‹é™å¯ä»¥æ±‚å–æœ€ä¼˜è§£å‘¢ï¼Ÿ âˆ‡_w^2 L = âˆ‡_w X^T(Xw-Y) = X^TXè€Œå¯¹äºçŸ©é˜µ $ X^TX $ u^T(X^TX)u = (Xu)^T(Xu) \geq 0å³æŸå¤±å‡½æ•°çš„HessiançŸ©é˜µ$âˆ‡_w^2 L$ä¸ºæ­£å®šçŸ©é˜µï¼Œ$L$ä¸ºå‡¸å‡½æ•°ï¼Œå­˜åœ¨å…¨å±€æœ€ä¼˜è§£ ä»æŠ•å½±çš„è§’åº¦ç†è§£çº¿æ€§å›å½’ çº¿æ€§å›å½’çš„æ­£åˆ™åŒ–ä¸ºå…‹æœè¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¯åŠ å…¥æ­£åˆ™åŒ–é¡¹$||w||_2^2$ï¼Œæ­¤æ—¶æŸå¤±å‡½æ•°å®šä¹‰ä¸º L(\hat{y}, y)=\frac{1}{2N} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_2^2æˆ–è€… L(\hat{y}, y)=\frac{1}{2N} \sum_i (\hat{y}^{(i)}-y^{(i)})^2 + \frac{\lambda}{2N}\sum_j w_j^2å…¶ä¸­$i = 1, â€¦, N_{sample}; j = 1, â€¦, N_{feature},j&gt;0 $ æ­¤æ—¶æ¢¯åº¦ä¸º \frac{âˆ‚L}{âˆ‚w_j} = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \frac{\lambda}{N}w_jå…¶ä¸­$j = 1, â€¦, N_{feature},j&gt;0 $ å±€éƒ¨åŠ æƒçº¿æ€§å›å½’ç›®æ ‡å‡½æ•°å®šä¹‰ä¸º L(y, t)=\frac{1}{2N}\sum_i w^{(i)} (\hat{y}^{(i)}-y^{(i)})^2å…¶ä¸­ w^{(i)} = e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}$x$è¡¨ç¤ºè¾“å…¥çš„é¢„æµ‹æ ·æœ¬ï¼Œ$x^{(i)}$è¡¨ç¤ºè®­ç»ƒæ ·æœ¬ ç¦»å¾ˆè¿‘çš„æ ·æœ¬ï¼Œæƒå€¼æ¥è¿‘äº1ï¼Œè€Œå¯¹äºç¦»å¾ˆè¿œçš„æ ·æœ¬ï¼Œæ­¤æ—¶æƒå€¼æ¥è¿‘äº0ï¼Œè¿™æ ·å°±æ˜¯åœ¨å±€éƒ¨æ„æˆçº¿æ€§å›å½’ï¼Œå®ƒä¾èµ–çš„ä¹Ÿåªæ˜¯å‘¨è¾¹çš„ç‚¹ã€‚ å¯¹äºçº¿æ€§å›å½’ç®—æ³•ï¼Œä¸€æ—¦æ‹Ÿåˆå‡ºé€‚åˆè®­ç»ƒæ•°æ®çš„å‚æ•°$w$ï¼Œä¿å­˜è¿™äº›å‚æ•°$w$ï¼Œå¯¹äºä¹‹åçš„é¢„æµ‹ï¼Œä¸éœ€è¦å†ä½¿ç”¨åŸå§‹è®­ç»ƒæ•°æ®é›†ï¼Œæ‰€ä»¥æ˜¯å‚æ•°å­¦ä¹ ç®—æ³•ã€‚è€Œå¯¹äºå±€éƒ¨åŠ æƒçº¿æ€§å›å½’ç®—æ³•ï¼Œæ¯æ¬¡è¿›è¡Œé¢„æµ‹éƒ½éœ€è¦å…¨éƒ¨çš„è®­ç»ƒæ•°æ®ï¼ˆæ¯æ¬¡è¿›è¡Œçš„é¢„æµ‹å¾—åˆ°ä¸åŒçš„å‚æ•°$w$ï¼‰ï¼Œæ²¡æœ‰å›ºå®šçš„å‚æ•°$w$ï¼Œæ‰€ä»¥æ˜¯éå‚æ•°ç®—æ³•ã€‚ ä»£ç @Github: Code for Linear Regression training step12345678910111213141516171819202122232425262728293031323334353637383940414243def fit(self, X, y, learning_rate=0.01, max_iter=5000, min_loss=10): # --------------- æ•°æ®é¢„å¤„ç†éƒ¨åˆ† --------------- # åŠ å…¥å…¨1åˆ— X = np.c_[np.ones(shape=(X.shape[0])), X] # æ„é€ é«˜æ¬¡ç‰¹å¾ if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] # ---------------- å‚æ•°è¿­ä»£éƒ¨åˆ† ---------------- # åˆå§‹åŒ–å‚æ•° self.theta = np.random.uniform(-1, 1, size=(X.shape[1],)) # æ•°æ®æ‰¹æ¬¡ n_batch = X.shape[0] if self.n_batch==-1 else self.n_batch batch_size = X.shape[0] // n_batch # åœæ­¢æ¡ä»¶ n_iter = 0; loss = float(&apos;inf&apos;) # å¼€å§‹è¿­ä»£ for n_iter in range(max_iter): for n in range(n_batch): n1, n2 = n*batch_size, (n+1)*batch_size X_batch = X[n1: n2]; y_batch = y[n1: n2] grad = self.lossFunctionDerivative(X_batch, y_batch) self.theta -= learning_rate * grad loss = self.score(y_batch, self.predict(X_batch)) if loss &lt; min_loss: print(&apos;ç¬¬%dæ¬¡è¿­ä»£, ç¬¬%dæ‰¹æ•°æ®&apos; % (n_iter, n)) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬æŸå¤±ä¸º: &quot;, loss) return self.theta if n_iter%100 == 0: print(&apos;ç¬¬%dæ¬¡è¿­ä»£&apos; % n_iter) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬æŸå¤±ä¸º: &quot;, loss) print(&quot;è¶…è¿‡è¿­ä»£æ¬¡æ•°&quot;) print(&quot;å½“å‰æ€»ä½“æ ·æœ¬æŸå¤±ä¸º: &quot;, loss) return self.thetadef lossFunctionDerivative(self, X, y): y_pred = self.predict(X) # theta = self.theta; # ï¼æ³¨æ„ï¼štheta = self.theta ä¸ä»…ä»…æ˜¯èµ‹å€¼ï¼Œç±»ä¼¼å¼•ç”¨ï¼Œä¿®æ”¹thetaä¼šå½±å“self.theta theta = self.theta.copy() theta[0] = 0 # Î¸0ä¸éœ€è¦æ­£åˆ™åŒ– return (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[0] predict step123456789def predict(self, X, preprocessed=False): if preprocessed: # åŠ å…¥å…¨1åˆ— X = np.c_[np.ones(shape=(X.shape[0])), X] # æ„é€ é«˜æ¬¡ç‰¹å¾ if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] return X.dot(self.theta) è¿è¡Œç»“æœ æ— æ­£åˆ™åŒ– æ­£åˆ™åŒ–]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
</search>
