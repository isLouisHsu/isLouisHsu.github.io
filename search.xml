<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ubuntu编译安装Tensorflow]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow%2F</url>
    <content type="text"><![CDATA[非常重要如果中途出现错误，xxxx文件找不到，不要怀疑！就是大天朝的网络问题！推荐科学上网！ 安装CUDA与CUDNN首先查看显卡是否支持CUDA加速，输入1$ nvidia-smi 在Ubuntu16.04 LTS下，推荐安装CUDA9.0和CUDNN 7。 CUDA CUDA Toolkit 9.0 Downloads | NVIDIA Developer https://developer.nvidia.com/cuda-90-download-archive 下载.run版本，安装方法如下 12$ sudo chmod +x cuda_9.0.176_384.81_linux.run $ sudo sh ./cuda_9.0.176_384.81_linux.run 服务条款很长。。。。 CUDNN NVIDIA cuDNN | NVIDIA Developer https://developer.nvidia.com/cudnn 1234$ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 安装后进行验证 1234$ cp -r /usr/src/cudnn_samples_v7/ $HOME$ cd $HOME/cudnn_samples_v7/mnistCUDNN$ make clean &amp;&amp; make$ ./mnistCUDNN 编译Tensorflow(CPU version)由于训练代码使用Python实现，故C++版本的Tensorflow不使用GPU，仅实现预测代码即可。 bazel Installing Bazel on Ubuntu - Bazel https://docs.bazel.build/versions/master/install-ubuntu.html一定要用源码安装！！！ download the Bazel binary installer named bazel-&lt;version&gt;-installer-linux-x86_64.sh from the Bazel releases page on GitHub. 123456$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python$ chmod +x bazel-&lt;version&gt;-installer-linux-x86_64.sh$ ./bazel-&lt;version&gt;-installer-linux-x86_64.sh --user$ sudo nano ~/.bashrc # export PATH=&quot;$PATH:$HOME/bin&quot;$ source ~/.bashrc $ bazel version 编译CPU版本的CPU查看java版本1234$ java -versionopenjdk version &quot;1.8.0_191&quot;OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) 安装依赖软件包环境1234$ sudo apt install python3-dev$ pip3 install six$ pip3 install numpy$ pip3 instal wheel 下载Tensorflow源码1$ git clone https://github.com/tensorflow/tensorflow 编译与安装12$ cd tensorflow$ ./configure 配置选项如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command &quot;bazel shutdown&quot;.INFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5You have bazel 0.20.0 installed.Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3Found possible Python library paths: /usr/local/lib/python3.5/dist-packages /usr/lib/python3/dist-packagesPlease input the desired Python library path to use. Default is [/usr/local/lib/python3.5/dist-packages]Do you wish to build TensorFlow with XLA JIT support? [Y/n]: nNo XLA JIT support will be enabled for TensorFlow.Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: nNo OpenCL SYCL support will be enabled for TensorFlow.Do you wish to build TensorFlow with ROCm support? [y/N]: nNo ROCm support will be enabled for TensorFlow.Do you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Do you wish to download a fresh release of clang? (Experimental) [y/N]: nClang will not be downloaded.Do you wish to build TensorFlow with MPI support? [y/N]: nNo MPI support will be enabled for TensorFlow.Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is -march=native -Wno-sign-compare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Preconfigured Bazel build configs. You can use any of the below by adding &quot;--config=&lt;&gt;&quot; to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=monolithic # Config for mostly static monolithic build. --config=gdr # Build with GDR support. --config=verbs # Build with libverbs support. --config=ngraph # Build with Intel nGraph support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.Preconfigured Bazel build configs to DISABLE default on features: --config=noaws # Disable AWS S3 filesystem support. --config=nogcp # Disable GCP support. --config=nohdfs # Disable HDFS support. --config=noignite # Disable Apacha Ignite support. --config=nokafka # Disable Apache Kafka support. --config=nonccl # Disable NVIDIA NCCL support.Configuration finished 使用bazel编译1$ bazel build --config=opt //tensorflow:libtensorflow_cc.so 出现错误 TF failing to build on Bazel CI · Issue #19464 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/19464Failure to build TF 1.12 from source - multiple definitions in grpc · Issue #23402 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5 解决方法 方法1 方法2 将tools/bazel.rc中内容粘到.tf_configure.bazelrc中，每次重新配置后需要重新粘贴一次。 源码安装protobuf3.6.0 https://github.com/protocolbuffers/protobuf 1234./autogen.sh./configuremakemake install 下载其他文件 12$ ./tensorflow/contrib/makefile/download_dependencies.shmkdir /tmp/eigen 值得注意，download_dependencies.sh中下载依赖包时，需要用到curl，但是默认方式安装 1$ sudo apt install curl &gt; 现在是2018/12/19/02:48，被这个问题折腾了3个小时。 时不支持`https`协议，故需要安装`OpenSSL`，并源码安装，详细资料见[curl提示不支持https协议解决方法 - 标配的小号 - 博客园](https://www.cnblogs.com/biaopei/p/8669810.html) - 执行`./autogen.sh`时，发生错误`autoreconf: not found`，则安装 12$ sudo apt install autoconf aotomake libtool$ sudo apt install libffi-dev 源码安装Eigen 12345cd tensorflow/contrib/makefile/Downloads/eigenmkdir buildcd buildcmakemake install 调用C++版本的Tensorflow创建文件目录如下1234|-- tf_test |-- build |-- main.cpp |-- CMakeLists.txt main.cpp文件内容如下1234567891011121314151617181920212223#include &quot;tensorflow/cc/client/client_session.h&quot;#include &quot;tensorflow/cc/ops/standard_ops.h&quot;#include &quot;tensorflow/core/framework/tensor.h&quot;int main() &#123; using namespace tensorflow; using namespace tensorflow::ops; Scope root = Scope::NewRootScope(); // Matrix A = [3 2; -1 0] auto A = Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125;&#125;); // Vector b = [3 5] auto b = Const(root, &#123; &#123;3.f, 5.f&#125;&#125;); // v = Ab^T auto v = MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true)); std::vector&lt;Tensor&gt; outputs; ClientSession session(root); // Run and fetch v TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs)); // Expect outputs[0] == [19; -3] LOG(INFO) &lt;&lt; outputs[0].matrix&lt;float&gt;(); return 0;&#125; CMakeLists.txt内容如下12345678910111213141516171819202122232425262728293031cmake_minimum_required (VERSION 2.8.8)project (tf_example)set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -g -std=c++11 -W&quot;)set(EIGEN_DIR /usr/local/include/eigen3)set(PROTOBUF_DIR /usr/local/include/google/protobuf)set(TENSORFLOW_DIR /home/louishsu/install/tensorflow-1.12.0)include_directories( $&#123;EIGEN_DIR&#125; $&#123;PROTOBUF_DIR&#125; $&#123;TENSORFLOW_DIR&#125; $&#123;TENSORFLOW_DIR&#125;/bazel-genfiles $&#123;TENSORFLOW_DIR&#125;/tensorflow/contrib/makefile/downloads/absl)link_directories( /usr/local/lib)add_executable( tf_test main.cpp)target_link_libraries( tf_test tensorflow_cc tensorflow_framework) 123$ mkdir build &amp;&amp; cd build$ cmake .. &amp;&amp; make$ ./tf_test install tensorflow-gpu for python可使用pip指令安装，推荐下载安装包， tensorflow · PyPI https://pypi.org/project/tensorflow/ 12$ cd ~/Downloads$ pip3 --default-timeout=1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user 安装后进行验证123456789101112131415161718192021222324252627$ python3Python 3.5.2 (default, Nov 12 2018, 13:43:14) [GCC 5.4.0 20160609] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; sess = tf.Session()2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758pciBusID: 0000:04:00.0totalMemory: 983.44MiB freeMemory: 177.19MiB2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 02018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)&gt;&gt;&gt; a = tf.Variable([233])&gt;&gt;&gt; init = tf.initialize_all_variables()WARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.Instructions for updating:Use `tf.global_variables_initializer` instead.&gt;&gt;&gt; sess.run(init)&gt;&gt;&gt; sess.run(a)array([233], dtype=int32)&gt;&gt;&gt; sess.close() 注意，如果异常中断程序，显存不会被释放，需要自行kill1$ nvidia-smi 获得PID序号，使用指令结束进程1$ kill -9 pid Reference TensorFlow C++动态库编译 - 简书 https://www.jianshu.com/p/d46596558640Tensorflow C++ 从训练到部署(1)：环境搭建 | 技术刘 http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/]]></content>
      <categories>
        <category>Linux</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu编译安装OpenCV]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV%2F</url>
    <content type="text"><![CDATA[下载源码 OpenCV library https://opencv.org/ 编译安装依赖软件包12$ sudo apt install cmake$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev 编译12345$ unzip opencv-3.4.4.zip$ cd opencv-3.4.4$ mkdir build &amp;&amp; cd build$ cmake ..$ make -j4 安装123$ sudo make install$ sudo nano /etc/ld.so.conf.d/opencv.conf # add `/usr/local/lib`$ sudo ldconfig 验证OpenCV自带验证程序，在opencv-3.4.4/samples/cpp/example_cmake中可以找到 1234$ cd opencv-3.4.4/samples/cpp/example_cmake$ cmake .$ make$ ./opencv_example 如果没问题，可以看到你的大脸了~ Reference Ubuntu16.04安装openCV3.4.4 - 辣屁小心的学习笔记 - CSDN博客 https://blog.csdn.net/weixin_39992397/article/details/84345197]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python读写配置文件]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[在深度学习中，有许多运行参数需要指定，有几种方法可以解决 定义.py文件存储变量 定义命名元组collections.namedtuple() 创建.config，.ini等配置文件 Python 读取写入配置文件很方便，使用内置模块configparser即可 读出首先创建文件test.config或test.ini，写入如下内容123456789[db]db_port = 3306db_user = rootdb_host = 127.0.0.1db_pass = test[concurrent]processor = 20thread = 10 读取操作如下1234567891011121314151617181920212223242526272829&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; configfile = &quot;./test.config&quot;&gt;&gt;&gt; inifile = &quot;./test.ini&quot;&gt;&gt;&gt; &gt;&gt;&gt; cf = configparser.ConfigParser()&gt;&gt;&gt; cf.read(configfile) # 读取文件内容&gt;&gt;&gt; &gt;&gt;&gt; sections = cf.sections() # 所有的section，以列表的形式返回&gt;&gt;&gt; sections[&apos;db&apos;, &apos;concurrent&apos;]&gt;&gt;&gt; &gt;&gt;&gt; options = cf.options(&apos;db&apos;) # 该section的所有option&gt;&gt;&gt; options[&apos;db_port&apos;, &apos;db_user&apos;, &apos;db_host&apos;, &apos;db_pass&apos;]&gt;&gt;&gt; &gt;&gt;&gt; items = cf.items(&apos;db&apos;) # 该section的所有键值对&gt;&gt;&gt; items[(&apos;db_port&apos;, &apos;3306&apos;), (&apos;db_user&apos;, &apos;root&apos;), (&apos;db_host&apos;, &apos;127.0.0.1&apos;), (&apos;db_pass&apos;, &apos;test&apos;)]&gt;&gt;&gt; &gt;&gt;&gt; db_user = cf.get(&apos;db&apos;, &apos;db_user&apos;) # section中option的值，返回为string类型&gt;&gt;&gt; db_user&apos;root&apos;&gt;&gt;&gt; &gt;&gt;&gt; db_port = cf.getint(&apos;db&apos;, &apos;db_port&apos;) # 得到section中option的值，返回为int类型&gt;&gt;&gt; # 类似的还有getboolean()与getfloat()&gt;&gt;&gt; db_port3306 写入12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; cf = configparser.ConfigParser()&gt;&gt;&gt; cf.add_section(&apos;test1&apos;) # 新增section&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) # 新增option：错误示范Traceback (most recent call last): File &quot;&lt;pyshell#7&gt;&quot;, line 1, in &lt;module&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1192, in set self._validate_value_types(option=option, value=value) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1177, in _validate_value_types raise TypeError(&quot;option values must be strings&quot;)TypeError: option values must be strings&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, &apos;1&apos;) # 新增option&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test1&quot;, &quot;opt1&quot;, &apos;ok&apos;) # 新增option&gt;&gt;&gt; cf.remove_option(&quot;test1&quot;, &quot;opt1&quot;) # 删除optionTrue&gt;&gt;&gt; &gt;&gt;&gt; cf.add_section(&apos;test2&apos;) # 新增section&gt;&gt;&gt; cf.remove_section(&apos;test2&apos;) # 删除sectionTrue&gt;&gt;&gt; &gt;&gt;&gt; with open(&quot;./test_wr.config&quot;, &apos;w+&apos;) as f: cf.write(f) # 写入文件test_wr.config &gt;&gt;&gt; 现在目录已创建文件test_wr.config，打开可以看到12[test1]count = 1]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python更新安装的包]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%2F</url>
    <content type="text"><![CDATA[pip不提供升级全部已安装模块的方法，以下指令可查看更新信息1$ pip list --outdate 得到输出信息如下123456789101112131415161718192021222324Package Version Latest Type----------------- --------- ---------- -----absl-py 0.3.0 0.6.1 sdistautopep8 1.3.5 1.4.2 sdistbleach 2.1.4 3.0.2 wheelcertifi 2018.8.24 2018.10.15 wheeldask 0.20.0 0.20.1 wheelgrpcio 1.14.1 1.16.0 wheelipykernel 5.0.0 5.1.0 wheelipython 7.0.1 7.1.1 wheeljedi 0.12.1 0.13.1 wheeljupyter-console 5.2.0 6.0.0 wheelMarkdown 2.6.11 3.0.1 wheelMarkupSafe 1.0 1.1.0 wheelmatplotlib 2.2.2 3.0.2 wheelmistune 0.8.3 0.8.4 wheelnumpy 1.14.5 1.15.4 wheelopencv-python 3.4.2.17 3.4.3.18 wheelPillow 5.2.0 5.3.0 wheelprometheus-client 0.3.1 0.4.2 sdistpyparsing 2.2.0 2.3.0 wheelpython-dateutil 2.7.3 2.7.5 wheelpytz 2018.5 2018.7 wheelurllib3 1.23 1.24.1 wheel 以下提供一键升级的方法，可能比较久hhhh12345678from pip._internal.utils.misc import get_installed_distributionsfrom subprocess import call for dist in get_installed_distributions(): modulename = dist.project_name print(&apos;start processing module &apos; + modulename) call(&quot;pip install --upgrade &quot; + modulename, shell=True) print(&apos;module &apos; + modulename + &apos;done!&apos;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python记录日志]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[前言日志可以用来记录应用程序的状态、错误和信息消息，也经常作为调试程序的工具。Python提供了一个标准的日志接口，就是logging模块。日志级别有DEBUG、INFO、WARNING、ERROR、CRITICAL五种。 logging — Logging facility for Python — Python 3.7.1 documentation 使用方法logger对象1234&gt;&gt;&gt; import logging&gt;&gt;&gt; logger = logging.getLogger(__name__)&gt;&gt;&gt; logger&lt;Logger __main__ (WARNING)&gt; 日志级别可输出五种不同的日志级别，分别为有DEBUG、INFO、WARNING、ERROR、CRITICAL12345678&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&gt;&gt;&gt; logger.info(&apos;test log&apos;)&gt;&gt;&gt; logger.warning(&apos;test log&apos;)test log&gt;&gt;&gt; logger.error(&apos;test log&apos;)test log&gt;&gt;&gt; logger.critical(&apos;test log&apos;)test log 可以看到只有WARNING及以上级别日志被输出，这是由于默认的日志级别是WARNING ，所以低于此级别的日志不会记录。 基础配置1logging.basicConfig(**kwarg) **kwarg中部分参数如下 format 12345678910%(levelname)：日志级别的名字格式%(levelno)s：日志级别的数字表示%(name)s：日志名字%(funcName)s：函数名字%(asctime)：日志时间，可以使用datefmt去定义时间格式，如上图。%(pathname)：脚本的绝对路径%(filename)：脚本的名字%(module)：模块的名字%(thread)：thread id%(threadName)：线程的名字 datefmt 1&apos;%Y-%m-%d %H:%M:%S&apos; level 默认为ERROR 12345logging.DEBUGlogging.INFOlogging.WARNINGlogging.ERRORlogging.CRITICAL 例如12345678910111213141516&gt;&gt;&gt; # 未输出debug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&gt;&gt;&gt; &gt;&gt;&gt; # 修改配置&gt;&gt;&gt; log_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;&gt;&gt;&gt; log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;&gt;&gt;&gt; log_level = logging.DEBUG&gt;&gt;&gt; logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level)&gt;&gt;&gt; &gt;&gt;&gt; # 输出debug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug(&apos;test log&apos;)&lt;pyshell#8&gt; [2018-11-13 11:59:52] [DEBUG] test log 输出到日志文件保存代码为文件log_test.py1234567891011121314151617181920import logginglog_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;log_level = logging.DEBUGlog_filename = &apos;./test.log&apos;log_filemode = &apos;a&apos; # 也可以为&apos;w&apos;, &apos;w+&apos;等logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level, filename=log_filename, filemode=log_filemode)logger = logging.getLogger(__name__)logger.debug(&apos;test log&apos;)logger.info(&apos;test log&apos;)logger.warning(&apos;test log&apos;)logger.error(&apos;test log&apos;)logger.critical(&apos;test log&apos;) 运行完毕，打开log_test.log文件可以看到12345log_test.py [2018-11-13 12:11:04] [DEBUG] test loglog_test.py [2018-11-13 12:11:04] [INFO] test loglog_test.py [2018-11-13 12:11:04] [WARNING] test loglog_test.py [2018-11-13 12:11:04] [ERROR] test loglog_test.py [2018-11-13 12:11:04] [CRITICAL] test log]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github博客搭建]]></title>
    <url>%2F2019%2F01%2F04%2FGithub-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言那么问题来了，现有的博客还是现有的这篇文章呢？ 软件安装安装node.js, git, hexo 博客搭建初始化推荐使用git命令窗口，执行如下指令12345678910111213141516171819202122232425262728293031$ mkdir Blog$ cd Blog$ hexo initINFO Cloning hexo-starter to ~\Desktop\BlogCloning into &apos;C:\Users\LouisHsu\Desktop\Blog&apos;...remote: Enumerating objects: 68, done.remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68Unpacking objects: 100% (68/68), done.Submodule &apos;themes/landscape&apos; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &apos;themes/landscape&apos;Cloning into &apos;C:/Users/LouisHsu/Desktop/Blog/themes/landscape&apos;...remote: Enumerating objects: 1, done.remote: Counting objects: 100% (1/1), done.remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866Receiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.Resolving deltas: 100% (459/459), done.Submodule path &apos;themes/landscape&apos;: checked out &apos;73a23c51f8487cfcd7c6deec96ccc7543960d350&apos;[32mINFO [39m Install dependenciesnpm WARN deprecated titlecase@1.1.2: no longer maintainednpm WARN deprecated postinstall-build@5.0.3: postinstall-build&apos;s behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.&gt; nunjucks@3.1.6 postinstall C:\Users\LouisHsu\Desktop\Blog\node_modules\nunjucks&gt; node postinstall-build.js srcnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)added 422 packages from 501 contributors and audited 4700 packages in 59.195sfound 0 vulnerabilitiesINFO Start blogging with Hexo! 生成目录结构如下123456\-- scaffolds\-- source \-- _posts\-- themes|-- _config.yml|-- package.json 继续123456$ npm installnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)audited 4700 packages in 5.99sfound 0 vulnerabilities 现在该目录执行指令，开启hexo服务器123$ hexo sINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 生成目录和标签1234$ hexo n page about$ hexo n page archives$ hexo n page categories$ hexo n page tags 修改/source/tags/index.md，其他同理1234567891011121301| ---02| title: tags03| date: 2019-01-04 17:34:1504| ----&gt;01| ---02| title: tags03| date: 2019-01-04 17:34:1504| type: &quot;tags&quot;05| comments: false06| --- 关联Github在Github新建一个仓库，命名为username.github.io，例如isLouisHsu.github.io，新建时勾选Initialize this repository with a README，因为这个仓库必须不能为空。 打开博客目录下的_config.yml配置文件，定位到最后的deploy选项，修改如下1234deploy: type: git repository: git@github.com:isLouisHsu/isLouisHsu.github.io.git branch: master 安装插件1$ npm install hexo-deployer-git --save 现在就可以将该目录内容推送到Github新建的仓库中了1$ hexo d 使用个人域名 在source目录下新建文件CNAME，输入解析后的个人域名 在Github主页修改域名 备份博客 没。没什么用我。我不备份了可以新建一个仓库专门保存文件试试 现在博客的源文件仅保存在PC上， 我们对它们进行备份，并将仓库作为博客文件夹 在仓库新建分支hexo，设置为默认分支 将仓库克隆至本地 1$ git clone https://github.com/isLouisHsu/isLouisHsu.github.io.git 克隆文件 将之前的Hexo文件夹中的 123456scffolds/source/themes/.gitignore_config.ymlpackage.json 复制到克隆下来的仓库文件夹isLouisHsu.github.io 安装包 123$ npm install$ npm install hexo --save$ npm install hexo-deployer-git --save 备份博客使用以下指令 123$ git add .$ git commit -m &quot;backup&quot;$ git push origin hexo 部署博客指令 1$ hexo g -d 单键提交 编写脚本commit.bat，双击即可 1234git add .git commit -m &apos;backup&apos;git push origin hexohexo g -d 使用方法 目录结构 public 生成的网站文件，发布的站点文件。 source 资源文件夹，用于存放内容。 tag 标签文件夹。 archive 归档文件夹。 category分类文件夹。 downloads/code include code文件夹。 :lang i18n_dir 国际化文件夹。 _config.yml 配置文件 指令 123456789101112131415161718192021222324252627$ hexo helpUsage: hexo &lt;command&gt;Commands: clean Remove generated files and cache. config Get or set configurations. deploy Deploy your website. generate Generate static files. help Get help on a command. init Create a new Hexo folder. list List the information of the site migrate Migrate your site from other system to Hexo. new Create a new post. publish Moves a draft post from _drafts to _posts folder. render Render files with renderer plugins. server Start the server. version Display version information.Global Options: --config Specify config file instead of using _config.yml --cwd Specify the CWD --debug Display all verbose messages in the terminal --draft Display draft posts --safe Disable all plugins and scripts --silent Hide output on consoleFor more help, you can use &apos;hexo help [command]&apos; for the detailed information or you can check the docs: http://hexo.io/docs/ 拓展功能支持插入图片1$ npm install hexo-asset-image --save 修改文件_config.yml1post_asset_folder: true 在执行$ hexo n [layout] &lt;title&gt;时会生成同名文件夹，把图片放在这个文件夹内，在.md文件中插入图片1![image_name](/title/image_name.png) 搜索功能12$ npm install hexo-generator-searchdb --save$ npm install hexo-generator-search --save 站点配置文件_config.yml中添加12345search: path: search.xml field: post format: html limit: 10000 修改主题配置文件/themes/xxx/_config.yml12local_search: enable: true 带过滤功能的首页插件在首页只显示指定分类下面的文章列表。12$ npm install hexo-generator-index2 --save$ npm uninstall hexo-generator-index --save 修改_config.yml1234567index_generator: per_page: 10 order_by: -date include: - category Web # 只包含Web分类下的文章 exclude: - tag Hexo # 不包含标签为Hexo的文章 数学公式支持hexo默认的渲染引擎是marked，但是marked不支持mathjax。kramed是在marked的基础上进行修改。1234$ npm uninstall hexo-math --save # 停止使用 hexo-math$ npm install hexo-renderer-mathjax --save # 安装hexo-renderer-mathjax包：$ npm uninstall hexo-renderer-marked --save # 卸载原来的渲染引擎$ npm install hexo-renderer-kramed --save # 安装新的渲染引擎 修改/node_modules/kramed/lib/rules/inline.js12345678911| escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,...20| em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,-&gt;11| escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,...20| em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 修改/node_modules/hexo-renderer-kramed/lib/renderer.js123456789101112131464| // Change inline math rule65| function formatText(text) &#123;66| // Fit kramed&apos;s rule: $$ + \1 + $$67| return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);68| &#125;-&gt;64| // Change inline math rule65| function formatText(text) &#123;66| // Fit kramed&apos;s rule: $$ + \1 + $$67| // return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);68| return text;69| &#125; 在主题中开启mathjax开关，例如next主题中1234# MathJax Supportmathjax: enable: true per_page: true 在文章中12345678---title: title.mddate: 2019-01-04 12:47:37categories:tags:mathjax: truetop:--- 测试 A = \left[\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix}\right]Reference 基于hexo+github搭建一个独立博客 - 牧云云 - 博客园 https://www.cnblogs.com/MuYunyun/p/5927491.htmlhexo+github pages轻松搭博客(1) | ex2tron’s Blog http://ex2tron.wang/hexo-blog-with-github-pages-1/hexo下LaTeX无法显示的解决方案 - crazy_scott的博客 - CSDN博客 https://blog.csdn.net/crazy_scott/article/details/79293576在Hexo中渲染MathJax数学公式 - 简书 https://www.jianshu.com/p/7ab21c7f0674怎么去备份你的Hexo博客 - 简书 https://www.jianshu.com/p/baab04284923Hexo中添加本地图片 - 蜕变C - 博客园 https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referralhexo 搜索功能 - 阿甘的博客 - CSDN博客 https://blog.csdn.net/ganzhilin520/article/details/79047983]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[EM & GMM]]></title>
    <url>%2F2018%2F11%2F12%2FEM-GMM%2F</url>
    <content type="text"><![CDATA[EM算法Expectation Maximization Algorithm，是 Dempster, Laind, Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 MLE 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据。 引例：先挖个坑给出李航《统计学习方法》的三硬币模型例子，假设有$3$枚硬币$A, B, C$，各自出现正面的概率分别为$\pi, p, q$，先进行如下实验：先投掷硬币$A$，若结果为正面，则选择硬币$B$投掷一次，否则选择$C$，记录投掷结果如下 1, 1, 0, 1, 0, 0, 1, 0, 1, 1只能观测到实验结果，而投掷过程未知，即硬币$A$的投掷结果未知，现欲估计三枚硬币的参数$\pi, p, q$。 解：根据题意可以得到三个随机变量$X_1, X_2, X_3$的概率分布如下 P(X_1) = \pi ^ {X_1} (1 - \pi) ^ {1 - X_1} P(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2} P(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}定义随机变量$X$表示观测结果为正面，由全概率公式可以得到 P(X) = P(X|X_1)P(X_1) + P(X|\overline{X_1})P(\overline{X_1}) = \pi p + (1 - \pi) q P(\overline{X}) = P(\overline{X}|X_1)P(X_1) + P(\overline{X}|\overline{X_1})P(\overline{X_1}) = \pi (1 - p) + (1 - \pi) (1 - q)即 P(X) = [\pi p + (1 - \pi) q] ^ {X} [\pi (1 - p) + (1 - \pi) (1 - q)] ^ {1 - X}利用最大似然估计，有 \log L(D | \theta) = 6 \log [\pi p + (1 - \pi) q] + 4 \log [\pi (1 - p) + (1 - \pi) (1 - q)]至此，我们一定能想到通过求似然函数极值来求解参数 \frac{∂ }{∂ \pi} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{∂ }{∂ p} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{∂ }{∂ q} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0但是好像出了问题，并不能求解，所以我们引入EM算法迭代求解。 推导以$x^{(i)}$表示训练数据，$w_k$表示类别，设当前迭代参数为$\theta^{(t)}$，则下一次迭代应有 \theta^{(t+1)} = \arg \max \sum_i \log P(x^{(i)}|\theta) \tag{1}由边缘概率公式 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \tag{2} $P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)}|x^{(i)}, \theta)$至此已得出引例中的表达式，其中$P(w_k^{(i)}|x^{(i)}, \theta)$与$P(x^{(i)} | w_k^{(i)}, \theta)$均未知，而通过求极值不能解得参数。 我们引入迭代参数$\theta^{(t)}$，即第$t$次迭代时的参数$\theta$，该参数为已知变量 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)} | \theta^{(t)})} {P(w_k^{(i)} | \theta^{(t)})} $P(w_k^{(i)}|\theta^{(t)})$表示样本$x^{(i)}$类别为$w_k^{(i)}$的概率，注意上标。 引入Jensen不等式： For a real convex function $\varphi$, numbers $x_1, …, x_n$ in its domain, and positive weights $a_i$, Jensen’s inequality can be stated as: \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \leq \frac{\sum a_i \varphi(x_i)}{\sum a_i}and the inquality is reversed if $\varphi$ is concave, which is \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \geq \frac{\sum a_i \varphi(x_i)}{\sum a_i}Equality holds if and only if $x_1 = … = x_n$ or $\varphi$ is linear. $\log(·)$为凹函数(concave)，且满足 \sum_k P(w_k^{(i)} | \theta^{(t)}) = 1所以有 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)}|\theta^{(t)})} {P(w_k^{(i)}|\theta^{(t)})} \geq \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{3}此时我们得到似然函数$\sum_i \log P(x^{(i)}|\theta)$的一个下界，但必须保证这个下界是紧的，也就是至少有点能使等号成立 由Jensen不等式，当且仅当$ P(x^{(i)}, w_k^{(i)}|\theta)=C $时取等号 定义 L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) - P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)})其中第一项即期望 E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) \tag{4}第二项为$P(w | X, \theta^{(t)})$的信息熵 H[P(w | X, \theta^{(t)})] = - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5}即 L(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] + H[P(w | X, \theta^{(t)})] \tag{E-step} 注意到$H[P(w | X, \theta^{(t)})]$项为常数，故也可设 Q(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] 代回$(1)$，得到优化目标 \theta^{(t+1)} = \arg \max L(\theta|\theta^{(t)}) \tag{M-step}我们需要不断最大化$L(\theta | \theta^{(t)})$来不断优化，这就是所谓的EM算法，E-step是指求出期望，M-step是指迭代更新参数 伪代码如下123456According to prior knowledge set $\theta$Repeat until convergence&#123; E-step: The expectation of hidden variables M-step: Finding the maximum of likelihood function&#125; 实际上，从边缘概率与条件概率入手 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta) \geq \sum_i \sum_k P(w_k^{(i)} | \theta) \log P(x^{(i)} | w_k^{(i)}, \theta) \tag{Jensen inequality} = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)}而由$(3)$，引入迭代变量可以得到 \sum_i \log P(x^{(i)}|\theta) \geq L(\theta|\theta^{(t)})其中 L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}则 \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta)}而由KL散度( Kullback–Leibler divergence)(又称相对熵(relative entropy))定义 D(P||Q) = \sum P(x) \log \frac{P(x)}{Q(x)} 可知 \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = D\left[ P(w_k^{(i)}|\theta^{(t)}) || P(w_k^{(i)}|\theta) \right]即迭代的$P(w_k^{(i)}|\theta^{(t)})$与真实的$P(w_k^{(i)}|\theta)$之间的相对熵！ 这里关于K-L散度的困扰了$N$久，终于搞出来了。 引例的求解 $Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$ 此题中 P(w_k|\pi) = \pi^{w_k}(1-\pi)^{1-w_k} P(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}} P(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}} $E-step$ Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) \log P(x^{(i)}, w_k^{(i)} | \pi, p, q) 先求$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$，即第一次投掷结果为$w_k$的概率 P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_k}} {\sum_j \left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_j} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_j}} 即 \begin{cases} P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\ P(w_2^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \end{cases} 记 \mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})\mu_2^{(i)} = 1 - \mu_1^{(i)} 注意$w^{(i)}_k$上标^{(i)} 再求$P(x^{(i)}, w_k^{(i)} | \pi, p, q)$，已知 P(x^{(i)}, w_k^{(i)} | \pi, p, q) = P(x^{(i)} | w_k^{(i)}, \pi, p, q) P(w_k^{(i)} | \pi, p, q) 所以 P(x^{(i)}, w_k^{(i)} | \pi, p,q) = \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} 综上 Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) = \sum_i \sum_{k=1}^2 \mu^{(i)}_k \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} = \sum_i \mu_1^{(i)} \log \pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \mu_1^{(i)}) \log (1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}} $M-step$ $\frac{∂Q}{∂\pi} = 0$ \frac{∂Q}{∂\pi} = \sum_i \mu_1^{(i)} \frac {p^{x^{(i)}}(1-p)^{1-x^{(i)}}} {\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} + (1 - \mu_1^{(i)}) \frac {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}} {(1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}} = \sum_i \frac{\mu_1^{(i)}}{\pi} + \frac{\mu_1^{(i)} - 1}{1 - \pi} = \sum_i \frac{\mu_1^{(i)} - \pi}{\pi(1 - \pi)} = \frac{\sum_i \mu_1^{(i)} - n\pi}{\pi(1 - \pi)} = 0 \Rightarrow \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)} $\frac{∂Q}{∂p} = 0$ \frac{∂Q}{∂p} = \sum_i \mu_1^{(i)} \left[ \frac{x^{(i)}}{p} - \frac{1 - x^{(i)}}{1 - p} \right] = \frac{1}{p(1 - p)} \sum_i \mu_1^{(i)} (x^{(i)} - p) = \frac{1}{p(1 - p)} \left[ \sum_i \mu_1^{(i)} x^{(i)} - p \sum_i \mu_1^{(i)} \right] = 0 \Rightarrow p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}} $\frac{∂Q}{∂q} = 0$ \frac{∂Q}{∂q} = \sum_i (1 - \mu_1^{(i)}) \left[ \frac{x^{(i)}}{q} - \frac{1 - x^{(i)}}{1 - q} \right] = \frac{1}{q(1 - q)} \sum_i (1 - \mu_1^{(i)}) (x^{(i)} - q) = \frac{1}{q(1 - q)} \left[ \sum_i (1 - \mu_1^{(i)}) x^{(i)} - q \sum_i (1 - \mu_1^{(i)}) \right] = 0 \Rightarrow q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})} 多次迭代即可求解，终止条件可设置为 || \theta^{(t+1)} - \theta^{(t)} || < \epsilon或 ||Q(\theta^{(t+1)} | \theta^{(t)}) - Q(\theta^{(t)} |\theta^{(t)})|| < \epsilonGMM模型Gaussian Mixture Model，是一种无监督学习算法，常用于聚类。当聚类问题中各个类别的尺寸不同、聚类间有相关关系的时候，往往使用GMM更合适。对一个样本来说，GMM得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被成为软聚类。一般说来， 任意形状的概率分布都可以用多个高斯分布函数去近似，因而，GMM的应用也比较广泛。 高斯混合模型，指具有如下形式的概率分布模型： P(x|\mu_k, \Sigma_k) = \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)其中 $\pi_k(0 \leq \pi_k \leq 1)$是系数，且$\sum_k \pi_k = 1$ $N(x|\mu_k, \Sigma_k)$为高斯密度函数 N(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) \right] 即多个高斯分布叠加出来的玩意； 现在我们需要求取系数$\pi_k$及高斯模型的参数$(\mu_k, \Sigma_k)$； 与K-Means等聚类方法区别是，GMM求出的是连续的分布模型，可计算出“归属于”哪一类的概率。 推导 \log P(X|\pi, \mu, \Sigma) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \Sigma_k) s.t. \sum_k \pi_k = 1暴力求解以$1$维高斯分布为例 N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}}构造拉格朗日(Lagrange)函数 L(\pi, \mu, \sigma^2) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \sigma_k^2) + \lambda \left(\sum_k \pi_k - 1 \right) \tag{5} \begin{cases} \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\mu_k}N(x^{(i)}|\mu_k, \sigma_k^2) \\ \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\sigma_k^2}N(x^{(i)}|\mu_k, \sigma_k^2) \end{cases} \tag{6}其中 \frac{∂}{∂\mu_k} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{x-\mu_k}{\sigma_k^2} = N(x|\mu_k, \sigma_k^2) · \frac{x-\mu_k}{\sigma_k^2} \frac{∂}{∂\sigma_k^2} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) $\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) = - \frac{\sigma_k^{-3}}{2}; \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) = - \frac{1}{\sigma_k^4}$ = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(- \frac{\sigma_k^{-3}}{2}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \left(- \frac{1}{\sigma_k^4}\right) = N(x|\mu_k, \sigma_k^2) \left[ \frac{(x - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2}代回$(6)$可以得到 \begin{cases} \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{x^{(i)}-\mu_k}{\sigma_k^2} \\ \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2} \end{cases} \tag{7}令 \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \tag{8} 通俗理解：$\gamma^{(i)}_k$表示样本$x^{(i)}$中来自类别$w_k$的“贡献百分比” 令$\frac{∂}{∂\mu_k} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到 \sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) = 0 \Rightarrow \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} 令$\frac{∂}{∂\sigma_k^2} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到 \sum_i \gamma^{(i)}_k \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] = 0 \Rightarrow \sigma_k^2 = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k)^2}{\sum_i \gamma^{(i)}_k} 对于$\frac{∂}{∂\pi_k} \log P(X|\pi, \mu, \sigma^2) = 0$，需要做一点处理 两边同乘$\pi_k$，得到 \sum_i \gamma^{(i)}_k = - \lambda \pi_k \tag{9} 然后两边对$k$作累加 \sum_k \sum_i \gamma^{(i)}_k = - \lambda \sum_k \pi_k $\sum_k \sum_i \gamma^{(i)}_k = \sum_i \sum_k \gamma^{(i)}_k = N, \sum_k \pi_k = 1$ N = - \lambda 或 \lambda = -N \tag{10} 代回$(9)$，得到 \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N} 综上，我们得到$4$个用于迭代的计算式，将其推广至多维即 \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)} \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} \Sigma_k = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)}_k} \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}用EM算法求解 $Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$ Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k) $ M-step $ P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)} = \gamma^{(i)}_k P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k) = P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k) P(w_k^{(i)}|\mu_k, \Sigma_k) = \pi_k N(x^{(i)}|\mu_k, \Sigma_k) 故 Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k \gamma^{(i)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k) 通过求解极值可得到与$\underline{暴力求解}$一样的等式，即 \gamma^{(i)(t)}_k = \frac{\pi^{(t)}_k N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})} \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k} \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k} \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N} 伪代码为 123456789101112According to prior knowledge set \pi^&#123;(t)&#125;(n_clusters,) \mu^&#123;(t)&#125;(n_clusters, n_features) \Sigma^&#123;(t)&#125;(n_clusters, n_features, n_features)Repeat until convergence&#123; # E-step: calculate \gamma^&#123;(t)&#125; \gamma(n_samples, n_clusters) # M-step: update \pi, \mu, \Sigma \pi^&#123;(t+1)&#125;(n_clusters,) \mu^&#123;(t+1)&#125;(n_clusters, n_features) \Sigma^&#123;(t+1)&#125;(n_clusters, n_features, n_features)&#125; 初始点的选择可以随机选择，也可使用K-Means GMM算法收敛过程如下 代码@Github: GMM123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class GMM(): &quot;&quot;&quot; Gaussian Mixture Model Attributes: n_clusters &#123;int&#125; prior &#123;ndarray(n_clusters,)&#125; mu &#123;ndarray(n_clusters, n_features)&#125; sigma &#123;ndarray(n_clusters, n_features, n_features)&#125; &quot;&quot;&quot; def __init__(self, n_clusters): self.n_clusters = n_clusters self.prior = None self.mu = None self.sigma = None def fit(self, X, delta=0.01): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; delta &#123;float&#125; Notes: - Initialize with k-means &quot;&quot;&quot; (n_samples, n_features) = X.shape # initialize with k-means clf = KMeans(n_clusters=self.n_clusters) clf.fit(X) self.mu = clf.cluster_centers_ self.prior = np.zeros(self.n_clusters) self.sigma = np.zeros((self.n_clusters, n_features, n_features)) for k in range(self.n_clusters): X_ = X[clf.labels_==k] self.prior[k] = X_.shape[0] / X_.shape[0] self.sigma[k] = np.cov(X_.T) while True: mu_ = self.mu.copy() # E-step: updata gamma gamma = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): denominator = 0 for j in range(self.n_clusters): post = self.prior[k] *\ multiGaussian(X[i], self.mu[j], self.sigma[j]) denominator += post if j==k: numerator = post gamma[i, k] = numerator/denominator # M-step: updata prior, mu, sigma for k in range(self.n_clusters): sum1 = 0 sum2 = 0 sum3 = 0 for i in range(n_samples): sum1 += gamma[i, k] sum2 += gamma[i, k] * X[i] x_ = np.reshape(X[i] - self.mu[k], (n_features, 1)) sum3 += gamma[i, k] * x_.dot(x_.T) self.prior[k] = sum1 / n_samples self.mu[k] = sum2 / sum1 self.sigma[k] = sum3 / sum1 # to stop mu_delta = 0 for k in range(self.n_clusters): mu_delta += nl.norm(self.mu[k] - mu_[k]) print(mu_delta) if mu_delta &lt; delta: break return self.prior, self.mu, self.sigma def predict_proba(self, X): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples, n_clusters)&#125; &quot;&quot;&quot; (n_samples, n_features) = X.shape y_pred_proba = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): y_pred_proba[i, k] = self.prior[k] *\ multiGaussian(X[i], self.mu[k], self.sigma[k]) return y_pred_proba def predict(self, X): &quot;&quot;&quot; Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples,)&#125; &quot;&quot;&quot; y_pred_proba = self.predict_proba(X) return np.argmax(y_pred_proba, axis=1)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[二次入坑raspberry-pi]]></title>
    <url>%2F2018%2F10%2F29%2F%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi%2F</url>
    <content type="text"><![CDATA[前言距上一次搭建树莓派平台已经两年了，保存的镜像出了问题，重新搭建一下。 系统下载从官网下载树莓派系统镜像，有以下几种可选 Raspberry Pi — Teach, Learn, and Make with Raspberry Pi Raspbian &amp; Raspbian Lite，基于Debian Noobs &amp; Noobs Lite Ubuntu MATE Snappy Ubuntu Core Windows 10 IOT 其余不太了解，之前安装的是Raspbian，对于Debian各种不适，换上界面优雅的Ubuntu Mate玩一下老老实实玩Raspbian，笑脸:-) 安装比较简单，准备micro-SD卡，用Win32 Disk Imager烧写镜像 Win32 Disk Imager download | SourceForge.net 安装完软件后可点击Read备份自己的镜像。 注意第二次开机前需要配置config.txt文件，否则hdmi无法显示 树莓派配置文档 config.txt 说明 | 树莓派实验室 123456disable_overscan=1 hdmi_force_hotplug=1hdmi_group=2 # DMThdmi_mode=32 # 1280x960hdmi_drive=2config_hdmi_boost=4 修改交换分区Ubuntu Mate查看交换分区1$ free -m 未设置时如下1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 0 0 0 创建和挂载12345678910111213141516# 获取权限$ sudo -i# 创建目录$ mkdir /swap$ cd /swap# 指定一个大小为1G的名为“swap”的交换文件$ dd if=/dev/zero of=swap bs=1M count=1k# 创建交换文件$ mkswap swap# 挂载交换分区$ swapon swap# 卸载交换分区# $ swapoff swap 查看交换分区1$ free -m 未设置时如下1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 RaspbianWe will change the configuration in the file /etc/dphys-swapfile:1$ sudo nano /etc/dphys-swapfile The default value in Raspbian is:1CONF_SWAPSIZE=100 We will need to change this to:1CONF_SWAPSIZE=1024 Then you will need to stop and start the service that manages the swapfile own Rasbian:12$ sudo /etc/init.d/dphys-swapfile stop$ sudo /etc/init.d/dphys-swapfile start You can then verify the amount of memory + swap by issuing the following command:1$ free -m The output should look like:1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 软件安装指令 apt-get 安装软件apt-get install softname1 softname2 softname3 ... 卸载软件apt-get remove softname1 softname2 softname3 ... 卸载并清除配置apt-get remove --purge softname1 更新软件信息数据库apt-get update 进行系统升级apt-get upgrade 搜索软件包apt-cache search softname1 softname2 softname3 ... 修正（依赖关系）安装：apt-get -f insta dpkg 安装.deb软件包dpkg -i xxx.deb 删除软件包dpkg -r xxx.deb 连同配置文件一起删除dpkg -r --purge xxx.deb 查看软件包信息dpkg -info xxx.deb 查看文件拷贝详情dpkg -L xxx.deb 查看系统中已安装软件包信息dpkg -l 重新配置软件包dpkg-reconfigure xx 卸载软件包及其配置文件，但无法解决依赖关系！sudo dpkg -p package_name 卸载软件包及其配置文件与依赖关系包sudo aptitude purge pkgname 清除所有已删除包的残馀配置文件dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P 软件源 备份原始文件 1$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup 修改文件并添加国内源 1$ vi /etc/apt/sources.list 注释元文件内的源并添加如下地址 123456789101112131415161718192021#Mirror.lupaworld.com 源更新服务器（浙江省杭州市双线服务器，网通同电信都可以用，亚洲地区官方更新服务器）：deb http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiversedeb http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiversedeb-src http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse#Ubuntu 官方源 deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiversedeb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiversedeb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse 或者 1234567891011121314151617181920212223#阿里云deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse#网易163deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse 放置非官方源的包不完整，可在为不添加官方源 1deb http://archive.ubuntu.org.cn/ubuntu-cn/ feisty main restricted universe multiverse 更新源 1$ sudo apt-get update 更新软件 1$ sudo apt-get dist-upgrade 常见的修复安装命令 1$ sudo apt-get -f install Python主要是Python和相关依赖包的安装，使用以下指令可导出已安装的依赖包1$ pip freeze &gt; requirements.txt 并使用指令安装到树莓派1$ pip install -r requirements.txt 注意pip更新1python -m pip install --upgrade pip 最新版本会报错1ImportError: cannot import name main 修改文件/usr/bin/pip123from pip import mainif __name__ == &apos;__main__&apos;: sys.exit(main()) 改为123from pip import __main__if __name__ == &apos;__main__&apos;: sys.exit(__main__._main()) 成功!!!失败了，笑脸:-)，手动安装吧。。。 部分包可使用pip3 123$ pip3 install numpy$ pip3 install pandas$ pip3 install sklearn 若需要权限，加入--user 部分包用apt-get，但是优先安装到Python2.7版本，笑脸:-) 123$ sudo apt-get install python-scipy$ sudo apt-get install python-matplotlib$ sudo apt-get install python-opencv 部分从PIPY下载.whl或.tar.gz文件 PyPI – the Python Package Index · PyPI tensorboardX-1.4-py2.py3-none-any.whl visdom-0.1.8.5.tar.gz 安装指令为 1$ pip3 install xxx.whl 12$ tar -zxvf xxx.tar.gz$ python setup.py install Pytorch源码安装 pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration 安装方法Installation - From Source 需要用到miniconda，安装方法如下，注意中间回车按慢一点，有两次输入。。。。。(行我慢慢看条款不行么。。笑脸:-)) 第一次是是否同意条款，yes 第二次是添加到环境变量，yes，否则自己修改/home/pi/.bashrc添加到环境变量 1234567891011$ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh$ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5$ sudo /bin/bash Miniconda3-latest-Linux-armv7l.sh # -&gt; change default directory to /home/pi/miniconda3$ sudo nano /home/pi/.bashrc # -&gt; add: export PATH=&quot;/home/pi/miniconda3/bin:$PATH&quot;$ sudo reboot -h now$ conda $ python --version$ sudo chown -R pi miniconda3 然后就可以安装了没有对应版本的mkl，笑脸:-) 12345678910111213export CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; # [anaconda root directory]# Disable CUDAexport NO_CUDA=1# Install basic dependenciesconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typingconda install -c mingfeima mkldnn# Install Pytorchgit clone --recursive https://github.com/pytorch/pytorchcd pytorchpython setup.py install tensorflow 安装tensorflow需要的一些依赖和工具 1234567$ sudo apt-get update# For Python 2.7$ sudo apt-get install python-pip python-dev# For Python 3.3+$ sudo apt-get install python3-pip python3-dev 安装tensorflow 若下载失败，手动打开下面网页下载.whl包 1234567# For Python 2.7$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp27-none-linux_armv7l.whl$ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl# For Python 3.4$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl$ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl 卸载，重装mock 1234567# For Python 2.7$ sudo pip uninstall mock$ sudo pip install mock# For Python 3.3+$ sudo pip3 uninstall mock$ sudo pip3 install mock 安装的版本tensorflow v1.1.0没有models，因为1.0版本以后models就被Sam Abrahams独立出来了，例如classify_image.py就在models/tutorials/image/imagenet/里 tensorflow/models 其余 输入法 12$ sudo apt-get install fcitx fcitx-googlepinyin $ fcitx-module-cloudpinyin fcitx-sunpinyin git 1$ sudo apt-get install git 配置git和ssh 12345$ git config --global user.name &quot;Louis Hsu&quot;$ git config --global user.email is.louishsu@foxmail.com$ ssh-keygen -t rsa -C &quot;is.louishsu@foxmail.com&quot;$ cat ~/.ssh/id_rsa.pub # 添加到github]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2018%2F10%2F23%2FSVD%2F</url>
    <content type="text"><![CDATA[引言奇异值分解Singular Value Decomposition是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。 原理从特征值分解(EVD)讲起我们知道对于一个$n$阶方阵$A_{n×n}$，有 A\alpha_i = \lambda_i \alpha_i i = 1, ..., n取 P = \left[\alpha_1, \alpha_2, ..., \alpha_n\right]有下式成立 AP = P\Lambda其中 \Lambda = \left[ \begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_n \\ \end{matrix} \right] 特征值一般从大到小排列 利用该式可将方阵$A_{n×n}$化作对角阵$\Lambda_{n×n}$ \Lambda = P^{-1}AP或者 A = P \Lambda P^{-1} = \sum_{i=1}^n \lambda_i (P_{,i})(P_{,i})^{-1} “$_{i}$”表示第$i$行，“$_{,i}$”表示第$i$列 这样我们就可以理解为，矩阵$A$是由$n$个$n$阶矩阵$P_{,i}P^{-1}_{i}$加权组成，特征值$\lambda_i$即为权重。 以上为个人理解，不妥之处可以指出。 奇异值分解(SVD)定义对于长方阵$A_{m×n}$，不能进行特征值分解，可进行如下分解 A_{m×n} = U_{m×m} \Sigma_{m×n} V_{n×n}^T其中$U \in \mathbb{R}^{m×m}, V \in \mathbb{R}^{n×n}$，均为正交矩阵。矩阵$\Sigma_{m×n}$如下 对于$m&gt;n$ \Sigma_{m×n} = \left[ \begin{matrix} S_{n×n} \\ --- \\ O_{(m-n)×n} \end{matrix} \right] 对于$m&lt;n$ \Sigma_{m×n} = \left[ \begin{matrix} S_{m×m} & | & O_{m×(n-m)} \end{matrix} \right] 矩阵$S_{n×n}$为对角阵，对角元素从大到小排列 S_{n×n} = \left[ \begin{matrix} \sigma_1 & & \\ & ... & \\ & & \sigma_n\\ \end{matrix} \right]直观表示SVD分解如下 当取$r&lt;n$时，有部分奇异值分解，可用于降维 A_{m×n} = U_{m×r} \Sigma_{r×r} V_{r×n}^T计算 以下仅考虑$m&gt;n$的情况 令矩阵$A^T$与$A$相乘，有 A^TA = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T A^TA = V \Sigma^T \Sigma V^T 矩阵$U$为正交阵，即满足$U^TU=I$ 其中 \Sigma^T \Sigma = \left[ \begin{matrix} S^T_{n×n} & | & O^T_{n×(m-n)} \end{matrix} \right] \left[ \begin{matrix} S_{n×n} \\ --- \\ O_{(m-n)×n} \end{matrix} \right] = S_{n×n}^2 = \left[ \begin{matrix} \sigma_1^2 & & \\ & ... & \\ & & \sigma_n^2\\ \end{matrix} \right] 则 A^T A = V S^2 V^T 即矩阵$A^T A$相似对角化为$S^2$，对角元素$\sigma_i^2$与矩阵$V$的列向量$v_i(i=1, …, n)$为矩阵$A^T A$的特征对。 那么对矩阵$A^T A$进行特征值分解，有 (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i 则 v_i = \alpha^{(1)}_i \sigma_i = \sqrt{\lambda^{(1)}_i} 注：对于二次型$x^T (A^T A) x$ x^T (A^T A) x = (Ax)^T(Ax) \geq 0故矩阵$A^T A$半正定，$\sigma_i = \sqrt{\lambda_i}$有解 同理，令矩阵$A$与$A^T$相乘，可证得 A A^T = U \Sigma \Sigma^T U^T 其中 \Sigma \Sigma^T = \left[ \begin{matrix} S_{n×n} \\ --- \\ O_{(m-n)×n} \end{matrix} \right] \left[ \begin{matrix} S^T_{n×n} & | & O^T_{n×(m-n)} \end{matrix} \right] = \left[ \begin{matrix} S^2_{n×n} & O_{n×(m-n)} \\ O_{(m-n)×n} & O_{(m-n)×(m-n)} \end{matrix} \right] 即矩阵$A A^T$相似对角化，对角元素$\sigma_i^2$与矩阵$U$的列向量$u_i(i=1, …, m)$为矩阵$A A^T$的特征对。 对矩阵$A A^T$进行特征值分解，有 (A^T A) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i 则 u_i = \alpha^{(2)}_i \sigma_i = \sqrt{\lambda^{(2)}_i} 同理可证得$A A^T$半正定，略。 一般来说，为减少计算量，计算奇异值分解只进行一次特征值分解，如对于矩阵$X_{m×n}(m&gt;n)$，选取$n$阶矩阵$X^T X$进行特征值分解计算$v_i$，计算$u_i$方法下面介绍。 根据前面推导，我们有特征值分解 (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i (A A^T) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i其中$\lambda^{(1)}_i = \lambda^{(2)}_i = \sigma_i^2$，$v_i = \alpha^{(1)}_i$，$u_i = \alpha^{(2)}_i$，即 A^T A v_i = \sigma_i^2 v_i \tag{1} A A^T u_i = \sigma_i^2 u_i \tag{2}$(1)$式左右乘$A$，有 A A^T A v_i = \sigma_i^2 A v_i发现什么？这是另一个特征值分解的表达式！ (A A^T) (A v_i) = \sigma_i^2 (A v_i)故 u_i \propto A v_i 或 u_i = k · A v_i \tag{3}现在求解系数$k$，根据定义 A = U \Sigma V^T \Rightarrow AV = U \Sigma则 A v_i = \sigma_i u_i \Rightarrow u_i = \frac{1}{\sigma_i} A v_i或者 U = A V \Sigma^{-1} 注：只能求前$n$个$u_i$，之后的需要列写方程求解 举栗将矩阵$A$进行分解 A = \left[ \begin{matrix} 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{matrix} \right]为减少计算量，取$A^T A$计算 A^T A = \left[ \begin{matrix} 2 & 1 \\ 1 & 2 \end{matrix} \right]特征值分解，有 A\left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] \left[ \begin{matrix} 3 & \\ & 1 \end{matrix} \right]故 \Sigma = \left[ \begin{matrix} \sqrt{3} & \\ & 1 \end{matrix} \right] V = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] U = A V \Sigma^{-1} = \left[ \begin{matrix} \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\ \frac{2}{\sqrt{6}} & 0 \\ \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \end{matrix} \right]123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; A = np.array([ [0, 1], [1, 1], [1, 0] ])&gt;&gt;&gt; ATA = A.T.dot(A)&gt;&gt;&gt; eigval, eigvec= np.linalg.eig(ATA)&gt;&gt;&gt; V = eigvec.copy()&gt;&gt;&gt; S = np.diag(np.sqrt(eigval))&gt;&gt;&gt; U = A.dot(V).dot(np.linalg.inv(S))&gt;&gt;&gt; Uarray([[ 0.40824829, 0.70710678], [ 0.81649658, 0. ], [ 0.40824829, -0.70710678]])&gt;&gt;&gt; Sarray([[1.73205081, 0. ], [0. , 1. ]])&gt;&gt;&gt; Varray([[ 0.70710678, -0.70710678], [ 0.70710678, 0.70710678]])&gt;&gt;&gt; # 验证&gt;&gt;&gt; U.dot(S).dot(V.T)array([[-2.23711432e-17, 1.00000000e+00], [ 1.00000000e+00, 1.00000000e+00], [ 1.00000000e+00, -2.23711432e-17]]) 理解展开表达式，取$r \leq n$时， A = U_{m×r} \Sigma_{r×r} V_{r×n}^T = \sum_{i=1}^r \sigma_i (U_{,i}) (V_{,i})^T就得到与PCA相同的结论，矩阵$A$可由$r$个$m×n$的矩阵$(U_{,i}) (V_{,i})^T$加权组成。一般来说，前$10\%$甚至$1\%$的奇异值就占了全部奇异值之和的$99\%$，极大地保留了信息，而大大减少了存储空间。 以图片为例，若原有24bit图片，其大小为(1024, 768)，则不计图片信息，仅仅数据共占1024×768×3 B，或2.25 MB。用奇异值分解进行压缩，保留$60\%$的奇异值，可达到几乎无损的程度，此时需要保存向量矩阵$U_{1024×60}$，$V_{60×768}$以及$60$个奇异值，以浮点数float32存储，一共占420 KB即可。 (1024 × 60 + 60 × 768 + 60) × 4 / 2^{10} = 420.23说句题外话，存储量的压缩必然以计算量的增大为代价，相反亦然，所以需要协调好RAM与ROM容量，考虑计算机的计算速度。换句话说，空间和时间上必然是互补的，哲学的味道hhhh。 分解结果的信息保留分解后各样本间的欧式距离与角度信息应不变，给出证明如下设有$m$组$n$维样本样本 X_{n×m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]经奇异值分解，有 X_{n×m} = U_{n×r} \Sigma_{r×r} V_{r×m}^T记 Z_{r×m} = \Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]有 X = U Z 欧式距离 || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2 = \left[ U (Z^{(i)} - Z^{(j)}) \right]^T \left[ U (Z^{(i)} - Z^{(j)}) \right] = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)}) = || Z^{(i)} - Z^{(j)} ||_2^2 即 || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2 角度信息 \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \sqrt{(UZ^{(j)})^T(UZ^{(j)})}} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} 即 \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} 代码@Github: Code of SVD对图片进行了分解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798class SVD(): &quot;&quot;&quot; Singular Value Decomposition Attributes: m &#123;int&#125; n &#123;int&#125; r &#123;int&#125;: if r == -1, then r = n isTrains &#123;bool&#125;: isTrains = True if input.shape[0] &lt; input.shape[1] U &#123;ndarray(m, r)&#125; S &#123;ndarray(r, )&#125; V &#123;ndarray(n, r)&#125; Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - Reassign r if eigvals contains zero - Singular values are stored in a 1-dim array `S` - X&apos; = U S V^T &quot;&quot;&quot; def __init__(self, r=-1): self.m = None self.n = None self.r = r self.isTrans = False self.U = None self.S = None self.V = None def fit(self, X): &quot;&quot;&quot; calculate components Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - reassign self.r if eigvals contains zero &quot;&quot;&quot; (self.m, self.n) = X.shape if self.m &lt; self.n: X = X.T self.m, self.n = self.n, self.m self.isTrans = True self.r = self.n if (self.r == -1) else self.r XTX = X.T.dot(X) eigval, eigvec = np.linalg.eig(X.T.dot(X)) eigval, eigvec = np.real(eigval), np.real(eigvec) self.S = np.sqrt(np.clip(eigval, 0, float(&apos;inf&apos;))) self.S = self.S[self.S &gt; 0] self.r = min(self.r, self.S.shape[0]) # reassign self.r order = np.argsort(eigval)[::-1][: self.r] # sort eigval from large to small eigval = eigval[order]; eigvec = eigvec[:, order] self.V = eigvec.copy() self.U = X.dot(self.V).dot( np.linalg.inv(np.diag(self.S))) return self.U, self.S, self.V def compose(self, r=-1): &quot;&quot;&quot; merge first r components Parameters: r &#123;int&#125;: if r==-1, merge all components Returns: X &#123;ndarray(m, n)&#125; &quot;&quot;&quot; if r == -1: X = self.U.dot(np.diag(self.S)).dot(self.V.T) X = X.T if self.isTrans else X else: (m, n) = (self.n, self.m) if self.isTrans else (self.m, self.n) X = np.zeros(shape=(m, n)) for i in range(r): X += self.__getitem__(i) return X def __getitem__(self, idx): &quot;&quot;&quot; get a component Parameters: index &#123;int&#125;: range from (0, self.r) &quot;&quot;&quot; u = self.U[:, idx] v = self.V[:, idx] s = self.S[idx] x = s * u.reshape(self.m, 1).\ dot(v.reshape(1, self.n)) x = x.T if self.isTrans else x return x def showComponets(self, r=-1): &quot;&quot;&quot; display components Notes: - Resize components&apos; shape into (40, 30) &quot;&quot;&quot; m, n = self.m, self.n r = self.r if r==-1 else r n_images = 10; m_images = r // n_images + 1 m_size, n_size = 40, 30 showfig = np.zeros(shape=(m_images*m_size, n_images*n_size)) for i in range(r): m_pos = i // n_images n_pos = i % n_images component = self.__getitem__(i) component = component.T if self.isTrans else component component = cv2.resize(component, (30, 40)) showfig[m_pos*m_size: (m_pos+1)*m_size, n_pos*n_size: (n_pos+1)*n_size] = component plt.figure(&apos;components&apos;) plt.imshow(showfig) plt.show() 用上面的代码进行实验1234567891011121314# 读取一张图片X = load_images()[0].reshape((32, 32))showmat2d(X)# 对图片进行奇异值分解decomposer = SVD(r=-1)decomposer.fit(X)# 显示一下分量decomposer.showComponets(r=-1)# 将全部分量组合，并显示X_ = decomposer.compose(r=-1)showmat2d(X_)# 将前5个分量组合，并显示X_ = decomposer.compose(r=5)showmat2d(X_) 载入原图如下 分量显示如下 组合分量显示如下 组合全部 组合前5个分量]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PCA]]></title>
    <url>%2F2018%2F10%2F22%2FPCA%2F</url>
    <content type="text"><![CDATA[引言PCA全称Principal Component Analysis，即主成分分析，是一种常用的数据降维方法。它可以通过线性变换将原始数据变换为一组各维度线性无关的表示，以此来提取数据的主要线性分量。 向量的投影现有两个任意不共线向量$\vec{u}, \vec{v}$，将$\vec{u}$投射到$\vec{v}$上 投影后，可以得到两个正交向量 \vec{u}' · (\vec{u} - \vec{u}') = 0我们设 \vec{u}' = \mu \vec{v} \tag{1}代入后有 \mu \vec{v} · (\vec{u} - \mu \vec{v}) = 0引入矩阵运算，即 (\mu v)^T (u - \mu v) = 0有 v^T u = \mu v^T v则得到$u’$以$v$为基向量的坐标 \mu = (v^T v)^{-1} v^T u \tag{2}所以得到 u' = v (v^T v)^{-1} v^T u \tag{*} 坐标变换求解投影向量：$u’$可视作$u$经坐标变换$u’ = P u$得到，所以 P = v (v^T v)^{-1} v^T 推广至多个向量的投影，即得到 P = X (X^T X)^{-1} X^T这与线性回归中得到的结论一致。 实际上 u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T u记单位向量$\frac{v}{||v||}$为$v_0$，得到 u' = v_0 v_0^T u由几何关系，可以计算得投影后的长度为 d = ||u|| \cos \theta = ||u|| \frac{v^T u}{||u||||v||} = v_0^T u所以在向量投影中，$u^T v_0$表示以$v_0$为基向量的坐标。 PCA现在有$N$维数据集$D=\{x^{(1)}, x^{(2)}, …, x^{(M)}\}$，其中$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, …, x^{(i)}_N\right]^T$，各维特征$D_{j}$间存在线性相关性，利用主成分分析可使 数据维度降低； 提取主成分，且各成分间不相关。 说明 由于选取的特征轴是正交的，所以计算结果线性无关； 提取了方差较大的几个特征，为主要线性分量。 以二维空间中的数据$x^{(i)} = \left[\begin{matrix} x^{(i)}_1 \\ x^{(i)}_2\end{matrix}\right]$为例，维度可降至一维，如下图所示。 主轴可有无穷多种选择，那么问题就是如何选取最优的主轴。先给出PCA的计算步骤。 计算步骤输入的$M$个$N$维样本，有样本矩阵 X_{N×M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right] = \left[ \begin{matrix} x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\ x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\ ... \\ x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\ \end{matrix} \right]投影 对每个维度(行)进行去均值化 X_j := X_j - \mu_j 其中$\mu_j = \overline{X_j}$，$j = 1, 2, …, N$ 求各维度间的协方差矩阵$\Sigma_{N×N}$ \Sigma_{ij} = Cov(x_i, x_j) 或 \Sigma = \frac{1}{M} X X^T 注： X X^T = \left[ \begin{matrix} \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\ \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M \left[ \begin{matrix} x^{(i)}_1 x^{(i)}_1 & x^{(i)}_1 x^{(i)}_2 & ... & x^{(i)}_1 x^{(i)}_N \\ x^{(i)}_2 x^{(i)}_1 & x^{(i)}_2 x^{(i)}_2 & ... & x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ x^{(i)}_N x^{(i)}_1 & x^{(i)}_N x^{(i)}_2 & ... & x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M x^{(i)} x^{(i)T} 协方差定义式 Cov(x,y)≝\frac{1}{n-1} ∑_{i=1}^n (x_i−\overline{x})^T(y_i−\overline{y})其中$x=[x_1, x_2, …, x_n]^T, y=[y_1, y_2, …, y_n]^T$ 求协方差矩阵$\Sigma$的特征值$λ_i$及其对应特征向量$α_i$，$i=1, …, N$； 按照特征值从大到小排列特征对$(λ_i,α_i)$，选取$K$个最大特征值对应的特征向量作为降维后的主轴$ \beta_1, \beta_2, …, \beta_K $，其中$\beta_k$为单位向量 \beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^T记 B_{N×K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]$K$的选取方法有如下两种： 指定选取$K$个主轴 保留$99\%$的方差\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99 将样本点投射到$K$维坐标系上 样本$X^{(i)}$投射到主成分轴$\beta_k$上，其坐标表示为向量，为 S^{(i)}_k = X^{(i)T}\beta_k 注意此时的基座标为$\beta_k$，或者说$X’^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$ 所有样本在主轴$\beta_k$上的投影坐标即 S = B^T X 其中$S_{K×M}$，$B_{N×K}$，$X_{N×M}$ 注：若取$K=N$，可重建数据，如下 复原第$5$步中，样本点向量$X^{(i)}$的主要分量投射到$K$个$N$维向量上，投影坐标为$S^{(i)}_k$，即 X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_k以上就是样本点的复原公式，矩阵形式即 \hat{X} = BS其中$\hat{X}_{N×M}$，$B_{N×K}$，$S_{K×M}$ 考虑到已去均值化，故 \hat{X}_j \approx \hat{X}_j + \mu_j证明 投影向量的$2$范数最大，或者说，投影后的坐标平方和最大 当所有样本$X$投射到第一主轴$\beta_1$上，其坐标为 S_1 = X^T \beta_1所有元素的平方和，或向量$S_1$的$2$范数为 ||S_1||_2^2 = S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}即优化目标为 \max ||S_1||_2^2 s.t. ||\beta_1||_2^2 = 1矩阵$C=XX^T$为对称矩阵，故可单位正交化 C = W \Lambda W^T W = \left[\begin{matrix} | & & |\\ w_1 & ... & w_M\\ | & & |\\ \end{matrix}\right] \Lambda = \left[\begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_M\\ \end{matrix}\right]其中$\lambda_1 &gt; …&gt; \lambda_M$，$w_i(i=1,…,M)$为矩阵$C$的特征向量(单位向量，互相正交) 实际上$R(C) \leq (n-1)$，即最多有$(n-1)$个特征值大于$0$。 ||S_1||_2^2 = \beta_1^T W \Lambda W^T \beta_1 \tag{2}令$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$，可得 ||S_1||_2^2 = \alpha_1^T \Lambda \alpha_1 \tag{3}即 ||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \tag{4}进一步 \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \leq \lambda_1 \sum_{i=1}^M \alpha_{1i}^2 \tag{5}且由于$\beta_1^T\beta_1 = 1$，故 1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = \sum_{i=1}^M \alpha_{1i}^2可得 ||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \leq \lambda_1 \tag{6}为使$(6)$取等号，即达最大值，可使 \begin{cases} \alpha_{11} = 1 \\ \alpha_{12} = ... = \alpha_{1M} = 0 \end{cases}即令 \beta_1 = W \alpha_1 = w_1 $\alpha_1 = [1, 0, …, 0]^T$ 所以$\beta_1$对应矩阵$C=XX^T$的特征向量$w_1$，且有 ||S_1||_2^2 = \lambda_1 或者第一主成分的证明也可以这样，建立优化目标 \beta_1 = \arg \max ||S_1||_2^2s.t. ||\beta_1||_2^2 = 1构造拉格朗日函数 L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)也即 L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)求其极值点 ▽_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0有 X X^T \beta_1 = \lambda_1 \beta_1可见$\beta_1$即方阵$X X^T$的特征向量 当我们希望用更多的主成分刻画数据，如已经求得主成分$\beta_1, …, \beta_{r-1}$，先需求解$\beta_r$，引入正交约束$\beta_r^T \beta_i = 0$，即目标函数为 ||S_r||_2^2 = \beta_r^T C \beta_r s.t. \beta_r^T \beta_i = 0, i = 1, ..., r-1 ||\beta_r||_2^2 = 1令$\beta_r = W \alpha_r$，则 ||S_r||_2^2 = \alpha_r^T \Lambda \alpha_r = \sum_i \lambda_i \alpha_{ri}^2而根据正交约束 0 = \beta_r^T \beta_i = \alpha_r^T W^T w_i = \alpha_{ri}, i = 1, ..., r-1 $ W^T w_i = \left[0, …, 1_i, …, 0\right]^T$ 所以 ||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{5}又因为$\beta_r^T \beta_r = 1$(单位向量)，故 \beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1于是类似的，为使$(5)$取最大，取 \begin{cases} \alpha_{rr} = 1\\ \alpha_{ri} = 0, i = 1, ..., M, i \neq r \end{cases} $\alpha_r = [0, …, 1_r, …, 0]$ 则此时 \beta_r = W \alpha_r = w_r且有 ||S_r||_2^2 = \lambda_r证毕。 白化(whitening)whitening的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。 数据的whitening必须满足两个条件： 不同特征间相关性最小，接近$0$； 所有特征的方差相等（不一定为$1$）。 常见的白化操作有PCA whitening和ZCA whitening。 Whitening - Ufldl PCA whitening PCA whitening指将数据$X$经过PCA降维为$S$后，可以看出$S$中每一维是独立的，满足whitening的第一个条件，这是只需要将$S$中的每一维都除以标准差就得到了每一维的方差为$1$，也就是说方差相等。 X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}} ZCA whitening ZCA whitening是指数据$X$先经过PCA变换为$S$，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为$1$的操作，最后将得到的矩阵左乘一个特征向量矩阵$U$即可。 X_{ZCAwhite} = U · X_{PCAwhite} Kernel PCAKernel PCA的思想是在高维的特征空间中求解协方差矩阵 \Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^T其中$\Phi(X^{(i)})$表示将样本$i$映射到高维空间后中的向量，即 \Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^T其中$N’ &gt; N$，由于$\Phi(X^{(i)})$为隐式的，故设置核函数求解，记 \kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T 关于核技巧，移步非线性支持向量机 应用可利用PCA与线性回归求解$3$维空间中平面的法向量 利用PCA重建数据(不降维，此时为$3$维)，此时第$1, 2$主成分轴可张成所求平面，即该平面可表示为 \Pi = span \{ \beta_1, \beta_2 \} 就是说，第一、二主成分是这些点“拉伸”最大的方向 :-)，好懂不？ 由正交投影可知，平面外一点$y$可通过最小二乘(线性回归)的方法投射到平面上，向量运算，不考虑偏置项，即 \hat{y} = \theta_1 x_1 + \theta_2 x_2 \tag{*} 其中$x_1, x_2$表示第一、第二主成分$\beta_1, \beta_2$，为$3$维向量 \hat{y} = \left[ \begin{matrix} \hat{y_1} \\ \hat{y_2} \\ \hat{y_3} \\ \end{matrix} \right] x_i = \left[ \begin{matrix} x_{i1} \\ x_{i2} \\ x_{i3} \\ \end{matrix} \right] 可利用公式求解回归参数$\theta$ \theta = (X^TX+\lambda I)^{-1} X^T y 注意：$X(n_samples, n_features)$，这里把$(x_{1j}, x_{2j}, y_{j})作为一组样本$ 此时该参数表示在主轴上的坐标$(\theta_1, \theta_2)$，带回$(*))$即可解得$\hat{y}$ \hat{y} = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*} 通俗理解，一掌把$y$拍平在了平面$\Pi$上，变成了$\hat{y}$，但是哪有这么好拍。。。这个时候刺在掌心里一定有一个垂直的向量分量，即为该平面的法向量 \vec{n} = y - \hat{y} 也可使用粗暴一点的方法，直接将第三主成分作为法向量。 或者直接上投影公式： \hat{y} = Py P = X (X^TX+\lambda I)^{-1} X^T ![projection](/PCA/projection.jpg) &gt; 总体的运算流程如下 &gt; - 利用所有样本点(近似平面)计算主成分，第一、二主成分张成平面$\Pi$； &gt; - 选出其中一个样本点，将平行于平面$\Pi$的成分投射到$\Pi$上； &gt; - 该样本点剩余分量即法向量； &gt; - 一般来说，取所有点法向量的均值。 程序@Github: PCA 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class PrincipalComponentAnalysis(): def __init__(self, n_component=-1): self.n_component = n_component self.meanVal = None self.axis = None def fit(self, X, prop=0.99): &apos;&apos;&apos; the parameter &apos;prop&apos; is only for &apos;n_component = -1&apos; &apos;&apos;&apos; # 第一步: 归一化 self.meanVal = np.mean(X, axis=0) # 训练样本每个特征上的的均值 X_normalized = (X - self.meanVal) # 归一化训练样本 # 第二步：计算协方差矩阵 # cov = X_normalized.T.dot(X_normalized) cov = np.cov(X_normalized.T) # 协方差矩阵 eigVal, eigVec = np.linalg.eig(cov) # EVD order = np.argsort(eigVal)[::-1] # 从大到小排序 eigVal = eigVal[order] eigVec = eigVec.T[order].T # 选择主成分的数量 if self.n_component == -1: sumOfEigVal = np.sum(eigVal) sum_tmp = 0 for k in range(eigVal.shape[0]): sum_tmp += eigVal[k] if sum_tmp &gt; prop * sumOfEigVal: # 平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值 self.n_component = k + 1 break # 选择投影坐标轴 self.axis = eigVec[:, :self.n_component] # 选择前n_component个特征向量作为投影坐标轴 def transform(self, X): # 第一步：归一化 X_normalized = (X - self.meanVal) # 归一化测试样本 # 第二步：投影 X_nxk · V_kxk&apos; = X&apos;_nxk&apos; X_transformed = X_normalized.dot(self.axis) return X_transformed def fit_transform(self, X, prop=0.99): self.fit(X, prop=prop) return self.transform(X) def transform_inv(self, X_transformed): # 视投影向量长度为一个单位长度，投影结果为投影向量上的坐标 # X&apos;_nxk&apos; · V_kxk&apos;.T = X&apos;&apos;_nxk X_restructed = X_transformed.dot(self.axis.T) # 还原数据 X_restructed = X_restructed + self.meanVal return X_restructed 实验结果 Demo1: PCA applied on 2-d datasets Demo2: PCA applied on wild face origin reduced restructured]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Softmax Regression]]></title>
    <url>%2F2018%2F10%2F18%2FSoftmax-Regression%2F</url>
    <content type="text"><![CDATA[Unsupervised Feature Learning and Deep Learning Tutorial 引言Logistic Regression中采用的非线性函数为Sigmoid，将输出值映射到$(0, 1)$之间作为概率输出，处理的是二分类问题，那么对于多分类的问题怎么处理呢？ 模型 由Logistic回归推广而来 SoftmaxSoftmax在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类$(K&gt;2)$问题，分类器最后的输出单元需要Softmax函数进行数值处理。 S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]其中$x$为矩阵形式的向量，其维度为$(K×1)$，$K$为类别数目。Softmax的输出向量维度与$x$相同，各元素$x_i$加和为$1$，可用于表示取各个类别的概率。 注意到，对于函数$e^x$ \lim_{x \rightarrow - \infty} e^x = 0\lim_{x \rightarrow + \infty} e^x = +\infty 假设所有的$x_i$等于某常数$c$，理论上对所有$x_i$上式结果为$\frac{1}{n}$ 若$c$为很小的负数，$e^c$下溢，结果为$NaN$； 若$c$量级很大，$e^c$上溢，结果为$NaN$。 在数值计算时并不稳定，但是Softmax所有输入增加同一常数时，输出不变，得稳定版本： S(x) := S(x - max(x_i)) e^{x_{max} - max(x_i)} = 1 减去最大值导致$e^x$最大为$1$，排除上溢； 分母中至少有一项为$1$，排除分母下溢导致处以$0$的情况。 其对数 log S(x)_i = x_i - log ({\sum_{k=1}^K exp(x_k)}) 注意到，第一项表示输入$x_i$总是对代价函数有直接的贡献。这一项不会饱和，所以即使$x_i$对上式的第二项的贡献很小，学习依然可以进行； 当最大化对数似然时，第一项鼓励$x_i$被推高，而第二项则鼓励所有的$x$被压低； 第二项$log ({\sum_{k=1}^K exp(x_k)})$可以大致近似为$max(x_k)$，这种近似是基于对任何明显小于$max(x_k)$的$x_k$都是不重要的，负对数似然代价函数总是强烈地惩罚最活跃的不正确预测 除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习 作者：NirHeavenX来源：CSDN原文：https://blog.csdn.net/qsczse943062710/article/details/61912464版权声明：本文为博主原创文章，转载请附上博文链接！ Softmax解决多分类问题对于具有$K$个分类的问题，每个类别训练一组参数$ w_k $ z_k^{(i)} = w_k^Tx^{(i)}或写作矩阵形式 z^{(i)} = W^Tx^{(i)}其中 x^{(i)} = \left[ \begin{matrix} x_0^{(i)}\\ x_1^{(i)}\\ ...\\ x_n^{(i)} \end{matrix} \right]_{n×1}, x_0^{(i)}=1 W = [w_1, w_2, ..., w_K]_{(n+1)×K} w_i = \left[ \begin{matrix} w_{i0}\\ w_{i1}\\ ...\\ w_{in} \end{matrix} \right]_{n×1}最终各类别输出概率为 \hat{y}^{(i)} = Softmax(z^{(i)}) 产生了一个奇怪的脑洞。。。二分类问题 p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }定义二分类线性单元输出的差值为 z = x_1 - x_2得到 p(x_1) = \frac{1}{1 + e^{-z}}以$x_1 = [x_{11}, x_{12}]^T$为例(二维特征)，取$w_1=1, w_2=2, b=3$ p(x_1) = \frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}} 而多分类问题，以$3$分类为例 p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }定义线性单元输出的差值为 z_{12} = x_1 - x_2 z_{13} = x_1 - x_3 p(x_1) = \frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }做出图像为 损失函数由交叉熵理解CrossEnt = \sum_j p_j log \frac{1}{q_j}而对于样本$ (X^{(i)}, y^{(i)}) $，为确定事件，故标签概率各元素的取值$p_j$为$ y^{(i)}_j ∈ \{0,1\}$，$ q_j即预测输出的概率值\hat{y}^{(i)}_j$ 一般取各个样本损失的均值$(\frac{1}{N})$ L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N 1\{y^{(i)}_j=k\} log (\hat{y}^{(i)}_j) 1\{y^{(i)}_j=k\} = \begin{cases} 1 & y^{(i)}_j = k \\ 0 & y^{(i)}_j \neq k \end{cases}可对实际标签$y^{(i)}$采取One-Hot编码，便于计算 y^{(i)} = \left[ \begin{matrix} 0, ..., 1_{y^{(i)}}, ..., 0 \end{matrix} \right]^T则 L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N y^{(i)T} log (\hat{y}^{(i)})由决策平面理解从贝叶斯决策和分类问题的决策平面可知，对于类别$c_i$，有 P(c_i|x) = \frac{P(x|c_i)}{\sum_{j=0}^KP(x|c_j)} 假设每个类别的样本服从正态分布，先验概率相等，各类别样本特征间协方差相等。证明略. 梯度推导Softmax函数的导数对于 S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]一般输出作为概率值，记 P = S(x)p_i = S(x)_i对向量$x$中某元素求导 \frac{∂S(x)}{∂x_i} = \frac{∂}{∂x_i} \left[ \begin{matrix} ...\\ \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}\\ ...\\ \end{matrix} \right] $(1)$ $i=k$$\frac{∂}{∂x_i} \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{exp’(x_i)·\sum_{j=1}^K exp(x_j) - exp(x_i)·(\sum_{j=1}^K exp(x_j))’}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)·\sum_{j=1}^K exp(x_j) - exp^2(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)} -(\frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)})^2$$ = p_i (1 - p_i)$ $(2)$ $i\neq k$$\frac{∂}{∂x_i} \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{exp’(x_k)·\sum_{j=1}^K exp(x_j) - exp(x_k)·(\sum_{j=1}^K exp(x_j))’}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{- exp(x_k)exp(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$= - p_i p_k$ 综上 \frac{∂S(x)}{∂x_i}_{K×1} = \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] - \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right] = \left( \left[ \begin{matrix} 0\\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i 损失函数梯度在OneHot编码下，损失函数形式为 L(\hat{y},y) = \frac{1}{N} \sum_{i=1}^N L (y^{(i)}, \hat{y}^{(i)}) L (y^{(i)}, \hat{y}^{(i)}) = - y^{(i)T} log \hat{y}^{(i)} \hat{y}^{(i)} = S(z^{(i)}) z^{(i)} = W^T x^{(i)}即只考虑实际分类对应的概率值 L (y^{(i)}, \hat{y}^{(i)}) = - log \hat{y}^{(i)}_{y^{(i)}} 由于 $S(z^{(i)})_{t^{(i)}}$与$z^{(i)}$向量各个元素都有关，由链式求导法则 \frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } ( \sum_{k=1}^K \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} \frac{∂z^{(i)}_k}{∂w_{pq}} )$1.$ 考察 $\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}$ \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} = ​ \begin{cases} ​ \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_k) & k=y^{(i)} \\ ​ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_k & k \neq y^{(i)} ​ \end{cases}$2.$ 考察 $\frac{∂z^{(i)}_k}{∂w_{pq}}$ \frac{∂z^{(i)}_k}{∂w_{pq}} = \begin{cases} \frac{∂z^{(i)}_k}{∂w_{pq}} = x^{(i)}_p & k=q\\ \frac{∂z^{(i)}_k}{∂w_{pq}} = 0 & k \neq q \end{cases} 综上所述 \frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q} \frac{∂z^{(i)}_q}{∂w_{pq}}其中 \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q} = \begin{cases} \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_q) & q = y^{(i)}\\ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_q & q \neq y^{(i)} \end{cases} \frac{∂z^{(i)}_q}{∂w_{pq}} = x^{(i)}_p故对于单个样本$(X^{(i)}, y^{(i)})$，当样本标签采用$OneHot$编码时 \frac{∂L^{(i)}}{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q} x^{(i)}_p = \begin{cases} (\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\ \hat{y}^{(i)}_qx^{(i)}_p & q \neq y^{(i)} \end{cases} 注： 这里可以约分去掉$\hat{y}^{(i)}_{y^{(i)}}$ \frac{∂L^{(i)}}{∂w_{pq}} = ( \hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_p更一般的，写成矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量) ∇_W L = X^T(\hat{Y} - Y) 用线性模型解决分类和回归问题时，形式竟如此统一! 至此为止，梯度推导结束，利用梯度下降法迭代求解参数矩阵$W$即可。 W := W - \alpha ∇_W L代码@GitHub: Code of Softmax Regression Softmax12345678def softmax(X): &quot;&quot;&quot; 数值计算稳定版本的softmax函数 @param &#123;ndarray&#125; X: shape(batch_size, n_labels) &quot;&quot;&quot; X_max = np.max(X, axis=1).reshape((-1, 1)) # 每行的最大值 X = X - X_max # 每行减去最大值 X = np.exp(X) return X / np.sum(X, axis=1).reshape((-1, 1)) cost function1234567891011def crossEnt(self, y_label_true, y_prob_pred): &quot;&quot;&quot; 计算交叉熵损失函数 @param &#123;ndarray&#125; y_label_true: 真实标签 shape(batch_size,) @param &#123;ndarray&#125; y_prob_pred: 预测输出 shape(batch_size, n_labels) &quot;&quot;&quot; mask = self.encoder.transform(y_label_true.reshape(-1, 1)).toarray() # shape(batch_size, n_labels) y_prob_masked = np.sum(mask * y_prob_pred, axis=1) # 每行真实标签对应的预测输出值 y_prob_masked[y_prob_masked==0.] = 1. y_loss = np.log(y_prob_masked) loss = - np.mean(y_loss) # 求各样本损失的均值 return loss gradient12345678910def grad(self, X_train, y_train, y_prob_pred): &quot;&quot;&quot; 计算梯度 \frac &#123;∂L&#125; &#123;∂W_&#123;pq&#125;&#125; @param X_train: 训练集特征 @param y_train: 训练集标签 @param y_prob_pred: 训练集预测概率输出 @param y_label_pred: 训练集预测标签输出 &quot;&quot;&quot; y_train = self.encoder.transform(y_train) dW = X_train.T.dot(y_prob_pred - y_train) return dW training step省略可视化和验证部分的代码123456789101112131415161718192021222324252627282930313233343536def fit(self, X_train, X_valid, y_train, y_valid, min_acc=0.95, max_epoch=20, batch_size=20): &quot;&quot;&quot; 训练 &quot;&quot;&quot; # 添加首1列，输入到偏置w0 X_train = np.c_[np.ones(shape=(X_train.shape[0],)), X_train] X_valid = np.c_[np.ones(shape=(X_valid.shape[0],)), X_valid] X_train = self.scaler.fit_transform(X_train) # 尺度归一化 X_valid = self.scaler.transform(X_valid) # 尺度归一化 self.encoder.fit(y_train.reshape(-1, 1)) self.n_features = X_train.shape[1] self.n_labels = self.encoder.transform(y_train).shape[1] # 初始化参数 self.W = np.random.normal(loc=0, scale=1.0, size=(self.n_features, self.n_labels)) n_batch = X_train.shape[0] // batch_size # 可视化相关 plt.ion() plt.figure(&apos;loss&apos;); plt.figure(&apos;accuracy&apos;) loss_train_epoch = []; loss_valid_epoch = [] acc_train_epoch = []; acc_valid_epoch = [] for i_epoch in range(max_epoch): for i_batch in range(n_batch): # 批处理梯度下降 n1, n2 = i_batch * batch_size, (i_batch + 1) * batch_size X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2] # 预测 y_prob_train = self.predict(X_train_batch, preprocessed=True) # 计算损失 loss_train_batch = self.crossEnt(y_train_batch, y_prob_train) # 计算准确率 y_label_train = np.argmax(y_prob_train, axis=1) a = y_train_batch.reshape((-1,)) acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((-1,))).astype(&apos;float&apos;)) # 计算梯度 dW dW = self.grad(X_train_batch, y_train_batch, y_prob_train) # 更新参数 self.W -= self.lr * dW predict step123456789101112def predict(self, X, preprocessed=False): &quot;&quot;&quot; 对输入的样本进行预测，输出标签 @param &#123;ndarray&#125; X: shape(batch_size, n_features) @return &#123;ndarray&#125; y_prob: probability, shape(batch_size, n_labels) &#123;ndarray&#125; y_label: labels, shape(batch_size,) &quot;&quot;&quot; if not preprocessed: # 训练过程中调用此函数时，不用加首1列 X = np.c_[np.ones(shape=(X.shape[0],)), X] # 添加首1项，输入到偏置w0 X = self.scaler.transform(X) y_prob = softmax(X.dot(self.W)) # 预测概率值 shape(batch_size, n_labels) return y_prob 实验结果以下蓝线为训练集参数，红线为验证集参数，若稳定训练(如batch_size = 20的结果)，最终准确率在$80\%$左右。 由于随机梯度下降(SGD)遍历次数太多，运行较慢，没有用SGD方法训练，就前几个epoch来看，效果没有batch_size = 20的好； 添加隐含层形成三层结构的前馈神经网络，可提高准确率； 还有一点，使用批处理梯度下降(n_batch = 1)训练时，可以看到损失值已经趋于$0$，但准确率却很低，说明已经陷入局部最优解。 batch size = 20 损失 准确率 batch_size = 200 损失 准确率 n_batch = 1 损失 准确率 感悟推公式要我老命。。。。 Softmax回归可以视作不含隐含层的前馈神经网络。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[引言逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 模型先给出模型，推导过程稍后给出，逻辑回归包含Sigmoid函数 f(z) = \frac{1}{1+e^{-z}}其图像如下 定义 z = w^Tx其中$x=[x_0, x_1, …, x_n]^T, x_0=1$ h_w(x) = g(z) = \frac{1}{1+e^{-z}}损失函数由最大似然估计推导对于二元分类问题，其取值作为随机变量，服从二项分布 $B(1, p)$，其中$p$即为预测输出概率$\hat{y}$ P(y_i^{(i)}) = (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}由极大似然估计 L = \prod_{i=0}^N P(y_i^{(i)}) = \prod_{i=0}^N (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}取对数似然函数 logL = \sum_{i=0}^N [y_i^{(i)} log \hat{y}_i^{(i)} + (1-y_i^{(i)}) log (1-\hat{y}_i^{(i)})]优化目标是 w = argmax_w logL优化问题一般表述成minimize问题，添加负号，构成Neg Log Likelihood损失 w = argmin_w (-logL)一般取均值 L(\hat{y}, y)=- \frac{1}{N} \sum_i [y_i^{(i)} log(\hat{y}_i^{(i)})+(1 - y_i^{(i)})log(1-\hat{y}_i^{(i)})]其中$y_i$表示真实值，$\hat{y}_i$表示预测值 从交叉熵理解已知交叉熵cross entropy定义如下 CrossEnt = \sum_i p_i log \frac{1}{q_i}而对于样本$ (X_i, y_i) $，为确定事件，故标签概率的取值为$ p_i = y_i ∈ \{0,1\}$，$ q_i即预测输出的概率值\hat{y}_i $，可得到与上面相同的推导结论 从决策平面和贝叶斯决策理解相关内容查看分类问题的决策平面和贝叶斯决策，逻辑回归考虑的一般是等先验概率问题，故决策函数定义为 $if$ $P(c_i|x)&gt;P(c_j|x)$ $then$ $ x \in c_i $, $ i, j = 1, 2 $ 从贝叶斯决策可知，对于类别$c_1$，有 P(c_1|x) = \frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}设在各个类别下，特征$x$服从正态分布 P(x|c_i) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i))则 P(c_1|x) = \frac {1} { 1 + exp(-z) } P(c_2|x) = 1 - P(c_1|x) = \frac{exp(-z)}{1+exp(-z)} $P(c_1|x) = \frac{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}$ $P(c_1|x) = \frac{1}{1 + \frac{exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}}$ 假定各分类的样本方差相等，$ \Sigma_1 = \Sigma_2 = \sigma^2 I $ $ P(c_1|x) = \frac {1}{1 + exp(- [ \frac{1}{\sigma^2} (\mu_1-\mu_2)^T x - \frac{1}{2 \sigma^2} (\mu_1^T\mu_1 - \mu_2^T\mu_2) ])}$ 令 w = \frac{1}{\sigma^2} (\mu_1 -\mu_2)b = - \frac{1}{2\sigma^2}(\mu_1^T \mu_1 - \mu_2^T \mu_2)即可得到 P(c_1|x) = \frac {1} { 1 + exp(-z) }其中 z = w^T x + b 梯度推导先推导Sigmoid函数的导数 f'(z) = (1 - f(z))f(z)值得注意的是，从$f’(z)$的图像可以看到，在$ x=0 $处$f’(z)$取极大值，且 f'(z)_{max} = f'(z)|_{z=0} = 0.25 \lim_{z \rightarrow \infty} f'(z) = 0在多层神经网络反向传播更新参数时，由于梯度多次累乘，Sigmoid作为激活函数会存在“梯度消失”的问题，使得参数更新非常缓慢。 $ f’(z) $$ = (\frac{1}{1+e^{-z}})’ $$ = \frac​ {-(1+e^{-z})’}​ {(1+e^{-z})^2} $$ = \frac​ {e^{-z}}​ {(1+e^{-z})^2} $$ = \frac​ {e^{-z}}​ {1+e^{-z}}​ \frac​ {1}​ {1+e^{-z}}$$ = (1 - f(z))f(z)$ 利用链式求导法则可得 $\frac{∂L}{∂w_j}$$= -\frac{∂}{∂w_j} \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})]$$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\frac{∂}{∂w_j}\hat{y}^{(i)}-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\frac{∂}{∂w_j}\hat{y}^{(i)}]$$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j]$$= - \frac{1}{N} \sum_i [y^{(i)} (1-\hat{y}^{(i)})w_j-(1-y^{(i)}) y^{(i)} w_j]$$= \frac{1}{N} \sum_i (\hat{y}^{(i)} - y^{(i)})w_j $ 写作矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量) ∇_w L = X^T (\hat{Y} - Y)训练和线性回归一样，采用梯度下降法求解 w := w - \alpha ∇_w L处理多分类问题假设有$K$个类别，则依次以类别$c_i$为正样本训练模型，一共训练$K$个。测试样本在每个模型上计算，最终将概率最大的作为分类结果。 这样划分数据集，会使训练集正负样本数目严重不对称，特别是类别很多的情况，对结果会产生影响。可推广至softmax回归解决这个问题。 程序代码@Github: Code for Logistic Regression cost function123456789101112131415def lossFunctionDerivative(self, X, theta, y_true): &apos;&apos;&apos; 计算损失函数对参数theta的梯度 对theta[j]的梯度为：(y_pred - y_true)*x[j] &apos;&apos;&apos; err = self.predict_prob(X, theta) - y_true return X.T.dot(err)/y_true.shape[0]def lossFunction(self, y_pred_prob, y_true): &apos;&apos;&apos; 未使用 计算损失值: Cross-Entropy y_pred_prob, y_true: NumPy array, shape=(n,) &apos;&apos;&apos; tmp = y_true*np.log(y_pred_prob) + (1 - y_true)*np.log(1 - y_pred_prob) return np.mean(-tmp) training step123456789101112131415161718192021def gradDescent(self, min_acc, learning_rate=0.01, max_iter=10000): acc = 0; n_iter = 0 for n_iter in range(max_iter): for n in range(self.n_batch): X_batch = self.X[n*self.batch_size:(n+1)*self.batch_size] t_batch = self.t[n*self.batch_size:(n+1)*self.batch_size] grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch) self.theta -= learning_rate * grad # 梯度下降 acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t) if acc &gt; min_acc: print(&apos;第%d次迭代, 第%d批数据&apos; % (n_iter, n)) print(&quot;当前总体样本准确率为: &quot;, acc) print(&quot;当前参数值为: &quot;, self.theta) return self.theta if n_iter%100 == 0: print(&apos;第%d次迭代&apos; % n_iter) print(&apos;准确率： &apos;, acc) print(&quot;超过迭代次数&quot;) print(&quot;当前总体样本准确率为: &quot;, acc) print(&quot;当前参数值为: &quot;, self.theta) return self.theta 实验结果]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLinear-Regression%2F</url>
    <content type="text"><![CDATA[引言线性回归可以说是机器学习最基础的算法 模型\hat{y}^{(i)} = w^Tx^{(i)}其中 x^{(i)}=[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T, x_0^{(i)}=1这里$x_0^{(i)}=1$表示偏置$b$，即$b=w_0$ \hat{y}^{(i)} = w^Tx^{(i)} + b 注：对于非线性的数据，可构造高次特征。 损失函数定义误差e^{(i)} = \hat{y}^{(i)} - y^{(i)}其中$y^{(i)}$表示真实值 定义损失函数单个样本的误差定义为 L_{single}(\hat{y}^{(i)}, y^{(i)})=\frac{1}{2}||e^{(i)}||_2^2=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2所有样本的误差定义为 L(y, t)=\frac{1}{2N}\sum_i (\hat{y}^{(i)}-y^{(i)})^2也可以定义为误差的和而不是均值，对结果无影响，可视作学习率$α$除去一个常数 梯度推导 $\frac{∂L}{∂w_j}$$= \frac{∂}{∂w_j}\frac{1}{2N}\sum_i(\hat{y}^{(i)}-y^{(i)})^2$$= \frac{1}{2N} \sum_i \frac{∂}{∂w_j} (\hat{y}^{(i)}-y^{(i)})^2$$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) \frac{∂t^{(i)}}{∂w_j}$$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)}$ 或者使用矩阵推导，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量) L = \frac{1}{2}(Xw-Y)^T(Xw-Y) ∇_w L = X^T(\hat{Y}-Y) $∇_w L$$= \frac{1}{2} ∇_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY)$$= \frac{1}{2} (2X^TXw - X^TY - X^TY)$$= X^T(Xw-Y) $ 在梯度为$\vec{0}$的点，即$∇_w L = \vec{0}$时对应最优解 X^T(Xw-Y) = 0 令X^T(Xw-Y) = 0 有X^TXw = X^TY w^*=(X^TX+\lambda I)^{-1}X^TY 其中$X^+=(X^TX+\lambda I)^{-1}X^T$，表示矩阵$X_{m×n}$的伪逆 训练采用梯度下降法求解 w := w - \alpha ∇_w L其中$w$表示参数向量 进一步思考：为什么使用梯度下降可以求取最优解呢？ ∇_w^2 L = ∇_w X^T(Xw-Y) = X^TX而对于矩阵 $ X^TX $ u^T(X^TX)u = (Xu)^T(Xu) \geq 0即损失函数的Hessian矩阵$∇_w^2 L$为正定矩阵，$L$为凸函数，存在全局最优解 从投影的角度理解线性回归 线性回归的正则化为克服过拟合问题，可加入正则化项$||w||_2^2$，此时损失函数定义为 L(\hat{y}, y)=\frac{1}{2N} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_2^2或者 L(\hat{y}, y)=\frac{1}{2N} \sum_i (\hat{y}^{(i)}-y^{(i)})^2 + \frac{\lambda}{2N}\sum_j w_j^2其中$i = 1, …, N_{sample}; j = 1, …, N_{feature},j&gt;0 $ 此时梯度为 \frac{∂L}{∂w_j} = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \frac{\lambda}{N}w_j其中$j = 1, …, N_{feature},j&gt;0 $ 局部加权线性回归目标函数定义为 L(y, t)=\frac{1}{2N}\sum_i w^{(i)} (\hat{y}^{(i)}-y^{(i)})^2其中 w^{(i)} = e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}$x$表示输入的预测样本，$x^{(i)}$表示训练样本 离很近的样本，权值接近于1，而对于离很远的样本，此时权值接近于0，这样就是在局部构成线性回归，它依赖的也只是周边的点。 对于线性回归算法，一旦拟合出适合训练数据的参数$w$，保存这些参数$w$，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。而对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数$w$），没有固定的参数$w$，所以是非参数算法。 代码@Github: Code for Linear Regression training step12345678910111213141516171819202122232425262728293031323334353637383940414243def fit(self, X, y, learning_rate=0.01, max_iter=5000, min_loss=10): # --------------- 数据预处理部分 --------------- # 加入全1列 X = np.c_[np.ones(shape=(X.shape[0])), X] # 构造高次特征 if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] # ---------------- 参数迭代部分 ---------------- # 初始化参数 self.theta = np.random.uniform(-1, 1, size=(X.shape[1],)) # 数据批次 n_batch = X.shape[0] if self.n_batch==-1 else self.n_batch batch_size = X.shape[0] // n_batch # 停止条件 n_iter = 0; loss = float(&apos;inf&apos;) # 开始迭代 for n_iter in range(max_iter): for n in range(n_batch): n1, n2 = n*batch_size, (n+1)*batch_size X_batch = X[n1: n2]; y_batch = y[n1: n2] grad = self.lossFunctionDerivative(X_batch, y_batch) self.theta -= learning_rate * grad loss = self.score(y_batch, self.predict(X_batch)) if loss &lt; min_loss: print(&apos;第%d次迭代, 第%d批数据&apos; % (n_iter, n)) print(&quot;当前总体样本损失为: &quot;, loss) return self.theta if n_iter%100 == 0: print(&apos;第%d次迭代&apos; % n_iter) print(&quot;当前总体样本损失为: &quot;, loss) print(&quot;超过迭代次数&quot;) print(&quot;当前总体样本损失为: &quot;, loss) return self.thetadef lossFunctionDerivative(self, X, y): y_pred = self.predict(X) # theta = self.theta; # ！注意：theta = self.theta 不仅仅是赋值，类似引用，修改theta会影响self.theta theta = self.theta.copy() theta[0] = 0 # θ0不需要正则化 return (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[0] predict step123456789def predict(self, X, preprocessed=False): if preprocessed: # 加入全1列 X = np.c_[np.ones(shape=(X.shape[0])), X] # 构造高次特征 if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] return X.dot(self.theta) 运行结果 无正则化 正则化]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
</search>
