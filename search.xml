<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习调参实验——序言</title>
      <link href="/2019/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E5%AE%9E%E9%AA%8C%E2%80%94%E2%80%94%E5%BA%8F%E8%A8%80/"/>
      <url>/2019/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E5%AE%9E%E9%AA%8C%E2%80%94%E2%80%94%E5%BA%8F%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>深度学习调参方法很重要，但一直没有机会做实验验证。<br>以下为一些调参时指定的超参数，以及模型设计手段，后续慢慢补齐。</p><ul><li>input size</li><li>batch size</li><li>input normalization</li><li>weight initialization</li><li>finetune</li><li>regularization / weight decay</li><li>optimizor</li><li>learing rate adjustment</li><li>batch normalization</li><li>dropout</li><li>activate function</li><li>loss</li><li>shortcut / residual</li><li>global average pooling</li></ul><h1 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h1><h2 id="总体设计"><a href="#总体设计" class="headerlink" title="总体设计"></a>总体设计</h2><p>本次实验为一个简单的回归任务，基于<code>Ising Model</code>生成图像数据集及其对应的混乱程度，利用神经网络给图像混乱程度进行打分。固定数据集划分方式、模型结构、初始化参数，针对上述方法设计对比试验。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>这里采用统计物理里非常经典的Ising模型的模拟来生成图片。</p><blockquote><p>伊辛模型<code>(Ising model)</code>是一类描述物质相变的随机过程模型。物质经过相变，要出现新的结构和物性。发生相变的系统一般是在分子之间有较强相互作用的系统，又称合作系统。—— 百度百科</p></blockquote><p>基于<code>Metropolis</code>算法对<code>Ising</code>模型的模拟，做了一些并行和随机生成图片的修改，在每次模拟的时候随机取一个时间（<code>1e3</code>到<code>1e7</code>之间）点输出到图片，图片中第一个字段是编号，第二个字段对应的分数可以大致认为是图片的有序程度，范围0~1。此外，为防止模型拟合过快，在生成的图像上添加椒盐噪声。</p><p>以下为生成的样本展示</p><ul><li>00000_0.2811850125283901.jpg<br>  <img src="/2019/02/19/深度学习调参实验——序言/00000_0.2811850125283901.jpg" alt="00000_0.2811850125283901.jpg"></li><li>00001_0.6961266452466924.jpg<br>  <img src="/2019/02/19/深度学习调参实验——序言/00001_0.6961266452466924.jpg" alt="00001_0.6961266452466924.jpg"></li><li>00002_0.9444440242248141.jpg<br>  <img src="/2019/02/19/深度学习调参实验——序言/00002_0.9444440242248141.jpg" alt="00002_0.9444440242248141.jpg"></li></ul><p>一共生成<code>1000</code>张的图片，为能导入预训练模型参数，图像分辨率与<code>ImageNet</code>数据集一致，为<code>224×224</code>。并固定训练集、验证集、测试集的划分方式，划分数量比例为<code>train: valid: test = 0.7: 0.25: 0.05</code>，标签保存在<code>.txt</code>文件中。</p><blockquote><p><a href="https://github.com/isLouisHsu/Deep-Learning-Experiments/tree/master/datasets" target="_blank" rel="noopener">Deep-Learning-Experiments/datasets at master · isLouisHsu/Deep-Learning-Experiments </a></p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>由于计算资源有限，数据量数目限制，选用<code>VGG16</code>模型时，拟合速度过快，不利于实验对比，故选择较小的模型<code>VGG11</code>，即下图中配置A。</p><p><img src="/2019/02/19/深度学习调参实验——序言/vgg.png" alt="VGG"></p><p>在此基础上对模型进行了部分改动</p><ul><li>第一层卷积层通道数由<code>3</code>改为<code>1</code></li><li>卷积层<code>(convolution layer)</code>到全连接层<code>(fully connected layer)</code>的过渡层，可选择为全局平均池化层<code>(global average pooling layer)</code></li><li>全连接层神经元数目更改，<code>512*7*7 - 4096 - 4096 - 1000</code><ul><li>若过渡层为全连接层，则更改为<code>512*7*7 - 512 - 128 - 1</code></li><li>若过渡层为全局平均池化层，则更改为<code>512 - 512 - 128 - 1</code></li></ul></li><li>最后输出层激活函数选择<code>sigmoid</code>，将结果映射到<code>[0, 1]</code>范围内</li><li>可任意配置<ul><li>是否包含批归一化层<code>(batchnorm)</code></li><li><code>dropout</code>参数</li><li>激活函数种类</li><li>参数初始化方法</li><li>是否载入预训练模型<code>(finetune)</code></li><li>是否使用全局平均赤化层<code>(global average pooling layer)</code></li></ul></li><li>参数初始化方法<ul><li>卷积核参数<ul><li><code>W</code>：<strong>可选，做对比实验</strong>。</li><li><code>b</code>：填充为<code>0</code></li></ul></li><li>批归一化参数<ul><li><code>W</code>：填充为<code>1</code></li><li><code>b</code>：填充为<code>0</code></li></ul></li><li>全连接参数<ul><li><code>W</code>：初始化后服从正态分布<code>N(0, 0.001)</code></li><li><code>b</code>：填充为<code>0</code></li></ul></li></ul></li><li>只对<code>features</code>进行<code>finetune</code></li><li>权重第一次初始化后固化，后续实验导入相同参数</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>可选损失函数有</p><ul><li><code>MSE</code></li><li><code>L1</code></li><li><code>Huber</code></li><li>…</li></ul><p>后续设计其他损失函数进行实验</p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><ol><li><p>Root Mean Squared Error(rmse)</p><script type="math/tex; mode=display">rmse = \sqrt{\frac{1}{N} \sum_{i=1}^N (y^{(i)}-\hat{y}^{(i)})^2}</script></li><li><p>Root Mean Squared Logarithmic Error(rmse_log)</p><script type="math/tex; mode=display">rmse_log = \sqrt{\frac{1}{N} \sum_{i=1}^N (\log y^{(i)}-\log \hat{y}^{(i)})^2}</script></li><li><p>Mean Absolute Error(mae)</p><script type="math/tex; mode=display">mae = \frac{1}{N} \sum_{i=1}^N | y^{(i)}-\hat{y}^{(i)} |</script></li><li><p>Mean Absolute Logarithmic Error(mae_log)</p><script type="math/tex; mode=display">mae_log = \frac{1}{N} \sum_{i=1}^N | \log y^{(i)}-\log \hat{y}^{(i)} |</script></li><li><p>Mean Absolute Relative Error(absrel)    </p><script type="math/tex; mode=display">absrel = \frac{1}{N} \sum_{i=1}^N | \frac{y^{(i)}-\hat{y}^{(i)}}{y^{(i)}} |</script></li><li><p>Mean Squared  Relative Error(sqrrel)</p><script type="math/tex; mode=display">sqrrel = \frac{1}{N} \sum_{i=1}^N | \frac{(y^{(i)}-\hat{y}^{(i)})^2}{y^{(i)}} |</script></li></ol><h1 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h1><p><a href="https://github.com/isLouisHsu/Deep-Learning-Experiments" target="_blank" rel="noopener">isLouisHsu/Deep-Learning-Experiments: Some experiments about Deep Learning</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>使用卷积神经网络做回归任务 - QQLQ - 博客园 <a href="https://www.cnblogs.com/laiqun/p/6287906.html" target="_blank" rel="noopener">https://www.cnblogs.com/laiqun/p/6287906.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 徐先生的调参日志 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python字符串格式化</title>
      <link href="/2019/02/19/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96/"/>
      <url>/2019/02/19/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Python操作字符串行云流水，当然也支持格式化字符串。</p><h1 id="通过格式符"><a href="#通过格式符" class="headerlink" title="通过格式符"></a>通过格式符</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;我叫%s, 今年%d岁&quot; % (&apos;Louis Hsu&apos;, 18))</span><br></pre></td></tr></table></figure><p>或者使用字典进行值传递<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;我叫%(name), 今年%(age)岁&quot; % &#123;&apos;name&apos;: &apos;Louis Hsu&apos;, &apos;age&apos;: 18&#125;)</span><br></pre></td></tr></table></figure></p><p><strong>typecode</strong><br>| 格式符 | 含义 |<br>| ——— | ——— |<br>| %s | 字符串 (采用str()的显示) |<br>| %r | 字符串 (采用repr()的显示) |<br>| %c | 单个字符 |<br>| %b | 二进制整数 |<br>| %d | 十进制整数 |<br>| %i | 十进制整数 |<br>| %o | 八进制整数 |<br>| %x | 十六进制整数 |<br>| %e | 指数 (基底写为e) |<br>| %E | 指数 (基底写为E) |<br>| %f | 浮点数 |<br>| %F | 浮点数，与上相同 |<br>| %g | 指数(e)或浮点数 (根据显示长度) |<br>| %G | 指数(E)或浮点数 (根据显示长度) |<br>| %% | 字符”%” | </p><p><strong>高阶</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">% [flags] [width].[precision] typecode</span><br><span class="line">- flags:        &apos;+&apos;(右对齐), &apos;-&apos;(左对齐), &apos; &apos;(左侧填充一个空格，与负数对齐), &apos;0&apos;(用0填充)</span><br><span class="line">- width:        显示宽度</span><br><span class="line">- precision:    小数精度位数，可使用&apos;*&apos;进行动态代入</span><br><span class="line">- typecode:     格式符</span><br></pre></td></tr></table></figure></p><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;pi is %+2.2f&apos; % (3.1415926)) -&gt; pi is +3.14</span><br><span class="line">print(&apos;pi is %-2.2f&apos; % (3.1415926)) -&gt; pi is 3.14</span><br><span class="line">print(&apos;pi is % 2.2f&apos; % (3.1415926)) -&gt; pi is  3.14</span><br><span class="line">print(&apos;pi is %02.2f&apos; % (3.1415926)) -&gt; pi is 3.14</span><br><span class="line"></span><br><span class="line"># 同</span><br><span class="line">print(&apos;pi is %+*.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is %-*.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is % *.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is %0*.*f&apos; % (2, 2, 3.1415926))</span><br></pre></td></tr></table></figure></p><h1 id="通过format"><a href="#通过format" class="headerlink" title="通过format"></a>通过format</h1><h2 id="位置传递"><a href="#位置传递" class="headerlink" title="位置传递"></a>位置传递</h2><ol><li><p>使用位置参数</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; li = [&apos;hoho&apos;,18]</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;&#125; ,age &#123;&#125;&apos;.format(&apos;hoho&apos;,18)</span><br><span class="line">&apos;my name is hoho ,age 18&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;1&#125; ,age &#123;0&#125;&apos;.format(10,&apos;hoho&apos;)</span><br><span class="line">&apos;my name is hoho ,age 10&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;1&#125; ,age &#123;0&#125; &#123;1&#125;&apos;.format(10,&apos;hoho&apos;)</span><br><span class="line">&apos;my name is hoho ,age 10 hoho&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;&#125; ,age &#123;&#125;&apos;.format(*li)</span><br><span class="line">&apos;my name is hoho ,age 18&apos;</span><br></pre></td></tr></table></figure></li><li><p>使用关键字参数</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; hash = &#123;&apos;name&apos;:&apos;hoho&apos;,&apos;age&apos;:18&#125;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;name&#125;,age is &#123;age&#125;&apos;.format(name=&apos;hoho&apos;,age=19)</span><br><span class="line">&apos;my name is hoho,age is 19&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;name&#125;,age is &#123;age&#125;&apos;.format(**hash)</span><br><span class="line">&apos;my name is hoho,age is 18&apos;</span><br></pre></td></tr></table></figure></li></ol><h2 id="格式限定"><a href="#格式限定" class="headerlink" title="格式限定"></a>格式限定</h2><p>基本格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;[:pad][align][sign][typecode]&#125;</span><br><span class="line">- :pad :    填充字符，空白位用该字符填充</span><br><span class="line">- align:    &apos;^&apos;, &apos;&lt;&apos;, &apos;&gt;&apos; 分别表示 &apos;居中&apos;, &apos;左对齐&apos;, &apos;右对齐&apos;(默认)，后面加宽度</span><br><span class="line">- sign :    &apos;+&apos;, &apos;-&apos; , &apos; &apos; 分别表示 &apos;正&apos;, &apos;负&apos;, &apos;正数前加空格&apos;</span><br><span class="line">- typecode: &apos;b&apos;, &apos;d&apos;, &apos;o&apos;, &apos;x&apos;, &apos;f&apos;, &apos;,&apos;, &apos;%&apos;, &apos;e&apos; 分别表示 &apos;二进制&apos;, &apos;十进制&apos;, &apos;八进制&apos;, &apos;十六进制&apos;, &apos;浮点数&apos;, &apos;逗号分隔&apos;, &apos;百分比格式&apos;,  &apos;指数记法&apos;</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>python之字符串格式化(format) - benric - 博客园 <a href="https://www.cnblogs.com/benric/p/4965224.html" target="_blank" rel="noopener">https://www.cnblogs.com/benric/p/4965224.html</a><br>Python format 格式化函数 | 菜鸟教程 <a href="http://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener">http://www.runoob.com/python/att-string-format.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python命令行参数解析</title>
      <link href="/2019/02/18/Python%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/02/18/Python%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="C-C-的参数传递"><a href="#C-C-的参数传递" class="headerlink" title="C/C++的参数传递"></a>C/C++的参数传递</h1><p>我们知道C/C++主函数形式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc,char * argv[],char * envp[])</span><br><span class="line">&#123;</span><br><span class="line">    // do something</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中各参数含义如下</p><ul><li><code>argc</code>：<code>argument count</code>，表示参数数量</li><li><code>argv</code>：<code>argument value</code>，表示参数值<br>  最后一个元素存放了一个NULL的指针</li><li><code>envp</code>：系统环境变量<br>  最后一个元素存放了一个NULL的指针</li></ul><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main(int argc,char * argv[],char * envp[])</span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;argc is %d \n&quot;, argc);</span><br><span class="line"> </span><br><span class="line">    int i;</span><br><span class="line">    for (i=0; i&lt;argc; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;arcv[%d] is %s\n&quot;, i, argv[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for (i=0; envp[i]!=NULL; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;envp[%d] is %s\n&quot;, i, envp[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试平台为Windows10，执行编译和运行操作，结果如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; gcc main.c -o main.exe</span><br><span class="line">&gt; ./main.exe param1 param2 param3 param4</span><br><span class="line">argc is 5</span><br><span class="line">arcv[0] is C:\OneDrive\▒ĵ▒\Louis&apos; Blog\source\_drafts\Python▒▒▒▒▒в▒▒▒▒▒▒▒\test.exe</span><br><span class="line">arcv[1] is param1</span><br><span class="line">arcv[2] is param2</span><br><span class="line">arcv[3] is param3</span><br><span class="line">arcv[4] is param4</span><br><span class="line">envp[0] is ACLOCAL_PATH=C:\MyApplications\Git\mingw64\share\aclocal;C:\MyApplica                                                                                                                tions\Git\usr\share\aclocal</span><br><span class="line">envp[1] is ALLUSERSPROFILE=C:\ProgramData</span><br><span class="line">...(省略)</span><br><span class="line">envp[71] is WINDIR=C:\WINDOWS</span><br><span class="line">envp[72] is _=./main.exe</span><br></pre></td></tr></table></figure></p><h1 id="Python的参数传递"><a href="#Python的参数传递" class="headerlink" title="Python的参数传递"></a>Python的参数传递</h1><p>可以使用sys模块得到命令行参数，主函数文件<code>main.py</code>如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    print(sys.argv)</span><br></pre></td></tr></table></figure></p><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; python main.py param1 param2 pram3</span><br><span class="line">[&apos;main.py&apos;, &apos;param1&apos;, &apos;param2&apos;, &apos;pram3&apos;]</span><br></pre></td></tr></table></figure></p><h1 id="getopt"><a href="#getopt" class="headerlink" title="getopt"></a>getopt</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import getopt</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    argv = sys.argv</span><br><span class="line">    </span><br><span class="line">    if len(argv) == 1:</span><br><span class="line">        print(</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            Usage: python main.py [option]</span><br><span class="line">            -h or --help:    显示帮助信息</span><br><span class="line">            -v or --version: 显示版本</span><br><span class="line">            -i or --input:   指定输入文件路径</span><br><span class="line">            -o or --output:  指定输出文件路径</span><br><span class="line"></span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        opts, args = getopt.getopt(args=argv[1:],</span><br><span class="line">                                   shortopts=&apos;hvi:o:&apos;,</span><br><span class="line">                                   longopts=[&apos;help&apos;, &apos;version&apos;, &apos;input=&apos;, &apos;output=&apos;]</span><br><span class="line">                                )</span><br><span class="line">    except getopt.GetoptError:</span><br><span class="line">        print(&quot;argv error,please input&quot;)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    for cmd, arg in opts:</span><br><span class="line"></span><br><span class="line">        if cmd in [&apos;-h&apos;, &apos;--help&apos;]:</span><br><span class="line">            print(&quot;help info&quot;)</span><br><span class="line">            sys.exit(0)</span><br><span class="line">        elif cmd in [&apos;-v&apos;, &apos;--version&apos;]:</span><br><span class="line">            print(&quot;main 1.0&quot;)</span><br><span class="line">            sys.exit(0)</span><br><span class="line"></span><br><span class="line">        if cmd in [&apos;-i&apos;, &apos;--input&apos;]:</span><br><span class="line">            input = arg</span><br><span class="line">        if cmd in [&apos;-o&apos;, &apos;--output&apos;]:</span><br><span class="line">            output = arg</span><br></pre></td></tr></table></figure><p>说明</p><ul><li><p><code>args=sys.argv[1:]</code><br>  传入的参数，除去<code>sys.argv[0]</code>，即主函数文件路径</p></li><li><p><code>shortopts=&#39;hvi:o:&#39;</code><br>  字符串，支持形如<code>-h</code>的选项</p><ul><li>若无需指定参数，形如<code>c</code>；</li><li>若必须指定参数，则需为<code>c:</code>；</li></ul></li><li><p><code>longopts=[&#39;help&#39;, &#39;version&#39;, &#39;input=&#39;, &#39;output=&#39;]</code><br>  字符串列表，可选参数，是否支持形如<code>--help</code>的选项</p><ul><li>若无需指定参数，形如<code>cmd</code>；</li><li>若必须指定参数，则需为<code>cmd=</code>；</li></ul></li></ul><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt; python main.py</span><br><span class="line"></span><br><span class="line">            Usage: python main.py [option]</span><br><span class="line">            -h or --help:    显示帮助信息</span><br><span class="line">            -v or --version: 显示版本</span><br><span class="line">            -i or --input:   指定输入文件路径</span><br><span class="line">            -o or --output:  指定输出文件路径</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt; python main.py -h</span><br><span class="line">help info</span><br><span class="line"></span><br><span class="line">&gt; python main.py -v</span><br><span class="line">main 1.0</span><br><span class="line"></span><br><span class="line">&gt; python main.py -i</span><br><span class="line">argv error,please input</span><br><span class="line"></span><br><span class="line">&gt; python main.py -i a.txt -o b.txt</span><br></pre></td></tr></table></figure></p><h1 id="argsparse"><a href="#argsparse" class="headerlink" title="argsparse"></a>argsparse</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">def show_args(args):</span><br><span class="line">    if args.opencv:</span><br><span class="line">        print(&quot;opencv is used &quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;opencv is not used &quot;)</span><br><span class="line"></span><br><span class="line">    print(args.steps)</span><br><span class="line">    print(args.file)</span><br><span class="line">    print(args.data)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = argparse.ArgumentParser(description=&quot;learn to use `argparse`&quot;)</span><br><span class="line"></span><br><span class="line">    # 标志位</span><br><span class="line">    parser.add_argument(&apos;--opencv&apos;, &apos;-cv&apos;, action=&apos;store_true&apos;, help=&apos;use opencv if set &apos;)</span><br><span class="line">    # 必需参数</span><br><span class="line">    parser.add_argument(&apos;--steps&apos;, &apos;-s&apos;, required=True, type=int, help=&apos;number of steps&apos;)</span><br><span class="line">    # 默认参数</span><br><span class="line">    parser.add_argument(&apos;--file&apos;, &apos;-f&apos;, default=&apos;a.txt&apos;)</span><br><span class="line">    # 候选参数</span><br><span class="line">    parser.add_argument(&apos;--data&apos;, &apos;-d&apos;, choices=[&apos;data1&apos;, &apos;data2&apos;])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    show_args(args)</span><br></pre></td></tr></table></figure><p>说明</p><ul><li>帮助信息<br>  参数<code>help</code>，用于显示在<code>-h</code>帮助信息中</li><li>标志位参数<br>  参数<code>action=&#39;store_true&#39;</code>，即保存该参数为<code>True</code></li><li>必需参数<br>  置位<code>required</code>，即运行该程序必须带上该参数，否则报错</li><li>默认参数<br>  参数<code>default</code>填写默认参数</li><li>候选参数<br>  参数<code>choices</code>填写候选参数列表</li></ul><p>运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># 显示帮助信息</span><br><span class="line">&gt; python main.py -h</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line"></span><br><span class="line">learn to use `argparse`</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message and exit</span><br><span class="line">  --opencv, -cv         use opencv if set</span><br><span class="line">  --steps STEPS, -s STEPS</span><br><span class="line">                        number of steps</span><br><span class="line">  --file FILE, -f FILE</span><br><span class="line">  --data &#123;data1,data2&#125;, -d &#123;data1,data2&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 测试必须参数</span><br><span class="line">&gt; python main.py</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line">main.py: error: the following arguments are required: --steps/-s</span><br><span class="line"></span><br><span class="line">&gt; python main.py -s 100</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试标志位参数</span><br><span class="line">&gt; python main.py -s 100 -cv</span><br><span class="line">opencv is used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试默认参数</span><br><span class="line">&gt; python main.py -s 100 -f b.txt</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">b.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试可选参数</span><br><span class="line">&gt; python main.py -s 100 -d data1</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">data1</span><br><span class="line"></span><br><span class="line">&gt; python main.py -s 100 -d data0</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line">main.py: error: argument --data/-d: invalid choice: &apos;data0&apos; (choose from &apos;data1&apos;, &apos;data2&apos;)</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Python命令行参数解析：getopt和argparse - 死胖子的博客 - CSDN博客 <a href="https://blog.csdn.net/lanzheng_1113/article/details/77574446" target="_blank" rel="noopener">https://blog.csdn.net/lanzheng_1113/article/details/77574446</a><br>Python模块之命令行参数解析 - 每天进步一点点！！！ - 博客园 <a href="https://www.cnblogs.com/madsnotes/articles/5687079.html" target="_blank" rel="noopener">https://www.cnblogs.com/madsnotes/articles/5687079.html</a><br>Python解析命令行读取参数 — argparse模块 - Arkenstone - 博客园 <a href="https://www.cnblogs.com/arkenstone/p/6250782.html" target="_blank" rel="noopener">https://www.cnblogs.com/arkenstone/p/6250782.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python生成词云图</title>
      <link href="/2019/02/17/Python%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91%E5%9B%BE/"/>
      <url>/2019/02/17/Python%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一个没什么用的小技能</p><h1 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h1><blockquote><p>wordcloud · PyPI <a href="https://pypi.org/project/wordcloud/" target="_blank" rel="noopener">https://pypi.org/project/wordcloud/</a></p></blockquote><p>安装该模块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; pip install wordcloud</span><br></pre></td></tr></table></figure></p><p>主要用到的为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wordcloud.WordCloud(font_path=None, width=400, height=200, margin=2,</span><br><span class="line">                ranks_only=None, prefer_horizontal=.9, mask=None, scale=1,</span><br><span class="line">                color_func=None, max_words=200, min_font_size=4,</span><br><span class="line">                stopwords=None, random_state=None, background_color=&apos;black&apos;,</span><br><span class="line">                max_font_size=None, font_step=1, mode=&quot;RGB&quot;,</span><br><span class="line">                relative_scaling=&apos;auto&apos;, regexp=None, collocations=True,</span><br><span class="line">                colormap=None, normalize_plurals=True, contour_width=0,</span><br><span class="line">                contour_color=&apos;black&apos;, repeat=False)</span><br></pre></td></tr></table></figure></p><h1 id="使用例程"><a href="#使用例程" class="headerlink" title="使用例程"></a>使用例程</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">from wordcloud import WordCloud</span><br><span class="line"></span><br><span class="line">font = &apos;C:/Windows/Fonts/SIMYOU.TTF&apos;    # 幼圆</span><br><span class="line">string = &apos;LouisHsu 单键 小叔叔 想静静 95后 傲娇 skrrrrrrr 大猫座 佛了 要秃 嘤嘤嘤 真香&apos;</span><br><span class="line"></span><br><span class="line">mask = cv2.imread(&apos;./mask.jpg&apos;, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">thresh, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">        font_path=font, </span><br><span class="line">        background_color=&apos;white&apos;,</span><br><span class="line">        color_func=lambda *args, **kwargs: (0,0,0),</span><br><span class="line">        mask=mask,</span><br><span class="line">        max_words=500,</span><br><span class="line">        min_font_size=4,</span><br><span class="line">        max_font_size=None,</span><br><span class="line">        contour_width=1,</span><br><span class="line">        repeat=True                     # 允许词重复</span><br><span class="line">    )</span><br><span class="line">wc.generate_from_text(string)</span><br><span class="line">wc.to_file(&apos;./wc.jpg&apos;)                  #保存图片</span><br></pre></td></tr></table></figure><p>输入原图为<br><img src="/2019/02/17/Python生成词云图/mask.jpg" alt="mask"></p><p>生成图像<br><img src="/2019/02/17/Python生成词云图/wc.jpg" alt="wc"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>python WordCloud 简单实例 - 博客 - CSDN博客 <a href="https://blog.csdn.net/cy776719526/article/details/80171790" target="_blank" rel="noopener">https://blog.csdn.net/cy776719526/article/details/80171790</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>makefile简单教程</title>
      <link href="/2019/01/05/makefile%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B/"/>
      <url>/2019/01/05/makefile%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="准备源文件"><a href="#准备源文件" class="headerlink" title="准备源文件"></a>准备源文件</h1><p>新建目录<code>demo/</code>，其结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\-- demo</span><br><span class="line">    \-- bin             # 二进制文件，即可执行文件</span><br><span class="line">    \-- include         # 头文件`.h`</span><br><span class="line">        |-- hello.h</span><br><span class="line">    \-- obj             # 目标文件`.o`</span><br><span class="line">        |-- *.o</span><br><span class="line">    \-- src             # 源文件`.c`</span><br><span class="line">        |-- hello.c</span><br><span class="line">        |-- test.c</span><br></pre></td></tr></table></figure></p><p>编辑<code>hello.h</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#ifndef __HELLO_H</span><br><span class="line">#define __HELLO_H</span><br><span class="line"></span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">void __hello();</span><br><span class="line"></span><br><span class="line">#endif</span><br></pre></td></tr></table></figure></p><p>编辑<code>hello.c</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;hello.h&quot;</span><br><span class="line"></span><br><span class="line">void __hello()</span><br><span class="line">&#123;</span><br><span class="line">printf(&quot;Hello world!\n&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>编辑<code>test.h</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;hello.h&quot;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">__hello();</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="命令行编译"><a href="#命令行编译" class="headerlink" title="命令行编译"></a>命令行编译</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test.c</span><br><span class="line">$ gcc test.c hello.c -o test</span><br><span class="line">$ ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test  test.c</span><br><span class="line">$ ./test</span><br><span class="line">Hello world!</span><br></pre></td></tr></table></figure><p>但是这样编译会每次都重新编译整个工程，时间比较长，所以可以先生成<code>.o</code>文件，当<code>test.c</code>代码改动后，重新生成<code>test.o</code>即可，<code>hello.o</code>不用重新编译<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test.c</span><br><span class="line">$ gcc test.c hello.c -c</span><br><span class="line">$ ls</span><br><span class="line">hello.c  hello.h  hello.o  makefile  README.md  test.c  test.o</span><br><span class="line">$ gcc test.o hello.o -o test</span><br><span class="line">$ ls</span><br><span class="line">hello.c  hello.h  hello.o  makefile  README.md  test  test.c  test.o</span><br><span class="line">$ ./test</span><br><span class="line">Hello world!</span><br></pre></td></tr></table></figure></p><h1 id="Makefile"><a href="#Makefile" class="headerlink" title="Makefile"></a>Makefile</h1><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;target&gt;: &lt;dependencies&gt;</span><br><span class="line">&lt;command&gt;# [TAB]&lt;command&gt;</span><br></pre></td></tr></table></figure><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test: test.c</span><br><span class="line">gcc test.c -o test</span><br></pre></td></tr></table></figure></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><p>编辑<code>Makefile</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># directories &amp; target name</span><br><span class="line">DIR_INC = ./include  </span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line">DIR_BIN = ./bin</span><br><span class="line">TARGET  = test</span><br><span class="line"></span><br><span class="line"># compile macro  </span><br><span class="line">CC      = gcc</span><br><span class="line">CFLAGS  = -g -Wall -I$&#123;DIR_INC&#125;                 # `-g`表示调试选项，`-Wall`表示编译后显示所有警告</span><br><span class="line"></span><br><span class="line"># load source files</span><br><span class="line">SRC = $(wildcard $&#123;DIR_SRC&#125;/*.c)                         # 匹配目录中所有的`.c`文件</span><br><span class="line"></span><br><span class="line"># build target</span><br><span class="line">OBJ = $(patsubst %.c, $&#123;DIR_OBJ&#125;/%.o, $&#123;notdir $&#123;SRC&#125;&#125;)  # 由`SRC`字符串内容，指定生成`.o`文件的名称与目录</span><br><span class="line">BIN = $&#123;DIR_BIN&#125;/$&#123;TARGET&#125;                               # 指定可执行文件名称与目录</span><br><span class="line"></span><br><span class="line"># build</span><br><span class="line">$&#123;BIN&#125;: $&#123;OBJ&#125;</span><br><span class="line">$(CC) $(OBJ) -o $@                               # 即 `$ gcc ./obj/*.o -o ./bin/test`</span><br><span class="line">$&#123;DIR_OBJ&#125;/%.o: $&#123;DIR_SRC&#125;/%.c</span><br><span class="line">$(CC) $(CFLAGS) -c $&lt; -o $@                      # 即 `$ gcc ./src/*.c -g -Wall -I./include -c ./obj/*.o`</span><br><span class="line"></span><br><span class="line"># clean</span><br><span class="line">.PHONY: clean                                            # 伪目标</span><br><span class="line">clean:</span><br><span class="line">find $&#123;DIR_OBJ&#125; -name *.o -exec rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></figure></p><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ make</span><br><span class="line">gcc -g -Wall -I./include   -c src/hello.c -o obj/hello.o</span><br><span class="line">gcc -g -Wall -I./include   -c src/test.c -o obj/test.o</span><br><span class="line">gcc  ./obj/hello.o  ./obj/test.o  -o bin/test</span><br><span class="line">$ ls obj/</span><br><span class="line">hello.o  test.o</span><br><span class="line">$ ls bin/</span><br><span class="line">test</span><br><span class="line">$ ./bin/test </span><br><span class="line">Hello world!</span><br><span class="line">$ make clean</span><br><span class="line">find ./obj -name *.o -exec rm -rf &#123;&#125; \;</span><br><span class="line">$ ls obj/</span><br><span class="line">$ ls bin/</span><br><span class="line">test</span><br></pre></td></tr></table></figure></p><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><ol><li>符号 <code>$@</code>, <code>$^</code>, <code>$&lt;</code>，<code>$?</code><ul><li><code>$@</code>: 表示目标文件</li><li><code>$^</code>: 表示所有的依赖文件</li><li><code>$&lt;</code>: 表示第一个依赖文件</li><li><code>$?</code>: 表示比目标还要新的依赖文件列表</li></ul></li><li><code>wildcard</code>，<code>notdir</code>，<code>patsubst</code><ul><li><code>wildcard</code>    : 扩展通配符<br>  <code>SOURCES = $(wildcard *.c)</code>: 产生一个所有以 ’.c’ 结尾的文件的列表，然后存入变量 SOURCES 里。 </li><li><code>notdir</code>    : 去除路径，可以在使用<code>wildcard</code>函数后，再配合使用<code>notdir</code>函数只得到文件名（不含路径）。</li><li><code>patsubst</code>    : 替换通配符，需要３个参数，第一个是个需要匹配的式样，第二个表示用什么来替换他，第三个是个需要被处理的由空格分隔的字列。<br>  <code>OBJS = $(patsubst %.c,%.o,$(SOURCES))</code><pre><code>  - 将处理所有在 SOURCES 字列中的字（一列文件名），如果他的 结尾是 `.c` ，就用 `.o` 把 `.c`取代  - 这里的 % 符号将匹配一个或多个字符，而他每次所匹配的字串叫做一个‘柄’(stem)   - 在第二个参数里， %被解读成用第一参数所匹配的那个柄。</code></pre></li></ul></li><li><code>-I</code>，<code>-L</code>，<code>-l</code><ul><li><code>-I</code>: 将指定目录作为第一个寻找头文件的目录</li><li><code>-L</code>: 将指定目录作为第一个寻找库文件的目录</li><li><code>-l</code>: 在库文件路径中寻找<code>.so</code>动态库文件（如果gcc编译选项中加入了<code>-static</code>表示寻找<code>.a</code>静态库文件）</li></ul></li><li><code>.PHONY</code>后面的<code>target</code>表示的也是一个伪造的<code>target</code>, 而不是真实存在的文件<code>target</code>，注意<code>Makefile</code>的<code>target</code>默认是文件。</li></ol><h2 id="关于三个函数的使用"><a href="#关于三个函数的使用" class="headerlink" title="关于三个函数的使用"></a>关于三个函数的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DIR_INC = ./include</span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line"></span><br><span class="line">SRC = $(wildcard $&#123;DIR_SRC&#125;/*.c)# 指定编译当前目录下所有`.c`文件，全路径`./src/*.c`</span><br><span class="line">DIR = $(notdir $&#123;SRC&#125;)# 去除路径名，只留下文件名`*.c`</span><br><span class="line">OBJ = $(patsubst %.c, $&#123;DIR_OBJ&#125;/%.o, $&#123;DIR&#125;)# 将`DIR`中匹配到的`%.c`，替换为`$&#123;DIR_OBJ&#125;/%.o`</span><br><span class="line"></span><br><span class="line">ALL:</span><br><span class="line">@echo $(SRC)</span><br><span class="line">@echo $(DIR)</span><br><span class="line">@echo $(OBJ)</span><br></pre></td></tr></table></figure><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ make</span><br><span class="line">./src/hello.c ./src/test.c</span><br><span class="line">hello.c test.c</span><br><span class="line">./obj/hello.o ./obj/test.o</span><br></pre></td></tr></table></figure></p><blockquote><p>注：若<code>./src</code>目录下还有子目录<code>./src/inc</code>，则<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SRC = $(wildcard $&#123;DIR_SRC&#125;/*.c) $(wildcard $&#123;DIR_SRC&#125;/inc/*.c)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Makefile 使用总结 - wang_yb - 博客园 <a href="https://www.cnblogs.com/wang_yb/p/3990952.html" target="_blank" rel="noopener">https://www.cnblogs.com/wang_yb/p/3990952.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Cmake编译库文件</title>
      <link href="/2019/01/05/Cmake%E7%BC%96%E8%AF%91%E5%BA%93%E6%96%87%E4%BB%B6/"/>
      <url>/2019/01/05/Cmake%E7%BC%96%E8%AF%91%E5%BA%93%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>库文件即源代码的二进制文件，我们通常把一些公用函数制作成函数库，供其它程序使用。函数库分为静态库和动态库两种。静态库在程序编译时会被连接到目标代码中，程序运行时将不再需要该静态库；动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此在程序运行时还需要动态库存在。</p><h1 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h1><p>以DarkNet为例，我们将其源代码编译成<code>.a</code>静态库文件。</p><ol><li><p>下载源码</p><blockquote><p>YOLO: Real-Time Object Detection <a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">https://pjreddie.com/darknet/yolo/</a></p></blockquote> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/pjreddie/darknet</span><br></pre></td></tr></table></figure></li><li><p>整理文件<br> 我们将<code>include/</code>与<code>src/</code>目录复制到新建文件夹<code>darknet/</code>。</p></li><li><p>在<code>darknet/</code>目录下编写<code>CmakeLists.txt</code>文件，内容如下</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">CMAKE_MINIMUM_REQUIRED(VERSION 2.8)                                # cmake需要的最小版本号</span><br><span class="line">PROJECT(darknet)                                        # 项目名</span><br><span class="line"></span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;----------------------------------------------------------------------&quot;</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;project name:      &quot; $&#123;PROJECT_NAME&#125;           # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;source directory:  &quot; $&#123;PROJECT_SOURCE_DIR&#125;     # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;binary directory:  &quot; $&#123;PROJECT_BINARY_DIR&#125;     # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;----------------------------------------------------------------------&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">INCLUDE_DIRECTORIES(                                # 头文件目录</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/include</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/src</span><br><span class="line">)                                                   </span><br><span class="line">AUX_SOURCE_DIRECTORY(                               # 源文件</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/src </span><br><span class="line">    lib_srcfile</span><br><span class="line">)                                                   </span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)  # 设置保存`.a`的目录</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">ADD_LIBRARY(                                        # 生成库文件，可选择`.a`或`.so`</span><br><span class="line">    $&#123;PROJECT_NAME&#125;</span><br><span class="line">    STATIC                                          # `.a`</span><br><span class="line">    # SHARED                                        # `.so`</span><br><span class="line">    $&#123;lib_srcfile&#125;</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">SET_TARGET_PROPERTIES(</span><br><span class="line">    $&#123;PROJECT_NAME&#125;</span><br><span class="line">    PROPERTIES</span><br><span class="line">    LINKER_LANGUAGE C</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>执行命令<br> 我们在<code>darknet/</code>目录打开终端</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir build</span><br><span class="line">$ cd build</span><br><span class="line">$ cmake ..</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - done</span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - done</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - done</span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - done</span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/louishsu/Work/Codes/makefile/test/build</span><br><span class="line">louishsu@ThinkPad-T440s:~/Work/Codes/makefile/test/build$ make</span><br><span class="line">Scanning dependencies of target Test</span><br><span class="line">[ 50%] Building C object CMakeFiles/Test.dir/src/test.c.o</span><br><span class="line">[100%] Linking C executable Test</span><br><span class="line">[100%] Built target Test</span><br><span class="line">louishsu@ThinkPad-T440s:~/Work/Codes/makefile/test/build$ cd ../../darknet/build/</span><br><span class="line">louishsu@ThinkPad-T440s:~/Work/Codes/makefile/darknet/build$ rm -rf *</span><br><span class="line">louishsu@ThinkPad-T440s:~/Work/Codes/makefile/darknet/build$ cmake ..</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - done</span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - done</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - done</span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - done</span><br><span class="line">-- ----------------------------------------------------------------------</span><br><span class="line">-- project name:      darknet</span><br><span class="line">-- source directory:  /home/louishsu/Work/Codes/makefile/darknet</span><br><span class="line">-- binary directory:  /home/louishsu/Work/Codes/makefile/darknet/build</span><br><span class="line">-- ----------------------------------------------------------------------</span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/louishsu/Work/Codes/makefile/darknet/build</span><br><span class="line">$ make</span><br><span class="line">Scanning dependencies of target darknet</span><br><span class="line">[  2%] Building C object CMakeFiles/darknet.dir/src/gemm.c.o</span><br><span class="line">[  4%] Building C object CMakeFiles/darknet.dir/src/reorg_layer.c.o</span><br><span class="line">[  6%] Building C object CMakeFiles/darknet.dir/src/logistic_layer.c.o</span><br><span class="line">[  8%] Building C object CMakeFiles/darknet.dir/src/activation_layer.c.o</span><br><span class="line">[ 10%] Building C object CMakeFiles/darknet.dir/src/network.c.o</span><br><span class="line">[ 12%] Building C object CMakeFiles/darknet.dir/src/region_layer.c.o</span><br><span class="line">[ 14%] Building C object CMakeFiles/darknet.dir/src/compare.c.o</span><br><span class="line">[ 16%] Building C object CMakeFiles/darknet.dir/src/yolo_layer.c.o</span><br><span class="line">[ 18%] Building C object CMakeFiles/darknet.dir/src/data.c.o</span><br><span class="line">[ 20%] Building C object CMakeFiles/darknet.dir/src/route_layer.c.o</span><br><span class="line">[ 22%] Building C object CMakeFiles/darknet.dir/src/detection_layer.c.o</span><br><span class="line">[ 25%] Building C object CMakeFiles/darknet.dir/src/list.c.o</span><br><span class="line">[ 27%] Building C object CMakeFiles/darknet.dir/src/option_list.c.o</span><br><span class="line">[ 29%] Building C object CMakeFiles/darknet.dir/src/activations.c.o</span><br><span class="line">[ 31%] Building C object CMakeFiles/darknet.dir/src/maxpool_layer.c.o</span><br><span class="line">[ 33%] Building C object CMakeFiles/darknet.dir/src/dropout_layer.c.o</span><br><span class="line">[ 35%] Building C object CMakeFiles/darknet.dir/src/cost_layer.c.o</span><br><span class="line">[ 37%] Building C object CMakeFiles/darknet.dir/src/crop_layer.c.o</span><br><span class="line">[ 39%] Building C object CMakeFiles/darknet.dir/src/convolutional_layer.c.o</span><br><span class="line">[ 41%] Building C object CMakeFiles/darknet.dir/src/softmax_layer.c.o</span><br><span class="line">[ 43%] Building C object CMakeFiles/darknet.dir/src/parser.c.o</span><br><span class="line">[ 45%] Building C object CMakeFiles/darknet.dir/src/utils.c.o</span><br><span class="line">[ 47%] Building C object CMakeFiles/darknet.dir/src/box.c.o</span><br><span class="line">[ 50%] Building C object CMakeFiles/darknet.dir/src/batchnorm_layer.c.o</span><br><span class="line">[ 52%] Building C object CMakeFiles/darknet.dir/src/iseg_layer.c.o</span><br><span class="line">[ 54%] Building C object CMakeFiles/darknet.dir/src/lstm_layer.c.o</span><br><span class="line">[ 56%] Building C object CMakeFiles/darknet.dir/src/connected_layer.c.o</span><br><span class="line">[ 58%] Building C object CMakeFiles/darknet.dir/src/deconvolutional_layer.c.o</span><br><span class="line">[ 60%] Building C object CMakeFiles/darknet.dir/src/image.c.o</span><br><span class="line">[ 62%] Building C object CMakeFiles/darknet.dir/src/im2col.c.o</span><br><span class="line">[ 64%] Building C object CMakeFiles/darknet.dir/src/col2im.c.o</span><br><span class="line">[ 66%] Building C object CMakeFiles/darknet.dir/src/l2norm_layer.c.o</span><br><span class="line">[ 68%] Building C object CMakeFiles/darknet.dir/src/demo.c.o</span><br><span class="line">[ 70%] Building C object CMakeFiles/darknet.dir/src/cuda.c.o</span><br><span class="line">[ 72%] Building C object CMakeFiles/darknet.dir/src/matrix.c.o</span><br><span class="line">[ 75%] Building C object CMakeFiles/darknet.dir/src/upsample_layer.c.o</span><br><span class="line">[ 77%] Building C object CMakeFiles/darknet.dir/src/rnn_layer.c.o</span><br><span class="line">[ 79%] Building C object CMakeFiles/darknet.dir/src/tree.c.o</span><br><span class="line">[ 81%] Building C object CMakeFiles/darknet.dir/src/gru_layer.c.o</span><br><span class="line">[ 83%] Building C object CMakeFiles/darknet.dir/src/crnn_layer.c.o</span><br><span class="line">[ 85%] Building C object CMakeFiles/darknet.dir/src/normalization_layer.c.o</span><br><span class="line">[ 87%] Building C object CMakeFiles/darknet.dir/src/layer.c.o</span><br><span class="line">[ 89%] Building C object CMakeFiles/darknet.dir/src/shortcut_layer.c.o</span><br><span class="line">[ 91%] Building C object CMakeFiles/darknet.dir/src/avgpool_layer.c.o</span><br><span class="line">[ 93%] Building C object CMakeFiles/darknet.dir/src/local_layer.c.o</span><br><span class="line">[ 95%] Building CXX object CMakeFiles/darknet.dir/src/image_opencv.cpp.o</span><br><span class="line">[ 97%] Building C object CMakeFiles/darknet.dir/src/blas.c.o</span><br><span class="line">[100%] Linking C static library lib/libdarknet.a</span><br><span class="line">[100%] Built target darknet</span><br></pre></td></tr></table></figure></li></ol><p>在<code>darknet/build/lib</code>目录下即可找到<code>libdarknet.a</code>库文件，当前目录结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">\-- darknet</span><br><span class="line">    \-- include</span><br><span class="line">        |-- darknet.h</span><br><span class="line">    \-- src</span><br><span class="line">        |-- *.h</span><br><span class="line">        |-- *.c</span><br><span class="line">    \--build</span><br><span class="line">        \-- lib</span><br><span class="line">            |-- libdarknet.a(.so)</span><br><span class="line">    |-- CMakeLists.txt</span><br></pre></td></tr></table></figure></p><h1 id="调用库函数"><a href="#调用库函数" class="headerlink" title="调用库函数"></a>调用库函数</h1><p>为测试该库函数是否编译成功，编写测试代码，新建目录<code>/test/</code>，其文件结构为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\-- test</span><br><span class="line">    \-- include</span><br><span class="line">        |-- test.h</span><br><span class="line">    \-- src</span><br><span class="line">        |-- test.c</span><br><span class="line">    \-- build</span><br><span class="line">        |-- test</span><br><span class="line">    |-- CMakeLists.txt</span><br></pre></td></tr></table></figure></p><p>头文件<code>/include/test.h</code>内容为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#ifndef TEST_H</span><br><span class="line">#define TEST_H</span><br><span class="line"></span><br><span class="line">#include &quot;darknet.h&quot;</span><br><span class="line"></span><br><span class="line">#endif</span><br></pre></td></tr></table></figure></p><p>源文件<code>/src/test.c</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;test.h&quot;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">printf(&quot;Hello! Darknet!\n&quot;);</span><br><span class="line">matrix M = make_matrix(4, 5);</span><br><span class="line">printf(&quot;The size of matrix M is %ld bytes\n&quot;, sizeof(M));</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>编译文件<code>CMakeLists.txt</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">CMAKE_MINIMUM_REQUIRED(VERSION 2.8)                        # cmake需要的最小版本号</span><br><span class="line">PROJECT(Test)                               # 项目名</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">SET(DARKNET ../darknet)</span><br><span class="line">INCLUDE_DIRECTORIES(                                            # 头文件目录</span><br><span class="line">$&#123;DARKNET&#125;/include</span><br><span class="line">$&#123;DARKNET&#125;/src</span><br><span class="line">)          </span><br><span class="line">LINK_DIRECTORIES(                                               # 库文件目录</span><br><span class="line">    $&#123;DARKNET&#125;/build/lib</span><br><span class="line">)                  </span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">INCLUDE_DIRECTORIES(./include)                                # 当前项目头文件目录</span><br><span class="line">AUX_SOURCE_DIRECTORY(./src SRC_FILES)                          # 当前项目源文件目录</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">ADD_EXECUTABLE($&#123;PROJECT_NAME&#125; $&#123;SRC_FILES&#125;)                 # 添加可执行文件</span><br><span class="line">TARGET_LINK_LIBRARIES(# 引用库</span><br><span class="line">$&#123;PROJECT_NAME&#125;</span><br><span class="line">darknet# darknet</span><br><span class="line">m# 数学函数库</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>执行指令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir build</span><br><span class="line">$ cd build</span><br><span class="line">$ cmake ..</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc</span><br><span class="line">-- Check for working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - done</span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - done</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check for working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - done</span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - done</span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/louishsu/Work/Codes/makefile/test/build</span><br><span class="line">$ make</span><br><span class="line">Scanning dependencies of target Test</span><br><span class="line">[ 50%] Building C object CMakeFiles/Test.dir/src/test.c.o</span><br><span class="line">[100%] Linking C executable Test</span><br><span class="line">[100%] Built target Test</span><br></pre></td></tr></table></figure></p><p>运行可执行文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ./Test</span><br><span class="line">Hello! Darknet!</span><br><span class="line">The size of matrix M is 16 bytes</span><br></pre></td></tr></table></figure></p><p>查看结构体<code>matrix</code>定义<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">typedef struct matrix&#123;</span><br><span class="line">    int rows, cols;</span><br><span class="line">    float **vals;</span><br><span class="line">&#125; matrix;</span><br></pre></td></tr></table></figure></p><p><code>int</code>占<code>32bit</code>，<code>float*</code>占<code>64bit</code>，故<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(32bit * 2 + 64bit) / 8 = 16byte</span><br></pre></td></tr></table></figure></p><p>运行成功。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>静态库和动态库的优缺点 - 默默淡然 - 博客园 <a href="https://www.cnblogs.com/liangxiaofeng/p/3228145.html" target="_blank" rel="noopener">https://www.cnblogs.com/liangxiaofeng/p/3228145.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu编译安装Tensorflow</title>
      <link href="/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/"/>
      <url>/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/</url>
      
        <content type="html"><![CDATA[<h1 id="非常重要"><a href="#非常重要" class="headerlink" title="非常重要"></a>非常重要</h1><p>如果中途出现错误，<code>xxxx</code>文件找不到，不要怀疑！就是大天朝的网络问题！推荐科学上网！</p><h1 id="安装CUDA与CUDNN"><a href="#安装CUDA与CUDNN" class="headerlink" title="安装CUDA与CUDNN"></a>安装CUDA与CUDNN</h1><p>首先查看显卡是否支持<code>CUDA</code>加速，输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure></p><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/nvidia-smi.png" alt="nvidia-smi"></p><p>在<code>Ubuntu16.04 LTS</code>下，推荐安装<code>CUDA9.0</code>和<code>CUDNN 7</code>。</p><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn1.png" alt="cuda_cudnn1"></p><ul><li><p>CUDA</p><blockquote><p>CUDA Toolkit 9.0 Downloads | NVIDIA Developer <a href="https://developer.nvidia.com/cuda-90-download-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-90-download-archive</a></p></blockquote><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda.png" alt="cuda"></p><p>  下载<code>.run</code>版本，安装方法如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chmod +x cuda_9.0.176_384.81_linux.run </span><br><span class="line">$ sudo sh ./cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure><p>  服务条款很长。。。。</p></li></ul><ul><li><p>CUDNN</p><blockquote><p>NVIDIA cuDNN | NVIDIA Developer <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a></p></blockquote><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cudnn1.png" alt="cudnn1"></p><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cudnn2.png" alt="cudnn2"></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz</span><br><span class="line">$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</span><br><span class="line">$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>  安装后进行验证</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cp -r /usr/src/cudnn_samples_v7/ $HOME</span><br><span class="line">$ cd  $HOME/cudnn_samples_v7/mnistCUDNN</span><br><span class="line">$ make clean &amp;&amp; make</span><br><span class="line">$ ./mnistCUDNN</span><br></pre></td></tr></table></figure><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying.png" alt="cuda_cudnn_verifying"></p><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying2.png" alt="cuda_cudnn_verifying2"></p></li></ul><h1 id="编译Tensorflow-CPU-version"><a href="#编译Tensorflow-CPU-version" class="headerlink" title="编译Tensorflow(CPU version)"></a>编译Tensorflow(CPU version)</h1><p>由于训练代码使用<code>Python</code>实现，故<code>C++</code>版本的<code>Tensorflow</code>不使用<code>GPU</code>，仅实现预测代码即可。</p><h2 id="bazel"><a href="#bazel" class="headerlink" title="bazel"></a>bazel</h2><blockquote><p>Installing Bazel on Ubuntu - Bazel <a href="https://docs.bazel.build/versions/master/install-ubuntu.html" target="_blank" rel="noopener">https://docs.bazel.build/versions/master/install-ubuntu.html</a><br>一定要用源码安装！！！</p></blockquote><p>download the Bazel binary installer named <code>bazel-&lt;version&gt;-installer-linux-x86_64.sh</code> from the <a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">Bazel releases page on GitHub</a>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python</span><br><span class="line">$ chmod +x bazel-&lt;version&gt;-installer-linux-x86_64.sh</span><br><span class="line">$ ./bazel-&lt;version&gt;-installer-linux-x86_64.sh --user</span><br><span class="line">$ sudo nano ~/.bashrc # export PATH=&quot;$PATH:$HOME/bin&quot;</span><br><span class="line">$ source ~/.bashrc </span><br><span class="line">$ bazel version</span><br></pre></td></tr></table></figure><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/bazel.png" alt="bazel"></p><h2 id="编译CPU版本的CPU"><a href="#编译CPU版本的CPU" class="headerlink" title="编译CPU版本的CPU"></a>编译CPU版本的CPU</h2><p>查看<code>java</code>版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br><span class="line">openjdk version &quot;1.8.0_191&quot;</span><br><span class="line">OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)</span><br><span class="line">OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br></pre></td></tr></table></figure></p><h2 id="安装依赖软件包环境"><a href="#安装依赖软件包环境" class="headerlink" title="安装依赖软件包环境"></a>安装依赖软件包环境</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install python3-dev</span><br><span class="line">$ pip3 install six</span><br><span class="line">$ pip3 install numpy</span><br><span class="line">$ pip3 instal wheel</span><br></pre></td></tr></table></figure><h2 id="下载Tensorflow源码"><a href="#下载Tensorflow源码" class="headerlink" title="下载Tensorflow源码"></a>下载<code>Tensorflow</code>源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/tensorflow/tensorflow</span><br></pre></td></tr></table></figure><h2 id="编译与安装"><a href="#编译与安装" class="headerlink" title="编译与安装"></a>编译与安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd tensorflow</span><br><span class="line">$ ./configure</span><br></pre></td></tr></table></figure><p>配置选项如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command &quot;bazel shutdown&quot;.</span><br><span class="line">INFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5</span><br><span class="line">You have bazel 0.20.0 installed.</span><br><span class="line">Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Found possible Python library paths:</span><br><span class="line">  /usr/local/lib/python3.5/dist-packages</span><br><span class="line">  /usr/lib/python3/dist-packages</span><br><span class="line">Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n</span><br><span class="line">No XLA JIT support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n</span><br><span class="line">No OpenCL SYCL support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with ROCm support? [y/N]: n</span><br><span class="line">No ROCm support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N]: n</span><br><span class="line">No CUDA support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to download a fresh release of clang? (Experimental) [y/N]: n</span><br><span class="line">Clang will not be downloaded.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with MPI support? [y/N]: n</span><br><span class="line">No MPI support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Please specify optimization flags to use during compilation when bazel option &quot;--config=opt&quot; is specified [Default is -march=native -Wno-sign-compare]: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n</span><br><span class="line">Not configuring the WORKSPACE for Android builds.</span><br><span class="line"></span><br><span class="line">Preconfigured Bazel build configs. You can use any of the below by adding &quot;--config=&lt;&gt;&quot; to your build command. See .bazelrc for more details.</span><br><span class="line">--config=mkl         # Build with MKL support.</span><br><span class="line">--config=monolithic  # Config for mostly static monolithic build.</span><br><span class="line">--config=gdr         # Build with GDR support.</span><br><span class="line">--config=verbs       # Build with libverbs support.</span><br><span class="line">--config=ngraph      # Build with Intel nGraph support.</span><br><span class="line">--config=dynamic_kernels# (Experimental) Build kernels into separate shared objects.</span><br><span class="line">Preconfigured Bazel build configs to DISABLE default on features:</span><br><span class="line">--config=noaws       # Disable AWS S3 filesystem support.</span><br><span class="line">--config=nogcp       # Disable GCP support.</span><br><span class="line">--config=nohdfs      # Disable HDFS support.</span><br><span class="line">--config=noignite    # Disable Apacha Ignite support.</span><br><span class="line">--config=nokafka     # Disable Apache Kafka support.</span><br><span class="line">--config=nonccl      # Disable NVIDIA NCCL support.</span><br><span class="line">Configuration finished</span><br></pre></td></tr></table></figure></p><p>使用<code>bazel</code>编译<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bazel build --config=opt //tensorflow:libtensorflow_cc.so</span><br></pre></td></tr></table></figure></p><p>出现错误</p><blockquote><p>TF failing to build on Bazel CI · Issue #19464 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/issues/19464" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/19464</a><br>Failure to build TF 1.12 from source - multiple definitions in grpc · Issue #23402 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197</a><br>Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/pull/23583" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/pull/23583</a><br>Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5</a></p></blockquote><p>解决方法</p><ul><li>方法1<br>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/tools_bazel.rc.png" alt="tools_bazel.rc"></li><li><p>方法2<br>  将<code>tools/bazel.rc</code>中内容粘到<code>.tf_configure.bazelrc</code>中，每次重新配置后需要重新粘贴一次。</p></li><li><p>源码安装<code>protobuf3.6.0</code></p><blockquote><p><a href="https://github.com/protocolbuffers/protobuf" target="_blank" rel="noopener">https://github.com/protocolbuffers/protobuf</a></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></blockquote></li><li><p>下载其他文件</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./tensorflow/contrib/makefile/download_dependencies.sh</span><br><span class="line">mkdir /tmp/eigen</span><br></pre></td></tr></table></figure><ul><li>值得注意，<code>download_dependencies.sh</code>中下载依赖包时，需要用到<code>curl</code>，但是默认方式安装  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install curl</span><br></pre></td></tr></table></figure></li></ul></li></ul><pre><code>    &gt; 现在是2018/12/19/02:48，被这个问题折腾了3个小时。时不支持`https`协议，故需要安装`OpenSSL`，并源码安装，详细资料见[curl提示不支持https协议解决方法 - 标配的小号 - 博客园](https://www.cnblogs.com/biaopei/p/8669810.html)- 执行`./autogen.sh`时，发生错误`autoreconf: not found`，则安装    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install autoconf aotomake libtool</span><br><span class="line">$ sudo apt install libffi-dev</span><br></pre></td></tr></table></figure></code></pre><ul><li>源码安装<code>Eigen</code>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd tensorflow/contrib/makefile/Downloads/eigen</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></li></ul><h1 id="调用C-版本的Tensorflow"><a href="#调用C-版本的Tensorflow" class="headerlink" title="调用C++版本的Tensorflow"></a>调用C++版本的Tensorflow</h1><p>创建文件目录如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">|-- tf_test</span><br><span class="line">    |-- build</span><br><span class="line">    |-- main.cpp</span><br><span class="line">    |-- CMakeLists.txt</span><br></pre></td></tr></table></figure></p><p><code>main.cpp</code>文件内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;tensorflow/cc/client/client_session.h&quot;</span><br><span class="line">#include &quot;tensorflow/cc/ops/standard_ops.h&quot;</span><br><span class="line">#include &quot;tensorflow/core/framework/tensor.h&quot;</span><br><span class="line"></span><br><span class="line">int main() </span><br><span class="line">&#123;</span><br><span class="line">    using namespace tensorflow;</span><br><span class="line">    using namespace tensorflow::ops;</span><br><span class="line">    Scope root = Scope::NewRootScope();</span><br><span class="line">    // Matrix A = [3 2; -1 0]</span><br><span class="line">    auto A = Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125;&#125;);</span><br><span class="line">    // Vector b = [3 5]</span><br><span class="line">    auto b = Const(root, &#123; &#123;3.f, 5.f&#125;&#125;);</span><br><span class="line">    // v = Ab^T</span><br><span class="line">    auto v = MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true));</span><br><span class="line">    std::vector&lt;Tensor&gt; outputs;</span><br><span class="line">    ClientSession session(root);</span><br><span class="line">    // Run and fetch v</span><br><span class="line">    TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs));</span><br><span class="line">    // Expect outputs[0] == [19; -3]</span><br><span class="line">    LOG(INFO) &lt;&lt; outputs[0].matrix&lt;float&gt;();</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>CMakeLists.txt</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required (VERSION 2.8.8)</span><br><span class="line">project (tf_example)</span><br><span class="line"></span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -g -std=c++11 -W&quot;)</span><br><span class="line"></span><br><span class="line">set(EIGEN_DIR /usr/local/include/eigen3)</span><br><span class="line">set(PROTOBUF_DIR/usr/local/include/google/protobuf)</span><br><span class="line">set(TENSORFLOW_DIR /home/louishsu/install/tensorflow-1.12.0)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line">$&#123;EIGEN_DIR&#125;</span><br><span class="line">$&#123;PROTOBUF_DIR&#125;</span><br><span class="line">   $&#123;TENSORFLOW_DIR&#125;</span><br><span class="line">$&#123;TENSORFLOW_DIR&#125;/bazel-genfiles</span><br><span class="line">$&#123;TENSORFLOW_DIR&#125;/tensorflow/contrib/makefile/downloads/absl</span><br><span class="line">)</span><br><span class="line">link_directories(</span><br><span class="line">/usr/local/lib</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">add_executable(</span><br><span class="line">tf_test</span><br><span class="line">main.cpp</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">target_link_libraries(</span><br><span class="line">tf_test</span><br><span class="line">tensorflow_cc</span><br><span class="line">tensorflow_framework</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir build &amp;&amp; cd build</span><br><span class="line">$ cmake .. &amp;&amp; make</span><br><span class="line">$ ./tf_test</span><br></pre></td></tr></table></figure><h1 id="install-tensorflow-gpu-for-python"><a href="#install-tensorflow-gpu-for-python" class="headerlink" title="install tensorflow-gpu for python"></a>install tensorflow-gpu for python</h1><p>可使用<code>pip</code>指令安装，推荐下载安装包，</p><blockquote><p>tensorflow · PyPI <a href="https://pypi.org/project/tensorflow/" target="_blank" rel="noopener">https://pypi.org/project/tensorflow/</a></p></blockquote><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/tensorflow_for_python.png" alt="tensorflow_for_python"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd ~/Downloads</span><br><span class="line">$ pip3 --default-timeout=1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user</span><br></pre></td></tr></table></figure><p>安装后进行验证<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ python3</span><br><span class="line">Python 3.5.2 (default, Nov 12 2018, 13:43:14) </span><br><span class="line">[GCC 5.4.0 20160609] on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; sess = tf.Session()</span><br><span class="line">2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: </span><br><span class="line">name: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758</span><br><span class="line">pciBusID: 0000:04:00.0</span><br><span class="line">totalMemory: 983.44MiB freeMemory: 177.19MiB</span><br><span class="line">2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0</span><br><span class="line">2018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 </span><br><span class="line">2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N </span><br><span class="line">2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)</span><br><span class="line">&gt;&gt;&gt; a = tf.Variable([233])</span><br><span class="line">&gt;&gt;&gt; init = tf.initialize_all_variables()</span><br><span class="line">WARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.</span><br><span class="line">Instructions for updating:</span><br><span class="line">Use `tf.global_variables_initializer` instead.</span><br><span class="line">&gt;&gt;&gt; sess.run(init)</span><br><span class="line">&gt;&gt;&gt; sess.run(a)</span><br><span class="line">array([233], dtype=int32)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; sess.close()</span><br></pre></td></tr></table></figure></p><p>注意，如果异常中断程序，显存不会被释放，需要自行<code>kill</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure></p><p>获得<code>PID</code>序号，使用指令结束进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -9 pid</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>TensorFlow C++动态库编译 - 简书 <a href="https://www.jianshu.com/p/d46596558640" target="_blank" rel="noopener">https://www.jianshu.com/p/d46596558640</a><br>Tensorflow C++ 从训练到部署(1)：环境搭建 | 技术刘 <a href="http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu编译安装OpenCV</title>
      <link href="/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV/"/>
      <url>/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV/</url>
      
        <content type="html"><![CDATA[<h1 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h1><blockquote><p>OpenCV library <a href="https://opencv.org/" target="_blank" rel="noopener">https://opencv.org/</a></p></blockquote><h1 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h1><h2 id="依赖软件包"><a href="#依赖软件包" class="headerlink" title="依赖软件包"></a>依赖软件包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install cmake</span><br><span class="line">$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev</span><br></pre></td></tr></table></figure><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ unzip opencv-3.4.4.zip</span><br><span class="line">$ cd opencv-3.4.4</span><br><span class="line">$ mkdir build &amp;&amp; cd build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make -j4</span><br></pre></td></tr></table></figure><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo make install</span><br><span class="line">$ sudo nano /etc/ld.so.conf.d/opencv.conf # add `/usr/local/lib`</span><br><span class="line">$ sudo ldconfig</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p><code>OpenCV</code>自带验证程序，在<code>opencv-3.4.4/samples/cpp/example_cmake</code>中可以找到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cd opencv-3.4.4/samples/cpp/example_cmake</span><br><span class="line">$ cmake .</span><br><span class="line">$ make</span><br><span class="line">$ ./opencv_example</span><br></pre></td></tr></table></figure><p>如果没问题，可以看到你的大脸了~</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Ubuntu16.04安装openCV3.4.4 - 辣屁小心的学习笔记 - CSDN博客 <a href="https://blog.csdn.net/weixin_39992397/article/details/84345197" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39992397/article/details/84345197</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python读写配置文件</title>
      <link href="/2019/01/04/Python%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"/>
      <url>/2019/01/04/Python%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>在深度学习中，有许多运行参数需要指定，有几种方法可以解决</p><ul><li>定义<code>.py</code>文件存储变量</li><li>定义命名元组<code>collections.namedtuple()</code></li><li>创建<code>.config</code>，<code>.ini</code>等配置文件</li></ul><p>Python 读取写入配置文件很方便，使用内置模块<code>configparser</code>即可</p><h1 id="读出"><a href="#读出" class="headerlink" title="读出"></a>读出</h1><p>首先创建文件<code>test.config</code>或<code>test.ini</code>，写入如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[db]</span><br><span class="line">db_port = 3306</span><br><span class="line">db_user = root</span><br><span class="line">db_host = 127.0.0.1</span><br><span class="line">db_pass = test</span><br><span class="line"></span><br><span class="line">[concurrent]</span><br><span class="line">processor = 20</span><br><span class="line">thread = 10</span><br></pre></td></tr></table></figure></p><p>读取操作如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; import configparser</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; configfile = &quot;./test.config&quot;</span><br><span class="line">&gt;&gt;&gt; inifile = &quot;./test.ini&quot;</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf = configparser.ConfigParser()</span><br><span class="line">&gt;&gt;&gt; cf.read(configfile)                     # 读取文件内容</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; sections = cf.sections()                # 所有的section，以列表的形式返回</span><br><span class="line">&gt;&gt;&gt; sections</span><br><span class="line">[&apos;db&apos;, &apos;concurrent&apos;]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; options = cf.options(&apos;db&apos;)              # 该section的所有option</span><br><span class="line">&gt;&gt;&gt; options</span><br><span class="line">[&apos;db_port&apos;, &apos;db_user&apos;, &apos;db_host&apos;, &apos;db_pass&apos;]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; items = cf.items(&apos;db&apos;)                  # 该section的所有键值对</span><br><span class="line">&gt;&gt;&gt; items</span><br><span class="line">[(&apos;db_port&apos;, &apos;3306&apos;), (&apos;db_user&apos;, &apos;root&apos;), (&apos;db_host&apos;, &apos;127.0.0.1&apos;), (&apos;db_pass&apos;, &apos;test&apos;)]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; db_user = cf.get(&apos;db&apos;, &apos;db_user&apos;)       # section中option的值，返回为string类型</span><br><span class="line">&gt;&gt;&gt; db_user</span><br><span class="line">&apos;root&apos;</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; db_port = cf.getint(&apos;db&apos;, &apos;db_port&apos;)    # 得到section中option的值，返回为int类型</span><br><span class="line">&gt;&gt;&gt;                                         # 类似的还有getboolean()与getfloat()</span><br><span class="line">&gt;&gt;&gt; db_port</span><br><span class="line">3306</span><br></pre></td></tr></table></figure></p><h1 id="写入"><a href="#写入" class="headerlink" title="写入"></a>写入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; import configparser</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf = configparser.ConfigParser()</span><br><span class="line">&gt;&gt;&gt; cf.add_section(&apos;test1&apos;)                 # 新增section</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1)              # 新增option：错误示范</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;pyshell#7&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    cf.set(&quot;test&quot;, &quot;count&quot;, 1)</span><br><span class="line">  File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1192, in set</span><br><span class="line">    self._validate_value_types(option=option, value=value)</span><br><span class="line">  File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1177, in _validate_value_types</span><br><span class="line">    raise TypeError(&quot;option values must be strings&quot;)</span><br><span class="line">TypeError: option values must be strings</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, &apos;1&apos;)            # 新增option</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test1&quot;, &quot;opt1&quot;, &apos;ok&apos;)           # 新增option</span><br><span class="line">&gt;&gt;&gt; cf.remove_option(&quot;test1&quot;, &quot;opt1&quot;)       # 删除option</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.add_section(&apos;test2&apos;)                 # 新增section</span><br><span class="line">&gt;&gt;&gt; cf.remove_section(&apos;test2&apos;)              # 删除section</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; with open(&quot;./test_wr.config&quot;, &apos;w+&apos;) as f:</span><br><span class="line">        cf.write(f)                         # 写入文件test_wr.config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>现在目录已创建文件<code>test_wr.config</code>，打开可以看到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[test1]</span><br><span class="line">count = 1</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python更新安装的包</title>
      <link href="/2019/01/04/Python%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85/"/>
      <url>/2019/01/04/Python%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<p><code>pip</code>不提供升级全部已安装模块的方法，以下指令可查看更新信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip list --outdate</span><br></pre></td></tr></table></figure></p><p>得到输出信息如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Package           Version   Latest     Type</span><br><span class="line">----------------- --------- ---------- -----</span><br><span class="line">absl-py           0.3.0     0.6.1      sdist</span><br><span class="line">autopep8          1.3.5     1.4.2      sdist</span><br><span class="line">bleach            2.1.4     3.0.2      wheel</span><br><span class="line">certifi           2018.8.24 2018.10.15 wheel</span><br><span class="line">dask              0.20.0    0.20.1     wheel</span><br><span class="line">grpcio            1.14.1    1.16.0     wheel</span><br><span class="line">ipykernel         5.0.0     5.1.0      wheel</span><br><span class="line">ipython           7.0.1     7.1.1      wheel</span><br><span class="line">jedi              0.12.1    0.13.1     wheel</span><br><span class="line">jupyter-console   5.2.0     6.0.0      wheel</span><br><span class="line">Markdown          2.6.11    3.0.1      wheel</span><br><span class="line">MarkupSafe        1.0       1.1.0      wheel</span><br><span class="line">matplotlib        2.2.2     3.0.2      wheel</span><br><span class="line">mistune           0.8.3     0.8.4      wheel</span><br><span class="line">numpy             1.14.5    1.15.4     wheel</span><br><span class="line">opencv-python     3.4.2.17  3.4.3.18   wheel</span><br><span class="line">Pillow            5.2.0     5.3.0      wheel</span><br><span class="line">prometheus-client 0.3.1     0.4.2      sdist</span><br><span class="line">pyparsing         2.2.0     2.3.0      wheel</span><br><span class="line">python-dateutil   2.7.3     2.7.5      wheel</span><br><span class="line">pytz              2018.5    2018.7     wheel</span><br><span class="line">urllib3           1.23      1.24.1     wheel</span><br></pre></td></tr></table></figure></p><p>以下提供一键升级的方法，可能比较久hhhh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pip._internal.utils.misc import get_installed_distributions</span><br><span class="line">from subprocess import call</span><br><span class="line"> </span><br><span class="line">for dist in get_installed_distributions():</span><br><span class="line">    modulename = dist.project_name</span><br><span class="line">    print(&apos;start processing module &apos; + modulename)</span><br><span class="line">    call(&quot;pip install --upgrade &quot; + modulename, shell=True)</span><br><span class="line">    print(&apos;module &apos; + modulename + &apos;done!&apos;)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python记录日志</title>
      <link href="/2019/01/04/Python%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/"/>
      <url>/2019/01/04/Python%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>日志可以用来记录应用程序的状态、错误和信息消息，也经常作为调试程序的工具。<br><code>Python</code>提供了一个标准的日志接口，就是<code>logging</code>模块。日志级别有<code>DEBUG</code>、<code>INFO</code>、<code>WARNING</code>、<code>ERROR</code>、<code>CRITICAL</code>五种。</p><p><a href="https://docs.python.org/3/library/logging.html" target="_blank" rel="noopener">logging — Logging facility for Python — Python 3.7.1 documentation</a></p><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><h2 id="logger对象"><a href="#logger对象" class="headerlink" title="logger对象"></a><code>logger</code>对象</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import logging</span><br><span class="line">&gt;&gt;&gt; logger = logging.getLogger(__name__)</span><br><span class="line">&gt;&gt;&gt; logger</span><br><span class="line">&lt;Logger __main__ (WARNING)&gt;</span><br></pre></td></tr></table></figure><h2 id="日志级别"><a href="#日志级别" class="headerlink" title="日志级别"></a>日志级别</h2><p>可输出五种不同的日志级别，分别为有<code>DEBUG</code>、<code>INFO</code>、<code>WARNING</code>、<code>ERROR</code>、<code>CRITICAL</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; logger.debug(&apos;test log&apos;)</span><br><span class="line">&gt;&gt;&gt; logger.info(&apos;test log&apos;)</span><br><span class="line">&gt;&gt;&gt; logger.warning(&apos;test log&apos;)</span><br><span class="line">test log</span><br><span class="line">&gt;&gt;&gt; logger.error(&apos;test log&apos;)</span><br><span class="line">test log</span><br><span class="line">&gt;&gt;&gt; logger.critical(&apos;test log&apos;)</span><br><span class="line">test log</span><br></pre></td></tr></table></figure></p><p>可以看到只有<code>WARNING</code>及以上级别日志被输出，这是由于默认的日志级别是<code>WARNING</code> ，所以低于此级别的日志不会记录。</p><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.basicConfig(**kwarg)</span><br></pre></td></tr></table></figure><p><code>**kwarg</code>中部分参数如下</p><ul><li><p><code>format</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%(levelname)：日志级别的名字格式</span><br><span class="line">%(levelno)s：日志级别的数字表示</span><br><span class="line">%(name)s：日志名字</span><br><span class="line">%(funcName)s：函数名字</span><br><span class="line">%(asctime)：日志时间，可以使用datefmt去定义时间格式，如上图。</span><br><span class="line">%(pathname)：脚本的绝对路径</span><br><span class="line">%(filename)：脚本的名字</span><br><span class="line">%(module)：模块的名字</span><br><span class="line">%(thread)：thread id</span><br><span class="line">%(threadName)：线程的名字</span><br></pre></td></tr></table></figure></li><li><p><code>datefmt</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;%Y-%m-%d %H:%M:%S&apos;</span><br></pre></td></tr></table></figure></li><li><p><code>level</code><br>  默认为<code>ERROR</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">logging.DEBUG</span><br><span class="line">logging.INFO</span><br><span class="line">logging.WARNING</span><br><span class="line">logging.ERROR</span><br><span class="line">logging.CRITICAL</span><br></pre></td></tr></table></figure></li></ul><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; # 未输出debug</span><br><span class="line">&gt;&gt;&gt; logger = logging.getLogger()</span><br><span class="line">&gt;&gt;&gt; logger.debug(&apos;test log&apos;)</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; # 修改配置</span><br><span class="line">&gt;&gt;&gt; log_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;</span><br><span class="line">&gt;&gt;&gt; log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;</span><br><span class="line">&gt;&gt;&gt; log_level = logging.DEBUG</span><br><span class="line">&gt;&gt;&gt; logging.basicConfig(format=log_format, </span><br><span class="line">                        datefmt=log_datefmt, </span><br><span class="line">                        level=log_level)</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; # 输出debug</span><br><span class="line">&gt;&gt;&gt; logger = logging.getLogger()</span><br><span class="line">&gt;&gt;&gt; logger.debug(&apos;test log&apos;)</span><br><span class="line">&lt;pyshell#8&gt; [2018-11-13 11:59:52] [DEBUG] test log</span><br></pre></td></tr></table></figure></p><h2 id="输出到日志文件"><a href="#输出到日志文件" class="headerlink" title="输出到日志文件"></a>输出到日志文件</h2><p>保存代码为文件<code>log_test.py</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line">log_format = &apos;%(filename)s [%(asctime)s] [%(levelname)s] %(message)s&apos;</span><br><span class="line">log_datefmt = &apos;%Y-%m-%d %H:%M:%S&apos;</span><br><span class="line">log_level = logging.DEBUG</span><br><span class="line">log_filename = &apos;./test.log&apos;</span><br><span class="line">log_filemode = &apos;a&apos;  # 也可以为&apos;w&apos;, &apos;w+&apos;等</span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=log_format,</span><br><span class="line">                    datefmt=log_datefmt, </span><br><span class="line">                    level=log_level,</span><br><span class="line">                    filename=log_filename, </span><br><span class="line">                    filemode=log_filemode)</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line">logger.debug(&apos;test log&apos;)</span><br><span class="line">logger.info(&apos;test log&apos;)</span><br><span class="line">logger.warning(&apos;test log&apos;)</span><br><span class="line">logger.error(&apos;test log&apos;)</span><br><span class="line">logger.critical(&apos;test log&apos;)</span><br></pre></td></tr></table></figure></p><p>运行完毕，打开<code>log_test.log</code>文件可以看到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log_test.py [2018-11-13 12:11:04] [DEBUG] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [INFO] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [WARNING] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [ERROR] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [CRITICAL] test log</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+Github博客搭建</title>
      <link href="/2019/01/04/Github-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/01/04/Github-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>那么问题来了，现有的博客还是现有的这篇文章呢？</p><h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><p>安装<a href="https://nodejs.org/en/" target="_blank" rel="noopener">node.js</a>, <a href="https://git-scm.com/" target="_blank" rel="noopener">git</a>, <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a></p><h1 id="博客搭建"><a href="#博客搭建" class="headerlink" title="博客搭建"></a>博客搭建</h1><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>推荐使用<code>git</code>命令窗口，执行如下指令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir Blog</span><br><span class="line">$ cd Blog</span><br><span class="line">$ hexo init</span><br><span class="line">INFO  Cloning hexo-starter to ~\Desktop\Blog</span><br><span class="line">Cloning into &apos;C:\Users\LouisHsu\Desktop\Blog&apos;...</span><br><span class="line">remote: Enumerating objects: 68, done.</span><br><span class="line">remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68</span><br><span class="line">Unpacking objects: 100% (68/68), done.</span><br><span class="line">Submodule &apos;themes/landscape&apos; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &apos;themes/landscape&apos;</span><br><span class="line">Cloning into &apos;C:/Users/LouisHsu/Desktop/Blog/themes/landscape&apos;...</span><br><span class="line">remote: Enumerating objects: 1, done.</span><br><span class="line">remote: Counting objects: 100% (1/1), done.</span><br><span class="line">remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866</span><br><span class="line">Receiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.</span><br><span class="line">Resolving deltas: 100% (459/459), done.</span><br><span class="line">Submodule path &apos;themes/landscape&apos;: checked out &apos;73a23c51f8487cfcd7c6deec96ccc7543960d350&apos;</span><br><span class="line">[32mINFO [39m Install dependencies</span><br><span class="line">npm WARN deprecated titlecase@1.1.2: no longer maintained</span><br><span class="line">npm WARN deprecated postinstall-build@5.0.3: postinstall-build&apos;s behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.</span><br><span class="line"></span><br><span class="line">&gt; nunjucks@3.1.6 postinstall C:\Users\LouisHsu\Desktop\Blog\node_modules\nunjucks</span><br><span class="line">&gt; node postinstall-build.js src</span><br><span class="line"></span><br><span class="line">npm notice created a lockfile as package-lock.json. You should commit this file.</span><br><span class="line">npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):</span><br><span class="line">npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)</span><br><span class="line"></span><br><span class="line">added 422 packages from 501 contributors and audited 4700 packages in 59.195s</span><br><span class="line">found 0 vulnerabilities</span><br><span class="line"></span><br><span class="line">INFO  Start blogging with Hexo!</span><br></pre></td></tr></table></figure></p><p>生成目录结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\-- scaffolds</span><br><span class="line">\-- source</span><br><span class="line">    \-- _posts</span><br><span class="line">\-- themes</span><br><span class="line">|-- _config.yml</span><br><span class="line">|-- package.json</span><br></pre></td></tr></table></figure></p><p>继续<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):</span><br><span class="line">npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)</span><br><span class="line"></span><br><span class="line">audited 4700 packages in 5.99s</span><br><span class="line">found 0 vulnerabilities</span><br></pre></td></tr></table></figure></p><p>现在该目录执行指令，开启<code>hexo</code>服务器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure></p><p><img src="/2019/01/04/Github-Hexo博客搭建/hexo_server.png" alt="hexo_server"></p><h2 id="生成目录和标签"><a href="#生成目录和标签" class="headerlink" title="生成目录和标签"></a>生成目录和标签</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hexo n page about</span><br><span class="line">$ hexo n page archives</span><br><span class="line">$ hexo n page categories</span><br><span class="line">$ hexo n page tags</span><br></pre></td></tr></table></figure><p>修改<code>/source/tags/index.md</code>，其他同理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">01| ---</span><br><span class="line">02| title: tags</span><br><span class="line">03| date: 2019-01-04 17:34:15</span><br><span class="line">04| ---</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">01| ---</span><br><span class="line">02| title: tags</span><br><span class="line">03| date: 2019-01-04 17:34:15</span><br><span class="line">04| type: &quot;tags&quot;</span><br><span class="line">05| comments: false</span><br><span class="line">06| ---</span><br></pre></td></tr></table></figure></p><h2 id="关联Github"><a href="#关联Github" class="headerlink" title="关联Github"></a>关联<code>Github</code></h2><p>在<code>Github</code>新建一个仓库，命名为<code>username.github.io</code>，例如<code>isLouisHsu.github.io</code>，新建时勾选<code>Initialize this repository with a README</code>，因为这个仓库必须不能为空。<br><img src="/2019/01/04/Github-Hexo博客搭建/github_io.png" alt="github_io"></p><p>打开博客目录下的<code>_config.yml</code>配置文件，定位到最后的<code>deploy</code>选项，修改如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repository: git@github.com:isLouisHsu/isLouisHsu.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure></p><p>安装插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>现在就可以将该目录内容推送到<code>Github</code>新建的仓库中了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure></p><h2 id="使用个人域名"><a href="#使用个人域名" class="headerlink" title="使用个人域名"></a>使用个人域名</h2><ol><li>在<code>source</code>目录下新建文件<code>CNAME</code>，输入解析后的个人域名</li><li>在<code>Github</code>主页修改域名</li></ol><h1 id="备份博客"><a href="#备份博客" class="headerlink" title="备份博客"></a>备份博客</h1><blockquote><p>没。没什么用<br>我。我不备份了<br>可以新建一个仓库专门保存文件试试</p></blockquote><p>现在博客的源文件仅保存在<code>PC</code>上， 我们对它们进行备份，并将仓库作为博客文件夹</p><ol><li>在仓库新建分支<code>hexo</code>，设置为默认分支<br> <img src="/2019/01/04/Github-Hexo博客搭建/create_branch_hexo.png" alt="create_branch_hexo"><br> <img src="/2019/01/04/Github-Hexo博客搭建/change_branch_hexo.png" alt="change_branch_hexo"></li><li><p>将仓库克隆至本地</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/isLouisHsu/isLouisHsu.github.io.git</span><br></pre></td></tr></table></figure></li><li><p>克隆文件<br> 将之前的Hexo文件夹中的</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scffolds/</span><br><span class="line">source/</span><br><span class="line">themes/</span><br><span class="line">.gitignore</span><br><span class="line">_config.yml</span><br><span class="line">package.json</span><br></pre></td></tr></table></figure><p> 复制到克隆下来的仓库文件夹<code>isLouisHsu.github.io</code><br> <img src="/2019/01/04/Github-Hexo博客搭建/backup_blog.png" alt="backup_blog"></p></li><li><p>安装包</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ npm install hexo --save</span><br><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p> 备份博客使用以下指令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;backup&quot;</span><br><span class="line">$ git push origin hexo</span><br></pre></td></tr></table></figure></li><li><p>部署博客指令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure></li><li><p><code>单键</code>提交<br> 编写脚本<code>commit.bat</code>，双击即可</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &apos;backup&apos;</span><br><span class="line">git push origin hexo</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure></li></ol><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><ul><li><p>目录结构</p><ul><li><code>public</code>  生成的网站文件，发布的站点文件。</li><li><code>source</code>  资源文件夹，用于存放内容。</li><li><code>tag</code>     标签文件夹。</li><li><code>archive</code> 归档文件夹。</li><li><code>category</code>分类文件夹。</li><li><code>downloads/code include code</code>文件夹。</li><li><code>:lang i18n_dir</code> 国际化文件夹。</li><li><code>_config.yml</code> 配置文件</li></ul></li><li><p>指令</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ hexo help</span><br><span class="line">Usage: hexo &lt;command&gt;</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">    clean     Remove generated files and cache.</span><br><span class="line">    config    Get or set configurations.</span><br><span class="line">    deploy    Deploy your website.</span><br><span class="line">    generate  Generate static files.</span><br><span class="line">    help      Get help on a command.</span><br><span class="line">    init      Create a new Hexo folder.</span><br><span class="line">    list      List the information of the site</span><br><span class="line">    migrate   Migrate your site from other system to Hexo.</span><br><span class="line">    new       Create a new post.</span><br><span class="line">    publish   Moves a draft post from _drafts to _posts folder.</span><br><span class="line">    render    Render files with renderer plugins.</span><br><span class="line">    server    Start the server.</span><br><span class="line">    version   Display version information.</span><br><span class="line"></span><br><span class="line">Global Options:</span><br><span class="line">    --config  Specify config file instead of using _config.yml</span><br><span class="line">    --cwd     Specify the CWD</span><br><span class="line">    --debug   Display all verbose messages in the terminal</span><br><span class="line">    --draft   Display draft posts</span><br><span class="line">    --safe    Disable all plugins and scripts</span><br><span class="line">    --silent  Hide output on console</span><br><span class="line"></span><br><span class="line">For more help, you can use &apos;hexo help [command]&apos; for the detailed information or you can check the docs: http://hexo.io/docs/</span><br></pre></td></tr></table></figure></li></ul><!-- # 修改主题 --><h1 id="拓展功能支持"><a href="#拓展功能支持" class="headerlink" title="拓展功能支持"></a>拓展功能支持</h1><h2 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><p>修改文件<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure></p><p>在执行<code>$ hexo n [layout] &lt;title&gt;</code>时会生成同名文件夹，把图片放在这个文件夹内，在<code>.md</code>文件中插入图片<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![image_name](/title/image_name.png)</span><br></pre></td></tr></table></figure></p><h2 id="搜索功能"><a href="#搜索功能" class="headerlink" title="搜索功能"></a>搜索功能</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-searchdb --save</span><br><span class="line">$ npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><p>站点配置文件<code>_config.yml</code>中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure></p><p>修改主题配置文件<code>/themes/xxx/_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure></p><h2 id="带过滤功能的首页插件"><a href="#带过滤功能的首页插件" class="headerlink" title="带过滤功能的首页插件"></a>带过滤功能的首页插件</h2><p>在首页只显示指定分类下面的文章列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-index2 --save</span><br><span class="line">$ npm uninstall hexo-generator-index --save</span><br></pre></td></tr></table></figure></p><p>修改<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">index_generator:</span><br><span class="line">  per_page: 10</span><br><span class="line">  order_by: -date</span><br><span class="line">  include:</span><br><span class="line">    - category Web  # 只包含Web分类下的文章</span><br><span class="line">  exclude:</span><br><span class="line">    - tag Hexo      # 不包含标签为Hexo的文章</span><br></pre></td></tr></table></figure></p><h2 id="数学公式支持"><a href="#数学公式支持" class="headerlink" title="数学公式支持"></a>数学公式支持</h2><p><code>hexo</code>默认的渲染引擎是<code>marked</code>，但是<code>marked</code>不支持<code>mathjax</code>。<code>kramed</code>是在<code>marked</code>的基础上进行修改。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-math --save              # 停止使用 hexo-math</span><br><span class="line">$ npm install hexo-renderer-mathjax --save    # 安装hexo-renderer-mathjax包：</span><br><span class="line">$ npm uninstall hexo-renderer-marked --save   # 卸载原来的渲染引擎</span><br><span class="line">$ npm install hexo-renderer-kramed --save     # 安装新的渲染引擎</span><br></pre></td></tr></table></figure></p><p>修改<code>/node_modules/kramed/lib/rules/inline.js</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">11| escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">...</span><br><span class="line">20| em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">11| escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">...</span><br><span class="line">20| em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><p>修改<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">64| // Change inline math rule</span><br><span class="line">65| function formatText(text) &#123;</span><br><span class="line">66|   // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">67|   return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">68| &#125;</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">64| // Change inline math rule</span><br><span class="line">65| function formatText(text) &#123;</span><br><span class="line">66|   // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">67|   // return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">68|   return text;</span><br><span class="line">69| &#125;</span><br></pre></td></tr></table></figure></p><p>在主题中开启<code>mathjax</code>开关，例如<code>next</code>主题中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br></pre></td></tr></table></figure></p><p>在文章中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: title.md</span><br><span class="line">date: 2019-01-04 12:47:37</span><br><span class="line">categories:</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">top:</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>测试</p><script type="math/tex; mode=display">A = \left[\begin{matrix}    a_{11} & a_{12} \\    a_{21} & a_{22}\end{matrix}\right]</script><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>基于hexo+github搭建一个独立博客 - 牧云云 - 博客园 <a href="https://www.cnblogs.com/MuYunyun/p/5927491.html" target="_blank" rel="noopener">https://www.cnblogs.com/MuYunyun/p/5927491.html</a><br>hexo+github pages轻松搭博客(1) | ex2tron’s Blog <a href="http://ex2tron.wang/hexo-blog-with-github-pages-1/" target="_blank" rel="noopener">http://ex2tron.wang/hexo-blog-with-github-pages-1/</a><br>hexo下LaTeX无法显示的解决方案 - crazy_scott的博客 - CSDN博客 <a href="https://blog.csdn.net/crazy_scott/article/details/79293576" target="_blank" rel="noopener">https://blog.csdn.net/crazy_scott/article/details/79293576</a><br>在Hexo中渲染MathJax数学公式 - 简书 <a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a><br>怎么去备份你的Hexo博客 - 简书 <a href="https://www.jianshu.com/p/baab04284923" target="_blank" rel="noopener">https://www.jianshu.com/p/baab04284923</a><br>Hexo中添加本地图片 - 蜕变C - 博客园 <a href="https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral" target="_blank" rel="noopener">https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral</a><br>hexo 搜索功能 - 阿甘的博客 - CSDN博客 <a href="https://blog.csdn.net/ganzhilin520/article/details/79047983" target="_blank" rel="noopener">https://blog.csdn.net/ganzhilin520/article/details/79047983</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>scikit-learn: 处理特征</title>
      <link href="/2018/11/24/scikit-learn-%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81/"/>
      <url>/2018/11/24/scikit-learn-%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<h1 id="数据预处理-preprocessing"><a href="#数据预处理-preprocessing" class="headerlink" title="数据预处理(preprocessing)"></a>数据预处理(preprocessing)</h1><h2 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import sklearn.preprocessing as preprocessing</span><br><span class="line">&gt;&gt;&gt; dir(preprocessing)</span><br><span class="line">[&apos;Binarizer&apos;, &apos;CategoricalEncoder&apos;, &apos;FunctionTransformer&apos;, &apos;Imputer&apos;, &apos;KBinsDiscretizer&apos;, </span><br><span class="line">&apos;KernelCenterer&apos;, &apos;LabelBinarizer&apos;, &apos;LabelEncoder&apos;, &apos;MaxAbsScaler&apos;, &apos;MinMaxScaler&apos;, </span><br><span class="line">&apos;MultiLabelBinarizer&apos;, &apos;Normalizer&apos;, &apos;OneHotEncoder&apos;, &apos;OrdinalEncoder&apos;, &apos;PolynomialFeatures&apos;, </span><br><span class="line">&apos;PowerTransformer&apos;, &apos;QuantileTransformer&apos;, &apos;RobustScaler&apos;, &apos;StandardScaler&apos;, </span><br><span class="line">&apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, </span><br><span class="line">&apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;, </span><br><span class="line">&apos;_discretization&apos;, &apos;_encoders&apos;, &apos;_function_transformer&apos;, </span><br><span class="line">&apos;add_dummy_feature&apos;, &apos;base&apos;, &apos;binarize&apos;, &apos;data&apos;, </span><br><span class="line">&apos;imputation&apos;, &apos;label&apos;, &apos;label_binarize&apos;, &apos;maxabs_scale&apos;, </span><br><span class="line">&apos;minmax_scale&apos;, &apos;normalize&apos;, &apos;power_transform&apos;, </span><br><span class="line">&apos;quantile_transform&apos;, &apos;robust_scale&apos;, &apos;scale&apos;]</span><br></pre></td></tr></table></figure><h1 id="特征抽取-feature-extraction"><a href="#特征抽取-feature-extraction" class="headerlink" title="特征抽取(feature extraction)"></a>特征抽取(feature extraction)</h1><h2 id="Module-1"><a href="#Module-1" class="headerlink" title="Module"></a>Module</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import sklearn.feature_extraction as feature_extraction</span><br><span class="line">&gt;&gt;&gt; dir(feature_extraction)</span><br><span class="line">[&apos;DictVectorizer&apos;, &apos;FeatureHasher&apos;, </span><br><span class="line">&apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, </span><br><span class="line">&apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;, </span><br><span class="line">&apos;_hashing&apos;, &apos;dict_vectorizer&apos;, &apos;grid_to_graph&apos;, &apos;hashing&apos;, </span><br><span class="line">&apos;image&apos;, &apos;img_to_graph&apos;, &apos;stop_words&apos;, &apos;text&apos;]</span><br></pre></td></tr></table></figure><h1 id="特征选择-feature-selection"><a href="#特征选择-feature-selection" class="headerlink" title="特征选择(feature selection)"></a>特征选择(feature selection)</h1><p>当数据预处理完成后，我们需要选择有意义的特征，将其输入到模型中训练，主要从两个方面考虑</p><ul><li><strong>特征是否发散</strong><br>  若一个特征不发散，其方差接近$0$，则表示该特征在各个样本上没有差别，对于样本的区分没什么用；</li><li><strong>特征与目标的相关性</strong><br>  与目标<code>(target)</code>相关性高的特征，应当优先选择。</li></ul><p>特征选择的方法可以根据特征选择的形式分为$3$种</p><ul><li><strong>过滤法<code>(Filter)</code></strong><br>  按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li><li><strong>包装法<code>(Wrapper)</code></strong><br>  根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。</li><li><strong>嵌入法<code>(Embedded)</code></strong><br>  先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于<code>Filter</code>方法，但是是通过训练来确定特征的优劣。</li></ul><h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><h3 id="移除低方差"><a href="#移除低方差" class="headerlink" title="移除低方差"></a>移除低方差</h3><blockquote><p><code>Removing features with low variance</code></p></blockquote><p>即移除那些方差较小的特征，当特征的取值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。</p><p>现实中这种方法作用不大，可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># class sklearn.feature_selection.VarianceThreshold(threshold=0.0)</span><br></pre></td></tr></table></figure><p>调用例程如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = [</span><br><span class="line">            [0, 0, 1], </span><br><span class="line">            [0, 1, 0], </span><br><span class="line">            [1, 0, 0], </span><br><span class="line">            [0, 1, 1], </span><br><span class="line">            [0, 1, 0], </span><br><span class="line">            [0, 1, 1],</span><br><span class="line">    ]</span><br><span class="line">&gt;&gt;&gt; feature_selection.VarianceThreshold(threshold=(.8 * (1 - .8))).fit_transform(X) </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [1, 0],</span><br><span class="line">       [0, 0],</span><br><span class="line">       [1, 1],</span><br><span class="line">       [1, 0],</span><br><span class="line">       [1, 1]])</span><br><span class="line">&gt;&gt;&gt; # 选出了第2, 3列特征</span><br></pre></td></tr></table></figure></p><h3 id="单变量特征选择"><a href="#单变量特征选择" class="headerlink" title="单变量特征选择"></a>单变量特征选择</h3><blockquote><p><code>Univariate feature selection</code></p></blockquote><p>分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。</p><p>指标适用情况：</p><ol><li>对于分类问题(<code>target</code>离散)<br> 卡方检验，f_classif, mutual_info_classif，互信息</li><li>对于回归问题(<code>target</code>连续)</li></ol><blockquote><p>注：分类与回归在一定程度上可以互相转换，</p></blockquote><h2 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h2><h2 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h2><h2 id="Module-2"><a href="#Module-2" class="headerlink" title="Module"></a>Module</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import sklearn.feature_selection as feature_selection</span><br><span class="line">&gt;&gt;&gt; dir(feature_selection)</span><br><span class="line">[&apos;GenericUnivariateSelect&apos;, &apos;RFE&apos;, &apos;RFECV&apos;, </span><br><span class="line">&apos;SelectFdr&apos;, &apos;SelectFpr&apos;, &apos;SelectFromModel&apos;, &apos;SelectFwe&apos;, </span><br><span class="line">&apos;SelectKBest&apos;, &apos;SelectPercentile&apos;, &apos;VarianceThreshold&apos;, </span><br><span class="line">&apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, </span><br><span class="line">&apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;, </span><br><span class="line">&apos;base&apos;, &apos;chi2&apos;, &apos;f_classif&apos;, &apos;f_oneway&apos;, </span><br><span class="line">&apos;f_regression&apos;, &apos;from_model&apos;, &apos;mutual_info_&apos;, &apos;mutual_info_classif&apos;, </span><br><span class="line">&apos;mutual_info_regression&apos;, &apos;rfe&apos;, &apos;univariate_selection&apos;, &apos;variance_threshold&apos;]</span><br></pre></td></tr></table></figure><h1 id="参考博客-reference"><a href="#参考博客-reference" class="headerlink" title="参考博客(reference)"></a>参考博客(reference)</h1><blockquote><p>使用sklearn优雅地进行数据挖掘 - jasonfreak - 博客园 <a href="http://www.cnblogs.com/jasonfreak/p/5448462.html" target="_blank" rel="noopener">http://www.cnblogs.com/jasonfreak/p/5448462.html</a><br>特征选择 (feature_selection) - 会飞的蝸牛 - 博客园 <a href="https://www.cnblogs.com/stevenlk/p/6543628.html" target="_blank" rel="noopener">https://www.cnblogs.com/stevenlk/p/6543628.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Metrics</title>
      <link href="/2018/11/21/Metrics/"/>
      <url>/2018/11/21/Metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="回归-regression-评估指标"><a href="#回归-regression-评估指标" class="headerlink" title="回归(regression)评估指标"></a>回归(regression)评估指标</h1><h2 id="解释方差-Explained-Variance"><a href="#解释方差-Explained-Variance" class="headerlink" title="解释方差(Explained Variance)"></a>解释方差(Explained Variance)</h2><script type="math/tex; mode=display">EV(\hat{y}, y)= 1 - \frac{Var(y-\hat{y})}{Var(y)}</script><p>解释方差越接近$1$表示回归效果越好。</p><h2 id="平均绝对误差-Mean-Absolute-Error-MAE"><a href="#平均绝对误差-Mean-Absolute-Error-MAE" class="headerlink" title="平均绝对误差(Mean Absolute Error - MAE)"></a>平均绝对误差(Mean Absolute Error - MAE)</h2><script type="math/tex; mode=display">MAE(\hat{y}, y) = E(||\hat{y} - y||_1)= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} |\hat{y}^{(i)} - y^{(i)}|</script><p>$MAE$越小表示回归效果越好。</p><h2 id="平均平方误差-Mean-Squared-Error-MSE"><a href="#平均平方误差-Mean-Squared-Error-MSE" class="headerlink" title="平均平方误差(Mean Squared Error - MSE)"></a>平均平方误差(Mean Squared Error - MSE)</h2><p>在<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归</a>一节，使用的损失函数即$MSE$</p><script type="math/tex; mode=display">MSE(\hat{y}, y) = E(||\hat{y} - y||_2^2)= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2</script><p>其中$y$与$\hat{y}$均为$1$维向量，$MSE$越小表示回归效果越好。</p><p>其含义比较直观，即偏差的平方和。也可以从最小化方差的角度解释，定义误差向量</p><script type="math/tex; mode=display">e = \hat{y} - y</script><p>我们假定其期望为$0$，即</p><script type="math/tex; mode=display">E(e) = 0　或　\overline{e} = 0</script><p>那么误差的方差为</p><script type="math/tex; mode=display">Var(e) = E[(e - \overline{e})^T (e - \overline{e})] = E(||e||_2^2)</script><p>也即$MSE$。</p><h2 id="均方根误差-Root-Mean-Squared-Error-RMSE"><a href="#均方根误差-Root-Mean-Squared-Error-RMSE" class="headerlink" title="均方根误差(Root Mean Squared Error - RMSE)"></a>均方根误差(Root Mean Squared Error - RMSE)</h2><script type="math/tex; mode=display">RMSE(\hat{y}, y) = \sqrt{\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2}</script><p>实质与$MSE$是一样的。只不过用于数据更好的描述，使计算得损失的值较小。$RMSE$越小表示回归效果越好。</p><h2 id="均方对数误差-Mean-Squard-Logarithmic-Error-MSLE"><a href="#均方对数误差-Mean-Squard-Logarithmic-Error-MSLE" class="headerlink" title="均方对数误差(Mean Squard Logarithmic Error - MSLE)"></a>均方对数误差(Mean Squard Logarithmic Error - MSLE)</h2><script type="math/tex; mode=display">MSLE(\hat{y}, y) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} \left[\log (1+y^{(i)}) - \log (1+\hat{y}^{(i)})\right]^2</script><p>通常用于输出指数增长的模型，如，人口统计，商品的平均销售量，以及一段时间内的平均销售量等。注意，由对数性质，这一指标对过小的预测的惩罚大于预测过大的预测的惩罚。</p><h2 id="中值绝对误差-Median-Absolute-Error-MedAE"><a href="#中值绝对误差-Median-Absolute-Error-MedAE" class="headerlink" title="中值绝对误差(Median Absolute Error - MedAE)"></a>中值绝对误差(Median Absolute Error - MedAE)</h2><script type="math/tex; mode=display">MedAE(\hat{y}, y) = median(|y - \hat{y}|)</script><h2 id="R决定系数-R2"><a href="#R决定系数-R2" class="headerlink" title="R决定系数(R2)"></a>R决定系数(R2)</h2><p>又称拟合优度，提供了一个衡量未来样本有多好的预测模型。最佳可能的分数是$1.0$，它可以是负的(因为模型可以任意恶化)。一个常数模型总是预测$y$的期望值，而不考虑输入特性，则得到$R^2$分数为$0.0$。</p><script type="math/tex; mode=display">R^2(\hat{y}, y) = 1 - \frac{\sum_{i=1}^{n_{samples}} (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{n_{samples}} (y^{(i)} - \overline{y})^2}</script><p>其中</p><script type="math/tex; mode=display">\overline{y} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} y^{(i)}</script><h1 id="分类-classification-评估指标"><a href="#分类-classification-评估指标" class="headerlink" title="分类(classification)评估指标"></a>分类(classification)评估指标</h1><p>先作如下定义<br><img src="/2018/11/21/Metrics/terminology_and_derivations_1.png" alt="terminology_and_derivations_1"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_2.png" alt="terminology_and_derivations_2"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_3.png" alt="terminology_and_derivations_3"></p><p><img src="/2018/11/21/Metrics/metrics_classification2.png" alt="metrics_classification2"></p><h2 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率(Accuracy)"></a>准确率(Accuracy)</h2><script type="math/tex; mode=display">Accuracy(y, \hat{y})= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} 1(y^{(i)}=\hat{y}^{(i)})</script><p>也即</p><script type="math/tex; mode=display">Accuracy= \frac{TN+TP}{TN+TP+FN+FP}</script><p>精度只是简单地计算出比例，但是没有对不同类别进行区分。因为不同类别错误代价可能不同。例如：判断这个病人是不是病危，如果不是病危错误判断为病危，那只是损失一点医务人员的时间和精力，如果是把病危的人判断为非病危状态，那损失的就是一条人命。他们之间存在重要性差异，这时候就不能用精度。对于样本不均衡的情况，也不是用精度来衡量。例如：有A类1000个，B类5个，如果我把这1005个样本都预测成A类，正确率=1000/1005=99.5%。</p><h2 id="精确率-Precision-与召回率-Recall"><a href="#精确率-Precision-与召回率-Recall" class="headerlink" title="精确率(Precision)与召回率(Recall)"></a>精确率(Precision)与召回率(Recall)</h2><ul><li><p>精确率<code>(Precision)</code><br>  即预测正样本中，实际为正样本的百分比，度量了分类器不会将真正的负样本错误地分为正样本的能力。</p><script type="math/tex; mode=display">  Precision = \frac{TP}{TP+FP}</script></li><li><p>召回率<code>(Recall)</code><br>  又称查全率，即实际正样本中，被预测为正样本的百分比，度量了分类器找到所有正样本的能力。</p><script type="math/tex; mode=display">  Recall = \frac{TP}{TP + FN}</script><p><img src="/2018/11/21/Metrics/precision_recall.png" alt="precision_recall"></p></li></ul><h2 id="F度量"><a href="#F度量" class="headerlink" title="F度量"></a>F度量</h2><blockquote><p><a href="https://en.wikipedia.org/wiki/F1_score" target="_blank" rel="noopener">F1 score - Wikipedia</a></p></blockquote><ul><li><p>$F_1$<br>  为精确率<code>(Precision)</code>与召回率<code>(Recall)</code>的调和均值<code>(harmonic mean)</code>。</p><script type="math/tex; mode=display">  \frac{1}{F_1}   = \frac{1}{2} (\frac{1}{Precision} + \frac{1}{Recall})</script><p>  也即</p><script type="math/tex; mode=display">  F_1 = 2 · \frac{Precision·Recall}{Precision + Recall}</script></li><li><p>$F_{\beta}$<br>  在$F_1$度量的基础上增加权值$\beta$，$\beta$越大，$Recall$的权重越大，否则$Precision$的权重越大。</p><script type="math/tex; mode=display">  \frac{1}{F_{\beta}} = \frac{1}{1+\beta^2} \frac{1}{Precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{Recall}</script><p>  也即</p><script type="math/tex; mode=display">  F_{\beta} = (1+\beta^2)·\frac{Precision·Recall}{(\beta^2·Precision) + Recall}</script></li></ul><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><code>Confusion matrix</code>，也被称作错误矩阵<code>(Error matrix)</code>，是一个特别的表。无监督学习中，通常称作匹配矩阵<code>(Matching matrix)</code>。每一列表达了分类器对样本的类别预测，每一行表达了样本所属的真实类别。</p><p>例如我们有$27$个待分类样本，将其划分为<code>Cat</code>，<code>Dog</code>，<code>Rabbit</code>，讲实际标签与预测标签数目统计后填入混淆矩阵。</p><p><img src="/2018/11/21/Metrics/confusion_matrix.png" alt="confusion_matrix"></p><p>例如实际上有$8$个样本为<code>Cat</code>，而该分类器将其中$3$个划分为<code>Dog</code>，将$2$个为<code>Dog</code>的样本划分为<code>Cat</code>。我们可以根据上述混淆矩阵得出结论，该分类器对<code>Dog</code>和<code>Cat</code>分类能力较弱，而对<code>Rabbit</code>分类能力较强。而且正确预测的样本数目都在对角线上，很容易直观地检查表中的预测错误。</p><p>以下为<code>scikit-learn</code>中混淆矩阵的<code>API</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.metrics import confusion_matrix</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]</span><br><span class="line">&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred)</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]</span><br><span class="line">&gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; # In the binary case, we can extract true positives, etc as follows:</span><br><span class="line">&gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()</span><br><span class="line">&gt;&gt;&gt; (tn, fp, fn, tp)</span><br><span class="line">(0, 2, 1, 1)</span><br></pre></td></tr></table></figure></p><h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p><code>Receiver Operating Characteristic</code>，是根据一系列不同的二分类方式(分界值或决定阈)，以召回率(真正率<code>TPR</code>、灵敏度)为纵坐标，<code>fall-out</code>(假正率<code>FPR</code>、$1$-特异度)为横坐标绘制的曲线。</p><ul><li><p><code>true positive rate - TPR</code><br>  所有阳性样本中有多少正确的阳性结果。</p><script type="math/tex; mode=display">  TPR = \frac{TP}{P} = \frac{TP}{TP + FN}</script></li><li><p><code>false positive rate - FPR</code><br>  所有阴性样本中有多少不正确的阳性结果。</p><script type="math/tex; mode=display">  FPR = \frac{FP}{N} = \frac{FP}{FP + TN}</script></li></ul><h3 id="ROC-space"><a href="#ROC-space" class="headerlink" title="ROC space"></a>ROC space</h3><p><img src="/2018/11/21/Metrics/模型的评估指标/ROC_space.png" alt="ROC_space"></p><ul><li>分别以<code>FPR</code>与<code>TPR</code>作为横纵轴(又称灵敏度-$1$特异度曲线<code>sensitivity vs (1 − specificity) plot</code>)；</li><li>每次预测结果或混淆矩阵的实例代表了<code>ROC</code>空间中的一个点；<br>  例如上图中$A, B, C, C’$是以下表数据计算得到的点。<br>  <img src="/2018/11/21/Metrics/ROC_space_samples.png" alt="ROC_space_samples"></li><li>在<code>ROC</code>空间中最左上方的点$(0, 1)$称作完美分类器<code>(perfect classification)</code>；</li><li>随机分类器的结果分布在<code>ROC space</code>对角线$(0, 0)-(1, 1)$上，当实验次数足够多，其分区趋向$(0.5, 0.5)$;</li><li>对角线以上的点代表好的分类结果(比随机的好)；线下的点代表坏的结果(比随机的差)；</li><li>注意，持续不良分类器的输出可以简单地反转以获得一个好的分类器，反转后的分类器与原分类器在平面上关于对角线对称，例如点$C’$。</li></ul><h3 id="ROC曲线的绘制"><a href="#ROC曲线的绘制" class="headerlink" title="ROC曲线的绘制"></a>ROC曲线的绘制</h3><p>若训练集样本中，正样本与负样本以正态分布的形式分布在样本平面上，如下图，左峰为负样本，右峰为正样本，存在部分重叠(不然就不用搞这么多分类算法了)。</p><p><img src="/2018/11/21/Metrics/ROC_curves.svg.png" alt="ROC_curves.svg"></p><p>若假设正样本概率密度为$f_1(x)$，负样本的概率密度为$f_0(x)$，给定阈值$T$，则右</p><script type="math/tex; mode=display">TPR(T) = \int_T^{\infty} f_1(x) dx</script><script type="math/tex; mode=display">FPR(T) = \int_T^{\infty} f_0(x) dx</script><p>选取不同的阈值划分分类器输出，就能得到<code>ROC</code>曲线。</p><p>在基于有限样本作<code>ROC</code>图时，可以看到曲线每次都是一个“爬坡”，遇到正例往上爬一格$(1/m+)$，错了往右爬一格$(1/m-)$，显然往上爬对于算法性能来说是最好的。<br><img src="/2018/11/21/Metrics/ROC_curves_up_right.png" alt="ROC_curves_up_right"></p><h3 id="Area-Under-the-Curve-AUC"><a href="#Area-Under-the-Curve-AUC" class="headerlink" title="Area Under the Curve - AUC"></a>Area Under the Curve - AUC</h3><p><code>ROC</code>曲线下的面积<code>AUC</code>物理意义为，任取一对正负样本，正样本的预测值大于负样本的预测值的概率。</p><script type="math/tex; mode=display">A = \int_{-\infty}^{\infty} TPR(T) dFPR(T)</script><script type="math/tex; mode=display">= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}I(T'> T)f_1(T') f_0(T)dT' dT</script><script type="math/tex; mode=display">= P(X_1 > X_0)</script><p>同样的，在有限个样本下，其面积用累加的方法计算(梯形面积)</p><p><img src="/2018/11/21/Metrics/ROC_curves_AUC.png" alt="ROC_curves_AUC"></p><script type="math/tex; mode=display">AUC = \sum_{i=1}^{m-1} \frac{1}{2} (y_{i+1} + y_i)(x_{i+1} - x_i)</script><ul><li>$AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>$0.5 &lt; AUC &lt; 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li><li>$AUC = 0.5$，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li><li>$AUC &lt; 0.5$，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li></ul><h3 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h3><p>以下为<code>scikit-learn</code>中混淆矩阵的<code>ROC</code>曲线<code>API</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y = np.array([1, 1, 2, 2])</span><br><span class="line">&gt;&gt;&gt; scores = np.array([0.1, 0.4, 0.35, 0.8])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)</span><br><span class="line">&gt;&gt;&gt; fpr</span><br><span class="line">array([ 0. ,  0.5,  0.5,  1. ])</span><br><span class="line">&gt;&gt;&gt; tpr</span><br><span class="line">array([ 0.5,  0.5,  1. ,  1. ])</span><br><span class="line">&gt;&gt;&gt; thresholds</span><br><span class="line">array([ 0.8 ,  0.4 ,  0.35,  0.1 ])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; metrics.auc(fpr, tpr)</span><br><span class="line">0.75</span><br></pre></td></tr></table></figure></p><h1 id="聚类-clustering-评估指标"><a href="#聚类-clustering-评估指标" class="headerlink" title="聚类(clustering)评估指标"></a>聚类(clustering)评估指标</h1><blockquote><ul><li><a href="https://blog.csdn.net/darkrabbit/article/details/80378597" target="_blank" rel="noopener">AI（005） - 笔记 - 聚类性能评估（Clustering Evaluation） - DarkRabbit的专栏 - CSDN博客 </a></li><li><a href="https://en.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener">Wikipedia, the free encyclopedia</a></li></ul></blockquote><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>聚类性能比较好，就是聚类结果簇内相似度<code>(intra-cluster similarity)</code>高，而簇间相似度<code>(inter-cluster similarity)</code>低，即同一簇的样本尽可能的相似，不同簇的样本尽可能不同。</p><p>聚类性能的评估（度量）分为两大类：</p><ul><li>外部评估<code>(external evaluation)</code>：将结果与某个参考模型<code>(reference model)</code>进行比较；</li><li>内部评估<code>(internal evaluation)</code>：直接考虑聚类结果而不利用任何参考模型。</li></ul><p>将$n_{samples}$个样本$\{x^{(1)}, …, x^{(n_{samples})}\}$用待评估聚类算法划分为$K$个类$\{X_1, …, X_K\}$，假定参考模型将其划分为$L$类$\{Y_1, …, Y_L\}$，将样本两辆匹配</p><script type="math/tex; mode=display">\begin{cases}    a = |SS| &  SS = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)}, x^{(j)} \in Y_l\} \\    b = |SD| &  SD = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \\    c = |DS| &  DS = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)}, x^{(j)} \in Y_l\} \\    d = |DD| &  DD = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\}\end{cases}</script><p>其中$k = 1, …, K; l = 1, …, L$</p><script type="math/tex; mode=display">a + b + c + d=   \left(        \begin{matrix}            n \\ 2        \end{matrix}    \right)= \frac{n(n-1)}{2}</script><blockquote><ul><li>$SS$包含两种划分中均属于同一类的样本对；</li><li>$SD$包含用待评估聚类算法划分中属于同一类，而在参考模型中属于不同类的样本对；</li><li>$DS$包含用待评估聚类算法划分中属于不同类，而在参考模型中属于同一类的样本对；</li><li>$DD$包含两种划分中均不属于同一类的样本对。</li></ul></blockquote><h2 id="常用外部评估-external-evaluation"><a href="#常用外部评估-external-evaluation" class="headerlink" title="常用外部评估(external evaluation)"></a>常用外部评估(external evaluation)</h2><h3 id="Rand-Index-RI"><a href="#Rand-Index-RI" class="headerlink" title="Rand Index(RI)"></a>Rand Index(RI)</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/Rand_index" target="_blank" rel="noopener">Rand index - Wikipedia</a></p></blockquote><script type="math/tex; mode=display">RI = \frac{a+d}{a + b + c + d} = \frac{a+d}{\left(\begin{matrix} n \\ 2 \end{matrix}\right)}</script><p>显然，结果值在$[0,1]$之间，且值越大越好。</p><ul><li>当为$0$时，两个聚类无重叠；</li><li>当为$1$时，两个聚类完全重叠。</li></ul><h3 id="Adjust-Rand-Index-ARI"><a href="#Adjust-Rand-Index-ARI" class="headerlink" title="Adjust Rand Index(ARI)"></a>Adjust Rand Index(ARI)</h3><p>让$RI$有了修正机会<code>(corrected-for-chance)</code>，在取值上从$[0,1]$变成$[-1, 1]$</p><p>对于$X$与$Y$的重叠可以用一个列联表<code>(contingency table)</code>表示，记作$[n_{ij}]$，$n_{ij} = |X_i \bigcap Y_j|$<br><img src="/2018/11/21/Metrics/聚类/ARI.svg" alt="ARI"></p><p>则定义$ARI$如下<br><img src="/2018/11/21/Metrics/聚类/ARI_Def.svg" alt="ARI_Def"></p><h3 id="互信息与调整互信息-Adjusted-Mutual-Information-AMI"><a href="#互信息与调整互信息-Adjusted-Mutual-Information-AMI" class="headerlink" title="互信息与调整互信息(Adjusted Mutual Information - AMI)"></a>互信息与调整互信息(Adjusted Mutual Information - AMI)</h3><blockquote><p>关于互信息可查看<a href="">熵</a>一节说明。</p></blockquote><p>$X_i$类别的概率定义为</p><script type="math/tex; mode=display">P(k) = \frac{|X_k|}{N}</script><p>则划分结果的熵定义为</p><script type="math/tex; mode=display">H(X) = - \sum_k P(k) \log P(k)</script><p>类似的</p><script type="math/tex; mode=display">P'(l) = \frac{|Y_l|}{N}</script><script type="math/tex; mode=display">H(Y) = - \sum_j P'(l) \log P'(l)</script><p>另外</p><script type="math/tex; mode=display">P(k, l) = \frac{|X_k, Y_l|}{N}</script><p>那么两种划分的互信息定义为</p><script type="math/tex; mode=display">MI(X, Y) = \sum_{k, l} P(k, l) \log \frac{P(k, l)}{P(k) P'(l)}</script><p>和$ARI$一样，我们对它进行调整。</p><script type="math/tex; mode=display">E[MI(X, Y)] = \sum_k \sum_l \sum_{n_{kl} = \max\{1, a_k + b_l - N\}}^{\min \{a_k, b_l\}}\frac{n_{kl}}{N}\log \left( \frac{N·n_{kl}}{a_k b_l} \right) ×</script><script type="math/tex; mode=display">\frac{a_k!b_l!(N-a_k)!(N-b_l)!}{N!n_{kl}!(a_k-n_{kl})!(b_l-n_{kl})!(N-a_k-b_l+n_{kl})!}</script><p>最终$AMI$表达式为</p><script type="math/tex; mode=display">AMI(X, Y) = \frac{MI(X, Y) - E[MI(X, Y)]}{\max \{H(X), H(Y)\} - E[MI(X, Y)]}</script><h3 id="同质性-Homogeneity-与完整性-Completeness"><a href="#同质性-Homogeneity-与完整性-Completeness" class="headerlink" title="同质性(Homogeneity)与完整性(Completeness)"></a>同质性(Homogeneity)与完整性(Completeness)</h3><p>这两个类似分类种的的准确率<code>(accuracy)</code>与召回率<code>(recall)</code>。</p><ul><li><p>同质性<code>(Homogeneity)</code><br>  即一个簇仅包含一个类别的样本</p><script type="math/tex; mode=display">  H = 1 - \frac{H(X|Y)}{H(X)}</script><p>  其中$H(X|Y)$为条件熵</p><script type="math/tex; mode=display">  H(X|Y) = \sum_k \sum_l P(X_k, Y_l) \log \frac{P(Y_l)}{P(X_k, Y_l)}  = \sum_k \sum_l \frac{n_{kl}}{N} \log \frac{n_{kl}}{N}</script></li><li><p>完整性<code>(Completeness)</code><br>  同类别样本被归类到相同簇中</p><script type="math/tex; mode=display">  C = 1 - \frac{H(Y|X)}{H(Y)}</script></li><li><p>$V-measure$<br>  <code>Homogeneity</code>和<code>Completeness</code>的调和平均</p><script type="math/tex; mode=display">  V = \frac{1}{\frac{1}{2} \left(\frac{1}{H} + \frac{1}{C}\right)} = \frac{2HC}{H + C}</script></li></ul><h3 id="Fowlkes-Mallows-index-FMI"><a href="#Fowlkes-Mallows-index-FMI" class="headerlink" title="Fowlkes-Mallows index(FMI)"></a>Fowlkes-Mallows index(FMI)</h3><p>成对精度和召回率的几何均值</p><blockquote><p><a href="https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index" target="_blank" rel="noopener">Fowlkes–Mallows index - Wikipedia</a></p></blockquote><p>定义</p><ul><li>$TP$ as the number of points that are present in the same cluster in both $A_1$ and $A_2$.</li><li>$FP$ as the number of points that are present in the same cluster in $A_1$ but not in $A_2$.</li><li>$FN$ as the number of points that are present in the same cluster in $A_2$ but not in $A_1$.</li><li>$TN$ as the number of points that are in different clusters in both $A_1$ and $A_2$.</li></ul><p>则</p><script type="math/tex; mode=display">TP + FP + TN + FN = \frac{n(n-1)}{2}</script><p>定义</p><script type="math/tex; mode=display">FMI = \sqrt{\frac{TP}{TP + FP} · \frac{TP}{TP + FN}}</script><h3 id="杰卡德系数-Jaccard-Coefficient-JC"><a href="#杰卡德系数-Jaccard-Coefficient-JC" class="headerlink" title="杰卡德系数(Jaccard Coefficient - JC)"></a>杰卡德系数(Jaccard Coefficient - JC)</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="noopener">Jaccard index - Wikipedia</a></p></blockquote><p>给定两个具有$n$个元素的集合$A, B$，定义</p><ul><li>$M_{11}$ represents the total number of attributes where $A$ and $B$ both have a value of $1$.</li><li>$M_{01}$ represents the total number of attributes where the attribute of $A$ is $0$ and the attribute of $B$ is $1$.</li><li>$M_{10}$ represents the total number of attributes where the attribute of $A$ is $1$ and the attribute of $B$ is $0$.</li><li>$M_{00}$ represents the total number of attributes where $A$ and $B$ both have a value of $0$.</li></ul><p>则有</p><script type="math/tex; mode=display">M_{11} + M_{01} + M_{10} + M_{00} = n</script><ul><li><p><code>Jaccard</code>相似度系数</p><script type="math/tex; mode=display">  J = \frac{M_{11}}{M_{01} + M_{10} + M_{11}}</script><blockquote><p>也即$J=\frac{A \cap B}{A \cup B}$</p></blockquote></li><li><p><code>Jaccard</code>距离</p><script type="math/tex; mode=display">  D_J = 1 - J</script></li></ul><h2 id="常用内部评估-internal-evaluation"><a href="#常用内部评估-internal-evaluation" class="headerlink" title="常用内部评估(internal evaluation)"></a>常用内部评估(internal evaluation)</h2><h3 id="轮廓系数-Silhouette-coefficient"><a href="#轮廓系数-Silhouette-coefficient" class="headerlink" title="轮廓系数(Silhouette coefficient)"></a>轮廓系数(Silhouette coefficient)</h3><p>又称侧影法，适用于实际类别信息未知的情况，对其中一个样本点$x^{(i)}$，记</p><ul><li>$a(i)$：到本簇其他样本点的距离的平均值</li><li>$b(i)$：该点到其他各个簇的样本点的平均距离的最小值</li></ul><p>定义轮廓系数</p><script type="math/tex; mode=display">S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}</script><p>或者</p><script type="math/tex; mode=display">S(i) = \begin{cases}    1 - \frac{a(i)}{b(i)} & a(i) < b(i) \\    0 & a(i) = b(i) \\    \frac{b(i)}{a(i)} - 1 & a(i) > b(i)\end{cases}</script><p>其含义如下</p><ul><li>当$a(i) \ll b(i)$时，无限接近于$1$，则意味着聚类合适；</li><li>当$a(i) \gg b(i)$时，无限接近于$-1$，则意味着把样本i聚类到相邻簇中更合适；</li><li>当$a(i)\approxeq b(i)$时，无限接近于$0$，则意味着样本在两个簇交集处。</li></ul><p>一般再对各个点的轮廓系数求均值</p><script type="math/tex; mode=display">\overline{S} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} S(i)</script><ul><li>当$\overline{S} &gt; 0.5$，表示聚类合适；</li><li>当$\overline{S} &lt; 0.2$，表示表明数据不存在聚类特征</li></ul><h3 id="Calinski-Harabaz-CH"><a href="#Calinski-Harabaz-CH" class="headerlink" title="Calinski-Harabaz(CH)"></a>Calinski-Harabaz(CH)</h3><p>也适用于实际类别信息未知的情况，以$K$分类为例</p><ul><li><p>类内散度$W$</p><script type="math/tex; mode=display">  W(K) = \sum_k \sum_{C(j)=k} ||x_j - \overline{x_k}||^2</script></li><li><p>类间散度$B$</p><script type="math/tex; mode=display">  B(K) = \sum_k a_k ||\overline{x_k} - \overline{x}||^2</script></li><li><p>$CH$</p><script type="math/tex; mode=display">  CH(K) = \frac{B(K)(N-K)}{W(K)(K-1)}</script></li></ul><h3 id="Davies-Bouldin-Index-DBI"><a href="#Davies-Bouldin-Index-DBI" class="headerlink" title="Davies-Bouldin Index(DBI)"></a>Davies-Bouldin Index(DBI)</h3><p>定义</p><ul><li>$c_k$：簇$C_k$的中心点</li><li>$\sigma_k$：簇$C_k$中所有元素到$c_k$的距离的均值</li><li>$d(c_i, c_j)$：簇中心$c_i$与$c_j$之间的距离</li></ul><p>则</p><script type="math/tex; mode=display">DBI = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)</script><p>$DBI$越小越好</p><h3 id="Dunn-index-DI"><a href="#Dunn-index-DI" class="headerlink" title="Dunn index(DI)"></a>Dunn index(DI)</h3><p>定义</p><ul><li>$d(i,j)$：两类簇的距离，定义方法多样，例如两类簇中心的距离；</li><li>$d’(k)$：簇$C_k$的类内距离，同样的，可定义多种，例如簇$C_k$中任意两点距离的最大值。</li></ul><p>则</p><script type="math/tex; mode=display">DI = \frac{\min_{1 \leq i < j \leq K} d(i, j)}{\max_{1 \leq k \leq K} d'(k)}</script><h1 id="sklearn中的评价指标"><a href="#sklearn中的评价指标" class="headerlink" title="sklearn中的评价指标"></a>sklearn中的评价指标</h1><blockquote><p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/model_evaluation.html" target="_blank" rel="noopener">3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.19.0 documentation - ApacheCN</a></p></blockquote><p><img src="/2018/11/21/Metrics/sklearn_metrics.png" alt="sklearn_metrics"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; dir(metrics)</span><br><span class="line">[&apos;SCORERS&apos;, &apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, </span><br><span class="line">&apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;,</span><br><span class="line"> &apos;accuracy_score&apos;, &apos;adjusted_mutual_info_score&apos;, &apos;adjusted_rand_score&apos;, </span><br><span class="line"> &apos;auc&apos;, &apos;average_precision_score&apos;, &apos;balanced_accuracy_score&apos;, </span><br><span class="line"> &apos;base&apos;, &apos;brier_score_loss&apos;, &apos;calinski_harabaz_score&apos;, &apos;check_scoring&apos;, </span><br><span class="line"> &apos;classification&apos;, &apos;classification_report&apos;, &apos;cluster&apos;, &apos;cohen_kappa_score&apos;, </span><br><span class="line"> &apos;completeness_score&apos;, &apos;confusion_matrix&apos;, &apos;consensus_score&apos;, </span><br><span class="line"> &apos;coverage_error&apos;, &apos;davies_bouldin_score&apos;, &apos;euclidean_distances&apos;, </span><br><span class="line"> &apos;explained_variance_score&apos;, &apos;f1_score&apos;, &apos;fbeta_score&apos;, </span><br><span class="line"> &apos;fowlkes_mallows_score&apos;, &apos;get_scorer&apos;, &apos;hamming_loss&apos;, &apos;hinge_loss&apos;, </span><br><span class="line"> &apos;homogeneity_completeness_v_measure&apos;, &apos;homogeneity_score&apos;, </span><br><span class="line"> &apos;jaccard_similarity_score&apos;, &apos;label_ranking_average_precision_score&apos;, </span><br><span class="line"> &apos;label_ranking_loss&apos;, &apos;log_loss&apos;, &apos;make_scorer&apos;, &apos;matthews_corrcoef&apos;, </span><br><span class="line"> &apos;mean_absolute_error&apos;, &apos;mean_squared_error&apos;, &apos;mean_squared_log_error&apos;, </span><br><span class="line"> &apos;median_absolute_error&apos;, &apos;mutual_info_score&apos;, </span><br><span class="line"> &apos;normalized_mutual_info_score&apos;, &apos;pairwise&apos;, &apos;pairwise_distances&apos;, </span><br><span class="line"> &apos;pairwise_distances_argmin&apos;, &apos;pairwise_distances_argmin_min&apos;, </span><br><span class="line"> &apos;pairwise_distances_chunked&apos;, &apos;pairwise_fast&apos;, &apos;pairwise_kernels&apos;, </span><br><span class="line"> &apos;precision_recall_curve&apos;, &apos;precision_recall_fscore_support&apos;, </span><br><span class="line"> &apos;precision_score&apos;, &apos;r2_score&apos;, &apos;ranking&apos;, &apos;recall_score&apos;, &apos;regression&apos;, </span><br><span class="line"> &apos;roc_auc_score&apos;, &apos;roc_curve&apos;, &apos;scorer&apos;, &apos;silhouette_samples&apos;, </span><br><span class="line"> &apos;silhouette_score&apos;, </span><br><span class="line"> &apos;v_measure_score&apos;, </span><br><span class="line"> &apos;zero_one_loss&apos;]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Entropy</title>
      <link href="/2018/11/21/Entropy/"/>
      <url>/2018/11/21/Entropy/</url>
      
        <content type="html"><![CDATA[<h1 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h1><p>概率$p$是对确定性的度量，那么信息量就是对不确定性的度量，公式定义为</p><script type="math/tex; mode=display">I(x) = - \log p(x) \tag{1}</script><p>信息量也被称为随机变量$x$的自信息<code>(self-information)</code></p><blockquote><p>底数为$2$时，单位为<code>bit</code>，底数为$e$时，单位为<code>nat</code></p></blockquote><p><img src="/2018/11/21/Entropy/信息量.png" alt="信息量"></p><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>信息熵<code>(information entropy)</code>定义为</p><script type="math/tex; mode=display">H(X) = - \sum_{x} p(x) \log p(x) \tag{2}</script><p>可看作<strong>信息量的期望</strong>,在$0-1$分布的信息熵为</p><script type="math/tex; mode=display">H(p) = - p \log p - (1 - p) \log (1 - p)</script><p>图像如下，可见在$p=0.5$时，熵最大。<br><img src="/2018/11/21/Entropy/entropy_of_01.png" alt="entropy_of_01"></p><blockquote><p>函数$y=x \log x$的图像<br><img src="/2018/11/21/Entropy/xlogx.png" alt="xlogx"><br>有</p><script type="math/tex; mode=display">\lim_{x \rightarrow 0} y = \lim_{x \rightarrow 1} y = 0</script></blockquote><h1 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h1><p>根据信息熵的定义，推广到多维随机变量，就得到联合熵的定义式，以$2$维随机变量为例</p><script type="math/tex; mode=display">H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y) \tag{3}</script><p>可推广至多维。</p><!-- 机器学习笔记十：各种熵总结 - 谢小小XH - CSDN博客 https://blog.csdn.net/xierhacker/article/details/53463567 --><h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>现在有关于样本集的两个概率分布$p(x)$和$q(x)$，其中$p(x)$为真实分布，$q(x)$非真实分布。</p><p>如果用真实分布$p(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p) = - \sum_x p(x) \log p(x)</script><p>如果用非真实分布$q(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p, q) = - \sum_x p(x) \log q(x) \tag{4}</script><p>注意</p><script type="math/tex; mode=display">H(p, q) - H(p)= \sum_x p(x) \log \frac{p(x)}{q(x)}= D_{KL}(p||q)</script><p>当用非真实分布$q(x)$得到的平均码长比真实分布$p(x)$得到的平均码长多出的比特数就是相对熵。我们希望通过最小化相对熵$D_{KL}(p||q)$使$q(x)$尽量趋近$p(x)$，即</p><script type="math/tex; mode=display">q(x) = \arg \min_{q(x)} D_{KL} (p||q)</script><p>而$H(p)$是样本集的熵，为固定的值，故</p><script type="math/tex; mode=display">q(x) = \arg \min_{q(x)} H(p, q)</script><p>即等价于最小化交叉熵。</p><h1 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h1><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。定义为在给定$X$下$Y$的条件概率分布的熵对$X$的期望，即</p><script type="math/tex; mode=display">H(Y|X) = E_{p(x)} H(Y|X=x)= \sum_x p(x) H(Y|X=x) \tag{5}</script><p>其中</p><script type="math/tex; mode=display">H(Y|X=x) = - \sum_y p(y|x) \log p(y|x)</script><p>故</p><script type="math/tex; mode=display">H(Y|X) = \sum_x p(x) \left[- \sum_y p(y|x) \log p(y|x)\right]</script><script type="math/tex; mode=display">= - \sum_x \sum_y p(x, y) \log p(y|x)</script><p>即</p><script type="math/tex; mode=display">H(Y|X) = - \sum_{x, y} p(x, y) \log p(y|x) \tag{6}</script><p>实际上，条件熵满足</p><script type="math/tex; mode=display">H(Y|X) = H(X, Y) - H(X) \tag{7}</script><blockquote><p>证明：<br>已知</p><script type="math/tex; mode=display">H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y)</script><script type="math/tex; mode=display">H(X) = - \sum_{x} p(x) \log p(x)</script><p>则</p><script type="math/tex; mode=display">H(X, Y) - H(X)</script><script type="math/tex; mode=display">= - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x} p(x) \log p(x)</script><script type="math/tex; mode=display">= - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x, y} p(x, y) \log p(x)</script><script type="math/tex; mode=display">= \sum_{x, y} p(x, y) \log \frac{p(x)}{p(x, y)}</script><script type="math/tex; mode=display">= \sum_{x, y} p(x, y) \log p(y|x)</script><script type="math/tex; mode=display">= H(Y|X)</script></blockquote><h1 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h1><p>相对熵<code>(relative entropy)</code>，又称<code>KL</code>散度<code>(Kullback–Leibler divergence)</code>。可以用来衡量两个概率分布之间的差异，就是求$p(x)$与$q(x)$之间的对数差在 pp 上的期望值。</p><script type="math/tex; mode=display">D_{KL} (p||q) = E_{p(x)} \log \frac{p(x)}{q(x)}= \sum_x p(x) \log \frac{p(x)}{q(x)} \tag{8}</script><p>注意</p><ul><li><p>相对熵不具有对称性，即</p><script type="math/tex; mode=display">  D_{KL} (p||q) \neq D_{KL} (q||p)</script></li><li><p>$D_{KL} (p||q) \geq 0$</p><blockquote><p>证明：</p><script type="math/tex; mode=display">D_{KL} (p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = - \sum_x p(x) \log \frac{q(x)}{p(x)}</script><p>由<code>Jensen inequality</code></p><script type="math/tex; mode=display">\sum_x p(x) \log \frac{q(x)}{p(x)}\leq \log \sum_x p(x) \frac{q(x)}{p(x)}= \log \sum_x q(x)</script><p>所以</p><script type="math/tex; mode=display">D_{KL} (p||q) \geq - \log \sum_x q(x)</script><p>而$0 \leq q(x) \leq 1$，故</p><script type="math/tex; mode=display">D_{KL} (p||q) \geq 0</script></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learing </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Non-parameter Estimation</title>
      <link href="/2018/11/19/Non-parameter-Estimation/"/>
      <url>/2018/11/19/Non-parameter-Estimation/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>若参数估计时我们不知道样本的分布形式，那么就无法确定需要估计的概率密度函数，无法用<a href="https://louishsu.xyz/2018/10/22/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">最大似然估计、贝叶斯估计等参数估计方法</a>，应该用非参数估计方法。</p><p>需要知道的是，作为非参数方法的共同问题是对样本数量需求较大，只要样本数目足够大众可以保证收敛于任何复杂的位置密度，但是计算量和存储量都比较大。当样本数很少时，如果能够对密度函数有先验认识，则参数估计能取得更好的估计效果。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>若有$M$个样本$x^{(1)}, …, x^{(M)}$，依概率密度函数$p(x)$独立同分布抽样得到。</p><p>一个样本$x$落在区域$R$中的概率$P$可表示为</p><script type="math/tex; mode=display">P = \int_R p(x) dx \tag{1}</script><p>我们通过计算$P$来估计概率密度$p(x)$。</p><p>$K$个样本落入区域$R$的概率$P_K$为二项分布，即$K \sim B(M, P)$</p><script type="math/tex; mode=display">P_K = \left(\begin{matrix} M\\K \end{matrix}\right) P^K (1-P)^{M-K} \tag{2}</script><p>则$K$的期望与方差分别为</p><script type="math/tex; mode=display">E(K) = MP;　D(K) = MP(1-P)</script><p>样本个数$M$越多，$D(K)$越大，即$K$在期望附近的波峰越明显，因此样本足够多时，用$K/M$作为$P$的一个估计非常准确，即</p><script type="math/tex; mode=display">P \approx \frac{K}{M} \tag{3}</script><p>若我们假设$p(x)$是连续的，且区域$R$足够小，记其体积为$V$，那么有</p><script type="math/tex; mode=display">P = \int_R p(x)dx \approx p(x) V \tag{4}</script><p>所以根据$(3)(4)$，得到</p><script type="math/tex; mode=display">p(x) \approx \frac{K/M}{V} \tag{*}</script><p>但是我们获得的其实为平滑后的概率密度函数</p><script type="math/tex; mode=display">\frac{P}{V} = \frac{\int_R p(x)dx}{\int_R dx}</script><p>我们希望其尽可能地趋近$p(x)$，那么必须要求$V \rightarrow 0$，但是这样就可能不包含任何样本，那么$p(x)\approx 0$，这样估计的结果毫无意义。</p><p>所以在实际中，一般构造多个包含样本$x$的区域$R_1, …, R_i, …, R_n$，第$i$个区域使用$i$个样本，记$V_i$为$R_i$的体积，$M_i$为落在$R_i$中的样本个数，则对$p(x)$第$i$次估计$p_i(x)$表示为</p><script type="math/tex; mode=display">p_i(x) \approx \frac{M_i / M}{V_i} \tag{5}</script><p>若要求$p_i(x)$收敛到$p(x)$，则必须满足</p><ul><li>$\lim_{i\rightarrow \infty} V_i = 0$</li><li>$\lim_{i\rightarrow \infty} M_i = 0$</li><li>$\lim_{i\rightarrow \infty} \frac{M_i}{M} = 0$</li></ul><h1 id="直方图法"><a href="#直方图法" class="headerlink" title="直方图法"></a>直方图法</h1><p>记不记得小学时的直方图统计，直方图方法的思想就是这样，以$1$维样本为例，我们将$x$的取值范围平均等分为$K$个区间，统计每个区间内样本的个数，由此计算区间的概率密度。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>若共有$N$维样本$M$组，在每个维度上$K$等分，就有$K^N$个小空间，每个小空间的体积$V_i$可以定义为</p><script type="math/tex; mode=display">V_i = \prod_{n=1}^N d_n,　i=1,...,K^N</script><p>其中</p><script type="math/tex; mode=display">d_n = \frac{\max x_n - \min x_n}{K}</script><p>假设样本落到各个小空间的概率相同，若第$i$个小空间包含$M_i$个样本，则该空间的概率密度$\hat{p_i}$为</p><script type="math/tex; mode=display">\hat{p_i} = \frac{M_i / M}{V_i} \tag{6}</script><p>估计的效果与小区间的大小密切相连，如果区域选择过大，会导致最终估计出来的概率密度函数非常粗糙；如果区域的选择过小，可能会导致有些区域内根本没有样本或者样本非常少，这样会导致估计出来的概率密度函数很不连续。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p><p>我们可以用<code>matplotlib.pyplot.hist()</code>或<code>numpy.histogram()</code>实现</p><ul><li><p><code>matplotlib</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n, bins, patches = plt.hist(arr, bins=10, normed=0, facecolor=&apos;black&apos;, edgecolor=&apos;black&apos;,alpha=1，histtype=&apos;bar&apos;)</span><br></pre></td></tr></table></figure><ul><li><code>Args</code><br>  参数很多，选几个常用的讲解<ul><li>arr: 需要计算直方图的一维数组</li><li>bins: 直方图的柱数，可选项，默认为10</li><li>normed: 是否将得到的直方图向量归一化。默认为0</li><li>facecolor: 直方图颜色</li><li>edgecolor: 直方图边框颜色</li><li>alpha: 透明度</li><li>histtype: 直方图类型，‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’</li></ul></li><li><code>Returns</code><ul><li>n: 直方图向量，是否归一化由参数normed设定</li><li>bins: 返回各个bin的区间范围</li><li>patches: 返回每个bin里面包含的数据，是一个list</li></ul></li></ul></li><li><p><code>numpy</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hist, bin_edges = histogram(a, bins=10, range=None, normed=None, weights=None, density=None)</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def histEstimate(X, n_bins, showfig=False):</span><br><span class="line">    &quot;&quot;&quot; 直方图密度估计</span><br><span class="line">    Args:</span><br><span class="line">        n_bins: &#123;int&#125; 直方图的条数</span><br><span class="line">    Returns:</span><br><span class="line">        hist: &#123;ndarray(n_bins,)&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    n, bins, patches = plt.hist(X, bins=n_bins, normed=1, facecolor=&apos;lightblue&apos;, edgecolor=&apos;white&apos;)</span><br><span class="line">    if showfig: plt.show()</span><br><span class="line">    return n, bins, patches</span><br></pre></td></tr></table></figure><p><code>matplotlib</code>直方图显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_matplotlib.png" alt="hist_matplotlib"></p><p>拟合各中心点显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_ploy.png" alt="hist_ploy"></p><h1 id="K-n-近邻估计法"><a href="#K-n-近邻估计法" class="headerlink" title="$K_n$近邻估计法"></a>$K_n$近邻估计法</h1><p>随着样本数的增加，区域的体积应该尽可能小，同时又必须保证区域内有充分多的样本，但是每个区域的样本数有必须是总样本数的很小的一部分，而不是与直方图估计那样体积不变。</p><p>那么我们想，能否根据样本的分布调整分区大小呢，$K$近邻估计法就是一种采用可变大小区间的密度估计方法。</p><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>根据总样本确定参数$K_n$，在求样本$x$处的密度估计$\hat{p}(x)$时，调整区域体积$V(x)$，直到区域内恰好落入$K_n$个样本，估计公式为</p><script type="math/tex; mode=display">\hat{p}(x) = \frac{K_n/M}{V(x)} \tag{7}</script><p>一般指定超参数$a$，取</p><script type="math/tex; mode=display">K_n = a × \sqrt{M} \tag{8}</script><blockquote><script type="math/tex; mode=display">\hat{p}(x) = \frac{a × \sqrt{M} /M}{V(x)} = \frac{K_n'/M}{V'(x)}</script><p>其中$K_n’ = a,V’(x) = V(x)×\frac{1}{\sqrt{M}}$</p></blockquote><p>在样本密度比较高的区域的体积就会比较小，而在密度低的区域的体积则会自动增大，这样就能够较好的兼顾在高密度区域估计的分辨率和在低密度区域估计的连续性。</p><h1 id="Parzen窗法"><a href="#Parzen窗法" class="headerlink" title="Parzen窗法"></a>Parzen窗法</h1><p>又称核密度估计。</p><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>我们暂时假设待估计点$x$的附近区间$R$为一个$N$维的<strong>超立方体</strong>，用$h$表示边的长度，那么</p><script type="math/tex; mode=display">V_i = h^N</script><p>即<br><img src="/2018/11/19/Non-parameter-Estimation/Parzen_window.jpg" alt="Parzen_window"><br>定义窗函数$\varphi(·)$，表示落入以$x$为中心的超立方体的区域的点</p><script type="math/tex; mode=display">\varphi \left(\frac{x_i-x}{h}\right) = \begin{cases}    1 & \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2},　n=1,...,N \\    0 & otherwise\end{cases} \tag{9}</script><blockquote><script type="math/tex; mode=display">\frac{|x_{in}-x_n|}{h} \leq \frac{1}{2}　即　(x_i-x)_n \leq \frac{h}{2}</script><p>这里的$h$起到单位化的作用，便于推广</p></blockquote><p>那么落入以$x$为中心的<strong>超立方体</strong>的区域的点的个数为</p><script type="math/tex; mode=display">M_i = \sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right) \tag{10}</script><p>代入$p(x) \approx \frac{M_i/M}{V_i}$，我们得到</p><script type="math/tex; mode=display">p(x) \approx \frac{\sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right)/M}{V_i}= \frac{1}{M} \sum_{i=1}^M \frac{1}{V_i} \varphi \left(\frac{x_i-x}{h}\right) \tag{11}</script><p>我们定义核函数(或称“窗函数”)</p><script type="math/tex; mode=display">\kappa(z) = \frac{1}{V_i} \varphi(z) \tag{12}</script><p>核函数反应了一个观测样本$x_i$对在$x$处的概率密度估计的贡献，与样本$x_i$和$x$的距离有关。而概率密度估计就是在这一点上把所有观测样本的贡献进行平均</p><script type="math/tex; mode=display">p(x) \approx \frac{1}{M} \sum_{i=1}^M \kappa\left(\frac{x_i-x}{h}\right) \tag{13}</script><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>核函数应满足概率密度的要求，即</p><script type="math/tex; mode=display">\kappa(z) \geq 0　\And　\int \kappa(z)dz = 1</script><p>通常有以下几种核函数</p><ul><li><p>均匀核</p><script type="math/tex; mode=display">  \kappa(z)  = \begin{cases}      1 & |z_n| \leq \frac{1}{2},　n=1,...,N \\      0 & otherwise  \end{cases}</script></li><li><p>高斯核(正态核)<br>  高斯核是将窗放大到整个空间，各个观测样本$x_i$对待观测点$x$的加权和(越远权值越小)。</p><script type="math/tex; mode=display">  \kappa(z)  = \frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}}  \exp \left(-\frac{1}{2} (z - \mu)^T \Sigma^{-1} (z - \mu)\right)</script></li><li><p>超球窗</p><script type="math/tex; mode=display">  \kappa(z)  = \begin{cases}      V^{-1} & ||z|| \leq 1 \\      0 & otherwise  \end{cases}</script><blockquote><p>$z=\frac{x_i-x}{h}$，故$||z||\leq 1$即$||x_i-x||^2\leq h^2$<br>此时$h$表示超球体的半径</p></blockquote></li></ul><h2 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h2><p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" target="_blank" rel="noopener">sklearn.neighbors.KernelDensity — scikit-learn 0.19.0 documentation - ApacheCN</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.neighbors import KernelDensity</span><br><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])</span><br><span class="line">&gt;&gt;&gt; kde = KernelDensity(kernel=&apos;gaussian&apos;, bandwidth=0.2).fit(X)</span><br><span class="line">&gt;&gt;&gt; kde.score_samples(X)</span><br><span class="line">array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,</span><br><span class="line">       -0.41076071])</span><br><span class="line">&gt;&gt;&gt; kde.sample(10)</span><br><span class="line">array([[ 1.80042291,  1.1030739 ],</span><br><span class="line">       [ 0.87299669,  1.0762352 ],</span><br><span class="line">       [-2.40180586, -1.19554374],</span><br><span class="line">       [-1.97985919, -1.19361193],</span><br><span class="line">       [-2.95866231, -2.1972637 ],</span><br><span class="line">       [-1.12739556, -0.80851063],</span><br><span class="line">       [ 1.03756706,  1.24855099],</span><br><span class="line">       [ 1.21729703,  1.02345815],</span><br><span class="line">       [-2.11816867, -1.0486257 ],</span><br><span class="line">       [-1.04875537, -0.89928711]])</span><br></pre></td></tr></table></figure></p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p>具体代码见<br><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p><p>定义核函数如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 高斯核</span><br><span class="line">gaussian = lambda z: np.exp(-0.5*(np.linalg.norm(z)**2)) / np.sqrt(2*np.pi)</span><br><span class="line"># 均匀核</span><br><span class="line">square = lambda z: 1 if (np.linalg.norm(z) &lt;= 0.5) else 0</span><br></pre></td></tr></table></figure></p><p>密度估计函数如下，需要对连续范围内的各个点，即$x \in [min(X), max(X)]$进行估计获得<code>p</code>，作图显示$x-p$即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def parzenEstimate(X, kernel, h, n_num=50):</span><br><span class="line">    &quot;&quot;&quot; 核参数估计</span><br><span class="line">    Args:</span><br><span class="line">        X: &#123;ndarray(n_samples,)&#125;</span><br><span class="line">        kernel: &#123;function&#125; 可调用的核函数</span><br><span class="line">        h: &#123;float&#125; 核函数的参数</span><br><span class="line">    Returns:</span><br><span class="line">        p: &#123;ndarray(n_num,)&#125;</span><br><span class="line">    Notes:</span><br><span class="line">        - 一维，故`V_i = h`</span><br><span class="line">        - p(x) = \frac&#123;1&#125;&#123;M&#125; \sum_&#123;i=1&#125;^M \kappa \left( \frac&#123;x_i - x&#125;&#123;h&#125; \right)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = np.linspace(np.min(X), np.max(X), num=n_num)</span><br><span class="line">    p = np.zeros(shape=(x.shape[0],))</span><br><span class="line">    z = lambda x, x_i, h: (x - x_i) / h</span><br><span class="line">    V_i = h; n_samples = X.shape[0]</span><br><span class="line">    for idx in range(x.shape[0]):</span><br><span class="line">        for i in range(X.shape[0]):</span><br><span class="line">            p[idx] += kernel(z(x[idx], X[i], h)) / V_i</span><br><span class="line">        p[idx] /= n_samples</span><br><span class="line">    return p</span><br></pre></td></tr></table></figure></p><h3 id="均匀核"><a href="#均匀核" class="headerlink" title="均匀核"></a>均匀核</h3><ul><li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.5.png" alt="01_h_0.5"></p></li><li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.8.png" alt="01_h_0.8"></p></li><li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_1.0.png" alt="01_h_1.0"></p></li><li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_2.0.png" alt="01_h_2.0"></p></li></ul><h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><ul><li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.5.png" alt="gaussian_h_0.5"></p></li><li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.8.png" alt="gaussian_h_0.8"></p></li><li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_1.0.png" alt="gaussian_h_1.0"></p></li><li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_2.0.png" alt="gaussian_h_2.0"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Parameter Estimation</title>
      <link href="/2018/11/19/Parameter-Estimation/"/>
      <url>/2018/11/19/Parameter-Estimation/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/20587681/answer/17435552" target="_blank" rel="noopener">贝叶斯学派与频率学派有何不同？ - 任坤的回答 - 知乎</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>参数估计<code>(parameter estimation)</code>，统计推断的一种。根据从总体中抽取的随机样书．来估计总体分布中未知参数的过程。主要介绍最大似然估计<code>(MLE: Maximum Likelihood Estimation)</code>，最大后验概率估计<code>(MAP: Maximum A Posteriori Estimation)</code>，贝叶斯估计<code>(Bayesian Estimation)</code>。</p><blockquote><p>解释一下“似然函数”和“后验概率”，在<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>一节，给出定义如下</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><p>上式中$ k=1,…,K  $，各部分定义如下<br>$P(c_k|x)$——<code>后验概率(posteriori probability)</code><br>$P(c_k)$——<code>先验概率(priori probability)</code><br>$p(x|c_k)$——$c_k$关于$x$的<code>似然函数(likelihood)</code><br>$p(x)$——<code>证据因子(evidence)</code></p></blockquote><h1 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h1><p>以最经典的掷硬币实验为例，假设有一枚硬币，投掷一次出现正面记$”1”$，投掷$10$次的实验结果如下</p><script type="math/tex; mode=display">\{ 0， 1， 1， 1， 1， 0， 1， 1， 1，0 \}</script><p>记硬币投掷结果为随机变量$X$，且$ x \in \{0, 1\}$，硬币投掷一次服从二项分布，估计二项分布的参数$\theta$</p><h1 id="最大似然估计-MLE"><a href="#最大似然估计-MLE" class="headerlink" title="最大似然估计(MLE)"></a>最大似然估计(MLE)</h1><h2 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h2><blockquote><p><a href="https://en.wikipedia.org/wiki/Likelihood_function#Definition" target="_blank" rel="noopener">Likelihood function - Wikipedia</a></p></blockquote><ul><li><p>离散型</p><script type="math/tex; mode=display">L(x | \theta) = p_{\theta}(x)=P_{\theta}(X = x)</script></li><li><p>连续型</p><script type="math/tex; mode=display">L(x | \theta) = f_{\theta}(x)</script></li></ul><blockquote><p>很多人能讲出一大堆哲学理论来阐明这一对区别。<br>但我觉得，从工程师角度来讲，这样理解就够了:<br>频率 $vs$ 贝叶斯 = $P(X; w)$ $vs$ $P(X|w)$ 或 $P(X,w)$</p><p>作者：许铁-巡洋舰科技<br>链接：<a href="https://www.zhihu.com/question/20587681/answer/122348889" target="_blank" rel="noopener">https://www.zhihu.com/question/20587681/answer/122348889</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>有数据集$D = \{x_1, x_2, …, x_N\}$，按$c$个类别分成$\{D_1, D_2, …, D_C\}$，各个类别服从的概率分布密度函数模型已给出，估计参数$\hat{\Theta} = \{\hat{\theta}_{c_1}, \hat{\theta}_{c_2}, …, \hat{\theta}_{c_C}\} $</p><p><strong>假定</strong></p><ul><li>类别间独立，且各自服从概率分布密度函数为$p(x|c_j)$</li><li>各类别的概率密度$p(x|c_j)$以参数$\theta_{c_j}$确定，即$p(x|c_j; \theta_{c_j})$</li></ul><p>故似然函数为</p><script type="math/tex; mode=display">L(D | \Theta) = P(x_1, x_2, ..., x_N | \Theta) = \prod_{i=1}^N p(x_i | \theta_{x_i \in c_j})</script><blockquote><p>理解为，在参数$\Theta$为何值的条件下，实验结果出现数据集$D$的概率最大</p></blockquote><p>求取其极大值对应的参数即可</p><ul><li>一般取对数似然函数<script type="math/tex; mode=display">\log L(D | \Theta) = \sum_{i=1}^N \log p(x_i | \theta_{x_i \in c_j})</script></li><li>极大值即对应梯度为$\vec{0}$的位置，即<script type="math/tex; mode=display">∇_\Theta  \log L(D | \Theta) = \vec{0}\Rightarrow\hat{\Theta}</script></li></ul><blockquote><p>Some comments about ML</p><ul><li>ML estimation is usually simpler than alternative methods. </li><li>Has good convergence properties as the number of training samples increases. </li><li>If the model chosen for p(x|θ) is correct, and independence assumptions among variables are true, ML will give very good results.</li><li>If the model is wrong, ML will give poor results.<div style="text-align: right"> —— Zhao Haitao. Maximum Likelihood and Bayes Estimation </div></li></ul></blockquote><h2 id="例：正态分布的最大似然估计"><a href="#例：正态分布的最大似然估计" class="headerlink" title="例：正态分布的最大似然估计"></a>例：正态分布的最大似然估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的的最大似然估计</p><script type="math/tex; mode=display">P(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">L(D | \mu, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}=\left( \frac{1}{\sqrt{2\pi} \sigma } \right)^N \prod_{i=1}^N e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><p>取对数似然</p><script type="math/tex; mode=display">\log L(D | \mu, \sigma^2) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2</script><h3 id="1-参数-mu-的估计"><a href="#1-参数-mu-的估计" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><script type="math/tex; mode=display">\frac{∂}{∂\mu} L(D | \mu, \sigma^2) = \frac{1}{\sigma^2} (\sum_{i=1}^N x_i - N\mu)= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\mu}= \frac{1}{N}\sum_{i=1}^N x_i</script><h3 id="2-参数-sigma-2-的估计"><a href="#2-参数-sigma-2-的估计" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2} \log L(D | \mu, \sigma^2) = - \frac{N}{2\sigma^2} +\frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\sigma^2} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2</script><blockquote><p>参数$\hat{\mu}, \hat{\sigma}^2$的值与样本均值和样本方差相等</p></blockquote><h1 id="最大后验概率估计-MAP"><a href="#最大后验概率估计-MAP" class="headerlink" title="最大后验概率估计(MAP)"></a>最大后验概率估计(MAP)</h1><!-- > [高斯混合模型(GMM)与最大期望算法(EM)]() --><h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>最大似然估计是求参数$\theta$, 使似然函数$P(D | \theta)$最大，最大后验概率估计则是求$\theta$使$P(\theta | D)$最大</p><blockquote><p>理解为，在已出现的实验样本$D$上，参数$\theta$取何值的概率最大</p></blockquote><p>且注意到</p><script type="math/tex; mode=display">P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}</script><p>故$MAP$不仅仅使似然函数$P(D | \theta)$最大，而且使$P(\theta)$最大，即</p><script type="math/tex; mode=display">\theta = argmax L(\theta | D)</script><script type="math/tex; mode=display">L(\theta | D) = P(\theta) P(D | \theta)= P(\theta) \prod_{i=1}^N p(x_i | \theta)</script><blockquote><p>比$ML$多了一项$P(\theta)$</p></blockquote><ul><li><p>取对数后</p><script type="math/tex; mode=display">\log L(\theta | D) = \sum_{i=1}^N \log p(x_i | \theta) + \log P(\theta)</script></li><li><p>求取极大值</p><script type="math/tex; mode=display">∇_\theta L(\theta | D) = 0\Rightarrow\hat{\theta}</script></li></ul><blockquote><p>$MAP$和$MLE$的区别：<br>$MAP$允许我们把先验知识加入到估计模型中，这在<strong>样本很少</strong>的时候是很有用的，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如<code>beta</code>分布的$\alpha, \beta$，我们还可以调节把估计的结果“拉”向先验的幅度，$\alpha, \beta$越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。<br><a href="https://blog.csdn.net/hustlx/article/details/51144710" target="_blank" rel="noopener">极大似然估计，最大后验概率估计(MAP)，贝叶斯估计 - 李鑫o_O - CSDN博客</a></p></blockquote><h2 id="例：正态分布的最大后验概率估计"><a href="#例：正态分布的最大后验概率估计" class="headerlink" title="例：正态分布的最大后验概率估计"></a>例：正态分布的最大后验概率估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的最大后验概率估计</p><script type="math/tex; mode=display">p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><blockquote><script type="math/tex; mode=display">\log p(x_i | \mu, \sigma^2)= - \frac{1}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (x_i - \mu)^2</script></blockquote><h3 id="1-参数-mu-的估计-1"><a href="#1-参数-mu-的估计-1" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p><script type="math/tex; mode=display">p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><blockquote><script type="math/tex; mode=display">\log p(\mu)= - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script></blockquote><p>则</p><script type="math/tex; mode=display">\log L(\mu, \sigma^2 | D)= - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script><p>则</p><script type="math/tex; mode=display">\frac{∂}{∂\mu} \log L(\mu, \sigma^2 | D)= \frac{1}{\sigma^2} \sum_{i=0}^N (x_i - \mu) - \frac{1}{\sigma_{\mu_0}^2} (\mu - \mu_0)= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\mu}= \frac{\mu_0 \sigma^2 + \sigma_{\mu_0}^2 \sum_{i=0}^N x_i}{\sigma^2 + N \sigma_{\mu_0}^2}= \frac{\mu_0 + \frac{\sigma_{\mu_0}^2}{\sigma^2} \sum_{i=0}^N x_i}{1 + \frac{\sigma_{\mu_0}^2}{\sigma^2} N }</script><h3 id="2-参数-sigma-2-的估计-1"><a href="#2-参数-sigma-2-的估计-1" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><p>给定先验条件：$\sigma^2$服从正态分布$N(\sigma_0^2, \sigma_{\sigma_0^2}^2)$，即</p><script type="math/tex; mode=display">p(\sigma^2) = \frac{1}{\sqrt{2\pi} \sigma_{\sigma_0^2}} e^ {-\frac{(\sigma^2- \sigma_0^2)^2}{2 \sigma_{\sigma_0^2} ^2}}</script><blockquote><script type="math/tex; mode=display">\log p(\sigma^2) = - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script></blockquote><p>则</p><script type="math/tex; mode=display">\log L(\mu, \sigma^2 | D)= - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script><p>则</p><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2} \log L(\mu, \sigma^2 | D)= - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2\sigma_{\sigma_0}^2} \frac{\sigma - \sigma_0}{\sigma}\Rightarrow\hat{\sigma^2}(略)</script><blockquote><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2}(\sigma - \sigma_0)^2= 2(\sigma - \sigma_0)\frac{∂}{∂\sigma^2} (\sigma - \sigma_0)= \frac{\sigma - \sigma_0}{\sigma}</script></blockquote><h1 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h1><h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><!-- 贝叶斯公式：$$P(c_i|x) = \frac{p(x|c_i)p(c_i)}{\sum_j p(x|c_j)p(c_j)}$$在给定数据集$D=\{x_1, x_2, ..., x_N\}$的情况下，可从数据中估计先验概率和似然函数，即$$P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i, D)}{\sum_j p(x|c_j, D)p(c_j, D)}$$**假定**- 先验概率密度函数为$p(c_i)$已知- 抽样结果几乎与真实分布一致，即$ p(c_i, D) \approx p(c_i) $则$$P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i)}{\sum_j p(x|c_j, D)p(c_j)}$$只需从样本中，估计每个类别的似然函数$p(x|c_i, D)$即可>-----------------------------------------------现考虑**单个类别**中抽取的数据集$D$，如何估计该类别的似然函数$p(x | \theta)$参数$\theta$呢？若概率密度函数为$p(x | \theta)$，记从数据集中估计得到的似然函数为$p(x | D)$，有$$p(x | \theta) \approx p(x | D)$$> $p(x | D)$ would be the estimate of $p(x | \theta)$ given $D$且$$p(x | D) = \int p(x, \theta | D) d \theta= \int p(x | \theta, D) p(\theta | D) d \theta$$> Links $p(x | D)$ with $p(θ | D)$其中- $p(x | \theta, D) \approx p(x | \theta) $- $p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)}= \frac{p(\theta) \prod_{i=1}^N p(x_i | \theta)}{P(D)}$故$$p(x | D) = \int p(x | \theta) p(\theta | D) d \theta$$总的来说 --><script type="math/tex; mode=display">p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)}= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)</script><p>其中$a$是使</p><script type="math/tex; mode=display">\int p(\theta | D)  = 1</script><p>利用“质心公式”求解贝叶斯的点估计</p><script type="math/tex; mode=display">θ_{Bayes} = \int θ·p(θ|D) d θ</script><h2 id="例：正态分布的贝叶斯估计"><a href="#例：正态分布的贝叶斯估计" class="headerlink" title="例：正态分布的贝叶斯估计"></a>例：正态分布的贝叶斯估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的贝叶斯估计</p><script type="math/tex; mode=display">p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><h3 id="参数-mu-的估计"><a href="#参数-mu-的估计" class="headerlink" title="参数$\mu$的估计"></a>参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p><script type="math/tex; mode=display">p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><p>则</p><script type="math/tex; mode=display">P(\mu | D) = a · p(\mu) \prod_{i=1}^N p(x_i | \mu)= a · \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}\prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">= a · \left( \frac{1}{\sqrt{2\pi}} \right)^{N + 1}\frac{1}{\sigma_{\mu_0} \sigma^N}e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2} -\sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2}}</script><p>易证</p><blockquote><p></p><p align="right">我已经想到了一个绝妙的证明,但是这台电脑的硬盘太小了,写不下。</p><br><!-- > <p align="right">诶嘿，显然成立，2333333</p> --><p></p></blockquote><script type="math/tex; mode=display">p(\mu | D) = \frac{1}{\sqrt{2\pi}\sigma_N} e^ {-\frac{(\mu - \mu_N)^2}{2\sigma_N^2}}</script><p>其中</p><script type="math/tex; mode=display">\mu_N = \frac{N \sigma_0^2}{N \sigma_0^2 + \sigma^2}\frac{1}{N} \sum_{i=1}^N x_i+\frac{\sigma^2}{N \sigma_0^2 + \sigma^2}\mu_0</script><script type="math/tex; mode=display">\sigma_N^2 = \frac{\sigma_0^2 \sigma^2}{N \sigma_0^2 + \sigma^2}</script><blockquote><p><strong>与$MLE$，$MAP$的区别</strong></p><ul><li>相比较$MLE$与$MAP$的点估计，贝叶斯估计得到的结果是参数$\theta$的密度函数$p(\theta | D)$</li><li><p>最大后验概率估计为求取对应最大后验概率的点</p><script type="math/tex; mode=display">\theta = argmax_\theta p(\theta | D)</script></li><li><p>贝叶斯估计为求取整个取值范围的概率密度$p(\theta | D)$，既然如此，必有</p><script type="math/tex; mode=display">\int p(\theta | D) d\theta = 1</script></li></ul><p><a href="https://www.cnblogs.com/zjh225901/p/7495505.html" target="_blank" rel="noopener">统计学习方法学习笔记（一）—极大似然估计与贝叶斯估计原理及区别 - YJ-20 - 博客园</a></p><script type="math/tex; mode=display">p(\theta | D) = \frac{p(\theta) \prod_{i=1}^N p(x_i | \theta)}{\int_\theta p(\theta) \prod_{i=1}^N p(x_i | \theta) d\theta}</script><p>由于$\theta$是满足一定概率分布的变量，所以在计算的时候需要将考虑所有$\theta$取值的情况，在计算过程中不可避免地高复杂度。所以计算时并不把所有地后验概率$p(\theta | D)$都找出来，而是采用类似于极大似然估计地思想，来极大化后验概率，得到这种有效的叫做$MAP$</p></blockquote><h1 id="引例的求解"><a href="#引例的求解" class="headerlink" title="引例的求解"></a>引例的求解</h1><p>已知硬币投掷结果服从$Bernoulli$分布</p><table>  <tr>    <th>X</th>    <th>0</th>    <th>1</th>  </tr>  <tr>    <td>P</td>    <td>1-θ</td>    <td>θ</td>  </tr></table><p>或者</p><script type="math/tex; mode=display">P(X_i) = \theta ^{X_i} (1 - \theta) ^{1 - X_i}</script><h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><p>实验结果中正面出现$7$次，反面出现$3$次，似然函数为</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{10} \theta ^{X_i} (1 - \theta) ^{1 - X_i} = \theta ^7 (1 - \theta) ^3</script><p>取对数似然函数并求极大值</p><script type="math/tex; mode=display">\log L(\theta) = 7 \log \theta + 3 \log (1 - \theta)</script><p>令</p><script type="math/tex; mode=display">\frac{∂}{∂ \theta} \log L(\theta)= \frac{7}{\theta} - \frac{3}{1-\theta} = 0</script><p>解得</p><script type="math/tex; mode=display">\theta = 0.7</script><p>即硬币服从$B(1, 0.7)$的概率分布</p><blockquote><p>做出$L(\theta)$图像验证，如下<br><img src="/2018/11/19/Parameter-Estimation/最大似然估计.png" alt="最大似然估计"></p></blockquote><h2 id="最大后验概率估计"><a href="#最大后验概率估计" class="headerlink" title="最大后验概率估计"></a>最大后验概率估计</h2><p>给定先验条件</p><script type="math/tex; mode=display">\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><p>则最大化</p><script type="math/tex; mode=display">L(\theta | D) = \theta ^7 (1 - \theta) ^3 · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}}</script><p>取对数</p><script type="math/tex; mode=display">\log L(\theta | D)= 7 \log \theta + 3 \log (1 - \theta) - \frac{1}{2} \log(2\pi \sigma_{\theta_0}^2) - \frac{1}{2\sigma_{\theta_0}^2} (\theta - \theta_0)^2</script><p>求取极大值点</p><script type="math/tex; mode=display">\frac{∂}{∂\theta} \log L(\theta | D) = \frac {7}{\theta} - \frac{3}{1-\theta} - \frac{\theta - \theta_0}{\sigma_{\theta_0}^2} = 0</script><p>得到</p><script type="math/tex; mode=display">\theta^3 - (\theta_0 + 1) \theta^2 + (\theta_0 - 10\sigma_{\theta_0}^2) \theta + 7\sigma_{\theta_0}^2 = 0</script><p>以下为选取不同先验条件时的$L(\theta | D)$图像，用于对比</p><blockquote><ul><li>第一张图为极大似然估计$L(D|\theta)$</li><li>第二张图为先验概率密度函数$P(\theta)$</li><li>第三张图为最大后验概率估计$L(\theta | D)$，$\hat{\theta}$由查表法求解<br>代码见<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/tree/Hexo/source/_posts//参数估计的几种方法/temp.py" target="_blank" rel="noopener">仓库</a></li></ul></blockquote><ul><li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.42$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.56$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\Rightarrow$ $\hat{\theta} = 0.50$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p></li></ul><blockquote><p>结论</p><ul><li>由图$1, 2, 3$，可以看到当$\theta_0$偏移$0.7$时，$MAP$结果也相应偏移；</li><li>由图$2, 4, 5$，可以看到当$\sigma_{\theta_0}^2$越小，即越确定先验概率分布时，$MAP$结果也越趋向于先验概率分布。</li></ul></blockquote><h2 id="贝叶斯估计-1"><a href="#贝叶斯估计-1" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>先验条件为正态分布</p><script type="math/tex; mode=display">\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><script type="math/tex; mode=display">p(\theta | D)= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)= a · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}} · \theta ^7 (1 - \theta) ^3</script><blockquote><p>参数$a$使用<code>scipy.integrate.quad</code>求解</p></blockquote><p>选取不同先验条件时的$L(\theta | D)$图像，用于对比</p><ul><li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Clustering</title>
      <link href="/2018/11/16/Clustering/"/>
      <url>/2018/11/16/Clustering/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="距离度量方法"><a href="#距离度量方法" class="headerlink" title="距离度量方法"></a>距离度量方法</h2><p>机器学习中距离度量方法有很多，以下简单介绍几种。</p><blockquote><p><a href="https://blog.csdn.net/taotiezhengfeng/article/details/80492128" target="_blank" rel="noopener">机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客</a><br><a href="https://blog.csdn.net/u014782458/article/details/58180885" target="_blank" rel="noopener">算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦123的博客 - CSDN博客 </a></p></blockquote><p>定义两个$n$维向量</p><script type="math/tex; mode=display">x = [x_1, x_2, ..., x_n]^T</script><script type="math/tex; mode=display">y = [y_1, y_2, ..., y_n]^T</script><ul><li><p>曼哈顿距离<code>(Manhattan Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_1 = \sum_i |x_i - y_i|</script></li><li><p>欧氏距离<code>(Euclidean Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_2 = \sqrt{\sum_i (x_i - y_i)^2}</script></li><li><p>闽可夫斯基距离<code>(Minkowski Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_p = \left(\sum_i | x_i - y_i |^{p} \right)^{\frac{1}{p}}</script><p>  当$p$取$1$时为曼哈顿距离，取$2$时为欧式距离。</p></li><li><p>余弦距离<code>(Cosine)</code></p><script type="math/tex; mode=display">  d = \frac{x^T y}{||x||_2 ||y||_2} = \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}}</script><blockquote><p>突然想到为什么向量的夹角余弦是怎么来的，高中学习一直背的公式，现在给一下证明。<br>证明：向量的夹角公式<br><img src="/2018/11/16/Clustering/cosine_distance.png" alt="cosine_distance"></p><p>从余弦定理(余弦定理用几何即可)出发，有</p><script type="math/tex; mode=display">\cos \theta = \frac{a^2+b^2-c^2}{2ab}</script><p>其中</p><script type="math/tex; mode=display">||\vec{a}|| = \sqrt{x_1^2 + y_1^2}</script><script type="math/tex; mode=display">||\vec{b}|| = \sqrt{x_2^2 + y_2^2}</script><script type="math/tex; mode=display">||\vec{c}|| = \sqrt{(x_1 - x_2)^2 + (x_2 - y_2)^2}</script><p>故</p><script type="math/tex; mode=display">\cos \theta = \frac  {(\sqrt{x_1^2 + y_1^2})^2 + (\sqrt{x_2^2 + y_2^2})^2 - (\sqrt{(x_1 - x_2)^2 + (x_2 - y_2))^2}}  {2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}</script><script type="math/tex; mode=display">= \frac  {x_1 x_2 + y_1 y_2}  {\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}  = \frac{a^T b}{||a||·||b||}</script></blockquote></li></ul><h2 id="hard-vs-soft-clustering"><a href="#hard-vs-soft-clustering" class="headerlink" title="hard vs. soft clustering"></a>hard vs. soft clustering</h2><ul><li>硬聚类<code>(hard clustering)</code><br>  计算的是一个硬分配<code>(hard ssignment)</code>过程,即每个样本仅仅属于一个簇。</li><li>软聚类<code>(soft clustering)</code><br>  分配过程是软的，即一个样本的分配结果是在所有簇上的一个分布，在软分配结果中，一个样本可能对多个簇都具有隶属度。</li></ul><h2 id="聚类方法的分类"><a href="#聚类方法的分类" class="headerlink" title="聚类方法的分类"></a>聚类方法的分类</h2><ul><li>划分方法<br>  <code>K-means</code>，<code>K-medoids</code>，<code>GMM</code>等。</li><li>层次方法<br>  <code>AGNES</code>，<code>DIANA</code>，<code>BIRCH</code>，<code>CURE</code>和<code>CURE-NS</code>等。</li><li>基于密度的方法<br>  <code>DBSCAN</code>，<code>OPTICS</code>，<code>DENCLUE</code>等。</li><li>其他<br>  如<code>STING</code>等。</li></ul><h1 id="常用聚类方法"><a href="#常用聚类方法" class="headerlink" title="常用聚类方法"></a>常用聚类方法</h1><h2 id="K均值-K-means"><a href="#K均值-K-means" class="headerlink" title="K均值(K-means)"></a>K均值(K-means)</h2><p>是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一，通常用于寻找次优解，再通过其他算法(如<code>GMM</code>)寻找更优的聚类结果。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>给定$N$维数据集</p><script type="math/tex; mode=display">X = [x^{(1)}, x^{(2)}, ..., x^{(M)}]</script><p>指定类别数$K$与初始中心点$\mu^{(0)}$，将样本划分到中心点距离其最近的簇中，再根据本次划分更新各簇的中心$\mu^{(t)}$，如此迭代直至得到最好的聚类结果。预测测试样本时，将其划分到中心点距其最近的簇，也可通过<code>KNN</code>等方法。</p><p>一般使用欧式距离度量样本到各中心点的距离，也可选择余弦距离等，这也是<code>K-means</code>算法的关键</p><script type="math/tex; mode=display">D(x^{(i)}, \mu_k) = || x^{(i)} - \mu_k ||_2^2</script><p>定义损失函数为</p><script type="math/tex; mode=display">J(\Omega) = \sum_i \sum_k r^{(i)}_k D(x^{(i)}, \mu_k)</script><p>其中</p><script type="math/tex; mode=display">r^{(i)}_k = \begin{cases}    1 & x^{(i)} \in C_k \\    0 & otherwise\end{cases}</script><p>或表示为</p><script type="math/tex; mode=display">r^{(i)} = [0, ..., 1_k, ..., 0]^T</script><p>在迭代过程中，损失函数的值不断下降，优化目标为</p><script type="math/tex; mode=display">\min J(\Omega)</script><h3 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h3><ol><li>随机选取$K$个中心点；</li><li>遍历所有数据，计算每个点到各中心点的距离；</li><li>将每个数据划分到最近的中心点中；</li><li>计算每个聚类的平均值，作为新的中心点；</li><li>重复步骤2-步骤4，直到这k个中线点不再变化(收敛)，或执行了足够多的迭代；</li></ol><p><code>K-means</code>更新迭代过程如下图<br><img src="/2018/11/16/Clustering/kmeans_example.gif" alt="kmeans_example"></p><h3 id="缺点与部分解决方法"><a href="#缺点与部分解决方法" class="headerlink" title="缺点与部分解决方法"></a>缺点与部分解决方法</h3><ul><li>局部最优</li><li>初值敏感<br>  初始点的选择会影响<code>K-means</code>聚类的结果，即可能会陷入局部最优解，如下图<br>  <img src="/2018/11/16/Clustering/k_means_init.png" alt="k_means_init"><br>  可通过如下方法解决<ul><li>多次选择初始点运行<code>K-means</code>算法，选择最优的作为输出结果；</li><li><code>K-means++</code></li></ul></li><li>需要定义<code>mean</code>，对于标称型<code>(categorical)</code>数据不适用</li><li>需要给定聚类簇数目$K$<br>  这里给出一种选择簇数目的方法，选择多个$K$值进行聚类，计算代价函数，做成折线图后如下，可以看到在$K=3$处损失值的变化率出现较大变化，则可选择簇的数目为$3$。<br>  <img src="/2018/11/16/Clustering/k_means_choose_K.png" alt="k_means_choose_K"></li><li>噪声数据干扰大</li><li>对于非凸集<code>(non-convex)</code>数据无能为力<br>  谱聚类可解决非凸集数据的聚类问题。</li></ul><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><ul><li><code>K-means++</code><br>  改进初始点选择方法，第$1$个中心点随机选择；之后的初始中心点根据前面选择的中心点决定，若已选取$n$个初始聚类中心$(0&lt;n&lt;K)$，选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。</li><li><code>ISODATA</code><br>  思想：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别.</li><li><code>Kernel K-means</code><br>  参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类。</li></ul><h3 id="类似的算法"><a href="#类似的算法" class="headerlink" title="类似的算法"></a>类似的算法</h3><p>与<code>K-means</code>类似的算法有很多，例如</p><ul><li><code>K-medoids</code><br>  <code>K-means</code>的取值范围可以是连续空间中的任意值，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。<code>K-medoids</code>的取值是数据样本范围中的样本，且可应用在非数值型数据样本上。</li><li><code>k-medians</code><br>  $K$中值，选择中位数更新各簇的中心点。</li><li><code>K-centers</code><br>  <a href="https://www.bjdxs.com/xueshu/28151.html" target="_blank" rel="noopener">混合类型数据的K-Centers聚类算法/The K-Centers Clustering Algorithm for Categorical and Mixe</a></li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-1-kmeans/KMeans.py" target="_blank" rel="noopener">@Github: K-Means</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">class KMeans():</span><br><span class="line">    def __init__(self, n_cluster, mode):</span><br><span class="line">        self.n_cluster = n_cluster  # 簇的个数</span><br><span class="line">        self.mode = mode            # 距离度量方式</span><br><span class="line">        self.centroids = None       # 簇的中心</span><br><span class="line">        self.loss = float(&apos;inf&apos;)    # 优化目标值</span><br><span class="line">        plt.ion()</span><br><span class="line">    def fit(self, X, max_iter=5, min_move=0.1, display=False):</span><br><span class="line">        def initializeCentroids():</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            选择初始点</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroid = np.zeros(shape=(self.n_cluster, X.shape[1])) # 保存选出的点</span><br><span class="line">            pointIdx = []                                           # 保存已选出的点的索引</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                idx = np.random.randint(0, X.shape[0])              # 随机选择一个点</span><br><span class="line">                while idx in pointIdx:                              # 若该点已选出，则丢弃重新选择</span><br><span class="line">                    idx = np.random.randint(0, X.shape[0])</span><br><span class="line">                pointIdx.append(idx)</span><br><span class="line">                centroid[n] = X[idx]</span><br><span class="line">            return centroid</span><br><span class="line">        def dist2Centroids(x, centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            返回向量x到k个中心点的距离值</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            d = np.zeros(shape=(self.n_cluster,))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                d[n] = mathFunc.distance(x, centroids[n], mode)</span><br><span class="line">            return d</span><br><span class="line">        def nearestInfo(centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            每个点最近的簇中心索引、距离</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            ctIdx = -np.ones(shape=(X.shape[0],), dtype=np.int8)    # 每个点最近的簇中心索引，初始化为-1，可作为异常条件</span><br><span class="line">            ctDist = np.ones(shape=(X.shape[0],), dtype=np.float)   # 每个点到最近簇中心的距离</span><br><span class="line">            for i in range(X.shape[0]):</span><br><span class="line">                dists = dist2Centroids(X[i], centroids, mode)</span><br><span class="line">                if mode == &apos;Euclidean&apos;: ctIdx[i] = np.argmin(dists)</span><br><span class="line">                elif mode == &apos;Cosine&apos;:  ctIdx[i] = np.argmax(dists)</span><br><span class="line">                ctDist[i] = dists[ctIdx[i]]             # 保存最相似的距离度量，用于计算loss</span><br><span class="line">            return ctIdx, ctDist</span><br><span class="line">        def updateCentroids(ctIdx):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            更新簇中心</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroids = np.zeros(shape=(self.n_cluster, X.shape[1]))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                X_ = X[ctIdx == n]                      # 筛选出离簇中心Cn最近的样本点</span><br><span class="line">                centroids[n] = np.mean(X_, axis=0)      # 根据筛选出的样本点更新中心值</span><br><span class="line">            return centroids</span><br><span class="line">        def loss(dist):</span><br><span class="line">            return np.mean(dist**2)</span><br><span class="line">        # -----------------------------------------</span><br><span class="line">        loss_min = float(&apos;inf&apos;)                         # 最优分类时的损失值，最小</span><br><span class="line">        n_iter = 0     </span><br><span class="line">        while n_iter &lt; max_iter:                        # 每次迭代选择不同的初始点</span><br><span class="line">            n_iter += 1; isDone = False                 # 表示本次迭代是否已收敛</span><br><span class="line">            centroids_tmp = initializeCentroids()       # 选择本次迭代的初始点</span><br><span class="line">            loss_last = float(&apos;inf&apos;)                    # 本次迭代中，中心点更新前的损失值</span><br><span class="line">            n_update = 0                                # 本次迭代的更新次数计数</span><br><span class="line">            while not isDone:</span><br><span class="line">                n_update += 1</span><br><span class="line">                ctIdx, ctDist = nearestInfo(centroids_tmp, mode=self.mode)</span><br><span class="line">                centroids_tmp = updateCentroids(ctIdx)  # 更新簇中心</span><br><span class="line">                # --- 可视化 ---</span><br><span class="line">                if (display==True) and (X.shape[1] == 2):</span><br><span class="line">                    plt.ion()</span><br><span class="line">                    plt.figure(n_iter); plt.cla()</span><br><span class="line">                    plt.scatter(X[:, 0], X[:, 1], c=ctIdx)</span><br><span class="line">                    plt.scatter(centroids_tmp[:, 0], centroids_tmp[:, 1], c=&apos;r&apos;)</span><br><span class="line">                    plt.pause(0.5)</span><br><span class="line">                # -------------</span><br><span class="line">                loss_now = loss(ctDist); moved = np.abs(loss_last - loss_now)</span><br><span class="line">                if moved &lt; min_move:                    # 若移动过小，则本次迭代收敛</span><br><span class="line">                    isDone = True</span><br><span class="line">                    print(&apos;第%d次迭代结束，中心点更新%d次&apos; % (n_iter, n_update))</span><br><span class="line">                else: loss_last = loss_now</span><br><span class="line">            if loss_now &lt; loss_min:</span><br><span class="line">                self.centroids = centroids_tmp          # 保存损失最小的模型(最优)</span><br><span class="line">                loss_min = loss_now</span><br><span class="line">                # print(&apos;聚类结果已更新&apos;)</span><br><span class="line">        self.loss = loss_min</span><br><span class="line">        print(&apos;=========== 迭代结束 ===========&apos;)</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        各个样本的最近簇中心索引</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        labels = -np.ones(shape=(X.shape[0],), dtype=np.int)    # 初始化为-1，可用作异常条件</span><br><span class="line">        for i in range(X.shape[0]):</span><br><span class="line">            dists_i = np.zeros(shape=(self.n_cluster,))         # 保存X[i]到中心点Cn的距离</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                dists_i[n] = mathFunc.distance(X[i], self.centroids[n], mode=self.mode)</span><br><span class="line">            if self.mode == &apos;Euclidean&apos;:</span><br><span class="line">                labels[i] = np.argmin(dists_i)</span><br><span class="line">            elif self.mode == &apos;Cosine&apos;:</span><br><span class="line">                labels[i] = np.argmax(dists_i)</span><br><span class="line">        return labels</span><br></pre></td></tr></table></figure></p><p>簇数的选择代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def chooseBestK(X, start, stop, step=1, mode=&apos;Euclidean&apos;):</span><br><span class="line">    Ks = np.arange(start, stop + 1, step, dtype=np.int) # 待选择的K</span><br><span class="line">    Losses = np.zeros(shape=Ks.shape)                   # 保存不同K值时的最小损失值</span><br><span class="line">    for k in range(1, Ks.shape[0] + 1):                 # 对于不同的K，训练模型，计算损失</span><br><span class="line">        print(&apos;K = %d&apos;, k)</span><br><span class="line">        estimator = KMeans(n_cluster=k, mode=mode)</span><br><span class="line">        estimator.fit(X, max_iter=10, min_move=0.01, display=False)</span><br><span class="line">        Losses[k - 1] = estimator.loss</span><br><span class="line">    plt.ioff()</span><br><span class="line">    plt.figure(); plt.xlabel(&apos;n_clusters&apos;); plt.ylabel(&apos;loss&apos;)</span><br><span class="line">    plt.plot(Ks, Losses)                                # 做出loss-K曲线</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="均值漂移-Meanshift"><a href="#均值漂移-Meanshift" class="headerlink" title="均值漂移(Meanshift)"></a>均值漂移(Meanshift)</h2><p>本质是一个迭代的过程，能够在一组数据的密度分布中寻找到局部极值，比较稳定，而且是无参密度估计(不需要事先知道样本数据的概率密度分布函数，完全依靠对样本点的计算)，而且在采样充分的情况下，一定会收敛，即可以对服从任意分布的数据进行密度估计。</p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>有一个滑动窗口的思想，即利用当前中心点一定范围内(通常为球域)的点迭代更新中心点，重复移动窗口，直到满足收敛条件。简单的说，<code>Meanshift</code>就是沿着密度上升的方向寻找同属一个簇的数据点。</p><p>定义点$x_0$的$\epsilon$球域如下</p><script type="math/tex; mode=display">S_h(x_0) = \{ x | (x - x_0)^T (x - x_0) \leq \epsilon \}</script><p>若有$n$个点$(x_1, …, x_n)$落在中心点$ptCentroid$的邻域内，其分布如图<br><img src="/2018/11/16/Clustering/DBSCAN4.jpg" alt="DBSCAN4"></p><p>则偏移向量计算方式为</p><script type="math/tex; mode=display">vecShift = \frac{1}{n} \sum_{i=1}^n (x_i - ptCentroid)</script><p>中心点更新公式为</p><script type="math/tex; mode=display">ptCentroid := ptCentroid + vecShift</script><blockquote><p>展开后可发现，其更新公式即</p><script type="math/tex; mode=display">vecShift= \frac{1}{n} \sum_{i=1}^n x_i - ptCentroid</script><script type="math/tex; mode=display">ptCentroid :=  \frac{1}{n} \sum_{i=1}^n x_i</script></blockquote><p><img src="/2018/11/16/Clustering/DBSCAN3.jpg" alt="DBSCAN3"></p><p>一个滑动窗口的动态更新过程如下图<br><img src="/2018/11/16/Clustering/meanshift_example1.gif" alt="meanshift_example1"><br>初始化多个滑动窗口进行<code>MeanShift</code>算法，其更新过程如下，其中每个黑点代表滑动窗口的质心，每个灰点代表一个数据点<br><img src="/2018/11/16/Clustering/meanshift_example2.gif" alt="meanshift_example2"></p><h3 id="高斯权重"><a href="#高斯权重" class="headerlink" title="高斯权重"></a>高斯权重</h3><p>基本思想是，距离当前中心点近的向量对更新结果权重大，而远的权重小，可减小远点的干扰，如下图，$vecShift_2$为高斯权重下的偏移向量<br><img src="/2018/11/16/Clustering/DBSCAN5.jpg" alt="DBSCAN5"></p><p>其偏移向量计算方式为</p><script type="math/tex; mode=display">vecShift = \frac{1}{n} \sum_{i=1}^n w_i · (x_i - ptCentroid)</script><script type="math/tex; mode=display">w_i = \frac{\kappa(x_i - ptCentroid)}{\sum_j \kappa(x_j - ptCentroid)}</script><p>其中</p><script type="math/tex; mode=display">\kappa(z) = \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{||z||^2}{2\sigma^2} \right)</script><p>中心点更新公式仍然为</p><script type="math/tex; mode=display">ptCentroid := ptCentroid + vecShift</script><blockquote><p>展开也可得到</p><script type="math/tex; mode=display">ptCentroid := \frac{\sum_{i=1}^n w_i x_i}{\sum_j w_j}</script></blockquote><h3 id="计算步骤-1"><a href="#计算步骤-1" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$\epsilon_0$，终止条件参数$\epsilon_1$，簇合并参数$\epsilon_2$，并指定样本距离度量方式，目标为将其划分为$K$个簇。</p><ol><li>初始化：<ul><li>在样本集中随机选择$K_0(K_0 \gg K)$个样本作为初始中心点，以邻域大小为$\epsilon_0$建立滑动窗口；</li><li>各个样本初始化一个标记向量，用于记录被各类别访问的次数；</li></ul></li><li>以单个滑动窗口分析，记其中心点为$ptCentroid$，找到滑动窗口内的所有点，记作集合$M$，认为这些点属于该滑动窗口所属的簇类别，同时，这些点被该簇访问的次数$+1$；</li><li>以$ptCentroid$为中心，计算其到集合$M$中各个元素的向量，以这些向量计算得到偏移向量$vecShift$；</li><li>更新中心点：$ptCentroid = ptCentroid + vecShift$，即滑动窗口沿着$vecShift$方向移动，距离为$||vecShift||$；</li><li>重复步骤$2-4$，直到$||vecShift||&lt;\epsilon_1$，保存当前中心点；</li><li>如果收敛时当前簇$ptCentroid$与其它已经存在的簇的中心的距离小于阈值$\epsilon_2$，那么这两个簇合并。否则，把当前簇作为新的簇类，增加$1$类；</li><li>重复迭代直到所有的点都被标记访问；</li><li>根据每个样本被各簇的访问频率，取访问频率最大的那个簇类别作为当前点集的所属类。</li></ol><blockquote><p>即不同类型的滑窗沿着密度上升的方向进行移动，对各样本点进行标记，最后将样本划分为标记最多的类别；当两类非常接近时，合并为一类。</p></blockquote><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p71_meanshift.py" target="_blank" rel="noopener">@Github: MeanShift</a></p><p>先定义了窗格对象<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">class SlidingWindow():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Attributes:</span><br><span class="line">        centroid: &#123;ndarray(n_features,)&#125;</span><br><span class="line">        epsilon: &#123;float&#125; 滑动窗格大小，为半径的平方</span><br><span class="line">        sigma: &#123;float&#125; 高斯核函数的参数</span><br><span class="line">        label: &#123;int&#125; 该窗格的标记</span><br><span class="line">        X: &#123;ndarray(n_samples, n_features)&#125;</span><br><span class="line">        containIdx: &#123;ndarray(n_contain,)&#125; 窗格内包含点的索引</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, centroid, epsilon, sigma, label, X):</span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.label = label</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">    def k(self, z):</span><br><span class="line">        &quot;&quot;&quot; 高斯核函数</span><br><span class="line">        Args:</span><br><span class="line">            z: &#123;ndarray(n_features,)&#125;</span><br><span class="line">        Notes:</span><br><span class="line">            - \kappa(z) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \exp \left( - \frac&#123;||z||^2&#125;&#123;2\sigma^2&#125; \right)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        norm = np.linalg.norm(z)</span><br><span class="line">        return np.exp(- 0.5 * (norm / self.sigma)**2) / np.sqrt(2*np.pi)</span><br><span class="line">    def step(self, X):</span><br><span class="line">        &quot;&quot;&quot; 更新滑动窗格的中心点和所包含点</span><br><span class="line">        Returns: &#123;float&#125;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        dshift = self.shift(X)</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">        return dshift</span><br><span class="line">    def shift(self, X):</span><br><span class="line">        &quot;&quot;&quot; 移动窗格</span><br><span class="line">        Args:</span><br><span class="line">            vecShift: &#123;ndarray(n_features,)&#125;</span><br><span class="line">        Returns:</span><br><span class="line">            dshift: &#123;float&#125; 移动的距离</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        n_contain = self.containIdx.shape[0]</span><br><span class="line">        contain_weighted_sum = np.zeros(shape=(n_features, ))</span><br><span class="line">        weight_sum = 0</span><br><span class="line">        # 按包含的点进行移动</span><br><span class="line">        for i_contain in range(n_contain):</span><br><span class="line">            vector = X[self.containIdx[i_contain]] - self.centroid</span><br><span class="line">            weight = self.k(vector)</span><br><span class="line">            contain_weighted_sum += weight*X[self.containIdx[i_contain]]</span><br><span class="line">            weight_sum += weight</span><br><span class="line">        centroid = contain_weighted_sum / weight_sum </span><br><span class="line">        # 计算移动的距离   </span><br><span class="line">        dshift = np.linalg.norm(self.centroid - centroid)</span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        return dshift</span><br><span class="line">    def updateContain(self, X):</span><br><span class="line">        &quot;&quot;&quot; 更新窗格内的点索引</span><br><span class="line">        Args:</span><br><span class="line">            X: &#123;ndarray(n_samples, n_features)&#125;</span><br><span class="line">        Notes:</span><br><span class="line">            - 用欧式距离作为度量</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        d = lambda x_i, x_j: np.linalg.norm(x_i - x_j)</span><br><span class="line">        n_samples = X.shape[0]</span><br><span class="line">        containIdx = np.array([], dtype=&apos;int&apos;)</span><br><span class="line">        for i_samples in range(n_samples):</span><br><span class="line">            if d(X[i_samples], self.centroid) &lt; self.epsilon:</span><br><span class="line">                containIdx = np.r_[containIdx, i_samples]</span><br><span class="line">        return containIdx</span><br></pre></td></tr></table></figure></p><p>聚类算法如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">class MeanShift():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Attributes:</span><br><span class="line">        n_clusters: &#123;int&#125; 划分簇的个数</span><br><span class="line">        n_windows: &#123;int&#125; 滑动窗格的个数</span><br><span class="line">        epsilon: &#123;float&#125; 滑动窗格的大小</span><br><span class="line">        sigma: &#123;float&#125; &#123;float&#125; 高斯核参数</span><br><span class="line">        thresh: &#123;float&#125; 若两个窗格中心距离小于thresh，则合并两类簇</span><br><span class="line">        min_move: &#123;float&#125; 终止条件</span><br><span class="line">        windows: &#123;list[class SlidingWindow()]&#125;</span><br><span class="line">    Note:</span><br><span class="line">        - 假设所有点均被窗格划过</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    def __init__(self, n_clusters, n_windows=-1, epsilon=0.5, sigma=2, thresh=1e-2, min_move=1e-3):</span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.n_windows = 5*n_clusters if (n_windows == -1) else n_windows</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.thresh = thresh</span><br><span class="line">        self.min_move = min_move</span><br><span class="line">        self.windows = []</span><br><span class="line">        self.centroids = None</span><br><span class="line">    def fit(self, X):</span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        # 创建窗格</span><br><span class="line">        for i_windows in range(self.n_windows):</span><br><span class="line">            idx = np.random.randint(n_samples)</span><br><span class="line">            window = SlidingWindow(X[idx], self.epsilon,</span><br><span class="line">                            self.sigma, i_windows, X)</span><br><span class="line">            # 将各窗格包含的点标记</span><br><span class="line">            n_contain = window.containIdx.shape[0]</span><br><span class="line">            self.windows.append(window)</span><br><span class="line"></span><br><span class="line">        dshift = float(&apos;inf&apos;)   # 初始化为无穷大</span><br><span class="line">        plt.figure(); plt.ion()</span><br><span class="line">        while dshift &gt; self.min_move:</span><br><span class="line">            # ------ 做图显示 ------</span><br><span class="line">            plt.cla()</span><br><span class="line">            plt.scatter(X[:, 0], X[:, 1], c=&apos;b&apos;)</span><br><span class="line">            for i_windows in range(self.n_windows):</span><br><span class="line">                centroid = self.windows[i_windows].centroid</span><br><span class="line">                plt.scatter(centroid[0], centroid[1], c=&apos;r&apos;)</span><br><span class="line">            plt.pause(0.5)</span><br><span class="line">            # ---------------------</span><br><span class="line">            dshift = self.step(X)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        </span><br><span class="line">        # 合并窗格</span><br><span class="line">        dists = np.zeros(shape=(self.n_windows, self.n_windows))</span><br><span class="line">        for i_windows in range(self.n_windows):</span><br><span class="line">            for j_windows in range(i_windows):</span><br><span class="line">                centroid_i = self.windows[i_windows].centroid</span><br><span class="line">                centroid_j = self.windows[j_windows].centroid</span><br><span class="line">                dists[i_windows, j_windows] = np.linalg.norm(centroid_i-centroid_j)</span><br><span class="line">                dists[j_windows, i_windows] = dists[i_windows, j_windows]</span><br><span class="line">        </span><br><span class="line">        # 获得距离相近索引</span><br><span class="line">        index = np.where(dists&lt;self.thresh)</span><br><span class="line">        # 用于标记类别</span><br><span class="line">        winlabel = np.zeros(shape=(self.n_windows,), dtype=&apos;int&apos;)</span><br><span class="line">        label = 1; winlabel[0] = label</span><br><span class="line">        for i_windows in range(self.n_windows):</span><br><span class="line">            idx_row = index[0][i_windows]</span><br><span class="line">            idx_col = index[1][i_windows]</span><br><span class="line">            # 若其中一个点被标记，则将令一个点并入该类</span><br><span class="line">            if winlabel[idx_row]!=0:</span><br><span class="line">                winlabel[idx_col] = winlabel[idx_row]</span><br><span class="line">            elif winlabel[idx_col]!=0:</span><br><span class="line">                winlabel[idx_row] = winlabel[idx_col]</span><br><span class="line">            # 否则新创建类别</span><br><span class="line">            else:</span><br><span class="line">                label += 1</span><br><span class="line">                winlabel[idx_row] = label</span><br><span class="line">                winlabel[idx_col] = label</span><br><span class="line">        </span><br><span class="line">        # 将标签一样的窗格合并</span><br><span class="line">        labels = list(set(winlabel))                            # 去重后的标签</span><br><span class="line">        n_labels = len(labels)                                  # 标签种类数</span><br><span class="line">        self.centroids = np.zeros(shape=(n_labels, n_features)) # 记录最终聚类中心</span><br><span class="line">        for i_labels in range(n_labels):</span><br><span class="line">            cnt = 0</span><br><span class="line">            for i_windows in range(self.n_windows):</span><br><span class="line">                if winlabel[i_windows] == labels[i_labels]:</span><br><span class="line">                    self.centroids[i_labels] += self.windows[i_windows].centroid</span><br><span class="line">                    cnt += 1</span><br><span class="line">            self.centroids[i_labels] /= cnt                    # 取同类窗格中心点的均值</span><br><span class="line">        return self.centroids</span><br><span class="line"></span><br><span class="line">    def step(self, X):</span><br><span class="line">        &quot;&quot;&quot; update all sliding windows</span><br><span class="line">        Returns:</span><br><span class="line">            dshift: \sum_i^&#123;n_windows&#125; dshift_&#123;i&#125;</span><br><span class="line">        &quot;&quot;&quot; </span><br><span class="line">        dshift = 0</span><br><span class="line">        for i_windows in range(self.n_windows):</span><br><span class="line">            dshift += self.windows[i_windows].step(X)</span><br><span class="line">            # label the points</span><br><span class="line">            n_contain = self.windows[i_windows].containIdx.shape[0]</span><br><span class="line">        return dshift</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot; 简单的用近邻的方法求</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        dists = np.zeros(shape=(n_samples, self.n_clusters))</span><br><span class="line">        for i_samples in range(n_samples):</span><br><span class="line">            for i_clusters in range(self.n_clusters):</span><br><span class="line">                dists[i_samples, i_clusters] = np.linalg.norm(X[i_samples]-self.centroids[i_clusters])</span><br><span class="line">        return np.argmin(dists, axis=1)</span><br></pre></td></tr></table></figure></p><h2 id="谱聚类-Spectral-Clustering"><a href="#谱聚类-Spectral-Clustering" class="headerlink" title="谱聚类(Spectral Clustering)"></a>谱聚类(Spectral Clustering)</h2><p>谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用，比起传统的<code>K-Means</code>算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多。</p><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><blockquote><p><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">谱聚类（spectral clustering）原理总结 - 刘建平Pinard - 博客园 </a></p></blockquote><h4 id="无向权重图"><a href="#无向权重图" class="headerlink" title="无向权重图"></a>无向权重图</h4><p>我们用点的集合$V$和边的集合$E$描述一个图，即$G(V, E)$，其中$V$即数据集中的点</p><script type="math/tex; mode=display">V = [v_1, v_2, ..., v_n]</script><p>而点$v_i, v_j$间连接权值$w_{ij}$组成邻接矩阵$W$，由于为无向图，故满足$w_{ij}=w_{ji}$</p><script type="math/tex; mode=display">W = \left[    \begin{matrix}        w_{11} & ... & w_{1n} \\        ... & ... & ... \\        w_{n1} & ... & w_{nn} \\    \end{matrix}    \right]</script><p>对于图中的任意一个点$v_i$，定义其度$d_i$为</p><script type="math/tex; mode=display">d_i = \sum_{j=1}^n w_{ij}</script><p>则我们可以得到一个度矩阵$D=diag(d_1, …, d_n)$</p><script type="math/tex; mode=display">D = \left[        \begin{matrix}            d_1 & & \\            & ... & \\             & & d_n\\        \end{matrix}    \right]</script><p>除此之外，对于$V$中子集$V_{sub} \subset V$，定义子集$V_{sub}$点的个数为</p><script type="math/tex; mode=display">|V_{sub}| := n_{sub}</script><p>另外，定义该子集中点的度之和为</p><script type="math/tex; mode=display">vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script><h4 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h4><p>上面讲到的邻接矩阵$W$可以指定权值，但对于数据量庞大的数据集，这显然不是一个$wise$的选择。我们可以用相似矩阵$S$来获得邻接矩阵$W$，基本思想是，距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高。</p><p>构建邻接矩阵$W$的方法有三类：$\epsilon$-邻近法，$K$邻近法和全连接法，定义距离</p><script type="math/tex; mode=display">d_{ij} = ||x^{(i)} - x^{(j)}||_2^2</script><ul><li><p>$\epsilon$-邻近法<br>  设置距离阈值$\epsilon$，用欧式距离度量两点的距离$d_{ij}$，然后通过下式确定邻接权值$w_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      0 & d_{ij} > \epsilon \\      \epsilon & otherwise  \end{cases}</script><blockquote><p>两点间的权重要不就是$\epsilon$，要不就是0，距离远近度量很不精确，因此在实际应用中，我们很少使用$\epsilon$-邻近法。</p></blockquote></li><li><p>$K$邻近法</p><ul><li><p>第一种<br>  只要一个点在另一个点的$K$近邻中，则保留$d_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　or　x^{(j)} \in KNN(x^{(i)}) \\      0 & otherwise  \end{cases}</script></li><li><p>第二种<br>  互为$K$近邻时保留$d_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　and　x^{(j)} \in KNN(x^{(i)}) \\      0 & otherwise  \end{cases}</script></li></ul></li><li><p>全连接法<br>  可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和<code>Sigmoid</code>核函数。最常用的是高斯核函数<code>RBF</code>，此时相似矩阵和邻接矩阵相同</p><script type="math/tex; mode=display">  w_{ij} = \exp \left( -\frac{d_{ij}}{2\sigma^2} \right)</script></li></ul><h4 id="拉普拉斯矩阵-Graph-Laplacians"><a href="#拉普拉斯矩阵-Graph-Laplacians" class="headerlink" title="拉普拉斯矩阵(Graph Laplacians)"></a>拉普拉斯矩阵(Graph Laplacians)</h4><p>定义</p><script type="math/tex; mode=display">L = D - W</script><p>正则化的拉普拉斯矩阵为</p><script type="math/tex; mode=display">L = D^{-1} (D - W)</script><p>具有的性质如下</p><ol><li>$L^T = L$</li><li>其特征值均为实数，即$\lambda_i \in \mathbb{R}$</li><li>正定性：$\lambda_i \geq 0$</li><li><p>对于任意向量$x$，都有</p><script type="math/tex; mode=display">x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script><blockquote><p>证明：</p><script type="math/tex; mode=display">x^T L x = x^T D x - x^T W x = \sum_i d_i > x_i^2 - \sum_{ij} w_{ij} x_i x_j</script><script type="math/tex; mode=display">= \frac{1}{2} \left[ \sum_i d_i x_i^2 - > 2\sum_{ij} w_{ij} x_i x_j + \sum_j d_j x_j^2 \right]</script><p>其中$ d_i = \sum_j w_{ij} $，所以</p><script type="math/tex; mode=display">x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script></blockquote></li></ol><h4 id="无向图的切图"><a href="#无向图的切图" class="headerlink" title="无向图的切图"></a>无向图的切图</h4><h5 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h5><p>我们希望把一张无向图$G(V, E)$按一定方法切成多个子图，各个子图间无连接，每个子图的点集为$V_1, …, V_K$，满足</p><ul><li>$\bigcup_{k=1}^K V_k = V$</li><li>$V_i \cap V_j = \emptyset$</li></ul><p>定义两个子图点集合$A, B$之间的切图权重为</p><script type="math/tex; mode=display">W(A, B) = \sum_{i \in A, j \in B} w_{ij}</script><blockquote><p>共有$n_A × n_B$个权值作累加</p></blockquote><p>那么对于$K$个子图点的集合$V_1, …, V_K$，定义切图为</p><script type="math/tex; mode=display">cut(V_1, ..., V_K) = \frac{1}{2} \sum_{i=1}^K cut(V_i, \overline{V_i})</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><p>其中$\overline{V_i}$表示$V_i$的补集，或者</p><script type="math/tex; mode=display">\overline{V_i} = \bigcup_{k \neq i} V_k</script><p>通过最小化$cut(V_1, …, V_K)$使子图内权重和大，而子图间权重和小。但是这种方法存在问题，如下图<br><img src="/2018/11/16/Clustering/cut.jpg" alt="cut"></p><p>选择一个权重最小的边缘的<strong>点</strong>，比如$C$和$H$之间进行$cut$，这样可以最小化$cut(V_1, …, V_K)$，但是却不是最优的切图。</p><p>为解决上述问题，需要对每个子图的规模做出限定，以下介绍两种切图方式。</p><h5 id="Ratio-Cut"><a href="#Ratio-Cut" class="headerlink" title="Ratio Cut"></a>Ratio Cut</h5><p>不仅考虑最小化$cut(V_1, …, V_K)$，而且最大化每个子图的点个数，即</p><script type="math/tex; mode=display">RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{|V_k|}</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote><ul><li>$W(V_k, \overline{V_k}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}$</li><li>$|V_k| = n_k$</li></ul></blockquote><p>如果按照遍历的方法求解，由前面分析，$W(V_k, \overline{V_k})$需计算$n_{V_k} × n_{\overline{V_k}}$次累加，计算量庞大，那么如何求解呢？</p><p>定义指示向量$h_k$，其构成矩阵$H$</p><script type="math/tex; mode=display">H = [ h_1, ..., h_k, ..., h_K]</script><p>其中</p><script type="math/tex; mode=display">h_k = \left[h_{k1}, h_{k2}, , ..., h_{kM} \right]^T</script><script type="math/tex; mode=display">h_{ki} = \begin{cases}    \frac{1}{\sqrt{|V_k|}} & x^{(i)}\in V_k \\    0 & otherwise\end{cases}</script><blockquote><p>$h_k$为单位向量，且两两正交</p><script type="math/tex; mode=display">h_i^T h_j =       \begin{cases}          \sum_{|V_i|} \frac{1}{|V_i|} = |V_i| · \frac{1}{|V_i|} = 1 & i = j \\          0 & i \neq j      \end{cases}</script></blockquote><p>那么由拉式矩阵性质$4$</p><script type="math/tex; mode=display">h_k^T L h_k = \frac{1}{2} \sum_{i,j} w_{ij} (h_{ki} - h_{kj})^2</script><script type="math/tex; mode=display">= \frac{1}{2}     [        \sum_{i \in V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 +         \sum_{i \notin V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 +</script><script type="math/tex; mode=display">        \sum_{i \in V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 +         \sum_{i \notin V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2    ]</script><script type="math/tex; mode=display">= \frac{1}{2}    [        \sum_{i \in V_k, j \in V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - \frac{1}{\sqrt{|V_k|}})^2 +         \sum_{i \notin V_k, j \in V_k} w_{ij} (0 - \frac{1}{\sqrt{|V_k|}})^2 +</script><script type="math/tex; mode=display">        \sum_{i \in V_k, j \notin V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - 0)^2 +        \sum_{i \notin V_k, j \notin V_k} w_{ij} (0 - 0)^2    ]</script><script type="math/tex; mode=display">= \frac{1}{2}    [        \sum_{i \notin V_k, j \in V_k} w_{ij} \frac{1}{|V_k|} +        \sum_{i \in V_k, j \notin V_k} w_{ij} \frac{1}{|V_k|}    ]</script><blockquote><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}</script></blockquote><script type="math/tex; mode=display">h_k^T L h_k = \frac{1}{2} [\frac{1}{|V_k|} cut(V_k, \overline{V_k}) + \frac{1}{|V_k|} cut(V_k, \overline{V_k})]</script><script type="math/tex; mode=display">= \frac{1}{|V_k|} cut(V_k, \overline{V_k})</script><p>推到这里就能理解为什么要定义$h_k$了</p><script type="math/tex; mode=display">RatioCut(V_1, ..., V_K)= \frac{1}{2} \sum_k h_k^T L h_k</script><p>并且</p><script type="math/tex; mode=display">h_k^T L h_k = tr(H^T L H)</script><blockquote><script type="math/tex; mode=display">H^T L H = \left[      \begin{matrix}          — & h_1^T & — \\           & ... &  \\          — & h_K^T & — \\      \end{matrix}\right]L\left[          \begin{matrix}            | & & | \\            h_1 & ... & h_K \\            | & & |        \end{matrix}    \right]</script><script type="math/tex; mode=display">= \left[      \begin{matrix}          h_1^T L h_1 & ... & h_1^T L h_K \\          ... & ... & ... \\          h_K^T L h_K & ... & h_K^T L h_K \\      \end{matrix}\right]</script></blockquote><p>所以最终优化目标为</p><script type="math/tex; mode=display">\min_H tr(H^T L H)</script><script type="math/tex; mode=display">s.t.　H^T H = I</script><blockquote><script type="math/tex; mode=display">H^T H = \left[      \begin{matrix}          h_1^T h_1 & ... & h_1^T h_K \\          ... & ... & ... \\          h_K^T h_K & ... & h_K^T h_K \\      \end{matrix}\right]</script></blockquote><p>而矩阵的正交相似变换$A = P \Lambda P^{-1}$满足</p><script type="math/tex; mode=display">tr(A) = tr(\Lambda) = \sum_i \lambda_i</script><p>故</p><script type="math/tex; mode=display">tr(H^T L H) = tr(L) = \sum_{i=1}^M \lambda_i</script><p>$\lambda_i$为矩阵$L$的特征值。</p><p>我们再进行维度规约，将维度从$M$降到$k_1$，即找到$k_1$个最小的特征值之和。</p><h5 id="N-Cut"><a href="#N-Cut" class="headerlink" title="N Cut"></a>N Cut</h5><p>推导过程与<code>RatioCut</code>完全一致，只是将分母$|V_i|$换成$vol(V_i)$</p><script type="math/tex; mode=display">NCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{vol(V_i)}</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote><script type="math/tex; mode=display">vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script></blockquote><h3 id="计算步骤-2"><a href="#计算步骤-2" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，将其划分为$K$类$(C_1, …, C_K)$</p><ol><li>根据输入的相似矩阵的生成方式构建样本的相似矩阵$S_{M×M}$；</li><li>根据相似矩阵$S$构建邻接矩阵$W_{M×M}$；</li><li>构建度矩阵$D_{M×M}$；</li><li>计算拉普拉斯矩阵$L_{M×M}$，可进行规范化$ L := D^{-1}L $；</li><li>对$L$进行特征值分解<code>(EVD)</code>，得到特征对$ (\lambda_i, \alpha_i), i=1,…,M $；</li><li>指定超参数$K_1$，选取$K_1$个最小特征值对应的特征向量组成矩阵$F_{M×K_1}$，并将其按行标准化；</li><li>以$F$的行向量作为新的样本数($k_1$维，这里也有降维操作)进行聚类，划分为$K$类，可使用<code>K-means</code>；</li><li>聚类结果即为输出结果</li></ol><p>注意是$K_1$个最小特征值对应的特征向量，别问我为什么知道。。。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p86_spectral_clustering.py" target="_blank" rel="noopener">@Github: Spectral Clustering</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">class SpectralClustering():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Attributes:</span><br><span class="line">        k: &#123;int&#125;, k &lt; n_samples</span><br><span class="line">        sigma: &#123;float&#125;</span><br><span class="line">    Notes:</span><br><span class="line">        Steps:</span><br><span class="line">            - similarity matrix [W_&#123;n×n&#125;]</span><br><span class="line">            - diagonal matrix [D_&#123;n×n&#125;] is defined as</span><br><span class="line">                    D_&#123;ii&#125; = \begin&#123;cases&#125;</span><br><span class="line">                                \sum_j W_&#123;ij&#125; &amp; i \neq j \\</span><br><span class="line">                                0 &amp; i = j</span><br><span class="line">                            \end&#123;cases&#125;</span><br><span class="line">            - Laplacian matrix [L_&#123;n×n&#125;], Laplacian matrix is defined as</span><br><span class="line">                    L = D - W or L = D^&#123;-1&#125; (D - W)</span><br><span class="line">            - EVD: L \alpha_i = \lambda_i \alpha_i</span><br><span class="line">            - takes the eigenvector corresponding to the largest eigenvalue as</span><br><span class="line">                    B_&#123;n×k&#125; = [\beta_1, \beta_2, ..., \beta_k]</span><br><span class="line">            - apply K-Means to the row vectors of matrix B</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, k, n_clusters=2, sigma=1.0):</span><br><span class="line">        self.kmeans = KMeans(n_clusters=n_clusters)</span><br><span class="line">        self.k = k</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        n_samples = X.shape[0]</span><br><span class="line">        # step 1</span><br><span class="line">        kernelGaussian = lambda z, sigma: np.exp(-0.5 * np.square(z/sigma))</span><br><span class="line">        W = np.zeros((n_samples, n_samples))</span><br><span class="line">        for i in range(n_samples):</span><br><span class="line">            for j in range(i):</span><br><span class="line">                W[i, j] = kernelGaussian(np.linalg.norm(X[i] - X[j]), self.sigma)</span><br><span class="line">                W[j, i] = W[i, j]</span><br><span class="line">        # step 2</span><br><span class="line">        D = np.diag(np.sum(W, axis=1))</span><br><span class="line">        # step 3</span><br><span class="line">        L = D - W</span><br><span class="line">        L = np.linalg.inv(D).dot(L)</span><br><span class="line">        # step 4</span><br><span class="line">        eigval, eigvec = np.linalg.eig(L)</span><br><span class="line">        # step 5</span><br><span class="line">        order = np.argsort(eigval)</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line">        beta = eigvec[:, :self.k]</span><br><span class="line">        # step 6</span><br><span class="line">        self.kmeans.fit(beta)</span><br><span class="line">        return self.kmeans.labels_</span><br></pre></td></tr></table></figure></p><p><img src="/2018/11/16/Clustering/spectral_clustering.png" alt="spectral_clustering"></p><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><p><code>DBSCAN(Density-Based Spatial Clustering of Applications with Noise)</code>，具有噪声的基于密度的聚类方法。假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的。</p><blockquote><p><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">DBSCAN密度聚类算法 - 刘建平Pinard - 博客园</a></p></blockquote><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>先介绍几个关于密度的概念</p><ul><li><p>$\epsilon$-邻域<br>  对于样本$x^{(i)}$，其$\epsilon$-邻域包含样本集中与$x^{(i)}$距离不大于$\epsilon$的子样本集，其样本个数记作$|N_{\epsilon}(x^{(i)})|$。</p><script type="math/tex; mode=display">  N_{\epsilon}(x^{(i)}) = \{ x^{(j)} | d_{ij} \leq \epsilon \}</script></li><li><p>核心对象<br>  对于任一样本$x^{(i)}$，若其$\epsilon$-邻域$N_{\epsilon}(x^{(i)})$至少包含$minPts$个样本，则该样本为核心对象。如图，选择若选取$\epsilon=5$，则红点均为核心对象</p></li><li>密度直达<br>  若样本$x^{(j)} \in N_{\epsilon}(x^{(i)})$，且$x^{(i)}$为核心对象，则称$x^{(j)}$由$x^{(i)}$密度直达。不满足对称性，即反之不一定成立，除非$x^{(j)}$也为核心对象。如图，$x^{(8)}$可由$x^{(6)}$密度直达，而反之$x^{(6)}$不可由$x^{(8)}$密度直达，因为$x^{(8)}$不为核心对象。</li><li>密度可达<br>  若存在样本序列$p_1, p_2, …, p_T$，满足$p_1 = x^{(i)}, p_T = x^{(j)}$，且$p_{t+1}$可由$p_t$密度直达，也就是说$p_1, p_2, …, p_{T-1}$均为核心对象，则称$x^{(j)}$由$x^{(i)}$密度可达。也不满足对称性。如图，$x^{(4)}$可由$x^{(1)}$密度可达，而$x^{(2)}$不可由$x^{(4)}$密度可达，因为$x^{(4)}$不为核心对象。</li><li>密度相连<br>  存在核心对象$x^{(k)}$，使得$x^{(i)}$与$x^{(j)}$均由$x^{(k)}$密度可达，则称$x^{(i)}$与$x^{(j)}$密度相连。注意密度相连满足对称性。如图，$x^{(8)}$与$x^{(4)}$均可由$x^{(1)}$密度可达，则$x^{(8)}$与$x^{(4)}$密度相连。</li></ul><p><img src="/2018/11/16/Clustering/DBSCAN1.jpg" alt="DBSCAN1"></p><h3 id="计算思想"><a href="#计算思想" class="headerlink" title="计算思想"></a>计算思想</h3><p><code>DBSCAN</code>的聚类思想是，由<strong>密度可达关系</strong>导出的最大密度相连的样本集合，即为我们最终聚类的一个簇，这个簇里可能只有一个核心对象，也可能有多个核心对象，若有多个，则簇里的任意一个核心对象的$\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。</p><p>另外，考虑以下三个问题</p><ul><li>噪音点<br>  一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，这些样本点标记为噪音点，<code>with Noise</code>就是这个意思。</li><li>距离的度量<br>  一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和<code>KNN</code>算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用<code>KDTree</code>或者球树来快速的搜索最近邻。</li><li>类别重复时的判别<br>  某些样本可能到两个核心对象的距离都小于$\epsilon$，但是这两个核心对象如下图所示，不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？<br>  <img src="/2018/11/16/Clustering/DBSCAN2.jpg" alt="DBSCAN2"><br>  一般来说，此时<code>DBSCAN</code>采用<strong>先来后到</strong>，先进行聚类的类别簇会标记这个样本为它的类别。也就是说<code>BDSCAN</code>不是完全稳定的算法。</li></ul><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$(\epsilon, minPts)$与样本距离度量方式，将其划分为$K$类。</p><ol><li>检测数据库中尚未检查过的对象$p$，如果$p$未被处理(归为某个簇或者标记为噪声)，则检查其邻域：<ul><li>若包含的对象数不小于$minPts$，建立新簇$C$，将其中的所有点加入候选集$N$；</li></ul></li><li>对候选集$N$中所有尚未被处理的对象$q$，检查其邻域：<ul><li>若至少包含$minPts$个对象，则将这些对象加入$N$；</li><li>如果$q$未归入任何一个簇，则将$q$加入$C$；</li></ul></li><li>重复步骤$2$，继续检查$N$中未处理的对象，直到当前候选集$N$为空；</li><li>重复步骤$1$-$3$，直到所有对象都归入了某个簇或标记为噪声。</li></ol><h2 id="高斯混合模型-GMM"><a href="#高斯混合模型-GMM" class="headerlink" title="高斯混合模型(GMM)"></a>高斯混合模型(GMM)</h2><p>详情查看<a href="https://louishsu.xyz/2018/11/12/EM%E7%AE%97%E6%B3%95-GMM%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">EM算法 &amp; GMM模型</a>。</p><h2 id="层次聚类-Hierarchical-Clustering"><a href="#层次聚类-Hierarchical-Clustering" class="headerlink" title="层次聚类(Hierarchical Clustering)"></a>层次聚类(Hierarchical Clustering)</h2><p>层次聚类更多的是一种思想，而不是方法，通过从下往上不断合并簇，或者从上往下不断分离簇形成嵌套的簇。例如上面讲到的<code>DBSCAN</code>最后簇的合并就有这种思想。</p><p>层次的类通过“树状图”来表示，如下<br><img src="/2018/11/16/Clustering/层次聚类.png" alt="层次聚类"></p><p>主要的思想或方法有两种</p><ul><li>自底向上的凝聚方法<code>(agglomerative hierarchical clustering)</code><br>  如<code>AGNES</code>。</li><li>自上向下的分裂方法<code>(divisive hierarchical clustering)</code><br>  如<code>DIANA</code>。</li></ul><h2 id="图团体检测-Graph-Community-Detection"><a href="#图团体检测-Graph-Community-Detection" class="headerlink" title="图团体检测(Graph Community Detection)"></a>图团体检测(Graph Community Detection)</h2><p>略</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EM &amp; GMM</title>
      <link href="/2018/11/12/EM-GMM/"/>
      <url>/2018/11/12/EM-GMM/</url>
      
        <content type="html"><![CDATA[<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p><code>Expectation Maximization Algorithm</code>，是 Dempster, Laind, Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 <code>MLE</code> 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据。</p><h2 id="引例：先挖个坑"><a href="#引例：先挖个坑" class="headerlink" title="引例：先挖个坑"></a>引例：先挖个坑</h2><p>给出李航《统计学习方法》的三硬币模型例子，假设有$3$枚硬币$A, B, C$，各自出现正面的概率分别为$\pi, p, q$，先进行如下实验：先投掷硬币$A$，若结果为正面，则选择硬币$B$投掷一次，否则选择$C$，记录投掷结果如下</p><script type="math/tex; mode=display">1, 1, 0, 1, 0, 0, 1, 0, 1, 1</script><p>只能观测到实验结果，而投掷过程未知，即硬币$A$的投掷结果未知，现欲估计三枚硬币的参数$\pi, p, q$。</p><p><strong>解</strong>：根据题意可以得到三个随机变量$X_1, X_2, X_3$的概率分布如下</p><script type="math/tex; mode=display">P(X_1) = \pi ^ {X_1} (1 - \pi) ^ {1 - X_1}</script><script type="math/tex; mode=display">P(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2}</script><script type="math/tex; mode=display">P(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}</script><p>定义随机变量$X$表示观测结果为正面，由全概率公式可以得到</p><script type="math/tex; mode=display">P(X) = P(X|X_1)P(X_1) + P(X|\overline{X_1})P(\overline{X_1})= \pi p + (1 - \pi) q</script><script type="math/tex; mode=display">P(\overline{X}) = P(\overline{X}|X_1)P(X_1) + P(\overline{X}|\overline{X_1})P(\overline{X_1})= \pi (1 - p) + (1 - \pi) (1 - q)</script><p>即</p><script type="math/tex; mode=display">P(X) = [\pi p + (1 - \pi) q] ^ {X} [\pi (1 - p) + (1 - \pi) (1 - q)] ^ {1 - X}</script><p>利用最大似然估计，有</p><script type="math/tex; mode=display">\log L(D | \theta) = 6 \log [\pi p + (1 - \pi) q] + 4 \log [\pi (1 - p) + (1 - \pi) (1 - q)]</script><p>至此，我们一定能想到通过求似然函数极值来求解参数</p><script type="math/tex; mode=display">\frac{∂ }{∂ \pi} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><script type="math/tex; mode=display">\frac{∂ }{∂ p} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><script type="math/tex; mode=display">\frac{∂ }{∂ q} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><p>但是好像出了问题，并不能求解，所以我们引入<code>EM算法</code>迭代求解。</p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>以$x^{(i)}$表示训练数据，$w_k$表示类别，设当前迭代参数为$\theta^{(t)}$，则下一次迭代应有</p><script type="math/tex; mode=display">\theta^{(t+1)} = \arg \max \sum_i \log P(x^{(i)}|\theta) \tag{1}</script><p>由边缘概率公式</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \tag{2}</script><blockquote><p>$P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)}|x^{(i)}, \theta)$<br>至此已得出引例中的表达式，其中$P(w_k^{(i)}|x^{(i)}, \theta)$与$P(x^{(i)} | w_k^{(i)}, \theta)$均未知，而通过求极值不能解得参数。</p></blockquote><p>我们引入迭代参数$\theta^{(t)}$，即第$t$次迭代时的参数$\theta$，该参数为已知变量</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)} | \theta^{(t)})}{P(w_k^{(i)} | \theta^{(t)})}</script><blockquote><p>$P(w_k^{(i)}|\theta^{(t)})$表示样本$x^{(i)}$类别为$w_k^{(i)}$的概率，注意上标。</p></blockquote><p>引入<code>Jensen不等式</code>：</p><blockquote><p>For a real convex function $\varphi$, numbers $x_1, …, x_n$ in its domain, and positive weights $a_i$, Jensen’s inequality can be stated as:</p><script type="math/tex; mode=display">\varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right)\leq \frac{\sum a_i \varphi(x_i)}{\sum a_i}</script><p>and the inquality is reversed if $\varphi$ is concave, which is</p><script type="math/tex; mode=display">\varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right)\geq \frac{\sum a_i \varphi(x_i)}{\sum a_i}</script><p>Equality holds if and only if $x_1 = … = x_n$ or $\varphi$ is linear.</p></blockquote><p>$\log(·)$为凹函数<code>(concave)</code>，且满足</p><script type="math/tex; mode=display">\sum_k P(w_k^{(i)} | \theta^{(t)}) = 1</script><p>所以有</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">\geq \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{3}</script><p>此时我们得到似然函数$\sum_i \log P(x^{(i)}|\theta)$的一个下界，但必须保证这个下界是紧的，也就是至少有点能使等号成立</p><blockquote><p>由<code>Jensen不等式</code>，当且仅当$　P(x^{(i)}, w_k^{(i)}|\theta)=C　$时取等号</p></blockquote><p>定义</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) - P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)})</script><p>其中第一项即期望</p><script type="math/tex; mode=display">E_w\left[    \log P(X, w|\theta) | X, \theta^{(t)}\right] = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) \tag{4}</script><p>第二项为$P(w | X, \theta^{(t)})$的信息熵</p><script type="math/tex; mode=display">H[P(w | X, \theta^{(t)})] = - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5}</script><p>即</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= E_w\left[    \log P(X, w|\theta) | X, \theta^{(t)}\right] +H[P(w | X, \theta^{(t)})] \tag{E-step}</script><blockquote><p>注意到$H[P(w | X, \theta^{(t)})]$项为常数，故也可设</p><script type="math/tex; mode=display">Q(\theta|\theta^{(t)}) = E_w\left[\log P(X, w|\theta) | X, \theta^{(t)} \right]</script></blockquote><p>代回$(1)$，得到优化目标</p><script type="math/tex; mode=display">\theta^{(t+1)} = \arg \max L(\theta|\theta^{(t)}) \tag{M-step}</script><p>我们需要不断最大化$L(\theta | \theta^{(t)})$来不断优化，这就是所谓的<code>EM算法</code>，<code>E-step</code>是指求出期望，<code>M-step</code>是指迭代更新参数<br><img src="/2018/11/12/EM-GMM/EM算法图解.png" alt="EM算法图解"></p><p>伪代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">According to prior knowledge set </span><br><span class="line">    $\theta$</span><br><span class="line">Repeat until convergence&#123;</span><br><span class="line">    E-step: The expectation of hidden variables</span><br><span class="line">    M-step: Finding the maximum of likelihood function</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>实际上，从边缘概率与条件概率入手</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta)</script><script type="math/tex; mode=display">= \sum_i \log \sum_k P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta)</script><script type="math/tex; mode=display">\geq \sum_i \sum_k P(w_k^{(i)} | \theta) \log P(x^{(i)} | w_k^{(i)}, \theta) \tag{Jensen inequality}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)}</script><p>而由$(3)$，引入迭代变量可以得到</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)\geq L(\theta|\theta^{(t)})</script><p>其中</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><p>则</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)})</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} -\sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta)}</script><p>而由<code>KL散度( Kullback–Leibler divergence)</code>(又称<code>相对熵(relative entropy)</code>)定义</p><blockquote><script type="math/tex; mode=display">D(P||Q) = \sum P(x) \log \frac{P(x)}{Q(x)}</script></blockquote><p>可知</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = D\left[ P(w_k^{(i)}|\theta^{(t)}) || P(w_k^{(i)}|\theta) \right]</script><p>即迭代的$P(w_k^{(i)}|\theta^{(t)})$与真实的$P(w_k^{(i)}|\theta)$之间的相对熵！</p><blockquote><p>这里关于<code>K-L散度</code>的困扰了$N$久，终于搞出来了。</p></blockquote><!-- 另外，附上证明一则> 证明对数似然函数$\sum_i \log P(x^{(i)} | \theta)$满足> $$> \sum_i \log P(x^{(i)} | \theta^{(t+1)}) > \geq \sum_i \log P(x^{(i)} | \theta^{(t)})> $$> > 证明：由$(M-step)$> $$\sum_i \log P(x^{(i)} | \theta^{(t+1)})> = \max L(\theta | \theta^{(t)})$$> > 而$\theta^{(t+1)}为函数L(\theta|\theta^{(t)})极值点$，所以> $$\max L(\theta | \theta^{(t)})> \geq L(\theta | \theta^{(t)})$$> > 其中> $$> L(\theta | \theta^{(t)})> = \sum_i \log P(x^{(i)} | \theta^{(t)})> $$> > 故> $$> \sum_i \log P(x^{(i)} | \theta^{(t+1)}) > \geq \sum_i \log P(x^{(i)} | \theta^{(t)})> $$ --><h2 id="引例的求解"><a href="#引例的求解" class="headerlink" title="引例的求解"></a>引例的求解</h2><blockquote><p>$Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$</p></blockquote><p>此题中</p><script type="math/tex; mode=display">P(w_k|\pi) = \pi^{w_k}(1-\pi)^{1-w_k}</script><script type="math/tex; mode=display">P(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}}</script><script type="math/tex; mode=display">P(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}}</script><ul><li><p>$E-step$</p><script type="math/tex; mode=display">  Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)})  = \sum_i \sum_k P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})   \log P(x^{(i)}, w_k^{(i)} | \pi, p, q)</script><p>  先求$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$，即第一次投掷结果为$w_k$的概率</p><script type="math/tex; mode=display">  P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})  = \frac  {\left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_k}}  {\sum_j \left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_j}  \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_j}}</script><p>  即</p><script type="math/tex; mode=display">  \begin{cases}      P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}}          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} +           (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\      P(w_2^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac          {(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} +           (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}  \end{cases}</script><p>  记</p><script type="math/tex; mode=display">\mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})</script><script type="math/tex; mode=display">\mu_2^{(i)} = 1 - \mu_1^{(i)}</script><blockquote><p>注意$w^{(i)}_k$上标<code>^{(i)}</code></p></blockquote><p>  再求$P(x^{(i)}, w_k^{(i)} | \pi, p, q)$，已知</p><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)} | \pi, p, q)  = P(x^{(i)} | w_k^{(i)}, \pi, p, q)  P(w_k^{(i)} | \pi, p, q)</script><p>  所以</p><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)} | \pi, p,q)  = \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k}</script><p>  综上</p><script type="math/tex; mode=display">  Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)})  = \sum_i \sum_{k=1}^2 \mu^{(i)}_k \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k}</script><script type="math/tex; mode=display">  = \sum_i \mu_1^{(i)} \log \pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \mu_1^{(i)}) \log (1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}</script></li><li><p>$M-step$</p><ul><li><p>$\frac{∂Q}{∂\pi} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂\pi}  = \sum_i \mu_1^{(i)}   \frac  {p^{x^{(i)}}(1-p)^{1-x^{(i)}}}  {\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} +   (1 - \mu_1^{(i)})  \frac  {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}}  {(1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}}</script><script type="math/tex; mode=display">  = \sum_i \frac{\mu_1^{(i)}}{\pi} + \frac{\mu_1^{(i)} - 1}{1 - \pi}  = \sum_i \frac{\mu_1^{(i)} - \pi}{\pi(1 - \pi)}  = \frac{\sum_i \mu_1^{(i)} - n\pi}{\pi(1 - \pi)} = 0</script><script type="math/tex; mode=display">  \Rightarrow \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)}</script></li><li><p>$\frac{∂Q}{∂p} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂p}  = \sum_i \mu_1^{(i)}   \left[      \frac{x^{(i)}}{p} - \frac{1 - x^{(i)}}{1 - p}  \right]</script><script type="math/tex; mode=display">  = \frac{1}{p(1 - p)}   \sum_i \mu_1^{(i)} (x^{(i)} - p)  = \frac{1}{p(1 - p)}  \left[      \sum_i \mu_1^{(i)} x^{(i)} - p \sum_i \mu_1^{(i)}  \right] = 0</script><script type="math/tex; mode=display">  \Rightarrow p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}}</script></li><li><p>$\frac{∂Q}{∂q} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂q}  = \sum_i (1 - \mu_1^{(i)})   \left[      \frac{x^{(i)}}{q} - \frac{1 - x^{(i)}}{1 - q}  \right]</script><script type="math/tex; mode=display">  = \frac{1}{q(1 - q)}  \sum_i (1 - \mu_1^{(i)})   (x^{(i)} - q)</script><script type="math/tex; mode=display">  = \frac{1}{q(1 - q)}  \left[      \sum_i (1 - \mu_1^{(i)}) x^{(i)} -      q \sum_i (1 - \mu_1^{(i)})  \right] = 0</script><script type="math/tex; mode=display">  \Rightarrow q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})}</script></li></ul></li></ul><p>多次迭代即可求解，终止条件可设置为</p><script type="math/tex; mode=display">|| \theta^{(t+1)} - \theta^{(t)} || < \epsilon</script><p>或</p><script type="math/tex; mode=display">||Q(\theta^{(t+1)} | \theta^{(t)}) - Q(\theta^{(t)} |\theta^{(t)})|| < \epsilon</script><h1 id="GMM模型"><a href="#GMM模型" class="headerlink" title="GMM模型"></a>GMM模型</h1><p><code>Gaussian Mixture Model</code>，是一种无监督学习算法，常用于聚类。当聚类问题中各个类别的尺寸不同、聚类间有相关关系的时候，往往使用<code>GMM</code>更合适。对一个样本来说，<code>GMM</code>得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被成为软聚类。一般说来， 任意形状的概率分布都可以用多个高斯分布函数去近似，因而，<code>GMM</code>的应用也比较广泛。</p><p>高斯混合模型，指具有如下形式的概率分布模型：</p><script type="math/tex; mode=display">P(x|\mu_k, \Sigma_k)= \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)</script><p>其中</p><ul><li>$\pi_k(0 \leq \pi_k \leq 1)$是系数，且$\sum_k \pi_k = 1$</li><li>$N(x|\mu_k, \Sigma_k)$为高斯密度函数</li></ul><script type="math/tex; mode=display">N(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[    -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right]</script><blockquote><ul><li>即多个高斯分布叠加出来的玩意；</li><li>现在我们需要求取系数$\pi_k$及高斯模型的参数$(\mu_k, \Sigma_k)$；</li><li>与<code>K-Means</code>等聚类方法区别是，<code>GMM</code>求出的是连续的分布模型，可计算出“归属于”哪一类的概率。</li></ul></blockquote><h2 id="推导-1"><a href="#推导-1" class="headerlink" title="推导"></a>推导</h2><script type="math/tex; mode=display">\log P(X|\pi, \mu, \Sigma)= \sum_i \log \sum_k \pi_k N(x|\mu_k, \Sigma_k)</script><script type="math/tex; mode=display">s.t.　\sum_k \pi_k = 1</script><h3 id="暴力求解"><a href="#暴力求解" class="headerlink" title="暴力求解"></a>暴力求解</h3><p>以$1$维高斯分布为例</p><script type="math/tex; mode=display">N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}}</script><p>构造拉格朗日<code>(Lagrange)</code>函数</p><script type="math/tex; mode=display">L(\pi, \mu, \sigma^2) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \sigma_k^2) + \lambda \left(\sum_k \pi_k - 1 \right) \tag{5}</script><script type="math/tex; mode=display">\begin{cases}    \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2)         = \sum_i        \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\    \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\mu_k}N(x^{(i)}|\mu_k, \sigma_k^2) \\    \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\sigma_k^2}N(x^{(i)}|\mu_k, \sigma_k^2)\end{cases} \tag{6}</script><p>其中</p><script type="math/tex; mode=display">\frac{∂}{∂\mu_k} N(x|\mu_k, \sigma_k^2)= \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{x-\mu_k}{\sigma_k^2}= N(x|\mu_k, \sigma_k^2) · \frac{x-\mu_k}{\sigma_k^2}</script><script type="math/tex; mode=display">\frac{∂}{∂\sigma_k^2} N(x|\mu_k, \sigma_k^2)= \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right)</script><blockquote><p>$\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) = - \frac{\sigma_k^{-3}}{2};　\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) = - \frac{1}{\sigma_k^4}$</p></blockquote><script type="math/tex; mode=display">= \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(- \frac{\sigma_k^{-3}}{2}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \left(- \frac{1}{\sigma_k^4}\right)</script><script type="math/tex; mode=display">= N(x|\mu_k, \sigma_k^2) \left[    \frac{(x - \mu_k)^2}{\sigma_k^2} - 1\right] \frac{1}{2 \sigma_k^2}</script><p>代回$(6)$可以得到</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\    \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{x^{(i)}-\mu_k}{\sigma_k^2} \\    \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \left[    \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1\right] \frac{1}{2 \sigma_k^2}\end{cases} \tag{7}</script><p>令</p><script type="math/tex; mode=display">\gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \tag{8}</script><blockquote><p>通俗理解：$\gamma^{(i)}_k$表示样本$x^{(i)}$中来自类别$w_k$的“贡献百分比”</p></blockquote><ul><li><p>令$\frac{∂}{∂\mu_k} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到</p><script type="math/tex; mode=display">  \sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) = 0  \Rightarrow   \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k}</script></li><li><p>令$\frac{∂}{∂\sigma_k^2} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到</p><script type="math/tex; mode=display">  \sum_i      \gamma^{(i)}_k       \left[          \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1      \right] = 0  \Rightarrow  \sigma_k^2 = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k)^2}{\sum_i \gamma^{(i)}_k}</script></li><li><p>对于$\frac{∂}{∂\pi_k} \log P(X|\pi, \mu, \sigma^2) = 0$，需要做一点处理<br>  两边同乘$\pi_k$，得到</p><script type="math/tex; mode=display">  \sum_i \gamma^{(i)}_k = - \lambda \pi_k \tag{9}</script><p>  然后两边对$k$作累加</p><script type="math/tex; mode=display">  \sum_k \sum_i \gamma^{(i)}_k = - \lambda \sum_k \pi_k</script><blockquote><p>$\sum_k \sum_i \gamma^{(i)}_k = \sum_i \sum_k \gamma^{(i)}_k = N,　\sum_k \pi_k = 1$</p></blockquote><script type="math/tex; mode=display">  N = - \lambda　或　\lambda = -N \tag{10}</script><p>  代回$(9)$，得到</p><script type="math/tex; mode=display">  \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}</script></li></ul><p>综上，我们得到$4$个用于迭代的计算式，将其推广至多维即</p><script type="math/tex; mode=display">\gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)}</script><script type="math/tex; mode=display">\mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k}</script><script type="math/tex; mode=display">\Sigma_k = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)}_k}</script><script type="math/tex; mode=display">\pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}</script><h3 id="用EM算法求解"><a href="#用EM算法求解" class="headerlink" title="用EM算法求解"></a>用<code>EM算法</code>求解</h3><blockquote><p>$Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$</p></blockquote><script type="math/tex; mode=display">Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})\log P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k)</script><ul><li><p>$ M-step $</p><script type="math/tex; mode=display">  P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})  = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)}  = \gamma^{(i)}_k</script><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k)  = P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k)   P(w_k^{(i)}|\mu_k, \Sigma_k)  = \pi_k N(x^{(i)}|\mu_k, \Sigma_k)</script><p>  故</p><script type="math/tex; mode=display">  Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)})   = \sum_i \sum_k \gamma^{(i)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k)</script><p>  通过求解极值可得到与$\underline{暴力求解}$一样的等式，即</p><script type="math/tex; mode=display">  \gamma^{(i)(t)}_k = \frac{\pi^{(t)}_k N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})}</script><script type="math/tex; mode=display">  \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k}</script><script type="math/tex; mode=display">  \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k}</script><script type="math/tex; mode=display">  \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N}</script><p>  伪代码为</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">According to prior knowledge set</span><br><span class="line">    \pi^&#123;(t)&#125;(n_clusters,)</span><br><span class="line">    \mu^&#123;(t)&#125;(n_clusters, n_features)</span><br><span class="line">    \Sigma^&#123;(t)&#125;(n_clusters, n_features, n_features)</span><br><span class="line">Repeat until convergence&#123;</span><br><span class="line">    # E-step: calculate \gamma^&#123;(t)&#125;</span><br><span class="line">        \gamma(n_samples, n_clusters)</span><br><span class="line">    # M-step: update \pi, \mu, \Sigma</span><br><span class="line">        \pi^&#123;(t+1)&#125;(n_clusters,)</span><br><span class="line">        \mu^&#123;(t+1)&#125;(n_clusters, n_features)</span><br><span class="line">        \Sigma^&#123;(t+1)&#125;(n_clusters, n_features, n_features)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>初始点的选择可以随机选择，也可使用<code>K-Means</code></p></blockquote><p>  <code>GMM</code>算法收敛过程如下<br>  <img src="/2018/11/12/EM-GMM/gmm.gif" alt="gmm"></p></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p82_gmm.py" target="_blank" rel="noopener">@Github: GMM</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">class GMM():</span><br><span class="line">    &quot;&quot;&quot; Gaussian Mixture Model</span><br><span class="line">    Attributes:</span><br><span class="line">        n_clusters &#123;int&#125;</span><br><span class="line">        prior &#123;ndarray(n_clusters,)&#125;</span><br><span class="line">        mu &#123;ndarray(n_clusters, n_features)&#125;</span><br><span class="line">        sigma &#123;ndarray(n_clusters, n_features, n_features)&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, n_clusters):</span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.prior = None</span><br><span class="line">        self.mu = None</span><br><span class="line">        self.sigma = None</span><br><span class="line">    def fit(self, X, delta=0.01):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            X &#123;ndarray(n_samples, n_features)&#125;</span><br><span class="line">            delta &#123;float&#125;</span><br><span class="line">        Notes:</span><br><span class="line">            - Initialize with k-means</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line"></span><br><span class="line">        # initialize with k-means</span><br><span class="line">        clf = KMeans(n_clusters=self.n_clusters)</span><br><span class="line">        clf.fit(X)</span><br><span class="line">        self.mu = clf.cluster_centers_ </span><br><span class="line">        self.prior = np.zeros(self.n_clusters)</span><br><span class="line">        self.sigma = np.zeros((self.n_clusters, n_features, n_features))</span><br><span class="line">        for k in range(self.n_clusters):</span><br><span class="line">            X_ = X[clf.labels_==k]</span><br><span class="line">            self.prior[k] = X_.shape[0] / X_.shape[0]</span><br><span class="line">            self.sigma[k] = np.cov(X_.T)</span><br><span class="line">        </span><br><span class="line">        while True:</span><br><span class="line">            mu_ = self.mu.copy()</span><br><span class="line">            # E-step: updata gamma</span><br><span class="line">            gamma = np.zeros((n_samples, self.n_clusters))</span><br><span class="line">            for i in range(n_samples):</span><br><span class="line">                for k in range(self.n_clusters):</span><br><span class="line">                    denominator = 0</span><br><span class="line">                    for j in range(self.n_clusters):</span><br><span class="line">                        post = self.prior[k] *\</span><br><span class="line">                                    multiGaussian(X[i], self.mu[j], self.sigma[j])</span><br><span class="line">                        denominator += post</span><br><span class="line">                        if j==k: numerator = post</span><br><span class="line">                    gamma[i, k] = numerator/denominator</span><br><span class="line">            # M-step: updata prior, mu, sigma</span><br><span class="line">            for k in range(self.n_clusters):</span><br><span class="line">                sum1 = 0</span><br><span class="line">                sum2 = 0</span><br><span class="line">                sum3 = 0</span><br><span class="line">                for i in range(n_samples):</span><br><span class="line">                    sum1 += gamma[i, k]</span><br><span class="line">                    sum2 += gamma[i, k] * X[i]</span><br><span class="line">                    x_ = np.reshape(X[i] - self.mu[k], (n_features, 1))</span><br><span class="line">                    sum3 += gamma[i, k] * x_.dot(x_.T)</span><br><span class="line">                self.prior[k]  = sum1 / n_samples</span><br><span class="line">                self.mu[k]     = sum2 / sum1</span><br><span class="line">                self.sigma[k]  = sum3 / sum1</span><br><span class="line">            # to stop</span><br><span class="line">            mu_delta = 0</span><br><span class="line">            for k in range(self.n_clusters):</span><br><span class="line">                mu_delta += nl.norm(self.mu[k] - mu_[k])</span><br><span class="line">            print(mu_delta)</span><br><span class="line">            if mu_delta &lt; delta: break</span><br><span class="line">        return self.prior, self.mu, self.sigma</span><br><span class="line">    def predict_proba(self, X):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            X &#123;ndarray(n_samples, n_features)&#125;</span><br><span class="line">        Returns:</span><br><span class="line">            y_pred_proba &#123;ndarray(n_samples, n_clusters)&#125;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        y_pred_proba = np.zeros((n_samples, self.n_clusters))</span><br><span class="line">        for i in range(n_samples):</span><br><span class="line">            for k in range(self.n_clusters):</span><br><span class="line">                y_pred_proba[i, k] = self.prior[k] *\</span><br><span class="line">                                multiGaussian(X[i], self.mu[k], self.sigma[k])</span><br><span class="line">        return y_pred_proba</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            X &#123;ndarray(n_samples, n_features)&#125;</span><br><span class="line">        Returns:</span><br><span class="line">            y_pred_proba &#123;ndarray(n_samples,)&#125;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        y_pred_proba = self.predict_proba(X)</span><br><span class="line">        return np.argmax(y_pred_proba, axis=1)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Data Augmentation</title>
      <link href="/2018/11/02/Data-Augmentation/"/>
      <url>/2018/11/02/Data-Augmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>“有时候不是由于算法好赢了。而是由于拥有很多其它的数据才赢了。”</p></blockquote><h1 id="数据集扩增"><a href="#数据集扩增" class="headerlink" title="数据集扩增"></a>数据集扩增</h1><p>在深度学习中,很多训练数据意味着能够用更深的网络，训练出更好的模型。既然这样，收集很多其它的数据不即可啦？假设能够收集很多其它能够用的数据当然好，比如<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>上图像数据量已达到$1400$万张，可是非常多时候，收集很多其它的数据意味着须要耗费很多其它的人力物力，这就需要使用一定的方法扩增数据集。</p><h1 id="图像扩增"><a href="#图像扩增" class="headerlink" title="图像扩增"></a>图像扩增</h1><p>大部分借助<code>OpenCV</code>库，这里推荐一位学长的博客，整理了大量的<code>OpenCV</code>使用方法.</p><blockquote><p><a href="http://ex2tron.wang/" target="_blank" rel="noopener">Ex2tron’s Blog</a></p></blockquote><p><code>TensorFlow</code>也提供相应图像处理方法<br><a href="https://tensorflow.google.cn/api_docs/python/tf/image" target="_blank" rel="noopener">Module: tf.image | TensorFlow </a></p><p>需要注意的是，扩增过程中，需注意图像数据类型，可以将数据归一化到$(0, 1)$间再进行处理</p><h2 id="翻转"><a href="#翻转" class="headerlink" title="翻转"></a>翻转</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def flip(image):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Parameters:</span><br><span class="line">        image &#123;ndarray(H, W, C)&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    rand_var = np.random.random()</span><br><span class="line">    image = image[:, ::-1, :] if rand_var &gt; 0.5 else image</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure><h2 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def rotate(image, degree):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Parameters:</span><br><span class="line">        image &#123;ndarray(H, W, C)&#125;</span><br><span class="line">        degree &#123;float&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    (h, w) = image.shape[:2]</span><br><span class="line">    center = (w // 2, h // 2)</span><br><span class="line">    random_angel = np.random.randint(-degree, degree)</span><br><span class="line">    M = cv2.getRotationMatrix2D(center, random_angel, 1.0)</span><br><span class="line">    image = cv2.warpAffine(image, M, (w, h))</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure><h2 id="噪声"><a href="#噪声" class="headerlink" title="噪声"></a>噪声</h2><p>可手动实现，如椒盐噪声代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def saltnoise(image, salt=0.0):</span><br><span class="line">    &quot;&quot;&quot; add salt &amp; pepper and gaussian noise</span><br><span class="line">    Parameters:</span><br><span class="line">        image &#123;ndarray(H, W, C)&#125;</span><br><span class="line">        salt &#123;float(0, 1)&#125; number of salt pixel = salt*h*w</span><br><span class="line">    Notes:</span><br><span class="line">        TODO: gaussain noise</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    (h, w) = image.shape[:2]</span><br><span class="line">    n_salt = int(salt * h * w)</span><br><span class="line">    for n in range(n_salt):</span><br><span class="line">        hr = np.random.randint(0, h)</span><br><span class="line">        wr = np.random.randint(0, w)</span><br><span class="line">        issalt = (np.random.rand(1) &gt; 0.5)</span><br><span class="line">        image[hr, wr] = 255 if issalt else 0</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure></p><p>也可调用<code>scikit-image</code>库，需要注意的是，<code>skimage.util.random_noise()</code>会将原图数据转换为$(0, 1)$间的浮点数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def noise(image, gaussian, salt, seed=None):</span><br><span class="line">    &quot;&quot;&quot; add noise to image TODO</span><br><span class="line">    Parameters:</span><br><span class="line">        image &#123;ndarray(H, W, C)&#125;</span><br><span class="line">        gaussian &#123;bool&#125;: </span><br><span class="line">        salt &#123;bool&#125;: </span><br><span class="line">    Notes:</span><br><span class="line">        Function to add random noise of various types to a floating-point image.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dtype = image.dtype</span><br><span class="line">    if gaussian:</span><br><span class="line">        image = skimage.util.random_noise(image, mode=&apos;gaussian&apos;, seed=seed)</span><br><span class="line">    if salt:</span><br><span class="line">        image = skimage.util.random_noise(image, mode=&apos;s&amp;p&apos;, seed=seed)</span><br><span class="line"></span><br><span class="line">    image = (image * 255).astype(dtype)</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure></p><h2 id="亮度与对比度调整"><a href="#亮度与对比度调整" class="headerlink" title="亮度与对比度调整"></a>亮度与对比度调整</h2><p>考虑到数据溢出，先转换为整形数据，再限制其值到$[0, 255]$</p><blockquote><p>注意数据类型</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def brightcontrast(image, brtadj=0, cstadj=1.0):</span><br><span class="line">    &quot;&quot;&quot; adjust bright and contrast value</span><br><span class="line">    Parameters:</span><br><span class="line">        image &#123;ndarray(H, W, C)&#125;</span><br><span class="line">        brtadj &#123;int&#125;    if true, adjust bright</span><br><span class="line">        cstadj &#123;float&#125;  if true, adjust contrast</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dtype = image.dtype</span><br><span class="line">    image = image.astype(&apos;int&apos;)*cstadj + brtadj</span><br><span class="line">    image = np.clip(image, 0, 255).astype(dtype)</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure><h2 id="投射变换"><a href="#投射变换" class="headerlink" title="投射变换"></a>投射变换</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def perspective(image, prop):</span><br><span class="line">    &quot;&quot;&quot; 透射变换</span><br><span class="line">    Parameters:</span><br><span class="line">        image &#123;ndarray(H, W, C)&#125;</span><br><span class="line">        prop &#123;float&#125;: 在四个顶点多大的方格内选取新顶点，方格大小为(H*prop, W*prop)</span><br><span class="line">    Notes:</span><br><span class="line">        在四个顶点周围随机选取新的点进行仿射变换，四个点对应左上、右上、左下、右下</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    (h, w) = image.shape[:2]</span><br><span class="line"></span><br><span class="line">    ptsrc = np.zeros(shape=(4, 2))</span><br><span class="line">    ptdst = np.array([[0, 0], [0, w], [h, 0], [h, w]])</span><br><span class="line">    for i in range(4):</span><br><span class="line">        hr = np.random.randint(0, int(h*prop))</span><br><span class="line">        wr = np.random.randint(0, int(w*prop))</span><br><span class="line">        if i == 0:</span><br><span class="line">            ptsrc[i] = np.array([hr, wr])</span><br><span class="line">        elif i == 1:</span><br><span class="line">            ptsrc[i] = np.array([hr, w - wr])</span><br><span class="line">        elif i == 2:</span><br><span class="line">            ptsrc[i] = np.array([h - hr, wr])</span><br><span class="line">        elif i == 3:</span><br><span class="line">            ptsrc[i] = np.array([h - hr, w - wr])</span><br><span class="line">    M = cv2.getPerspectiveTransform(ptsrc.astype(&apos;float32&apos;), ptdst.astype(&apos;float32&apos;))</span><br><span class="line">    image = cv2.warpPerspective(image, M, (w, h))</span><br><span class="line">    return image</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>二次入坑raspberry-pi</title>
      <link href="/2018/10/29/%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi/"/>
      <url>/2018/10/29/%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>距上一次搭建树莓派平台已经两年了，保存的镜像出了问题，重新搭建一下。</p><h1 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>从官网下载树莓派系统镜像，有以下几种可选</p><blockquote><p><a href="https://www.raspberrypi.org/" target="_blank" rel="noopener">Raspberry Pi — Teach, Learn, and Make with Raspberry Pi </a></p><ol><li>Raspbian &amp; Raspbian Lite，基于Debian</li><li>Noobs &amp; Noobs Lite</li><li>Ubuntu MATE</li><li>Snappy Ubuntu Core</li><li>Windows 10 IOT</li></ol></blockquote><p><del>其余不太了解，之前安装的是Raspbian，对于Debian各种不适，换上界面优雅的Ubuntu Mate玩一下</del><br>老老实实玩Raspbian，笑脸:-)</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>比较简单，准备micro-SD卡，用Win32 Disk Imager烧写镜像</p><blockquote><p><a href="https://sourceforge.net/projects/win32diskimager/" target="_blank" rel="noopener">Win32 Disk Imager download | SourceForge.net</a></p><p><img src="/2018/10/29/二次入坑raspberry-pi/Win32DiskImager.jpg" alt="Win32DiskImager"></p></blockquote><p>安装完软件后可点击<code>Read</code>备份自己的镜像。</p><p>注意第二次开机前需要配置<code>config.txt</code>文件，否则<code>hdmi</code>无法显示</p><blockquote><p><a href="http://shumeipai.nxez.com/2015/11/23/raspberry-pi-configuration-file-config-txt-nstructions.html" target="_blank" rel="noopener">树莓派配置文档 config.txt 说明 | 树莓派实验室</a></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">disable_overscan=1 </span><br><span class="line">hdmi_force_hotplug=1</span><br><span class="line">hdmi_group=2    # DMT</span><br><span class="line">hdmi_mode=32    # 1280x960</span><br><span class="line">hdmi_drive=2</span><br><span class="line">config_hdmi_boost=4</span><br></pre></td></tr></table></figure><h2 id="修改交换分区"><a href="#修改交换分区" class="headerlink" title="修改交换分区"></a>修改交换分区</h2><h3 id="Ubuntu-Mate"><a href="#Ubuntu-Mate" class="headerlink" title="Ubuntu Mate"></a>Ubuntu Mate</h3><p>查看交换分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>未设置时如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:            0        0        0</span><br></pre></td></tr></table></figure></p><p>创建和挂载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 获取权限</span><br><span class="line">$ sudo -i</span><br><span class="line"></span><br><span class="line"># 创建目录</span><br><span class="line">$ mkdir /swap</span><br><span class="line">$ cd /swap</span><br><span class="line"></span><br><span class="line"># 指定一个大小为1G的名为“swap”的交换文件</span><br><span class="line">$ dd if=/dev/zero of=swap bs=1M count=1k</span><br><span class="line"># 创建交换文件</span><br><span class="line">$ mkswap swap</span><br><span class="line"># 挂载交换分区</span><br><span class="line">$ swapon swap</span><br><span class="line"></span><br><span class="line"># 卸载交换分区</span><br><span class="line"># $ swapoff swap</span><br></pre></td></tr></table></figure></p><p>查看交换分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>未设置时如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:         1023        0     1023</span><br></pre></td></tr></table></figure></p><h3 id="Raspbian"><a href="#Raspbian" class="headerlink" title="Raspbian"></a>Raspbian</h3><p>We will change the configuration in the file <code>/etc/dphys-swapfile</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nano /etc/dphys-swapfile</span><br></pre></td></tr></table></figure></p><p>The default value in Raspbian is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONF_SWAPSIZE=100</span><br></pre></td></tr></table></figure></p><p>We will need to change this to:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONF_SWAPSIZE=1024</span><br></pre></td></tr></table></figure></p><p>Then you will need to stop and start the service that manages the swapfile own Rasbian:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /etc/init.d/dphys-swapfile stop</span><br><span class="line">$ sudo /etc/init.d/dphys-swapfile start</span><br></pre></td></tr></table></figure></p><p>You can then verify the amount of memory + swap by issuing the following command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>The output should look like:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:         1023        0     1023</span><br></pre></td></tr></table></figure></p><h1 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h1><h2 id="安装指令"><a href="#安装指令" class="headerlink" title="安装指令"></a>安装指令</h2><ul><li><p><code>apt-get</code></p><ul><li>安装软件<br><code>apt-get install softname1 softname2 softname3 ...</code></li><li>卸载软件<br><code>apt-get remove softname1 softname2 softname3 ...</code></li><li>卸载并清除配置<br><code>apt-get remove --purge softname1</code></li><li>更新软件信息数据库<br><code>apt-get update</code></li><li>进行系统升级<br><code>apt-get upgrade</code></li><li>搜索软件包<br><code>apt-cache search softname1 softname2 softname3 ...</code></li><li>修正（依赖关系）安装：<br><code>apt-get -f insta</code></li></ul></li><li><p><code>dpkg</code></p><ul><li>安装<code>.deb</code>软件包<br><code>dpkg -i xxx.deb</code></li><li>删除软件包<br><code>dpkg -r xxx.deb</code></li><li>连同配置文件一起删除<br><code>dpkg -r --purge xxx.deb</code></li><li>查看软件包信息<br><code>dpkg -info xxx.deb</code></li><li>查看文件拷贝详情<br><code>dpkg -L xxx.deb</code></li><li>查看系统中已安装软件包信息<br><code>dpkg -l</code></li><li><p>重新配置软件包<br><code>dpkg-reconfigure xx</code></p></li><li><p>卸载软件包及其配置文件，但无法解决依赖关系！<br><code>sudo dpkg -p package_name</code></p></li><li>卸载软件包及其配置文件与依赖关系包<br><code>sudo aptitude purge pkgname</code></li><li>清除所有已删除包的残馀配置文件<br><code>dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P</code></li></ul></li></ul><h2 id="软件源"><a href="#软件源" class="headerlink" title="软件源"></a>软件源</h2><ol><li><p>备份原始文件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup</span><br></pre></td></tr></table></figure></li><li><p>修改文件并添加国内源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/apt/sources.list</span><br></pre></td></tr></table></figure></li><li><p>注释元文件内的源并添加如下地址</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#Mirror.lupaworld.com 源更新服务器（浙江省杭州市双线服务器，网通同电信都可以用，亚洲地区官方更新服务器）：</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#Ubuntu 官方源 </span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><p> 或者</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#阿里云</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#网易163</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure></li><li><p>放置非官方源的包不完整，可在为不添加官方源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deb http://archive.ubuntu.org.cn/ubuntu-cn/ feisty main restricted universe multiverse</span><br></pre></td></tr></table></figure></li><li><p>更新源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></li><li><p>更新软件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get dist-upgrade</span><br></pre></td></tr></table></figure></li><li><p>常见的修复安装命令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get -f install</span><br></pre></td></tr></table></figure></li></ol><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>主要是<code>Python</code>和相关依赖包的安装，使用以下指令可导出已安装的依赖包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure></p><p>并使用指令安装到树莓派<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install -r requirements.txt</span><br></pre></td></tr></table></figure></p><p>注意<code>pip</code>更新<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install --upgrade pip</span><br></pre></td></tr></table></figure></p><p>最新版本会报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: cannot import name main</span><br></pre></td></tr></table></figure></p><p>修改文件<code>/usr/bin/pip</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from pip import main</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    sys.exit(main())</span><br></pre></td></tr></table></figure></p><p>改为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from pip import __main__</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    sys.exit(__main__._main())</span><br></pre></td></tr></table></figure></p><hr><p><del>成功!!!</del><br>失败了，笑脸:-)，手动安装吧。。。</p><ul><li><p>部分包可使用<code>pip3</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install numpy</span><br><span class="line">$ pip3 install pandas</span><br><span class="line">$ pip3 install sklearn</span><br></pre></td></tr></table></figure><blockquote><p>若需要权限，加入<code>--user</code></p></blockquote></li><li><p>部分包用<code>apt-get</code>，但是优先安装到<code>Python2.7</code>版本，笑脸:-)</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install python-scipy</span><br><span class="line">$ sudo apt-get install python-matplotlib</span><br><span class="line">$ sudo apt-get install python-opencv</span><br></pre></td></tr></table></figure></li><li><p>部分从<code>PIPY</code>下载<code>.whl</code>或<code>.tar.gz</code>文件</p><blockquote><p><a href="https://pypi.org/" target="_blank" rel="noopener">PyPI – the Python Package Index · PyPI</a></p><ul><li>tensorboardX-1.4-py2.py3-none-any.whl</li><li>visdom-0.1.8.5.tar.gz</li></ul></blockquote><p>  安装指令为</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install xxx.whl</span><br></pre></td></tr></table></figure>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf xxx.tar.gz</span><br><span class="line">$ python setup.py install</span><br></pre></td></tr></table></figure></li><li><p><code>Pytorch</code>源码安装</p><blockquote><p><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration </a></p></blockquote><p>  安装方法<a href="https://github.com/pytorch/pytorch#from-source" target="_blank" rel="noopener">Installation - From Source</a></p><p>  需要用到<code>miniconda</code>，安装方法如下，注意中间回车按慢一点，有两次输入。。。。。(行我慢慢看条款不行么。。笑脸:-))</p><ul><li>第一次是是否同意条款，<code>yes</code></li><li><p>第二次是添加到环境变量，<code>yes</code>，否则自己修改<code>/home/pi/.bashrc</code>添加到环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh</span><br><span class="line">$ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5</span><br><span class="line">$ sudo /bin/bash Miniconda3-latest-Linux-armv7l.sh </span><br><span class="line"># -&gt; change default directory to /home/pi/miniconda3</span><br><span class="line">$ sudo nano /home/pi/.bashrc </span><br><span class="line"># -&gt; add: export PATH=&quot;/home/pi/miniconda3/bin:$PATH&quot;</span><br><span class="line">$ sudo reboot -h now</span><br><span class="line"></span><br><span class="line">$ conda </span><br><span class="line">$ python --version</span><br><span class="line">$ sudo chown -R pi miniconda3</span><br></pre></td></tr></table></figure><p><del>然后就可以安装了</del>没有对应版本的<code>mkl</code>，笑脸:-)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; # [anaconda root directory]</span><br><span class="line"></span><br><span class="line"># Disable CUDA</span><br><span class="line">export NO_CUDA=1</span><br><span class="line"></span><br><span class="line"># Install basic dependencies</span><br><span class="line">conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing</span><br><span class="line">conda install -c mingfeima mkldnn</span><br><span class="line"></span><br><span class="line"># Install Pytorch</span><br><span class="line">git clone --recursive https://github.com/pytorch/pytorch</span><br><span class="line">cd pytorch</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>tensorflow</code><br>  安装tensorflow需要的一些依赖和工具</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line"></span><br><span class="line"># For Python 2.7</span><br><span class="line">$ sudo apt-get install python-pip python-dev</span><br><span class="line"></span><br><span class="line"># For Python 3.3+</span><br><span class="line">$ sudo apt-get install python3-pip python3-dev</span><br></pre></td></tr></table></figure><p>  安装<code>tensorflow</code></p><blockquote><p>若下载失败，手动打开下面网页下载<code>.whl</code>包</p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># For Python 2.7</span><br><span class="line">$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp27-none-linux_armv7l.whl</span><br><span class="line">$ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl</span><br><span class="line"></span><br><span class="line"># For Python 3.4</span><br><span class="line">$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl</span><br><span class="line">$ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl</span><br></pre></td></tr></table></figure><p>  卸载，重装mock</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># For Python 2.7</span><br><span class="line">$ sudo pip uninstall mock</span><br><span class="line">$ sudo pip install mock</span><br><span class="line"></span><br><span class="line"># For Python 3.3+</span><br><span class="line">$ sudo pip3 uninstall mock</span><br><span class="line">$ sudo pip3 install mock</span><br></pre></td></tr></table></figure><p>  安装的版本<code>tensorflow v1.1.0</code>没有<code>models</code>，因为1.0版本以后models就被<code>Sam Abrahams</code>独立出来了，例如<code>classify_image.py</code>就在<code>models/tutorials/image/imagenet/</code>里</p><blockquote><p><a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">tensorflow/models</a></p></blockquote></li></ul><h2 id="其余"><a href="#其余" class="headerlink" title="其余"></a>其余</h2><ol><li><p>输入法 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install fcitx fcitx-googlepinyin </span><br><span class="line">$ fcitx-module-cloudpinyin fcitx-sunpinyin</span><br></pre></td></tr></table></figure></li><li><p><code>git</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install git</span><br></pre></td></tr></table></figure><p>配置<code>git</code>和<code>ssh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name &quot;Louis Hsu&quot;</span><br><span class="line">$ git config --global user.email is.louishsu@foxmail.com</span><br><span class="line"></span><br><span class="line">$ ssh-keygen -t rsa -C &quot;is.louishsu@foxmail.com&quot;</span><br><span class="line">$ cat ~/.ssh/id_rsa.pub  # 添加到github</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Underfitting &amp; Overfitting</title>
      <link href="/2018/10/26/Underfitting-Overfitting/"/>
      <url>/2018/10/26/Underfitting-Overfitting/</url>
      
        <content type="html"><![CDATA[<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>放上一张非常经典的图，以下分别表示二分类模型中的欠拟合(underfit)、恰好(just right)、过拟合(overfit)，来自吴恩达课程笔记。<br><img src="/2018/10/26/Underfitting-Overfitting/underfit_justright_overfit.png" alt="underfit_justright_overfit"></p><ul><li>欠拟合的成因大多是模型不够复杂、拟合函数的能力不够；</li><li>过拟合成因是给定的数据集相对过于简单，使得模型在拟合函数时过分地考虑了噪声等不必要的数据间的关联，或者说相对于给定数据集，模型过于复杂、拟合能力过强。</li></ul><h1 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h1><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>可通过学习曲线<code>(Learning curve)</code>进行欠拟合与过拟合的判别。</p><p>学习曲线就是通过画出<strong>不同训练集大小</strong>时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。</p><h2 id="绘制"><a href="#绘制" class="headerlink" title="绘制"></a>绘制</h2><p>横轴为训练样本的数量，纵轴为损失或其他<a href="">评估准则</a>。<br><code>sklearn</code>中学习曲线绘制例程如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.model_selection import learning_curve</span><br><span class="line">from sklearn.model_selection import ShuffleSplit</span><br><span class="line"></span><br><span class="line">digits = load_digits(); X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)</span><br><span class="line">estimator = GaussianNB()</span><br><span class="line">train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(&quot;Learning Curves (Naive Bayes)&quot;)</span><br><span class="line">plt.xlabel(&quot;Training examples&quot;)</span><br><span class="line">plt.ylabel(&quot;Score&quot;)</span><br><span class="line"></span><br><span class="line">train_scores_mean = np.mean(train_scores, axis=1)</span><br><span class="line">train_scores_std = np.std(train_scores, axis=1)</span><br><span class="line">test_scores_mean = np.mean(test_scores, axis=1)</span><br><span class="line">test_scores_std = np.std(test_scores, axis=1)</span><br><span class="line"></span><br><span class="line">plt.fill_between(train_sizes, </span><br><span class="line">                train_scores_mean - train_scores_std,</span><br><span class="line">                train_scores_mean + train_scores_std,</span><br><span class="line">                alpha=0.1, color=&quot;r&quot;)</span><br><span class="line">plt.fill_between(train_sizes,</span><br><span class="line">                test_scores_mean - test_scores_std,</span><br><span class="line">                test_scores_mean + test_scores_std,</span><br><span class="line">                alpha=0.1, color=&quot;g&quot;)</span><br><span class="line">plt.plot(train_sizes, train_scores_mean, &apos;o-&apos;, color=&quot;r&quot;, label=&quot;Training score&quot;)</span><br><span class="line">plt.plot(train_sizes, test_scores_mean,  &apos;o-&apos;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;)</span><br><span class="line"></span><br><span class="line">plt.grid(); plt.legend(loc=&quot;best&quot;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/2018/10/26/Underfitting-Overfitting/learning_curve_nb.png" alt="learning_curve_nb"></p><h2 id="判别"><a href="#判别" class="headerlink" title="判别"></a>判别</h2><ul><li><strong>欠拟合</strong>，即高偏差<code>(high bias)</code>，训练集和测试集的误差收敛但却很高；</li><li><strong>过拟合</strong>，即高方差<code>(high variance)</code>，训练集和测试集的误差之间有大的差距。</li></ul><p><img src="/2018/10/26/Underfitting-Overfitting/learning_curve.png" alt="learning_curve"></p><h1 id="欠拟合解决方法"><a href="#欠拟合解决方法" class="headerlink" title="欠拟合解决方法"></a>欠拟合解决方法</h1><ul><li>增加迭代次数继续训练</li><li>增加模型复杂度</li><li>增加特征</li><li>减少正则化程度</li><li>采用Boosting等集成方法</li></ul><p>此时增加数据集并不能改善欠拟合问题。</p><h1 id="过拟合解决方法"><a href="#过拟合解决方法" class="headerlink" title="过拟合解决方法"></a>过拟合解决方法</h1><ul><li>提前停止训练</li><li>获取更多样本或数据扩增<ul><li>重采样</li><li>上采样</li><li>增加随机噪声</li><li><code>GAN</code></li><li>图像数据的空间变换（平移旋转镜像）</li><li>尺度变换（缩放裁剪）</li><li>颜色变换</li><li>改变分辨率</li><li>对比度</li><li>亮度</li></ul></li><li>降低模型复杂度</li><li>减少特征</li><li>增加正则化程度</li><li>神经网络可采用<code>Dropout</code></li><li>多模型投票方法</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Cross Validation &amp; Hyperparameter</title>
      <link href="/2018/10/26/Cross-Validation-Hyperparameter/"/>
      <url>/2018/10/26/Cross-Validation-Hyperparameter/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉验证与超参数选择"><a href="#交叉验证与超参数选择" class="headerlink" title="交叉验证与超参数选择"></a>交叉验证与超参数选择</h1><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>以下简称交叉验证<code>(Cross Validation)</code>为<code>CV</code>.<code>CV</code>是用来验证分类器的性能一种统计分析方法,基本思想是把在某种意义下将原始数据<code>(dataset)</code>进行分组,一部分做为训练集<code>(train set)</code>,另一部分做为验证集<code>(validation set)</code>,首先用训练集对分类器进行训练,在利用验证集来测试训练得到的模型<code>(model)</code>,以此来做为评价分类器的性能指标。</p><h3 id="交叉验证的几种方法"><a href="#交叉验证的几种方法" class="headerlink" title="交叉验证的几种方法"></a>交叉验证的几种方法</h3><ul><li><p>k折交叉验证(K-fold)</p><ol><li>将全部训练集$S$分成$k$个不相交的子集，假设$S$中的训练样例个数为$m$，则每个子集中有$(\frac{m}{k})$个训练样例，相应子集称作$\{s_1, s_2, …, s_k\}$；</li><li>每次从分好的子集中，拿出$1$个作为测试集，其他$k-1$个作为训练集；</li><li>在$k-1$个训练集上训练出学习器模型，将模型放到测试集上，得到分类率；</li><li>计算k次求得的分类率平均值，作为该模型或者假设函数的真实分类率<br><img src="/2018/10/26/Cross-Validation-Hyperparameter/k-fold.jpg" alt="k-fold"></li></ol></li><li><p>留一法交叉验证(Leave One Out - LOO)<br>  假设有$N$个样本，将每个样本作为测试样本，其他$(N-1)$个样本作为训练样本。这样得到$N$个分类器，$N$个测试结果。用这$N$个结果的平均值衡量模型的性能。</p></li><li><p>留P法交叉验证(Leave P Out - LPO)<br>  将$P$个样本作为测试样本，其他$(N-P)$个样本作为训练样本。这样得到$\left(\begin{matrix}</p><pre><code>  P \\ N</code></pre><p>  \end{matrix}\right)$个训练测试对。当$P＞1$时，测试集会发生重叠。当$P=1$时，变成$LOO$。<br>  <img src="/2018/10/26/Cross-Validation-Hyperparameter/LPO.jpg" alt="LPO"></p></li></ul><h3 id="scikit-learn中的交叉验证"><a href="#scikit-learn中的交叉验证" class="headerlink" title="scikit-learn中的交叉验证"></a><code>scikit-learn</code>中的交叉验证</h3><p><img src="/2018/10/26/Cross-Validation-Hyperparameter/cross_validation_sklearn.png" alt="cross_validation_sklearn"></p><ul><li><p>K-fold</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn.model_selection import KFold</span><br><span class="line">&gt;&gt;&gt; X = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]</span><br><span class="line">&gt;&gt;&gt; kf = KFold(n_splits=2)</span><br><span class="line">&gt;&gt;&gt; for train, test in kf.split(X):</span><br><span class="line">... print(&quot;%s %s&quot; % (train, test))</span><br><span class="line">[2 3] [0 1]</span><br><span class="line">[0 1] [2 3]</span><br></pre></td></tr></table></figure></li><li><p>Leave One Out (LOO)</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.model_selection import LeaveOneOut</span><br><span class="line">&gt;&gt;&gt; X = [1, 2, 3, 4]</span><br><span class="line">&gt;&gt;&gt; loo = LeaveOneOut()</span><br><span class="line">&gt;&gt;&gt; for train, test in loo.split(X):</span><br><span class="line">... print(&quot;%s %s&quot; % (train, test))</span><br><span class="line">[1 2 3] [0]</span><br><span class="line">[0 2 3] [1]</span><br><span class="line">[0 1 3] [2]</span><br><span class="line">[0 1 2] [3]</span><br></pre></td></tr></table></figure></li><li><p>Leave P Out (LPO)</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.model_selection import LeavePOut</span><br><span class="line">&gt;&gt;&gt; X = np.ones(4)</span><br><span class="line">&gt;&gt;&gt; lpo = LeavePOut(p=2)</span><br><span class="line">&gt;&gt;&gt; for train, test in lpo.split(X):</span><br><span class="line">... print(&quot;%s %s&quot; % (train, test))</span><br><span class="line">[2 3] [0 1]</span><br><span class="line">[1 3] [0 2]</span><br><span class="line">[1 2] [0 3]</span><br><span class="line">[0 3] [1 2]</span><br><span class="line">[0 2] [1 3]</span><br><span class="line">[0 1] [2 3]</span><br></pre></td></tr></table></figure></li></ul><h2 id="使用交叉验证调整超参数"><a href="#使用交叉验证调整超参数" class="headerlink" title="使用交叉验证调整超参数"></a>使用交叉验证调整超参数</h2><p>超参数的定义：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。<br>超参数例如</p><ul><li>模型（<code>SVM</code>，<code>Softmax</code>，<code>Multi-layer Neural Network</code>,…)；</li><li>迭代算法（<code>Adam</code>, <code>SGD</code>, …)(不同的迭代算法还有各种不同的超参数，如<code>beta1</code>,<code>beta2</code>等等，但常见的做法是使用默认值，不进行调参）；</li><li>学习率（<code>learning rate</code>)；</li><li>正则化方程的选择(<code>L0</code>,<code>L1</code>,<code>L2</code>)，正则化系数；</li><li><code>dropout</code>的概率</li><li>…</li></ul><h3 id="确定调节范围"><a href="#确定调节范围" class="headerlink" title="确定调节范围"></a>确定调节范围</h3><p>超参数的种类多，调节范围大，需要先进行简单的测试确定调参范围。</p><ul><li><p>模型选择<br>  模型的选择很大程度上取决于具体的实际问题，但必须通过几项基本测试。 </p><ul><li>可以通过第一个epoch的loss，观察模型能否无BUG运行，注意此过程需要设置正则项系数为0，因为正则项引入的loss难以估算。 </li><li>模型必须可以对于小数据集过拟合，否则应该尝试其他或者更复杂的模型。</li><li><p>若训练集与验证集loss均较大，则应该尝试其他或者更复杂的模型。</p><blockquote><p>模型选择的方法为：</p><ol><li>使用训练集训练出 10 个模型</li><li>用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li><li>选取代价函数值最小的模型</li><li>用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）<p align="right"> —— Andrew Ng, Stanford University </p></li></ol></blockquote></li></ul></li><li><p>学习率</p><ul><li>loss基本不变：学习率过低 </li><li>loss波动明显或者溢出：学习率过高 </li></ul></li><li><p>正则项系数</p><ul><li>val_acc与acc相差较大：正则项系数过小 </li><li>loss逐渐增大：正则项系数过大 </li></ul></li></ul><h3 id="超参数的确定"><a href="#超参数的确定" class="headerlink" title="超参数的确定"></a>超参数的确定</h3><ul><li><p>先粗调，再细调<br> 先通过数量少，间距大的粗调确定细调的大致范围。然后在小范围内部进行间距小，数量大的细调。</p></li><li><p>尝试在对数空间内进行调节<br>  即在对数空间内部随机生成测试参数，而不是在原空间生成，通常用于学习率以及正则项系数等的调节。出发点是该超参数的指数项对于模型的结果影响更显著；而同阶的数据之间即便原域相差较大，对于模型结果的影响反而不如不同阶的数据差距大。</p></li><li><p>超参数搜索<br>  随机搜索参数值，而不是网格搜索。</p></li></ul><h3 id="超参数搜索"><a href="#超参数搜索" class="headerlink" title="超参数搜索"></a>超参数搜索</h3><p><code>scikit-learn</code>提供超参数搜索方法，可参考官方文档</p><ul><li>网格搜索<br>  <a href="http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#exhaustive-grid-search" target="_blank" rel="noopener">3.2.1. Exhaustive Grid Search</a><br>  调用例程如下  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from time import time</span><br><span class="line">from scipy.stats import randint as sp_randint</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">from sklearn.model_selection import RandomizedSearchCV</span><br><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line"># get some data</span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"># build a classifier</span><br><span class="line">clf = RandomForestClassifier(n_estimators=20)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Utility function to report best scores</span><br><span class="line">def report(results, n_top=3):</span><br><span class="line">    for i in range(1, n_top + 1):</span><br><span class="line">        candidates = np.flatnonzero(results[&apos;rank_test_score&apos;] == i)</span><br><span class="line">        for candidate in candidates:</span><br><span class="line">            print(&quot;Model with rank: &#123;0&#125;&quot;.format(i))</span><br><span class="line">            print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format(</span><br><span class="line">                results[&apos;mean_test_score&apos;][candidate],</span><br><span class="line">                results[&apos;std_test_score&apos;][candidate]))</span><br><span class="line">            print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&apos;params&apos;][candidate]))</span><br><span class="line">            print(&quot;&quot;)</span><br><span class="line"></span><br><span class="line"># use a full grid over all parameters</span><br><span class="line">param_grid = &#123;&quot;max_depth&quot;: [3, None],</span><br><span class="line">            &quot;max_features&quot;: [1, 3, 10],</span><br><span class="line">            &quot;min_samples_split&quot;: [2, 3, 10],</span><br><span class="line">            &quot;min_samples_leaf&quot;: [1, 3, 10],</span><br><span class="line">            &quot;bootstrap&quot;: [True, False],</span><br><span class="line">            &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125;</span><br><span class="line"></span><br><span class="line"># run grid search</span><br><span class="line">grid_search = GridSearchCV(clf, param_grid=param_grid)</span><br><span class="line">start = time()</span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(&quot;GridSearchCV took %.2f seconds for %d candidate parameter settings.&quot;</span><br><span class="line">    % (time() - start, len(grid_search.cv_results_[&apos;params&apos;])))</span><br><span class="line">report(grid_search.cv_results_)</span><br></pre></td></tr></table></figure></li></ul><ul><li>随机搜索<br>  <a href="http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#randomized-parameter-optimization" target="_blank" rel="noopener">3.2.2. Randomized Parameter Optimization</a><br>  调用例程如下  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from time import time</span><br><span class="line">from scipy.stats import randint as sp_randint</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import GridSearchCV</span><br><span class="line">from sklearn.model_selection import RandomizedSearchCV</span><br><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line"># get some data</span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"># build a classifier</span><br><span class="line">clf = RandomForestClassifier(n_estimators=20)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Utility function to report best scores</span><br><span class="line">def report(results, n_top=3):</span><br><span class="line">    for i in range(1, n_top + 1):</span><br><span class="line">        candidates = np.flatnonzero(results[&apos;rank_test_score&apos;] == i)</span><br><span class="line">        for candidate in candidates:</span><br><span class="line">            print(&quot;Model with rank: &#123;0&#125;&quot;.format(i))</span><br><span class="line">            print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format(</span><br><span class="line">                results[&apos;mean_test_score&apos;][candidate],</span><br><span class="line">                results[&apos;std_test_score&apos;][candidate]))</span><br><span class="line">            print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&apos;params&apos;][candidate]))</span><br><span class="line">            print(&quot;&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># specify parameters and distributions to sample from</span><br><span class="line">param_dist = &#123;&quot;max_depth&quot;: [3, None],</span><br><span class="line">            &quot;max_features&quot;: sp_randint(1, 11),</span><br><span class="line">            &quot;min_samples_split&quot;: sp_randint(2, 11),</span><br><span class="line">            &quot;min_samples_leaf&quot;: sp_randint(1, 11),</span><br><span class="line">            &quot;bootstrap&quot;: [True, False],</span><br><span class="line">            &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125;</span><br><span class="line"></span><br><span class="line"># run randomized search</span><br><span class="line">n_iter_search = 20</span><br><span class="line">random_search = RandomizedSearchCV(clf, param_distributions=param_dist,</span><br><span class="line">                                n_iter=n_iter_search)</span><br><span class="line"></span><br><span class="line">start = time()</span><br><span class="line">random_search.fit(X, y)</span><br><span class="line">print(&quot;RandomizedSearchCV took %.2f seconds for %d candidates&quot;</span><br><span class="line">    &quot; parameter settings.&quot; % ((time() - start), n_iter_search))</span><br><span class="line">report(random_search.cv_results_)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spam Classification</title>
      <link href="/2018/10/26/Spam-Classification/"/>
      <url>/2018/10/26/Spam-Classification/</url>
      
        <content type="html"><![CDATA[<blockquote><p>踩坑？？？全部给我踩平！！！</p></blockquote><p>来自<a href="https://www.lintcode.com/ai/spam-message-classification/overview" target="_blank" rel="noopener">LintCode垃圾短信分类</a><br><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/tree/master/spam%20or%20ham" target="_blank" rel="noopener">@Github: spam or ham</a></p><h1 id="垒代码"><a href="#垒代码" class="headerlink" title="垒代码"></a>垒代码</h1><h2 id="预处理及向量化"><a href="#预处理及向量化" class="headerlink" title="预处理及向量化"></a>预处理及向量化</h2><p>观察各文本后，发现各文本中包含的单词多种多样，包含标点、数字等，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</span><br><span class="line">- XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http://wap. </span><br><span class="line">- 07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder.</span><br></pre></td></tr></table></figure></p><p>且按空格分词后，部分单词中仍包含<code>whitespace</code>，故选择的预处理方案是，<strong>去除分词后文本中的标点、数字、空格等，并将单词中字母全部转为小写</strong>。</p><blockquote><p>中文分词可采用<code>jieba</code>(街霸？)</p></blockquote><p>预处理后，按当前的文本内容建立字典，并统计各样本的词数向量，详细代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">class Words2Vector():</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    建立字典，将输入的词列表转换为向量，表示各词出现的次数</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.dict = None</span><br><span class="line">        self.n_word = None</span><br><span class="line">    def fit_transform(self, words):</span><br><span class="line">        self.fit(words)</span><br><span class="line">        return self.transform(words)</span><br><span class="line">    def fit(self, words):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;list[list[str]]&#125; words</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        words = _flatten(words)                                                 # 展开为1维列表</span><br><span class="line">        words = self.filt(words)                                                # 滤除空格、数字、标点</span><br><span class="line"></span><br><span class="line">        self.word = list(set(words))                                            # 去重</span><br><span class="line">        self.n_word = len(set(words))                                           # 统计词的个数</span><br><span class="line">        self.dict = dict(zip(self.word, [_ for _ in range(self.n_word)]))       # 各词在字典中的位置</span><br><span class="line">    def transform(self, words):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;list[list[str]]&#125; words</span><br><span class="line">        @return &#123;ndarray&#125; retarray: vector</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        retarray = np.zeros(shape=(len(words), self.n_word))                    # 返回的词数向量</span><br><span class="line">        for i in range(len(words)):</span><br><span class="line">            words[i] = self.filt(words[i])                                      # 滤除空格、数字、标点</span><br><span class="line">        for i in range(len(words)):</span><br><span class="line">            for w in words[i]:</span><br><span class="line">                if w in self.word:                                              # 是否在训练集生成的字典中</span><br><span class="line">                    retarray[i, self.dict[w]] += 1                              # 查询字典，找到对应特征的下标</span><br><span class="line">        return retarray</span><br><span class="line">    def filt(self, flattenWords):</span><br><span class="line">        retWords = []</span><br><span class="line">        en_stops = set(stopwords.words(&apos;english&apos;))                              # 停用词列表</span><br><span class="line">        for word in flattenWords:</span><br><span class="line">            word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.whitespace))     # 去除空白</span><br><span class="line">            word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.punctuation))    # 去除标点</span><br><span class="line">            word = word.translate(str.maketrans(&apos;&apos;, &apos;&apos;, string.digits))         # 去除数字</span><br><span class="line">            if word not in en_stops and (len(word) &gt; 1):                        # 删除停用词，并除去长度小于等于2的词</span><br><span class="line">                retWords.append(word.lower())</span><br><span class="line">        return retWords</span><br></pre></td></tr></table></figure></p><h2 id="TF-IDF方法"><a href="#TF-IDF方法" class="headerlink" title="TF-IDF方法"></a>TF-IDF方法</h2><p>由词数向量可计算词频，但只用词频忽略了各文本在不同文档中的重要程度，关于<code>TF-IDF</code>，在<a href="https://louishsu.xyz/2018/10/25/TF-IDF/" target="_blank" rel="noopener">另一篇博文</a>中详细说明。</p><p>由于剔除了停用词等，部分向量不包含任何内容，即词数向量为$\vec{0}$，这时计算词频和单位化时，会出现<code>nan</code>的运算结果，故只对非空向量进行计算。</p><p>训练后需要保存的是<code>IDF</code>向量，<code>TF</code>向量在新样本输入后重新计算，故无需保存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class TfidfVectorizer():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.idf = None</span><br><span class="line">    def fit_transform(self, num_vec):</span><br><span class="line">        self.fit(num_vec)</span><br><span class="line">        return self.transform(num_vec)</span><br><span class="line">    def fit(self, num_vec):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        num_vec[num_vec&gt;0] = 1</span><br><span class="line">        n_doc = num_vec.shape[0]</span><br><span class="line">        n_term = np.sum(num_vec, axis=0)    # 各词出现过的文档次数</span><br><span class="line">        self.idf = np.log((n_doc + 1) / (n_term + 1)) + 1</span><br><span class="line">        return self.idf</span><br><span class="line">    def transform(self, num_vec):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 求解词频向量，由于部分向量为空，故下句会出现问题</span><br><span class="line">        # tf = num_vec / np.sum(num_vec, axis=1).reshape(-1, 1) =&gt; nan</span><br><span class="line">        # 解决方法：只对非空向量进行词频计算</span><br><span class="line">        tf = np.zeros(shape=num_vec.shape)</span><br><span class="line">        n_terms = np.sum(num_vec, axis=1); idx = (n_terms!=0)</span><br><span class="line"></span><br><span class="line">        tf[idx] = num_vec[idx] / n_terms[idx].reshape(-1, 1)            # 计算词频，只对非空向量进行</span><br><span class="line">        </span><br><span class="line">        tfidf = tf * self.idf</span><br><span class="line">        tfidf[idx] /= np.linalg.norm(tfidf, axis=1)[idx].reshape(-1, 1) # 单位化，只对非空向量进行</span><br><span class="line">        </span><br><span class="line">        return tfidf</span><br></pre></td></tr></table></figure><h2 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h2><p>各文本向量化后，就可通过机器学习算法进行模型的训练和预测，这里采用的是贝叶斯决策的方法，需要注意的有以下几点</p><ul><li>似然函数$p(x|c_k)$与<a href="https://louishsu.xyz/2018/10/18/Bayes-Decision/" target="_blank" rel="noopener">贝叶斯决策</a>文中例不同，这里宜采用高斯分布作为分布模型；</li><li><p>按朴素贝叶斯计算$p(x|c_k)$，但注意此处不能将各维特征单独训练$1$维高斯分布模型，然后计算预测样本似然函数值时进行累乘，如下</p><script type="math/tex; mode=display">p(x|c_k) = \prod_{j=1}^{N_feature} p(x_j|c_k)</script><p>因为特征维度特别高，各个特征单独用$1$维高斯分布描述，累乘计算会下溢，故这里采用多元高斯分布</p><script type="math/tex; mode=display">p(x|c_k) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}} · e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}</script><ul><li>且经主成分分析后，各维度间线性相关性降低，故假定<script type="math/tex; mode=display">\Sigma_k = diag\{\sigma_{k1}, ..., \sigma_{kn}\}</script></li><li><p>但分母$(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}$在计算时不稳定，且各特征标准差大小相差无几，故这里假定</p><script type="math/tex; mode=display">\Sigma_k = I</script></li><li><p>最终简化后的似然函数计算方法为</p><script type="math/tex; mode=display">p(x|c_k) =  e^{-\frac{1}{2} (x - \mu_k)^T (x - \mu_k)}</script></li></ul></li></ul><h3 id="贝叶斯决策模型训练"><a href="#贝叶斯决策模型训练" class="headerlink" title="贝叶斯决策模型训练"></a>贝叶斯决策模型训练</h3><p>基于上述假设，只需训练多元高斯分布的各维均值$\mu_j$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def fit(self, labels, text):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param &#123;ndarray&#125; labels: shape(N_samples, ), labels[i] \in &#123;0, 1&#125;</span><br><span class="line">    @param &#123;list[list[str]]&#125; words</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">    labels = self.encodeLabel(labels); words = self.text2words(self.clean(text))</span><br><span class="line"></span><br><span class="line">    vecwords = self.numvectorizer.fit_transform(words)              # 向量化</span><br><span class="line">    vecwords = self.tfidfvectorizer.fit_transform(vecwords)         # tfidf, shape(N_samples, N_features)</span><br><span class="line"></span><br><span class="line">    isnotEmpty = (np.sum(vecwords, axis=1)!=0)                      # 去掉空的样本</span><br><span class="line">    vecwords = vecwords[isnotEmpty]; labels = labels[isnotEmpty]</span><br><span class="line"></span><br><span class="line">    # vecwords = self.reduce_dim.fit_transform(vecwords)              # 降维，计算量太大</span><br><span class="line">    self.n_features = vecwords.shape[1]</span><br><span class="line"></span><br><span class="line">    labels = OneHotEncoder().fit_transform(labels.reshape((-1, 1))).toarray()</span><br><span class="line">        self.priori = np.mean(labels, axis=0)                           # 先验概率</span><br><span class="line"></span><br><span class="line">    self.likelihood_mu = np.zeros(shape=(2, vecwords.shape[1]))    # 设似然函数p(x|c)为高斯分布</span><br><span class="line">    for i in range(2):</span><br><span class="line">        vec = vecwords[labels[:, i]==1]</span><br><span class="line">        self.likelihood_mu[i] = np.mean(vec, axis=0)</span><br></pre></td></tr></table></figure><h3 id="贝叶斯决策模型预测"><a href="#贝叶斯决策模型预测" class="headerlink" title="贝叶斯决策模型预测"></a>贝叶斯决策模型预测</h3><p>决策函数为</p><script type="math/tex; mode=display">if　p(x|c_i)P(c_i) > p(x|c_j)P(c_j),　then　x \in c_i</script><p>但实际效果显示，等先验概率$P(c_j)$结果更好$(???)$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def multigaussian(self, x, mu):</span><br><span class="line">    &quot;&quot;&quot; 简化</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x = x - mu</span><br><span class="line">    a = np.exp(-0.5 * x.T.dot(x))</span><br><span class="line">    return a</span><br><span class="line">def predict(self, text):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    @param &#123;list[list[str]]&#125; words</span><br><span class="line">    @note:</span><br><span class="line">                      p(x|c)P(c)</span><br><span class="line">            P(c|x) = ------------</span><br><span class="line">                         p(x)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    pred_porba = np.ones(shape=(len(self.clean(text)), 2))      </span><br><span class="line">        </span><br><span class="line">    words = self.text2words(text)</span><br><span class="line">    vecwords = self.tfidfvectorizer.transform(</span><br><span class="line">                                self.numvectorizer.transform(words))    # 向量化</span><br><span class="line"></span><br><span class="line">    for i in range(vecwords.shape[0]):</span><br><span class="line">        for c in range(2):</span><br><span class="line">            # pred_porba[i, c] = self.priori[c] * self.multigaussian(vecwords[i], self.likelihood_mu[c])</span><br><span class="line">            pred_porba[i, c] = self.multigaussian(vecwords[i], self.likelihood_mu[c])</span><br><span class="line"></span><br><span class="line">    pred = np.argmax(pred_porba, axis=1)</span><br><span class="line">    return self.decodeLabel(pred)</span><br></pre></td></tr></table></figure><h1 id="调包"><a href="#调包" class="headerlink" title="调包"></a>调包</h1><p>主要用到了<code>scikit-learn</code>机器学习包以下几个功能</p><ul><li><code>sklearn.feature_extraction.text.TfidfVectorizer()</code></li><li><code>sklearn.decomposition.PCA()</code></li><li><code>sklearn.naive_bayes.BernoulliNB()</code></li></ul><p>最终准确率在$97\%$左右，代码比较简单，不进行说明。</p><blockquote><p>采用<code>sklearn.linear_model import.LogisticRegressionCV()</code>效果更佳</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def main():</span><br><span class="line">    trainfile = &quot;./data/train.csv&quot;</span><br><span class="line">    testfile = &quot;./data/test.csv&quot;</span><br><span class="line">    </span><br><span class="line">    # 读取原始数据</span><br><span class="line">    data_train = pd.read_csv(trainfile, names=[&apos;Label&apos;, &apos;Text&apos;])</span><br><span class="line">    txt_train  = list(data_train[&apos;Text&apos;])[1: ]; label_train = list(data_train[&apos;Label&apos;])[1: ]</span><br><span class="line">    drop(txt_train)                                             # 删除数字和标点</span><br><span class="line">    txt_test   = list(pd.read_csv(testfile, names=[&apos;Text&apos;])[&apos;Text&apos;])[1: ]</span><br><span class="line">    drop(txt_test)                                              # 删除数字和标点</span><br><span class="line"></span><br><span class="line">    # 训练</span><br><span class="line">    vectorizer = TfidfVectorizer(stop_words=&apos;english&apos;)          # 删除英文停用词</span><br><span class="line">    vec_train = vectorizer.fit_transform(txt_train).toarray()   # 提取文本特征向量</span><br><span class="line">    # reduce_dim = PCA(n_components = 4096)</span><br><span class="line">    # vec_train = reduce_dim.fit_transform(vec_train)</span><br><span class="line">    estimator = BernoulliNB()</span><br><span class="line">    estimator.fit(vec_train, label_train)                       # 训练朴素贝叶斯模型</span><br><span class="line"></span><br><span class="line">    # 测试</span><br><span class="line">    label_train_pred = estimator.predict(vec_train)</span><br><span class="line">    acc = np.mean((label_train_pred==label_train).astype(&apos;float&apos;))</span><br><span class="line">    </span><br><span class="line">    # 预测</span><br><span class="line">    vec_test = vectorizer.transform(txt_test).toarray()</span><br><span class="line">    # vec_test = reduce_dim.transform(vec_test)</span><br><span class="line">    label_test_pred = estimator.predict(vec_test)</span><br><span class="line">    with open(&apos;./data/sampleSubmission.txt&apos;, &apos;w&apos;) as f:</span><br><span class="line">        for i in range(label_test_pred.shape[0]):</span><br><span class="line">            f.write(label_test_pred[i] + &apos;\n&apos;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TF-IDF</title>
      <link href="/2018/10/25/TF-IDF/"/>
      <url>/2018/10/25/TF-IDF/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>正在做<a href="https://www.lintcode.com/" target="_blank" rel="noopener">LintCode</a>上的垃圾邮件分类，使用<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">朴素贝叶斯</a>方法解决，涉及到文本特征的提取。<br>TF-IDF（词频-逆文档频率）算法是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p><h1 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h1><h2 id="词频-TF"><a href="#词频-TF" class="headerlink" title="词频(TF)"></a>词频(TF)</h2><p><code>Term Frequency</code>，就是某个关键字出现的频率，具体来讲，就是词库中的<strong>某个词</strong>在<strong>当前文章</strong>中出现的频率。那么我们可以写出它的计算公式：</p><script type="math/tex; mode=display">TF_{ij} = \frac{n_{ij}}{\sum_k n_{i, k}}</script><p>其中，$n_{ij}$表示关键词$j$在文档$i$中的出现次数。</p><p>单纯使用TF来评估关键词的重要性忽略了常用词的干扰。常用词就是指那些文章中大量用到的，但是不能反映文章性质的那种词，比如：因为、所以、因此等等的连词，在英文文章里就体现为and、the、of等等的词。这些词往往拥有较高的TF，所以仅仅使用TF来考察一个词的关键性，是不够的。</p><h2 id="逆文档频率-IDF"><a href="#逆文档频率-IDF" class="headerlink" title="逆文档频率(IDF)"></a>逆文档频率(IDF)</h2><p><code>Inverse Document Frequency</code>，文档频率就是一个词在整个文库词典中出现的频率，逆文档频率用下式计算</p><script type="math/tex; mode=display">IDF_j = \log \frac{|D|}{|D_j| + 1}</script><p>其中，$|D|$表示总的文档数目，$|D_j|$表示关键词$j$出现过的文档数目</p><p><code>scikit-learn</code>内为</p><script type="math/tex; mode=display">IDF_j = \log \frac{|D| + 1}{|D_j| + 1} + 1</script><p><img src="/2018/10/25/TF-IDF/sklearn.jpg" alt="sklearn_tfidf"></p><h2 id="词频-逆文档频率-TF-IDF"><a href="#词频-逆文档频率-TF-IDF" class="headerlink" title="词频-逆文档频率(TF-IDF)"></a>词频-逆文档频率(TF-IDF)</h2><script type="math/tex; mode=display">TF-IDF_{i} = TF_i × IDF</script><h1 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h1><p>例如有如下$3$个文本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：My dog ate my homework.</span><br><span class="line">文本2：My cat ate the sandwich.</span><br><span class="line">文本3：A dolphin ate the homework.</span><br></pre></td></tr></table></figure></p><p>提取字典，一般需要处理大小写、去除停用词<code>a</code>，处理结果为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ate, cat, dog, dolphin, homework, my, sandwich, the</span><br></pre></td></tr></table></figure></p><p>故各个文本的词数向量为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：[1, 0, 1, 0, 1, 2, 0, 0]</span><br><span class="line">文本2：[1, 1, 0, 0, 0, 1, 1, 1]</span><br><span class="line">文本3：[1, 0, 0, 1, 1, 0, 0, 1]</span><br></pre></td></tr></table></figure></p><p>各个文本的词频向量(TF)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ]</span><br><span class="line">文本2：[0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ]</span><br><span class="line">文本3：[0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]</span><br></pre></td></tr></table></figure></p><p>各词出现过的文档次数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3, 1, 1, 1, 2, 2, 1, 2]</span><br></pre></td></tr></table></figure></p><p>总文档数为$3$，各词的逆文档频率(IDF)向量</p><blockquote><p>这里使用<code>scikit-learn</code>内的方法求解</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207,  1.28768207, 1.69314718, 1.28768207]</span><br></pre></td></tr></table></figure><p>故各文档的TF-IDF向量为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文本1：</span><br><span class="line">[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ]</span><br><span class="line">文本2：</span><br><span class="line">[0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641]</span><br><span class="line">文本3：</span><br><span class="line">[0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]</span><br></pre></td></tr></table></figure></p><p>经单位化后，有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文本1：</span><br><span class="line">[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ]</span><br><span class="line">文本2：</span><br><span class="line">[0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178]</span><br><span class="line">文本3：</span><br><span class="line">[0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; vec_num = np.array([</span><br><span class="line">[1, 0, 1, 0, 1, 2, 0, 0],</span><br><span class="line">[1, 1, 0, 0, 0, 1, 1, 1],</span><br><span class="line">[1, 0, 0, 1, 1, 0, 0, 1]</span><br><span class="line">])</span><br><span class="line">&gt;&gt;&gt; vec_tf = vec_num / np.sum(vec_num, axis=1).reshape(-1, 1)</span><br><span class="line">&gt;&gt;&gt; vec_tf</span><br><span class="line">array([[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ],</span><br><span class="line">       [0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ],</span><br><span class="line">       [0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; vec_num[vec_num&gt;0] = 1</span><br><span class="line">&gt;&gt;&gt; n_showup = np.sum(vec_num, axis=0)</span><br><span class="line">&gt;&gt;&gt; n_showup</span><br><span class="line">array([3, 1, 1, 1, 2, 2, 1, 2])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; d = 3</span><br><span class="line">&gt;&gt;&gt; vec_idf = np.log((d + 1) / (n_showup + 1)) + 1</span><br><span class="line">&gt;&gt;&gt; vec_idf</span><br><span class="line">array([1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; vec_tfidf = vec_tf * vec_idf</span><br><span class="line">&gt;&gt;&gt; vec_tfidf</span><br><span class="line">array([[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ],</span><br><span class="line">       [0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641],</span><br><span class="line">       [0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; vec_tfidf = vec_tfidf / np.linalg.norm(vec_tfidf, axis=1).reshape((-1, 1))</span><br><span class="line">&gt;&gt;&gt; vec_tfidf</span><br><span class="line">array([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805, 0.73861611, 0.        , 0.        ],</span><br><span class="line">       [0.31544415, 0.53409337, 0.        , 0.        , 0.        , 0.40619178, 0.53409337, 0.40619178],</span><br><span class="line">       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 , 0.        , 0.        , 0.4804584 ]])</span><br></pre></td></tr></table></figure><h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><p>使用<code>scikit-learn</code>机器学习包计算结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">&gt;&gt;&gt; vectorizer = TfidfVectorizer()</span><br><span class="line">&gt;&gt;&gt; text = [</span><br><span class="line">&quot;My dog ate my homework&quot;,</span><br><span class="line">&quot;My cat ate the sandwich&quot;,</span><br><span class="line">&quot;A dolphin ate the homework&quot;]</span><br><span class="line">&gt;&gt;&gt; vectorizer.fit_transform(text).toarray()</span><br><span class="line">array([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ],</span><br><span class="line">       [0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178],</span><br><span class="line">       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]])</span><br><span class="line">&gt;&gt;&gt; vectorizer.get_feature_names()</span><br><span class="line">[&apos;ate&apos;, &apos;cat&apos;, &apos;dog&apos;, &apos;dolphin&apos;, &apos;homework&apos;, &apos;my&apos;, &apos;sandwich&apos;, &apos;the&apos;]</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Practice </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVD</title>
      <link href="/2018/10/23/SVD/"/>
      <url>/2018/10/23/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>奇异值分解<code>Singular Value Decomposition</code>是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="从特征值分解-EVD-讲起"><a href="#从特征值分解-EVD-讲起" class="headerlink" title="从特征值分解(EVD)讲起"></a>从特征值分解(EVD)讲起</h2><p>我们知道对于一个$n$阶方阵$A_{n×n}$，有</p><script type="math/tex; mode=display">A\alpha_i = \lambda_i \alpha_i　i = 1, ..., n</script><p>取</p><script type="math/tex; mode=display">P = \left[\alpha_1, \alpha_2, ..., \alpha_n\right]</script><p>有下式成立</p><script type="math/tex; mode=display">AP = P\Lambda</script><p>其中</p><script type="math/tex; mode=display">\Lambda = \left[        \begin{matrix}            \lambda_1 & & \\            & ... & \\            & & \lambda_n \\        \end{matrix}\right]</script><blockquote><p>特征值一般从大到小排列</p></blockquote><p>利用该式可将方阵$A_{n×n}$化作对角阵$\Lambda_{n×n}$</p><script type="math/tex; mode=display">\Lambda = P^{-1}AP</script><p>或者</p><script type="math/tex; mode=display">A = P \Lambda P^{-1} = \sum_{i=1}^n \lambda_i (P_{,i})(P_{,i})^{-1}</script><blockquote><p>“$_{i}$”表示第$i$行，“$_{,i}$”表示第$i$列</p></blockquote><p>这样我们就可以理解为，矩阵$A$是由$n$个$n$阶矩阵$P_{,i}P^{-1}_{i}$加权组成，特征值$\lambda_i$即为权重。</p><blockquote><p>以上为个人理解，不妥之处可以指出。</p></blockquote><h2 id="奇异值分解-SVD"><a href="#奇异值分解-SVD" class="headerlink" title="奇异值分解(SVD)"></a>奇异值分解(SVD)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>对于长方阵$A_{m×n}$，不能进行特征值分解，可进行如下分解</p><script type="math/tex; mode=display">A_{m×n} = U_{m×m} \Sigma_{m×n} V_{n×n}^T</script><p>其中$U \in \mathbb{R}^{m×m}, V \in \mathbb{R}^{n×n}$，均为正交矩阵。矩阵$\Sigma_{m×n}$如下</p><ul><li><p>对于$m&gt;n$</p><script type="math/tex; mode=display">  \Sigma_{m×n} = \left[          \begin{matrix}              S_{n×n} \\              --- \\              O_{(m-n)×n}          \end{matrix}  \right]</script></li><li><p>对于$m&lt;n$</p><script type="math/tex; mode=display">  \Sigma_{m×n} = \left[          \begin{matrix}              S_{m×m} & | & O_{m×(n-m)}          \end{matrix}  \right]</script></li></ul><p>矩阵$S_{n×n}$为对角阵，对角元素从大到小排列</p><script type="math/tex; mode=display">S_{n×n} = \left[    \begin{matrix}        \sigma_1 & & \\         & ... & \\         & & \sigma_n\\    \end{matrix}\right]</script><p>直观表示<code>SVD</code>分解如下<br><img src="/2018/10/23/SVD/直观表示SVD.jpg" alt="直观表示SVD"></p><p>当取$r&lt;n$时，有部分奇异值分解，可用于降维</p><script type="math/tex; mode=display">A_{m×n} = U_{m×r} \Sigma_{r×r} V_{r×n}^T</script><h3 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h3><blockquote><p>以下仅考虑$m&gt;n$的情况</p></blockquote><ol><li><p>令矩阵$A^T$与$A$相乘，有</p><script type="math/tex; mode=display"> A^TA = (U \Sigma V^T)^T (U \Sigma V^T)</script><script type="math/tex; mode=display"> = V \Sigma^T U^T U \Sigma V^T</script><script type="math/tex; mode=display"> A^TA = V \Sigma^T \Sigma V^T</script><blockquote><p>矩阵$U$为正交阵，即满足$U^TU=I$</p></blockquote><p> 其中</p><script type="math/tex; mode=display"> \Sigma^T \Sigma =          \left[             \begin{matrix}                 S^T_{n×n} & | & O^T_{n×(m-n)}             \end{matrix}         \right]         \left[             \begin{matrix}                 S_{n×n} \\                 --- \\                 O_{(m-n)×n}             \end{matrix}         \right]</script><script type="math/tex; mode=display"> = S_{n×n}^2  = \left[     \begin{matrix}         \sigma_1^2 & & \\         & ... & \\         & & \sigma_n^2\\     \end{matrix} \right]</script><p> 则</p><script type="math/tex; mode=display"> A^T A = V S^2  V^T</script><p> 即矩阵$A^T A$相似对角化为$S^2$，对角元素$\sigma_i^2$与矩阵$V$的列向量$v_i(i=1, …, n)$为矩阵$A^T A$的特征对。</p><p> 那么对矩阵$A^T A$进行特征值分解，有</p><script type="math/tex; mode=display"> (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i</script><p> 则</p><script type="math/tex; mode=display"> v_i = \alpha^{(1)}_i　\sigma_i = \sqrt{\lambda^{(1)}_i}</script><blockquote><p>注：对于二次型$x^T (A^T A) x$</p><script type="math/tex; mode=display">x^T (A^T A) x = (Ax)^T(Ax) \geq 0</script><p>故矩阵$A^T A$半正定，$\sigma_i = \sqrt{\lambda_i}$有解</p></blockquote></li></ol><ol><li><p>同理，令矩阵$A$与$A^T$相乘，可证得</p><script type="math/tex; mode=display"> A A^T = U \Sigma \Sigma^T U^T</script><p> 其中</p><script type="math/tex; mode=display"> \Sigma \Sigma^T =          \left[             \begin{matrix}                 S_{n×n} \\                 --- \\                 O_{(m-n)×n}             \end{matrix}         \right]         \left[             \begin{matrix}                 S^T_{n×n} & | & O^T_{n×(m-n)}             \end{matrix}         \right]</script><script type="math/tex; mode=display"> = \left[     \begin{matrix}         S^2_{n×n} & O_{n×(m-n)} \\         O_{(m-n)×n} & O_{(m-n)×(m-n)}     \end{matrix} \right]</script><p> 即矩阵$A A^T$相似对角化，对角元素$\sigma_i^2$与矩阵$U$的列向量$u_i(i=1, …, m)$为矩阵$A A^T$的特征对。</p><p> 对矩阵$A A^T$进行特征值分解，有</p><script type="math/tex; mode=display"> (A^T A) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i</script><p> 则</p><script type="math/tex; mode=display"> u_i = \alpha^{(2)}_i　\sigma_i = \sqrt{\lambda^{(2)}_i}</script><blockquote><p>同理可证得$A A^T$半正定，略。</p></blockquote></li></ol><p>一般来说，为减少计算量，计算奇异值分解只进行一次特征值分解，如对于矩阵$X_{m×n}(m&gt;n)$，选取$n$阶矩阵$X^T X$进行特征值分解计算$v_i$，计算$u_i$方法下面介绍。</p><p>根据前面推导，我们有特征值分解</p><script type="math/tex; mode=display">(A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i</script><script type="math/tex; mode=display">(A A^T) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i</script><p>其中$\lambda^{(1)}_i = \lambda^{(2)}_i = \sigma_i^2$，$v_i = \alpha^{(1)}_i$，$u_i = \alpha^{(2)}_i$，即</p><script type="math/tex; mode=display">A^T A v_i = \sigma_i^2 v_i \tag{1}</script><script type="math/tex; mode=display">A A^T u_i = \sigma_i^2 u_i \tag{2}</script><p>$(1)$式左右乘$A$，有</p><script type="math/tex; mode=display">A A^T A v_i = \sigma_i^2 A v_i</script><p>发现什么？这是另一个特征值分解的表达式！</p><script type="math/tex; mode=display">(A A^T) (A v_i) = \sigma_i^2 (A v_i)</script><p>故</p><script type="math/tex; mode=display">u_i \propto A v_i　或　u_i = k · A v_i \tag{3}</script><p>现在求解系数$k$，根据定义</p><script type="math/tex; mode=display">A = U \Sigma V^T　\Rightarrow　AV = U \Sigma</script><p>则</p><script type="math/tex; mode=display">A v_i = \sigma_i u_i　\Rightarrow　u_i = \frac{1}{\sigma_i} A v_i</script><p>或者</p><script type="math/tex; mode=display">U = A V \Sigma^{-1}</script><blockquote><p>注：只能求前$n$个$u_i$，之后的需要列写方程求解</p></blockquote><h1 id="举栗"><a href="#举栗" class="headerlink" title="举栗"></a>举栗</h1><p>将矩阵$A$进行分解</p><script type="math/tex; mode=display">A = \left[    \begin{matrix}        0 & 1 \\        1 & 1 \\        1 & 0    \end{matrix}\right]</script><p>为减少计算量，取$A^T A$计算</p><script type="math/tex; mode=display">A^T A = \left[    \begin{matrix}        2 & 1 \\        1 & 2     \end{matrix}\right]</script><p>特征值分解，有</p><script type="math/tex; mode=display">A\left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]= \left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]\left[    \begin{matrix}        3 &  \\          & 1    \end{matrix} \right]</script><p>故</p><script type="math/tex; mode=display">\Sigma = \left[    \begin{matrix}        \sqrt{3} &  \\          & 1    \end{matrix} \right]　V = \left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]</script><script type="math/tex; mode=display">U = A V \Sigma^{-1} = \left[    \begin{matrix}        \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\        \frac{2}{\sqrt{6}} & 0 \\        \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}}    \end{matrix} \right]</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; A = np.array([</span><br><span class="line">[0, 1], [1, 1], [1, 0]</span><br><span class="line">])</span><br><span class="line">&gt;&gt;&gt; ATA = A.T.dot(A)</span><br><span class="line">&gt;&gt;&gt; eigval, eigvec= np.linalg.eig(ATA)</span><br><span class="line">&gt;&gt;&gt; V = eigvec.copy()</span><br><span class="line">&gt;&gt;&gt; S = np.diag(np.sqrt(eigval))</span><br><span class="line">&gt;&gt;&gt; U = A.dot(V).dot(np.linalg.inv(S))</span><br><span class="line">&gt;&gt;&gt; U</span><br><span class="line">array([[ 0.40824829,  0.70710678],</span><br><span class="line">       [ 0.81649658,  0.        ],</span><br><span class="line">       [ 0.40824829, -0.70710678]])</span><br><span class="line">&gt;&gt;&gt; S</span><br><span class="line">array([[1.73205081, 0.        ],</span><br><span class="line">       [0.        , 1.        ]])</span><br><span class="line">&gt;&gt;&gt; V</span><br><span class="line">array([[ 0.70710678, -0.70710678],</span><br><span class="line">       [ 0.70710678,  0.70710678]])</span><br><span class="line">&gt;&gt;&gt; # 验证</span><br><span class="line">&gt;&gt;&gt; U.dot(S).dot(V.T)</span><br><span class="line">array([[-2.23711432e-17,  1.00000000e+00],</span><br><span class="line">       [ 1.00000000e+00,  1.00000000e+00],</span><br><span class="line">       [ 1.00000000e+00, -2.23711432e-17]])</span><br></pre></td></tr></table></figure><h1 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h1><p>展开表达式，取$r \leq n$时，</p><script type="math/tex; mode=display">A = U_{m×r} \Sigma_{r×r} V_{r×n}^T = \sum_{i=1}^r \sigma_i (U_{,i}) (V_{,i})^T</script><p>就得到与<code>PCA</code>相同的结论，矩阵$A$可由$r$个$m×n$的矩阵$(U_{,i}) (V_{,i})^T$加权组成。一般来说，前$10\%$甚至$1\%$的奇异值就占了全部奇异值之和的$99\%$，极大地保留了信息，而大大减少了存储空间。</p><blockquote><p>以图片为例，若原有<code>24bit</code>图片，其大小为<code>(1024, 768)</code>，则不计图片信息，仅仅数据共占<code>1024×768×3 B</code>，或<code>2.25 MB</code>。用奇异值分解进行压缩，保留$60\%$的奇异值，可达到几乎无损的程度，此时需要保存向量矩阵$U_{1024×60}$，$V_{60×768}$以及$60$个奇异值，以浮点数<code>float32</code>存储，一共占<code>420 KB</code>即可。</p><script type="math/tex; mode=display">(1024 × 60 + 60 × 768 + 60) × 4 / 2^{10} = 420.23</script><p>说句题外话，存储量的压缩必然以计算量的增大为代价，相反亦然，所以需要协调好<code>RAM</code>与<code>ROM</code>容量，考虑计算机的计算速度。换句话说，空间和时间上必然是互补的，哲学的味道hhhh。</p></blockquote><h1 id="分解结果的信息保留"><a href="#分解结果的信息保留" class="headerlink" title="分解结果的信息保留"></a>分解结果的信息保留</h1><p>分解后各样本间的欧式距离与角度信息应不变，给出证明如下<br>设有$m$组$n$维样本样本</p><script type="math/tex; mode=display">X_{n×m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]</script><p>经奇异值分解，有</p><script type="math/tex; mode=display">X_{n×m} = U_{n×r} \Sigma_{r×r} V_{r×m}^T</script><p>记</p><script type="math/tex; mode=display">Z_{r×m} = \Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]</script><p>有</p><script type="math/tex; mode=display">X = U Z</script><ul><li><p>欧式距离</p><script type="math/tex; mode=display">  || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2</script><script type="math/tex; mode=display">  = \left[ U (Z^{(i)} - Z^{(j)}) \right]^T \left[ U (Z^{(i)} - Z^{(j)}) \right]</script><script type="math/tex; mode=display">  = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)})</script><script type="math/tex; mode=display">  = || Z^{(i)} - Z^{(j)} ||_2^2</script><p>  即</p><script type="math/tex; mode=display">  || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2</script></li><li><p>角度信息</p><script type="math/tex; mode=display">  \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2}</script><script type="math/tex; mode=display">  = \frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2}</script><script type="math/tex; mode=display">  = \frac{(UZ^{(i)})^T(UZ^{(j)})}{\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \sqrt{(UZ^{(j)})^T(UZ^{(j)})}}</script><script type="math/tex; mode=display">  = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}</script><p>  即</p><script type="math/tex; mode=display">  \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} =   \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}</script></li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p15_svd.py" target="_blank" rel="noopener">@Github: Code of SVD</a><br>对图片进行了分解<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">class SVD():</span><br><span class="line">    &quot;&quot;&quot; Singular Value Decomposition</span><br><span class="line">    Attributes:</span><br><span class="line">        m &#123;int&#125;</span><br><span class="line">        n &#123;int&#125;</span><br><span class="line">        r &#123;int&#125;: if r == -1, then r = n</span><br><span class="line">        isTrains &#123;bool&#125;: isTrains = True if input.shape[0] &lt; input.shape[1]</span><br><span class="line">        U &#123;ndarray(m, r)&#125;</span><br><span class="line">        S &#123;ndarray(r, )&#125;</span><br><span class="line">        V &#123;ndarray(n, r)&#125;</span><br><span class="line">    Notes:</span><br><span class="line">        - Transpose input matrix if m &lt; n, and m, n := n, m</span><br><span class="line">        - Reassign r if eigvals contains zero</span><br><span class="line">        - Singular values are stored in a 1-dim array `S`</span><br><span class="line">        - X&apos; = U S V^T</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, r=-1):</span><br><span class="line">        self.m = None</span><br><span class="line">        self.n = None</span><br><span class="line">        self.r = r</span><br><span class="line">        self.isTrans = False</span><br><span class="line">        self.U = None</span><br><span class="line">        self.S = None</span><br><span class="line">        self.V = None</span><br><span class="line">    def fit(self, X):</span><br><span class="line">        &quot;&quot;&quot; calculate components</span><br><span class="line">        Notes:</span><br><span class="line">            - Transpose input matrix if m &lt; n, and m, n := n, m</span><br><span class="line">            - reassign self.r if eigvals contains zero</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        (self.m, self.n) = X.shape</span><br><span class="line">        if self.m &lt; self.n:</span><br><span class="line">            X = X.T</span><br><span class="line">            self.m, self.n = self.n, self.m</span><br><span class="line">            self.isTrans = True</span><br><span class="line">        self.r = self.n if (self.r == -1) else self.r</span><br><span class="line"></span><br><span class="line">        XTX = X.T.dot(X)</span><br><span class="line">        eigval, eigvec = np.linalg.eig(X.T.dot(X))</span><br><span class="line">        eigval, eigvec = np.real(eigval), np.real(eigvec)</span><br><span class="line">        </span><br><span class="line">        self.S = np.sqrt(np.clip(eigval, 0, float(&apos;inf&apos;)))</span><br><span class="line">        self.S = self.S[self.S &gt; 0]</span><br><span class="line">        self.r = min(self.r, self.S.shape[0])               # reassign self.r</span><br><span class="line">        order = np.argsort(eigval)[::-1][: self.r]          # sort eigval from large to small</span><br><span class="line">        eigval = eigval[order]; eigvec = eigvec[:, order]</span><br><span class="line">        self.V = eigvec.copy()</span><br><span class="line">        self.U = X.dot(self.V).dot(</span><br><span class="line">                    np.linalg.inv(np.diag(self.S)))</span><br><span class="line">        return self.U, self.S, self.V</span><br><span class="line">    def compose(self, r=-1):</span><br><span class="line">        &quot;&quot;&quot; merge first r components</span><br><span class="line">        Parameters:</span><br><span class="line">            r &#123;int&#125;: if r==-1, merge all components</span><br><span class="line">        Returns:</span><br><span class="line">            X &#123;ndarray(m, n)&#125;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if r == -1:</span><br><span class="line">            X = self.U.dot(np.diag(self.S)).dot(self.V.T)</span><br><span class="line">            X = X.T if self.isTrans else X</span><br><span class="line">        else:</span><br><span class="line">            (m, n) = (self.n, self.m) if self.isTrans else (self.m, self.n)</span><br><span class="line">            X = np.zeros(shape=(m, n))</span><br><span class="line">            for i in range(r):</span><br><span class="line">                X += self.__getitem__(i)</span><br><span class="line">        return X</span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        &quot;&quot;&quot; get a component</span><br><span class="line">        Parameters:</span><br><span class="line">            index &#123;int&#125;: range from (0, self.r)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        u = self.U[:, idx]</span><br><span class="line">        v = self.V[:, idx]</span><br><span class="line">        s = self.S[idx]</span><br><span class="line">        x = s * u.reshape(self.m, 1).\</span><br><span class="line">                    dot(v.reshape(1, self.n))</span><br><span class="line">        x = x.T if self.isTrans else x</span><br><span class="line">        return x</span><br><span class="line">    def showComponets(self, r=-1):</span><br><span class="line">        &quot;&quot;&quot; display components</span><br><span class="line">        Notes:</span><br><span class="line">            - Resize components&apos; shape into (40, 30)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        m, n = self.m, self.n</span><br><span class="line">        r = self.r if r==-1 else r</span><br><span class="line">        n_images = 10; m_images = r // n_images + 1</span><br><span class="line">        m_size, n_size = 40, 30</span><br><span class="line">        showfig = np.zeros(shape=(m_images*m_size, n_images*n_size))</span><br><span class="line">        for i in range(r):</span><br><span class="line">            m_pos = i // n_images</span><br><span class="line">            n_pos = i %  n_images</span><br><span class="line">            component = self.__getitem__(i)</span><br><span class="line">            component = component.T if self.isTrans else component</span><br><span class="line">            component = cv2.resize(component, (30, 40))</span><br><span class="line">            showfig[m_pos*m_size: (m_pos+1)*m_size, n_pos*n_size: (n_pos+1)*n_size] = component</span><br><span class="line">        plt.figure(&apos;components&apos;)</span><br><span class="line">        plt.imshow(showfig)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure></p><p>用上面的代码进行实验<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 读取一张图片</span><br><span class="line">X = load_images()[0].reshape((32, 32))</span><br><span class="line">showmat2d(X)</span><br><span class="line"># 对图片进行奇异值分解</span><br><span class="line">decomposer = SVD(r=-1)</span><br><span class="line">decomposer.fit(X)</span><br><span class="line"># 显示一下分量</span><br><span class="line">decomposer.showComponets(r=-1)</span><br><span class="line"># 将全部分量组合，并显示</span><br><span class="line">X_ = decomposer.compose(r=-1)</span><br><span class="line">showmat2d(X_)</span><br><span class="line"># 将前5个分量组合，并显示</span><br><span class="line">X_ = decomposer.compose(r=5)</span><br><span class="line">showmat2d(X_)</span><br></pre></td></tr></table></figure></p><ul><li><p>载入原图如下<br><img src="/2018/10/23/SVD/source.png" alt="source"></p></li><li><p>分量显示如下<br><img src="/2018/10/23/SVD/components.png" alt="components"></p></li><li><p>组合分量显示如下</p><ul><li>组合全部<br>  <img src="/2018/10/23/SVD/merge_all.png" alt="merge_all"></li><li>组合前5个分量<br>  <img src="/2018/10/23/SVD/merge_5.png" alt="merge_5"></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>删除停用词</title>
      <link href="/2018/10/23/%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D/"/>
      <url>/2018/10/23/%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.yiibai.com/python_text_processing/python_remove_stopwords.html" target="_blank" rel="noopener">删除停用词 - Python文本处理教程™</a></p></blockquote><p>停用词是对句子没有多大意义的英语单词。 在不牺牲句子含义的情况下，可以安全地忽略它们。 例如，the, he, have等等的单词已经在名为语料库的语料库中捕获了这些单词。</p><h1 id="下载语料库"><a href="#下载语料库" class="headerlink" title="下载语料库"></a>下载语料库</h1><ul><li><p>安装<code>nltk</code>模块</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install nltk</span><br></pre></td></tr></table></figure></li><li><p>下载语料库</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line">nltk.download(&apos;stopwords&apos;)</span><br></pre></td></tr></table></figure></li></ul><h1 id="使用库料库"><a href="#使用库料库" class="headerlink" title="使用库料库"></a>使用库料库</h1><ul><li><p>验证停用词</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line">&gt;&gt;&gt; stopwords.words(&apos;english&apos;)</span><br><span class="line">[&apos;i&apos;, &apos;me&apos;, &apos;my&apos;, &apos;myself&apos;, &apos;we&apos;, &apos;our&apos;, &apos;ours&apos;, &apos;ourselves&apos;, </span><br><span class="line">&apos;you&apos;, &quot;you&apos;re&quot;, &quot;you&apos;ve&quot;, &quot;you&apos;ll&quot;, &quot;you&apos;d&quot;, &apos;your&apos;, &apos;yours&apos;, </span><br><span class="line">&apos;yourself&apos;, &apos;yourselves&apos;, &apos;he&apos;, &apos;him&apos;, &apos;his&apos;, &apos;himself&apos;, &apos;she&apos;,</span><br><span class="line">&quot;she&apos;s&quot;, &apos;her&apos;, &apos;hers&apos;, &apos;herself&apos;, &apos;it&apos;, &quot;it&apos;s&quot;, &apos;its&apos;, </span><br><span class="line">&apos;itself&apos;, &apos;they&apos;, &apos;them&apos;, &apos;their&apos;, &apos;theirs&apos;, &apos;themselves&apos;, </span><br><span class="line">&apos;what&apos;, &apos;which&apos;, &apos;who&apos;, &apos;whom&apos;, &apos;this&apos;, &apos;that&apos;, &quot;that&apos;ll&quot;, </span><br><span class="line">&apos;these&apos;, &apos;those&apos;, &apos;am&apos;, &apos;is&apos;, &apos;are&apos;, &apos;was&apos;, &apos;were&apos;, &apos;be&apos;, </span><br><span class="line">&apos;been&apos;, &apos;being&apos;, &apos;have&apos;, &apos;has&apos;, &apos;had&apos;, &apos;having&apos;, &apos;do&apos;, &apos;does&apos;, </span><br><span class="line">&apos;did&apos;, &apos;doing&apos;, &apos;a&apos;, &apos;an&apos;, &apos;the&apos;, &apos;and&apos;, &apos;but&apos;, &apos;if&apos;, &apos;or&apos;, </span><br><span class="line">&apos;because&apos;, &apos;as&apos;, &apos;until&apos;, &apos;while&apos;, &apos;of&apos;, &apos;at&apos;, &apos;by&apos;, &apos;for&apos;, </span><br><span class="line">&apos;with&apos;, &apos;about&apos;, &apos;against&apos;, &apos;between&apos;, &apos;into&apos;, &apos;through&apos;, </span><br><span class="line">&apos;during&apos;, &apos;before&apos;, &apos;after&apos;, &apos;above&apos;, &apos;below&apos;, &apos;to&apos;, &apos;from&apos;, </span><br><span class="line">&apos;up&apos;, &apos;down&apos;, &apos;in&apos;, &apos;out&apos;, &apos;on&apos;, &apos;off&apos;, &apos;over&apos;, &apos;under&apos;, </span><br><span class="line">&apos;again&apos;, &apos;further&apos;, &apos;then&apos;, &apos;once&apos;, &apos;here&apos;, &apos;there&apos;, &apos;when&apos;,</span><br><span class="line">&apos;where&apos;, &apos;why&apos;, &apos;how&apos;, &apos;all&apos;, &apos;any&apos;, &apos;both&apos;, &apos;each&apos;, &apos;few&apos;, </span><br><span class="line">&apos;more&apos;, &apos;most&apos;, &apos;other&apos;, &apos;some&apos;, &apos;such&apos;, &apos;no&apos;, &apos;nor&apos;, &apos;not&apos;, </span><br><span class="line">&apos;only&apos;, &apos;own&apos;, &apos;same&apos;, &apos;so&apos;, &apos;than&apos;, &apos;too&apos;, &apos;very&apos;, &apos;s&apos;, &apos;t&apos;, </span><br><span class="line">&apos;can&apos;, &apos;will&apos;, &apos;just&apos;, &apos;don&apos;, &quot;don&apos;t&quot;, &apos;should&apos;, &quot;should&apos;ve&quot;, </span><br><span class="line">&apos;now&apos;, &apos;d&apos;, &apos;ll&apos;, &apos;m&apos;, &apos;o&apos;, &apos;re&apos;, &apos;ve&apos;, &apos;y&apos;, &apos;ain&apos;, &apos;aren&apos;, </span><br><span class="line">&quot;aren&apos;t&quot;, &apos;couldn&apos;, &quot;couldn&apos;t&quot;, &apos;didn&apos;, &quot;didn&apos;t&quot;, &apos;doesn&apos;, </span><br><span class="line">&quot;doesn&apos;t&quot;, &apos;hadn&apos;, &quot;hadn&apos;t&quot;, &apos;hasn&apos;, &quot;hasn&apos;t&quot;, &apos;haven&apos;, </span><br><span class="line">&quot;haven&apos;t&quot;, &apos;isn&apos;, &quot;isn&apos;t&quot;, &apos;ma&apos;, &apos;mightn&apos;, &quot;mightn&apos;t&quot;, &apos;mustn&apos;,</span><br><span class="line">&quot;mustn&apos;t&quot;, &apos;needn&apos;, &quot;needn&apos;t&quot;, &apos;shan&apos;, &quot;shan&apos;t&quot;, &apos;shouldn&apos;, </span><br><span class="line">&quot;shouldn&apos;t&quot;, &apos;wasn&apos;, &quot;wasn&apos;t&quot;, &apos;weren&apos;, &quot;weren&apos;t&quot;, &apos;won&apos;, </span><br><span class="line">&quot;won&apos;t&quot;, &apos;wouldn&apos;, &quot;wouldn&apos;t&quot;]</span><br></pre></td></tr></table></figure><p>  除了英语之外，具有这些停用词的各种语言如下。</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; stopwords.fileids()</span><br><span class="line">[&apos;arabic&apos;, &apos;azerbaijani&apos;, &apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, </span><br><span class="line">&apos;french&apos;, &apos;german&apos;, &apos;greek&apos;, &apos;hungarian&apos;, &apos;indonesian&apos;, &apos;italian&apos;, </span><br><span class="line">&apos;kazakh&apos;, &apos;nepali&apos;, &apos;norwegian&apos;, &apos;portuguese&apos;, &apos;romanian&apos;, &apos;russian&apos;,</span><br><span class="line">&apos;spanish&apos;, &apos;swedish&apos;, &apos;turkish&apos;]</span><br></pre></td></tr></table></figure></li><li><p>示例<br>  从单词列表中删除停用词。</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line">&gt;&gt;&gt; en_stops = set(stopwords.words(&apos;english&apos;))</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; all_words = [&apos;There&apos;, &apos;is&apos;, &apos;a&apos;, &apos;tree&apos;,&apos;near&apos;,&apos;the&apos;,&apos;river&apos;]</span><br><span class="line">&gt;&gt;&gt; for word in all_words:</span><br><span class="line">if word not in en_stops:</span><br><span class="line">print(word)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">There</span><br><span class="line">tree</span><br><span class="line">near</span><br><span class="line">river</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PCA</title>
      <link href="/2018/10/22/PCA/"/>
      <url>/2018/10/22/PCA/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><code>PCA</code>全称<code>Principal Component Analysis</code>，即主成分分析，是一种常用的数据降维方法。它可以通过线性变换将原始数据变换为一组各维度线性无关的表示，以此来提取数据的主要线性分量。</p><h1 id="向量的投影"><a href="#向量的投影" class="headerlink" title="向量的投影"></a>向量的投影</h1><p>现有两个任意不共线向量$\vec{u}, \vec{v}$，将$\vec{u}$投射到$\vec{v}$上<br><img src="/2018/10/22/PCA/向量投影.jpg" alt="向量投影"></p><p>投影后，可以得到两个正交向量</p><script type="math/tex; mode=display">\vec{u}' · (\vec{u} - \vec{u}') = 0</script><p>我们设</p><script type="math/tex; mode=display">\vec{u}' = \mu \vec{v} \tag{1}</script><p>代入后有</p><script type="math/tex; mode=display">\mu \vec{v} · (\vec{u} - \mu \vec{v}) = 0</script><p>引入矩阵运算，即</p><script type="math/tex; mode=display">(\mu v)^T (u - \mu v) = 0</script><p>有</p><script type="math/tex; mode=display">v^T u = \mu v^T v</script><p>则得到$u’$以$v$为基向量的坐标</p><script type="math/tex; mode=display">\mu  = (v^T v)^{-1} v^T u \tag{2}</script><p>所以得到</p><script type="math/tex; mode=display">u' = v (v^T v)^{-1} v^T u \tag{*}</script><blockquote><ul><li><p>坐标变换求解投影向量：$u’$可视作$u$经坐标变换$u’ = P u$得到，所以</p><script type="math/tex; mode=display">P = v (v^T v)^{-1} v^T</script></li><li><p>推广至多个向量的投影，即得到</p><script type="math/tex; mode=display">P = X (X^T X)^{-1} X^T</script><p>这与<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归</a>中得到的结论一致。</p></li></ul></blockquote><p>实际上</p><script type="math/tex; mode=display">u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T u</script><p>记单位向量$\frac{v}{||v||}$为$v_0$，得到</p><script type="math/tex; mode=display">u' = v_0 v_0^T u</script><p>由几何关系，可以计算得投影后的长度为</p><script type="math/tex; mode=display">d = ||u|| \cos \theta = ||u|| \frac{v^T u}{||u||||v||}= v_0^T u</script><p>所以在向量投影中，$u^T v_0$表示以$v_0$为基向量的坐标。</p><h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><p>现在有$N$维数据集$D=\{x^{(1)}, x^{(2)}, …, x^{(M)}\}$，其中$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, …, x^{(i)}_N\right]^T$，各维特征$D_{j}$间存在线性相关性，利用主成分分析可使</p><ul><li>数据维度降低；</li><li>提取主成分，且各成分间不相关。</li></ul><blockquote><p>说明</p><ul><li>由于选取的特征轴是正交的，所以计算结果线性无关；</li><li>提取了方差较大的几个特征，为主要线性分量。</li></ul></blockquote><p>以二维空间中的数据$x^{(i)} = \left[\begin{matrix}<br>    x^{(i)}_1 \\ x^{(i)}_2<br>\end{matrix}\right]$为例，维度可降至一维，如下图所示。<br><img src="/2018/10/22/PCA/PCA动态图.gif" alt="PCA动态图"></p><p>主轴可有无穷多种选择，那么问题就是<strong>如何选取最优的主轴</strong>。先给出<code>PCA</code>的计算步骤。</p><h2 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h2><p>输入的$M$个$N$维样本，有样本矩阵</p><script type="math/tex; mode=display">X_{N×M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right]= \left[    \begin{matrix}        x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\        x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\        ... \\        x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\    \end{matrix}\right]</script><h3 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h3><ol><li><p>对每个维度(行)进行去均值化</p><script type="math/tex; mode=display">X_j := X_j - \mu_j</script><p> 其中$\mu_j = \overline{X_j}$，$j = 1, 2, …, N$</p></li><li><p>求各维度间的协方差矩阵$\Sigma_{N×N}$</p><script type="math/tex; mode=display">\Sigma_{ij} = Cov(x_i, x_j)</script><p> 或</p><script type="math/tex; mode=display"> \Sigma = \frac{1}{M} X X^T</script></li></ol><blockquote><p>注：</p><ol><li><script type="math/tex; mode=display">X X^T = \left[           \begin{matrix}     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\     ... &     ... &     ... &     ... \\     \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_N x^{(i)}_N \end{matrix}\right]</script><script type="math/tex; mode=display">= \sum_{i=1}^M \left[           \begin{matrix}         x^{(i)}_1 x^{(i)}_1 &          x^{(i)}_1 x^{(i)}_2 &         ... &         x^{(i)}_1 x^{(i)}_N \\         x^{(i)}_2 x^{(i)}_1 &          x^{(i)}_2 x^{(i)}_2 &         ... &         x^{(i)}_2 x^{(i)}_N \\         ... &         ... &         ... &         ... \\         x^{(i)}_N x^{(i)}_1 &          x^{(i)}_N x^{(i)}_2 &         ... &         x^{(i)}_N x^{(i)}_N \end{matrix}\right]</script><script type="math/tex; mode=display">= \sum_{i=1}^M x^{(i)} x^{(i)T}</script></li><li><p>协方差定义式</p><script type="math/tex; mode=display">   Cov(x,y)≝\frac{1}{n-1} ∑_{i=1}^n (x_i−\overline{x})^T(y_i−\overline{y})</script><p>其中$x=[x_1, x_2, …, x_n]^T, y=[y_1, y_2, …, y_n]^T$</p></li></ol></blockquote><ol><li>求协方差矩阵$\Sigma$的特征值$λ_i$及其对应特征向量$α_i$，$i=1, …, N$；</li><li><p>按照特征值从大到小排列特征对$(λ_i,α_i)$，选取$K$个最大特征值对应的特征向量作为降维后的主轴$ \beta_1, \beta_2, …, \beta_K $，其中$\beta_k$为单位向量</p><script type="math/tex; mode=display">\beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^T</script><p>记</p><script type="math/tex; mode=display">B_{N×K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]</script><p>$K$的选取方法有如下两种：</p><ul><li>指定选取$K$个主轴</li><li>保留$99\%$的方差<script type="math/tex; mode=display">\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99</script></li></ul></li></ol><ol><li><p>将样本点投射到$K$维坐标系上<br> 样本$X^{(i)}$投射到主成分轴$\beta_k$上，其坐标表示为向量，为</p><script type="math/tex; mode=display"> S^{(i)}_k = X^{(i)T}\beta_k</script><blockquote><p>注意此时的基座标为$\beta_k$，或者说$X’^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$</p></blockquote><p> 所有样本在主轴$\beta_k$上的投影坐标即</p><script type="math/tex; mode=display"> S = B^T X</script><p> 其中$S_{K×M}$，$B_{N×K}$，$X_{N×M}$</p></li></ol><blockquote><p>注：若取$K=N$，可重建数据，如下<br><img src="/2018/10/22/PCA/pca_restructure1.png" alt="pca_restructure1"><br><img src="/2018/10/22/PCA/pca_restructure2.png" alt="pca_restructure2"></p></blockquote><h3 id="复原"><a href="#复原" class="headerlink" title="复原"></a>复原</h3><p>第$5$步中，样本点向量$X^{(i)}$的主要分量投射到$K$个$N$维向量上，投影坐标为$S^{(i)}_k$，即</p><script type="math/tex; mode=display">X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_k</script><p>以上就是样本点的复原公式，矩阵形式即</p><script type="math/tex; mode=display">\hat{X} = BS</script><p>其中$\hat{X}_{N×M}$，$B_{N×K}$，$S_{K×M}$</p><p>考虑到已去均值化，故</p><script type="math/tex; mode=display">\hat{X}_j \approx \hat{X}_j + \mu_j</script><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><blockquote><p>投影向量的$2$范数最大，或者说，投影后的坐标平方和最大</p></blockquote><p>当所有样本$X$投射到第一主轴$\beta_1$上，其坐标为</p><script type="math/tex; mode=display">S_1 = X^T \beta_1</script><p>所有元素的平方和，或向量$S_1$的$2$范数为</p><script type="math/tex; mode=display">||S_1||_2^2 = S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}</script><p>即优化目标为</p><script type="math/tex; mode=display">\max ||S_1||_2^2</script><script type="math/tex; mode=display">s.t.　||\beta_1||_2^2 = 1</script><p>矩阵$C=XX^T$为对称矩阵，故可单位正交化</p><script type="math/tex; mode=display">C = W \Lambda W^T</script><script type="math/tex; mode=display">W = \left[\begin{matrix}    | & & |\\    w_1 & ... & w_M\\    | & & |\\\end{matrix}\right]　\Lambda = \left[\begin{matrix}    \lambda_1 &  & \\     & ... & \\     &  & \lambda_M\\\end{matrix}\right]</script><p>其中$\lambda_1 &gt; …&gt; \lambda_M$，$w_i(i=1,…,M)$为矩阵$C$的特征向量(单位向量，互相正交)</p><blockquote><p>实际上$R(C) \leq (n-1)$，即最多有$(n-1)$个特征值大于$0$。</p></blockquote><script type="math/tex; mode=display">||S_1||_2^2= \beta_1^T W \Lambda W^T \beta_1 \tag{2}</script><p>令$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$，可得</p><script type="math/tex; mode=display">||S_1||_2^2= \alpha_1^T \Lambda \alpha_1 \tag{3}</script><p>即</p><script type="math/tex; mode=display">||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \tag{4}</script><p>进一步</p><script type="math/tex; mode=display">\sum_{i=1}^M \lambda_i \alpha_{1i}^2\leq \lambda_1 \sum_{i=1}^M \alpha_{1i}^2 \tag{5}</script><p>且由于$\beta_1^T\beta_1 = 1$，故</p><script type="math/tex; mode=display">1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = \sum_{i=1}^M \alpha_{1i}^2</script><p>可得</p><script type="math/tex; mode=display">||S_1||_2^2= \sum_{i=1}^M \lambda_i \alpha_{1i}^2\leq \lambda_1  \tag{6}</script><p>为使$(6)$取等号，即达最大值，可使</p><script type="math/tex; mode=display">\begin{cases}    \alpha_{11} = 1 \\    \alpha_{12} = ... = \alpha_{1M} = 0\end{cases}</script><p>即令</p><script type="math/tex; mode=display">\beta_1 = W \alpha_1 = w_1</script><blockquote><p>$\alpha_1 = [1, 0, …, 0]^T$</p></blockquote><p>所以$\beta_1$对应矩阵$C=XX^T$的特征向量$w_1$，且有</p><script type="math/tex; mode=display">||S_1||_2^2 = \lambda_1</script><blockquote><p>或者第一主成分的证明也可以这样，建立优化目标</p><script type="math/tex; mode=display">\beta_1 = \arg \max　||S_1||_2^2</script><script type="math/tex; mode=display">s.t.　||\beta_1||_2^2 = 1</script><p>构造拉格朗日函数</p><script type="math/tex; mode=display">L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)</script><p>也即</p><script type="math/tex; mode=display">L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)</script><p>求其极值点</p><script type="math/tex; mode=display">▽_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0</script><p>有</p><script type="math/tex; mode=display">X X^T \beta_1 = \lambda_1 \beta_1</script><p>可见$\beta_1$即方阵$X X^T$的特征向量</p></blockquote><p>当我们希望用更多的主成分刻画数据，如已经求得主成分$\beta_1, …, \beta_{r-1}$，先需求解$\beta_r$，引入正交约束$\beta_r^T \beta_i = 0$，即目标函数为</p><script type="math/tex; mode=display">||S_r||_2^2 = \beta_r^T C \beta_r</script><script type="math/tex; mode=display">s.t.　\beta_r^T \beta_i = 0, i = 1, ..., r-1</script><script type="math/tex; mode=display">||\beta_r||_2^2 = 1</script><p>令$\beta_r = W \alpha_r$，则</p><script type="math/tex; mode=display">||S_r||_2^2= \alpha_r^T \Lambda \alpha_r= \sum_i \lambda_i \alpha_{ri}^2</script><p>而根据正交约束</p><script type="math/tex; mode=display">0 = \beta_r^T \beta_i = \alpha_r^T W^T w_i = \alpha_{ri},　i = 1, ..., r-1</script><blockquote><p>$ W^T w_i = \left[0, …, 1_i, …, 0\right]^T$</p></blockquote><p>所以</p><script type="math/tex; mode=display">||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{5}</script><p>又因为$\beta_r^T \beta_r = 1$(单位向量)，故</p><script type="math/tex; mode=display">\beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1</script><p>于是类似的，为使$(5)$取最大，取</p><script type="math/tex; mode=display">\begin{cases}    \alpha_{rr} = 1\\    \alpha_{ri} = 0,　i = 1, ..., M, i \neq r\end{cases}</script><blockquote><p>$\alpha_r = [0, …, 1_r, …, 0]$</p></blockquote><p>则此时</p><script type="math/tex; mode=display">\beta_r = W \alpha_r = w_r</script><p>且有</p><script type="math/tex; mode=display">||S_r||_2^2 = \lambda_r</script><p>证毕。</p><h2 id="白化-whitening"><a href="#白化-whitening" class="headerlink" title="白化(whitening)"></a>白化(whitening)</h2><p><code>whitening</code>的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。</p><p>数据的<code>whitening</code>必须满足两个条件：</p><ol><li>不同特征间相关性最小，接近$0$；</li><li>所有特征的方差相等（不一定为$1$）。</li></ol><p>常见的白化操作有<code>PCA whitening</code>和<code>ZCA whitening</code>。</p><blockquote><p><a href="http://deeplearning.stanford.edu/wiki/index.php/Whitening" target="_blank" rel="noopener">Whitening - Ufldl</a></p></blockquote><ul><li>PCA whitening<br>  <code>PCA whitening</code>指将数据$X$经过<code>PCA</code>降维为$S$后，可以看出$S$中每一维是独立的，满足<code>whitening</code>的第一个条件，这是只需要将$S$中的每一维都除以标准差就得到了每一维的方差为$1$，也就是说方差相等。<script type="math/tex; mode=display">  X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}}</script></li></ul><ul><li>ZCA whitening<br>  <code>ZCA whitening</code>是指数据$X$先经过<code>PCA</code>变换为$S$，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足<code>whtienning</code>的第一个条件，特征间相互独立。然后同样进行方差为$1$的操作，最后将得到的矩阵左乘一个特征向量矩阵$U$即可。<script type="math/tex; mode=display">  X_{ZCAwhite} = U · X_{PCAwhite}</script></li></ul><h1 id="Kernel-PCA"><a href="#Kernel-PCA" class="headerlink" title="Kernel PCA"></a>Kernel PCA</h1><p><code>Kernel PCA</code>的思想是在高维的特征空间中求解协方差矩阵</p><script type="math/tex; mode=display">\Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><p>其中$\Phi(X^{(i)})$表示将样本$i$映射到高维空间后中的向量，即</p><script type="math/tex; mode=display">\Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^T</script><p>其中$N’ &gt; N$，由于$\Phi(X^{(i)})$为隐式的，故设置核函数求解，记</p><script type="math/tex; mode=display">\kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><blockquote><p>关于核技巧，移步<a href="">非线性支持向量机</a></p></blockquote><p><img src="/2018/10/22/PCA/kernel_pca.jpg" alt="kernel_pca"></p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>可利用<code>PCA</code>与线性回归求解$3$维空间中平面的法向量</p><ol><li>利用<code>PCA</code>重建数据(不降维，此时为$3$维)，此时第$1, 2$主成分轴可张成所求平面，即该平面可表示为<script type="math/tex; mode=display"> \Pi = span \{ \beta_1, \beta_2 \}</script></li></ol><blockquote><p>就是说，第一、二主成分是这些点“拉伸”最大的方向 :-)，好懂不？</p></blockquote><ol><li><p>由<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">正交投影</a>可知，平面外一点$y$可通过最小二乘(线性回归)的方法投射到平面上，向量运算，不考虑偏置项，即</p><script type="math/tex; mode=display"> \hat{y}  = \theta_1 x_1 + \theta_2 x_2 \tag{*}</script><p> 其中$x_1, x_2$表示第一、第二主成分$\beta_1, \beta_2$，为$3$维向量</p><script type="math/tex; mode=display"> \hat{y} = \left[     \begin{matrix}         \hat{y_1} \\         \hat{y_2} \\         \hat{y_3} \\     \end{matrix} \right]　 x_i = \left[     \begin{matrix}         x_{i1} \\         x_{i2} \\         x_{i3} \\     \end{matrix} \right]</script><p> 可利用公式求解回归参数$\theta$</p><script type="math/tex; mode=display"> \theta = (X^TX+\lambda I)^{-1} X^T y</script><blockquote><p>注意：$X(n_samples, n_features)$，这里把$(x_{1j}, x_{2j}, y_{j})作为一组样本$</p></blockquote><p> 此时该参数表示在主轴上的坐标$(\theta_1, \theta_2)$，带回$(*))$即可解得$\hat{y}$</p><script type="math/tex; mode=display"> \hat{y}  = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*}</script><p> 通俗理解，一掌把$y$拍平在了平面$\Pi$上，变成了$\hat{y}$，但是哪有这么好拍。。。这个时候刺在掌心里一定有一个垂直的向量分量，即为该平面的法向量</p><script type="math/tex; mode=display"> \vec{n} = y - \hat{y}</script><p> <strong>也可使用粗暴一点的方法，直接将第三主成分作为法向量。</strong></p><blockquote><p>或者直接上投影公式：</p><script type="math/tex; mode=display">\hat{y} = Py</script><script type="math/tex; mode=display">　P = X (X^TX+\lambda I)^{-1} X^T</script></blockquote></li></ol><pre><code>![projection](/PCA/projection.jpg)&gt; 总体的运算流程如下&gt; - 利用所有样本点(近似平面)计算主成分，第一、二主成分张成平面$\Pi$；&gt; - 选出其中一个样本点，将平行于平面$\Pi$的成分投射到$\Pi$上；&gt; - 该样本点剩余分量即法向量；&gt; - 一般来说，取所有点法向量的均值。</code></pre><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-3-pca" target="_blank" rel="noopener">@Github: PCA</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">class PrincipalComponentAnalysis():</span><br><span class="line">    def __init__(self, n_component=-1):</span><br><span class="line">        self.n_component = n_component</span><br><span class="line">        self.meanVal = None</span><br><span class="line">        self.axis = None</span><br><span class="line">    def fit(self, X, prop=0.99):</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        the parameter &apos;prop&apos; is only for &apos;n_component = -1&apos;</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        # 第一步: 归一化</span><br><span class="line">        self.meanVal = np.mean(X, axis=0)                   # 训练样本每个特征上的的均值</span><br><span class="line">        X_normalized = (X - self.meanVal)                   # 归一化训练样本</span><br><span class="line">        # 第二步：计算协方差矩阵</span><br><span class="line">        # cov = X_normalized.T.dot(X_normalized)</span><br><span class="line">        cov = np.cov(X_normalized.T)                        # 协方差矩阵</span><br><span class="line">        eigVal, eigVec = np.linalg.eig(cov)                 # EVD</span><br><span class="line">        order = np.argsort(eigVal)[::-1]                    # 从大到小排序</span><br><span class="line">        eigVal = eigVal[order]</span><br><span class="line">        eigVec = eigVec.T[order].T</span><br><span class="line">        # 选择主成分的数量</span><br><span class="line">        if self.n_component == -1:</span><br><span class="line">            sumOfEigVal = np.sum(eigVal)</span><br><span class="line">            sum_tmp = 0</span><br><span class="line">            for k in range(eigVal.shape[0]):</span><br><span class="line">                sum_tmp += eigVal[k]</span><br><span class="line">                if sum_tmp &gt; prop * sumOfEigVal:            # 平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值</span><br><span class="line">                    self.n_component = k + 1</span><br><span class="line">                    break</span><br><span class="line">        # 选择投影坐标轴</span><br><span class="line">        self.axis = eigVec[:, :self.n_component]            # 选择前n_component个特征向量作为投影坐标轴</span><br><span class="line">    def transform(self, X):</span><br><span class="line">        # 第一步：归一化</span><br><span class="line">        X_normalized = (X - self.meanVal)                   # 归一化测试样本</span><br><span class="line">        # 第二步：投影 X_nxk · V_kxk&apos; = X&apos;_nxk&apos;</span><br><span class="line">        X_transformed = X_normalized.dot(self.axis)</span><br><span class="line">        return X_transformed</span><br><span class="line">    def fit_transform(self, X, prop=0.99):</span><br><span class="line">        self.fit(X, prop=prop)</span><br><span class="line">        return self.transform(X)</span><br><span class="line">    def transform_inv(self, X_transformed):</span><br><span class="line">        # 视投影向量长度为一个单位长度，投影结果为投影向量上的坐标</span><br><span class="line">        # X&apos;_nxk&apos; · V_kxk&apos;.T = X&apos;&apos;_nxk</span><br><span class="line">        X_restructed = X_transformed.dot(self.axis.T)</span><br><span class="line">        # 还原数据</span><br><span class="line">        X_restructed = X_restructed + self.meanVal</span><br><span class="line">        return X_restructed</span><br></pre></td></tr></table></figure><p>实验结果</p><ul><li><p>Demo1: PCA applied on 2-d datasets<br>  <img src="/2018/10/22/PCA/2d_restructed.png" alt="2d_restructed"></p></li><li><p>Demo2: PCA applied on wild face</p><ul><li>origin<br><img src="/2018/10/22/PCA/face_origin.png" alt="origin"></li><li>reduced<br><img src="/2018/10/22/PCA/face_reduced.png" alt="reduced"></li><li>restructured<br><img src="/2018/10/22/PCA/face_restructed.png" alt="restructured"></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Activate Functions</title>
      <link href="/2018/10/20/Activate-Functions/"/>
      <url>/2018/10/20/Activate-Functions/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247483977&amp;idx=1&amp;sn=401b211bf72bc70f733d6ac90f7352cc&amp;chksm=fdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">SigAI 理解神经网络的激活函数</a><br><a href="https://www.cnblogs.com/silence-tommy/p/7113405.html" target="_blank" rel="noopener">机器学习笔记：形象的解释神经网络激活函数的作用是什么？ - 不说话的汤姆猫 - 博客园</a></p></blockquote><h1 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h1><h2 id="复合函数"><a href="#复合函数" class="headerlink" title="复合函数"></a>复合函数</h2><p>神经网络可以看作一个多层复合函数，以下图隐含层的激活函数为例，讲解其非线性作用。<br><img src="/2018/10/20/Activate-Functions/激活函数的非线性作用.png" alt="激活函数的非线性作用"></p><p>记激活函数为$\sigma(·)$，上图神经网络各层间具有如下关系</p><script type="math/tex; mode=display">a = \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1)</script><script type="math/tex; mode=display">b = \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2)</script><script type="math/tex; mode=display">c = \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3)</script><p>输出层采用线性单元</p><script type="math/tex; mode=display">A = w^{(2)}_{1}a + w^{(2)}_{2}b + w^{(2)}_{3}c + b^{(2)}</script><!-- 或者写作复合函数$$A = w^{(2)}_{1} \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1) +     w^{(2)}_{2} \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2) +     w^{(2)}_{3} \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3) +     b^{(2)}$$ --><p>为便于作图，固定参数</p><script type="math/tex; mode=display">W^{(1)} = \left[    \begin{matrix}        1   &  1 \\        0.1 & -1 \\        1   & -1    \end{matrix}\right],b^{(1)} = \left[    \begin{matrix}        -2  \\        1.5 \\        -1    \end{matrix}\right]W^{(2)} = \left[    \begin{matrix}        1 & 2 & 3    \end{matrix}\right],b^{(2)} = \left[    \begin{matrix}        -1    \end{matrix}\right]</script><ul><li><p>线性单元作为激活函数<br>  此时神经网络的输出为</p><script type="math/tex; mode=display">  A = (x + y - 2) +       2 (0.1x - y + 1.5) +       3 (x - y - 1)- 1</script><p>  可见仍为线性函数，做出图像如下所示<br>  <img src="/2018/10/20/Activate-Functions/Linear.png" alt="Linear"></p></li><li><p>非线性单元作为激活函数<br>  此时神经网络的输出为</p><script type="math/tex; mode=display">  A = \sigma(x + y - 2) +       2 \sigma(0.1x - y + 1.5) +       3 \sigma(x - y - 1)- 1</script><p>  激活函数选择<code>Sigmoid</code>，做出图像如下所示<br>  <img src="/2018/10/20/Activate-Functions/nonLinear.png" alt="nonLinear"></p></li></ul><h2 id="分割平面"><a href="#分割平面" class="headerlink" title="分割平面"></a>分割平面</h2><p>神经网络可实现逻辑运算，各个神经元视作分割超平面时，可分割出不同形状的平面，在线性和非线性激活函数时分割效果如图。当神经元组合的情况更复杂时，表达能力就会更强。<br><img src="/2018/10/20/Activate-Functions/激活函数的非线性作用.jpg" alt=""></p><h1 id="激活函数的性质"><a href="#激活函数的性质" class="headerlink" title="激活函数的性质"></a>激活函数的性质</h1><p>已经证明，只要激活函数选择得当，神经元个数足够多，使用3层即包含一个隐含层的神经网络就可以实现对任何一个从输入向量到输出向量的连续映射函数的逼近，这个结论称为万能逼近（universal approximation）定理。</p><blockquote><p>如果$\varphi(x)$是一个非常数、有界、单调递增的连续函数，$I_{m}$是$m$维的单位立方体，$I_{m}$中的连续函数空间为$C(I_{m})$。对于任意$\varepsilon&gt;0$以及函数$f\in C(I_{m})$，存在整数$N$，实数$v_{i},b_{i}$，实向量$w_{i}\in R^{m}$，通过它们构造函数$F(x)$作为函数$f$的逼近：</p><script type="math/tex; mode=display">F(x) = \sum_{i=1}^N v_i \varphi(w_i^T x + b_i)</script><p>对任意的$X\in I_{m}$满足：</p><script type="math/tex; mode=display">| F(x) - f(x) | < \varepsilon</script><p>Cybenko, G. Approximation by superpositions of a sigmoid function. Mathematics of Control, Signals, and Systems, 2, 303-314, 1989.</p></blockquote><p>这个定理对激活函数的要求是<strong>必须非常数、有界、单调递增，并且连续</strong>。</p><p>神经网络的训练使用梯度下降法进行求解，需要计算损失函数对参数的梯度值，涉及到计算激活函数的导数，因此激活函数必须是可导的。实际应用时并不要求它在定义域内处处可导，只要是几乎处处可导即可。</p><blockquote><p>定义$R$为一维欧氏空间，$E\subset R$是它的一个子集，$mE$为点集$E$的<strong>Lebesgue测度</strong>。如果$E$为$R$中的可测集，$f(x)$为定义在上$E$的实函数，如果存在$N\subset E$，满足：$mN=0$，对于任意的$x_{0}\in E/N$函数$f(x)$在$x_{0}$处都可导，则称$f(x)$在$E$上几乎处处可导。</p></blockquote><p>如果将激活函数输入值$x$看做是随机变量，则它落在这些不可导点处的概率是$0$。在计算机实现时，因此有一定的概率会落在不可导点处，但概率非常小。</p><blockquote><p>例如ReLU函数在$x=0$处不可导</p><script type="math/tex; mode=display">f(x) = \begin{cases}    x & x \geq 0 \\    0 & x < 0\end{cases}</script></blockquote><h1 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h1><p><img src="/2018/10/20/Activate-Functions/常用的激活函数.jpg" alt="常用的激活函数"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Feedforward Neural Network</title>
      <link href="/2018/10/20/Feedforward-Neural-Network/"/>
      <url>/2018/10/20/Feedforward-Neural-Network/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一，既可以用于解决分类问题，也可以用于解决回归问题。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>前馈神经网络也叫作多层感知机，包含输入层，隐含层和输出层三个部分。它的目的是为了实现输入到输出的映射。</p><script type="math/tex; mode=display">y = f(x;W)</script><p>由于各层采用了非线性激活函数，神经网络具有良好的非线性特性，如下图所示。</p><ul><li>激活函数为线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/Linear.png" alt="Linear"></li><li>激活函数为非线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/nonLinear.png" alt="nonLinear"></li></ul><p>前馈神经网络可用于解决非线性的分类或回归问题，参数通过反向传播算法<code>(Back Propagation)</code>学习。</p><h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><h2 id="神经元与网络结构图"><a href="#神经元与网络结构图" class="headerlink" title="神经元与网络结构图"></a>神经元与网络结构图</h2><p>单个神经元的示意图如下，输入为前一层的输出参数$X^{(l-1)}$</p><script type="math/tex; mode=display">h_{w, b}(x) = \sigma (WX + b)</script><p>$\sigma(·)$表示激活函数。</p><p><img src="/2018/10/20/Feedforward-Neural-Network/单个神经元示意图.png" alt="单个神经元示意图"></p><p>以下为典型的神经网络结构图<br><img src="/2018/10/20/Feedforward-Neural-Network/前馈神经网络结构图.png" alt="前馈神经网络结构图"></p><ul><li>第一层为输入层<code>input layer</code>，一般不设置权值，预处理在输入网络前完成；</li><li>最后一层为输出层<code>output layer</code>；</li><li>其余层称为隐藏层<code>hidden layer</code>，隐藏层用于提取数据特征，隐藏层层数与各层神经元个数为超参数。</li></ul><blockquote><p>神经元权值取值不同，可实现不同的逻辑运算，单个超平面只能进行二元划分，利用逻辑运算可将多个超平面划分的区域拼接起来，如图<br><img src="/2018/10/20/Feedforward-Neural-Network/超平面划分区域的拼接.jpg" alt="超平面划分区域的拼接"></p><p>以下说明逻辑运算的实现方法<br><img src="/2018/10/20/Feedforward-Neural-Network/二元逻辑运算.png" alt="二元逻辑运算"><br>其中</p><script type="math/tex; mode=display">f(z) = \begin{cases}    1 & z \geq 0 \\    0 & otherwise\end{cases}</script><ul><li><p>与运算 $a ∧ b$</p><script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -30</script></li><li><p>或运算 $a ∧ b$</p><script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -10</script></li><li><p>非运算 $a = \overline{b}$</p><script type="math/tex; mode=display">w_1 = -20, w_2 = 0, b = 0</script></li><li><p>异或运算 $a \bigoplus b$，可通过组合运算实现</p><script type="math/tex; mode=display">a \bigoplus b = (\overline{a} ∧ b) ∨ (a ∧ \overline{b})</script></li></ul></blockquote><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul><li><p>隐藏层的激活函数，详情可查看<a href="https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">另一篇博文：神经网络的激活函数</a>；</p></li><li><p>输出层的激活函数</p><ul><li><p>回归问题时，采用线性单元即可</p><script type="math/tex; mode=display">  f(x) = x</script></li><li><p>分类问题时，一般有以下几种选择</p><ul><li><p>单类别概率输出<br>  即每个神经元的输出对应该类别的$0-1$分布输出，这就需要将输出值限制在$[0, 1]$内，例如</p><script type="math/tex; mode=display">P(y=1|x )= max\{0, min\{1, z\}\}</script><p>  <img src="/2018/10/20/Feedforward-Neural-Network/clf_linearout.png" alt="线性输出单元"></p><p>  但是可以看到，当$(w^Tx+b)$处于单位区间外时，模型的输出对它的参数的梯度都将为$0$ ，不利于网络的训练，故采用$S$形函数<code>Sigmoid</code>(<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)</p><script type="math/tex; mode=display">  P(y=1|x ) = \frac{1}{1+e^{-(w^Tx+b)}}</script><blockquote><p>$(1)$ <code>Sigmoid</code>函数定义域为$(-\infty, \infty)$，值域为$(0, 1)$，且在整个定义域上单调递增，即为单值函数，故可将线性输出单元的结果映射到$(0, 1)$范围内；<br>$(2)$ 在定义域上处处可导。</p></blockquote></li><li><p>多类别的概率输出<br>  即每个神经元的输出对应判别为该类别的概率，且有</p><script type="math/tex; mode=display">  \sum_{i=1}^C y_i = 1</script><p>  例如</p><script type="math/tex; mode=display">  y_i = \frac{z_i}{\sum_j z_j}</script><p>  但是分式求导异常麻烦，故采用<code>Softmax</code>函数(<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)作为输出结点的激活函数，该函数求导结果比较简洁，且可利用输出计算导数，计算量减少。</p><script type="math/tex; mode=display">  Softmax(x) = \frac              {1}              {\sum_{k=1}^K exp(x_k)}              \left[                  \begin{matrix}                      exp(x_1)\\                      exp(x_2)\\                      ...\\                      exp(x_K)                  \end{matrix}              \right]</script></li></ul></li></ul></li></ul><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><ul><li><p>回归问题<br>  常见的用于回归问题的损失函数为<code>MSE</code>，即</p><script type="math/tex; mode=display">  L(y, \hat{y}) = \frac{1}{2M} \sum_{i=1}^M (\hat{y}^{(i)} - y^{(i)})^2</script></li><li><p>分类问题<br>  一般采用交叉熵作为损失函数，如下</p><script type="math/tex; mode=display">  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M 1\{y^{(i)}_j=k\}   \log (\hat{y}^{(i)}_j)</script><script type="math/tex; mode=display">  1\{y^{(i)}_j=k\} =       \begin{cases}          1 & y^{(i)}_j = k \\          0 & y^{(i)}_j \neq k       \end{cases}　j = 1, ..., N</script><p>  或者</p><script type="math/tex; mode=display">  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M   y^{(i)T} \log (\hat{y}^{(i)})</script><p>  其中$y^{(i)}, \hat{y}^{(i)}$均表示向量，采用<code>one-hot</code>编码。</p></li></ul><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><p>以上内容网上资料一大堆，进入重点，反向传播时的梯度推导，给出网络结构如下。</p><ul><li>回归与分类在输出层有所区别；</li><li>各层激活函数的输入变量以$z^{(l)}$表示，输出变量均以$x^{(l)}$表示；</li><li>$W^{(l)}$表示从第$l$层到第$(l+1)$层的权值矩阵，则$w^{(l)}_{ij}$表示第$l$层第$j$个神经元到$(l+1)$层第$i$个神经元的连接权值；</li><li>$b^{(l)}$表示第$l$层到第$(l+1)$层的偏置，则$b^{(l)}_i$表示到第$(l+1)$层第$i$个神经元的偏置值；</li><li>各层变量维度推广为输入$d_{i}$，中间层$d_{h}$，输出层$d_{o}$；</li><li>全连接，部分线条已省略，激活函数已省略；</li></ul><p><img src="/2018/10/20/Feedforward-Neural-Network/fnn.jpg" alt="FNN"></p><p>则各层参数矩阵为</p><script type="math/tex; mode=display">W^{(1)} = \left[        \begin{matrix}            w^{(1)}_{11} & ... & w^{(1)}_{1d_i} \\            ... & ... & ... \\            w^{(1)}_{d_h1} & ... & w^{(1)}_{d_hd_i}        \end{matrix}\right]　b^{(1)} = \left[        \begin{matrix}            b^{(1)}_{1} \\            ... \\            b^{(1)}_{d_h}        \end{matrix}\right]</script><script type="math/tex; mode=display">W^{(2)} = \left[        \begin{matrix}            w^{(2)}_{11} & ... & w^{(2)}_{1d_h} \\            ... & ... & ... \\            w^{(2)}_{d_o1} & ... & w^{(2)}_{d_od_h}        \end{matrix}\right]　b^{(2)} = \left[        \begin{matrix}            b^{(2)}_{1} \\            ... \\            b^{(2)}_{d_o}        \end{matrix}\right]</script><p>有</p><script type="math/tex; mode=display">Z^{(2)} = W^{(1)} X^{(1)} + b^{(1)}</script><script type="math/tex; mode=display">X^{(2)} = \sigma_1 (Z^{(2)})</script><script type="math/tex; mode=display">Z^{(3)} = W^{(2)} X^{(2)} + b^{(2)}</script><script type="math/tex; mode=display">X^{(3)} = \sigma_2 (Z^{(3)})</script><script type="math/tex; mode=display">X^{(1)} = X　\hat{Y} = X^{(3)}</script><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>损失函数采用<code>MSE</code>，即</p><script type="math/tex; mode=display">L(Y, \hat{Y}) = \frac{1}{M} \sum_{i=1}^M L(Y^{(i)}, \hat{Y}^{(i)})</script><script type="math/tex; mode=display">L(Y^{(i)}, \hat{Y}^{(i)}) = \frac{1}{2} || \hat{Y}^{(i)} - Y^{(i)} ||_2^2= \frac{1}{2} \sum_{d_2=1}^{d_o}(\hat{y}^{(i)}_{d_2} - y^{(i)}_{d_2})^2</script><p>下面推导单个样本的损失函数的梯度，该批数据的梯度为均值。</p><blockquote><p>省略样本标记<code>$^{(i)}$</code></p></blockquote><ul><li><p>隐含层到输出层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = \frac{∂}{∂w^{(2)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2} \tag{1}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}  \end{cases}</script><p>  且</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)}) \frac{∂z_{d_2}^{(3)}}{∂w^{(2)}_{ij}} \tag{2}</script><script type="math/tex; mode=display">  \frac{∂}{∂w^{(2)}_{ij}} z_{d_2}^{(3)} =       \begin{cases}          x^{(2)}_{d_1} & d_1 = j, d_2 = i \\          0 & otherwise      \end{cases} \tag{3}</script><p>  $(3)$代入$(2)$，再代入$(1)$可得到</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) x^{(2)}_{d_1} | _{d_1=j, d_2=i}  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \tag{*1}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i}  = \frac{∂}{∂b^{(2)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(2)}_i} \hat{y}_{d_2} \tag{4}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}  \end{cases}</script><p>  有</p><script type="math/tex; mode=display">  \frac{∂}{∂b^{(2)}_i} z_{d_2}^{(3)} =       \begin{cases}          1 &  d_2 = i \\          0 & otherwise      \end{cases} \tag{5}</script><p>  所以</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) | _{d_2=i}  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \tag{*2}</script></li></ul></li><li><p>输入层到隐含层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = \frac{∂}{∂w^{(1)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2} \tag{6}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \\      x^{(2)}_{d_1} = \sigma_1 (z_{d_1}^{(2)}) \\      z_{d_1}^{(2)} = \sum_{d_0=1}^{d_i} w^{(1)}_{d_1 d_0} x^{(1)}_{d_0} + b^{(1)}_{d_1}  \end{cases}</script><p>  故</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}  = \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}}       \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \tag{7}</script><p>  其中</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}}   = \sigma_2' (z_{d_2}^{(3)})  \tag{8}</script><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}   = \sum_{d1=1}^{d_h} w^{(2)}_{d_2 d_1} \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} \tag{9}</script><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}}  = \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}}      \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}}  \tag{10}</script><p>  而其中</p><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}} = x^{(2)}_{d_1} = \sigma_1' (z_{d_1}^{(2)}) \tag{11}</script><script type="math/tex; mode=display">  \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}} =   \begin{cases}      x^{(1)}_{d_0} & d_1 = i, d_0 = j\\      0 & otherwise  \end{cases} \tag{12}</script><p>  $(11),(12)$代入$(10)$得到</p><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} =   \sigma_1' (z_{d_1}^{(2)})      x^{(1)}_{d_0} | _{d_1 = i, d_0 = j} \tag{13}</script><p>  $(13)$代回$(9)$，有</p><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}   = \sum_{d1=1}^{d_h}   \left[      w^{(2)}_{d_2 d_1}       \sigma_1' (z_{d_1}^{(2)})      x^{(1)}_{d_0}  \right] | _{d_1 = i, d_0 = j}</script><script type="math/tex; mode=display">  = w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{14}</script><p>  将$(8),(14)$代入$(7)$得到</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{15}</script><p>  $(15)$代入$(6)$有</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = \sum_{d_2=1}^{d_o}       (\hat{y}_{d_2} - y_{d_2})            \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{*3}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i}  = \frac{∂}{∂b^{(1)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2} \tag{16}</script><p>  同理可得</p><script type="math/tex; mode=display">  \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})  \tag{17}</script><p>  所以</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i} =  \sum_{d_2=1}^{d_o}       (\hat{y}_{d_2} - y_{d_2})            \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)}) \tag{*4}</script></li></ul></li></ul><p>综上所述</p><script type="math/tex; mode=display">\frac{∂L}{∂w^{(2)}_{ij}}= (\hat{y}_{i} - y_{i})     \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j}</script><script type="math/tex; mode=display">\frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{i} - y_{i})     \sigma_2' (z_i^{(3)})</script><script type="math/tex; mode=display">\frac{∂L}{∂w^{(1)}_{ij}}= \sum_{d_2=1}^{d_o}     (\hat{y}_{d_2} - y_{d_2})          \sigma_2' (z_{d_2}^{(3)})     w^{(2)}_{d_2 i}     \sigma_1' (z_i^{(2)})    x^{(1)}_j</script><script type="math/tex; mode=display">\frac{∂L}{∂b^{(1)}_i} = \sum_{d_2=1}^{d_o}     (\hat{y}_{d_2} - y_{d_2})          \sigma_2' (z_{d_2}^{(3)})     w^{(2)}_{d_2 i}     \sigma_1' (z_i^{(2)})</script><p>令</p><script type="math/tex; mode=display">\begin{cases}    \delta^{(2)}_i     = (\hat{y}_{i} - y_{i})         \sigma_2' (z_i^{(3)}) \\    \delta^{(1)}_i     = \sum_{d_2=1}^{d_o}         \delta^{(2)}_{d_2}         w^{(2)}_{d_2 i}         \sigma_1' (z_i^{(2)})\end{cases}</script><p>有</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂L}{∂w^{(2)}_{ij}} = \delta^{(2)}_i x^{(2)}_{j}\\    \frac{∂L}{∂b^{(2)}_i}    = \delta^{(2)}_i\\    \frac{∂L}{∂w^{(1)}_{ij}} = \delta^{(1)}_i x^{(1)}_j\\    \frac{∂L}{∂b^{(1)}_i}    = \delta^{(1)}_i\end{cases}</script><p>至此推导完毕。</p><blockquote><p>当隐藏层采用<code>Sigmoid</code>函数，输出层采用线性单元，可得到</p><script type="math/tex; mode=display">\sigma_1' (z_i^{(2)}) = \sigma_1 (z_i^{(2)})     \left[1 - \sigma_1 (z_i^{(2)}) \right]= x_i^{(2)} (1 - x_i^{(2)})</script><script type="math/tex; mode=display">\sigma_2' (z_i^{(3)}) = z_i^{(3)}</script><p>此时</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) z_i^{(3)} x^{(2)}_{j} \\    \frac{∂L}{∂b^{(2)}_i}    = (\hat{y}_{i} - y_{i}) z_i^{(3)} \\    \frac{∂L}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} x^{(1)}_j \\    \frac{∂L}{∂b^{(1)}_i}    = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)}\end{cases}</script><p>可以看到，计算梯度时使用的数据在上一次前向传播时已计算得，故可减少计算量。</p></blockquote><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>损失函数采用<code>Cross Entropy</code>，即</p><script type="math/tex; mode=display">L(\hat{y}, y) = \frac{1}{M} \sum_{i=1}^M L(\hat{y}^{(i)}, y^{(i)})</script><script type="math/tex; mode=display">L(\hat{y}^{(i)}, y^{(i)}) = - y^{(i)T} \log (\hat{y}^{(i)})</script><p>上式中，$y^{(i)}, \hat{y}^{(i)}$均为列向量，且$y^{(i)}$表示<code>one-hot</code>编码后的标签向量，也可写作</p><script type="math/tex; mode=display">L(\hat{y}^{(i)}, y^{(i)})= - \log \hat{y}^{(i)}_{y^{(i)}}</script><ul><li>由该式可以看出，若输出层激活函数采用<code>Sigmoid</code>作为激活函数，则隐藏层——输出层之间权值矩阵$W^{(2)}$只会更新$w^{(i)}_{y^{(i)} d_1}, d_1 = 1, …, d_h$；</li><li>一般采用<code>SoftMax</code>作为输出层激活函数，<code>Sigmoid</code>下面不作推导。</li></ul><blockquote><p>关于<code>SoftMax</code>的梯度，移步<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">SoftMax Regression</a>中查看详细推导过程，这里直接给出结论。<br>对于</p><script type="math/tex; mode=display">S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]</script><p>其梯度为</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i}_{K×1} =  \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] -  \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right]= \left( \left[ \begin{matrix}  0 \\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i</script><p>省略样本标记<code>$^{(i)}$</code></p></blockquote><ul><li><p>隐含层到输出层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = - \frac{∂}{∂w^{(2)}_{ij}} \log \hat{y}_{y}  = - \frac{1}{\hat{y}_y}       \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}} \tag{18}</script><p>  其中$\hat{y}_{y}$与$z^{(3)}_{d_2}(d_2 = 1, …, d_o) $均有联系，故</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}}  = \sum_{d2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}} \tag{19}</script><p>  而</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}  = \begin{cases}      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\      - \hat{y}_{y} \hat{y}_{d_2} & otherwise  \end{cases}</script><script type="math/tex; mode=display">  \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}}  = \begin{cases}      x^{(2)}_{d_1} & i = d_2, j = d_1 \\      0 & otherwise  \end{cases}</script><blockquote><p>$z^{(3)}_{d_2} = \sum_{d_1=0}^{d_h} w^{(2)}_{d_2d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}$</p></blockquote><p>  代回$(19)$，再带回$(18)$，有</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = - \frac{1}{\hat{y}_{y}}       \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       x^{(2)}_{d_1} | _{d_2=i, d_1=j}</script><script type="math/tex; mode=display">  = \begin{cases}      - \frac{1}{\hat{y}_{y}} \hat{y}_{y} (1 - \hat{y}_i) x^{(2)}_j & i = y \\      - \frac{1}{\hat{y}_{y}} (- \hat{y}_{y} \hat{y}_i) x^{(2)}_j & otherwise  \end{cases}</script><script type="math/tex; mode=display">  = \begin{cases}      (\hat{y}_i - 1) x^{(2)}_j & i = y \\      \hat{y}_i x^{(2)}_j & otherwise  \end{cases}</script><p>  即</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = (\hat{y}_i - y_i) x^{(2)}_j \tag{*5}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i}  = \hat{y}_i - y_i \tag{*6}</script></li></ul></li><li><p>输入层到隐含层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = - \frac{∂}{∂w^{(1)}_{ij}} \log \hat{y}_{y}  = - \frac{1}{\hat{y}_{y}}       \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}} \tag{20}</script><p>  其中</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}}  = \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}} \tag{21}</script><p>  $\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}$部分与回归相同，有</p><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}  = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><p>  由上面分析可得</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}  = \begin{cases}      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\      - \hat{y}_{y} \hat{y}_{d_2} & otherwise  \end{cases}</script><p>  故代回$(20)$可得到</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = - \frac{1}{\hat{y}_{y}}      \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}</script><script type="math/tex; mode=display">  = - \frac{1}{\hat{y}_{y}}      \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">  = \left[       \sum_{d_2=1, d_2 \neq y}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} +       (\hat{y}_y - 1) w^{(2)}_{y i}   \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">  = \left[       \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} -       w^{(2)}_{y i}  \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*7}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i}  = \left[       \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} -       w^{(2)}_{y i}  \right] \sigma_1' (z_i^{(2)}) \tag{*8}</script></li></ul></li></ul><p>至此推导完毕。</p><blockquote><p>这个推导，仅供参考</p></blockquote><h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>和其他算法一样，前馈神经网络也存在过拟合的问题，解决方法有以下几种</p><ul><li><p>正则化<br>  与线性回归类似，神经网络也可以加入范数惩罚项，以下$C$表示普通的损失函数，$\lambda$为惩罚系数，$n$为样本数目，$w$表示权值参数。</p><ul><li><code>L1</code>正则化<br>  惩罚项为网络所有权值的绝对值之和。<script type="math/tex; mode=display">  C = C_0 + \frac{\lambda}{n} \sum_w |w|</script></li><li><code>L2</code>正则化<br>  又称权值衰减<code>weights decay</code>，惩罚项为网络所有权值的平方和。<script type="math/tex; mode=display">  C = C_0 + \frac{\lambda}{2n} \sum_w w^2</script></li></ul></li><li><p>Dropout<br>  以概率大小为<code>p</code>使部分神经元输出值直接为0，如此可以使反向传播时相关权值系数不做更新，只有被保留下来的权值和偏置值会被更新。<br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_1.png" alt="dropout_1"><br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_2.png" alt="dropout_2"></p></li><li><p>增加训练数据大小<br>  可在原数据上加以变换或噪声，图像的扩增方法可查看<a href="https://louishsu.xyz/2018/11/02/%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%89%A9%E5%A2%9E-Augment-%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">图像数据集扩增</a>。</p></li></ul><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-PyTorch-Tutorial/blob/master/NeuralNetwork_ANN_MNIST.py" target="_blank" rel="noopener">@Github: Code of Neural Network</a></p><p>使用<code>PyTorch</code>实现神经网络，以下为模型定义<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class AnnNet(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(AnnNet, self).__init__()</span><br><span class="line">        self.input_size = 28 * 28</span><br><span class="line">        self.hidden_size = 100</span><br><span class="line">        self.output_size = 10</span><br><span class="line">        self.fc1 = nn.Linear(self.input_size,  self.hidden_size)    # input   - hidden</span><br><span class="line">        self.fc2 = nn.Linear(self.hidden_size, self.output_size )   # hidden  - output</span><br><span class="line">        # self.activate = nn.Sigmoid()  # 参数更新非常慢，特别是层数多时</span><br><span class="line">        self.activate = nn.ReLU()       # 事实证明ReLU作为激活函数更加合适</span><br><span class="line">        self.softmax = nn.Softmax()</span><br><span class="line">    def forward(self, X):</span><br><span class="line">        h = self.activate(self.fc1(X))</span><br><span class="line">        y_pred = self.softmax(self.fc2(h))</span><br><span class="line">        return y_pred</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>分类问题的决策平面</title>
      <link href="/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/"/>
      <url>/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>对于分类问题，计算结果一般为概率值，那么如何根据计算得的概率进行判别分类呢？</p><blockquote><p>这部分理解后，<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">Logistic回归</a>与<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">Softmax回归</a>的模型就很容易推得。</p></blockquote><h1 id="判别函数"><a href="#判别函数" class="headerlink" title="判别函数"></a>判别函数</h1><p>对于一个类别为$K$的分类问题，如果对于所有的$ i,j=1,…,K, j\neq i$，有</p><script type="math/tex; mode=display">g_i(x) > g_j(x)</script><p>则此分类器将这个样本对应的特征向量$x$判别为$w_i$，则此分类器的作用是，计算$K$个判别函数并选取与最大判别值最大对应的类别。</p><blockquote><p>判别函数的形式并不唯一，可以将所有的判别函数乘上相同的正常数或者加上一个相同的常量而不影响其判决结果。更一般的情况下，我们使用单调递增函数$f(·)$进行映射，将每一个$g_i(x)$替换成$f(g_i(x))$，分类结果不变。</p><div style="text-align: right"> ——《模式识别原理与应用课程笔记》</div></blockquote><p>例如<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">最小风险贝叶斯决策</a></p><h1 id="正态分布下的判别函数"><a href="#正态分布下的判别函数" class="headerlink" title="正态分布下的判别函数"></a>正态分布下的判别函数</h1><blockquote><p><a href="https://www.cnblogs.com/bingjianing/p/9117330.html" target="_blank" rel="noopener">多元高斯分布（The Multivariate normal distribution） - bingjianing - 博客园</a></p></blockquote><p>由大数定理可知，在样本足够的情况下，数据服从正态分布。多元正态分布形式如下</p><script type="math/tex; mode=display">f(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))</script><p>其中</p><script type="math/tex; mode=display">x = [x_1, ..., x_n]^T</script><script type="math/tex; mode=display">\mu = [\mu_1, ..., \mu_n]^T</script><script type="math/tex; mode=display">\Sigma_{ij} = cov(x_i, x_j)</script><!-- $$\Sigma =  \left[​    \begin{matrix}​        1 & 2 & 3 \\​        4 & 5 & 6 \\​        7 & 8 & 9​    \end{matrix}\right] \tag{3}$$ --><p>在<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">最小错误率判别</a>时</p><script type="math/tex; mode=display">g_i(x) = P(x|c_i)P(c_i)</script><p>即</p><script type="math/tex; mode=display">g_i(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}}exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i)) ·P(c_i)</script><p>取对数运算，并舍去常数项，展开整理得</p><script type="math/tex; mode=display">g_i(x) = -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x  -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i) \tag{0}</script><blockquote><p><code>注：</code> 协方差矩阵 $ \Sigma^T = \Sigma $</p></blockquote><h2 id="1-Sigma-i-sigma-2-I"><a href="#1-Sigma-i-sigma-2-I" class="headerlink" title="1. $\Sigma_i = \sigma^2 I$"></a>1. $\Sigma_i = \sigma^2 I$</h2><p>$\Sigma_i^{-1} = \frac{1}{\sigma^2} I$代入$(0)$，有</p><script type="math/tex; mode=display">g_i(x) = \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)\tag{1}</script><p>定义</p><script type="math/tex; mode=display">w_i = \frac{1}{\sigma^2}\mu_i^T</script><script type="math/tex; mode=display">w_0 =  - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)</script><p>有一般形式如下，表示取$c_i$的概率</p><script type="math/tex; mode=display">g_i(x) = w_i x + w_0\tag{2}</script><p>设决策平面为</p><script type="math/tex; mode=display">w^T (x−x_0)=0\tag{3}</script><p>决策平面上，取$c_i$和$c_j$的概率相等，即</p><script type="math/tex; mode=display">g_i(x) = g_j(x)</script><p>可得</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} \tag{4}</script><blockquote><p>推导过程如下，将$(1)$代入上式<br>$ \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i) = \frac{1}{\sigma^2}\mu_j^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_j ^T \mu_j) + ln P(c_j) $<br>$ \mu_i^T x - \frac{1}{2} \mu_i ^T \mu_i + ln P(c_i) = \mu_j^T x - \frac{1}{2} \mu_j ^T \mu_j + ln P(c_j) $<br>$ (\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} $</p></blockquote><p>由$(3)$$(4)$，利用待定系数法，可得</p><script type="math/tex; mode=display">w = \mu_i - \mu_j</script><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)}</script><p>特别地，当等先验概率时，即$P(c_i) = P(c_j)$时</p><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j)</script><p>故</p><script type="math/tex; mode=display">x_0 = \frac{1}{2}(\mu_i + \mu_j)</script><p>结论：等先验概率时超平面$ w^T (x−x_0)=0 $平分判别空间</p><blockquote><p>$\mu_i$与$\mu_j$分别表示两个类别的中心，由向量运算，$x_0$为两类中心的连线的中点。</p></blockquote><h2 id="2-Sigma-i-Sigma"><a href="#2-Sigma-i-Sigma" class="headerlink" title="2. $\Sigma_i = \Sigma$"></a>2. $\Sigma_i = \Sigma$</h2><p>代入$(0)$后可得</p><script type="math/tex; mode=display">g_i(x) =  \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) \tag{5}</script><p>定义</p><script type="math/tex; mode=display">w_i = \mu_i^T \Sigma ^{-1}</script><script type="math/tex; mode=display">w_0 = - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i)</script><p>有一般形式如下，表示取$c_i$的概率</p><script type="math/tex; mode=display">g_i(x) = w_i x + w_0\tag{6}</script><p>同样的，设决策平面为</p><script type="math/tex; mode=display">w^T (x−x_0)=0\tag{7}</script><p>决策平面上，取$c_i$和$c_j$的概率相等，即</p><script type="math/tex; mode=display">g_i(x) = g_j(x)</script><p>有</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i - \mu_j)^T \Sigma ^{-1} (\mu_i - \mu_j) - ln \frac{P(c_i)}{P(c_j)}</script><blockquote><p>$ \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $<br>$ \mu_i^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $<br>$ (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j) - ln \frac{P(c_i)}{P(c_j)} $</p></blockquote><p>特别的，当取等先验概率时</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)</script><p>由$(7)$$(8)$，利用待定系数法</p><script type="math/tex; mode=display">w^T = (\mu_i - \mu_j)^T \Sigma^{-1}</script><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)</script><blockquote><p><code>注：</code> 协方差矩阵 $ \Sigma^T = \Sigma $</p></blockquote><script type="math/tex; mode=display">w = \Sigma^{-1}(\mu_i - \mu_j)</script><script type="math/tex; mode=display">x_0 = \frac{1}{2} (\mu_i + \mu_j)</script><p>由于通常$w=Σ^{−1}(μ_i−μ_j)$并非朝着$(μ_i−μ_j)$的方向，因而通常分离两类的超平面也并非与均值的连线垂直正交。但是， 如果先验概率相等，其判定面确实是与均值连线交于中点$x_0$处的。如果先验概率不等，最优边界超平面将远离可能性较大的均值。同前，如果偏移量足够大，判定面可以不落在两个均值向量之间。</p><h2 id="3-Sigma-i-Sigma-i-∀"><a href="#3-Sigma-i-Sigma-i-∀" class="headerlink" title="3. $\Sigma_i = \Sigma_i(∀) $"></a>3. $\Sigma_i = \Sigma_i(∀) $</h2><script type="math/tex; mode=display">g_i(x) =  -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x  -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)</script><p>定义</p><script type="math/tex; mode=display">W_i = -\frac{1}{2} \Sigma_i ^{-1}</script><script type="math/tex; mode=display">w_i = \mu_i^T \Sigma_i ^{-1}</script><script type="math/tex; mode=display">w_0 = -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)</script><p>有</p><script type="math/tex; mode=display">g_i(x) = x^TW_ix + w_ix + w_0</script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Bayes Decision</title>
      <link href="/2018/10/18/Bayes-Decision/"/>
      <url>/2018/10/18/Bayes-Decision/</url>
      
        <content type="html"><![CDATA[<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>基于贝叶斯公式</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><script type="math/tex; mode=display">P(x)=\sum_j p(x|c_j)P(c_j)</script><h1 id="几种常用的贝叶斯决策"><a href="#几种常用的贝叶斯决策" class="headerlink" title="几种常用的贝叶斯决策"></a>几种常用的贝叶斯决策</h1><h2 id="最小错误率贝叶斯决策"><a href="#最小错误率贝叶斯决策" class="headerlink" title="最小错误率贝叶斯决策"></a>最小错误率贝叶斯决策</h2><p>在分类问题中，我们往往希望尽可能减少分类错误，即目标是追求最小错误率。假设有$K$分类问题，由贝叶斯公式</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><p>上式中$ k=1,…,K  $，各部分定义如下</p><blockquote><p>$P(c_k|x)$——<code>后验概率(posteriori probability)</code><br>$P(c_k)$——<code>先验概率(priori probability)</code>，<br>$p(x|c_k)$——$c_k$关于$x$的<code>似然函数(likelihood)</code>，<br>$p(x)$——<code>证据因子(evidence)</code>，</p></blockquote><p>证据因子由下式计算</p><script type="math/tex; mode=display">p(x)=\sum_{j=0}^K p(x|c_j)P(c_j)</script><p>以上就是从样本中训练的参数，在预测阶段，定义决策规则为</p><blockquote><p>$if$ $P(c_i|x)&gt;P(c_j|x)$, $then$ $ x \in c_i $</p></blockquote><p>由于分母为标量，对于任意输入的样本特征$x$，$P(x)$一定，故决策规则可简化为</p><blockquote><p>$if$ $P(x|c_i)P(c_i)&gt;P(x|c_j)P(c_j)$, $then$ $ x \in c_i $</p></blockquote><p>而对于分类错误的样本，如样本$x$属于分类$c_i$，但错误分类为$c_{err}, err \neq i$，样本的错误分类概率为</p><script type="math/tex; mode=display">P(error|x) = P(c_{err}|x)</script><p>上式被称作<code>误差概率</code>，某类后验概率越大，则相应的误差概率就越小，定义平均误差概率</p><script type="math/tex; mode=display">P_{mean} = \int P(error|x)P(x)dx</script><h2 id="带有拒绝域的最小错误率贝叶斯决策"><a href="#带有拒绝域的最小错误率贝叶斯决策" class="headerlink" title="带有拒绝域的最小错误率贝叶斯决策"></a>带有拒绝域的最小错误率贝叶斯决策</h2><p>一些情况下，某样本对应特征$x$计算结果中，属于各类别的概率没有显著比较大的数值，换句话说都比较小，那么对这次的判别就不太信任，选择拒绝决策结果。<br>将决策平面划分为两个区域</p><script type="math/tex; mode=display">Acquired = \{x|max_j P(c_j|x)\geq 1-t\}</script><script type="math/tex; mode=display">Rejected = \{x|max_j P(c_j|x) < 1-t\}</script><p>其中$t$为阈值，$t$越小时，拒绝域$Rejected$越大，当满足</p><script type="math/tex; mode=display">1-t \leq \frac{1}{K}</script><p>或者 </p><script type="math/tex; mode=display">t \geq \frac{K-1}{K}</script><p>此时拒绝域为</p><script type="math/tex; mode=display">Rejected = \{x|max_j P(c_j|x) < \frac{1}{K}\}</script><p>而当且仅当各分类概率相等时才有 $ max_j P(c_j|x) = \frac{1}{K} $，因此此时拒绝域为空，接受所有决策结果</p><h2 id="最小风险贝叶斯决策"><a href="#最小风险贝叶斯决策" class="headerlink" title="最小风险贝叶斯决策"></a>最小风险贝叶斯决策</h2><p>在决策过程中，不同类型的决策错误所产生的代价是不同的。引入风险函数</p><script type="math/tex; mode=display">\lambda_{i, j} = \lambda (\alpha_i|c_j)</script><p>表示实际类别为$c_j$时，采取错误判断为$c_i$的行为$\alpha_i$所产生的损失。该函数称为损失函数，通常它可以用表格的形式给出，叫做决策表，形如<br><img src="/2018/10/18/Bayes-Decision/decision_table.jpg" alt="决策表"><br>定义条件风险</p><script type="math/tex; mode=display">R(\alpha_i|c_j) = \sum_j \lambda (\alpha_i|c_j) P(c_j|x)</script><p>特别地，取$0-1$损失时，即最小错误率贝叶斯决策</p><script type="math/tex; mode=display">\lambda (\alpha_i|c_j) = \begin{cases}0 & i = j \\1 & i \neq j\end{cases}</script><!-- $$函数名 = \begin{cases}公式1 & 条件1 \\公式2 & 条件2 \\公式3 & 条件3 \end{cases}$$--><p>可能比较抽象，这里举了一个例子</p><div style="align: center"><img src="/2018/10/18/Bayes-Decision/最小风险贝叶斯决策例.png"></div><h1 id="关于判别函数"><a href="#关于判别函数" class="headerlink" title="关于判别函数"></a>关于判别函数</h1><p>可查看<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a></p><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><img src="/2018/10/18/Bayes-Decision/李航-例4.1.png" alt="例4.1"></p><p>为帮助理解，先手动计算一遍结果</p><blockquote><p>先验概率(<code>priori probability</code>):<br>$ P(Y = -1) = \frac{6}{15} $<br>$ P(Y = 1) = \frac{9}{15} $<br>似然函数(<code>likelihood</code>)<br>$ P(X^{(1)} = 1|Y=-1) = \frac{3}{6}$<br>$ P(X^{(1)} = 2|Y=-1) = \frac{2}{6}$<br>$ P(X^{(1)} = 3|Y=-1) = \frac{1}{6}$<br>$ P(X^{(2)} = S|Y=-1) = \frac{3}{6}$<br>$ P(X^{(2)} = M|Y=-1) = \frac{2}{6}$<br>$ P(X^{(2)} = L|Y=-1) = \frac{1}{6}$<br>$ P(X^{(1)} = 1|Y=1) = \frac{2}{9}$<br>$ P(X^{(1)} = 2|Y=1) = \frac{3}{9}$<br>$ P(X^{(1)} = 3|Y=1) = \frac{4}{9}$<br>$ P(X^{(2)} = S|Y=1) = \frac{1}{9}$<br>$ P(X^{(2)} = M|Y=1) = \frac{4}{9}$<br>$ P(X^{(2)} = L|Y=1) = \frac{4}{9}$</p></blockquote><p>注意：证据因子(<code>evidence</code>)不能用如下朴素贝叶斯求解</p><script type="math/tex; mode=display">P(X) = P(X^{(1)}) P(X^{(2)})</script><p>而是</p><script type="math/tex; mode=display">P(X) =  P(X^{(1)}|Y=-1)P(Y = -1) + P(X^{(2)}|Y=-1)P(Y = -1)</script><p>一般分子用朴素贝叶斯求解</p><script type="math/tex; mode=display">P(X|Y) = P(X^{(1)}|Y) P(X^{(2)}|Y)</script><p>将其加和作为分母</p><script type="math/tex; mode=display">c_k: P(X)_k = \sum_{k=0}^2 P(X^{(1)}|Y=k) P(X^{(2)}|Y=k)</script><script type="math/tex; mode=display">P(Y_k|X) = \frac{P(X|Y_k)P(Y_k)}{P(X)_k}</script><p>选取最大概率的$ k $类别作为判别类别</p><script type="math/tex; mode=display">k = argmax_k P(Y_k|X)</script><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Statistical%20Learning%20Method%2C%20Li%20Hang/naive_bayes_algorithm_demo.py" target="_blank" rel="noopener">@Github: Code for Naive Bayes Decision</a></p><h3 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def fit(self, X, y):</span><br><span class="line">    X_encoded = self.featureEncoder.fit_transform(X).toarray()</span><br><span class="line">    y_encoded = OneHotEncoder().fit_transform(y.reshape((-1, 1))).toarray()</span><br><span class="line">    self.P_X = np.mean(X_encoded, axis=0)                           # one-hot编码下，各列的均值即各特征的概率</span><br><span class="line">    self.P_Y = np.mean(y_encoded, axis=0)                           # one-hot编码下，各列的均值即各了别的概率</span><br><span class="line">    self.n_labels, self.n_features = y_encoded.shape[1], X_encoded.shape[1]   </span><br><span class="line">    self.P_X_Y = np.zeros(shape=(self.n_labels, self.n_features))   # 各个类别下，分别统计各特征的概率</span><br><span class="line">    for i in range(self.n_labels):</span><br><span class="line">        X_encoded_of_yi = X_encoded[y_encoded[:, i]==1]             # 取出属于i类别的样本</span><br><span class="line">        self.P_X_Y[i] = np.mean(X_encoded_of_yi, axis=0)            # one-hot编码下，各列的均值即各特征的概率</span><br></pre></td></tr></table></figure><h3 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def predict(self, X):</span><br><span class="line">    X_encoded = self.featureEncoder.transform(X).toarray()</span><br><span class="line">    n_samples = X_encoded.shape[0]</span><br><span class="line">    y_pred_prob = np.zeros(shape=(n_samples, self.n_labels))</span><br><span class="line">    for i in range(n_samples):</span><br><span class="line">        for j in range(self.n_labels):</span><br><span class="line">            P_Xi_encoded_Yj = X_encoded[i] * self.P_X_Y[j]          # 在Yj类别下，选出输入样本Xi对应的条件概率</span><br><span class="line">            P_Xi_encoded_Yj[P_Xi_encoded_Yj==0.0] = 1.0             # 将为0值替换为1，便于求解ΠP(Xi|yc)，只要将各元素累乘即可</span><br><span class="line">            y_pred_prob[i, j] = self.P_Y[j] * P_Xi_encoded_Yj.prod()</span><br><span class="line">        y_pred_prob[i] /= np.sum(y_pred_prob[i])                    # 分母一般是将分子加和，不能假定各特征独立并用朴素贝叶斯计算分母</span><br><span class="line">    return np.argmax(y_pred_prob, axis=1)</span><br></pre></td></tr></table></figure><h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = [</span><br><span class="line">    [1, 0], [1, 1], [1, 1], [1, 0], [1, 0],</span><br><span class="line">    [2, 0], [2, 1], [2, 1], [2, 2], [2, 2],</span><br><span class="line">    [3, 2], [3, 1], [3, 2], [3, 2], [3, 2]</span><br><span class="line">]</span><br><span class="line">y = [0 ,0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]</span><br><span class="line"></span><br><span class="line">estimator = NaiveBayes()</span><br><span class="line">estimator.fit(X, y)</span><br><span class="line"></span><br><span class="line">X_test = np.array([[2, 0], [1, 1]])</span><br><span class="line">y_pred = estimator.predict(X_test)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Softmax Regression</title>
      <link href="/2018/10/18/Softmax-Regression/"/>
      <url>/2018/10/18/Softmax-Regression/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" target="_blank" rel="noopener">Unsupervised Feature Learning and Deep Learning Tutorial</a></p></blockquote><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><code>Logistic Regression</code>中采用的非线性函数为<code>Sigmoid</code>，将输出值映射到$(0, 1)$之间作为概率输出，处理的是二分类问题，那么对于多分类的问题怎么处理呢？</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><blockquote><p>由<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">Logistic回归</a>推广而来</p></blockquote><h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p><code>Softmax</code>在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类$(K&gt;2)$问题，分类器最后的输出单元需要<code>Softmax</code>函数进行数值处理。</p><script type="math/tex; mode=display">S(x) = \frac            {1}            {\sum_{k=1}^K exp(x_k)}            \left[                \begin{matrix}                    exp(x_1)\\                    exp(x_2)\\                    ...\\                    exp(x_K)                \end{matrix}            \right]</script><p>其中$x$为矩阵形式的向量，其维度为$(K×1)$，$K$为类别数目。<code>Softmax</code>的输出向量维度与$x$相同，各元素$x_i$加和为$1$，可用于表示取各个类别的概率。</p><p>注意到，对于函数$e^x$</p><script type="math/tex; mode=display">\lim_{x \rightarrow - \infty} e^x = 0</script><script type="math/tex; mode=display">\lim_{x \rightarrow + \infty} e^x = +\infty</script><blockquote><p>假设所有的$x_i$等于某常数$c$，理论上对所有$x_i$上式结果为$\frac{1}{n}$</p><ul><li>若$c$为很小的负数，$e^c$下溢，结果为$NaN$；</li><li>若$c$量级很大，$e^c$上溢，结果为$NaN$。</li></ul></blockquote><p>在数值计算时并不稳定，但是<code>Softmax</code>所有输入增加同一常数时，输出不变，得稳定版本：</p><script type="math/tex; mode=display">S(x) := S(x - max(x_i))</script><blockquote><script type="math/tex; mode=display">e^{x_{max} - max(x_i)} = 1</script><ul><li>减去最大值导致$e^x$最大为$1$，排除上溢；</li><li>分母中至少有一项为$1$，排除分母下溢导致处以$0$的情况。</li></ul><p>其对数</p><script type="math/tex; mode=display">log S(x)_i = x_i - log ({\sum_{k=1}^K exp(x_k)})</script><ul><li>注意到，第一项表示输入$x_i$总是对代价函数有直接的贡献。这一项不会饱和，所以即使$x_i$对上式的第二项的贡献很小，学习依然可以进行；</li><li>当最大化对数似然时，第一项鼓励$x_i$被推高，而第二项则鼓励所有的$x$被压低；</li><li>第二项$log ({\sum_{k=1}^K exp(x_k)})$可以大致近似为$max(x_k)$，这种近似是基于对任何明显小于$max(x_k)$的$x_k$都是不重要的，<strong>负对数似然代价函数总是强烈地惩罚最活跃的不正确预测</strong></li><li>除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习</li></ul><hr><p>作者：NirHeavenX<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/qsczse943062710/article/details/61912464" target="_blank" rel="noopener">https://blog.csdn.net/qsczse943062710/article/details/61912464</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p></blockquote><h2 id="Softmax解决多分类问题"><a href="#Softmax解决多分类问题" class="headerlink" title="Softmax解决多分类问题"></a>Softmax解决多分类问题</h2><p>对于具有$K$个分类的问题，每个类别训练一组参数$ w_k $</p><script type="math/tex; mode=display">z_k^{(i)} = w_k^Tx^{(i)}</script><p>或写作矩阵形式</p><script type="math/tex; mode=display">z^{(i)} = W^Tx^{(i)}</script><p>其中</p><script type="math/tex; mode=display">x^{(i)} =     \left[        \begin{matrix}            x_0^{(i)}\\            x_1^{(i)}\\            ...\\            x_n^{(i)}        \end{matrix}    \right]_{n×1},x_0^{(i)}=1</script><script type="math/tex; mode=display">W = [w_1, w_2, ..., w_K]_{(n+1)×K}</script><script type="math/tex; mode=display">w_i =     \left[        \begin{matrix}            w_{i0}\\            w_{i1}\\            ...\\            w_{in}        \end{matrix}    \right]_{n×1}</script><p>最终各类别输出概率为</p><script type="math/tex; mode=display">\hat{y}^{(i)} = Softmax(z^{(i)})</script><blockquote><p><strong>产生了一个奇怪的脑洞。。。</strong><br>二分类问题</p><script type="math/tex; mode=display">p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }</script><p>定义二分类线性单元输出的差值为</p><script type="math/tex; mode=display">z = x_1 - x_2</script><p>得到</p><script type="math/tex; mode=display">p(x_1) = \frac{1}{1 + e^{-z}}</script><p>以$x_1 = [x_{11}, x_{12}]^T$为例(二维特征)，取$w_1=1, w_2=2, b=3$</p><script type="math/tex; mode=display">p(x_1) = \frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}}</script><p><img src="/2018/10/18/Softmax-Regression/Sigmoid_2dim.png" alt="特征为2时的决策平面"></p><p>而多分类问题，以$3$分类为例</p><script type="math/tex; mode=display">p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }</script><p>定义线性单元输出的差值为</p><script type="math/tex; mode=display">z_{12} = x_1 - x_2</script><script type="math/tex; mode=display">z_{13} = x_1 - x_3</script><script type="math/tex; mode=display">p(x_1) = \frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }</script><p>做出图像为<br><img src="/2018/10/18/Softmax-Regression/3D_sigmoid_1.png" alt="3D_sigmoid_1"><br><img src="/2018/10/18/Softmax-Regression/3D_sigmoid_2.png" alt="3D_sigmoid_2"></p></blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="由交叉熵理解"><a href="#由交叉熵理解" class="headerlink" title="由交叉熵理解"></a>由交叉熵理解</h2><script type="math/tex; mode=display">CrossEnt = \sum_j p_j log \frac{1}{q_j}</script><p>而对于样本$ (X^{(i)}, y^{(i)}) $，为确定事件，故标签概率各元素的取值$p_j$为$ y^{(i)}_j ∈ \{0,1\}$，$ q_j即预测输出的概率值\hat{y}^{(i)}_j$</p><p>一般取各个样本损失的均值$(\frac{1}{N})$</p><script type="math/tex; mode=display">L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N 1\{y^{(i)}_j=k\} log (\hat{y}^{(i)}_j)</script><script type="math/tex; mode=display">1\{y^{(i)}_j=k\} =     \begin{cases}        1 & y^{(i)}_j = k \\        0 & y^{(i)}_j \neq k     \end{cases}</script><p>可对实际标签$y^{(i)}$采取<code>One-Hot</code>编码，便于计算</p><script type="math/tex; mode=display">y^{(i)} = \left[         \begin{matrix}            0, ..., 1_{y^{(i)}}, ..., 0        \end{matrix}     \right]^T</script><p>则</p><script type="math/tex; mode=display">L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N y^{(i)T}log (\hat{y}^{(i)})</script><h2 id="由决策平面理解"><a href="#由决策平面理解" class="headerlink" title="由决策平面理解"></a>由决策平面理解</h2><p>从<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>和<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a>可知，对于类别$c_i$，有</p><script type="math/tex; mode=display">P(c_i|x) = \frac{P(x|c_i)}{\sum_{j=0}^KP(x|c_j)}</script><blockquote><p>假设每个类别的样本服从正态分布，先验概率相等，各类别样本特征间协方差相等。证明略.</p></blockquote><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><h2 id="Softmax函数的导数"><a href="#Softmax函数的导数" class="headerlink" title="Softmax函数的导数"></a>Softmax函数的导数</h2><p>对于</p><script type="math/tex; mode=display">S(x) = \frac            {1}            {\sum_{k=1}^K exp(x_k)}            \left[                \begin{matrix}                    exp(x_1)\\                    exp(x_2)\\                    ...\\                    exp(x_K)                \end{matrix}            \right]</script><p>一般输出作为概率值，记</p><script type="math/tex; mode=display">P = S(x)</script><script type="math/tex; mode=display">p_i = S(x)_i</script><p>对向量$x$中某元素求导</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i} = \frac{∂}{∂x_i}                    \left[                        \begin{matrix}                            ...\\                            \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}\\                            ...\\                        \end{matrix}                    \right]</script><blockquote><p>$(1)$ $i=k$<br>$<br>\frac{∂}{∂x_i} \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)}$<br>$ = \frac{exp’(x_i)·\sum_{j=1}^K exp(x_j) - exp(x_i)·(\sum_{j=1}^K exp(x_j))’}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{exp(x_i)·\sum_{j=1}^K exp(x_j) - exp^2(x_i)}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)} -<br>(\frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)})^2<br>$<br>$ = p_i (1 - p_i)<br>$</p><p>$(2)$ $i\neq k$<br>$<br>\frac{∂}{∂x_i} \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}$<br>$ = \frac{exp’(x_k)·\sum_{j=1}^K exp(x_j) - exp(x_k)·(\sum_{j=1}^K exp(x_j))’}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{- exp(x_k)exp(x_i)}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$= - p_i p_k$</p><p>综上</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i}_{K×1} =  \left[                          \begin{matrix}                              0\\                              ...\\                              p_i\\                              ...\\                              0                           \end{matrix}                      \right] -                       \left[                          \begin{matrix}                                p_i p_1\\                                ...\\                                p_i^2\\                                ...\\                                p_i p_K                            \end{matrix}                      \right]=     \left(                      \left[                          \begin{matrix}                              0\\                              ...\\                              1\\                              ...\\                              0                           \end{matrix}                      \right] -                       p      \right)p_i</script></blockquote><h2 id="损失函数梯度"><a href="#损失函数梯度" class="headerlink" title="损失函数梯度"></a>损失函数梯度</h2><p>在<code>OneHot</code>编码下，损失函数形式为</p><script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{N} \sum_{i=1}^N L (y^{(i)}, \hat{y}^{(i)})</script><script type="math/tex; mode=display">L (y^{(i)}, \hat{y}^{(i)}) = - y^{(i)T}log \hat{y}^{(i)}</script><script type="math/tex; mode=display">\hat{y}^{(i)} = S(z^{(i)})</script><script type="math/tex; mode=display">z^{(i)} = W^T x^{(i)}</script><p>即只考虑实际分类对应的概率值</p><script type="math/tex; mode=display">L (y^{(i)}, \hat{y}^{(i)}) = - log \hat{y}^{(i)}_{y^{(i)}}</script><blockquote><p>由于 $S(z^{(i)})_{t^{(i)}}$与$z^{(i)}$向量各个元素都有关，由链式求导法则</p><script type="math/tex; mode=display">\frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } (\sum_{k=1}^K  \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}  \frac{∂z^{(i)}_k}{∂w_{pq}})</script><p>$1.$ 考察 $\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}$</p><script type="math/tex; mode=display">  \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} = ​      \begin{cases}​          \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_k) & k=y^{(i)} \\​          - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_k & k \neq y^{(i)} ​      \end{cases}</script><p>$2.$ 考察 $\frac{∂z^{(i)}_k}{∂w_{pq}}$</p><script type="math/tex; mode=display">  \frac{∂z^{(i)}_k}{∂w_{pq}} =       \begin{cases}            \frac{∂z^{(i)}_k}{∂w_{pq}} = x^{(i)}_p & k=q\\            \frac{∂z^{(i)}_k}{∂w_{pq}} = 0 & k \neq q        \end{cases}</script></blockquote><p>综上所述</p><script type="math/tex; mode=display">\frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} }   \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}  \frac{∂z^{(i)}_q}{∂w_{pq}}</script><p>其中</p><script type="math/tex; mode=display">\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}= \begin{cases}        \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_q) & q = y^{(i)}\\        - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_q & q \neq y^{(i)}    \end{cases}</script><script type="math/tex; mode=display">\frac{∂z^{(i)}_q}{∂w_{pq}} = x^{(i)}_p</script><p>故对于单个样本$(X^{(i)}, y^{(i)})$，当样本标签采用$OneHot$编码时</p><script type="math/tex; mode=display">\frac{∂L^{(i)}}{∂w_{pq}}  = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} }   \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}  x^{(i)}_p= \begin{cases}    (\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\    \hat{y}^{(i)}_qx^{(i)}_p & q \neq y^{(i)}\end{cases}</script><blockquote><p>注： 这里可以约分去掉$\hat{y}^{(i)}_{y^{(i)}}$</p></blockquote><script type="math/tex; mode=display">\frac{∂L^{(i)}}{∂w_{pq}} = ( \hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_p</script><p>更一般的，写成矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">∇_W L = X^T(\hat{Y} - Y)</script><blockquote><p><strong>用线性模型解决分类和回归问题时，形式竟如此统一!</strong></p></blockquote><p>至此为止，梯度推导结束，利用梯度下降法迭代求解参数矩阵$W$即可。</p><script type="math/tex; mode=display">W := W - \alpha ∇_W L</script><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex3-2-softmax_regression" target="_blank" rel="noopener">@GitHub: Code of Softmax Regression</a></p><h2 id="Softmax-1"><a href="#Softmax-1" class="headerlink" title="Softmax"></a>Softmax</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def softmax(X):</span><br><span class="line">    &quot;&quot;&quot; 数值计算稳定版本的softmax函数</span><br><span class="line">    @param &#123;ndarray&#125; X: shape(batch_size, n_labels)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X_max = np.max(X, axis=1).reshape((-1, 1))  # 每行的最大值</span><br><span class="line">    X = X - X_max                        # 每行减去最大值</span><br><span class="line">    X = np.exp(X)</span><br><span class="line">    return X / np.sum(X, axis=1).reshape((-1, 1))</span><br></pre></td></tr></table></figure><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def crossEnt(self, y_label_true, y_prob_pred):</span><br><span class="line">    &quot;&quot;&quot; 计算交叉熵损失函数</span><br><span class="line">    @param &#123;ndarray&#125; y_label_true: 真实标签 shape(batch_size,)</span><br><span class="line">    @param &#123;ndarray&#125; y_prob_pred: 预测输出 shape(batch_size, n_labels)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    mask = self.encoder.transform(y_label_true.reshape(-1, 1)).toarray()  # shape(batch_size, n_labels)</span><br><span class="line">    y_prob_masked = np.sum(mask * y_prob_pred, axis=1)          # 每行真实标签对应的预测输出值</span><br><span class="line">    y_prob_masked[y_prob_masked==0.] = 1.</span><br><span class="line">    y_loss = np.log(y_prob_masked)</span><br><span class="line">    loss = - np.mean(y_loss)                                    # 求各样本损失的均值</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure><h2 id="gradient"><a href="#gradient" class="headerlink" title="gradient"></a>gradient</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def grad(self, X_train, y_train, y_prob_pred):</span><br><span class="line">    &quot;&quot;&quot; 计算梯度 \frac &#123;∂L&#125; &#123;∂W_&#123;pq&#125;&#125;</span><br><span class="line">    @param X_train: 训练集特征</span><br><span class="line">    @param y_train: 训练集标签</span><br><span class="line">    @param y_prob_pred:  训练集预测概率输出</span><br><span class="line">    @param y_label_pred: 训练集预测标签输出</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    y_train = self.encoder.transform(y_train)</span><br><span class="line">    dW = X_train.T.dot(y_prob_pred - y_train)</span><br><span class="line">    return dW</span><br></pre></td></tr></table></figure><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><p>省略可视化和验证部分的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">def fit(self, X_train, X_valid, y_train, y_valid, min_acc=0.95, max_epoch=20, batch_size=20):</span><br><span class="line">    &quot;&quot;&quot; 训练</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 添加首1列，输入到偏置w0</span><br><span class="line">    X_train = np.c_[np.ones(shape=(X_train.shape[0],)), X_train]</span><br><span class="line">    X_valid = np.c_[np.ones(shape=(X_valid.shape[0],)), X_valid]</span><br><span class="line">    X_train = self.scaler.fit_transform(X_train)    # 尺度归一化</span><br><span class="line">    X_valid = self.scaler.transform(X_valid)        # 尺度归一化</span><br><span class="line">    self.encoder.fit(y_train.reshape(-1, 1))</span><br><span class="line">    self.n_features = X_train.shape[1]</span><br><span class="line">    self.n_labels = self.encoder.transform(y_train).shape[1]</span><br><span class="line">    # 初始化参数</span><br><span class="line">    self.W = np.random.normal(loc=0, scale=1.0, size=(self.n_features, self.n_labels))</span><br><span class="line">    n_batch = X_train.shape[0] // batch_size</span><br><span class="line">        </span><br><span class="line">    # 可视化相关</span><br><span class="line">    plt.ion()</span><br><span class="line">    plt.figure(&apos;loss&apos;); plt.figure(&apos;accuracy&apos;)</span><br><span class="line">    loss_train_epoch = []; loss_valid_epoch = []</span><br><span class="line">    acc_train_epoch = [];  acc_valid_epoch = []</span><br><span class="line">    for i_epoch in range(max_epoch):</span><br><span class="line">        for i_batch in range(n_batch):              # 批处理梯度下降</span><br><span class="line">            n1, n2 = i_batch * batch_size, (i_batch + 1) * batch_size</span><br><span class="line">            X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2]</span><br><span class="line">            # 预测</span><br><span class="line">            y_prob_train = self.predict(X_train_batch, preprocessed=True)</span><br><span class="line">            # 计算损失</span><br><span class="line">            loss_train_batch = self.crossEnt(y_train_batch, y_prob_train)</span><br><span class="line">            # 计算准确率</span><br><span class="line">            y_label_train = np.argmax(y_prob_train, axis=1)</span><br><span class="line">            a = y_train_batch.reshape((-1,))</span><br><span class="line">            acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((-1,))).astype(&apos;float&apos;))</span><br><span class="line">            # 计算梯度 dW</span><br><span class="line">            dW = self.grad(X_train_batch, y_train_batch, y_prob_train)</span><br><span class="line">            # 更新参数</span><br><span class="line">            self.W -= self.lr * dW</span><br></pre></td></tr></table></figure></p><h2 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def predict(self, X, preprocessed=False):</span><br><span class="line">    &quot;&quot;&quot; 对输入的样本进行预测，输出标签</span><br><span class="line">    @param &#123;ndarray&#125; X: shape(batch_size, n_features)</span><br><span class="line">    @return &#123;ndarray&#125; y_prob: probability, shape(batch_size, n_labels)</span><br><span class="line">            &#123;ndarray&#125; y_label: labels, shape(batch_size,)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if not preprocessed:    # 训练过程中调用此函数时，不用加首1列</span><br><span class="line">        X = np.c_[np.ones(shape=(X.shape[0],)), X]              # 添加首1项，输入到偏置w0</span><br><span class="line">    X = self.scaler.transform(X)</span><br><span class="line"></span><br><span class="line">    y_prob = softmax(X.dot(self.W))                             # 预测概率值 shape(batch_size, n_labels)</span><br><span class="line">    return y_prob</span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>以下蓝线为训练集参数，红线为验证集参数，若稳定训练(如<code>batch_size = 20</code>的结果)，最终准确率在$80\%$左右。</p><blockquote><ul><li>由于<code>随机梯度下降(SGD)</code>遍历次数太多，运行较慢，没有用<code>SGD</code>方法训练，就前几个<code>epoch</code>来看，效果没有<code>batch_size = 20</code>的好；</li><li>添加隐含层形成三层结构的<code>前馈神经网络</code>，可提高准确率；</li><li>还有一点，使用<code>批处理梯度下降(n_batch = 1)</code>训练时，可以看到损失值已经趋于$0$，但准确率却很低，说明已经陷入局部最优解。</li></ul></blockquote><ul><li><p>batch size = 20</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batchsize_20.png" alt="loss_batchsize_20"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batchsize_20.png" alt="accuracy_batchsize_20"></li></ul></li><li><p>batch_size = 200</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batchsize_200.png" alt="loss_batchsize_200"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batchsize_200.png" alt="accuracy_batchsize_200"></li></ul></li><li><p>n_batch = 1</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batch_1.png" alt="loss_batch_1"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batch_1.png" alt="accuracy_batch_1"></li></ul></li></ul><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>推公式要我老命。。。。</p><p><code>Softmax</code>回归可以视作<strong>不含隐含层的<a href="https://louishsu.xyz/2018/10/20/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">前馈神经网络</a></strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression</title>
      <link href="/2018/10/18/Logistic-Regression/"/>
      <url>/2018/10/18/Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>先给出模型，推导过程稍后给出，逻辑回归包含<code>Sigmoid</code>函数</p><script type="math/tex; mode=display">f(z) = \frac{1}{1+e^{-z}}</script><p>其图像如下<br><img src="/2018/10/18/Logistic-Regression/Sigmoid.png" alt="`Sigmod函数`"></p><p>定义</p><script type="math/tex; mode=display">z = w^Tx</script><p>其中$x=[x_0, x_1, …, x_n]^T, x_0=1$</p><script type="math/tex; mode=display">h_w(x) = g(z) =  \frac{1}{1+e^{-z}}</script><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="由最大似然估计推导"><a href="#由最大似然估计推导" class="headerlink" title="由最大似然估计推导"></a>由最大似然估计推导</h2><p>对于二元分类问题，其取值作为随机变量，服从二项分布 $B(1, p)$，其中$p$即为预测输出概率$\hat{y}$</p><script type="math/tex; mode=display">P(y_i^{(i)}) = (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}</script><p>由极大似然估计</p><script type="math/tex; mode=display">L = \prod_{i=0}^N P(y_i^{(i)}) = \prod_{i=0}^N (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}</script><p>取对数似然函数</p><script type="math/tex; mode=display">logL = \sum_{i=0}^N [y_i^{(i)} log \hat{y}_i^{(i)} + (1-y_i^{(i)}) log (1-\hat{y}_i^{(i)})]</script><p>优化目标是</p><script type="math/tex; mode=display">w = argmax_w logL</script><p>优化问题一般表述成<code>minimize</code>问题，添加负号，构成<code>Neg Log Likelihood</code>损失</p><script type="math/tex; mode=display">w = argmin_w (-logL)</script><p>一般取均值</p><script type="math/tex; mode=display">L(\hat{y}, y)=- \frac{1}{N} \sum_i [y_i^{(i)} log(\hat{y}_i^{(i)})+(1 - y_i^{(i)})log(1-\hat{y}_i^{(i)})]</script><p>其中$y_i$表示真实值，$\hat{y}_i$表示预测值</p><h2 id="从交叉熵理解"><a href="#从交叉熵理解" class="headerlink" title="从交叉熵理解"></a>从交叉熵理解</h2><p>已知交叉熵<code>cross entropy</code>定义如下</p><script type="math/tex; mode=display">CrossEnt = \sum_i p_i log \frac{1}{q_i}</script><p>而对于样本$ (X_i, y_i) $，为确定事件，故标签概率的取值为$ p_i = y_i ∈ \{0,1\}$，$ q_i即预测输出的概率值\hat{y}_i $，可得到与上面相同的推导结论</p><h2 id="从决策平面和贝叶斯决策理解"><a href="#从决策平面和贝叶斯决策理解" class="headerlink" title="从决策平面和贝叶斯决策理解"></a>从决策平面和贝叶斯决策理解</h2><p>相关内容查看<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a>和<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>，逻辑回归考虑的一般是等先验概率问题，故决策函数定义为</p><blockquote><p>$if$ $P(c_i|x)&gt;P(c_j|x)$ $then$ $ x \in c_i $,  $ i, j = 1, 2 $</p></blockquote><p>从贝叶斯决策可知，对于类别$c_1$，有</p><script type="math/tex; mode=display">P(c_1|x) = \frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}</script><p>设在各个类别下，特征$x$服从正态分布</p><script type="math/tex; mode=display">P(x|c_i) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}}exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i))</script><p>则</p><script type="math/tex; mode=display">P(c_1|x) = \frac{1}{    1 + exp(-z)}</script><script type="math/tex; mode=display">P(c_2|x) = 1 - P(c_1|x) = \frac{exp(-z)}{1+exp(-z)}</script><blockquote><p>$<br>P(c_1|x) = \frac<br>{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}<br>{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}<br>$</p><p>$<br>P(c_1|x) = \frac<br>{1}<br>{1 + \frac{exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}<br>}<br>$</p><p>假定各分类的样本方差相等，$ \Sigma_1 = \Sigma_2 = \sigma^2 I $</p><p>$ P(c_1|x) = \frac {1}{1 + exp(- [ \frac{1}{\sigma^2} (\mu_1-\mu_2)^T x - \frac{1}{2 \sigma^2} (\mu_1^T\mu_1 - \mu_2^T\mu_2) ])}<br>$</p><p>令</p><script type="math/tex; mode=display">w = \frac{1}{\sigma^2} (\mu_1 -\mu_2)</script><script type="math/tex; mode=display">b = - \frac{1}{2\sigma^2}(\mu_1^T \mu_1 - \mu_2^T \mu_2)</script><p>即可得到</p><script type="math/tex; mode=display">P(c_1|x) = \frac{1}{    1 + exp(-z)}</script><p>其中</p><script type="math/tex; mode=display">z = w^T x + b</script></blockquote><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><p>先推导<code>Sigmoid</code>函数的导数</p><script type="math/tex; mode=display">f'(z) = (1 - f(z))f(z)</script><p>值得注意的是，从$f’(z)$的图像可以看到，在$ x=0 $处$f’(z)$取极大值，且</p><script type="math/tex; mode=display">f'(z)_{max} = f'(z)|_{z=0} = 0.25</script><script type="math/tex; mode=display">\lim_{z \rightarrow \infty} f'(z) = 0</script><p>在多层神经网络反向传播更新参数时，由于梯度多次累乘，<code>Sigmoid</code>作为<a href="https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">激活函数</a>会存在“梯度消失”的问题，使得参数更新非常缓慢。</p><p><img src="/2018/10/18/Logistic-Regression/Sigmoid_gradient.png" alt="`Sigmod导函数`"></p><blockquote><p>$ f’(z) $<br>$ = (\frac{1}{1+e^{-z}})’ $<br>$ = \frac<br>​          {-(1+e^{-z})’}<br>​          {(1+e^{-z})^2} $<br>$ = \frac<br>​          {e^{-z}}<br>​          {(1+e^{-z})^2} $<br>$ = \frac<br>​          {e^{-z}}<br>​          {1+e^{-z}}<br>​    \frac<br>​          {1}<br>​          {1+e^{-z}}$<br>$ = (1 - f(z))f(z)$</p></blockquote><p>利用链式求导法则可得</p><blockquote><p>$\frac{∂L}{∂w_j}$<br>$= -\frac{∂}{∂w_j} \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})]$<br>$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\frac{∂}{∂w_j}\hat{y}^{(i)}-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\frac{∂}{∂w_j}\hat{y}^{(i)}]$<br>$= - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})w_j]$<br>$= - \frac{1}{N} \sum_i [y^{(i)} (1-\hat{y}^{(i)})w_j-(1-y^{(i)}) y^{(i)} w_j]$<br>$=  \frac{1}{N} \sum_i (\hat{y}^{(i)} - y^{(i)})w_j $</p></blockquote><p>写作矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">∇_w L = X^T (\hat{Y} - Y)</script><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>和线性回归一样，采用梯度下降法求解</p><script type="math/tex; mode=display">w := w - \alpha ∇_w L</script><h1 id="处理多分类问题"><a href="#处理多分类问题" class="headerlink" title="处理多分类问题"></a>处理多分类问题</h1><p>假设有$K$个类别，则依次以类别$c_i$为正样本训练模型，一共训练$K$个。测试样本在每个模型上计算，最终将概率最大的作为分类结果。</p><blockquote><p>这样划分数据集，会使训练集正负样本数目严重不对称，特别是类别很多的情况，对结果会产生影响。可推广至<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">softmax回归</a>解决这个问题。</p></blockquote><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex2-logisticregression/LogReg.py" target="_blank" rel="noopener">@Github: Code for Logistic Regression</a></p><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def lossFunctionDerivative(self, X, theta, y_true):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    计算损失函数对参数theta的梯度</span><br><span class="line">    对theta[j]的梯度为：(y_pred - y_true)*x[j]</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    err = self.predict_prob(X, theta) - y_true</span><br><span class="line">    return X.T.dot(err)/y_true.shape[0]</span><br><span class="line">def lossFunction(self, y_pred_prob, y_true):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    未使用</span><br><span class="line">    计算损失值: Cross-Entropy</span><br><span class="line">    y_pred_prob, y_true: NumPy array, shape=(n,)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    tmp = y_true*np.log(y_pred_prob) + (1 - y_true)*np.log(1 - y_pred_prob)</span><br><span class="line">    return np.mean(-tmp)</span><br></pre></td></tr></table></figure><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def gradDescent(self, min_acc, learning_rate=0.01, max_iter=10000):</span><br><span class="line">    acc = 0; n_iter = 0</span><br><span class="line">    for n_iter in range(max_iter):</span><br><span class="line">        for n in range(self.n_batch):</span><br><span class="line">            X_batch = self.X[n*self.batch_size:(n+1)*self.batch_size]</span><br><span class="line">            t_batch = self.t[n*self.batch_size:(n+1)*self.batch_size]</span><br><span class="line">            grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch)</span><br><span class="line">            self.theta -= learning_rate * grad # 梯度下降</span><br><span class="line">            acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t)</span><br><span class="line">            if acc &gt; min_acc:</span><br><span class="line">                print(&apos;第%d次迭代, 第%d批数据&apos; % (n_iter, n))</span><br><span class="line">                print(&quot;当前总体样本准确率为: &quot;, acc)</span><br><span class="line">                print(&quot;当前参数值为: &quot;, self.theta)</span><br><span class="line">                return self.theta</span><br><span class="line">        if n_iter%100 == 0:</span><br><span class="line">            print(&apos;第%d次迭代&apos; % n_iter)</span><br><span class="line">            print(&apos;准确率： &apos;, acc)</span><br><span class="line">    print(&quot;超过迭代次数&quot;)</span><br><span class="line">    print(&quot;当前总体样本准确率为: &quot;, acc)</span><br><span class="line">    print(&quot;当前参数值为: &quot;, self.theta)</span><br><span class="line">    return self.theta</span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/2018/10/18/Logistic-Regression/logistic_regression_result.png" alt="实验结果"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2018/10/18/Linear-Regression/"/>
      <url>/2018/10/18/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>线性回归可以说是机器学习最基础的算法</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><script type="math/tex; mode=display">\hat{y}^{(i)} = w^Tx^{(i)}</script><p>其中</p><script type="math/tex; mode=display">x^{(i)}=[x_0^{(i)}, x_1^{(i)}, ..., x_n^{(i)}]^T, x_0^{(i)}=1</script><p>这里$x_0^{(i)}=1$表示偏置$b$，即$b=w_0$</p><script type="math/tex; mode=display">\hat{y}^{(i)} = w^Tx^{(i)} + b</script><blockquote><p><code>注</code>：对于非线性的数据，可构造高次特征。</p></blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="定义误差"><a href="#定义误差" class="headerlink" title="定义误差"></a>定义误差</h2><script type="math/tex; mode=display">e^{(i)} = \hat{y}^{(i)} - y^{(i)}</script><p>其中$y^{(i)}$表示真实值</p><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>单个样本的误差定义为</p><script type="math/tex; mode=display">L_{single}(\hat{y}^{(i)}, y^{(i)})=\frac{1}{2}||e^{(i)}||_2^2=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2</script><p>所有样本的误差定义为</p><script type="math/tex; mode=display">L(y, t)=\frac{1}{2N}\sum_i (\hat{y}^{(i)}-y^{(i)})^2</script><p>也可以定义为误差的和而不是均值，对结果无影响，可视作学习率$α$除去一个常数</p><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><blockquote><p>$\frac{∂L}{∂w_j}$<br>$= \frac{∂}{∂w_j}\frac{1}{2N}\sum_i(\hat{y}^{(i)}-y^{(i)})^2$<br>$= \frac{1}{2N} \sum_i \frac{∂}{∂w_j} (\hat{y}^{(i)}-y^{(i)})^2$<br>$= \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) \frac{∂t^{(i)}}{∂w_j}$<br>$=  \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)}$</p></blockquote><p>或者使用矩阵推导，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">L = \frac{1}{2}(Xw-Y)^T(Xw-Y)</script><script type="math/tex; mode=display">∇_w L = X^T(\hat{Y}-Y)</script><blockquote><p>$∇_w L$<br>$= \frac{1}{2} ∇_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY)$<br>$= \frac{1}{2} (2X^TXw - X^TY - X^TY)$<br>$= X^T(Xw-Y) $</p></blockquote><p>在梯度为$\vec{0}$的点，即$∇_w L = \vec{0}$时对应最优解</p><script type="math/tex; mode=display">X^T(Xw-Y) = 0</script><blockquote><p>令<script type="math/tex">X^T(Xw-Y) = 0</script></p><p>有<script type="math/tex">X^TXw = X^TY</script></p><script type="math/tex; mode=display">w^*=(X^TX+\lambda I)^{-1}X^TY</script></blockquote><p>其中$X^+=(X^TX+\lambda I)^{-1}X^T$，表示矩阵$X_{m×n}$的伪逆</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>采用梯度下降法求解</p><script type="math/tex; mode=display">w := w - \alpha ∇_w L</script><p>其中$w$表示参数向量</p><blockquote><p>进一步思考：为什么使用梯度下降可以求取最优解呢？</p><script type="math/tex; mode=display">∇_w^2 L = ∇_w X^T(Xw-Y) = X^TX</script><p>而对于矩阵 $ X^TX $</p><script type="math/tex; mode=display">u^T(X^TX)u = (Xu)^T(Xu) \geq 0</script><p>即损失函数的<code>Hessian</code>矩阵$∇_w^2 L$为正定矩阵，$L$为凸函数，存在全局最优解</p></blockquote><h1 id="从投影的角度理解线性回归"><a href="#从投影的角度理解线性回归" class="headerlink" title="从投影的角度理解线性回归"></a>从投影的角度理解线性回归</h1><p><img src="/2018/10/18/Linear-Regression/projection_linreg2.png" alt="投影理解"></p><p><img src="/2018/10/18/Linear-Regression/projection_linreg3.png" alt="用投影推导最优解"></p><h1 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h1><p>为克服过拟合问题，可加入正则化项$||w||_2^2$，此时损失函数定义为</p><script type="math/tex; mode=display">L(\hat{y}, y)=\frac{1}{2N} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_2^2</script><p>或者</p><script type="math/tex; mode=display">L(\hat{y}, y)=\frac{1}{2N} \sum_i (\hat{y}^{(i)}-y^{(i)})^2 +  \frac{\lambda}{2N}\sum_j w_j^2</script><p>其中$i = 1, …, N_{sample}; j = 1, …, N_{feature},j&gt;0 $</p><p>此时梯度为</p><script type="math/tex; mode=display">\frac{∂L}{∂w_j} = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \frac{\lambda}{N}w_j</script><p>其中$j = 1, …, N_{feature},j&gt;0 $</p><h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><p>目标函数定义为</p><script type="math/tex; mode=display">L(y, t)=\frac{1}{2N}\sum_i w^{(i)} (\hat{y}^{(i)}-y^{(i)})^2</script><p>其中</p><script type="math/tex; mode=display">w^{(i)} = e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}</script><p>$x$表示输入的预测样本，$x^{(i)}$表示训练样本</p><p><div style="align: center"><img src="/2018/10/18/Linear-Regression/w_i_x_i.png"></div><br>离很近的样本，权值接近于1，而对于离很远的样本，此时权值接近于0，这样就是在局部构成线性回归，它依赖的也只是周边的点。</p><p>对于线性回归算法，一旦拟合出适合训练数据的参数$w$，保存这些参数$w$，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。而对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数$w$），没有固定的参数$w$，所以是非参数算法。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex5-regularizedllinearregression" target="_blank" rel="noopener">@Github: Code for Linear Regression</a></p><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def fit(self, X, y, learning_rate=0.01, max_iter=5000, min_loss=10):</span><br><span class="line">    # --------------- 数据预处理部分 ---------------</span><br><span class="line">    # 加入全1列</span><br><span class="line">    X = np.c_[np.ones(shape=(X.shape[0])), X]</span><br><span class="line">    # 构造高次特征</span><br><span class="line">    if self.n_ploy &gt; 1:</span><br><span class="line">        for i in range(2, self.n_ploy + 1):</span><br><span class="line">            X = np.c_[X, X[:, 1]**i]</span><br><span class="line">    # ---------------- 参数迭代部分 ----------------</span><br><span class="line">    # 初始化参数</span><br><span class="line">    self.theta = np.random.uniform(-1, 1, size=(X.shape[1],))</span><br><span class="line">    # 数据批次</span><br><span class="line">    n_batch = X.shape[0] if self.n_batch==-1 else self.n_batch</span><br><span class="line">    batch_size = X.shape[0] // n_batch</span><br><span class="line">    # 停止条件</span><br><span class="line">    n_iter = 0; loss = float(&apos;inf&apos;)</span><br><span class="line">    # 开始迭代</span><br><span class="line">    for n_iter in range(max_iter):</span><br><span class="line">        for n in range(n_batch):</span><br><span class="line">            n1, n2 = n*batch_size, (n+1)*batch_size</span><br><span class="line">            X_batch = X[n1: n2]; y_batch = y[n1: n2]</span><br><span class="line">            </span><br><span class="line">            grad = self.lossFunctionDerivative(X_batch, y_batch)</span><br><span class="line">            self.theta -= learning_rate * grad</span><br><span class="line">            </span><br><span class="line">            loss = self.score(y_batch, self.predict(X_batch))</span><br><span class="line">            if loss &lt; min_loss:</span><br><span class="line">                print(&apos;第%d次迭代, 第%d批数据&apos; % (n_iter, n))</span><br><span class="line">                print(&quot;当前总体样本损失为: &quot;, loss)</span><br><span class="line">                return self.theta</span><br><span class="line">        if n_iter%100 == 0:</span><br><span class="line">            print(&apos;第%d次迭代&apos; % n_iter)</span><br><span class="line">            print(&quot;当前总体样本损失为: &quot;, loss)</span><br><span class="line">    print(&quot;超过迭代次数&quot;)</span><br><span class="line">    print(&quot;当前总体样本损失为: &quot;, loss)</span><br><span class="line">    return self.theta</span><br><span class="line"></span><br><span class="line">def lossFunctionDerivative(self, X, y):</span><br><span class="line">    y_pred = self.predict(X)</span><br><span class="line">    # theta = self.theta;     # ！注意：theta = self.theta 不仅仅是赋值，类似引用，修改theta会影响self.theta</span><br><span class="line">    theta = self.theta.copy()</span><br><span class="line">    theta[0] = 0            # θ0不需要正则化</span><br><span class="line">    return (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[0]</span><br></pre></td></tr></table></figure><h2 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def predict(self, X, preprocessed=False):</span><br><span class="line">    if preprocessed:</span><br><span class="line">        # 加入全1列</span><br><span class="line">        X = np.c_[np.ones(shape=(X.shape[0])), X]</span><br><span class="line">        # 构造高次特征</span><br><span class="line">        if self.n_ploy &gt; 1:</span><br><span class="line">            for i in range(2, self.n_ploy + 1):</span><br><span class="line">                X = np.c_[X, X[:, 1]**i]</span><br><span class="line">    return X.dot(self.theta)</span><br></pre></td></tr></table></figure><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><ul><li><p>无正则化<br>  <img src="/2018/10/18/Linear-Regression/result_linreg_noreg.png" alt="无正则化的线性回归结果"></p></li><li><p>正则化<br>  <img src="/2018/10/18/Linear-Regression/result_linreg_reg.png" alt="正则化的线性回归结果"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
