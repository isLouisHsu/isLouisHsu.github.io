<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【算法】回溯法与分支定界法</title>
      <link href="/2020/03/23/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%9B%9E%E6%BA%AF%E6%B3%95%E4%B8%8E%E5%88%86%E6%94%AF%E5%AE%9A%E7%95%8C%E6%B3%95/"/>
      <url>/2020/03/23/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%9B%9E%E6%BA%AF%E6%B3%95%E4%B8%8E%E5%88%86%E6%94%AF%E5%AE%9A%E7%95%8C%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="回溯法"><a href="#回溯法" class="headerlink" title="回溯法"></a>回溯法</h1><p><strong>回溯法</strong>(backtracking)是对候选集进行系统检查的两种方法，该算法策略有以下几步</p><ol><li>定义问题的<strong>解空间</strong>(solution space)，这个空间至少包含一个问题的(最优的)解；</li><li><strong>组织解空间</strong>，使解空间便于搜索，典型的组织方法是图和树；<br> 例如<strong>迷宫老鼠问题</strong>：指定$3 \times 3$的迷宫，部分位置不可通行，求解最短的通行路径。该问题如果用树描述，那么不考虑是否能通行，每个节点处有$4$个抉择，树结构较大，用图描述较为简便，如下<br> <img src="/2020/03/23/【算法】回溯法与分支定界法/maze_eg.jpg" alt="maze_eg"></li><li>在确定解空间的组织方法后，这个空间可以按<strong>深度优先方式</strong>进行搜索。<ol><li>初始化<strong>E-结点</strong>(expansion node)为起始点，标记搜索过程中经过的节点为<strong>活动节点</strong>(live node)，那么起始点也是一个活动节点；</li><li>在E-节点处，尝试可能的选择节点并移动：1) <strong>若存在这样的节点</strong>，将当前E-节点新到达节点成为E-节点，并标记新节点也为活动节点；2)<strong>若不存在这样的节点</strong>(<strong>没有可供选择的节点，或超过约束限制</strong>)，将当前E-节点标记为死节点(用来杀死该节点的策略称为<strong>界定函数</strong>(bounding function))，并将E-节点返回至最近的活动节点(<strong>回溯</strong>)；</li><li>循环步骤2直至遍历节空间，在搜索过程中，实时更新最优解。</li></ol></li></ol><blockquote><p>以迷宫老鼠为例，初始节点$(1, 1)$标记为活动节点，同时也为E-节点，在该节点处可达节点为$(1,2),(2,1)$，那么标记$(1,2)$为活动节点，E-节点更新为$(1,2)$。当E-节点为$(1,3)$时，该节点无可达节点(即界定函数)，标记为死结点，并更新E-节点为最近的活动节点，即$(1,2)$，此时该节点也被标记为死结点，E-节点更新为$(1,1)$。此时存在可达节点$(2,1)$，将其标记为活动节点，并更新E-节点。以此类推，当E-节点为出口$(3,3)$时，结束算法。<br>注：此时活动节点有$(1,1), (2,1), (3,1), (3,2), (3,3)$。<br><img src="/2020/03/23/【算法】回溯法与分支定界法/maze_dfs.jpg" alt="maze_dfs"></p></blockquote><p>注意区别回溯法和动态规划，两者有一定程度的相似性。动态规划是自底向上的，回溯法是自顶向下的。</p><p><strong>例(旅行商问题, TSP)：给定一个$n$顶点的有权网络(有向或无向)，找出一个包含$n$个顶点且具有最小耗费的环路，例如给定下图的网络：</strong><br><img src="/2020/03/23/【算法】回溯法与分支定界法/tsp_eg.jpg" alt="tsp_eg"></p><p>该问题的<strong>约束条件</strong>是：1) 环路包含$n$个节点；2) 节点间拓扑结构已确定。<strong>优化目标</strong>是环路耗费最低，用矩阵描述该图为</p><script type="math/tex; mode=display">\begin{bmatrix}    \infty & 30 & 35 & \infty & 20 \\    30 & \infty & 20 & 10 & \infty \\    35 & 20 & \infty & 5 & 15 \\    \infty & 10 & 5 & \infty & 25 \\    20 & \infty & 15 & 25 & \infty \\\end{bmatrix}</script><p>考虑分支问题可以用栈来解决，用递归的形式<strong>借助函数调用栈</strong>，求解指定节点$i$所能到达死结点的路径。考虑到已经过节点不能再次经过，<strong>将能到达$i$的路径全部删除(置为无穷大)</strong>。但采用函数调用栈可能导致函数调用层级很深而溢出。</p><blockquote><p>用下列函数求解得<code>(80.0, [0, 1, 3, 2, 4])</code>，即路径为$1 \rightarrow 2 \rightarrow 4 \rightarrow 3 \rightarrow 5 \rightarrow 1$，最小耗费为$80$。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solveTSP</span><span class="params">(A)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取从节点0开始的，所有最深路径</span></span><br><span class="line">    path = getPath(A, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除不包含所有节点的路径</span></span><br><span class="line">    path = list(filter(<span class="keyword">lambda</span> x: len(x) == <span class="number">5</span>, path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每条路径的成本</span></span><br><span class="line">    cost = list(map(<span class="keyword">lambda</span> x: getCost(A, x), path))</span><br><span class="line">    </span><br><span class="line">    minCost = min(cost)</span><br><span class="line">    minPath = path[cost.index(minCost)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> minCost, minPath</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPath</span><span class="params">(A, i)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">        i: &#123;int&#125;</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        path: &#123;list[list]&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        - 深度优先遍历；</span></span><br><span class="line"><span class="string">        - 返回A为图下，以节点i为起始的最深路径，包含i；</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = A.shape[<span class="number">0</span>]</span><br><span class="line">    path = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算i能到达的每个节点的最深路径</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> np.isinf(A[i, j]): <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        B = A.copy(); </span><br><span class="line">        B[:, i] = float(<span class="string">'inf'</span>)      <span class="comment"># 把所有能到达i的路径删除，使i不重复经过</span></span><br><span class="line">        path += getPath(B, j)       <span class="comment"># 在不能到达i节点的情况下，求解j节点到达死结点的路径</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加节点i</span></span><br><span class="line">    <span class="keyword">if</span> len(path) == <span class="number">0</span>:              <span class="comment"># i不能到达任何节点，那么只返回1条包含i的路径</span></span><br><span class="line">        path = [[i]]</span><br><span class="line">    <span class="keyword">else</span>:                           <span class="comment"># i能到达其他死结点，那么每条路径前添加i节点</span></span><br><span class="line">        path = list(map(<span class="keyword">lambda</span> x: [i] + x, path))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCost</span><span class="params">(A, path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">        path: &#123;list&#125;</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cost</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        - 计算环路的耗费；</span></span><br><span class="line"><span class="string">        - path路径不包含头节点，即不为环路</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(path) - <span class="number">1</span>):</span><br><span class="line">        cost += A[path[i], path[i+<span class="number">1</span>]]</span><br><span class="line">    cost += A[path[<span class="number">-1</span>], path[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    </span><br><span class="line">    A = np.array([</span><br><span class="line">        [float(<span class="string">'inf'</span>), <span class="number">30</span>, <span class="number">35</span>, float(<span class="string">'inf'</span>), <span class="number">20</span>],</span><br><span class="line">        [<span class="number">30</span>, float(<span class="string">'inf'</span>), <span class="number">20</span>, <span class="number">10</span>, float(<span class="string">'inf'</span>)],</span><br><span class="line">        [<span class="number">35</span>, <span class="number">20</span>, float(<span class="string">'inf'</span>), <span class="number">5</span>, <span class="number">15</span>],</span><br><span class="line">        [float(<span class="string">'inf'</span>), <span class="number">10</span>, <span class="number">5</span>, float(<span class="string">'inf'</span>), <span class="number">25</span>],</span><br><span class="line">        [<span class="number">20</span>, float(<span class="string">'inf'</span>), <span class="number">15</span>, <span class="number">25</span>, float(<span class="string">'inf'</span>)]</span><br><span class="line">    ])</span><br><span class="line">    print(solveTSP(A))</span><br></pre></td></tr></table></figure><p>下面是用<strong>栈</strong>实现的<strong>深度优先搜索</strong>，需保存当前图、E-节点和当前已加入的路径</p><p><div id="dfs"></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._data = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self._data += [x]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._data.pop(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(n, n)&#125; 原始图</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = A.shape[<span class="number">0</span>]</span><br><span class="line">    path = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从0号结点开始</span></span><br><span class="line">    stack = Stack()</span><br><span class="line">    stack.push((A, <span class="number">0</span>, [<span class="number">0</span>]))  <span class="comment"># 栈内元素：(当前图，当前节点，当前路径)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(len(stack) &gt; <span class="number">0</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># E-节点出栈</span></span><br><span class="line">        a, i, p = stack.pop()</span><br><span class="line">        <span class="comment"># 已经过所有节点，保存退出</span></span><br><span class="line">        <span class="keyword">if</span> len(p) == n:</span><br><span class="line">            path += [p]</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 删除可达E-节点的路径</span></span><br><span class="line">        b = a.copy(); b[:, i] = float(<span class="string">'inf'</span>)</span><br><span class="line">        <span class="comment"># E-节点的可达节点入栈，这些节点即活动节点</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> np.isinf(b[i, j]): <span class="keyword">continue</span></span><br><span class="line">            stack.push((b, j, p + [j]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取最小耗费的路径</span></span><br><span class="line">    cost = list(map(<span class="keyword">lambda</span> x: getCost(A, x), path))</span><br><span class="line">    index = cost.index(min(cost))</span><br><span class="line">    <span class="keyword">return</span> cost[index], path[index]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    </span><br><span class="line">    A = np.array([</span><br><span class="line">        [float(<span class="string">'inf'</span>), <span class="number">30</span>, <span class="number">35</span>, float(<span class="string">'inf'</span>), <span class="number">20</span>],</span><br><span class="line">        [<span class="number">30</span>, float(<span class="string">'inf'</span>), <span class="number">20</span>, <span class="number">10</span>, float(<span class="string">'inf'</span>)],</span><br><span class="line">        [<span class="number">35</span>, <span class="number">20</span>, float(<span class="string">'inf'</span>), <span class="number">5</span>, <span class="number">15</span>],</span><br><span class="line">        [float(<span class="string">'inf'</span>), <span class="number">10</span>, <span class="number">5</span>, float(<span class="string">'inf'</span>), <span class="number">25</span>],</span><br><span class="line">        [<span class="number">20</span>, float(<span class="string">'inf'</span>), <span class="number">15</span>, <span class="number">25</span>, float(<span class="string">'inf'</span>)]</span><br><span class="line">    ])</span><br><span class="line">    print(solveTSP2(A))</span><br></pre></td></tr></table></figure></p><h1 id="分支定界法"><a href="#分支定界法" class="headerlink" title="分支定界法"></a>分支定界法</h1><p>和回溯法类似，分支定界法也经常把解空间组织成树或图的结构然后进行搜搜。<strong>回溯法使用先深搜索，而分支定界法采用先广搜索</strong>。每个节点只有一次机会成为E-节点，E-节点的可达节点都是生成的新节点，那些不可能到处(最优)可行解的节点被舍弃(死结点)。分支定界法也可采用<strong>定界函数</strong>提前结束搜索。</p><p>有两种常用的方法选择E-节点：</p><ol><li>先进先出(FIFO)：借助队列，可参考二叉树的层次遍历。将首个节点加入队列，然后依次出队列作为E-节点，将E-节点可达节点全部加入队列，直至队列为空；</li><li>最小耗费或最大收益法：借助小根堆/大根堆，将堆顶节点弹出作为E-节点，并将其可达节点全部加入堆。</li></ol><blockquote><p>那么迷宫老鼠问题中，迷宫的路径搜索节点顺序如下<br><img src="/2020/03/23/【算法】回溯法与分支定界法/maze_bfs.jpg" alt="maze_bfs"></p></blockquote><p>以下借助<strong>队列</strong>实现<strong>广先搜索</strong>解决TSP问题，将<a href="#dfs">回溯法</a>中栈修改为队列即可</p><p><div id="bfs"></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Queue</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._data = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self._data += [x]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._data.pop(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(n, n)&#125; 原始图</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = A.shape[<span class="number">0</span>]</span><br><span class="line">    path = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从0号结点开始</span></span><br><span class="line">    queue = Queue()</span><br><span class="line">    queue.push((A, <span class="number">0</span>, [<span class="number">0</span>]))  <span class="comment"># 栈内元素：(当前图，当前节点，当前路径)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(len(queue) &gt; <span class="number">0</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># E-节点出栈</span></span><br><span class="line">        a, i, p = queue.pop()</span><br><span class="line">        <span class="comment"># 已经过所有节点，保存退出</span></span><br><span class="line">        <span class="keyword">if</span> len(p) == n:</span><br><span class="line">            path += [p]</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 删除可达E-节点的路径</span></span><br><span class="line">        b = a.copy(); b[:, i] = float(<span class="string">'inf'</span>)</span><br><span class="line">        <span class="comment"># E-节点的可达节点入栈，这些节点即活动节点</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> np.isinf(b[i, j]): <span class="keyword">continue</span></span><br><span class="line">            queue.push((b, j, p + [j]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取最小耗费的路径</span></span><br><span class="line">    cost = list(map(<span class="keyword">lambda</span> x: getCost(A, x), path))</span><br><span class="line">    index = cost.index(min(cost))</span><br><span class="line">    <span class="keyword">return</span> cost[index], path[index]</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【算法】动态规划</title>
      <link href="/2020/03/21/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
      <url>/2020/03/21/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<p>动态规划和贪婪算法一样，对一个问题的解是一系列抉择的结果。在贪婪算法中已做出的抉择是不可更改的，而动态规划需考察<strong>一个最优抉择序列是否包含最优抉择子序列</strong>。称<strong>无论第一次选择是什么，接下来的选择一定是当前状态下的最优选择</strong>为<strong>最优原则</strong>(principal of optimality)。</p><p><strong>用动态规划求解的步骤</strong>如下</p><ol><li>证实最优原则适用，否则不能使用动态规划；</li><li>建立<strong>动态规划递归方程</strong>(dynamic-programming recurrence equation)；</li><li>求解动态规划的递归方程式获得<strong>最优解</strong>；</li><li>沿最优解生成过程进行<strong>回溯</strong>(traceback)。</li></ol><p><strong>例1 (0/1背包问题)：设物品$i, 1 \leq i \leq n$的重量为$w_i$，价值为$p_i$，放入总容量为$c$的背包中，求最优解$x = [x_1, x_2, \cdots, x_n], x_i \in \{0, 1\}$使总价值$\sum_i p_i x_i$最大。</strong></p><p><strong>分析</strong>：物品按次序拜访，当需要对第$i$个物品进行抉择时，可以对存在的两种可能(放入$i$/不放入$i$)下，规划前$i-1$个物品的抉择。</p><p>设$f(i, y)$表示<strong>问题状态</strong>为剩余容量为$y$时，确定物品$1, 2, \cdots, i$的的最优规划下的准则，那么对于物品$i$，可以<strong>不放入</strong>，前$i-1$个物品可以在背包容量为$c$的状态下进行最优选择，且不包含本物品价值，设$f(n-1, c)$；而$i$<strong>放入</strong>时，前$i-1$个物品只能在背包容量为$c - w_i$的状态下进行最优选择，包含本物品价值，设$f(n-1, c - w_i) + p_i$。根据$f(n-1, c)$与$f(n-1, c - w_i) + p_i$的大小，确定物品$i$的最优抉择，如下</p><script type="math/tex; mode=display">f(i, y) = \begin{cases}    \max\{ f(i - 1, y), f(i - 1, y - w_i) + p_i \} & y \geq w_i \\    f(i - 1, y) & 0 \leq y < w_i\end{cases}</script><p>其中$f(i - 1, y)， f(i - 1, y - w_i) + p_i$分别表示在不放入/放入物品$i$的问题状态下求解的价值，<strong>递归的终止条件</strong>为$i=1$时</p><script type="math/tex; mode=display">f(1, y) = \begin{cases}    p_n & y \geq w_n \\    0 & 0 \leq y < w_n\end{cases}</script><blockquote><p>如$n=3, w = [10, 3, 2], p = [20, 18, 15], c = 14$时，有</p><script type="math/tex; mode=display">\begin{aligned}    f(1, y) = \begin{cases} 15 & y \geq 10 \\ 0 & 0 \leq y < 10 \end{cases} \\    f(2, y) = \begin{cases}  \max\{ f(1, y), f(1, y - 3) + 18 \} & y \geq 3 \\ f(1, y) & 0 \leq y < 3 \end{cases} \\     f(3, y) = \begin{cases} \max\{ f(2, y), f(2, y - 2) + 20 \} & y \geq 2 \\ f(2, y) & 0 \leq y < 2 \end{cases}\end{aligned}</script><p>那么在初始情况下$y=14$时，代入有</p><script type="math/tex; mode=display">f(3, 116) = 38</script></blockquote><p>用表格的形式求解，<strong>表格元素<code>table[i][y]</code>表示在背包容量为$y$的情况下，物品子集和$\{0(无物品), 1, 2, \cdots, i\}$的最优装载总价值</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span>* w, <span class="keyword">int</span>* p, <span class="keyword">int</span> n, <span class="keyword">int</span> c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 (n + 1) × (c + 1) 表格</span></span><br><span class="line">    <span class="keyword">int</span>** table = <span class="keyword">new</span> <span class="keyword">int</span>* [n + <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n + <span class="number">1</span>; i++) &#123;</span><br><span class="line">        table[i] = <span class="keyword">new</span> <span class="keyword">int</span>[c + <span class="number">1</span>];</span><br><span class="line">        <span class="built_in">memset</span>(table[i], <span class="number">0</span>, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * (c + <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开始填表</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n + <span class="number">1</span>; i++) &#123;        <span class="comment">// 已经判定是否装入背包的物品依次增加，即`1, 2, ..., i-1`已装入</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">1</span>; y &lt; c + <span class="number">1</span>; y++) &#123;       <span class="comment">// 剩余容量依次增加，即剩余容量为`y`</span></span><br><span class="line">            <span class="keyword">if</span> (y &lt; w[i - <span class="number">1</span>]) &#123;        <span class="comment">// 容量不够，物品`i`不装入</span></span><br><span class="line">                table[i][y] = table[i - <span class="number">1</span>][y];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;                    <span class="comment">// 容量够</span></span><br><span class="line">                <span class="keyword">int</span> p1 = table[i - <span class="number">1</span>][y];   <span class="comment">// 不装入物品`i`</span></span><br><span class="line">                <span class="keyword">int</span> p2 = table[i - <span class="number">1</span>][y - w[i - <span class="number">1</span>]] + p[i - <span class="number">1</span>];<span class="comment">// 前`i-1`个物品在容量为`y - w[i]`下的最优装载 + 物品`i`价值</span></span><br><span class="line">                table[i][y] = p1 &gt; p2 ? p1 : p2;    <span class="comment">// 取大</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印结果</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n + <span class="number">1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; c + <span class="number">1</span>; j++) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; table[i][j] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 释放内存</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n + <span class="number">1</span>; i++) </span><br><span class="line">    <span class="keyword">delete</span>[] table[i];</span><br><span class="line">    <span class="keyword">delete</span>[] table;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">3</span>] = &#123;<span class="number">10</span>, <span class="number">3</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> p[<span class="number">3</span>] = &#123; <span class="number">20</span>, <span class="number">18</span>, <span class="number">15</span> &#125;;</span><br><span class="line">solve(w, p, <span class="number">3</span>, <span class="number">14</span>);</span><br><span class="line"></span><br><span class="line">    system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当$n=3, c=14, p=[20, 18, 15], w=[10, 3, 2]$时，最优装载价值为$38$<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       </span><br><span class="line">0       0       0       0       0       0       0       0       0       0       20      20      20      20      20      </span><br><span class="line">0       0       0       18      18      18      18      18      18      18      20      20      20      38      38      </span><br><span class="line">0       0       15      18      18      33      33      33      33      33      33      33      35      38      38      </span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure></p><p><strong>例2 (优惠券问题)：有一张满$100$可减免$20$的优惠券，从价格为$\{30, 20, 35, 55, 45\}$的物件商品中选择合适的子集，使商品价格总和超过$100$但又最小。</strong></p><p>原问题有两个约束：1) 商品价格最小；2) 商品价格超过$100$。将原问题修改为：从物品中删除若干件，使删除的物品价格总和不超过$\sum_i p_i - 100 = 85$，此时约束只有上限。$f(i, y)$表示在剩余可删除价格为$y$的状态下，前$0, \cdots, i$的最大价格总和，那么</p><script type="math/tex; mode=display">f(i, y)\begin{cases}    \max\{ f(i-1, y), f(i-1, y-p_i) + p_i \} & y \geq p_i \\    f(i-1, y) & y < p_i\end{cases}</script><p>用表格的形式求解，得最多删除$85$元<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span>* p, <span class="keyword">int</span> n, <span class="keyword">int</span> c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 初始化 (n + 1) × (c + 1) 表格</span></span><br><span class="line">    <span class="keyword">int</span>** table = <span class="keyword">new</span> <span class="keyword">int</span>* [n + <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n + <span class="number">1</span>; i++) &#123;</span><br><span class="line">        table[i] = <span class="keyword">new</span> <span class="keyword">int</span>[c + <span class="number">1</span>];</span><br><span class="line">        <span class="built_in">memset</span>(table[i], <span class="number">0</span>, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * (c + <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 开始填表</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n + <span class="number">1</span>; i++) &#123;<span class="comment">// 已经判定是否删除物品依次增加，即`1, 2, ..., i-1`中的某些组合</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> y = <span class="number">1</span>; y &lt; c + <span class="number">1</span>; y++) &#123;<span class="comment">// 剩余价格为`y`</span></span><br><span class="line">            <span class="keyword">if</span> (y &lt; p[i - <span class="number">1</span>]) &#123;<span class="comment">// 剩余价格不够，物品`i`不删除</span></span><br><span class="line">                table[i][y] = table[i - <span class="number">1</span>][y];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;<span class="comment">// 价格够</span></span><br><span class="line">                <span class="keyword">int</span> p1 = table[i - <span class="number">1</span>][y];<span class="comment">// 不删除物品`i`</span></span><br><span class="line">                <span class="keyword">int</span> p2 = table[i - <span class="number">1</span>][y - p[i - <span class="number">1</span>]] + p[i - <span class="number">1</span>];<span class="comment">// 前`i-1`个物品在剩余价格为`y - p[i - 1]`下最多的删除总价 + 物品`i`价格</span></span><br><span class="line">                table[i][y] = p1 &gt; p2 ? p1 : p2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印结果</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n + <span class="number">1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; c + <span class="number">1</span>; j++) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; table[i][j] &lt;&lt; <span class="string">'\t'</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 释放内存</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n + <span class="number">1</span>; i++)</span><br><span class="line">        <span class="keyword">delete</span>[] table[i];</span><br><span class="line">    <span class="keyword">delete</span>[] table;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0</span><br><span class="line">0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30</span><br><span class="line">0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       20      20      20      20      20      20      20      20      20      20      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      30      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50      50</span><br><span class="line">0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       20      20      20      20      20      20      20      20      20      20      30      30      30      30      30      35      35      35      35      35      35      35      35      35      35      35      35      35      35      35      50      50      50      50      50      55      55      55      55      55      55      55      55      55      55      65      65      65      65      65      65      65      65      65      65      65      65      65      65      65      65      65      65      65      65      85</span><br><span class="line">0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       20      20      20      20      20      20      20      20      20      20      30      30      30      30      30      35      35      35      35      35      35      35      35      35      35      35      35      35      35      35      50      50      50      50      50      55      55      55      55      55      55      55      55      55      55      65      65      65      65      65      65      65      65      65      65      75      75      75      75      75      75      75      75      75      75      85</span><br><span class="line">0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       0       20      20      20      20      20      20      20      20      20      20      30      30      30      30      30      35      35      35      35      35      35      35      35      35      35      45      45      45      45      45      50      50      50      50      50      55      55      55      55      55      55      55      55      55      55      65      65      65      65      65      65      65      65      65      65      75      75      75      75      75      80      80      80      80      80      85</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure><p><strong>例3 (<a href="https://www.lintcode.com/problem/binary-tree-maximum-path-sum/description" target="_blank" rel="noopener">二叉树中的最大路径和</a>)：给出一棵二叉树，寻找一条路径使其路径和最大，路径可以在任一节点中开始和结束（路径和为两个节点之间所在路径上的节点权值之和。</strong></p><blockquote><p>例如给定$\{1, 2, 3, 4, 2, 5, 10, #, #, #, #, 7, 18, #, #, 20\}$，最大路径和为$20 + 7 + 5 + 18$</p></blockquote><p><strong>分析：</strong></p><ul><li><strong>分治策略</strong>，查找子树的最大路径和；</li><li><strong>动态规划策略</strong>，对于高为$h$的二叉树，以第$i(i=1, \cdots, h-1)$层节点为起始的<strong>单边路径和最大路径</strong>，一定包含以第$i+1(i=1, \cdots, h-1)$层节点为起始的单边路径和最大路径，且$i=h$时路径仅包含叶子节点；</li><li>每条最长路径都肯定会以某个顶点为根，然后两边是以那个节点为跟到叶子节点的最长路径(<strong>“八”字形</strong>，除根节点外每个节点最多包含一个孩子)，故只需比较每棵树这种情况下路径和；</li><li>某节点处的上述路径，是由<strong>该节点的值、左右子树的最大路径</strong>组成的；</li><li>注意<strong>负数的处理</strong>，负数添加到路径中必定会使路径和减小，故用<code>max(0, x)</code>处理；</li></ul><p>将所有内部节点按深度优先进行标号，设<strong>节点$i$到所有叶子节点的单边路径中，最大路径和</strong>为$f(i)$。考虑到$f(i.child)$可能为负值，将小于$0$的单边路径加入后，该单边路径不可能是最大，所以本节点加上单边路径时，考虑$\max\{0, f(i.child)\}$</p><script type="math/tex; mode=display">f(i) = \begin{cases}    i.val + \max \{ \max\{0, f(i.left)\}, \max\{0, f(i.right)\} \}  & i不是叶子节点 \\    i.val & i是叶子节点\end{cases}</script><blockquote><script type="math/tex; mode=display">\max\{0, f(i.left)\}, \max\{0, f(i.right)\} = \max \{ 0, \max\{ f(i.left), f(i, i.right) \} \}</script></blockquote><p>设<strong>节点$i$处“八”字形最大路径和为</strong>$g(i)$，</p><script type="math/tex; mode=display">g(i) = i.val +  \max\{0, f(i.left)\} +  \max\{0, f(i.right)\}</script><p>最优为</p><script type="math/tex; mode=display">G = \max g(i)</script><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition of TreeNode:</span></span><br><span class="line"><span class="comment"> * class TreeNode &#123;</span></span><br><span class="line"><span class="comment"> * public:</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left, *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int val) &#123;</span></span><br><span class="line"><span class="comment"> *         this-&gt;val = val;</span></span><br><span class="line"><span class="comment"> *         this-&gt;left = this-&gt;right = NULL;</span></span><br><span class="line"><span class="comment"> *     &#125;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * @param root: The root of binary tree.</span></span><br><span class="line"><span class="comment">     * @return: An integer</span></span><br><span class="line"><span class="comment">     * @description: 返回整棵树中最大路径和</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxPathSum</span><span class="params">(TreeNode * root)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write your code here</span></span><br><span class="line">        <span class="keyword">if</span> (!root) <span class="keyword">return</span> INT_MIN;</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> psRoot  = maxBinaryLeafPathSum(root);   <span class="comment">// 以root为根，两边到叶子节点的最长路径</span></span><br><span class="line">        <span class="keyword">int</span> psLeft  = maxPathSum(root-&gt;left);       <span class="comment">// 左子节点的以root为根，两边到叶子节点的最长路径</span></span><br><span class="line">        <span class="keyword">int</span> psRight = maxPathSum(root-&gt;right);      <span class="comment">// 右子节点的以root为根，两边到叶子节点的最长路径</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> max(max(psRoot, psLeft), psRight);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * @param root: The root of binary tree.</span></span><br><span class="line"><span class="comment">     * @return: An integer</span></span><br><span class="line"><span class="comment">     * @description: 返回以root为根，两边到叶子节点的最大路径和</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxBinaryLeafPathSum</span><span class="params">(TreeNode * root)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) <span class="keyword">return</span> INT_MIN;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> lpsL = maxLeafPathSum(root-&gt;left);</span><br><span class="line">        <span class="keyword">int</span> lpsR = maxLeafPathSum(root-&gt;right);</span><br><span class="line">        <span class="keyword">return</span> root-&gt;val + max(<span class="number">0</span>, lpsL) + max(<span class="number">0</span>, lpsR);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * @param root: The root of binary tree.</span></span><br><span class="line"><span class="comment">     * @return: An integer</span></span><br><span class="line"><span class="comment">     * @description: 返回节点root跟到叶子节点的最大路径和，包含root</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxLeafPathSum</span><span class="params">(TreeNode * root)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) <span class="keyword">return</span> INT_MIN;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> lpsL = maxLeafPathSum(root-&gt;left);</span><br><span class="line">        <span class="keyword">int</span> lpsR = maxLeafPathSum(root-&gt;right);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> root-&gt;val + max(<span class="number">0</span>, max(lpsL, lpsR));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>以上解法的缺陷是，在计算某节点的“八”自行树时，对其子节点进行了遍历，但该节点回溯后，即计算更高层节点时，到该节点仍需要节点遍历，造成许多不必要的计算浪费，采用<strong>指针</strong>可以改进算法，如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * @param root: The root of binary tree.</span></span><br><span class="line"><span class="comment">     * @return: An integer</span></span><br><span class="line"><span class="comment">     * @description: 返回整棵树中最大路径和</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxPathSum</span><span class="params">(TreeNode * root)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write your code here</span></span><br><span class="line">        <span class="keyword">if</span> (!root) <span class="keyword">return</span> INT_MIN;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> ret = INT_MIN;</span><br><span class="line">        maxLeafPathSum(root, ret);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * @param root: The root of binary tree.</span></span><br><span class="line"><span class="comment">     * @return: An integer</span></span><br><span class="line"><span class="comment">     * @description: 返回单边节点root跟到叶子节点的最大路径和，包含root</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxLeafPathSum</span><span class="params">(TreeNode * root, <span class="keyword">int</span>&amp; ret)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!root) <span class="keyword">return</span> INT_MIN;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> lpsL = maxLeafPathSum(root-&gt;left, ret);         <span class="comment">// `root-&gt;left`为起始的单边路径和最大路径</span></span><br><span class="line">        <span class="keyword">int</span> lpsR = maxLeafPathSum(root-&gt;right, ret);        <span class="comment">// `root-&gt;right`为起始的单边路径和最大路径</span></span><br><span class="line">        </span><br><span class="line">        ret = max(ret, root-&gt;val + max(<span class="number">0</span>, lpsL) + max(<span class="number">0</span>, lpsR));     <span class="comment">// `root`为根节点的"八"字形子树</span></span><br><span class="line">        <span class="keyword">return</span> root-&gt;val + max(max(<span class="number">0</span>, lpsL), max(<span class="number">0</span>, lpsR));          <span class="comment">// 该路径满足：`root`为起始的、单边路径和最大的</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>例4 (最大子数组)：给定一个整数数组，找到一个具有最大和的子数组，返回其最大和。</strong></p><blockquote><p>输入：[−2,2,−3,4,−1,2,1,−5,3]<br>输出：6<br>解释：符合要求的子数组为[4,−1,2,1]，其最大和为 6。</p></blockquote><p>这一题的关键点是<strong>数组连续的判定</strong>。</p><p>考虑$\{4, -3, 2, 3\}$与$\{4, -5, 2, 3\}$，可以看到，<strong>连续子数组只会以大于$0$的数开始，在小于$0$的元素处断开</strong>。一旦“<strong>在第$i$个数添加时，出现该数与之前数的最大和子数组小于$0$</strong>”这一条件触发，有两层含义：<strong>1) 在此之前数组连续；2) 之前连续的数组在该值处断开</strong>。</p><p><strong>证明</strong>：$\{s_1, v, s_2\}, \sum s_1 &gt; 0，\sum s_2 &gt; 0$的形式下，只有</p><script type="math/tex; mode=display">\begin{cases}\sum s_1 + v + \sum s_2 \geq \sum s_1 \\ \sum s_1 + v + \sum s_2 \geq \sum s_2\end{cases}</script><p>时，$\{s_1, v, s_2\}$才有可能是最大连续数组，那么</p><script type="math/tex; mode=display">\begin{cases}v \geq - \sum s_1 \\ v \geq -\sum s_2\end{cases}</script><p>所以$v &lt; - \sum s_1 \Rightarrow \sum s_1 + v &lt; 0$时，$\{s_1, v, s_2\}$必定不是最大和子数组，连续数组$s_1$和也达到最大，可重新累加新的连续数组的和。</p><p>设$f(i)$为<strong>前$1, \cdots, i-1$不小于$0$的累加，与第$i$个数的和</strong>，那么有</p><script type="math/tex; mode=display">f(i) = \begin{cases}    nums[i] & i = 1 \\    nums[i] + \max\{ 0, f(i-1) \}  & i > 1\end{cases}</script><p>那么就有($i \geq 1$)</p><script type="math/tex; mode=display">\begin{cases}    f(i) < 0 \Rightarrow f(i+1) = nums[i+1] & nums[i]是上述v \\    f(i) < 0 \Rightarrow f(i+1) = nums[i+1] + f(i) & nums[i]不是上述v，可视作当前连续数组(包括nums[i]不包括nums[i+1])的持续累加**。\end{cases}</script><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * @param nums: A list of integers</span></span><br><span class="line"><span class="comment">     * @return: An integer indicate the sum of max subarray</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write your code here</span></span><br><span class="line">        <span class="keyword">int</span> ret = INT_MIN;</span><br><span class="line">        f(nums, nums.size(), &amp;ret);</span><br><span class="line">        <span class="keyword">return</span> ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> n, <span class="keyword">int</span>* ret)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">0</span>, b = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">            a = b = nums[<span class="number">0</span>];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            a = f(nums, n - <span class="number">1</span>, ret);</span><br><span class="line">            b = max(<span class="number">0</span>, a) + nums[n - <span class="number">1</span>];    <span class="comment">// a &lt; 0时，数组不连续，nums[n - 1]作为新的连续数组的起始</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        *ret = max(*ret, max(a, b));        <span class="comment">// 记录最大连续数组和</span></span><br><span class="line">        <span class="keyword">return</span> b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>例5 (旅行商问题, TSP)：给定一个$n$顶点的有权网络(有向或无向)，找出一个包含$n$个顶点且具有最小耗费的环路，例如给定下图的网络：</strong><br><img src="/2020/03/21/【算法】动态规划/tsp_eg.jpg" alt="tsp_eg"></p><p><strong>分析</strong>：节点间的先后次序会影响下一个节点的选择，故本例和背包问题不同，不能将节点作为单独的抉择，关键是如何把有序问题转换为无序问题，转换思路，<strong>节点的顺序方案却是无序的</strong>，<strong>那么定义$f(x, n)$为：在给定$x$节点顺序的方案下，再选择$n$个可达节点，使路径耗费最小</strong>，下一个抉择方案的可能性是所有$x[-1]$可达顶点$v$。</p><script type="math/tex; mode=display">f(x, n) = \begin{cases}    f(x + v, n - 1) & v可达 \\    f(x, n) & v 不可达 \end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solveTSP</span><span class="params">(A)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> f(A, A, A.shape[<span class="number">0</span>], [<span class="number">0</span>], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(A, B, num, ordered, cost)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(n, n)&#125; 原始图</span></span><br><span class="line"><span class="string">        B: &#123;ndarray(n, n)&#125; 删除“可到达已到达节点路径”后的图</span></span><br><span class="line"><span class="string">        num: &#123;int&#125; 还需选择的顶点个数</span></span><br><span class="line"><span class="string">        ordered: &#123;list[int]&#125; 当前已选择的顶点顺序</span></span><br><span class="line"><span class="string">        cost: &#123;float&#125; 当前路径的耗费累计</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        ordered: &#123;list[int]&#125; 当前顶点顺序下，加入新顶点后的最优路径</span></span><br><span class="line"><span class="string">        costNew: &#123;float&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 加入起始点</span></span><br><span class="line">    <span class="keyword">if</span> num == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> ordered + [ordered[<span class="number">0</span>]], cost + A[ordered[<span class="number">-1</span>], ordered[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历ordered[-1]可达节点，选择耗费最低</span></span><br><span class="line">    path = []</span><br><span class="line">    listCost = []</span><br><span class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(B[ordered[<span class="number">-1</span>]]):</span><br><span class="line">        <span class="keyword">if</span> np.isinf(c): <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 节点可达，将其加入路径</span></span><br><span class="line">        <span class="comment">## 将所有能到达已选顶点的路径删除</span></span><br><span class="line">        C = B.copy(); C[:, ordered + [i]] = float(<span class="string">'inf'</span>)</span><br><span class="line">        <span class="comment">## 计算加入后最短</span></span><br><span class="line">        orderedNew, costNew = f(A, C, num - <span class="number">1</span>, ordered + [i], cost + A[ordered[<span class="number">-1</span>], i])</span><br><span class="line"></span><br><span class="line">        path += [orderedNew]; listCost += [costNew]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 无可达节点，该路死路</span></span><br><span class="line">    <span class="keyword">if</span> len(path) == <span class="number">0</span>: </span><br><span class="line">        <span class="keyword">return</span> ordered + [ordered[<span class="number">0</span>]], cost + A[ordered[<span class="number">-1</span>], ordered[<span class="number">0</span>]]</span><br><span class="line">    </span><br><span class="line">    minCost = min(listCost)</span><br><span class="line">    index = listCost.index(minCost)</span><br><span class="line">    <span class="keyword">return</span> path[index], listCost[index]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    </span><br><span class="line">    A = np.array([</span><br><span class="line">        [float(<span class="string">'inf'</span>), <span class="number">30</span>, <span class="number">35</span>, float(<span class="string">'inf'</span>), <span class="number">20</span>],</span><br><span class="line">        [<span class="number">30</span>, float(<span class="string">'inf'</span>), <span class="number">20</span>, <span class="number">10</span>, float(<span class="string">'inf'</span>)],</span><br><span class="line">        [<span class="number">35</span>, <span class="number">20</span>, float(<span class="string">'inf'</span>), <span class="number">5</span>, <span class="number">15</span>],</span><br><span class="line">        [float(<span class="string">'inf'</span>), <span class="number">10</span>, <span class="number">5</span>, float(<span class="string">'inf'</span>), <span class="number">25</span>],</span><br><span class="line">        [<span class="number">20</span>, float(<span class="string">'inf'</span>), <span class="number">15</span>, <span class="number">25</span>, float(<span class="string">'inf'</span>)]</span><br><span class="line">    ])</span><br><span class="line">    print(solveTSP(A))</span><br><span class="line">    <span class="comment"># 输出 ([0, 1, 3, 2, 4, 0], 80.0)</span></span><br></pre></td></tr></table></figure><hr><p><strong>一些感受：</strong></p><p>动态规划可以理解为<strong>根据当前的选择，调整过去的抉择，获得未来的最优</strong>。在问题中，需要确定<strong>优化目标</strong>，以及<strong>问题的约束</strong>。在约束下，确定一个抉择序列“抉择$1$ $\rightarrow$ 抉择$2$ $\rightarrow$ … $\rightarrow$ 抉择$n$”，在进行第$i$个抉择时，可以对其$m$个<strong>可能性</strong>$c_{i1}, c_{i2}, \cdots, c_{im}$进行假设，<strong>每次假设中，抉择$1 \sim i$中唯一确定的只有抉择$i$，$1 \sim i-1$需要根据抉择$i$引起的约束变更进行最优规划</strong>，而这些规划可通过<strong>递归</strong>求解。实际上，这些一系列的抉择，可构成一个树型结构。</p><p>总结，动态规划在以下情况时适用，从上往下分析问题、从下往上求解，并保存子问题的最优解避免重复计算</p><ol><li>该问题可以分为若干子问题，且子问题间还有重叠的更小的子问题；</li><li>整体问题的最优解依赖各个子问题的最优解；</li><li>子问题仍旧适用1，可递归解决。</li></ol><p><strong>以$0/1$背包为例</strong>，约束是背包容量，优化目标是总价值。在确定第$n$个物品是否放入时，需要考虑放入后对剩余背包容量$y$的更改，即$y - w_n$。那么假设第$n$个物品放入，规划子问题就变成$1, \cdots, n-1$物品在背包容量为$y - w_n$下的最优规划；假设第$n$个物品不放入，<strong>规划子问题</strong>就变成$1, \cdots, n-1$物品在背包容量为$y $下的最优规划。仍旧以$n=3, w = [10, 3, 2], p = [20, 18, 15], c = 14$为例，用树形图可表示如下</p><p><img src="/2020/03/21/【算法】动态规划/01bag.jpg" alt="01bag"></p><p>在<strong>最大连续子数组</strong>中，条件进行了更改，数字是必须加入数组的，约束是“连续”，优化目标是最大和。需要进行判别，确定第$i$个数字是否会引起之前连续数组的“断裂”，<strong>这个影响是确定的</strong>，不需要对多种可能性假设，那么只需记录当前连续数组的累加即可。</p><p>那么哪些问题是适用动态规划的呢？个人理解，在<strong>前后决策无序，但会相互影响，需要在限制条件下求取最优搭配的问题</strong>中，动态规划是可以求取最优的。如果说贪婪算法是在贪婪准则下的最优抉择，那么动态规划可以视作在<strong>假设搜索空间</strong>内的贪婪算法。若假设空间非常大，就不便用动态规划求解了。</p><p>另外，如何用如何用动态规划<strong>解决前后决策有序的问题</strong>呢？例如旅行商问题，节点的选择看似是有序的，但节点方案的选择是无序的。具体见例5。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【算法】分而治之</title>
      <link href="/2020/03/21/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B/"/>
      <url>/2020/03/21/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B/</url>
      
        <content type="html"><![CDATA[<p><strong>分治算法</strong>是将一个问题的大实例分成若干个更小的实例，分别解决后将小实例的解组合成原始大实例的解。一般来说，大实例分解前是用重复的步骤进行求解的，分解后的子问题也可通过同一种算法解决，用<strong>递归</strong>解决。</p><p><strong>例(金块问题)：有$n$个金块，从中找出最重与最轻的金块。</strong></p><p>如果用顺序查找的方式，先通过$n-1$次比较选出最重的，再通过$n-2$次比较选出最轻的，共进行$2n-3$次比较，时间复杂度为$O(n)$。</p><p>通过分治的方式，<strong>将金块分为若干份，每份中选取最大和最小的金块</strong>，如递归二分，将$n$个金块分为$n/2 + n/2$，$n/2$又可划分为$n/4 + n/4$，以此类推，直至划分集合中剩余$1$或$2$个金块，选出最大和最小，在合并过程中<strong>比较每份中的最大和最小</strong>即可，最终得到全部金块中的最大和最小。</p><p>共划分为$\lceil n/2 \rceil$个小组，第$1$次需进行$\lceil n/2 \rceil$次比较，第$i(i \geq 2)$次在$\lceil n / 2^{i} \rceil$份金子间，进行$\lceil n / 2^{i} \rceil \times 2$次比较(最大、最小比较)，共需进行$\lceil \log_2 n \rceil$层的比较，总比较次数为</p><script type="math/tex; mode=display">\lceil n/2 \rceil + 2 \times \sum_{i=2}^{\lceil \log_2 n \rceil} \lceil n / 2^{i} \rceil</script><p>以$n=17$为例，共进行$9 + 2 \times (5 + 3 + 2 + 1) = 22$次，而顺序比较时进行了$2 \times 17 - 3 = 31$次比较，减少超过$25\%$的比较次数。</p><p><strong>例(递归排序)：利用分治思想设计排序。</strong></p><p>将$n$个数进行<strong>二路划分</strong>，对每个子集合进行排(如插入、冒泡等)，再依次组合归并有序集合得到整个有序集合，可以看作是别的排序算法的递归实现。注意到将两个子集合归并时，两子集和已有序，故改进合并算法为<strong>直接归并排序</strong>(straight merge sort)，不采用其他排序算法进行合并，减少时间复杂度，相应的会增加空间空间复杂度。</p><p>如下图，对$\{8, 4, 7, 3, 6, 1, 2, 9, 10, 5\}$进行排序，归并排序共进行$22$次比较，而使用冒泡排序时，需进行$39$次比较。</p><p><img src="/2020/03/21/【算法】分而治之/merge_sort.jpg" alt="merge_sort"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(T a[], <span class="keyword">int</span> begin, <span class="keyword">int</span> middle, <span class="keyword">int</span> end)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    T* b = <span class="keyword">new</span> T[end];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将有序子数组合并到b</span></span><br><span class="line">    <span class="keyword">int</span> i = begin, j = middle, k = begin;</span><br><span class="line">    <span class="keyword">while</span> (k &lt; end) &#123;</span><br><span class="line">        <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (i == middle)</span><br><span class="line">            idx = j++;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (j == end)</span><br><span class="line">            idx = i++;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            idx = a[i] &lt; a[j] ? i++ : j++;</span><br><span class="line">        &#125;</span><br><span class="line">        b[k++] = a[idx]; mvCnt++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拷贝至原数组</span></span><br><span class="line">    <span class="keyword">for</span> (i = begin; i &lt; end; a[i] = b[i++]);</span><br><span class="line">    <span class="keyword">delete</span> [] b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(T a[], <span class="keyword">int</span> begin, <span class="keyword">int</span> end)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (end - begin &lt; <span class="number">2</span>) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> middle = (begin + end) / <span class="number">2</span>;</span><br><span class="line">    mergeSort(a, begin, middle);</span><br><span class="line">    mergeSort(a, middle, end);</span><br><span class="line"></span><br><span class="line">    merge(a, begin, middle, end);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(T a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    mergeSort(a, <span class="number">0</span>, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>例(快速排序)：一种不稳定但速度很快的排序算法</strong></p><p>快速排序是在待排序数组中寻找一个<strong>支点</strong>(pivot)，将当前待排序数组进行划分，使支点左侧的数据均小于支点，右侧的数据均大于支点，然后再分别对两侧进行迭代的排序进行<strong>分治</strong>，子数组排序完成后依次<strong>回溯</strong>使整个数组完成排序，示意图如下</p><p>与冒泡等不同，<strong>当有序数组输入时为最坏情况</strong>。此时支点始终在最左侧或最右侧，时间复杂度为$O(n)$。为改善上述情况，可将用<strong>三值取中原则</strong>(median-of-three rule)进行支点的选择，在$a[begin], a[(begin + end)/2], a[end]$三个元素中选择大小居中的元素。</p><p><img src="/2020/03/21/【算法】分而治之/quicksort.jpg" alt="quicksort"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(T a[], <span class="keyword">int</span> begin, <span class="keyword">int</span> end)</span>   <span class="comment">// a[begin] ~ a[end - 1]</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> cmpCnt = <span class="number">0</span>; <span class="keyword">int</span> mvCnt = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (end - begin &lt; <span class="number">2</span>) <span class="keyword">return</span>;    <span class="comment">// 仅包含1个数，无需排序</span></span><br><span class="line"></span><br><span class="line">    T pivot = a[begin];<span class="comment">// 当前子数组的头元素视作支点</span></span><br><span class="line">    <span class="keyword">int</span> left = begin, right = end - <span class="number">1</span>;<span class="comment">// 左右索引</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">while</span> (a[left] &lt;= pivot &amp;&amp; left &lt; end) left++;<span class="comment">// 搜索从左至右大于支点的元素a[left]</span></span><br><span class="line">        <span class="keyword">while</span> (a[right] &gt;= pivot &amp;&amp; right &gt; <span class="number">0</span>) right--;<span class="comment">// 搜索从右至左小于支点的元素a[right]</span></span><br><span class="line"></span><br><span class="line">        cmpCnt++;</span><br><span class="line">            <span class="keyword">if</span> (left &lt; right) &#123;<span class="comment">// 交换a[left]与a[right]</span></span><br><span class="line">            mvCnt++;</span><br><span class="line">            T temp = a[left];</span><br><span class="line">            a[left] = a[right];</span><br><span class="line">            a[right] = temp;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 此时索引为`begin + 1 ~ left - 1`的数都小于支点，索引为`left ~ end - 1`的数都大于支点；</span></span><br><span class="line">    <span class="comment">// 将支点调整至left - 1处，使支点左侧元素都小于支点，右侧都大于支点</span></span><br><span class="line">    mvCnt++;</span><br><span class="line">    a[begin] = a[left - <span class="number">1</span>];</span><br><span class="line">    a[left - <span class="number">1</span>] = pivot;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 划分子数组，递归</span></span><br><span class="line">    quickSort(a, begin, left - <span class="number">1</span>);  <span class="comment">// a[left-1]位置已确定</span></span><br><span class="line">    quickSort(a, left, end);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"比较"</span> &lt;&lt; cmpCnt &lt;&lt; <span class="string">"次 移动"</span> &lt;&lt; mvCnt &lt;&lt; <span class="string">"次"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(T a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    quickSort(a, <span class="number">0</span>, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>例：将快速排序应用到寻找数组中升序排序中，次序为$k$的数</strong></p><p>在快速排序的某次递归中，支点左侧均小于支点，右侧均大于支点，那么支点的位置在整个数组中已确定。在寻找次序为$k$的数时，不在最坏情况下，不需要等排序结束就可解决。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findK</span><span class="params">(T a[], <span class="keyword">int</span> begin, <span class="keyword">int</span> end, <span class="keyword">int</span> k)</span> <span class="comment">// a[begin] ~ a[end - 1]</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (end - begin &lt; <span class="number">2</span>) <span class="keyword">return</span> a[begin];<span class="comment">// 仅包含1个数，一定为查找的数</span></span><br><span class="line"></span><br><span class="line">    T pivot = a[begin];<span class="comment">// 当前子数组的头元素视作支点</span></span><br><span class="line">    <span class="keyword">int</span> left = begin, right = end - <span class="number">1</span>;<span class="comment">// 左右索引</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">while</span> (a[left] &lt;= pivot &amp;&amp; left &lt; end) left++;<span class="comment">// 搜索从左至右大于支点的元素a[left]</span></span><br><span class="line">        <span class="keyword">while</span> (a[right] &gt;= pivot &amp;&amp; right &gt; <span class="number">0</span>) right--;<span class="comment">// 搜索从右至左小于支点的元素a[right]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (left &lt; right) &#123;<span class="comment">// 交换a[left]与a[right]</span></span><br><span class="line">            T temp = a[left];</span><br><span class="line">            a[left] = a[right];</span><br><span class="line">            a[right] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 此时索引为`begin + 1 ~ left - 1`的数都小于支点，索引为`left ~ end - 1`的数都大于支点；</span></span><br><span class="line">    <span class="comment">// 将支点调整至left - 1处，使支点左侧元素都小于支点，右侧都大于支点</span></span><br><span class="line">    a[begin] = a[left - <span class="number">1</span>];</span><br><span class="line">    a[left - <span class="number">1</span>] = pivot;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 划分子数组，递归</span></span><br><span class="line">    <span class="keyword">if</span> (k &lt; left)</span><br><span class="line">        findK(a, begin, left - <span class="number">1</span>, k);<span class="comment">// 在左侧寻找第`k`大的数</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        findK(a, left, end, k);<span class="comment">// 在右侧寻找第`k-left`大的数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findK</span><span class="params">(T a[], <span class="keyword">int</span> n, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> findK(a, <span class="number">0</span>, n, k);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>例(<a href="https://www.lintcode.com/problem/fast-power/description" target="_blank" rel="noopener">快速幂</a>)：设计时间复杂度为$\log(n)$的算法，计算$a^n \% b$。</strong></p><p>有以下处理方法</p><ul><li>$(a \times b) \% c = (a \% c) \times b \% c$</li><li>$a^n = \begin{cases} a^{\lfloor n / 2 \rfloor} \times a^{\lfloor n / 2 \rfloor} \times a^{n \% 2 } &amp; n &gt; 1 \\ 1 &amp; n = 0 \\ a &amp; n = 1 \end{cases}$</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * @param a: A 32bit integer</span></span><br><span class="line"><span class="comment">     * @param b: A 32bit integer</span></span><br><span class="line"><span class="comment">     * @param n: A 32bit integer</span></span><br><span class="line"><span class="comment">     * @return: An integer</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">fastPower</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// write your code here</span></span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span> % b;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">long</span> temp = fastPower(a, b, n / <span class="number">2</span>);</span><br><span class="line">        temp = (temp * temp) % b;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (n % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">              temp = (temp * a) % b;</span><br><span class="line">              </span><br><span class="line">        <span class="keyword">return</span> temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【算法】贪婪算法</title>
      <link href="/2020/03/21/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95/"/>
      <url>/2020/03/21/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在<strong>贪婪算法</strong>(greedy method)中，逐步构造一个最优解，每一步都是在<strong>贪婪准则</strong>(greedy criterion)下做出的最优决策。</p><p><strong>例($0/1$背包问题)：有$n$个物品和一个容量为$c$的背包，第$i$个物品重量为$w_i$、价值为$p_i$，求取物品总价值最高的可行的背包装载，即</strong></p><script type="math/tex; mode=display">\begin{aligned}    \max \sum_{i=1}^n p_i x_i \\    s.t. \quad \sum_{i=1}^n w_i x_i \leq c \\    x_i \in \{0, 1\} \quad 1 \leq i \leq n\end{aligned}</script><p>该问题是$NP$-复杂问题，难以求取最优解，有以下几种<strong>简单的贪婪策略</strong></p><ol><li><strong>价值贪婪准则</strong>：从剩余物品中选出可以装入背包的价值最大的物品，但如$n=3, w = [10, 10, 10], p = [20, 15, 15], c = 15$时，该准则选取的装载$x = [1, 0, 0]$不是最优的；</li><li><strong>重量贪婪准则</strong>：从剩余物品中选出可以装入背包的重量最小的物品，但如$n=3, w = [20, 10, 10], p = [20, 5, 5], c = 25$时，该准则选取的装载$x = [0, 1, 1]$不是最优的；</li><li><strong>价值密度贪婪准则</strong>：从剩余物品中选出可以装入背包的$p_i/w_i$最大的物品，但如$n=3, w = [20, 15, 15], p = [40, 25, 25], c = 25$时，$p/w = [2, 1.67, 1.67]$，该准则选取的装载$x = [1, 0, 0]$不是最优的。</li></ol><p>价值密度贪婪法则不能保证最优解，但这是一个好的启发式算法，在很多时候接近最优解。对其进行改进，使解的结果与最优解之差在最优值的$x\%(x &lt; 100)$之内：选择<strong>至多$k$件物品</strong>放入背包(构成子集)，<strong>首先选择价值最高的可行物品集合，再将剩余子集按$p_i/w_i$的递减顺序装入背包</strong>，如$n=4, w = [2, 4, 6, 7], p = [6, 10, 12, 13], c = 11$，选择$k=2$，有以下子集<br>| 子集($k \leq 2$) | 重量 | 价值 | 价值密度 |<br>| —- | —- | —- | ——— |<br>| 1 | 2 | 6 | 3 |<br>| 2 | 4 | 10 | 2.5 |<br>| 3 | 6 | 12 | 2 |<br>| 4 | 7 | 13 | 1.86 |<br>| 1, 2 | 6 | 16 | 2.67 |<br>| 1, 3 | 8 | 18 | 2.25 |<br>| 1, 4 | 9 | 19 | 2.11 |<br>| 2, 3 | 10 | 22 | 2.2 |<br>| 2, 4 | 11 | 23 | 2.09 |<br>| 3, 4 | 13 | 25 | 1.92 |</p><p>根据价值，首先选择装载$3, 4$，但该方案重量为$13$不可行，选择$2, 4$可行，此时背包剩余空间$2$，剩余方案$1, 3, 1+3$按价值密度降序为$1, 1+3, 3$，但重量均超过$c$，故不继续装入背包，该方案最终为$x = [0, 1, 0, 1], w_x = 11, p_x = 23$。</p><p><strong>例(最小成本生成树)：给出加权图，求取最小生成树，使得成本(选取路径的成本之和)最低。</strong></p><p>$n$个节点的最小生成树有$n-1$条边，有三种不同的贪婪策略，产生两个算法：Kruskal、Prim。</p><p><img src="/2020/03/21/【算法】贪婪算法/kruskal_prim_sollin_eg.jpg" alt="kruskal_prim_sollin_eg"></p><ol><li><p>Kruskal<br> 分步选择$n-1$条边，从剩下的边中选择<strong>成本最小</strong>且不<strong>会产生回路</strong>的边，加入已选择的边集。<br> “不会产生回路”可通过检查<strong>待加入边两个顶点是否在已选定边组成的图中已连通</strong>判定，判断连通可通过<strong>检查其中一个顶点通过的路径中是否包含另一个顶点</strong>。用该算法对上图进行的生成树选择示意图如下<br> <img src="/2020/03/21/【算法】贪婪算法/kruskal_prim_sollin_kruskal.jpg" alt="kruskal_prim_sollin_kruskal"></p></li><li><p>Prim<br> 分步选择$n-1$条边，从剩下的边中选择<strong>成本最小</strong>的边，加入已选择的边集，<strong>使该边集构成一棵树</strong>。<br> 即选择<strong>与已选边集的顶点关联的、成本最小的边</strong>加入边集。<br> <img src="/2020/03/21/【算法】贪婪算法/kruskal_prim_sollin_prim.jpg" alt="kruskal_prim_sollin_prim"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】图</title>
      <link href="/2020/03/16/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%9B%BE/"/>
      <url>/2020/03/16/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="定义与概念"><a href="#定义与概念" class="headerlink" title="定义与概念"></a>定义与概念</h1><p><strong>定义</strong>：<strong>图</strong>(graph)是有限集$V$和$E$的<strong>有序对</strong>$G(V, E)$，其中$V$中的元素称为<strong>顶点</strong>(也称节点或点)，$E$的元素称为<strong>边</strong>(也称弧或线)。每一条边连接两个不同的顶点，可以用元组$(v_i, v_j)$来表示，$v_i, v_j$表示边所连接的两个顶点；当且仅当$(v_i, v_j)$是图的边，称顶点$v_i, v_j$是<strong>邻接的</strong>(adjacent)、边$(v_i, v_j)$<strong>关联</strong>(incident)于顶点$v_i, v_j$。</p><p>根据定义，一个图<strong>不能有重复的边</strong>；无向图任意两个节点间，<strong>最多有一条边</strong>，而有向图任意两个节点间<strong>每个方向最多有一条边</strong>；一个图不可能包含<strong>自连边</strong>(self-edge)或<strong>环</strong>(loop)，即$(v_i, v_i)$。</p><p>带方向的边叫<strong>有向边</strong>(directed edge)，不带方向的边叫<strong>无向边</strong>(undirected edge)；有向边$(v_i, v_j)$是<strong>关联至</strong>(incident to)顶点$v_j$而<strong>关联于</strong>(incident from)顶点$v_i$、顶点$v_i$<strong>邻接至</strong>(adjacent to)$v_j$，顶点$v_j$<strong>邻接于</strong>(adjacent from)$v_i$。</p><p>如果图的所有边都是无向边，那么该图叫做<strong>无向图</strong>(undirected graph)，如果所有的边都是有向边，那么该图叫做<strong>有向图</strong>(directed graph或digraph)。</p><p>若每条边被赋予一个表示成本的值，该值被称之为<strong>权</strong>，则此时图称为<strong>加权有向图</strong>(weighted digraph)和<strong>加权无向图</strong>(weighted undirected graph)。</p><p><strong>概念</strong>：</p><ul><li><strong>简单路径</strong>是除始点和终点外，其余顶点都不相同的路径。</li><li><strong>环路</strong>(cycle)是<strong>始点和终点相同</strong>的<strong>简单路径</strong>。</li><li>一个无向图$G = (V, E)$当且仅当$G$的<strong>每一对节点</strong>之间都有一条<strong>路径</strong>时，$G$是<strong>连通的</strong>(connected)，具有$n$个顶点的连通无向图至少有$n-1$条边。</li><li>没有环路的连通无向图是一棵<strong>树</strong>。</li><li>$G$的子图，如果包含$G$的所有顶点，且是一棵树，则称该子图为$G$的<strong>生成树</strong>(spanning tree)。</li></ul><blockquote><p>生成树可将网络建设成本减至最小，且保证网络的连通。</p></blockquote><p><img src="/2020/03/16/【数据结构】图/graph_eg.jpg" alt="graph_eg"></p><h1 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h1><p><strong>特性</strong>：设$G = (V, E)$是一个无向图，与顶点$v_i$相关联的边数称为该顶点的<strong>度</strong>(degree)$d_i$。则有</p><ol><li>$\sum_{i=1}^{|V|} d_i = 2 |E|$</li><li>$0 \leq |E| \leq \frac{|V|(|V| - 1)}{2}$</li></ol><p><strong>特性</strong>：设$G = (V, E)$是一个有向图中，顶点$v_i$的<strong>入度</strong>(in-degree)$d_i^{in}$是指关联至该顶点的边数，<strong>出度</strong>(out-degree)$d_i^{out}$是指关联于该顶点的边数，则有</p><ol><li>$\sum_{i=1}^{|V|} d_i^{in} = \sum_{i=1}^{|V|} d_i^{out} = |E|$</li><li>$0 \leq |E| \leq |V|(|V| - 1)$</li></ol><h1 id="图的描述"><a href="#图的描述" class="headerlink" title="图的描述"></a>图的描述</h1><h2 id="无权图"><a href="#无权图" class="headerlink" title="无权图"></a>无权图</h2><h3 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h3><p>一个$n$个顶点图$G = (V, E)$的<strong>邻接矩阵</strong>(adjacent matrix)是一个$n \times n$的矩阵$A$。对于无权无向图，元素定义如下</p><script type="math/tex; mode=display">A[i, j] = \begin{cases}    1 & (v_i, v_j) \in E 或 (v_j, v_i) \in E \\    0 & 其他 \end{cases}</script><p>对于无权有向图</p><script type="math/tex; mode=display">A[i, j] = \begin{cases}    1 & (v_i, v_j) \in E \\    0 & 其他 \end{cases}</script><p>如下图，两个图与其对应矩阵<br><img src="/2020/03/16/【数据结构】图/description_unweighted.jpg" alt="description_unweighted"></p><script type="math/tex; mode=display">\begin{aligned}    A_1 = \begin{bmatrix}        0 & 1 & 1 & 1 \\        1 & 0 & 1 & 0 \\        1 & 1 & 0 & 1 \\        1 & 0 & 1 & 0    \end{bmatrix}; &    A_2 = \begin{bmatrix}        0 & 1 & 1 & 1 \\        0 & 0 & 1 & 0 \\        0 & 0 & 0 & 1 \\        0 & 0 & 0 & 0    \end{bmatrix}\end{aligned}</script><p>以下改进可<strong>减少邻接矩阵存储空间</strong></p><ol><li>省略左对角线元素；</li><li>存储为稀疏矩阵；</li><li>无向图只存储上三角/下三角元素。</li></ol><h3 id="邻接链表-数组"><a href="#邻接链表-数组" class="headerlink" title="邻接链表/数组"></a>邻接链表/数组</h3><p>顶点$v_i$的<strong>邻接表</strong>(adjacent list)是一个线性表，可以用链表或数组描述，<strong>存储所有邻接与顶点$v_i$的顶点</strong>。一个图的邻接链表描述中，每个顶点都有一个邻接表。</p><p><img src="/2020/03/16/【数据结构】图/description_unweighted_linked_array.jpg" alt="description_unweighted_linked_array"></p><h2 id="有权图"><a href="#有权图" class="headerlink" title="有权图"></a>有权图</h2><p>将无权图的描述进行扩充，可得到加权图的描述。</p><ul><li>用<strong>成本邻接矩阵</strong>(cost-adjacency matrix)$C$描述加权图。$C[i, j]$表示边$(v_i. v_j)$的边的权，如果不存在需指定一个值，一般是很大的值；</li><li><strong>链表/数组描述</strong>中，节点元素用<strong>数对</strong>表示，添加成员权<code>weight</code>，可从相应的无权图的邻接链表/数组得到加权图的邻接链表/数组；</li></ul><p><img src="/2020/03/16/【数据结构】图/description_weighted.jpg" alt="description_weighted"></p><p>上图对应矩阵为</p><script type="math/tex; mode=display">\begin{aligned}    A_1 = \begin{bmatrix}         0 & 10 & 20 & 30 \\        10 &  0 & 40 &  0 \\        20 & 40 &  0 & 50 \\        30 &  0 & 50 &  0    \end{bmatrix}; &    A_2 = \begin{bmatrix}         0 & 10 & 20 & 30 \\         0 &  0 & 40 &  0 \\         0 &  0 &  0 & 50 \\         0 &  0 &  0 &  0    \end{bmatrix}\end{aligned}</script><p><img src="/2020/03/16/【数据结构】图/description_weighted_linked_array.jpg" alt="description_weighted_linked_array"></p><h1 id="图的遍历"><a href="#图的遍历" class="headerlink" title="图的遍历"></a>图的遍历</h1><p>图的遍历有两种常用的方法：广度优先搜索(breadth-first search, BFS)和深度优先搜索(depth-first search, DFS)，可用于搜索从某个顶点开始可达到的所有顶点。</p><h2 id="广度优先搜索"><a href="#广度优先搜索" class="headerlink" title="广度优先搜索"></a>广度优先搜索</h2><p><strong>广度优先搜索</strong>借助<strong>队列</strong>实现，算法描述如下</p><ol><li>从某个指定顶点$v$出发，初始化队列$Q$，将$v$<strong>入队</strong>，初始化；</li><li>从$Q$<strong>出队</strong>一个顶点$w$，若$w$存在邻接顶点，将所有邻接顶点入队，并将这些顶点标记为可达到顶点；否则跳过；</li><li>循环2直至$Q$为空。</li></ol><p><strong>广度优先生成树</strong>(breadth-first spanning tree)是按BFS所得到的生成树。</p><h2 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h2><p><strong>深度优先搜索</strong>用递归的方式实现，算法描述如下</p><ol><li>从某个指定顶点$v$出发，若$v$存在邻接顶点，对所有邻接顶点标记，并<strong>依次进行</strong>深度优先搜索；</li><li>直至递归完成。</li></ol><p><strong>深度优先生成树</strong>(depth-first spanning tree)是按DFS所得到的生成树。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>用Python语言简单实现上述两种遍历方式，并返回下图中的有向无权图的生成树<br><img src="/2020/03/16/【数据结构】图/bfs_dfs_eg.jpg" alt="bfs_dfs_eg"></p><p>该图用矩阵描述，为</p><script type="math/tex; mode=display">A = \begin{bmatrix}    0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\    0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\    0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\    0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\    0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \end{bmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bfs</span><span class="params">(A, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(n, n)&#125; 邻接矩阵</span></span><br><span class="line"><span class="string">        v: &#123;int&#125; 起始点标号</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        T: &#123;ndarray(n, n)&#125; 最小生成树</span></span><br><span class="line"><span class="string">        V: &#123;list&#125; 所有可达到的顶点</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    Q = [v]                 <span class="comment"># 用列表实现队列</span></span><br><span class="line">    T = np.zeros_like(A)    <span class="comment"># 最小生成树</span></span><br><span class="line">    V = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> len(Q) &gt; <span class="number">0</span>:</span><br><span class="line">        u = Q[<span class="number">0</span>]; Q.pop(<span class="number">0</span>)  <span class="comment"># 出队列</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> != A[u, j] <span class="keyword">and</span> j <span class="keyword">not</span> <span class="keyword">in</span> V:</span><br><span class="line">                T[u, j] = A[u, j]; V += [j]    <span class="comment"># 标记节点</span></span><br><span class="line">                Q += [j]    <span class="comment"># 入队列</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> T, V</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(A, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(n, n)&#125; 邻接矩阵</span></span><br><span class="line"><span class="string">        v: &#123;int&#125; 起始点标号</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        T: &#123;ndarray(n, n)&#125; 最小生成树</span></span><br><span class="line"><span class="string">        V: &#123;list&#125; 所有可达到的顶点</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dfs</span><span class="params">(u, T, V)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> != A[u, j] <span class="keyword">and</span> j <span class="keyword">not</span> <span class="keyword">in</span> V:</span><br><span class="line">                T[u, j] = A[u, j]; V += [j]</span><br><span class="line">                _dfs(j, T, V)       <span class="comment"># 迭代</span></span><br><span class="line"></span><br><span class="line">    T = np.zeros_like(A); V = []</span><br><span class="line">    _dfs(v, T, V)   <span class="comment"># 从v开始搜索</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> T, V</span><br></pre></td></tr></table></figure><p>主函数如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    A = [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    T1, V1 = bfs(np.array(A), <span class="number">0</span>)</span><br><span class="line">    T2, V2 = dfs(np.array(A), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    print(T1, V1, T2, V2)</span><br></pre></td></tr></table></figure></p><p>输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[[0 1 1 1 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 1 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 1 1 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 1 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 1 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]] </span><br><span class="line"> [1, 2, 3, 4, 5, 6, 7, 8] </span><br><span class="line"> </span><br><span class="line"> [[0 1 1 1 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 1 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 1 1 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 1 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 1 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]</span><br><span class="line"> [0 0 0 0 0 0 0 0 0 0]] </span><br><span class="line"> [1, 4, 7, 2, 3, 5, 6, 8]</span><br></pre></td></tr></table></figure></p><p>在本例中，两种方式生成树相同，示意图如下<br><img src="/2020/03/16/【数据结构】图/bfs_dfs_result.jpg" alt="bfs_dfs_result"></p><h1 id="最短路径问题"><a href="#最短路径问题" class="headerlink" title="最短路径问题"></a>最短路径问题</h1><p>可用<strong>Floyd算法</strong>计算每对节点间的最短路径：递推产生一个<strong>矩阵序列</strong>$A_1, \cdots, A_k, \cdots, A_n$，其中矩阵$A_k$的第$i$行第$j$列元素$A_k(i, j)$表示从顶点$v_i$到顶点$v_j$的路径上所经过的<strong>顶点个数不大于$k$的最短路径长度</strong>，计算时利用迭代公式</p><script type="math/tex; mode=display">\begin{aligned}    A_k(i, j) = \min \{ A_{k-1}(i, j), \quad A_{k-1}(i, t) + A_{k-1}(t, j) \}\\ 其中 k > 1, t = 1, \cdots, n\end{aligned}</script><p>应有</p><script type="math/tex; mode=display">A_1 = W</script><blockquote><p>即从$v_i$到$v_j$途中，依次经过最多$0, 1, 2, \cdots, k$个<strong>中转点</strong>，保存这些路径中最小的距离。</p></blockquote><p>下面是改进的Floyd算法，除了可求解每对节点间最短路径，还可将最短路径保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">floyd</span><span class="params">(W)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        W: &#123;ndarray(N, N)&#125; 邻接矩阵</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        A: &#123;ndarray(N, N)&#125; 两两之间最短路</span></span><br><span class="line"><span class="string">        R: &#123;list[list[list(k)](N)](N)&#125; 从节点`v_i`到节点`v_j`的最短路中间节点，保存在`R[i][j]`中</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A = W.copy(); N = A.shape[<span class="number">0</span>]</span><br><span class="line">    R = [[[] <span class="keyword">for</span> i <span class="keyword">in</span> range(N)] <span class="keyword">for</span> j <span class="keyword">in</span> range(N)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, N):       <span class="comment"># 第k次中间节点寻找</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(N):</span><br><span class="line">                <span class="comment"># 比较经过中转前后路径长度</span></span><br><span class="line">                d = np.minimum(A[i, j], A[i, :] + A[:, j])   <span class="comment"># i -&gt; t -&gt; j, t = 1, ..., N</span></span><br><span class="line">                min_d = np.min(d)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 保存最短路径</span></span><br><span class="line">                <span class="keyword">if</span> min_d &lt; A[i, j]:</span><br><span class="line">                    idx = np.argmin(d)</span><br><span class="line">                    R[i][j] = R[i][idx] + [idx] + R[idx][j]</span><br><span class="line">                <span class="comment"># 保存最短路径的长度</span></span><br><span class="line">                A[i, j] = min_d</span><br><span class="line">    <span class="keyword">return</span> A, R</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】平衡搜索树——分裂树和B树</title>
      <link href="/2020/03/15/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%B9%B3%E8%A1%A1%E6%90%9C%E7%B4%A2%E6%A0%91%E2%80%94%E2%80%94%E5%88%86%E8%A3%82%E6%A0%91%E5%92%8CB%E6%A0%91/"/>
      <url>/2020/03/15/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%B9%B3%E8%A1%A1%E6%90%9C%E7%B4%A2%E6%A0%91%E2%80%94%E2%80%94%E5%88%86%E8%A3%82%E6%A0%91%E5%92%8CB%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h1 id="分裂树"><a href="#分裂树" class="headerlink" title="分裂树"></a>分裂树</h1><h2 id="定义与概念"><a href="#定义与概念" class="headerlink" title="定义与概念"></a>定义与概念</h2><p>在字典的很多实际应用中，令我们更感兴趣的不是一个单独操作所需时间，而是一个操作序列所需时间，此时<strong>应用的时间复杂度取决于一个字典操作序列而不是任意一个操作</strong>。</p><p>伸展树基于以下<strong>假设</strong>：想要对一个二叉查找树执行<strong>一系列的查找操作</strong>，为了使整个查找时间更小，根据每次的搜索关键字对树的结构进行<strong>自调整</strong>，使得<strong>被查频率高</strong>的那些条目就应当经常处于<strong>靠近树根的位置</strong>。</p><p><strong>定义</strong>：<strong>分裂树</strong>(splay tree)，又叫伸展树，<strong>是一种二叉搜索树</strong>，对一个单独的字典操作，其时间复杂度是$O(n)$，而对于$f$个查找、$i$个插入、$d$个删除所组成的操作序列，其时间复杂度是$O((f + i + d) \log i)$，与使用AVL树或RB树的渐近时间复杂度相同，编码更容易。</p><p><strong>定义</strong>：<strong>分裂节点</strong>(slay node)是在字典操作中所检查的最深层的节点。<strong>插入操作</strong>时，可能生成新节点，或覆盖已存在节点。此时新<strong>生成的节点</strong>或<strong>被覆盖节点</strong>即分裂节点；<strong>删除操作</strong>时，被删除节点不在树中，不可能为分裂节点，所以<strong>被删除节点的父节点</strong>成为分裂节点。</p><p><strong>分裂操作</strong>是通过一个<strong>分裂步骤</strong>序列，将<strong>分裂节点</strong>移动到<strong>根节点的位置</strong>上。分裂步骤可将指定节点向上移动一层或两层，移动一层的可分为$L, R 2$种，移动两层的可分为$LL, LR, RR, RL 4$种。记$q$分别为分裂节点、$p, g$分别为$q$的父节点、祖父节点，<strong>当$g$存在时选用移动两层的分裂操作</strong>。分裂操作示意图如下，<strong>注意分裂步骤与AVL树的相似性和区别，子树$\mathcal{T}_g$的$LL/RR$型分裂操作是指自顶向下的，$LR/RL$型分裂操作是自底向上的。</strong></p><p><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/slay_options.jpg" alt="slay_options"></p><p>AVL树在插入时遇到$LL$型不平衡时，旋转矫正前后对比图如下</p><p><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/avl_insert_ll.jpg" alt="avl_insert_ll"></p><p>以下图所示的二叉搜索树为例，展示<strong>对节点$2$与节点$3$两次查询</strong>后，树的结构变化</p><p><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/splay_tree_eg.jpg" alt="splay_tree_eg"></p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>由<code>BinarySearchTree&lt;K, V&gt;</code>公有派生，添加分裂操作<code>slay</code>，重写插入<code>insert</code>、删除<code>erase</code>函数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplayTree</span> :</span> <span class="keyword">public</span> BinarySearchTree&lt;K, V&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">SplayTree() : BinarySearchTree&lt;K, V&gt;() &#123;&#125;</span><br><span class="line">~SplayTree()&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> K&amp;, <span class="keyword">const</span> V&amp;)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span><span class="params">(<span class="keyword">const</span> K&amp;)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">slay</span><span class="params">(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>分裂操作实现时有以下<strong>注意点</strong></p><ul><li>用$3bit$内存对$6$种情况进行编码：最高位为<code>0</code>表示一层的分裂操作，次高位表示$g$与$p$的位置关系，最低位表示$p$与$q$的位置关系；</li><li>若$L/R$型分裂时父节点为根节点，或$LL/LR/RR/RL$分裂时祖父节点为根节点，则需<strong>修改整棵树的根节点</strong>为分裂节点；</li><li>在$LR/RL$操作时，为<strong>自底向上</strong>操作，$p$节点地址为<code>node-&gt;parent</code>，第一次旋转结束后，$q$的父节点已经变为$g$，所以对$g$进行旋转时，该节点地址也为<code>node-&gt;parent</code>。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> SplayTree&lt;K, V&gt;::slay(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 分裂操作</span></span><br><span class="line"><span class="keyword">while</span> (node-&gt;parent) &#123;</span><br><span class="line"><span class="keyword">int</span> type = node-&gt;parent-&gt;isRoot() &lt;&lt; <span class="number">2</span>;</span><br><span class="line"><span class="keyword">if</span> (type) &#123;<span class="comment">// 无祖父节点，即父节点为根节点</span></span><br><span class="line">type += node-&gt;isLeft();</span><br><span class="line"><span class="keyword">this</span>-&gt;m_tnRoot = node;<span class="comment">// 修改根节点</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;<span class="comment">// 有祖父节点</span></span><br><span class="line">type += (node-&gt;parent-&gt;isLeft() &lt;&lt; <span class="number">1</span>) + node-&gt;isLeft();</span><br><span class="line"><span class="keyword">if</span> (node-&gt;parent-&gt;parent-&gt;isRoot())</span><br><span class="line"><span class="keyword">this</span>-&gt;m_tnRoot = node;<span class="comment">// 修改根节点</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> (type)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="number">0b0101</span>:<span class="comment">// L</span></span><br><span class="line">SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="number">0b0100</span>:<span class="comment">// R</span></span><br><span class="line">SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="number">0b0011</span>:<span class="comment">// LL</span></span><br><span class="line">SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent-&gt;parent);</span><br><span class="line">SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="number">0b0010</span>:<span class="comment">// LR</span></span><br><span class="line">SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent);</span><br><span class="line">SplayTree&lt;K, V&gt;::leftRotate (node-&gt;parent);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="number">0b0000</span>:<span class="comment">// RR</span></span><br><span class="line">SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent-&gt;parent);</span><br><span class="line">SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="number">0b0001</span>:<span class="comment">// RL</span></span><br><span class="line">SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent);</span><br><span class="line">SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在搜索二叉树的插入、删除基础上，添加分类操作<code>slay()</code>即可</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> SplayTree&lt;K, V&gt;::insert(<span class="keyword">const</span> K&amp; key, <span class="keyword">const</span> V&amp; value)</span><br><span class="line">&#123;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = <span class="keyword">this</span>-&gt;find(key, <span class="literal">true</span>);</span><br><span class="line">Pair&lt;K, V&gt;* p = node-&gt;get();</span><br><span class="line"><span class="keyword">if</span> (p-&gt;getKey() != key)</span><br><span class="line">p-&gt;setKey(key);</span><br><span class="line">p-&gt;setVal(value);</span><br><span class="line"></span><br><span class="line">slay(node);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> SplayTree&lt;K, V&gt;::erase(<span class="keyword">const</span> K&amp; key)</span><br><span class="line">&#123;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = <span class="keyword">this</span>-&gt;find(key, <span class="literal">false</span>);</span><br><span class="line"><span class="keyword">if</span> (!node) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* slayNode = node-&gt;parent;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ------------------ 重新组织二叉树 ------------------</span></span><br><span class="line"><span class="comment">// 查找左子树的最大值，或右子树的最小值</span></span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* replace = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">while</span> (!node-&gt;isLeaf()) &#123;<span class="comment">// 直到搜索到叶节点为止</span></span><br><span class="line"><span class="keyword">if</span> (node-&gt;left) &#123;</span><br><span class="line">replace = <span class="keyword">this</span>-&gt;max(node-&gt;left);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (node-&gt;right) &#123;</span><br><span class="line">replace = <span class="keyword">this</span>-&gt;min(node-&gt;right);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (replace) &#123;<span class="comment">// 找到可替换子节点</span></span><br><span class="line">Pair&lt;K, V&gt;* np = node-&gt;get();</span><br><span class="line">Pair&lt;K, V&gt;* rp = replace-&gt;get();</span><br><span class="line">np-&gt;setKey(rp-&gt;getKey());</span><br><span class="line">np-&gt;setVal(rp-&gt;getVal());</span><br><span class="line">node = replace;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (node-&gt;parent) &#123;<span class="comment">// 修改父节点信息</span></span><br><span class="line"><span class="keyword">if</span> (node == node-&gt;parent-&gt;left)</span><br><span class="line">node-&gt;parent-&gt;left = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">node-&gt;parent-&gt;right = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;<span class="comment">// 无父节点，即整棵树只有一个根节点，则修改根节点为空</span></span><br><span class="line"><span class="keyword">this</span>-&gt;m_tnRoot = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ------------------ 释放资源 ------------------</span></span><br><span class="line">Pair&lt;K, V&gt;* pair = node-&gt;get();</span><br><span class="line"><span class="keyword">delete</span> pair; <span class="keyword">delete</span> node;</span><br><span class="line"></span><br><span class="line">slay(slayNode);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">string</span> names[<span class="number">6</span>] = &#123;</span><br><span class="line"><span class="string">"甲"</span>, <span class="string">"乙"</span>, <span class="string">"丙"</span>, <span class="string">"丁"</span>, <span class="string">"戊"</span>, <span class="string">"己"</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> numbers[<span class="number">6</span>] = &#123; <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span> &#125;;</span><br><span class="line"></span><br><span class="line">SplayTree&lt;<span class="keyword">int</span>, <span class="built_in">string</span>&gt; tree;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入键值对</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"插入"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">tree.insert(numbers[i], names[i]);</span><br><span class="line">&#125;</span><br><span class="line">tree.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询2</span></span><br><span class="line">tree.insert(<span class="number">6</span>, <span class="string">"乙"</span>);</span><br><span class="line">tree.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询3</span></span><br><span class="line">tree.insert(<span class="number">3</span>, <span class="string">"丙"</span>);</span><br><span class="line">tree.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除2</span></span><br><span class="line">tree.erase(<span class="number">2</span>);</span><br><span class="line">tree.print();</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下均为<strong>层级遍历</strong>输出，可以看到对指定关键字进行字典操作后，分裂节点将被调整至根节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">插入</span><br><span class="line">[6]己 [5]戊 [4]丙 [1]丁 [2]甲 [3]乙</span><br><span class="line">[6]乙 [5]戊 [4]丙 [1]丁 [2]甲 [3]乙</span><br><span class="line">[3]丙 [2]甲 [6]乙 [1]丁 [4]丙 [5]戊</span><br><span class="line">[3]丙 [1]丁 [6]乙 [4]丙 [5]戊</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure><h1 id="B-树"><a href="#B-树" class="headerlink" title="B-树"></a>B-树</h1><p>当字典足够小时，AVL树和RB树都能保证良好的时间性能。而对于存储在磁盘上的大型字典(外部字典或文件)，则<strong>需要度数更高的搜索树</strong>来改善字典操作性能。</p><h2 id="定义与概念-1"><a href="#定义与概念-1" class="headerlink" title="定义与概念"></a>定义与概念</h2><h3 id="索引顺序访问方法-ISAM"><a href="#索引顺序访问方法-ISAM" class="headerlink" title="索引顺序访问方法(ISAM)"></a>索引顺序访问方法(ISAM)</h3><p><strong>索引顺序访问方法</strong>(indexed sequential access method, ISAM)本质上是一种数组描述方法。在该方法中，可用的磁盘空间被划分为很多块，<strong>字典元素以升序存储在块中</strong>。</p><p>在<strong>顺序访问</strong>时，块按序输入，每一个块中元素按升序搜索；若要支持<strong>随机访问</strong>，就要维持一个索引，索引包括每个块的最大关键字，且组成的<strong>块索引表</strong>足以驻留在内存中，在访问关键字为$k$的元素时，先查找索引表确定<strong>块索引</strong>，然后把相应的块从磁盘中读取进行内部搜索，这样仅需一次磁盘访问即可完成。</p><p>若同时存在多个磁盘，每个磁盘存在一个<strong>磁盘索引</strong>，保存该磁盘中的最大关键字，全部磁盘索引组成<strong>磁盘索引表</strong>。在查找元素时，按<strong>磁盘索引、块索引、内部搜索的顺序进行元素查找</strong>，此时需要两次磁盘访问(读取磁盘索引、磁盘中读取块)。</p><blockquote><ul><li><strong>磁盘的块</strong>是磁盘空间中用来<strong>输入或输出的最小单位</strong>，一般具有和此道一样的长度，且<strong>可以在一次搜索和延迟中完成输入或输出</strong>，按照一种顺序来组织，使得从一块到另一块的延时最短；</li><li>为减少块与块之间的元素拷贝，可在每块中预留一些空间供少量元素的插入，删除时可将闲置空间保留，不必进行元素移动操作。</li></ul></blockquote><p>对于存储在磁盘上的数据，B-树是一种适合于索引方法的数据结构。</p><h3 id="m-叉搜索树"><a href="#m-叉搜索树" class="headerlink" title="$m$叉搜索树"></a>$m$叉搜索树</h3><p><strong>定义</strong>：<strong>$m$叉搜索树</strong>(m-way search tree)可以是一棵空树，如果非空，需满足以下特征：</p><ol><li>进行扩充(外部节点替换空指针)后的搜索树，每个<strong>内部节点</strong>最多包含$m$个孩子、$1 \sim m-1$个元素；</li><li>包含$p$个元素($k_1, \cdots, k_p$)的节点有$p + 1$个孩子($c_0, c_1, \cdots, c_p$)；</li><li>对于包含$p$个元素的节点，<strong>元素关键字</strong>满足$k_i &lt; k_{i + 1}, 1 \leq i \leq p - 1$；</li><li>对于包含$p$个元素的节点，<strong>子树的元素关键字</strong>满足：<ul><li>子树$\mathcal{T}_{c_0}$的元素关键字都小于$k_1$；</li><li>子树$\mathcal{T}_{c_p}$的元素关键字都大于$k_p$；</li><li>$\mathcal{T}_{c_i}(0 &lt; i &lt; p)$中的元素关键字大于$k_i$而小于$k_{i+1}$。</li></ul></li></ol><p>下图为省略外部节点的$7$叉搜索树</p><p><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/m_way_search_tree_eg.jpg" alt="m_way_search_tree_eg"></p><p>一棵高度为$h$的$m$叉搜索树(不包含外部节点)，最少有$h$个元素，最多有$m^h-1$个元素。</p><blockquote><ul><li>当每层仅有$1$个节点，且每个节点只有$1$个元素时，这棵树元素个数最少；</li><li>第$i(i=1, …, h)$层最多有$m^{h-1}$个节点，而每个节点最多有$m-1$个元素，则总的元素个数为$(m - 1) \times \sum_{i=1}^h m^{j-1} = m^h - 1$。</li></ul></blockquote><p>对$m$叉搜索树进行<strong>查找</strong>时，类似二叉搜索树。从根节点开始搜索关键字$k$，若根节点中不存在查找关键字，则根据特征4，在对应的子节点中进行搜索，直至找到对应关键字或空节点(外部节点)为止。</p><blockquote><p>例如在上图中查找关键字为$33$的数值对时，从根节点出发，在根结点中不存在$33$，在其$10-18$间的子节点内搜索，而该子节点中也不存在$33$，在$30-40$间的子节点内搜索，还是不存在，于是在$32-36$的子节点中查找，但此时为外部节点，故整棵树中$33$不存在，退出算法。</p></blockquote><p><strong>插入操作</strong>时，首先查找指定的关键字$k$，<strong>若$k$存在</strong>，则将值修改后退出算法；<strong>若$k$不存在</strong>，并且<strong>搜索路径上存在元素个数少于$m$、满足特征4的节点</strong>，将其插入，如果找不到这样的节点，则在最后一个节点内，根据元素大小，选择合适的节点位置<strong>创建新节点</strong>，然后将数值对插入。</p><blockquote><p>例如，插入键为$33$的数值对时，用查找算法搜索确定不存在相同键的元素，每个节点可容纳$6$个元素，此时节点$[10, 18]$元素未满，但$33$比子节点$[20, 30, 40, 50, 60, 70]$中元素小，故不满足特征$4$，子节点$[20, 30, 40, 50, 60, 70]$元素已满，第$3$层$[32, 36]$符合要求，故将数值对插入该节点，成为第$2$个元素，即$[32, 33, 36]$。</p></blockquote><p><strong>删除</strong>某个元素时，若该元素相邻子节点空，可直接删除，否则将该元素与<strong>相邻</strong>的<strong>左子节点中的键最大元素</strong>，或<strong>右子节点中最键小元素</strong>替换，再将元素删除。</p><h3 id="m-阶B-树"><a href="#m-阶B-树" class="headerlink" title="$m$阶B-树"></a>$m$阶B-树</h3><p><strong>定义</strong>：$m$阶B-树(B-Tree of order $m$)是一棵$m$叉搜索树，如果B-树非空，那么相应的<strong>扩充树满足</strong>以下特征：</p><ol><li>根节点至少有$2$个孩子，$1$个元素；</li><li>除根节点外，内部节点至少有$d = \lceil m/2 \rceil$个孩子，$d-1$个元素；</li><li>外部节点在同一层。</li></ol><ul><li>$2$阶B-树，内部节点(包括根节点)都恰好有$2$个孩子，且外部节点在同一层，所以$2$阶B-树是满二叉树；</li><li>$3$阶B-树，内部节点(除根节点)有$2 \sim 3$个孩子，也称$2-3$树；</li><li>$4$阶B-树，内部节点(除根节点)有$2 \sim 4$个孩子，也称$2-3-4$树；</li></ul><blockquote><p>疑问：$2$阶B-树特征2：$\lceil m/2 \rceil = 1$，为什么内部节点至少有$2$个孩子？</p></blockquote><p>下图为一棵$2-3$树，当插入$14,16$到$[10]$节点后，改树又变成了$2-3-4$树</p><p><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/3_order.jpg" alt="3_order"></p><p><strong>定理</strong>：设$\mathcal{T}$是一棵高度为$h$的$m$阶B-树。令$d = \lceil m/2 \rceil$，$n$是$\mathcal{T}$的元素个数，那么</p><ol><li>$2d^{h-1} - 1 \leq n \leq m^h - 1$</li><li>$\log_m (n + 1) \leq h \leq \log_d(\frac{n + 1}{2}) + 1$</li></ol><blockquote><ol><li>第$1, 2$层的节点最少个数为$1, 2$，第$3, 4, \cdots, h$层的节点的父节点至少有$d$个孩子，那么这些层的节点最少个数为$2d, 2d^2, \cdots, 2d^{h-2}$，每个节点有$d-1$个元素，故下限为$(1 + 2 + 2d + 2d^2 + \cdots + 2d^{h-2}) \times (d - 1) = 2d^{h-1} - 1$；上限由$m$叉搜索树决定；</li><li>由1可推得2。</li></ol></blockquote><p><strong>B-树的搜索与$m$叉搜索树算法相同</strong>，<strong>插入操作</strong>有所区别，首先搜索相同关键字的元素，若不存在，可将元素插入在搜索路径中<strong>最后一个内部节点</strong>中，但<strong>如果该节点已饱和，对节点进行元素插入时需要分裂该节点</strong>。</p><p>现将<strong>带有空指针</strong>的新元素$e$插入饱和节点$P$，得到具有$m$个元素和$m+1$个孩子的<strong>溢出节点</strong>，$(e_i, c_i)$表示元素与该元素的右子节点，那么溢出节点可用序列表示如下</p><script type="math/tex; mode=display">m, (c_0), (e_1, c_1), \cdots, (e_m, c_m)</script><p><strong>使该节点在元素$e_d, d = \lceil m/2 \rceil$处分裂，将右边的节点保存在新节点$Q$中，再将$(e_d, Q)$插入$P$的父节点，使$e_d$成为$P$父节点的元素，$Q$成为$P$的兄弟节点</strong>，完成分裂，此时两节点序列为</p><script type="math/tex; mode=display">\begin{aligned}    P: d-1, (c_0), (e_1, c_1), \cdots, (e_{d-1}, c_{d-1}) \\    Q: m-d, (c_d), (e_{d+1}, c_{d+1}), \cdots, (e_m, c_m)\end{aligned}</script><p><strong>若$e_d$插入父节点导致父节点满，则需要对父节点进行分裂操作，以此类推直到至根节点路径上没有饱和节点</strong>。</p><p>当插入操作引起$s$个节点分裂时，磁盘访问的次数为$h + 2s + 1$(读取搜索路径上的节点 + 回写分裂出的两个新节点 + 回写新的根节点火插入后没有导致分裂的节点)，最多可达到$3h + 1$。</p><blockquote><p>以下图$7$阶B-树为例，插入$3$时，在最后一个内部节点节点$[2, 4, 6]$处搜索失败，且该节点未饱和，将其插入该节点得到$[2, 3, 4, 6]$<br><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/insert_3.jpg" alt="insert_3"></p><p>而插入元素$25$时，节点$[20, 30, 40, 50, 60, 70]$是最后一个内部节点，但已饱和，需要进行分裂操作。将新元素插入后溢出节点序列为</p><script type="math/tex; mode=display">7, (0), (20, 0), (25, 0), (30, 0), (40, 0), (50, 0), (60, 0), (70, 0)</script><p>此时$d = 4$，在元素$40$处进行分裂，得到两个新节点序列</p><script type="math/tex; mode=display">\begin{aligned}    P: 3, (0), (20, 0), (25, 0), (30, 0) \\    Q: 3, (0), (50, 0), (60, 0), (70, 0)\end{aligned}</script><p>将$(40, Q)$插入父节点，此时父节点为($L, R$表示$10$左边的节点，$R$表示$80$右边的节点)</p><script type="math/tex; mode=display">3, (L), (10, P), (40, Q), (80, R)</script><p>如下图<br><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/insert_25.jpg" alt="insert_25"></p></blockquote><p><strong>删除操作</strong>可分为两种情况：</p><ol><li>该元素位于<strong>非叶子节点</strong><br> 用被删除元素的<strong>左相邻子树的最大元素</strong>，或<strong>右相邻子树的最小元素</strong>替换被删除元素，<strong>替换元素必在叶节点</strong>，此时转化为第2种情况；</li><li>该元素位于<strong>叶子节点</strong><ol><li>被删除元素所在叶节点的元素个数大于最少数(性质1)，那么<strong>直接删除</strong>；</li><li>被删除元素在非根节点且该节点元素个数最少，可用其<strong>最近邻的左兄弟节点中最大元素</strong>或<strong>最近邻的右兄弟中的最小元素</strong>替换；</li><li>最近邻兄弟节点不包含额外元素时，<strong>将删除元素的节点($d-2$个元素)、最近邻兄弟节点($d-1$个元素)、父节点中介于两兄弟间的元素</strong>合并成一个节点($2d-2$个元素)；</li><li>若合并导致父节点元素个数低于最小元素个数，<strong>对该父节点重复上述操作</strong>，直至根节点(最坏情况下)。</li></ol></li></ol><blockquote><p>以$7$阶B-树为例，删除元素$99$<br><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/delete_90.jpg" alt="delete_90"><br>首先，寻找可以替换的叶子节点中的元素，可选择左近邻节点中的$86$，或右近邻中的$92$。</p><ul><li>当选择$86$作为替换元素时，节点$2, (0), (82, 0), (84, 0)$缺少一个元素，此时可从该节点的最近邻左兄弟节点取择元素$70$加入<br><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/delete_90_86.jpg" alt="delete_90_86"></li><li>当选择$96$作为替换元素时，节点$2, (0), (92, 0), (94, 0)$缺少一个元素，且$3, (0), (82, 0), (84, 0), (84, 0)$不包含多余元素，那么将这两个节点与父节点中的$92$合并为$6, (0), (82, 0), (84, 0), (84, 0), (86, 0), (92, 0), (94, 0), (96, 0)$<br><img src="/2020/03/15/【数据结构】平衡搜索树——分裂树和B树/delete_90_96.jpg" alt="delete_90_96"><br>以上两种方式都不会使根节点少于$2$个孩子。</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】平衡搜索树——AVL树与RB树</title>
      <link href="/2020/03/14/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%B9%B3%E8%A1%A1%E6%90%9C%E7%B4%A2%E6%A0%91%E2%80%94%E2%80%94AVL%E6%A0%91%E4%B8%8ERB%E6%A0%91/"/>
      <url>/2020/03/14/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%B9%B3%E8%A1%A1%E6%90%9C%E7%B4%A2%E6%A0%91%E2%80%94%E2%80%94AVL%E6%A0%91%E4%B8%8ERB%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p><strong>定义</strong>：最坏情况下高度为$O(\log n)$的树称为<strong>平衡树</strong>(balanced tree)。如果搜索树的高度总是$O(\log n)$，就能保证查找、插入和删除的时间为$O(\log n)$。</p><p>在实际应用中，当操作是以关键字进行查找、插入和删除时，散列技术在性能方面超过了平衡搜索树。但以下情况提倡使用平衡搜索树</p><ol><li><strong>按照关键字</strong>实施字典操作，而且<strong>操作时间</strong>不能超过指定的范围；<br>2.<strong> 按名次</strong>实施查找和删除；</li><li><strong>不按精确的关键字匹配</strong>进行的字典操作(如寻找关键字大于$k$的最小元素)。</li></ol><p>本节包含两种平衡二叉树结构——AVL和红-黑树(适合内部存储的应用)，下一节介绍B-树(度数更大，高度更小，适合外部存储的应用，如磁盘上的大型词典)。<strong>这些平衡树结构能在最坏情况下用时$O(\log n)$实现字典操作和按名次操作。</strong></p><h1 id="搜索二叉树的旋转"><a href="#搜索二叉树的旋转" class="headerlink" title="搜索二叉树的旋转"></a>搜索二叉树的旋转</h1><p>记对子树$\mathcal{T_N}$进行的左旋操作为$lr(\mathcal{N})$，右旋操作为$rr(\mathcal{N})$</p><ul><li>搜索二叉树进行左旋或右旋后，该树根节点发生变化，仍满足搜索二叉树结构，可用于平衡搜索树的不平衡矫正；</li><li><strong>单旋转</strong>(single rorating)是对子树的根节点进行一次旋转；</li><li><strong>双旋转</strong>(double rotating)是对根节点进行旋转前，先对其中一棵子树进行旋转。</li></ul><p>下图为对以$\mathcal{A}$为根节点的树$\mathcal{T_A}$进行<strong>左旋、右旋</strong>的示意图，</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/rotating.jpg" alt="rotating"></p><blockquote><p>巧记：以旋转的树的根节点$\mathcal{N}$与$\mathcal{N_L}, \mathcal{N_R}$作圆，<strong>右旋时，该圆下方向右</strong>。</p></blockquote><p><code>BinarySearchTree&lt;K, V&gt;</code>添加静态成员函数如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinarySearchTree</span> :</span> </span><br><span class="line"><span class="keyword">public</span> LinkedBinaryTree&lt;Pair&lt;K, V&gt;*&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">leftRotate</span><span class="params">(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">rightRotate</span><span class="params">(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> BinarySearchTree&lt;K, V&gt;::leftRotate(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 修改根节点</span></span><br><span class="line"><span class="keyword">if</span> (A-&gt;parent) &#123;</span><br><span class="line"><span class="keyword">if</span> (A == A-&gt;parent-&gt;left)</span><br><span class="line">A-&gt;parent-&gt;left = A-&gt;left;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">A-&gt;parent-&gt;right = A-&gt;left;</span><br><span class="line">&#125;</span><br><span class="line">A-&gt;left-&gt;parent = A-&gt;parent;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改A为Al右孩</span></span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* Alr = A-&gt;left-&gt;right;</span><br><span class="line">A-&gt;parent = A-&gt;left;</span><br><span class="line">A-&gt;parent-&gt;right = A;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改Alr为A左孩</span></span><br><span class="line">A-&gt;left = Alr;</span><br><span class="line"><span class="keyword">if</span> (Alr) Alr-&gt;parent = A;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> BinarySearchTree&lt;K, V&gt;::rightRotate(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 修改根节点</span></span><br><span class="line"><span class="keyword">if</span> (A-&gt;parent) &#123;</span><br><span class="line"><span class="keyword">if</span> (A == A-&gt;parent-&gt;left)</span><br><span class="line">A-&gt;parent-&gt;left = A-&gt;right;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">A-&gt;parent-&gt;right = A-&gt;right;</span><br><span class="line">&#125;</span><br><span class="line">A-&gt;right-&gt;parent = A-&gt;parent;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改A为Ar左孩</span></span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* Arl = A-&gt;right-&gt;left;</span><br><span class="line">A-&gt;parent = A-&gt;right;</span><br><span class="line">A-&gt;parent-&gt;left = A;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改Arl为A右孩</span></span><br><span class="line">A-&gt;right = Arl;</span><br><span class="line"><span class="keyword">if</span> (Arl) Arl-&gt;parent = A;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="AVL树"><a href="#AVL树" class="headerlink" title="AVL树"></a>AVL树</h1><p>AVL树是<strong>通过限制左右子节点高度差</strong>进行整棵树的高度限制。</p><h2 id="定义及概念"><a href="#定义及概念" class="headerlink" title="定义及概念"></a>定义及概念</h2><p><strong>定义</strong>：<strong>AVL树</strong>由Adelson-Velskii和Landis在1962年提出。<strong>一棵空的二叉树是AVL树</strong>；若$\mathcal{T}$是一棵非空二叉树，$\mathcal{T_L}$和$\mathcal{T_R}$是其左子树和右子树，那么<strong>满足以下条件</strong>时，$\mathcal{T}$是一棵AVL树：</p><ol><li>$\mathcal{T_L}$和$\mathcal{T_R}$是AVL树；</li><li>$|h_{\mathcal{L}} - h_{\mathcal{R}}| \leq 1$，其中$|h_{<em>}|$是子树$\mathcal{T_</em>}$的高。</li></ol><p>既是二叉搜索树，也是AVL树的树称为<strong>AVL搜索树</strong>；既是索引二叉搜索树，也是AVL树的树称为<strong>索引AVL搜索树</strong>。下图为一棵AVL搜索树</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/avl_tree_eg.jpg" alt="avl_tree_eg"></p><p>AVL树具有以下<strong>特征</strong></p><ol><li>一棵$n$个元素的AVL树，其<strong>高度</strong>是$O(\log n)$，最坏情况下，AVL树的高度是搜索树中最小的；</li><li>对于每一个$n$，$n \geq 0$，都<strong>存在</strong>一棵AVL树；</li><li>对一棵$n$个元素的AVL搜索树，在$O(height) = O(\log n)$的时间内可以实现<strong>查找</strong>；</li><li>将一个元素<strong>插入</strong>一棵$n$个元素的AVL搜索树，可以得到$n+1$个元素的AVL树，且插入用时为$O(\log n)$；</li><li>将一个元素从一棵$n$个元素的AVL搜索树中<strong>删除</strong>，可以得到$n-1$个元素的AVL树，且删除用时为$O(\log n)$。</li></ol><blockquote><p><strong>关于AVL树的高度</strong>，对一棵高度为$h$的AVL树，令$N_h$是其最少的结点数。在最坏情况下，根的一棵子树高度为$h-1$，另一棵子树高度为$h-2$，那么</p><script type="math/tex; mode=display">N_h = N_{h-1} + N_{h-2} + 1, N_0 = 0 且 N_1 = 1</script><p>注意由斐波那契数列定义</p><script type="math/tex; mode=display">F_n = F_{n-1} + F_{n-2}, F_0 = 0且F_1 = 1</script><p>可证得</p><script type="math/tex; mode=display">N_h = F_{h+2} - 1 , h \geq 0</script><p>由斐波那契定理</p><script type="math/tex; mode=display">F_h \approx \Phi^h / \sqrt{5}, 其中\Phi = (1 + \sqrt{5}) / 2</script><p>那么</p><script type="math/tex; mode=display">\begin{aligned}    N_h \approx \Phi^{h+2} / \sqrt{5} - 1 \\    \Rightarrow h = \log_{\Phi} \left[ \sqrt{5}(N_h + 1) \right] - 2 \\    \approx 1.44 \log_2(n + 2) = O(\log n)\end{aligned}</script></blockquote><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>AVL树的节点$\mathcal{N}$增加<strong>平衡因子</strong></p><script type="math/tex; mode=display">bf(\mathcal{N}) = h(\mathcal{T_L}) - h(\mathcal{T_L})</script><p>其中$h(\mathcal{<em>})$为树$\mathcal{T_{</em>}}$的高度，由平衡搜索定义可知，<strong>平衡因子取值范围</strong>为$\{ 0, \pm 1, \pm 2 \}$，$bf = \pm2$时表示以该节点为根节点的子树不平衡。</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/balanced_factor.jpg" alt="balanced_factor"></p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>AVL搜索树的搜索，沿用二叉搜索树的搜索方式即可，搜索时间与高度成正比，为$O(\log n)$。</p><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>记</p><ul><li>$\mathcal{X}$为<strong>寻找插入节点位置路径上最后一个具有平衡因子$\pm 1$的节点</strong></li><li>$\mathcal{A}$为距新插入节点$\mathcal{N}$<strong>最近的、平衡因子为$\pm 2$的祖先节点</strong></li></ul><p>以下图为例，插入关键字为$36$的新节点$\mathcal{N}$，导致右子树失去AVL树的平衡结构<br><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/insert1.jpg" alt="insert1"></p><h4 id="插入时有以下特性"><a href="#插入时有以下特性" class="headerlink" title="插入时有以下特性"></a>插入时有以下特性</h4><ol><li>只有从根节点到新插入节点的<strong>路径上的节点</strong>，其平衡因子在插入后会改变；</li><li>插入操作只会使平衡因子<strong>增减$0$或$1$</strong>，平衡因子为$\pm 2$的节点在插入前，平衡因子为$\pm 1$，插入操作后$bf(\mathcal{X})$从$\pm 1$变为$\pm 2$是<strong>导致AVL失去平衡的唯一过程</strong>；</li><li>从$\mathcal{A}$到$\mathcal{N}$的路径上，<strong>在插入操作前</strong>，所有节点的平衡因子都是$0$。</li></ol><h4 id="节点-mathcal-A-的不平衡情况"><a href="#节点-mathcal-A-的不平衡情况" class="headerlink" title="节点$\mathcal{A}$的不平衡情况"></a>节点$\mathcal{A}$的不平衡情况</h4><ul><li>可分为$L$型不平衡($\mathcal{N}$在$\mathcal{A}$左子树中)与$R$型不平衡($\mathcal{N}$在$\mathcal{A}$右子树中)两类；</li><li>根据$\mathcal{A}$的<strong>孙节点</strong>情况，又可分为$LL$(孙节点在$\mathcal{A}$左子树的左子树中), $LR$(孙节点在$\mathcal{A}$左子树的右子树中)，$RL$，$RR$不平衡。</li></ul><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/insert2.jpg" alt="insert2"></p><h4 id="不平衡的矫正"><a href="#不平衡的矫正" class="headerlink" title="不平衡的矫正"></a>不平衡的矫正</h4><ul><li><strong>LL型不平衡的矫正</strong>，只需将$\mathcal{T_A}$进行左旋；<strong>RR型同理</strong>；</li><li><strong>LR型不平衡的矫正</strong>，，对$LR$型不平衡所作的<strong>双旋转</strong>可视作$\mathcal{T_{A_L}}$的右旋，再进行$\mathcal{T_A}$的左旋，<strong>RL型同理</strong>；</li><li>由于插入前子树的高度与插入并矫正后的子树高度相同，故在一次旋转后，整棵树是平和的，<strong>不需要继续向根节点搜索不平衡节点</strong>；</li></ul><div class="table-container"><table><thead><tr><th>不平衡类型</th><th>LL</th><th>LR</th><th>RR</th><th>RL</th></tr></thead><tbody><tr><td>矫正</td><td>$lr(\mathcal{T_{A}})$</td><td>$rr(\mathcal{T_{A_L}}) + lr(\mathcal{T_{A}})$</td><td>$rr(\mathcal{T_{A}})$</td><td>$lr(\mathcal{T_{A_R}}) + rr(\mathcal{T_{A}})$</td></tr></tbody></table></div><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/insert3.jpg" alt="insert3"><br><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/insert4.jpg" alt="insert4"></p><h4 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a>算法描述</h4><ol><li>给定元素的关键字$\mathcal{K}$，从根结点出发查找，若<strong>找到具有相同关键字的节点</strong>$\mathcal{N}$，则返回该节点，否则进入2；</li><li>从根结点出发查找插入位置，并在该位置创建新节点$\mathcal{N}$，更新$\mathcal{N}$至根节点的路径上的节点的<strong>平衡因子</strong>，若不存在$bf=\pm 2$的节点，返回$\mathcal{N}$退出算法，否则进入3；</li><li>记距离$\mathcal{N}$最近的、且$bf = \pm 2$的祖先节点为$\mathcal{A}$，确定$\mathcal{A}$的<strong>不平衡类型</strong>，并执行相应旋转；</li><li>更新$\mathcal{N}$相关节点的平衡因子，返回$\mathcal{N}$退出算法。</li></ol><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>记</p><ul><li>删除节点$\mathcal{N}$的父节点为$\mathcal{P}$；</li><li>从$\mathcal{P}$至根节点的路径上第一个平衡因子变为$\pm 2$的节点记作$\mathcal{A}$。</li></ul><p>如下图，删除关键字为$25$的结点后，该树不满足平衡搜索树条件<br><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete1.jpg" alt="delete1"></p><h4 id="删除时有以下特性"><a href="#删除时有以下特性" class="headerlink" title="删除时有以下特性"></a>删除时有以下特性</h4><ul><li>节点$\mathcal{N}$删除后，从根节点到$\mathcal{P}$路径上的<strong>一些节点或全部节点的平衡因子都改变</strong>了，故要从$\mathcal{P}$原路返回；</li><li>如删除发生在$\mathcal{P_L}$(左子节点)，那么$bf(\mathcal{P})$减$1$，发生在$\mathcal{P_R}$(右子节点)时，$bf(\mathcal{P})$加$1$；</li><li>$bf(\mathcal{P})$更新后记作$bf_*(\mathcal{P})$，则有<script type="math/tex; mode=display">  \begin{cases}      bf_*(\mathcal{P}) = 0 && \mathcal{T_P}高度减少1，需改变它的祖先节点的平衡因子 \\      bf_*(\mathcal{P}) = \pm 1 && \mathcal{T_P}高度不变，无需改变它的祖先节点的平衡因子 \\      bf_*(\mathcal{P}) = \pm 2 && \mathcal{P}不平衡  \end{cases}</script></li></ul><h4 id="节点-mathcal-A-的不平衡情况-1"><a href="#节点-mathcal-A-的不平衡情况-1" class="headerlink" title="节点$\mathcal{A}$的不平衡情况"></a>节点$\mathcal{A}$的不平衡情况</h4><ul><li>可分为$L$型不平衡($\mathcal{N}$在$\mathcal{A}$左子树中，$bf(\mathcal{P}) = -2$)与$R$型不平衡($\mathcal{N}$在$\mathcal{A}$右子树中，$bf(\mathcal{P}) = 2$)<strong>两类</strong>；</li><li>若发生$R$型不平衡，考察$\mathcal{P}$的左子节点$\mathcal{P_L}$，根据$bf(\mathcal{P_L})$又<strong>可分为$R0, R1, R-1$型不平衡</strong>；$L$型同理；</li></ul><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete2.jpg" alt="delete2"></p><h4 id="不平衡的矫正-1"><a href="#不平衡的矫正-1" class="headerlink" title="不平衡的矫正"></a>不平衡的矫正</h4><ul><li><strong>R0型不平衡的矫正</strong>，将$\mathcal{T_A}$左旋，注意平衡后$bf(\mathcal{A_L})=-1$，子树高度仍为$h+2$未改变，故<strong>无需改变根节点途径上的平衡因子</strong>；</li><li><strong>R1型不平衡的矫正</strong>，同$R0$型不平衡矫正，将$\mathcal{T_A}$进行左旋，区别是平衡后$bf(\mathcal{A_L})=0$，子树高度减少为$h+1$，<strong>某些祖先平衡因子改变，可能需要进行旋转保持平衡</strong>；</li><li><strong>R-1型不平衡的矫正</strong>，先将$\mathcal{T_{A_L}}$右旋，再将$\mathcal{T_A}$左旋，调整后该树高度为$h+1$，<strong>某些祖先平衡因子改变，可能需要进行旋转保持平衡</strong>；</li><li><strong>L1型不平衡与L-1型不平衡矫正分别与R-1型不平衡、R1型不平衡对应</strong>，即<strong>L1为双旋转</strong>。</li></ul><div class="table-container"><table><thead><tr><th>不平衡类型</th><th>R-1</th><th>R0</th><th>R1</th><th>L-1</th><th>L0</th><th>L1</th></tr></thead><tbody><tr><td>矫正</td><td>$rr(\mathcal{T_{A_L}}) + lr(\mathcal{T_{A}})$</td><td>$lr(\mathcal{T_{A}})$</td><td>$lr(\mathcal{T_{A}})$</td><td>$rr(\mathcal{T_{A}})$</td><td>$rr(\mathcal{T_{A}})$</td><td>$lr(\mathcal{T_{A_R}}) + rr(\mathcal{T_{A}})$</td></tr></tbody></table></div><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete3.jpg" alt="delete3"></p><h4 id="算法描述-2"><a href="#算法描述-2" class="headerlink" title="算法描述"></a>算法描述</h4><ol><li>给定元素的关键字$\mathcal{K}$，从根结点出发查找，若<strong>未找到具有相同关键字的节点</strong>$\mathcal{N}$，退出算法，否则进入2；</li><li>从根结点出发查找删除节点$\mathcal{N}$，其父节点为$\mathcal{P}$，记距离$\mathcal{P}$最近的、且$bf = \pm 2$的祖先节点为$\mathcal{A}$，确定$\mathcal{A}$的<strong>不平衡类型</strong>，并执行相应旋转；</li><li>根据$bf(\mathcal{P})$判断是否需要<strong>向根节点进行平衡树结构调整</strong>，不需要或调整后，退出算法。</li></ol><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><h3 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h3><p>由<code>BinarySearchTree&lt;K, V&gt;</code>派生为<code>AVLTree&lt;K, V&gt;</code>，重载<code>insert</code>与<code>erase</code>函数，<code>find</code>无需重载，相应添加部分功能性函数.声明如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AVLTree</span> :</span></span><br><span class="line"><span class="keyword">public</span> BinarySearchTree&lt;K, V&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">AVLTree() : BinarySearchTree&lt;K, V&gt;() &#123;&#125;</span><br><span class="line">~AVLTree() &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> K&amp;, <span class="keyword">const</span> V&amp;)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span><span class="params">(<span class="keyword">const</span> K&amp;)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">balancedFactor</span><span class="params">(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*)</span></span>;</span><br><span class="line"><span class="keyword">static</span> BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* findImbalancedAncestor(</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*, LinkedStack&lt;<span class="keyword">bool</span>&gt;**);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ol><li><p>节点平衡因子<br> 为减少代码改动，节点的平衡因子通过调用函数的方式,动态求解</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">int</span> AVLTree&lt;K, V&gt;::balancedFactor(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> AVLTree&lt;K, V&gt;::heightofNode(node-&gt;left) - \</span><br><span class="line">        AVLTree&lt;K, V&gt;::heightofNode(node-&gt;right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 由节点$\mathcal{N}$向上搜索最近的不平衡因子，并<strong>将路径保存在栈内</strong></p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* AVLTree&lt;K, V&gt;::\</span><br><span class="line">    findImbalancedAncestor(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node, </span><br><span class="line">                            LinkedStack&lt;<span class="keyword">bool</span>&gt;** routine)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 向上查找节点A，并记录路径</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A = node;</span><br><span class="line">    <span class="keyword">int</span> bf = balancedFactor(A);</span><br><span class="line">    <span class="keyword">while</span> (bf &gt; <span class="number">-2</span> &amp;&amp; bf &lt; <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!A-&gt;parent) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (routine) &#123;</span><br><span class="line">            <span class="keyword">if</span> (A == A-&gt;parent-&gt;left)</span><br><span class="line">                (*routine)-&gt;push(<span class="literal">false</span>);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                (*routine)-&gt;push(<span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        A = A-&gt;parent;</span><br><span class="line">        bf = balancedFactor(A);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> A;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>插入</p><p> 插入时，首先调用<code>find</code>进行位置查找，若不存在该节点则创建；由新建节点向根节点查找最近的不平衡祖先并记录路径；根据路径判断不平衡类型做相应旋转</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> AVLTree&lt;K, V&gt;::insert(<span class="keyword">const</span> K&amp; key, <span class="keyword">const</span> V&amp; value)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 查找插入位置</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = <span class="keyword">this</span>-&gt;find(key, <span class="literal">true</span>);</span><br><span class="line">    Pair&lt;K, V&gt;* p = node-&gt;get();</span><br><span class="line">    <span class="keyword">if</span> (p-&gt;getKey() != key)</span><br><span class="line">        p-&gt;setKey(key);</span><br><span class="line">    p-&gt;setVal(value);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查找最近的不平衡祖先，并记录路径</span></span><br><span class="line">    LinkedStack&lt;<span class="keyword">bool</span>&gt;* routine = <span class="keyword">new</span> LinkedStack&lt;<span class="keyword">bool</span>&gt;;<span class="comment">// 向左为false</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A = findImbalancedAncestor(node, &amp;routine);</span><br><span class="line">    <span class="keyword">if</span> (!A) &#123;</span><br><span class="line">        <span class="keyword">delete</span> routine;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断不平衡类型，并相应旋转</span></span><br><span class="line">    <span class="keyword">int</span> type = (routine-&gt;pop() &lt;&lt; <span class="number">1</span>) + routine-&gt;pop();</span><br><span class="line">    <span class="keyword">switch</span> (type)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:<span class="comment">// LL</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;leftRotate(A);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:<span class="comment">// LR</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;rightRotate(A-&gt;left);</span><br><span class="line">        <span class="keyword">this</span>-&gt;leftRotate(A);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>:<span class="comment">// RL</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;leftRotate(A-&gt;right);</span><br><span class="line">        <span class="keyword">this</span>-&gt;rightRotate(A);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>:<span class="comment">// RR</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;rightRotate(A);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 若A为根节点，旋转后需改变根节点指针</span></span><br><span class="line">    <span class="keyword">if</span> (!A-&gt;parent-&gt;parent) <span class="keyword">this</span>-&gt;m_tnRoot = A-&gt;parent;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">delete</span> routine;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>删除<br> 删除时，首先进行节点查找，若找到，记录其父节点信息(在搜索树结构重组织时会失去该节点信息，故先进行保存)；重新组织搜索二叉树结构后，维持平衡二叉树，</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> AVLTree&lt;K, V&gt;::erase(<span class="keyword">const</span> K&amp; key)</span><br><span class="line">&#123;</span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = <span class="keyword">this</span>-&gt;find(key, <span class="literal">false</span>);</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 记录父节点与其兄弟节点</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* P = node-&gt;parent;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------ 重新组织二叉树 ------------------</span></span><br><span class="line">    <span class="comment">// 查找左子树的最大值，或右子树的最小值</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* replace = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">while</span> (!node-&gt;isLeaf()) &#123;<span class="comment">// 直到搜索到叶节点为止</span></span><br><span class="line">        <span class="keyword">if</span> (node-&gt;left) &#123;</span><br><span class="line">            replace = <span class="keyword">this</span>-&gt;max(node-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (node-&gt;right) &#123;</span><br><span class="line">            replace = <span class="keyword">this</span>-&gt;min(node-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (replace) &#123;<span class="comment">// 找到可替换子节点</span></span><br><span class="line">            Pair&lt;K, V&gt;* np = node-&gt;get();</span><br><span class="line">            Pair&lt;K, V&gt;* rp = replace-&gt;get();</span><br><span class="line">            np-&gt;setKey(rp-&gt;getKey());</span><br><span class="line">            np-&gt;setVal(rp-&gt;getVal());</span><br><span class="line">            node = replace;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (node-&gt;parent) &#123;<span class="comment">// 修改父节点信息</span></span><br><span class="line">        <span class="keyword">if</span> (node == node-&gt;parent-&gt;left)</span><br><span class="line">            node-&gt;parent-&gt;left = <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            node-&gt;parent-&gt;right = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;<span class="comment">// 无父节点，即整棵树只有一个根节点，则修改根节点为空</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;m_tnRoot = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// ------------------ 维持平衡二叉树 ------------------</span></span><br><span class="line">    <span class="comment">// 查找最近的不平衡祖先，并记录路径</span></span><br><span class="line">    LinkedStack&lt;<span class="keyword">bool</span>&gt;* routine = <span class="keyword">new</span> LinkedStack&lt;<span class="keyword">bool</span>&gt;;<span class="comment">// 向左为false</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A = findImbalancedAncestor(P, &amp;routine);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向根节点进行</span></span><br><span class="line">    <span class="keyword">while</span> (A) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断类型并矫正</span></span><br><span class="line">        <span class="keyword">bool</span> deleteRight = <span class="literal">false</span>;<span class="comment">// 若为`true`，表示R类型。否则L类型</span></span><br><span class="line">        <span class="keyword">if</span> (routine-&gt;empty()) &#123;<span class="comment">// P即为A，此时必有一子节点为空</span></span><br><span class="line">            <span class="keyword">if</span> (!A-&gt;left) <span class="comment">// 左节点为空</span></span><br><span class="line">                deleteRight = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (!A-&gt;right) <span class="comment">// 右节点为空</span></span><br><span class="line">                deleteRight = <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            deleteRight = routine-&gt;pop();</span><br><span class="line">        &#125;</span><br><span class="line">        BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* B = \</span><br><span class="line">            deleteRight ? A-&gt;left : A-&gt;right;<span class="comment">// 不平衡节点的另一侧子树</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> type = (deleteRight &lt;&lt; <span class="number">2</span>) + balancedFactor(B);</span><br><span class="line">        <span class="keyword">switch</span> (type)</span><br><span class="line">        &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">-1</span>:<span class="comment">// L-1</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;rightRotate(A);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">0</span>:<span class="comment">// L0</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;rightRotate(A);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:<span class="comment">// L1</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;leftRotate(A-&gt;right);</span><br><span class="line">            <span class="keyword">this</span>-&gt;rightRotate(A);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>:<span class="comment">// R-1</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;rightRotate(A-&gt;left);</span><br><span class="line">            <span class="keyword">this</span>-&gt;leftRotate(A);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">4</span>:<span class="comment">// R0</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;leftRotate(A);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">5</span>:<span class="comment">// R1</span></span><br><span class="line">            <span class="keyword">this</span>-&gt;leftRotate(A);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改整棵树的根节点</span></span><br><span class="line">        <span class="keyword">if</span> (!A-&gt;parent-&gt;parent) <span class="keyword">this</span>-&gt;m_tnRoot = A-&gt;parent;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 向根节点搜索不平衡节点，更新A</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; routine-&gt;size(); i++) routine-&gt;pop();<span class="comment">// 清除栈</span></span><br><span class="line">        A = findImbalancedAncestor(A, &amp;routine);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------ 释放资源 ------------------</span></span><br><span class="line">    <span class="keyword">delete</span> routine;</span><br><span class="line">    Pair&lt;K, V&gt;* pair = node-&gt;get();</span><br><span class="line"> <span class="keyword">delete</span> pair; <span class="keyword">delete</span> node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="插入-1"><a href="#插入-1" class="headerlink" title="插入"></a>插入</h4><p>按顺序插入以下关键字</p><div class="table-container"><table><thead><tr><th>序号</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th></tr></thead><tbody><tr><td>关键字</td><td>20</td><td>15</td><td>25</td><td>12</td><td>10</td><td>23</td><td>24</td><td>19</td></tr></tbody></table></div><p>在插入$10$以前，该树无不平衡情况。在插入$10$之后导致键为$15$的节点$LL$不平衡，需进行左旋调整，如下</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/test_insert_ll.jpg" alt="test_insert_ll"></p><p>此时二叉树按<strong>层次遍历</strong>输出如下，结果正确<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">插入10: [20]甲 [12]丁 [25]丙 [10]戊 [15]乙</span><br></pre></td></tr></table></figure></p><p>在插入$24$时，节点$25$形成$LR$不平衡，调整过程如下</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/test_insert_lr.jpg" alt="test_insert_lr"></p><p><strong>层次遍历</strong>输出结果正确<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">插入24: [20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙</span><br></pre></td></tr></table></figure></p><p>全部插入后形成平衡二叉树与搜索二叉树对比如下</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/test_insert_all.jpg" alt="test_insert_all"></p><h4 id="删除-1"><a href="#删除-1" class="headerlink" title="删除"></a>删除</h4><p>在上述最终生成的AVL中，删除节点$10$导致节点$12$出现$L-1$不平衡，进行单旋转矫正，<strong>层次遍历</strong>输出结果正确<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[20]甲 [15]乙 [24]庚 [12]丁 [19]辛 [23]己 [25]丙</span><br></pre></td></tr></table></figure></p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/test_erase_l_1.jpg" alt="test_erase_l_1"></p><p>继续删除$12, 15, 19$导致节点$20$出现$L0$不平衡，进行单旋转矫正，<strong>层次遍历</strong>输出结果正确<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[24]庚 [20]甲 [25]丙 [23]己</span><br></pre></td></tr></table></figure></p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/test_erase_l0.jpg" alt="test_erase_l0"></p><p>重新初始化该树，依次删除$25, 12, 10, 15, 19$导致节点$20$出现$L1$不平衡，进行双旋转矫正，<strong>层次遍历</strong>输出结果正确<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[23]己 [20]甲 [24]庚</span><br></pre></td></tr></table></figure></p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/test_erase_l1.jpg" alt="test_erase_l1"></p><h4 id="主函数与输出"><a href="#主函数与输出" class="headerlink" title="主函数与输出"></a>主函数与输出</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">string</span> names[<span class="number">8</span>] = &#123;</span><br><span class="line"><span class="string">"甲"</span>, <span class="string">"乙"</span>, <span class="string">"丙"</span>, <span class="string">"丁"</span>, <span class="string">"戊"</span>, <span class="string">"己"</span>, <span class="string">"庚"</span>, <span class="string">"辛"</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> numbers[<span class="number">8</span>] = &#123; <span class="number">20</span>, <span class="number">15</span>, <span class="number">25</span>, <span class="number">12</span>, <span class="number">10</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">19</span> &#125;;</span><br><span class="line"></span><br><span class="line">AVLTree&lt;<span class="keyword">int</span>, <span class="built_in">string</span>&gt; tree;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入键值对</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"插入"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"插入"</span> &lt;&lt; numbers[i] &lt;&lt; <span class="string">": "</span>;</span><br><span class="line">tree.insert(numbers[i], names[i]);</span><br><span class="line">tree.print();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除键值对</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"删除"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">tree.erase(<span class="number">10</span>); </span><br><span class="line">tree.print();<span class="comment">// 12 L-1不平衡</span></span><br><span class="line"></span><br><span class="line">tree.erase(<span class="number">12</span>); </span><br><span class="line">tree.erase(<span class="number">15</span>);</span><br><span class="line">tree.erase(<span class="number">19</span>); </span><br><span class="line">tree.print();<span class="comment">// L0不平衡</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入键值对</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"重新插入"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line">tree.insert(numbers[i], names[i]);</span><br><span class="line">&#125;</span><br><span class="line">tree.print();</span><br><span class="line"></span><br><span class="line">tree.erase(<span class="number">25</span>); </span><br><span class="line">tree.erase(<span class="number">12</span>); </span><br><span class="line">tree.erase(<span class="number">10</span>); </span><br><span class="line">tree.erase(<span class="number">15</span>); </span><br><span class="line">tree.erase(<span class="number">19</span>); </span><br><span class="line">tree.print();<span class="comment">// L1不平衡</span></span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">插入</span><br><span class="line">插入20: [20]甲</span><br><span class="line">插入15: [20]甲 [15]乙</span><br><span class="line">插入25: [20]甲 [15]乙 [25]丙</span><br><span class="line">插入12: [20]甲 [15]乙 [25]丙 [12]丁</span><br><span class="line">插入10: [20]甲 [12]丁 [25]丙 [10]戊 [15]乙</span><br><span class="line">插入23: [20]甲 [12]丁 [25]丙 [10]戊 [15]乙 [23]己</span><br><span class="line">插入24: [20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙</span><br><span class="line">插入19: [20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙 [19]辛</span><br><span class="line"></span><br><span class="line">删除</span><br><span class="line">[20]甲 [15]乙 [24]庚 [12]丁 [19]辛 [23]己 [25]丙</span><br><span class="line">[24]庚 [20]甲 [25]丙 [23]己</span><br><span class="line"></span><br><span class="line">重新插入</span><br><span class="line">[20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙 [19]辛</span><br><span class="line">[23]己 [20]甲 [24]庚</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure><h1 id="红-黑树-RB树"><a href="#红-黑树-RB树" class="headerlink" title="红-黑树(RB树)"></a>红-黑树(RB树)</h1><p>红黑树通过节点颜色的限制，进行整棵树的高度限制。</p><h2 id="定义及概念-1"><a href="#定义及概念-1" class="headerlink" title="定义及概念"></a>定义及概念</h2><p><strong>定义</strong>：<strong>红-黑树</strong>(red-black tree)是一棵<strong>二叉搜索树</strong>，将每个空指针用外部节点来代替，得到<strong>扩充二叉树</strong>，<strong>树中每个节点的颜色是黑色或是红色</strong>，如下图，图中外部节点(扩充)为黑色正方形。</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/red_black_tree_eg.jpg" alt="red_black_tree_eg"></p><p>红-黑树具有以下性质</p><ol><li><strong>根节点、外部节点都是黑色</strong>；</li><li>根节点$\rightarrow$外部节点路径上，<strong>没有连续两个节点是红色</strong>；</li><li>根节点$\rightarrow$外部节点路径上，<strong>黑色节点数目相同</strong>。</li></ol><blockquote><p>由2，<strong>每个红色节点的子节点必定是黑色节点</strong>；</p></blockquote><p>红-黑树的另一种<strong>等价</strong>，<strong>取决于父子节点间指针颜色</strong>，</p><ol><li><strong>父节点$\rightarrow$黑色孩子</strong>指针是<strong>黑色</strong>的；</li><li><strong>父节点$\rightarrow$红色孩子</strong>指针是<strong>红色</strong>的；</li><li><strong>内部节点$\rightarrow$外部节点的指针为黑色</strong>；</li><li>根节点$\rightarrow$外部节点路径上，<strong>没有连续两个指针是红色</strong>；</li><li>根节点$\rightarrow$外部节点路径上，<strong>黑色指针数目相同</strong>。</li></ol><p><strong>定义</strong>：某节点的<strong>阶</strong>(rank)，是从<strong>该节点$\rightarrow$外部节点路径上黑色指针的数目(或黑色内部节点个数)</strong>，一个外部节点的阶是$0$。</p><p><strong>定理1</strong>：设根节点$\rightarrow$外部节点的<strong>路径长度</strong>(length),是为该路径的<strong>指针数量(或内部节点个数)</strong>，$P, Q$是红-黑树中两条根节点$\rightarrow$外部节点的路径，那么</p><script type="math/tex; mode=display">\rm{length}(P) \leq 2 \times \rm{length}(Q)</script><blockquote><p>根节点$\rightarrow$外部节点路径上，黑色内部节点数目即阶$N_b =   r$；且红色节点子节点必定为黑色节点，而黑色节点子节点颜色未定，故红色节点的数目$0 \leq N_r\leq r$，那么</p><script type="math/tex; mode=display">r \leq \rm{length} = N_r + N_b \leq 2 \times r</script><p>所以对于任意两条根节点$\rightarrow$外部节点的路径$P, Q$，长度都满足</p><script type="math/tex; mode=display">\rm{length}(P) \leq 2 \times \rm{length}(Q)</script></blockquote><p><strong>定理2</strong>：令$h$是一棵红-黑树的高度(不包括外部节点)，$n$是内部节点数目，$r$是根节点的阶，那么</p><ol><li>$h \leq 2r$；</li><li>$n \geq 2^r - 1$；</li><li>$h \leq 2 \times \log_2 (n + 1)$。</li></ol><blockquote><p>$h$即根节点$\rightarrow$外部节点的路径长度</p><ol><li>见定理1的证明；</li><li><strong>树的第$1 \sim r$层均为内部节点</strong>，且数目为$2^r - 1$，那么总内部节点数目$n \geq 2^r - 1$；</li><li>联立1，2可得3，略。</li></ol></blockquote><h2 id="算法描述-3"><a href="#算法描述-3" class="headerlink" title="算法描述"></a>算法描述</h2><h3 id="搜索-1"><a href="#搜索-1" class="headerlink" title="搜索"></a>搜索</h3><p>同样的，红-黑树搜索仍可沿用二叉搜索树的搜索方法。</p><h3 id="插入-2"><a href="#插入-2" class="headerlink" title="插入"></a>插入</h3><p>首先确定<strong>新节点插入时的颜色</strong>。若新节点为黑色，插入后必定违反<strong>性质3(黑色节点数目约束)</strong>；若新节点为红色，可能违反<strong>性质2(红色节点不连续约束)</strong>，故新节点颜色确定为<strong>红</strong>。</p><p>记红色新节点为$\mathcal{U}$，插入后违反<strong>性质2</strong>，此时$\mathcal{U}$<strong>必存在红色父节点</strong>$\mathcal{P}$；由<strong>性质1(根节点与外部节点颜色约束)</strong>，$\mathcal{U}$<strong>必存在黑色祖父节点</strong>$\mathcal{G}$。</p><p>此时不平衡情况可<strong>分为$8$种情况，命名为$XYc$</strong>，其中$X, Y$取值$\{L, R\}$表示 $\mathcal{U}$为$\mathcal{G}$的$X$子节点的$Y$子节点；$c$取值$\{r, b\}$，表示$\mathcal{G}$右子节点的颜色。</p><blockquote><p>例如$LLr$表示 $\mathcal{U}$为$\mathcal{G}$左子节点($L$)的左子节点($L$)，且$\mathcal{G}$右子节点$\mathcal{G_R}$为红色($r$)。</p></blockquote><p><strong>根据$\mathcal{G_R}$颜色</strong>分为两大类进行<strong>树的平衡</strong></p><ol><li><p>$XYr$</p><p> 当$\mathcal{G_R}$为红色时，该类不平衡可通过<strong>颜色调整</strong>来处理。将$\mathcal{P}$与$\mathcal{G_R}$的颜色调整为黑色；<strong>若$\mathcal{G}$不是根节点，需调整为红色，并将$\mathcal{G}$作为$\mathcal{U}$向根节点进行判别是否违反性质2</strong>。如下图</p><p> <img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/insert5.jpg" alt="insert5"></p></li><li><p>$XYb$</p><p> 当$\mathcal{G_R}$为黑色时，此类不平衡需通过<strong>旋转操作</strong>进行平衡，<strong>操作类似AVL树</strong>，除此之外，<strong>改变$\mathcal{G}$和$\mathcal{P}$节点的颜色</strong>。重新平衡后，该子树从根节点到外部节点的路径上，黑色节点数目不变，且向根节点搜索路径上，不存在连续的红色节点，故<strong>不需要继续平衡</strong>。</p><p> 如下图示意，$LLb$不平衡时，将子树$\mathcal{T_G}$左旋，并修改该子树根节点$\mathcal{G}$与不平衡端子树根节点$\mathcal{P}$，$LRb$同理。</p><p> <img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/insert6.jpg" alt="insert6"></p></li></ol><h3 id="删除-2"><a href="#删除-2" class="headerlink" title="删除"></a>删除</h3><p>用常规搜索二叉树的删除算法进行节点$\mathcal{N}$的物理删除，记$\mathcal{N}’$为替代$\mathcal{N}$的节点，<strong>当且仅当</strong>$\mathcal{N}$为黑色，且$\mathcal{N}’$不是树根节点时，会<strong>违反性质3(黑色节点数目约束)</strong>；$\mathcal{N}$为红色时，该树维持平衡。</p><p>记$\mathcal{N}’$的父节点为$\mathcal{P}$，$\mathcal{N}’$的同胞节点为$\mathcal{B}$，<strong>注意$\mathcal{N}’$在替代后，必定为黑色</strong>。根据$\mathcal{P, B}$的情况，<strong>不平衡情况可分为$4$类，命名为$Xc$，</strong>$X$取值为$\{L, R\}$，表示$\mathcal{N}’$是$\mathcal{P}$的左孩或右孩；$c$取值为$\{r, b\}$，表示$\mathcal{B}$的颜色。</p><ol><li><p>$Xb$型</p><p> <strong>根据$\mathcal{B}$的红色子节点个数</strong>，可分为$Xb0, Xb1, Xb2$三种。</p><ol><li><p>$Xb0$</p><p> <strong>修改$\mathcal{B}$(黑色)为红色节点$\mathcal{B}’$，$\mathcal{P}$(颜色待定)为黑色节点$\mathcal{P}’$</strong>，根节点$\mathcal{R}$至$\mathcal{N}’$的路径上增加了一个黑色节点，解决该路不平衡问题。但是考虑$\mathcal{P}$颜色：</p><ul><li><strong>若$\mathcal{P}$为红色节点</strong>，而根节点$\mathcal{R}$至$\mathcal{B}$路径上，黑色节点数目不变，保持该路平衡状态；</li><li><p><strong>若$\mathcal{P}$为黑色节点</strong>，将使得该路上缺少一个黑色节点，$\mathcal{P}$为根节点时无需改动，否则将$\mathcal{P}$视作新的$\mathcal{N}’$直至根节点为止，继续修改路径上节点的颜色。</p><p><img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete4.jpg" alt="delete4"></p></li></ul></li><li><p>$Xb1$与$Xb2$</p><p> 又可根据$\mathcal{B}$红色子孩的位置分为$Xb1(i)$和$Xb1(ii)$两类。可通过旋转的方式进行矫正，可以证明旋转后，整棵树保持平衡。</p><ul><li><p>$Xb1(i)$</p><p>  将树$\mathcal{T_P}$左旋，并修改$\mathcal{B}’$颜色与$\mathcal{P}$一致，$\mathcal{P}$修改为黑色。</p><p>  <img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete5.jpg" alt="delete5"></p></li><li><p>$Xb1(ii)$与$Xb2$</p><p>  将树$\mathcal{T_B}$右旋后，将树$\mathcal{T_P}$左旋，并修改$\mathcal{B_R}$颜色与$\mathcal{P}$一致，$\mathcal{P}$修改为黑色，$\mathcal{B_L}$颜色保持不变。</p><p>  <img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete6.jpg" alt="delete6"></p></li></ul></li></ol></li><li><p>$Xr$型</p><p> $Xr$型可<strong>根据$\mathcal{B_R}$红色孩子数目</strong>，分为$Xr0, Xr1, Xr2$三种类型。与$Xb$型不同，<strong>此时$\mathcal{P}$必定为黑色节点</strong>。该类型都可通过旋转后保持平衡，无需向根节点搜索不平衡节点，</p><ul><li><p>$Xr0$<br>  将树$\mathcal{T_P}$进行左旋，修改$\mathcal{B}$为黑色，$\mathcal{B_R}$为红色</p><p>  <img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete7.jpg" alt="delete7"></p></li><li><p>$Xr1$与$Xr2$</p><p>  又可根据$\mathcal{B_R}$红色子孩的位置分为$Xr1(i)$和$Xr1(ii)$两类。</p><ul><li><p>$Xr1(i)$<br>  将树$\mathcal{T_B}$进行右旋，再将树$\mathcal{T_P}$进行左旋，修改$\mathcal{B_{R_L}}$为黑色</p><p>  <img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete8.jpg" alt="delete8"></p></li><li><p>$Xr1(ii)$与$Xr2$</p><p>  将树$\mathcal{T_{B_R}}$进行右旋后，再将树$\mathcal{T_B}$进行右旋，再将树$\mathcal{T_P}$进行左旋，修改$\mathcal{B_{R_R}}$为黑色</p><p>  <img src="/2020/03/14/【数据结构】平衡搜索树——AVL树与RB树/delete9.jpg" alt="delete9"></p></li></ul></li></ul></li></ol><h1 id="AVL树与红黑树的对比"><a href="#AVL树与红黑树的对比" class="headerlink" title="AVL树与红黑树的对比"></a>AVL树与红黑树的对比</h1><ul><li>AVL树通过子树高度限制进行<strong>严格平衡</strong>，而红黑树通过节点添加颜色的方式进行平衡，<strong>不追求严格的平衡</strong>；</li><li>在插入与删除操作时，两种维持平衡的方式也不相同，AVL树可通过$1 \sim 2$次旋转使子树获得平衡，RB树通过修改颜色或$1 \sim 3$次旋转来矫正。如下表；</li><li>AVL树在$L-1, R1, L1, R-1$(删除)时，需要对至根节点路径上的不平衡子树继续矫正；而RB树只有在$XYr$(插入)与$Rb0$(删除)时继续搜索不平衡子树，这两种情况都可以通过修改节点颜色来矫正；</li><li><strong>平均情况下，AVL树总的旋转次数比RB树多</strong>，RB树统计性能高于AVL。</li></ul><p>C++的STL标准库中<code>map</code>采用的时RB树实现。</p><table border="0" cellpadding="0" cellspacing="0" width="979" style="border-collapse: collapse;table-layout:fixed;width:733pt"> <col class="xl65" width="68" span="2" style="width:51pt"> <col class="xl65" width="228" style="mso-width-source:userset;mso-width-alt:7782; width:171pt"> <col class="xl65" width="82" style="mso-width-source:userset;mso-width-alt:2781; width:61pt"> <col class="xl65" width="287" style="mso-width-source:userset;mso-width-alt:9796; width:215pt"> <col class="xl65" width="246" style="mso-width-source:userset;mso-width-alt:8379; width:184pt"> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="68" style="height:13.9pt;width:51pt">树</td>  <td class="xl66" width="68" style="width:51pt">操作</td>  <td class="xl66" width="228" style="width:171pt">判别</td>  <td class="xl66" width="82" style="width:61pt">类型</td>  <td class="xl66" width="287" style="width:215pt">矫正</td>  <td class="xl66" width="246" style="width:184pt">备注</td> </tr> <tr height="19" style="height:13.9pt">  <td rowspan="10" height="190" class="xl66" width="68" style="height:139.0pt;  width:51pt">AVL</td>  <td rowspan="4" class="xl66" width="68" style="width:51pt">插入</td>  <td rowspan="4" class="xl66" width="228" style="width:171pt">查找插入节点N最近的不平衡祖先A(bf=-2/2)；根据N所在A子树的子树位置，分为4种不平衡类型(如N在A左子树的左子树则记作LL)</td>  <td class="xl66" width="82" style="width:61pt">LL</td>  <td class="xl65" width="287" style="width:215pt">A左旋</td>  <td rowspan="4" class="xl66" width="246" style="width:184pt">矫正后子树高度不变，整棵树平衡</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">RR</td>  <td class="xl65" width="287" style="width:215pt">A右旋</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">LR</td>  <td class="xl65" width="287" style="width:215pt">AL右旋，A左旋</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">RL</td>  <td class="xl65" width="287" style="width:215pt">AR左旋，A右旋</td> </tr> <tr height="19" style="height:13.9pt">  <td rowspan="6" height="114" class="xl66" width="68" style="height:83.4pt;width:51pt">删除</td>  <td rowspan="6" class="xl66" width="228" style="width:171pt">查找删除节点N最近的不平衡祖先A(bf=-2/2)；根据N所在A的子树位置，可分为L,  R两种；考察A另一棵子树平衡因子，L(R)型又可分为0, 1, -1三种，共2×3=6种类型</td>  <td class="xl66" width="82" style="width:61pt">L0</td>  <td class="xl65" width="287" style="width:215pt">A右旋</td>  <td rowspan="2" class="xl66" width="246" style="width:184pt">矫正后子树高度不变，整棵树平衡</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">R0</td>  <td class="xl65" width="287" style="width:215pt">A左旋</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">L-1</td>  <td class="xl65" width="287" style="width:215pt">A右旋</td>  <td rowspan="4" class="xl66" width="246" style="width:184pt">矫正后子树高度减1，某些祖先节点可能不平衡，需要对在至根节点的路径上的不平衡子树继续矫正</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">R1</td>  <td class="xl65" width="287" style="width:215pt">A左旋</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">L1</td>  <td class="xl65" width="287" style="width:215pt">AR左旋，A右旋</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">R-1</td>  <td class="xl65" width="287" style="width:215pt">AL右旋，A左旋</td> </tr> <tr height="61" style="mso-height-source:userset;height:45.75pt">  <td rowspan="12" height="306" class="xl66" width="68" style="height:226.35pt;  width:51pt">RB树</td>  <td rowspan="5" class="xl66" width="68" style="width:51pt">插入</td>  <td rowspan="5" class="xl65" width="228" style="width:171pt">插入节点N初始为红，其父节点P为红时不平衡，祖父节点G必为黑色；根据N与G的节点关系(LL,  RR, LR, RL)、G的另一孩子颜色(r, b)，可分为4×2=8种类型XYc</td>  <td class="xl66" width="82" style="width:61pt">XYr</td>  <td class="xl65" width="287" style="width:215pt">修改P与GR为黑色节点，若G不为根节点修改为红否则为黑</td>  <td class="xl65" width="246" style="width:184pt">G颜色改变，能导致G的父节点不平衡(红-红)，故需要对至根节点的路径上不平衡子树继续矫正</td> </tr> <tr height="19" style="mso-height-source:userset;height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">LLb</td>  <td class="xl65" width="287" style="width:215pt">G左旋，修改P为黑，G为红</td>  <td rowspan="4" class="xl66" width="246" style="width:184pt">重新平衡后，该子树从根节点到外部节点的路径上，黑色节点数目不变，且向根节点搜索路径上，不存在连续的红色节点，整棵树平衡</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">RRb</td>  <td class="xl65" width="287" style="width:215pt">G右旋，修改P为黑，G为红</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">LRb</td>  <td class="xl65" width="287" style="width:215pt">AL右旋，A左旋，修改U为黑，G为红</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">RLb</td>  <td class="xl65" width="287" style="width:215pt">AR左旋，A右旋，修改U为黑，G为红</td> </tr> <tr height="19" style="mso-height-source:userset;height:13.9pt">  <td rowspan="7" height="169" class="xl66" width="68" style="height:125.0pt;  width:51pt">删除</td>  <td rowspan="7" class="xl66" width="228" style="width:171pt">删除节点N(黑色时不平衡)后，由搜索树算法找到替换节点N'(修改为黑)，N'的父节点为P(Xb型时颜色不定，Xr型时必为黑)、同胞节点为B；根据N'与P的位置关系(L,  R)、B的颜色(r, b)、B的红色子节点个数(0, 1, 2)、B红色结点数为1时该红色子节点相对于B位置(i&lt;左&gt;,  ii&lt;右&gt;)分别矫正；共2×2×(1+2+1)=16种类型</td>  <td class="xl66" width="82" style="width:61pt">Rb0</td>  <td class="xl65" width="287" style="width:215pt">修改B为红，P为黑</td>  <td class="xl65" width="246" style="width:184pt">P为红色时，整棵树平衡；否则向跟节点继续矫正</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">Rb1(i)</td>  <td class="xl65" width="287" style="width:215pt">P左旋，修改B与P一致，再修改P为黑</td>  <td rowspan="2" class="xl66" width="246" style="width:184pt">矫正后，整棵树平衡</td> </tr> <tr height="37" style="height:27.75pt">  <td height="37" class="xl66" width="82" style="height:27.75pt;width:61pt">Rb1(ii),  Rb2</td>  <td class="xl65" width="287" style="width:215pt">B右旋，P左旋，修改BR与P一致(BL不变)，再修改P为黑</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">Rr0</td>  <td class="xl65" width="287" style="width:215pt">P左旋，修改B为黑，BR为红</td>  <td rowspan="3" class="xl66" width="246" style="width:184pt">矫正后，整棵树平衡</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">Rr1(i)</td>  <td class="xl65" width="287" style="width:215pt">B右旋，P左旋，修改BRL(红)为黑</td> </tr> <tr height="37" style="height:27.75pt">  <td height="37" class="xl66" width="82" style="height:27.75pt;width:61pt">Rr1(ii),  Rr2</td>  <td class="xl65" width="287" style="width:215pt">BR右旋，B右旋，P左旋，修改BRR(红，此时为子树跟节点)为黑</td> </tr> <tr height="19" style="height:13.9pt">  <td height="19" class="xl66" width="82" style="height:13.9pt;width:61pt">Lcn</td>  <td colspan="2" class="xl66" width="533" style="width:399pt">与之类似，略</td> </tr> <![if supportMisalignedColumns]> <tr height="0" style="display:none">  <td width="68" style="width:51pt"></td>  <td width="68" style="width:51pt"></td>  <td width="228" style="width:171pt"></td>  <td width="82" style="width:61pt"></td>  <td width="287" style="width:215pt"></td>  <td width="246" style="width:184pt"></td> </tr> <![endif]></table>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】搜索树</title>
      <link href="/2020/03/08/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%90%9C%E7%B4%A2%E6%A0%91/"/>
      <url>/2020/03/08/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%90%9C%E7%B4%A2%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h1 id="定义与概念"><a href="#定义与概念" class="headerlink" title="定义与概念"></a>定义与概念</h1><p>考虑用散列表述的字典，字典插入、查找和删除等操作所需要的平均时间为$\Theta(1)$，最坏情况下时间与元素个数呈线性关系。若给字典添加如下操作，那么散列便不再具有良好的平均性能</p><ul><li>按关键字的升序输出字典元素；</li><li>按升序找到第$k$各元素；</li><li>删除第$k$个元素。</li></ul><p><strong>定义</strong>：<strong>二叉搜索树</strong>(binary search tree)是一棵二叉树，可能为空；一棵非空的二叉搜索树满足以下特征</p><ol><li>每个元素对应一个关键字，且所有<strong>关键字都唯一</strong>；</li><li>根节点的<strong>左子树</strong>中，元素关键字都<strong>小于</strong>根节点的关键字；</li><li>根节点的<strong>右子树</strong>中，元素关键字都<strong>大于</strong>根节点的关键字；</li><li>根节点的<strong>左、右子树</strong>也是二叉搜索树。</li></ol><blockquote><ul><li>由$2, 3$可知，某节点$\mathcal{N}$的键，<strong>大于</strong>该节点的左子树元素的键，<strong>小于</strong>该节点的右子树的键；</li><li>以节点$\mathcal{N}$为根节点的树的<strong>最小值</strong>位于最左端的叶子节点，<strong>最大值</strong>位于最右端的叶子节点；</li><li>当节点$\mathcal{N}$的<strong>左子树的键最大节点或右子树的键最小节点</strong>替换该结点时，该树仍满足二叉搜索树的特性。</li></ul></blockquote><p>若条件$1$弱化，即不同元素可包含相同的关键字，且$2, 3$中小于/大于改为不大于/不小于，这样的树称为<strong>有重复值的二叉搜索树</strong>(binary search tree with duplicates)。</p><p><strong>定义</strong>：<strong>索引二叉搜索树</strong>(indexed binary search tree)源于普通二叉搜索树，是在每个节点中添加$leftSize$域，记录<strong>该节点左子树的元素个数</strong>。</p><blockquote><ul><li>节点$\mathcal{N}$的$leftSize$与索引$index$比较大小，判断<strong>向左或向右</strong>搜索；</li><li>某节点$\mathcal{N}$的$leftSize$视为<strong>该节点在以该节点为根节点的树$\mathcal{T}_{\mathcal{N}}$中的索引</strong>，如下图$20$在树中索引为$5$，$15$在左子树中，索引为$1$等，以此类推；</li></ul><p>下图中元素按照升序排列时$12, 15, 16, 18, 19, 20, 25, 30$<br><img src="/2020/03/08/【数据结构】搜索树/indexed_binary_search_tree.jpg" alt="indexed_binary_search_tree"></p></blockquote><h1 id="抽象数据类型"><a href="#抽象数据类型" class="headerlink" title="抽象数据类型"></a>抽象数据类型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">抽象数据类型 BSTree</span><br><span class="line">&#123;</span><br><span class="line">    实例:</span><br><span class="line">        二叉树，每个节点包含数值对；满足二叉搜索树的特征；</span><br><span class="line">    操作：</span><br><span class="line">        find(k)：返回关键字为k的数对</span><br><span class="line">        insert(p)：插入数对p</span><br><span class="line">        erase(k)：删除关键字为k的数对</span><br><span class="line">        ascend()：按关键字升序输出所有数对</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>索引二叉搜索树支持二叉搜索树的所有操作，另外它还支持按名字进行的查找和删除操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">抽象数据类型 IndexedBSTree</span><br><span class="line">&#123;</span><br><span class="line">    实例:</span><br><span class="line">        二叉树，每个节点包含数值对；满足二叉搜索树的特征；</span><br><span class="line">    操作：</span><br><span class="line">        find(k)：返回关键字为k的数对</span><br><span class="line">        get(index)：返回第index个数对</span><br><span class="line">        insert(p)：插入数对p</span><br><span class="line">        erase(k)：删除关键字为k的数对</span><br><span class="line">        ascend()：按关键字升序输出所有数对</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h2><h3 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinarySearchTree</span> :</span> </span><br><span class="line"><span class="keyword">public</span> LinkedBinaryTree&lt;Pair&lt;K, V&gt;*&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">BinarySearchTree() : LinkedBinaryTree&lt;Pair&lt;K, V&gt;*&gt;() &#123;&#125;</span><br><span class="line">~BinarySearchTree() &#123;&#125;</span><br><span class="line"></span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* find(<span class="keyword">const</span> K&amp;, <span class="keyword">bool</span> insert=<span class="literal">false</span>);</span><br><span class="line"><span class="function">V&amp; <span class="title">get</span><span class="params">(<span class="keyword">const</span> K&amp;)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> K&amp;, <span class="keyword">const</span> V&amp;)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span><span class="params">(<span class="keyword">const</span> K&amp;)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;<span class="comment">// 注意</span></span><br><span class="line"><span class="comment">// - 不能声明为const，否则无法调用非const的printPairNode；</span></span><br><span class="line"><span class="comment">// - 类的静态成员无法声明为const。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ascending</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">printPairNode</span><span class="params">(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*)</span></span>;</span><br><span class="line"><span class="keyword">static</span> BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* max(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*);</span><br><span class="line"><span class="keyword">static</span> BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* min(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ol><li><p>查找</p><p> 根据搜索二叉树特性，当搜索到达节点$\mathcal{N}$时，<strong>若查找值小于当前节点的键，则向其左子树搜索，否则向右子树搜索</strong>，直至找到对应键(找到)，或直至叶子节点(未找到)；在<strong>插入模式</strong>(<code>bool insert=true</code>)下查找时，若不存在则创建新节点并返回。</p><p> <img src="/2020/03/08/【数据结构】搜索树/find.jpg" alt="find"></p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* BinarySearchTree&lt;K, V&gt;::find(<span class="keyword">const</span> K&amp; key, <span class="keyword">bool</span> insert)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 若树为空</span></span><br><span class="line">    <span class="keyword">if</span> (!<span class="keyword">this</span>-&gt;m_tnRoot) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!insert) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">        Pair&lt;K, V&gt;* p = <span class="keyword">new</span> Pair&lt;K, V&gt;;</span><br><span class="line">        <span class="keyword">this</span>-&gt;m_tnRoot = <span class="keyword">new</span> BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;;</span><br><span class="line">        <span class="keyword">this</span>-&gt;m_tnRoot-&gt;<span class="built_in">set</span>(p); <span class="keyword">return</span> <span class="keyword">this</span>-&gt;m_tnRoot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 树不为空</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = <span class="keyword">this</span>-&gt;m_tnRoot;</span><br><span class="line">    <span class="keyword">bool</span> toLeft = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">while</span> (node) &#123;</span><br><span class="line">        <span class="comment">// 找到对应键的数值对</span></span><br><span class="line">        <span class="keyword">if</span> (node-&gt;get()-&gt;getKey() == key) <span class="keyword">return</span> node;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 当前节点的键比搜查键更大，则向左子树搜索</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (node-&gt;get()-&gt;getKey() &gt; key) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!node-&gt;left) &#123;<span class="comment">// 左子树为空</span></span><br><span class="line">                toLeft = <span class="literal">true</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            node = node-&gt;left;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 当前节点的键比搜查键更小，则向右子树搜索</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (!node-&gt;right) &#123;<span class="comment">// 右子树为空</span></span><br><span class="line">                toLeft = <span class="literal">false</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            node = node-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 未找到</span></span><br><span class="line">    <span class="keyword">if</span> (!insert) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建新节点n</span></span><br><span class="line">    Pair&lt;K, V&gt;* p = <span class="keyword">new</span> Pair&lt;K, V&gt;;</span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* n = \</span><br><span class="line">        <span class="keyword">new</span> BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;(node);</span><br><span class="line">    n-&gt;<span class="built_in">set</span>(p);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 将节点加入搜索树</span></span><br><span class="line">    <span class="keyword">if</span> (toLeft)</span><br><span class="line">        node-&gt;left = n;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        node-&gt;right = n;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> n;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">V&amp; BinarySearchTree&lt;K, V&gt;::get(<span class="keyword">const</span> K&amp; key)</span><br><span class="line">&#123;</span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = find(key, <span class="literal">false</span>);</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> node-&gt;get()-&gt;getVal();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>插入</p><p> 调用函数<code>find</code>(插入模式)查找结果，设置搜查到的节点的数值对即可。</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> BinarySearchTree&lt;K, V&gt;::insert(<span class="keyword">const</span> K&amp; key, <span class="keyword">const</span> V&amp; value)</span><br><span class="line">&#123;</span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = find(key, <span class="literal">true</span>);</span><br><span class="line">    Pair&lt;K, V&gt;* p = node-&gt;get();</span><br><span class="line">    <span class="keyword">if</span> (p-&gt;getKey() != key)</span><br><span class="line">        p-&gt;setKey(key);</span><br><span class="line">    p-&gt;setVal(value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>删除</p><p> 删除结点时，需考虑剩余节点的重组织问题，与<a href="https://louishsu.xyz/2020/03/03/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/" target="_blank" rel="noopener">最大堆</a>不同，二叉搜索树具有良好的结构特性，故删除某节点$\mathcal{N}$时，只需将其<strong>左子树元素中键最大的，或右子树中元素键最小的</strong>与该节点位置替换即可。考虑到节点间拓扑关系修改复杂，以下仅<strong>修改</strong>节点$\mathcal{N}$的键值对，且继续搜索直至找到叶子节点作为<strong>替换节点</strong>并释放节点内存。</p><p> 例如删除键为$20$的节点$\mathcal{N}_{20}$时，查找左子树中键最大的节点$\mathcal{N}_{19}$，或右子树中键最小的节点$\mathcal{N}_{25}$(<strong>由于$\mathcal{N}_{25}$非叶子节点</strong>，需查找$\mathcal{N}_{25}$为根节点的树$\mathcal{T}_{\mathcal{N}_{25}}$中最大左子树节点或最小右子树节点进行替换)，代替$\mathcal{N}_{20}$，如下图</p><p> <img src="/2020/03/08/【数据结构】搜索树/erase.jpg" alt="erase"></p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> BinarySearchTree&lt;K, V&gt;::erase(<span class="keyword">const</span> K&amp; key)</span><br><span class="line">&#123;</span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = find(key, <span class="literal">false</span>);</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------ 重新组织二叉树 ------------------</span></span><br><span class="line">    <span class="comment">// 查找左子树的最大值，或右子树的最小值</span></span><br><span class="line">    BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* replace = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">while</span> (!node-&gt;isLeaf())&#123;<span class="comment">// 直到搜索到叶节点为止</span></span><br><span class="line">        <span class="keyword">if</span> (node-&gt;left) &#123;</span><br><span class="line">            replace = max(node-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (node-&gt;right) &#123;</span><br><span class="line">            replace = min(node-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (replace) &#123;<span class="comment">// 找到可替换子节点</span></span><br><span class="line">            Pair&lt;K, V&gt;* np = node-&gt;get();</span><br><span class="line">            Pair&lt;K, V&gt;* rp = replace-&gt;get();</span><br><span class="line">            np-&gt;setKey(rp-&gt;getKey());</span><br><span class="line">            np-&gt;setVal(rp-&gt;getVal());</span><br><span class="line">            node = replace;             <span class="comment">// 替换节点node</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (node-&gt;parent) &#123;<span class="comment">// 修改父节点信息</span></span><br><span class="line">        <span class="keyword">if</span> (node == node-&gt;parent-&gt;left)</span><br><span class="line">            node-&gt;parent-&gt;left = <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            node-&gt;parent-&gt;right = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;<span class="comment">// 无父节点，即整棵树只有一个根节点，则修改根节点为空</span></span><br><span class="line">        <span class="keyword">this</span>-&gt;m_tnRoot = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------ 释放资源 ------------------</span></span><br><span class="line">    Pair&lt;K, V&gt;* pair = node-&gt;get();</span><br><span class="line">    <span class="keyword">delete</span> pair; <span class="keyword">delete</span> node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* BinarySearchTree&lt;K, V&gt;::max(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span> (node-&gt;right) </span><br><span class="line">        node = node-&gt;right;</span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* BinarySearchTree&lt;K, V&gt;::min(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">while</span> (node-&gt;left)</span><br><span class="line">        node = node-&gt;left;</span><br><span class="line">    <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>升序打印</p><p> 将元素按<strong>中序遍历</strong>进行打印即可</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> BinarySearchTree&lt;K, V&gt;::ascending() </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;inOrder(&amp;printPairNode, <span class="keyword">this</span>-&gt;m_tnRoot);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> <code>print</code>实现将元素按<strong>层次遍历</strong>的顺序打印</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> BinarySearchTree&lt;K, V&gt;::print()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;levelOrder(&amp;printPairNode, <span class="keyword">this</span>-&gt;m_tnRoot);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> BinarySearchTree&lt;K, V&gt;::printPairNode(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>;</span><br><span class="line">    Pair&lt;K, V&gt;* p = node-&gt;get();</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">'['</span> &lt;&lt; p-&gt;getKey() &lt;&lt; <span class="string">']'</span> &lt;&lt; p-&gt;getVal() &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">string</span> names[<span class="number">8</span>] = &#123;</span><br><span class="line"><span class="string">"甲"</span>, <span class="string">"乙"</span>, <span class="string">"丙"</span>, <span class="string">"丁"</span>, <span class="string">"戊"</span>, <span class="string">"己"</span>, <span class="string">"庚"</span>, <span class="string">"辛"</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> numbers[<span class="number">8</span>] = &#123; <span class="number">20</span>, <span class="number">15</span>, <span class="number">25</span>, <span class="number">12</span>, <span class="number">18</span>, <span class="number">30</span>, <span class="number">16</span>, <span class="number">19</span> &#125;;</span><br><span class="line"></span><br><span class="line">BinarySearchTree&lt;<span class="keyword">int</span>, <span class="built_in">string</span>&gt; tree;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入键值对</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"插入"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"插入"</span> &lt;&lt; numbers[i] &lt;&lt; <span class="string">": "</span>;</span><br><span class="line">tree.insert(numbers[i], names[i]);</span><br><span class="line">tree.print();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"升序排列: "</span>;</span><br><span class="line">tree.ascending();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除键值对</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"删除"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">tree.erase(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"删除"</span> &lt;&lt; numbers[i] &lt;&lt; <span class="string">": "</span>;</span><br><span class="line">tree.erase(numbers[i]);</span><br><span class="line">tree.print();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">插入</span><br><span class="line">插入20: [20]甲</span><br><span class="line">插入15: [20]甲 [15]乙</span><br><span class="line">插入25: [20]甲 [15]乙 [25]丙</span><br><span class="line">插入12: [20]甲 [15]乙 [25]丙 [12]丁</span><br><span class="line">插入18: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊</span><br><span class="line">插入30: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己</span><br><span class="line">插入16: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚</span><br><span class="line">插入19: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚 [19]辛</span><br><span class="line">升序排列: [12]丁 [15]乙 [16]庚 [18]戊 [19]辛 [20]甲 [25]丙 [30]己</span><br><span class="line"></span><br><span class="line">删除</span><br><span class="line">删除20: [19]辛 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚</span><br><span class="line">删除15: [19]辛 [12]丁 [25]丙 [18]戊 [30]己 [16]庚</span><br><span class="line">删除25: [19]辛 [12]丁 [30]己 [18]戊 [16]庚</span><br><span class="line">删除12: [19]辛 [16]庚 [30]己 [18]戊</span><br><span class="line">删除18: [19]辛 [16]庚 [30]己</span><br><span class="line">删除30: [19]辛 [16]庚</span><br><span class="line">删除16: [19]辛</span><br><span class="line">删除19:</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure><p>删除时节点的重组织示意图如下</p><p><img src="/2020/03/08/【数据结构】搜索树/bst1.jpg" alt="bst1"></p><h2 id="索引二叉搜索树"><a href="#索引二叉搜索树" class="headerlink" title="索引二叉搜索树"></a>索引二叉搜索树</h2><h3 id="声明-1"><a href="#声明-1" class="headerlink" title="声明"></a>声明</h3><p>由<code>BinarySearchTree&lt;K, V&gt;</code>派生而来，添加操作符<code>[]</code>的重载</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IndexedBinarySearchTree</span> :</span></span><br><span class="line"><span class="keyword">public</span> BinarySearchTree&lt;K, V&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">IndexedBinarySearchTree() : BinarySearchTree&lt;K, V&gt;() &#123;&#125;</span><br><span class="line">~IndexedBinarySearchTree() &#123;&#125;</span><br><span class="line"></span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* <span class="keyword">operator</span>[] (<span class="keyword">int</span> index);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>初始化偏置$offset = 0$，在向右搜索时，需更新偏置$offset$使右子树中各节点的$leafSize$仍能用于索引</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* IndexedBinarySearchTree&lt;K, V&gt;::<span class="keyword">operator</span>[] (<span class="keyword">int</span> index)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (index &lt; <span class="number">0</span>) <span class="keyword">throw</span> <span class="string">"索引必须大于0！"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> leftSize = <span class="number">-1</span>; <span class="keyword">int</span> offset = <span class="number">0</span>;</span><br><span class="line">BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = <span class="keyword">this</span>-&gt;m_tnRoot;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (leftSize != index &amp;&amp; node) &#123;</span><br><span class="line">leftSize = IndexedBinarySearchTree&lt;K, V&gt;::sizeofNode(node-&gt;left);</span><br><span class="line"><span class="keyword">if</span> (offset + leftSize == index) &#123;</span><br><span class="line">IndexedBinarySearchTree&lt;K, V&gt;::printPairNode(node);</span><br><span class="line"><span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (offset + leftSize &gt; index)</span><br><span class="line">node = node-&gt;left;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">node = node-&gt;right;</span><br><span class="line">offset += (leftSize + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">string</span> names[<span class="number">8</span>] = &#123;</span><br><span class="line"><span class="string">"甲"</span>, <span class="string">"乙"</span>, <span class="string">"丙"</span>, <span class="string">"丁"</span>, <span class="string">"戊"</span>, <span class="string">"己"</span>, <span class="string">"庚"</span>, <span class="string">"辛"</span> &#125;;</span><br><span class="line"><span class="keyword">int</span> numbers[<span class="number">8</span>] = &#123; <span class="number">20</span>, <span class="number">15</span>, <span class="number">25</span>, <span class="number">12</span>, <span class="number">18</span>, <span class="number">30</span>, <span class="number">16</span>, <span class="number">19</span> &#125;;</span><br><span class="line"></span><br><span class="line">IndexedBinarySearchTree&lt;<span class="keyword">int</span>, <span class="built_in">string</span>&gt; tree;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入键值对</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"插入"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"插入"</span> &lt;&lt; numbers[i] &lt;&lt; <span class="string">": "</span>;</span><br><span class="line">tree.insert(numbers[i], names[i]);</span><br><span class="line">tree.print();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"升序排列: "</span>;</span><br><span class="line">tree.ascending();</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"索引"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line">tree[i];</span><br><span class="line">&#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下图为例，讲解索引$index=2$的过程</p><ol><li>初始化$offset = 0$；</li><li>$index = 2$时，首先在节点$\mathcal{N}_{20}$比较$2 + offset &lt; 5$，向左搜索；</li><li>在节点$\mathcal{N}_{15}$处$2 + offset&gt; 1$，向右搜索，并更新$offset = offset + \mathcal{N}_{15}.leafSize + 1 = 2$；</li><li>节点$\mathcal{N}_{18}$处$2 &lt; 1 + offset$，向左搜索；在节点$\mathcal{N}_{16}$处$2 = 0 + offset$，返回该节点。</li></ol><p><img src="/2020/03/08/【数据结构】搜索树/index.jpg" alt="index"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">插入</span><br><span class="line">插入20: [20]甲</span><br><span class="line">插入15: [20]甲 [15]乙</span><br><span class="line">插入25: [20]甲 [15]乙 [25]丙</span><br><span class="line">插入12: [20]甲 [15]乙 [25]丙 [12]丁</span><br><span class="line">插入18: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊</span><br><span class="line">插入30: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己</span><br><span class="line">插入16: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚</span><br><span class="line">插入19: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚 [19]辛</span><br><span class="line">升序排列: [12]丁 [15]乙 [16]庚 [18]戊 [19]辛 [20]甲 [25]丙 [30]己</span><br><span class="line"></span><br><span class="line">索引</span><br><span class="line">[12]丁 [15]乙 [16]庚 [18]戊 [19]辛 [20]甲 [25]丙 [30]己</span><br><span class="line"></span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】竞赛树</title>
      <link href="/2020/03/06/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%AB%9E%E8%B5%9B%E6%A0%91/"/>
      <url>/2020/03/06/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%AB%9E%E8%B5%9B%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h1 id="定义及概念"><a href="#定义及概念" class="headerlink" title="定义及概念"></a>定义及概念</h1><p><strong>定义</strong>：<strong>竞赛树</strong>(tournament tree)也是完全二叉树，它的基本操作是替换最大(或最小)元素。如果有$n$个元素，这个基本操作的用时为$\Theta(\log n)$。</p><p>竞赛树可分为<strong>赢者树</strong>(winner tree)和<strong>输者树</strong>(loser tree)，每个内部节点分别记录比赛的赢者和输者。在<strong>最小赢者树</strong>(min winner tree)中，分数小的选手获胜，分数相等则左孩子获胜，<strong>最大赢者树</strong>(max winner tree)反之。</p><p><strong>定义</strong>：有$n$个选手的<strong>赢者树</strong>是一棵完全二叉树，它有$n$个外部节点和$n-1$个内部节点，每个内部节点记录的是在该节点比赛的赢者。它的优点是，当一名选手分数改变时，修改竞赛树比较容易，由底至顶重新对该选手进行比赛即可，需要修改的比赛场次数介于$1 \sim \lceil \log_2 n \rceil$之间。</p><h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p>可用数组二叉树表示，存储效率最高。为了便于计算机实现，把赢者树限制为完全二叉树。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaxWinnerTree</span> :</span> <span class="keyword">public</span> ArrayBinaryTree&lt;T&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">MaxWinnerTree() : ArrayBinaryTree&lt;T&gt;() &#123;&#125;</span><br><span class="line">MaxWinnerTree(T* <span class="built_in">set</span>, <span class="keyword">int</span> n);</span><br><span class="line">~MaxWinnerTree() &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> i, <span class="keyword">const</span> T&amp; value)</span> </span>&#123; <span class="keyword">this</span>-&gt;m_TElements[i + offset()] = value; &#125;  <span class="comment">// 设置选手i的值</span></span><br><span class="line"><span class="function">T&amp; <span class="title">get</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> i)</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;m_TElements[i + offset()]; &#125;   <span class="comment">// 获取选手i的值</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">index</span><span class="params">(<span class="keyword">const</span> T&amp; value)</span> <span class="keyword">const</span></span>;    <span class="comment">// 查询选手索引</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">play</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> i = <span class="number">-1</span>)</span></span>;    <span class="comment">// 在忽略选手i的情况下比赛，即始终被视作输者</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">replay</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> i)</span></span>;       <span class="comment">// 在i的值修改后，对i一系列进行重赛</span></span><br><span class="line"><span class="function">T&amp; <span class="title">winner</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;m_TElements[<span class="number">0</span>]; &#125;  <span class="comment">// 返回胜者</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">offset</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> (<span class="keyword">this</span>-&gt;m_iCount - <span class="number">1</span>) / <span class="number">2</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">MaxWinnerTree&lt;T&gt;::MaxWinnerTree(T* <span class="built_in">set</span>, <span class="keyword">int</span> n) </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> size = <span class="number">2</span> * n - <span class="number">1</span>;<span class="comment">// 包含n-1个内部节点与n个外部节点</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;m_TElements = <span class="keyword">new</span> T[size];</span><br><span class="line">    <span class="built_in">std</span>::copy(<span class="built_in">set</span>, <span class="built_in">set</span> + n, <span class="keyword">this</span>-&gt;m_TElements + n - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;m_iCount = <span class="keyword">this</span>-&gt;m_iSize = size;</span><br><span class="line">    play();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxWinnerTree&lt;T&gt;::play(<span class="keyword">const</span> <span class="keyword">int</span> i) &#123;</span><br><span class="line"><span class="comment">// 从最后一个内部节点开始竞赛</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="keyword">this</span>-&gt;parent(<span class="keyword">this</span>-&gt;m_iCount - <span class="number">1</span>); index &gt;= <span class="number">0</span>; index--) &#123;</span><br><span class="line"><span class="keyword">int</span> left = <span class="keyword">this</span>-&gt;left(index); <span class="keyword">int</span> right = <span class="keyword">this</span>-&gt;right(index);</span><br><span class="line"><span class="comment">// 包含忽略的选手</span></span><br><span class="line"><span class="keyword">if</span> (left - offset() == i &amp;&amp; i &gt;= <span class="number">0</span>)</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements[index] = <span class="keyword">this</span>-&gt;m_TElements[right];</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (right - offset() == i &amp;&amp; i &gt;= <span class="number">0</span>)</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements[index] = <span class="keyword">this</span>-&gt;m_TElements[left];</span><br><span class="line"><span class="comment">// 所有选手参加比赛</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements[index] = <span class="keyword">this</span>-&gt;m_TElements[left] &gt; <span class="keyword">this</span>-&gt;m_TElements[right] ? \</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements[left]: <span class="keyword">this</span>-&gt;m_TElements[right];</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxWinnerTree&lt;T&gt;::replay(<span class="keyword">const</span> <span class="keyword">int</span> i) &#123;</span><br><span class="line"><span class="comment">// 对选手i进行重赛</span></span><br><span class="line"><span class="keyword">int</span> index = <span class="keyword">this</span>-&gt;parent(i + offset());<span class="comment">// 寻找父节点</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">index = <span class="keyword">this</span>-&gt;parent(index);</span><br><span class="line"><span class="keyword">int</span> left = <span class="keyword">this</span>-&gt;left(index); </span><br><span class="line"><span class="keyword">int</span> right = <span class="keyword">this</span>-&gt;right(index);</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements[index] = <span class="keyword">this</span>-&gt;m_TElements[left] &gt;= <span class="keyword">this</span>-&gt;m_TElements[right] ? \</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements[left]: <span class="keyword">this</span>-&gt;m_TElements[right];</span><br><span class="line"><span class="keyword">if</span> (index == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">int</span> MaxWinnerTree&lt;T&gt;::index(<span class="keyword">const</span> T&amp; value) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">T* start = <span class="keyword">this</span>-&gt;m_TElements + offset();</span><br><span class="line">T* end = <span class="keyword">this</span>-&gt;m_TElements + <span class="keyword">this</span>-&gt;m_iCount;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">int</span>)(<span class="built_in">std</span>::find(start, end, value) - start);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在完全二叉树的假设下，上述实现可处理奇数个或偶数个参赛选手的情况，测试如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> symbols[<span class="number">32</span>] = &#123;</span><br><span class="line"><span class="number">3</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">20</span>, </span><br><span class="line"><span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">12</span>, <span class="number">15</span>, </span><br><span class="line"><span class="number">30</span>, <span class="number">17</span>&#125;;</span><br><span class="line">MaxWinnerTree&lt;<span class="keyword">int</span>&gt; tree(symbols, <span class="number">12</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"size: "</span> &lt;&lt; tree.size() &lt;&lt; <span class="built_in">endl</span> \</span><br><span class="line">&lt;&lt; <span class="string">"height: "</span> &lt;&lt; tree.height() &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">tree.print();</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Winner: "</span> &lt;&lt; tree.winner() &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">tree.play(<span class="number">10</span>);      <span class="comment">// 忽略30重赛</span></span><br><span class="line">tree.print();</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Winner: "</span> &lt;&lt; tree.winner() &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">tree.<span class="built_in">set</span>(<span class="number">10</span>, <span class="number">2</span>);    <span class="comment">// 修改 30 为 2</span></span><br><span class="line">tree.replay(<span class="number">10</span>);    <span class="comment">// 重赛</span></span><br><span class="line">tree.print();</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">size: 23</span><br><span class="line">height: 5</span><br><span class="line"></span><br><span class="line">30</span><br><span class="line">30 7</span><br><span class="line">20 30 5 7</span><br><span class="line">20 9 15 30 3 5 6 7</span><br><span class="line">20 8 2 9 12 15 30 17</span><br><span class="line">Winner: 30</span><br><span class="line"></span><br><span class="line">20</span><br><span class="line">20 7</span><br><span class="line">20 17 5 7</span><br><span class="line">20 9 15 17 3 5 6 7</span><br><span class="line">20 8 2 9 12 15 30 17</span><br><span class="line">Winner: 20</span><br><span class="line"></span><br><span class="line">20</span><br><span class="line">20 7</span><br><span class="line">20 17 5 7</span><br><span class="line">20 9 15 17 3 5 6 7</span><br><span class="line">20 8 2 9 12 15 2 17</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure><h1 id="应用：箱子装载问题"><a href="#应用：箱子装载问题" class="headerlink" title="应用：箱子装载问题"></a>应用：箱子装载问题</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>箱子数量不限，每个箱子的容量为$C_i$，待装箱的物体有$n$个，物品$j$的需要占用$c_j, 0 \leq c_j \leq C_i$。<strong>可行装载</strong>(feasible packing)是指所有物品均装入箱子且不溢出，<strong>最优装载</strong>(optimal packing)是指使用箱子最少的可行装载。</p><h2 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h2><p>箱子装载问题是NP-hard问题，常用近似算法求解，求取次优解。有以下几种常用的算法</p><ol><li><strong>最先适配法</strong>(First Fit, FF)：物品与箱子均依次排列，第$j$个物品放入最左边的可装载箱子；</li><li><strong>最优适配法</strong>(Best Fit, BF)：第$j$个物品放入剩余可用容量$\overline{C}_i$最小但不小于$c_j$的箱子；</li><li><strong>最先适配递减法</strong>(First Fit Decreasing, FFD)：类似FF，区别是将物品按所需容量递减排列，即$c_j \geq c_{j+1}$；</li><li><strong>最优适配递减法</strong>(Best Fit Decreasing, BFD)：类似BF，区别是将物品按所需容量递减排列，即$c_j \geq c_{j+1}$。</li></ol><p><strong>定理</strong>：设$I$为箱子装载问题的任一实例，$b(I)$为最优装载所用的箱子数。那么，FF和BF所用箱子不会超过$\frac{17}{10}b(I) + 2$，FFD和BFD所用箱子不会超过$\frac{11}{9}b(I) + 4$。</p><p>用竞赛数实现FF适配法，可由根节点向叶节点搜索，减少时间复杂度。<strong>用竞赛数实现BF适配法</strong>：</p><ul><li>指定箱子数<code>n_bins</code>目与箱子容量<code>capacity</code>，初始化竞赛树所需内存空间与尺寸信息等；</li><li>结果保存为<code>result</code>，保存各个物体所选箱子的索引(0, …, <code>n_objs</code>)；</li><li>输入各物品的所需容量信息<code>objs</code>进行求解；<ol><li>优先查找当前使用容量最大的箱子，也即<strong>剩余容量最小的箱子</strong>，若该箱子能放下物品，则继续下一物品的箱子选择；否则进入2；</li><li>将不符合要求的箱子已使用容量置为<code>-1</code>，使其在竞赛数中<strong>始终无法获胜</strong>，重新比赛查找当前使用容量最大的箱子；</li><li><strong>循环2</strong>直至找到箱子，继续下一物品的箱子选择。</li></ol></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinPackingBF</span> :</span> <span class="keyword">public</span> MaxWinnerTree&lt;<span class="keyword">int</span>&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">BinPackingBF(<span class="keyword">int</span> n = <span class="number">8</span>, <span class="keyword">int</span> capacity = <span class="number">10</span>);<span class="comment">// 箱子数与箱子容量</span></span><br><span class="line">~BinPackingBF() &#123; <span class="keyword">if</span> (result) <span class="keyword">delete</span> result; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span>* objs, <span class="keyword">int</span> n)</span></span>;<span class="comment">// 求解</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="keyword">int</span> n_bins; <span class="keyword">int</span> capacity;</span><br><span class="line"><span class="keyword">int</span>* result; <span class="keyword">int</span> n_objs;    <span class="comment">// 保存每个箱子所在的箱子索引</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">BinPackingBF::BinPackingBF(<span class="keyword">int</span> n, <span class="keyword">int</span> capacity)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> size = <span class="number">2</span> * n - <span class="number">1</span>;</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements = <span class="keyword">new</span> <span class="keyword">int</span>[size];</span><br><span class="line"><span class="keyword">this</span>-&gt;m_iCount = <span class="keyword">this</span>-&gt;m_iSize = size;</span><br><span class="line"><span class="keyword">this</span>-&gt;n_bins = n; <span class="keyword">this</span>-&gt;capacity = capacity;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> BinPackingBF::solve(<span class="keyword">int</span>* objs, <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">result = <span class="keyword">new</span> <span class="keyword">int</span>[n];  n_objs = n;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存每次物品放置前的箱子使用情况</span></span><br><span class="line"><span class="keyword">int</span>* bins = <span class="keyword">new</span> <span class="keyword">int</span>[n_bins];</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">memset</span>(bins, <span class="number">0</span>, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * n_bins);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line"><span class="keyword">if</span> (objs[n] &gt; capacity)</span><br><span class="line"><span class="keyword">throw</span> <span class="string">"物体太大！"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将当前箱子使用情况赋值给外部节点</span></span><br><span class="line"><span class="built_in">memcpy</span>(<span class="keyword">this</span>-&gt;m_TElements + <span class="keyword">this</span>-&gt;offset(), bins, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * n_bins);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找当前剩余容量最小的箱子</span></span><br><span class="line"><span class="keyword">this</span>-&gt;play();</span><br><span class="line"><span class="keyword">int</span> used = <span class="keyword">this</span>-&gt;winner();<span class="comment">// 当前已使用容量最大的箱子容量</span></span><br><span class="line"><span class="keyword">int</span> index = <span class="keyword">this</span>-&gt;index(used);<span class="comment">// 当前已使用容量最大的箱子索引</span></span><br><span class="line"><span class="keyword">if</span> (capacity - used &gt;= objs[i]) &#123;<span class="comment">// 剩余容量大于物品大小</span></span><br><span class="line">result[i] = index;</span><br><span class="line">bins[index] += objs[i];</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当前剩余容量最小的箱子无法装入物品i</span></span><br><span class="line"><span class="keyword">int</span> choosen = <span class="number">-1</span>;</span><br><span class="line"><span class="keyword">while</span> (choosen &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements[index + offset()] = <span class="number">-1</span>;<span class="comment">// 该箱子始终无法获胜</span></span><br><span class="line"><span class="keyword">this</span>-&gt;play();<span class="comment">// 重新竞赛</span></span><br><span class="line">used = <span class="keyword">this</span>-&gt;winner();<span class="comment">// 当前已使用容量最大的箱子容量</span></span><br><span class="line">index = <span class="keyword">this</span>-&gt;index(used);<span class="comment">// 当前已使用容量最大的箱子索引</span></span><br><span class="line"><span class="keyword">if</span> (capacity - used &gt;= objs[i])</span><br><span class="line">choosen = index;</span><br><span class="line">&#125;</span><br><span class="line">result[i] = choosen;</span><br><span class="line">bins[choosen] += objs[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">delete</span>[] bins;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> BinPackingBF::print() <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n_objs; i++)</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; result[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主函数测试如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="function">BinPackingBF <span class="title">puzzle</span><span class="params">(<span class="number">8</span>, <span class="number">10</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> objs[<span class="number">8</span>] = &#123; <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">8</span>&#125;;</span><br><span class="line">puzzle.solve(objs, <span class="number">8</span>);</span><br><span class="line"></span><br><span class="line">puzzle.print();</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0 0 0 1 1 2 1 3</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure><p>以上输出表示</p><div class="table-container"><table><thead><tr><th>箱子索引</th><th>0</th><th>1</th><th>2</th><th>3</th><th>…</th></tr></thead><tbody><tr><td>箱子物品所需容量</td><td>3, 5, 2</td><td>4, 2, 4</td><td>5</td><td>8</td><td>/</td></tr></tbody></table></div><p>将箱子排序后输入，即为<strong>BFD适配法</strong>：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">iint <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="function">BinPackingBF <span class="title">puzzle</span><span class="params">(<span class="number">8</span>, <span class="number">10</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> objs[<span class="number">8</span>] = &#123; <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">8</span> &#125;;</span><br><span class="line">sort(objs, objs + <span class="number">8</span>, </span><br><span class="line">[](<span class="keyword">int</span> x, <span class="keyword">int</span> y) &#123; <span class="keyword">return</span> x &gt; y; &#125;);</span><br><span class="line">puzzle.solve(objs, <span class="number">8</span>);</span><br><span class="line"></span><br><span class="line">puzzle.print();</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0 1 1 2 2 3 0 2</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>箱子索引</th><th>0</th><th>1</th><th>2</th><th>3</th><th>…</th></tr></thead><tbody><tr><td>箱子物品所需容量</td><td>8, 2</td><td>5, 5</td><td>4, 4, 2</td><td>3</td><td>/</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】优先级队列</title>
      <link href="/2020/03/03/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/"/>
      <url>/2020/03/03/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97/</url>
      
        <content type="html"><![CDATA[<h1 id="定义及抽象数据类型"><a href="#定义及抽象数据类型" class="headerlink" title="定义及抽象数据类型"></a>定义及抽象数据类型</h1><p><strong>优先级队列</strong>(priority queue)是$0$个或多个元素的集合，每个元素都有一个优先权或值，在<strong>最小优先级队列</strong>(min priority queue)中，对其进行的元素操作都是当前优先级最小的，<strong>最大优先级队列</strong>(max priority queue)则相反。优先级队列的元素可以有相同的优先级，对这些元素其顺序任意。优先级队列的操作有</p><ul><li>查找一个元素；</li><li>插入一个元素；</li><li>删除一个元素。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">抽象数据类型 PriorityQueue</span><br><span class="line">&#123;</span><br><span class="line">实例：</span><br><span class="line"></span><br><span class="line">操作：</span><br><span class="line">    empty():    队列是否为空</span><br><span class="line">    size():     队列的元素个数</span><br><span class="line">    top():      返回当前优先级最大/最小的元素</span><br><span class="line">    push(x, p): 插入元素，根据优先级调整队列顺序</span><br><span class="line">    pop():      删除并返回当前优先级最大/最小的元素</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="大根堆-小根堆"><a href="#大根堆-小根堆" class="headerlink" title="大根堆/小根堆"></a>大根堆/小根堆</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p><strong>定义</strong>：<strong>大根树/小根树</strong>是指每个节点的值都大于其子节点的值的树，节点的子节点个数可以任意。<br><strong>定义</strong>：<strong>大根堆/小根堆</strong>即是大根树/小根数，也是<strong>完全二叉树</strong>。</p><p>利用二叉树的特性，用节点在数组描述中的位置来表示它在堆中的位置。堆作为完全二叉树，具有$n$个元素时高度为$\lceil\log_2(n+1)\rceil$。因此如果能够在$O(height)$时间内完成插入和删除操作，那么操作的复杂度为$O(\log n)$。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="元素的声明与定义"><a href="#元素的声明与定义" class="headerlink" title="元素的声明与定义"></a>元素的声明与定义</h3><p>优先级队列的元素，不仅包含存储的数据/对象，还需指定优先级，故元素的声明与定义如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Element</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">Element(<span class="keyword">const</span> T&amp; value, <span class="keyword">const</span> <span class="keyword">int</span> priority) &#123;</span><br><span class="line">v = <span class="keyword">new</span> T; *v = value;</span><br><span class="line">p = <span class="keyword">new</span> <span class="keyword">int</span>; *p = priority;</span><br><span class="line">&#125;</span><br><span class="line">~Element() &#123; </span><br><span class="line"><span class="keyword">delete</span> p; <span class="keyword">delete</span> v; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setPriority</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> priority)</span> </span>&#123; *p = priority; &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setValue</span><span class="params">(<span class="keyword">const</span> T&amp; value)</span> </span>&#123; *v = value; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getPriority</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> *p; &#125;</span><br><span class="line"><span class="function">T   <span class="title">getValue</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> *v; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">T* v;</span><br><span class="line"><span class="keyword">int</span>* p;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="大根堆-小根堆的链表描述"><a href="#大根堆-小根堆的链表描述" class="headerlink" title="大根堆/小根堆的链表描述"></a>大根堆/小根堆的链表描述</h3><h4 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h4><p>最大堆可采用链表描述的二叉树实现，由<code>LinkedBinaryTree&lt;Element&lt;T&gt;*&gt;</code>派生，实现抽象类中的函数，声明如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaxHeap</span> :</span> <span class="keyword">public</span> LinkedBinaryTree&lt;Element&lt;T&gt;*&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">MaxHeap() : LinkedBinaryTree&lt;Element&lt;T&gt;*&gt;() &#123;&#125;</span><br><span class="line">~MaxHeap() &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">const</span> T&amp; value, <span class="keyword">const</span> <span class="keyword">int</span> priority)</span></span>;</span><br><span class="line"><span class="function">T <span class="title">pop</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">T <span class="title">top</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;m_tnRoot-&gt;get()-&gt;getValue(); &#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxPriority</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;m_tnRoot-&gt;get()-&gt;getPriority(); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(BinaryTreeNode&lt;Element&lt;T&gt;*&gt;*, BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><blockquote><p>使用数组描述更为简单，但插入操作时，数组时间复杂度为$O(n)$，二叉树描述时间复杂度更低为$O(\log n)$。</p></blockquote><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>比较关键的是静态保护成员函数<code>insert()</code>，其作用是将已初始化的节点<code>node</code>，插入已定义的树<code>tree</code>。该函数需满足以下要求</p><ol><li><strong>最大堆是完全二叉树</strong>，进行节点的插入时，需满足完全二叉树的特性。<strong>一棵高度为$H$的完全二叉树，第h(h&lt;H)$层的节点不存在空子节点</strong>。故只需找到第一个存在空子节点的节点<code>N</code>，将元素放入<code>N</code>的空节点即可(左子树优先)；</li><li>调整节点的顺序。即“<strong>起泡操作</strong>”，保证该二叉树每个节点的优先级均大于其子节点优先级；</li><li>在调整节点顺序的过程可通过修改节点元素完成。</li></ol><blockquote><ul><li>需要修改树<code>tree</code>的结构，故参数$2$为<strong>二级指针</strong>；</li><li>为使函数更通用，将<code>tree</code>根节点信息保存后置为空，<strong>不作为子树</strong>，在函数退出时恢复其子树信息。</li></ul></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxHeap&lt;T&gt;::insert(BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node, BinaryTreeNode&lt;Element&lt;T&gt;*&gt;** tree)</span><br><span class="line">&#123;</span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* root = (*tree)-&gt;parent; (*tree)-&gt;parent = <span class="literal">nullptr</span>;<span class="comment">// 保存根节点信息，不将tree视作子树</span></span><br><span class="line"></span><br><span class="line">LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**&gt;* <span class="built_in">queue</span> = MaxHeap&lt;T&gt;::tree2flattenQueue(tree);<span class="comment">// 调用继承的父类静态函数</span></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">queue</span>-&gt;size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 根据完全二叉树，寻找堆的最后一个位置</span></span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* parent = *<span class="built_in">queue</span>-&gt;pop();</span><br><span class="line"></span><br><span class="line"><span class="comment">/************************** 插入节点 **************************/</span></span><br><span class="line"><span class="comment">// 无子树空，不进行节点插入</span></span><br><span class="line"><span class="keyword">if</span> (parent-&gt;left &amp;&amp; parent-&gt;right) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 存在子树为空，优先对左子树进行插入</span></span><br><span class="line"><span class="keyword">if</span> (!parent-&gt;left) &#123;</span><br><span class="line">parent-&gt;left = node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!parent-&gt;right) &#123;</span><br><span class="line">parent-&gt;right = node;</span><br><span class="line">&#125;</span><br><span class="line">node-&gt;parent = parent;</span><br><span class="line"><span class="comment">/************************** 插入节点 **************************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/************************ 调整节点顺序 ************************/</span></span><br><span class="line"><span class="keyword">while</span> (node-&gt;get()-&gt;getPriority() &gt; parent-&gt;get()-&gt;getPriority()) &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 交换父节点与子节点的值</span></span><br><span class="line">Element&lt;T&gt;* temp = node-&gt;get(); node-&gt;<span class="built_in">set</span>(parent-&gt;get()); parent-&gt;<span class="built_in">set</span>(temp);</span><br><span class="line">node = node-&gt;parent;</span><br><span class="line">parent = node-&gt;parent;</span><br><span class="line"><span class="keyword">if</span> (!parent) <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/************************ 调整节点顺序 ************************/</span></span><br><span class="line"></span><br><span class="line">(*tree)-&gt;parent = root;<span class="comment">// 恢复子树身份</span></span><br><span class="line"><span class="keyword">delete</span> <span class="built_in">queue</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxHeap&lt;T&gt;::insert(BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node, BinaryTreeNode&lt;Element&lt;T&gt;*&gt;** tree)</span><br><span class="line">&#123;</span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* root = (*tree)-&gt;parent; (*tree)-&gt;parent = <span class="literal">nullptr</span>;<span class="comment">// 保存根节点信息，不将tree视作子树</span></span><br><span class="line"></span><br><span class="line">LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**&gt;* <span class="built_in">queue</span> = MaxHeap&lt;T&gt;::tree2flattenQueue(tree);<span class="comment">// 调用继承的父类静态函数</span></span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">queue</span>-&gt;size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// 根据完全二叉树，寻找堆的最后一个位置</span></span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* parent = *<span class="built_in">queue</span>-&gt;pop();</span><br><span class="line"></span><br><span class="line"><span class="comment">/************************** 插入节点 **************************/</span></span><br><span class="line"><span class="comment">// 无子树空，不进行节点插入</span></span><br><span class="line"><span class="keyword">if</span> (parent-&gt;left &amp;&amp; parent-&gt;right) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 存在子树为空，优先对左子树进行插入</span></span><br><span class="line"><span class="keyword">if</span> (!parent-&gt;left) &#123;</span><br><span class="line">parent-&gt;left = node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (!parent-&gt;right) &#123;</span><br><span class="line">parent-&gt;right = node;</span><br><span class="line">&#125;</span><br><span class="line">node-&gt;parent = parent;</span><br><span class="line"><span class="comment">/************************** 插入节点 **************************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/************************ 调整节点顺序 ************************/</span></span><br><span class="line"><span class="keyword">while</span> (node-&gt;get()-&gt;getPriority() &gt; parent-&gt;get()-&gt;getPriority()) &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/************ 修改节点间的链接关系 **************/</span></span><br><span class="line"><span class="comment">// 1. 修改插入节点的父节点信息</span></span><br><span class="line">node-&gt;parent = parent-&gt;parent;</span><br><span class="line"><span class="comment">// 2. 修改父节点的父节点的子树信息</span></span><br><span class="line"><span class="keyword">if</span> (parent-&gt;parent) &#123;</span><br><span class="line"><span class="comment">// 2.1 父节点不为根节点，则将祖父节点作为父节点</span></span><br><span class="line"><span class="keyword">if</span> (parent-&gt;parent-&gt;left == parent) &#123;</span><br><span class="line">parent-&gt;parent-&gt;left = node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">parent-&gt;parent-&gt;right = node;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 2.2 若父节点是树的根节点，则修改为根节点</span></span><br><span class="line">*tree = node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 3. 修改插入节点的子树信息、父节点的子树信息</span></span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* temp;</span><br><span class="line"><span class="keyword">if</span> (parent-&gt;left == node) &#123;</span><br><span class="line"><span class="comment">// 3.1 插入节点为父节点的左子节点</span></span><br><span class="line"><span class="comment">// 3.1.1 左子树更新</span></span><br><span class="line">temp = node-&gt;left;</span><br><span class="line">node-&gt;left = parent;</span><br><span class="line">parent-&gt;left = temp;</span><br><span class="line"><span class="comment">// 3.1.2 右子树互换</span></span><br><span class="line">temp = node-&gt;right;</span><br><span class="line">node-&gt;right = parent-&gt;right;</span><br><span class="line">parent-&gt;right = temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 3.2 插入节点为父节点的右子节点</span></span><br><span class="line"><span class="comment">// 3.2.1 右子树更新</span></span><br><span class="line">temp = node-&gt;right;</span><br><span class="line">node-&gt;right = parent;</span><br><span class="line">parent-&gt;right = temp;</span><br><span class="line"><span class="comment">// 3.1.2 左子树互换</span></span><br><span class="line">temp = node-&gt;left;</span><br><span class="line">node-&gt;left = parent-&gt;left;</span><br><span class="line">parent-&gt;left = temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 4. 修改父节点的父节点信息</span></span><br><span class="line">parent-&gt;parent = node;</span><br><span class="line"><span class="comment">/************ 修改节点间的链接关系 **************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/******* 更新node的父节点，多次“气泡”操作 *******/</span></span><br><span class="line">parent = node-&gt;parent;</span><br><span class="line"><span class="keyword">if</span> (!parent) <span class="keyword">break</span>;</span><br><span class="line"><span class="comment">/******* 更新node的父节点，多次“气泡”操作 *******/</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/************************ 调整节点顺序 ************************/</span></span><br><span class="line"></span><br><span class="line">(*tree)-&gt;parent = root;<span class="comment">// 恢复子树身份</span></span><br><span class="line">&#125;</span><br><span class="line">``` </span><br><span class="line">&gt; 几天之后对这份代码的评价：愚蠢！不需要将一棵子树全部节点插入到另外一棵，实际上可以将根节点与完全二叉树层级遍历最后一个字节点元素交换，调整根节点元素位置，然后释放叶子节点即可，详细可参考堆排序。--&gt;</span><br><span class="line"></span><br><span class="line">插入与删除节点时，均调用上述函数。插入函数结构较为简单，只需将初始化完成的节点`node`插入以根节点`m_tnRoot`表示的完全二叉树即可。</span><br><span class="line">``` c++</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxHeap&lt;T&gt;::push(<span class="keyword">const</span> T&amp; value, <span class="keyword">const</span> <span class="keyword">int</span> priority)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 创建一个节点</span></span><br><span class="line">Element&lt;T&gt;* element = <span class="keyword">new</span> Element&lt;T&gt;(value, priority);</span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node = <span class="keyword">this</span>-&gt;createNode(); node-&gt;<span class="built_in">set</span>(element);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 无元素时，直接放在根节点，退出函数</span></span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) &#123; <span class="keyword">this</span>-&gt;m_tnRoot = node; <span class="keyword">return</span>; &#125;</span><br><span class="line"></span><br><span class="line">insert(node, &amp;<span class="keyword">this</span>-&gt;m_tnRoot);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于每次出队列操作时，都将释放二叉树的根节点，它的左右子树此时为两棵单独的完全二叉树，<strong>需重新组织两棵子树的节点组成一棵新的二叉树</strong>。以下采用的解决方案是<strong>将一棵树每个节点依次插入另一棵子树中</strong>，一些细节总结如下</p><ol><li>考虑到时间复杂度，<strong>选择结点数较多的子树的作为<code>trunk</code>分支，另一棵为<code>branch</code></strong>，将<code>branch</code>转化为队列，依次将各节点插入主分支<code>trunk</code>；</li><li><p>在插入<code>queue</code>队首节点时，需要将该节点的父、子节点置为<code>nullptr</code>，如果直接对<code>LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**&gt;*</code>队列弹出的二级指针<code>BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**</code>解引用后得到的节点进行空指针赋值，<strong>会使树节点丢失</strong>，所以先将节点解引用转化为<code>LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;*&gt;*</code>后进行操作；</p><blockquote><p>以下两句的差别，非常的<strong>迷惑</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node = *<span class="built_in">queue</span>-&gt;pop();<span class="comment">// 会造成节点丢失</span></span><br><span class="line">&gt; BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node = nodes-&gt;pop();</span><br><span class="line">&gt; node-&gt;parent = node-&gt;left = node-&gt;right = <span class="literal">nullptr</span>;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p><code>insert()</code>函数将<strong>自动组织调整节点间的关系</strong>；</p></li><li>注意<strong>根节点<code>m_tnRoot</code>的修改</strong>，不可遗漏。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">T MaxHeap&lt;T&gt;::pop()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) <span class="keyword">throw</span> <span class="string">"堆为空！"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 取出堆顶元素</span></span><br><span class="line">T value = <span class="keyword">this</span>-&gt;m_tnRoot-&gt;get()-&gt;getValue();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最后一个叶子节点</span></span><br><span class="line">LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**&gt;* <span class="built_in">queue</span> = MaxHeap&lt;T&gt;::tree2flattenQueue(&amp;<span class="keyword">this</span>-&gt;m_tnRoot);<span class="comment">// 调用继承的父类静态函数</span></span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node = *(<span class="built_in">queue</span>-&gt;back());</span><br><span class="line"><span class="keyword">delete</span> <span class="built_in">queue</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 覆盖根节点的值</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">this</span>-&gt;m_tnRoot-&gt;get();</span><br><span class="line"><span class="keyword">this</span>-&gt;m_tnRoot-&gt;<span class="built_in">set</span>(node-&gt;get());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 释放叶子节点</span></span><br><span class="line"><span class="keyword">if</span> (!node-&gt;isRoot()) &#123;</span><br><span class="line"><span class="keyword">if</span> (node-&gt;isLeft()) </span><br><span class="line">node-&gt;parent-&gt;left = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">node-&gt;parent-&gt;right = <span class="literal">nullptr</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>-&gt;m_tnRoot = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">delete</span> node;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调整堆顶元素数据</span></span><br><span class="line">node = <span class="keyword">this</span>-&gt;m_tnRoot;</span><br><span class="line"><span class="keyword">while</span> (node) &#123;</span><br><span class="line"><span class="keyword">if</span> (node-&gt;left &amp;&amp; node-&gt;get()-&gt;getPriority() &lt; node-&gt;left-&gt;get()-&gt;getPriority()) &#123;</span><br><span class="line"><span class="comment">// 交换节点的值</span></span><br><span class="line">Element&lt;T&gt;* temp = node-&gt;get(); node-&gt;<span class="built_in">set</span>(node-&gt;left-&gt;get()); node-&gt;left-&gt;<span class="built_in">set</span>(temp);</span><br><span class="line">node = node-&gt;left; <span class="keyword">continue</span>;</span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">if</span> (node-&gt;right &amp;&amp; node-&gt;get()-&gt;getPriority() &lt; node-&gt;right-&gt;get()-&gt;getPriority()) &#123;</span><br><span class="line"><span class="comment">// 交换节点的值</span></span><br><span class="line">Element&lt;T&gt;* temp = node-&gt;get(); node-&gt;<span class="built_in">set</span>(node-&gt;right-&gt;get()); node-&gt;right-&gt;<span class="built_in">set</span>(temp);</span><br><span class="line">node = node-&gt;right; <span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">T MaxHeap&lt;T&gt;::pop()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) <span class="keyword">throw</span> <span class="string">"堆为空！"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**************** 取出根节点与左右子树 ****************/</span></span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* toDel = <span class="keyword">this</span>-&gt;m_tnRoot;</span><br><span class="line">T value = toDel-&gt;get()-&gt;getValue();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 若根节点已经是叶节点，返回值并退出</span></span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>-&gt;m_tnRoot-&gt;isLeaf()) &#123;</span><br><span class="line"><span class="keyword">this</span>-&gt;m_tnRoot = <span class="literal">nullptr</span>;<span class="comment">// 注意根节点的变更</span></span><br><span class="line"><span class="keyword">delete</span> toDel; <span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* left  = <span class="keyword">this</span>-&gt;m_tnRoot-&gt;left ;</span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* right = <span class="keyword">this</span>-&gt;m_tnRoot-&gt;right;</span><br><span class="line"><span class="comment">/**************** 取出根节点与左右子树 ****************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/********************* 选择枝干 **********************/</span></span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* trunk = <span class="literal">nullptr</span>;</span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* branch = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!right) &#123;</span><br><span class="line">trunk = left;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 将节点多的作为主枝</span></span><br><span class="line"><span class="keyword">if</span> (LinkedBinaryTree&lt;Element&lt;T&gt;*&gt;::sizeofNode(left) &lt; \</span><br><span class="line">LinkedBinaryTree&lt;Element&lt;T&gt;*&gt;::sizeofNode(right)) &#123;</span><br><span class="line">trunk = right; branch = left;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">branch = right; trunk = left;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">trunk-&gt;parent = <span class="literal">nullptr</span>; </span><br><span class="line"><span class="keyword">this</span>-&gt;m_tnRoot = trunk;<span class="comment">// 注意根节点的变更</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 若只包含主枝干，则无需进行插入操作</span></span><br><span class="line"><span class="keyword">if</span> (!branch) &#123;</span><br><span class="line"><span class="keyword">delete</span> toDel; <span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/********************* 选择枝干 **********************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**************** 将branch插入master *****************/</span></span><br><span class="line"><span class="comment">// 创建节点链表，解引用后再插入节点，防止</span></span><br><span class="line">LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**&gt;* <span class="built_in">queue</span> = \</span><br><span class="line">LinkedBinaryTree&lt;Element&lt;T&gt;*&gt;::tree2flattenQueue(&amp;branch);</span><br><span class="line">LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;*&gt;* nodes = \</span><br><span class="line"><span class="keyword">new</span> LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;*&gt;;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">queue</span>-&gt;size() &gt; <span class="number">0</span>) nodes-&gt;push(*<span class="built_in">queue</span>-&gt;pop());</span><br><span class="line"><span class="keyword">delete</span> <span class="built_in">queue</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 依次插入</span></span><br><span class="line"><span class="keyword">while</span> (nodes-&gt;size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node = nodes-&gt;pop();</span><br><span class="line"><span class="comment">// BinaryTreeNode&lt;Element&lt;T&gt;*&gt;* node = *queue-&gt;pop();// 该操作会导致，后续置`nullptr`时节点丢失</span></span><br><span class="line"><span class="keyword">if</span> (!node) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">node-&gt;parent = node-&gt;left = node-&gt;right = <span class="literal">nullptr</span>;</span><br><span class="line">insert(node, &amp;<span class="keyword">this</span>-&gt;m_tnRoot);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">delete</span> nodes;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 释放节点，返回值</span></span><br><span class="line"><span class="keyword">delete</span> toDel; <span class="keyword">return</span> value;</span><br><span class="line"><span class="comment">/**************** 将branch插入master *****************/</span></span><br><span class="line">&#125;</span><br><span class="line">``` --&gt;</span><br><span class="line"></span><br><span class="line">打印函数如下</span><br><span class="line">``` c++</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxHeap&lt;T&gt;::print()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据优先级打印堆</span></span><br><span class="line">LinkedQueue&lt;BinaryTreeNode&lt;Element&lt;T&gt;*&gt;**&gt;* <span class="built_in">queue</span> = <span class="keyword">this</span>-&gt;tree2flattenQueue(&amp;<span class="keyword">this</span>-&gt;m_tnRoot);</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">queue</span>-&gt;size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">Element&lt;T&gt;* element = (*(<span class="built_in">queue</span>-&gt;pop()))-&gt;get();</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">'['</span> &lt;&lt; element-&gt;getPriority() &lt;&lt; <span class="string">']'</span> \</span><br><span class="line">&lt;&lt; element-&gt;getValue() &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="图解算法"><a href="#图解算法" class="headerlink" title="图解算法"></a>图解算法</h3><p>假设有以下$6$名同学，参加某活动时，主办方给他们分配了不同的优先级，如下表</p><div class="table-container"><table><thead><tr><th style="text-align:center">编号</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td style="text-align:center">姓名</td><td>刘一</td><td>陈二</td><td>张三</td><td>李四</td><td>王五</td><td>赵六</td></tr><tr><td style="text-align:center">优先级</td><td>14</td><td>15</td><td>10</td><td>21</td><td>20</td><td>2</td></tr></tbody></table></div><p>根据编号顺序依次进入某准备室，进入后根据优先级进行排列，优先级越大的先参加活动。下图中，每个节点内的三个小圆点分别表示父节点、左子树节点、右子树节点。</p><p><img src="/【数据结构】优先级队列/students.jpg" alt="students"></p><p>主函数如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> P[<span class="number">6</span>] = &#123;<span class="number">14</span>, <span class="number">15</span>, <span class="number">10</span>, <span class="number">21</span>, <span class="number">20</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="built_in">string</span> N[<span class="number">6</span>] = &#123;<span class="string">"刘一"</span>, <span class="string">"陈二"</span>, <span class="string">"张三"</span>, <span class="string">"李四"</span>, <span class="string">"王五"</span>, <span class="string">"赵六"</span>&#125;;</span><br><span class="line">MaxHeap&lt;<span class="built_in">string</span>&gt; maxHeap;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"开始插入。。。"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line">maxHeap.push(N[i], P[i]);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"当前树结构: "</span>;</span><br><span class="line">maxHeap.print();</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"当前优先级最高的节点: ["</span> &lt;&lt; maxHeap.maxPriority() &lt;&lt; <span class="string">']'</span> &lt;&lt; maxHeap.top() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"开始删除。。。"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; i++) &#123;</span><br><span class="line"><span class="built_in">string</span> value = maxHeap.pop();</span><br><span class="line"><span class="keyword">if</span> (maxHeap.empty()) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"当前树结构: "</span>;</span><br><span class="line">maxHeap.print();</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"当前优先级最高的节点: ["</span> &lt;&lt; maxHeap.maxPriority() &lt;&lt; <span class="string">']'</span> &lt;&lt; maxHeap.top() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>单个节点的插入，以李四同学为例，图中黑色表示更改的部分<br><img src="/【数据结构】优先级队列/students_push_node.jpg" alt="students_push_node"></p><p>所以插入过程，二叉树结构调整如下，图中黑色节点表示插入的节点<br><img src="/【数据结构】优先级队列/students_push.jpg" alt="students_push"></p><p>单个节点的删除，以陈二同学为例，图中黑色表示更改的部分<br><img src="/【数据结构】优先级队列/students_pop_node.jpg" alt="students_pop_node"></p><p>删除过程，二叉树结构调整如下，图中第$i(i&gt;1)$棵树的根节点在第$(i-1)$棵树中的位置已用相同颜色注明，黑色节点表示删除的节点<br><img src="/【数据结构】优先级队列/students_pop.jpg" alt="students_pop"></p><p>主函数输出结果如下，正确<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">开始插入。。。</span><br><span class="line">当前树结构: [14]刘一</span><br><span class="line">当前优先级最高的节点: [14]刘一</span><br><span class="line">当前树结构: [15]陈二 [14]刘一</span><br><span class="line">当前优先级最高的节点: [15]陈二</span><br><span class="line">当前树结构: [15]陈二 [14]刘一 [10]张三</span><br><span class="line">当前优先级最高的节点: [15]陈二</span><br><span class="line">当前树结构: [21]李四 [15]陈二 [10]张三 [14]刘一</span><br><span class="line">当前优先级最高的节点: [21]李四</span><br><span class="line">当前树结构: [21]李四 [20]王五 [10]张三 [14]刘一 [15]陈二</span><br><span class="line">当前优先级最高的节点: [21]李四</span><br><span class="line">当前树结构: [21]李四 [20]王五 [10]张三 [14]刘一 [15]陈二 [2]赵六</span><br><span class="line">当前优先级最高的节点: [21]李四</span><br><span class="line"></span><br><span class="line">开始删除。。。</span><br><span class="line">当前树结构: [20]王五 [14]刘一 [15]陈二 [10]张三 [2]赵六</span><br><span class="line">当前优先级最高的节点: [20]王五</span><br><span class="line">当前树结构: [15]陈二 [14]刘一 [2]赵六 [10]张三</span><br><span class="line">当前优先级最高的节点: [15]陈二</span><br><span class="line">当前树结构: [14]刘一 [10]张三 [2]赵六</span><br><span class="line">当前优先级最高的节点: [14]刘一</span><br><span class="line">当前树结构: [10]张三 [2]赵六</span><br><span class="line">当前优先级最高的节点: [10]张三</span><br><span class="line">当前树结构: [2]赵六</span><br><span class="line">当前优先级最高的节点: [2]赵六</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure></p>-->]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】二叉树及其他树</title>
      <link href="/2020/02/29/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%BA%8C%E5%8F%89%E6%A0%91%E5%8F%8A%E5%85%B6%E4%BB%96%E6%A0%91/"/>
      <url>/2020/02/29/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%BA%8C%E5%8F%89%E6%A0%91%E5%8F%8A%E5%85%B6%E4%BB%96%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h1 id="树"><a href="#树" class="headerlink" title="树"></a>树</h1><p><strong>定义</strong>：一棵树$t$是一个非空的有限元素的集合，其中一个元素为<strong>根</strong>(root)，其余元素组成$t$的子树。根据树的形态，有如下术语：<strong>孩子</strong>(child)、<strong>父母</strong>(parent)、<strong>兄弟</strong>(sibling)、<strong>孙子</strong>(grandchild)、<strong>祖父</strong>(grandparent)、<strong>祖先</strong>(ancestor)、<strong>后代</strong>(descendent)等等。树中没有孩子的元素称作<strong>叶子</strong>(leaf)。</p><p>树的另一常用术语为<strong>级</strong>(level)。树根为$1$级，其孩子为$2$级，以此类推。一棵树的<strong>高度</strong>(height)或<strong>深度</strong>(depth)是树的级个数。<strong>一个元素的度</strong>(degree of an element)是指其孩子的个数，叶节点的度为$0$。<strong>一棵树的度</strong>(degree of a tree)是其元素的度的最大值。</p><p><img src="/2020/02/29/【数据结构】二叉树及其他树/trees.jpg" alt="trees"></p><h1 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h1><h2 id="定义及概念"><a href="#定义及概念" class="headerlink" title="定义及概念"></a>定义及概念</h2><p><strong>定义</strong>：二叉树(binary tree)是树的一种，其中一个元素为根，其余元素被划分为两棵二叉树，分别称<strong>左子树</strong>和<strong>右子树</strong>。二叉树和树的根本区别是：</p><ul><li>二叉树每个元素恰好有两棵子树(其中存在空树)，而树的元素孩子数目不定；</li><li>二叉树的子树是有序的；</li><li>二叉树可以为空，而树不可以。</li></ul><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p><strong>特性(1)</strong>：一棵二叉树有$n(n&gt;0)$个元素，他有$(n-1)$条边。</p><blockquote><p>除根节点外，其余元素有且仅有一个父节点，其间对应一条边。</p></blockquote><p><strong>特性(2)</strong>：一棵二叉树的高度为$h(h \geq 0)$，它最少有$h$个元素，最多有$2^h-1$个元素</p><blockquote><ul><li>二叉树的第$1$级包含$1$个元素；</li><li>第$i(i \geq 2)$级最少有$1$个元素，$\Sigma_{i=1}^h 1 = h$；</li><li>第$i(i \geq 2)$级最多有$2^{(i-1)}$个元素，则$\Sigma_{i=2}^h 2^{(i-1)} + 1 = 2^h - 1$。</li></ul></blockquote><p><strong>特性(3)</strong>：一棵二叉树有$n(n&gt;0)$个元素，它的最大高度为$n$，最小高度为$\log_2(n+1)$。</p><p><strong>定义(满二叉树)</strong>：高度为$h$的二叉树恰好有$2^h-1$个元素称为满二叉树(full binary tree)。<br><img src="/2020/02/29/【数据结构】二叉树及其他树/full_binary_tree.jpg" alt="full_binary_tree"></p><p><strong>定义(完全二叉树)</strong>：高度为$h$的满二叉树，其元素从第$1$级到第$h$级，<strong>每一级从左至右按顺序从$1$到$2^{h-1}$编号</strong>，并且删除$k$个编号为$(2^h-i)$的元素，其中$1 \leq i \leq k &lt; 2^h$，剩余元素组成的树称为完全二叉树(complete binary tree)。<br><img src="/2020/02/29/【数据结构】二叉树及其他树/complete_binary_tree_1.jpg" alt="complete_binary_tree_1"><br><img src="/2020/02/29/【数据结构】二叉树及其他树/complete_binary_tree_2.jpg" alt="complete_binary_tree_2"></p><p><strong>特性</strong>： 设完全二叉树的一元素其编号为$i$，$1 \leq i \leq n$，则<br>1) 若$i=1$，该元素为根；否则其父节点编号为$\lfloor n/2 \rfloor$；<br>2) 若$2i &gt; n$，则该元素无左孩子；否则其左孩子的编号为$2i$；<br>3) 若$2i+1 &gt; n$，则该元素无右孩子；否则其右孩子的编号为$2i+1$；</p><h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><h3 id="数组描述"><a href="#数组描述" class="headerlink" title="数组描述"></a>数组描述</h3><p>用数组描述二叉树时，将二叉树表示为缺少了部分元素的完全二叉树，利用完全二叉树的特性对孩子进行索引。当根节点意外的每个节点都是其父节点的右孩子时，所需存储空间最大，称为<strong>右斜二叉树</strong>(right-skewed binary tree)。</p><ol><li><p>不完全二叉树<br> <img src="/2020/02/29/【数据结构】二叉树及其他树/array_description.jpg" alt="array_description"></p></li><li><p>右斜二叉树<br> <img src="/2020/02/29/【数据结构】二叉树及其他树/array_description_right_skewed.jpg" alt="array_description_right_skewed"></p></li></ol><h3 id="链表描述"><a href="#链表描述" class="headerlink" title="链表描述"></a>链表描述</h3><p>用链表描述二叉树时，除元素数据存储外，每个元素描述为包含两个子节点<code>leftChild</code>与<code>rightChild</code>的节点，若某孩子节点为空则置为<code>NULL</code>。</p><ol><li><p>不完全二叉树<br> <img src="/2020/02/29/【数据结构】二叉树及其他树/chain_description.jpg" alt="chain_description"></p></li><li><p>右斜二叉树<br> <img src="/2020/02/29/【数据结构】二叉树及其他树/chain_description_right_skewed.jpg" alt="chain_description_right_skewed"></p></li></ol><h3 id="抽象数据描述"><a href="#抽象数据描述" class="headerlink" title="抽象数据描述"></a>抽象数据描述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">抽象数据类型 binaryTree</span><br><span class="line">&#123;</span><br><span class="line">实例</span><br><span class="line">    元素集合；左子树、右子树；</span><br><span class="line">操作</span><br><span class="line">    empty() 若树为空返回true否则false</span><br><span class="line">    size()  返回二叉树的节点/元素个数</span><br><span class="line">    preOrder(visit)     前序遍历，visit为二叉树节点操作函数</span><br><span class="line">    inOrder(visit)      中序遍历</span><br><span class="line">    postOrder(visit)    后序遍历</span><br><span class="line">    levelOrder(visit)   层次遍历</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><h3 id="链表描述-1"><a href="#链表描述-1" class="headerlink" title="链表描述"></a>链表描述</h3><h4 id="子节点的声明与定义"><a href="#子节点的声明与定义" class="headerlink" title="子节点的声明与定义"></a>子节点的声明与定义</h4><p>首先，定义二叉树节点如下</p><ul><li>包含三个<code>BinaryTreeNode*</code>类型的公有成员指针变量，保存结点地址信息：父节点、左子树节点、右子树节点；</li><li>私有成员指针变量<code>T*</code>存放节点元素值；</li><li>添加成员函数<code>isRoot()</code>与<code>isLeaf()</code>，用于返回节点部分信息。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinaryTreeNode</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="comment">// 左右子树的维护，放在二叉树`LinkedBinaryTree`中处理</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">BinaryTreeNode(BinaryTreeNode&lt;T&gt;* p = <span class="literal">nullptr</span>) : parent(p) &#123;</span><br><span class="line">left  = right = <span class="literal">nullptr</span>;  value = <span class="keyword">new</span> T; &#125;</span><br><span class="line">~BinaryTreeNode() &#123; <span class="keyword">delete</span> value; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isRoot</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> !parent; &#125;<span class="comment">// 是否为根节点</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isLeaf</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> (!left) &amp;&amp; (!right); &#125;<span class="comment">// 是否为叶子节点</span></span><br><span class="line"></span><br><span class="line"><span class="function">T <span class="title">get</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> *value; &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">const</span> T&amp; v)</span> </span>&#123; *value = v; &#125;</span><br><span class="line"></span><br><span class="line">BinaryTreeNode* parent;<span class="comment">// 增加父母节点信息</span></span><br><span class="line">BinaryTreeNode* left, *right;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">T* value;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="树的声明"><a href="#树的声明" class="headerlink" title="树的声明"></a>树的声明</h4><p>以下采用链表描述实现二叉树</p><ul><li>私有指针变量<code>m_tnRoot</code>存放根节点的地址；</li><li>重载构造函数，进行根节点的初始化或树的创建；析构函数中做节点内存释放的清理工作；</li><li>共有成员函数<code>size()</code>,<code>height()</code>,<code>empty()</code>,<code>print()</code>用以获取树的信息；</li><li><code>[*]Order</code>以不同的遍历顺序访问和修改节点；</li><li>私有静态成员函数<code>[*]Node</code>为单个节点的访问和修改函数，供<code>[*]Order</code>调用；</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkedBinaryTree</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">LinkedBinaryTree();</span><br><span class="line">LinkedBinaryTree(T*, <span class="keyword">int</span>);<span class="comment">// 由给定集合生成二叉树，空叶节点用`#`表示，形如`&#123;3,9,20,#,#,15,7&#125;`</span></span><br><span class="line">LinkedBinaryTree(<span class="keyword">const</span> LinkedBinaryTree&lt;T&gt;&amp;);</span><br><span class="line">~LinkedBinaryTree();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span></span>;<span class="comment">// 二叉树结点数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">height</span><span class="params">()</span></span>;<span class="comment">// 二叉树高度</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span></span>;<span class="comment">// 二叉树是否为空</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">int</span> mode = <span class="number">3</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">()</span></span>;<span class="comment">// 交换二叉树的左右子节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ------------------ 深度优先搜索 ------------------ </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">preOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node = m_tnRoot)</span></span>;<span class="comment">// 前序遍历</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">inOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node = m_tnRoot)</span></span>;<span class="comment">// 中序遍历</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">postOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node = m_tnRoot)</span></span>;<span class="comment">// 后序遍历</span></span><br><span class="line"><span class="comment">// ------------------ 广度优先搜索 ------------------ </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">levelOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt; * node), BinaryTreeNode&lt;T&gt; * node = m_tnRoot)</span></span>;<span class="comment">// 层次遍历</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="keyword">static</span> LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* tree2flattenQueue(BinaryTreeNode&lt;T&gt;** ptr);</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 节点操作，注意需要定义为static成员函数，才能取址供遍历函数调用</span></span><br><span class="line">    <span class="keyword">static</span> BinaryTreeNode&lt;T&gt;* createNode(BinaryTreeNode&lt;T&gt;* p = <span class="literal">nullptr</span>);</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteNode</span><span class="params">(BinaryTreeNode&lt;T&gt;* node)</span></span>;</span><br><span class="line">BinaryTreeNode&lt;T&gt;* m_tnRoot;<span class="comment">// 根节点</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">printNode</span><span class="params">(BinaryTreeNode&lt;T&gt;* node)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swapNode</span><span class="params">(BinaryTreeNode&lt;T&gt;* node)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">int</span>  <span class="title">sizeofNode</span><span class="params">(BinaryTreeNode&lt;T&gt;* node)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">heightofNode</span><span class="params">(BinaryTreeNode&lt;T&gt;* node)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="树的遍历方法"><a href="#树的遍历方法" class="headerlink" title="树的遍历方法"></a>树的遍历方法</h4><p><strong>定义</strong>：二叉树的<strong>遍历</strong>(traversal)中，每个元素仅被访问一次。有四种常用的遍历方法<br>1) 前序遍历；<br>2) 中序遍历；<br>3) 后序遍历；<br>4) 层次遍历。</p><p>其中，前三种为<strong>深度优先探索方法</strong>，最后一种为<strong>广度优先搜索方法</strong>。</p><ol><li><p>前序遍历<br> 前序遍历是指在访问某节点<code>node</code>时，<strong>先对该节点进行值的访问或修改，再依次对其左子树、右子树进行同样的嵌套操作</strong>。在<a href="#%e6%a0%91%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%8e%e5%a3%b0%e6%98%8e">树的定义与声明</a>成员函数<code>preOrder</code>定义如下</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::preOrder(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>; </span><br><span class="line">    visit(node); </span><br><span class="line">    preOrder(visit, node-&gt;left); </span><br><span class="line">    preOrder(visit, node-&gt;right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>中序遍历<br> 中序遍历是指在访问某节点<code>node</code>时，<strong>对其元素的访问或修改放在左子树、右子树的嵌套操作之间</strong>。在<a href="#%e6%a0%91%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%8e%e5%a3%b0%e6%98%8e">树的定义与声明</a>成员函数<code>inOrder</code>定义如下</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::inOrder(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>; </span><br><span class="line">    inOrder(visit, node-&gt;left); </span><br><span class="line">    visit(node); </span><br><span class="line">    inOrder(visit, node-&gt;right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>后序遍历<br> 后序遍历是指在访问某节点<code>node</code>时，<strong>对其元素的访问或修改放在左子树、右子树的嵌套操作之后</strong>。考虑到后续遍历与前序遍历的不同，其<strong>左右子节点的访问也倒序访问</strong>，在<a href="#%e6%a0%91%e7%9a%84%e5%ae%9a%e4%b9%89%e4%b8%8e%e5%a3%b0%e6%98%8e">树的定义与声明</a>成员函数<code>postOrder</code>定义如下</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::postOrder(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>; </span><br><span class="line">    postOrder(visit, node-&gt;right);</span><br><span class="line">    postOrder(visit, node-&gt;left); </span><br><span class="line">    visit(node);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>层次遍历<br> 层次遍历是一种广度优先搜索方法，需要某层的节点<strong>按完全二叉树的顺序</strong>依次访问。</p><ol><li><p>将二叉树非空节点按顺序存入队列<code>LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* ordered</code>中，具体过程为</p><ol><li>初始化队列<code>queue</code>，将树/子树的根节点存入；</li><li>初始化队列<code>ordered</code>，暂时为空；</li><li>从<code>queue</code>中弹出一个节点作为父节点，将此节点存入<code>ordered</code>，并将其<strong>非空左右子节点</strong>存入<code>queue</code>作为后续的父节点，即“<strong>出一进二</strong>”，不断循环直至<code>queue</code>为空；</li><li><p>返回节点<code>ordered</code>。</p><p>值得注意的是，<code>tree2flattenQueue</code>函数需要传入父节点的地址<code>&amp;root</code>。</p></li></ol></li><li><p>由队列特性，按先入先出的顺序，依次对<code>ordered</code>的元素进行访问。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::levelOrder(<span class="keyword">void</span> (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>;</span><br><span class="line">    LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* ordered = tree2flattenQueue(&amp;node);</span><br><span class="line">    <span class="keyword">while</span> (ordered-&gt;size() &gt; <span class="number">0</span>) &#123; </span><br><span class="line">        BinaryTreeNode&lt;T&gt;** n = ordered-&gt;pop();</span><br><span class="line">        visit(*n); </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> ordered;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* LinkedBinaryTree&lt;T&gt;::tree2flattenQueue(BinaryTreeNode&lt;T&gt;** ptr) &#123;</span><br><span class="line">    <span class="comment">// - 需要获取`m_tnRoot`的地址，不可定义形参`BinaryTreeNode&lt;T&gt;* node = m_tnRoot`再将`&amp;node`存储，因为函数返回后，形参`node`将被释放</span></span><br><span class="line">    <span class="comment">// - 注意这里要定义二级指针，否则在出队列时，会将节点破坏</span></span><br><span class="line">    LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* <span class="built_in">queue</span> = <span class="keyword">new</span> LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;;<span class="comment">// 中间过程使用的队列</span></span><br><span class="line">    LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* ordered = <span class="keyword">new</span> LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;;<span class="comment">// 已排序的队列</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">queue</span>-&gt;push(ptr);</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">queue</span>-&gt;size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        BinaryTreeNode&lt;T&gt;** p = <span class="built_in">queue</span>-&gt;pop();<span class="comment">// 弹出队首元素</span></span><br><span class="line">        ordered-&gt;push(p);<span class="comment">// 将该节点存入已排序队列</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将子节点存入队列</span></span><br><span class="line">        <span class="keyword">if</span> ((*p)-&gt;left) <span class="built_in">queue</span>-&gt;push(&amp;((*p)-&gt;left));</span><br><span class="line">        <span class="keyword">if</span> ((*p)-&gt;right) <span class="built_in">queue</span>-&gt;push(&amp;((*p)-&gt;right));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> <span class="built_in">queue</span>;</span><br><span class="line">    <span class="keyword">return</span> ordered;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol></li></ol><h4 id="树的函数定义"><a href="#树的函数定义" class="headerlink" title="树的函数定义"></a>树的函数定义</h4><ol><li><p>构造函数、复制构造函数、析构函数</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">LinkedBinaryTree&lt;T&gt;::LinkedBinaryTree() </span><br><span class="line">&#123; </span><br><span class="line">    m_tnRoot = <span class="literal">nullptr</span>; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">LinkedBinaryTree&lt;T&gt;::~LinkedBinaryTree() </span><br><span class="line">&#123; </span><br><span class="line">    postOrder(&amp;deleteNode, m_tnRoot); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 重载了构造函数，可传入数组变量进行树的构造，各元素按完全二叉树的编号顺序进行输入，若节点为空，数组中置为字符<code>&#39;#&#39;</code>。在节点的输入时，参考层级遍历的方法，借助队列实现。</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">LinkedBinaryTree&lt;T&gt;::LinkedBinaryTree(T* <span class="built_in">set</span>, <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 初始化根节点</span></span><br><span class="line">    m_tnRoot = createNode(); m_tnRoot-&gt;<span class="built_in">set</span>(<span class="built_in">set</span>[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化队列</span></span><br><span class="line">    LinkedQueue&lt;BinaryTreeNode&lt;T&gt;*&gt; queueParent;<span class="comment">// 中间过程使用的队列</span></span><br><span class="line">    queueParent.push(m_tnRoot);<span class="comment">// 放入根节点</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (queueParent.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        BinaryTreeNode&lt;T&gt;* parent = queueParent.pop();        <span class="comment">// 弹出父节点</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// ------------- 左子树 -------------</span></span><br><span class="line">        <span class="keyword">if</span> ((++cnt) &gt;= n) <span class="keyword">break</span>;</span><br><span class="line">        T val = <span class="built_in">set</span>[cnt];</span><br><span class="line">        <span class="keyword">if</span> (val != (T)<span class="string">'#'</span>) &#123;</span><br><span class="line">            BinaryTreeNode&lt;T&gt;* left = createNode(parent);<span class="comment">// 创建左子树</span></span><br><span class="line">            left-&gt;<span class="built_in">set</span>(val);<span class="comment">// 赋值</span></span><br><span class="line">            parent-&gt;left = left;        <span class="comment">// 子节点链接到父节点</span></span><br><span class="line">            queueParent.push(left);        <span class="comment">// 置入队列</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// ------------- 右子树 -------------</span></span><br><span class="line">        <span class="keyword">if</span> ((++cnt) &gt;= n) <span class="keyword">break</span>;</span><br><span class="line">        val = <span class="built_in">set</span>[cnt];</span><br><span class="line">        <span class="keyword">if</span> (val != (T)<span class="string">'#'</span>) &#123;</span><br><span class="line">            BinaryTreeNode&lt;T&gt;* right = createNode(parent);<span class="comment">// 创建右子树</span></span><br><span class="line">            right-&gt;<span class="built_in">set</span>(val);<span class="comment">// 赋值</span></span><br><span class="line">            parent-&gt;right = right;<span class="comment">// 子节点链接到父节点</span></span><br><span class="line">            queueParent.push(right);<span class="comment">// 置入队列</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 复制构造函数主要实现树间的<strong>深拷贝</strong>，即必须保证两棵树没有内存共享的情况下值完全一致，有以下关键点</p><ul><li>节点的创建，即开辟新的内存并赋值；</li><li><p>节点间的关系，由<code>parent</code>、<code>left</code>、<code>right</code>三个指针共同维护一棵树的结构，因此只需各个节点的指针信息正确对应所指节点，将父节点为空的节点作为树的根节点，一棵树就能“<strong>立起来</strong>”。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">LinkedBinaryTree&lt;T&gt;::LinkedBinaryTree(<span class="keyword">const</span> LinkedBinaryTree&lt;T&gt;&amp; tree)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 初始化待拷贝数组</span></span><br><span class="line">    LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* queuePtrFrom = tree2flattenQueue(<span class="keyword">const_cast</span>&lt;BinaryTreeNode&lt;T&gt;**&gt;(&amp;tree.m_tnRoot));    <span class="comment">// 取消`const`特性</span></span><br><span class="line">    LinkedQueue&lt; BinaryTreeNode&lt;T&gt;*&gt;* queueFrom = <span class="keyword">new</span> LinkedQueue&lt; BinaryTreeNode&lt;T&gt;*&gt;;</span><br><span class="line">    <span class="keyword">while</span> (queuePtrFrom-&gt;size() &gt; <span class="number">0</span>) &#123; queueFrom-&gt;push(*(queuePtrFrom-&gt;pop())); &#125;</span><br><span class="line">    <span class="keyword">delete</span> queuePtrFrom;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 更改类型为列表，便于查找，实际上不修改可也</span></span><br><span class="line">    ChainList&lt;BinaryTreeNode&lt;T&gt;*&gt;* listFrom = queueFrom;</span><br><span class="line">    ChainList&lt;BinaryTreeNode&lt;T&gt;*&gt;* listTo = <span class="keyword">new</span> ChainList&lt;BinaryTreeNode&lt;T&gt;*&gt;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 复制数组的值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; listFrom-&gt;size(); i++) &#123;</span><br><span class="line">        BinaryTreeNode&lt;T&gt;* nodeFrom = listFrom-&gt;get(i);</span><br><span class="line">        BinaryTreeNode&lt;T&gt;* nodeTo = <span class="keyword">new</span> BinaryTreeNode &lt;T&gt;;<span class="comment">// 创建节点</span></span><br><span class="line">        nodeTo-&gt;<span class="built_in">set</span>(nodeFrom-&gt;get());<span class="comment">// 复制值</span></span><br><span class="line">        listTo-&gt;insert(listTo-&gt;size(), nodeTo);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 链接节点间关系</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; listFrom-&gt;size(); i++) &#123;</span><br><span class="line">        BinaryTreeNode&lt;T&gt;* nodeFrom = listFrom-&gt;get(i);</span><br><span class="line">        BinaryTreeNode&lt;T&gt;* nodeTo   = listTo  -&gt;get(i);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">-1</span>;</span><br><span class="line">        <span class="comment">// 父节点</span></span><br><span class="line">        <span class="keyword">if</span> (nodeFrom-&gt;parent) &#123;</span><br><span class="line">            index = listFrom-&gt;indexOf(nodeFrom-&gt;parent);</span><br><span class="line">            nodeTo-&gt;parent = listTo-&gt;get(index);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            m_tnRoot = nodeTo;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 左右子树节点</span></span><br><span class="line">        <span class="keyword">if</span> (nodeFrom-&gt;left) &#123;</span><br><span class="line">            index = listFrom-&gt;indexOf(nodeFrom-&gt;left);</span><br><span class="line">            nodeTo-&gt;left = listTo-&gt;get(index);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (nodeFrom-&gt;right) &#123;</span><br><span class="line">            index = listFrom-&gt;indexOf(nodeFrom-&gt;right);</span><br><span class="line">            nodeTo-&gt;right = listTo-&gt;get(index);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> listFrom; <span class="keyword">delete</span> listTo; <span class="comment">// 回收内存</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>树的信息</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">int</span> LinkedBinaryTree&lt;T&gt;::size() </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">return</span> sizeofNode(m_tnRoot); </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">int</span>  LinkedBinaryTree&lt;T&gt;::sizeofNode(BinaryTreeNode&lt;T&gt;* node) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 采用后续遍历</span></span><br><span class="line">    <span class="keyword">int</span> sizeL = sizeofNode(node-&gt;left);</span><br><span class="line">    <span class="keyword">int</span> sizeR = sizeofNode(node-&gt;right);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> + sizeL + sizeR;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">int</span> LinkedBinaryTree&lt;T&gt;::height() </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">return</span> heightofNode(m_tnRoot); </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">int</span> LinkedBinaryTree&lt;T&gt;::heightofNode(BinaryTreeNode&lt;T&gt;* node) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 采用后续遍历</span></span><br><span class="line">    <span class="keyword">int</span> heightL = heightofNode(node-&gt;left);</span><br><span class="line">    <span class="keyword">int</span> heightR = heightofNode(node-&gt;right);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> + (heightL &gt; heightR ? heightL : heightR);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">bool</span> LinkedBinaryTree&lt;T&gt;::empty() </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">return</span> size() == <span class="number">0</span>; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::print(<span class="keyword">int</span> mode) &#123;</span><br><span class="line">    <span class="keyword">switch</span> (mode)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"preOrder: "</span>;</span><br><span class="line">        preOrder(&amp;printNode, m_tnRoot);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"inOrder: "</span>;</span><br><span class="line">        inOrder(&amp;printNode, m_tnRoot);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"postOrder: "</span>;</span><br><span class="line">        postOrder(&amp;printNode, m_tnRoot);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"levelOrder: "</span>;</span><br><span class="line">        levelOrder(&amp;printNode, m_tnRoot);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>树的操作<br> 交换每个节点的左右子树</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::swap()</span><br><span class="line">&#123;</span><br><span class="line">    preOrder(&amp;swapNode, m_tnRoot);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>节点操作与访问</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">BinaryTreeNode&lt;T&gt;* LinkedBinaryTree&lt;T&gt;::createNode(BinaryTreeNode&lt;T&gt;* p) </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BinaryTreeNode&lt;T&gt;(p); </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::deleteNode(BinaryTreeNode&lt;T&gt;* node) </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>; </span><br><span class="line">    <span class="keyword">delete</span> node; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::printNode(BinaryTreeNode&lt;T&gt;* node) </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>; </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; node-&gt;get() &lt;&lt; <span class="string">" "</span>; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> LinkedBinaryTree&lt;T&gt;::swapNode(BinaryTreeNode&lt;T&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span>;</span><br><span class="line">    BinaryTreeNode&lt;T&gt;* temp = node-&gt;left;</span><br><span class="line">    node-&gt;left = node-&gt;right;</span><br><span class="line">    node-&gt;right = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="数组描述-1"><a href="#数组描述-1" class="headerlink" title="数组描述"></a>数组描述</h3><p>数组描述比较简单，只需将节点按照完全二叉树进行标号，正确计算节点间的索引关系即可，访问元素更为便捷并节省空间。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArrayBinaryTree</span>:</span> <span class="keyword">public</span> ArrayList&lt;T&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">ArrayBinaryTree() :ArrayList&lt;T&gt;() &#123;&#125;</span><br><span class="line">ArrayBinaryTree(T* <span class="built_in">set</span>, <span class="keyword">int</span> n) &#123;<span class="comment">// 由给定集合生成二叉树，空叶节点用`#`表示，形如`&#123;3,9,20,#,#,15,7&#125;`</span></span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements = <span class="keyword">new</span> T[n];</span><br><span class="line"><span class="built_in">std</span>::copy(<span class="built_in">set</span>, <span class="built_in">set</span> + n, <span class="keyword">this</span>-&gt;m_TElements);</span><br><span class="line"><span class="keyword">this</span>-&gt;m_iCount = <span class="keyword">this</span>-&gt;m_iSize = n;</span><br><span class="line">&#125;</span><br><span class="line">ArrayBinaryTree(<span class="keyword">const</span> ArrayBinaryTree&lt;T&gt;&amp; tree) &#123;</span><br><span class="line"><span class="keyword">int</span> n = tree.size();</span><br><span class="line"><span class="keyword">this</span>-&gt;m_TElements = <span class="keyword">new</span> T[n];</span><br><span class="line"><span class="built_in">std</span>::copy(tree.m_TElements, tree.m_TElements + n, <span class="keyword">this</span>-&gt;m_TElements);</span><br><span class="line"><span class="keyword">this</span>-&gt;m_iCount = <span class="keyword">this</span>-&gt;m_iSize = n;</span><br><span class="line">&#125;</span><br><span class="line">~ArrayBinaryTree() &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> sizeOfNode(<span class="number">0</span>); &#125;<span class="comment">// 二叉树结点数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">height</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> heightofNode(<span class="number">0</span>); &#125;<span class="comment">// 二叉树高度</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> size() == <span class="number">0</span>; &#125;<span class="comment">// 二叉树是否为空</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> H = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;m_iCount; i++) &#123;</span><br><span class="line">                <span class="keyword">int</span> h = <span class="built_in">std</span>::<span class="built_in">floor</span>(<span class="built_in">std</span>::log2(i + <span class="number">1</span>));</span><br><span class="line">                <span class="keyword">if</span> (h &gt; H) &#123;</span><br><span class="line">                    H = h;</span><br><span class="line">                    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="keyword">this</span>-&gt;m_TElements[i] &lt;&lt; <span class="string">' '</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">// ------------------ 深度优先搜索 ------------------ </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">preOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(<span class="keyword">int</span> node), <span class="keyword">int</span> node = <span class="number">0</span>)</span> </span>&#123;<span class="comment">// 前序遍历</span></span><br><span class="line"><span class="keyword">if</span> (!checkNode(node)) <span class="keyword">return</span>; visit(node); preOrder(visit, left(node)); preOrder(visit, right(node)); &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">inOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(<span class="keyword">int</span> node), <span class="keyword">int</span> node = <span class="number">0</span>)</span> </span>&#123;<span class="comment">// 中序遍历</span></span><br><span class="line"><span class="keyword">if</span> (!checkNode(node)) <span class="keyword">return</span>; inOrder(visit, left(node)); visit(node); inOrder(visit, right(node)); &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">postOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(<span class="keyword">int</span> node), <span class="keyword">int</span> node = <span class="number">0</span>)</span> </span>&#123;<span class="comment">// 后序遍历</span></span><br><span class="line"><span class="keyword">if</span> (!checkNode(node)) <span class="keyword">return</span>; postOrder(visit, left(node)); postOrder(visit, right(node)); visit(node); &#125;</span><br><span class="line"><span class="comment">// ------------------ 广度优先搜索 ------------------ </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">levelOrder</span><span class="params">(<span class="keyword">void</span> (*visit)(<span class="keyword">int</span> node), <span class="keyword">int</span> node = <span class="number">0</span>)</span> </span>&#123;<span class="comment">// 层次遍历</span></span><br><span class="line"><span class="keyword">if</span> (!checkNode(node)) <span class="keyword">return</span>; <span class="keyword">for</span> (<span class="keyword">int</span> i = node; i &lt; <span class="keyword">this</span>-&gt;m_iCount; i++) visit(i); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">checkNode</span><span class="params">(<span class="keyword">int</span> node)</span> </span>&#123; </span><br><span class="line"><span class="keyword">if</span> (node &lt; <span class="number">0</span> || node &gt;= <span class="keyword">this</span>-&gt;m_iCount) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>-&gt;m_TElements[node] == (T)<span class="string">'#'</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">parent</span><span class="params">(<span class="keyword">int</span> node)</span> </span>&#123; <span class="keyword">return</span> (node - <span class="number">1</span>) / <span class="number">2</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">left</span><span class="params">(<span class="keyword">int</span> node)</span> </span>&#123; <span class="keyword">return</span> <span class="number">2</span> * (node + <span class="number">1</span>) - <span class="number">1</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">right</span><span class="params">(<span class="keyword">int</span> node)</span> </span>&#123; <span class="keyword">return</span> <span class="number">2</span> * (node + <span class="number">1</span>); &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sizeOfNode</span><span class="params">(<span class="keyword">int</span> node)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!checkNode(node)) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> sizeL = sizeOfNode(left(node));</span><br><span class="line"><span class="keyword">int</span> sizeR = sizeOfNode(right(node));</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span> + sizeL + sizeR;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">heightofNode</span><span class="params">(<span class="keyword">int</span> node)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (!checkNode(node)) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> heightL = heightofNode(left(node));</span><br><span class="line"><span class="keyword">int</span> heightR = heightofNode(right(node));</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span> + (heightL &gt; heightR ? heightL : heightR);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>二叉树的一个应用是计算图的表示，单个节点的值为操作符，左右子树为操作数，例如对于运算式</p><script type="math/tex; mode=display">((a + b) > (c > e)) | a < b \& (x < y | y > z)</script><p>其运算图为<br><img src="/2020/02/29/【数据结构】二叉树及其他树/calculate.jpg" alt="calculate"></p><p>由<code>LinkedBinaryTree&lt;T&gt;</code>派生出类<code>Expression</code>，指定其输入输入类型为字符(<code>char</code>)</p><blockquote><ol><li>子类调用父类成员时，需添加<code>this-&gt;</code>指针；</li><li>公有继承时，父类公有成员与保护成员权限不变，但子类不继承私有成员；</li></ol></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Expression</span> :</span> <span class="keyword">public</span> LinkedBinaryTree&lt;<span class="keyword">char</span>&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">Expression(<span class="keyword">char</span>* symbols, <span class="keyword">int</span> n) : LinkedBinaryTree&lt;<span class="keyword">char</span>&gt;(symbols, n) &#123; translate(); &#125;</span><br><span class="line">~Expression() &#123; <span class="keyword">if</span> (expression) <span class="keyword">delete</span> expression; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span>   </span>&#123; <span class="keyword">return</span> LinkedBinaryTree&lt;<span class="keyword">char</span>&gt;::size(); &#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">height</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> LinkedBinaryTree&lt;<span class="keyword">char</span>&gt;::height(); &#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> LinkedBinaryTree&lt;<span class="keyword">char</span>&gt;::empty(); &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">(<span class="keyword">int</span> mode = <span class="number">3</span>)</span> </span>&#123; LinkedBinaryTree&lt;<span class="keyword">char</span>&gt;::print(mode); &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">fCalculate</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="keyword">char</span>* expression;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">translate</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">translateNode</span><span class="params">(BinaryTreeNode&lt;<span class="keyword">char</span>&gt;* node, <span class="keyword">char</span>* buff, <span class="keyword">int</span> n)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">float</span> <span class="title">fCalculateNode</span><span class="params">(BinaryTreeNode&lt;<span class="keyword">char</span>&gt;* node)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">float</span> <span class="title">char2float</span><span class="params">(<span class="keyword">char</span>&amp; c)</span> </span>&#123; <span class="keyword">return</span> c - <span class="string">'0'</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>可通过打印各节点对比查看<strong>前序遍历、中序遍历、后序遍历、层级遍历间的区别</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">char</span> symbols[<span class="number">32</span>] = &#123;</span><br><span class="line"><span class="string">'|'</span>,</span><br><span class="line"><span class="string">'&gt;'</span>, <span class="string">'&amp;'</span>,</span><br><span class="line"><span class="string">'+'</span>, <span class="string">'-'</span>, <span class="string">'&lt;'</span>, <span class="string">'|'</span>,</span><br><span class="line"><span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'e'</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'&lt;'</span>, <span class="string">'&gt;'</span>,</span><br><span class="line"><span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'x'</span>, <span class="string">'y'</span>, <span class="string">'y'</span>, <span class="string">'z'</span> &#125;;</span><br><span class="line"><span class="function">Expression <span class="title">expression</span><span class="params">(symbols, <span class="number">31</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"size: "</span> &lt;&lt; expression.size() &lt;&lt; <span class="built_in">endl</span> \</span><br><span class="line">&lt;&lt; <span class="string">"height: "</span> &lt;&lt; expression.height() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) expression.print(i);</span><br><span class="line"></span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其输出如下，得到分别为<strong>前缀</strong>(prefix)、<strong>中缀</strong>(infix)、<strong>后缀</strong>(postfix)形式的输出<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">size: 19</span><br><span class="line">height: 5</span><br><span class="line">preOrder: | &gt; + a b - c e &amp; &lt; a b | &lt; x y &gt; y z</span><br><span class="line">inOrder: a + b &gt; c - e | a &lt; b &amp; x &lt; y | y &gt; z</span><br><span class="line">postOrder: z y &gt; y x &lt; | b a &lt; &amp; e c - b a + &gt; |</span><br><span class="line">levelOrder: | &gt; &amp; + - &lt; | a b c e a b &lt; &gt; x y y z</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></table></figure></p><p>可以看到三张顺序输出结果差异较大，如下图所示，<strong>中序遍历<code>inOrder</code>是通常的书写形式</strong>。<br><img src="/2020/02/29/【数据结构】二叉树及其他树/calculate_pre.jpg" alt="calculate_pre"><br><img src="/2020/02/29/【数据结构】二叉树及其他树/calculate_in.jpg" alt="calculate_in"><br><img src="/2020/02/29/【数据结构】二叉树及其他树/calculate_post.jpg" alt="calculate_post"><br><img src="/2020/02/29/【数据结构】二叉树及其他树/calculate_lv.jpg" alt="calculate_lv"></p><p>在实际使用该类用于计算时，应考虑<strong>运算优先级</strong>，有以下几种解决方法</p><ol><li>给不同的运算符设置不同的优先级，例如<code>×, ÷</code>比<code>+, -</code>优先级更高；</li><li><p>将操作符的每个操作数用括号括起来，如其成员函数<code>translate()</code>与<code>translateNode()</code>实现如下</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Expression::translate()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (!expression) &#123;</span><br><span class="line">        expression = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">256</span>]; expression[<span class="number">0</span>] = <span class="string">'\0'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    translateNode(<span class="keyword">this</span>-&gt;m_tnRoot, expression, <span class="number">256</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> Expression::translateNode(BinaryTreeNode&lt;<span class="keyword">char</span>&gt;* node, <span class="keyword">char</span>* buff, <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 嵌套中止条件：叶节点</span></span><br><span class="line">    <span class="keyword">if</span> (node-&gt;isLeaf()) &#123;</span><br><span class="line">        sprintf_s(buff, n, <span class="string">"%s%c%c%c"</span>, buff, <span class="string">'('</span>, node-&gt;get(), <span class="string">')'</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    translateNode(node-&gt;left, buff, n);<span class="comment">// 左子树</span></span><br><span class="line">    sprintf_s(buff, n, <span class="string">"%c%s%c"</span>, <span class="string">'('</span>, buff, node-&gt;get());</span><br><span class="line">    translateNode(node-&gt;right, buff, n);<span class="comment">// 右子树</span></span><br><span class="line">    sprintf_s(buff, n, <span class="string">"%s%c"</span>, buff, <span class="string">')'</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 调用<code>cout &lt;&lt; expression.expression &lt;&lt; endl;</code>后，输出为$ ((((((((((a)+(b))&gt;(c)-(e)))|(a)&lt;(b))\&amp;(x)&lt;(y))|(y)&gt;(z)))))$，<strong>外层冗余括号不影响计算结果</strong>。</p></li><li><p><strong>按节点计算</strong>时，已根据输入形式自动考虑运算优先次序，实现如下</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> Expression::fCalculate() <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> fCalculateNode(<span class="keyword">this</span>-&gt;m_tnRoot);<span class="comment">// 计算根节点的值</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> Expression::fCalculateNode(BinaryTreeNode&lt;<span class="keyword">char</span>&gt;* node)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (!node) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 若为叶子节点，返回操作数</span></span><br><span class="line">    <span class="keyword">if</span> (node-&gt;isLeaf()) &#123;</span><br><span class="line">        <span class="keyword">char</span> c = node-&gt;get();</span><br><span class="line">        <span class="keyword">return</span> char2float(c);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 不为叶子节点，获取左右子树的运算结果</span></span><br><span class="line">    <span class="keyword">float</span> x = fCalculateNode(node-&gt;left);</span><br><span class="line">    <span class="keyword">float</span> y = fCalculateNode(node-&gt;right);</span><br><span class="line">    <span class="comment">// 从自身读取操作符，并运算</span></span><br><span class="line">    <span class="keyword">char</span> op = node-&gt;get();</span><br><span class="line">    <span class="keyword">switch</span> (op)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">'+'</span>:</span><br><span class="line">        <span class="keyword">return</span> x + y;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">'-'</span>:</span><br><span class="line">        <span class="keyword">return</span> x - y;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">'*'</span>:</span><br><span class="line">        <span class="keyword">return</span> x * y;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">'/'</span>:</span><br><span class="line">        <span class="keyword">return</span> x / y;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">throw</span> <span class="string">"未定义的运算符"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 输入$(1+2)/(5\times6-7)$得到输出 <code>(((((1)+(2))/(5)*(6))-(7))) = 0.130435</code></p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> symbols[<span class="number">32</span>] = &#123;</span><br><span class="line">        <span class="string">'/'</span>,</span><br><span class="line">        <span class="string">'+'</span>, <span class="string">'-'</span>, </span><br><span class="line">        <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'*'</span>, <span class="string">'7'</span>,</span><br><span class="line">        <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'#'</span>, <span class="string">'5'</span>, <span class="string">'6'</span>&#125;;</span><br><span class="line">    <span class="function">Expression <span class="title">expression</span><span class="params">(symbols, <span class="number">13</span>)</span></span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; expression.expression &lt;&lt; <span class="string">" = "</span> &lt;&lt; expression.fCalculate() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    system(<span class="string">"pause"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】跳表和散列</title>
      <link href="/2020/02/24/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E8%B7%B3%E8%A1%A8%E5%92%8C%E6%95%A3%E5%88%97/"/>
      <url>/2020/02/24/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E8%B7%B3%E8%A1%A8%E5%92%8C%E6%95%A3%E5%88%97/</url>
      
        <content type="html"><![CDATA[<h1 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h1><h2 id="定义及抽象数据描述"><a href="#定义及抽象数据描述" class="headerlink" title="定义及抽象数据描述"></a>定义及抽象数据描述</h2><p>定义：<strong>字典</strong>(dictionary)是由一些形如$(k, v)$的数对所组成的集合，其中$k$为<strong>关键字</strong>，$v$是与关键字$k$对应的<strong>值</strong>。</p><p>抽象数据描述：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">抽象数据类型 dictionary</span><br><span class="line">&#123;</span><br><span class="line">    实例:</span><br><span class="line">        关键字各不相同的一组数据对；</span><br><span class="line">    操作：</span><br><span class="line">        empty();        <span class="comment">// 判断是否为空</span></span><br><span class="line">        size();         <span class="comment">// 返回字典内数对个数</span></span><br><span class="line">        find(k);        <span class="comment">// 返回关键字为k的数对</span></span><br><span class="line">        insert(k, v);   <span class="comment">// 插入数据对</span></span><br><span class="line">        erase(k);       <span class="comment">// 删除关键字为k的数对</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="线性表描述"><a href="#线性表描述" class="headerlink" title="线性表描述"></a>线性表描述</h2><p>将字典保存在线性表$p_0, p_1, \dots$中，其中$p_is$是字典中<strong>按关键字递增次序排列</strong>的数对。利用二分查找进行数据对的插入、查询与删除操作。其时间复杂度为$O(\log n)$。</p><p>在插入数据对时，首先对关键字进行查询，若未找到数对则创建新的数对，插入到对应位置；否则更新已有数对。</p><p>若采用数组描述的线性表，其二分查找较容易实现(实现略)。而对于链表描述的线性表，可引入跳表(<a href="#%e8%b7%b3%e8%a1%a8%e6%8f%8f%e8%bf%b0">跳表描述</a>)减少时间复杂度，</p><h2 id="跳表描述"><a href="#跳表描述" class="headerlink" title="跳表描述"></a>跳表描述</h2><p>对于线性表描述(链表描述)的字典，对$n$个数对进行查找，至多需要$n$此关键字比较。若添加中间节点的指针信息，其比较次数可减少到$(n/2 + 1)$。由此引入<strong>跳表</strong>(skiplist)，在不同的节点位置处添加多级指针信息。</p><p>跳表可用<strong>对数形式</strong>进行<strong>级的分配</strong>。例如，对于$n$个数，$0$级链表包括所有数对，$1$级链表每$2$个数对取一个，$2$级链表每$4$个数对取一个，以此类推。那么对于一个<strong>属于$i$级链表的数对</strong>，当且仅当它属于$0~i$级链表，但不属于$i+1$级链表。如下图所示，键为$20$的数对属于$0,1$级链表，不属于$2$级链表。</p><p><img src="/2020/02/24/【数据结构】跳表和散列/skiplist.jpg" alt="skiplist"></p><p>但是采用对数形式描述跳表，需在插入或删除数对后更新各级链表，其<strong>指针不易维护</strong>。对于该问题，可采用<strong>概率形式</strong>解决，<strong>在查询位置处(仍需满足有序线性表)</strong>插入结点时生成随机数判定级别，使s它属于$i-1$级链表的情况下，又属于$i$级链表的概率服从<strong>均匀分布</strong>，使$n$个数对在各级中分布是均匀的，即</p><script type="math/tex; mode=display">P(属于i级链表|属于i-1级链表) = p</script><p>具体实现如下，构造键值对对象<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//linkeddictionary.h</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">Pair() &#123; m_key = <span class="keyword">new</span> K; m_value = <span class="keyword">new</span> V; &#125;;</span><br><span class="line">Pair(<span class="keyword">const</span> K k, <span class="keyword">const</span> V v) &#123; m_key = <span class="keyword">new</span> K; *m_key = k;  m_value = <span class="keyword">new</span> V; *m_value = v; &#125;;</span><br><span class="line">~Pair() &#123; <span class="keyword">delete</span> m_key; <span class="keyword">delete</span> m_value; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setKey</span><span class="params">(<span class="keyword">const</span> K k)</span> </span>&#123; *m_key = k; &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setVal</span><span class="params">(<span class="keyword">const</span> V v)</span> </span>&#123; *m_value = v; &#125;</span><br><span class="line"><span class="function">K <span class="title">getKey</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> *m_key; &#125;</span><br><span class="line"><span class="function">V <span class="title">getVal</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> *m_value; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">K* m_key;</span><br><span class="line">V* m_value;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>以下几点说明：</p><ul><li>跳表内各级链表为<code>ChainList&lt;T&gt;</code>模板类的实例，将键值对<code>Pair&lt;K, V&gt;*</code>视作各节点的值，即实例为<code>ChainList&lt;Pair&lt;K, V&gt;* &gt;</code>；</li><li>指定最大级数，各级链表地址存储在<code>ChainList&lt;Pair&lt;K, V&gt;*&gt;** m_kvp</code>中；</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//linkeddictionary.h</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">SkipList(<span class="keyword">int</span> maxLevel=<span class="number">3</span>, <span class="keyword">float</span> p=<span class="number">0.5</span>);</span><br><span class="line">~SkipList();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> size() == <span class="number">0</span>;  &#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span>  <span class="title">size</span><span class="params">()</span>  <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> m_kvp[<span class="number">0</span>]-&gt;size(); &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> K&amp; k, <span class="keyword">const</span> V&amp; v)</span></span>;</span><br><span class="line"><span class="function">V <span class="title">get</span>  <span class="params">(<span class="keyword">const</span> K&amp; k)</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span><span class="params">(<span class="keyword">const</span> K&amp; k)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* find(<span class="keyword">const</span> K&amp; k) <span class="keyword">const</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">level</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> lv = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (; <span class="built_in">std</span>::rand() &gt; m_iCutoff; lv++);</span><br><span class="line"><span class="keyword">return</span> (lv &gt; m_iMaxLevel - <span class="number">1</span>) ? (m_iMaxLevel - <span class="number">1</span>) : lv;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> m_iCutoff;<span class="comment">// 条件概率，截断</span></span><br><span class="line"><span class="keyword">int</span> m_iMaxLevel;<span class="comment">// 最大链表层数</span></span><br><span class="line">ChainList&lt;Pair&lt;K, V&gt;*&gt;** m_kvp;<span class="comment">// 多层链表数组</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">SkipList&lt;K, V&gt;::SkipList(<span class="keyword">int</span> maxLevel, <span class="keyword">float</span> p) : </span><br><span class="line">m_iMaxLevel(maxLevel), m_iCutoff(p * RAND_MAX)</span><br><span class="line">&#123;</span><br><span class="line">m_kvp = <span class="keyword">new</span> ChainList&lt;Pair&lt;K, V&gt;*&gt; * [maxLevel];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iMaxLevel; i++)</span><br><span class="line">m_kvp[i] = <span class="keyword">new</span> ChainList&lt;Pair&lt;K, V&gt;*&gt;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">SkipList&lt;K, V&gt;::~SkipList()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iMaxLevel; i++)</span><br><span class="line"><span class="keyword">delete</span> m_kvp[i];</span><br><span class="line"><span class="keyword">delete</span>[] m_kvp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> SkipList&lt;K, V&gt;::print() <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iMaxLevel; i++) &#123;</span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* pn = m_kvp[i]-&gt;getNode(<span class="number">0</span>);<span class="comment">// 当前级链表的首节点</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Level ["</span> &lt;&lt; i &lt;&lt; <span class="string">"]: "</span>;</span><br><span class="line"><span class="keyword">while</span> (pn-&gt;ptr &amp;&amp; pn-&gt;next) &#123;</span><br><span class="line">Pair&lt;K, V&gt;* kvp = pn-&gt;getVal();</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; kvp-&gt;getKey() &lt;&lt; <span class="string">": "</span> &lt;&lt; kvp-&gt;getVal() &lt;&lt; <span class="string">" -&gt; "</span>;</span><br><span class="line">pn = pn-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在搜索时，自最高级链表向低级搜索，因为若高级链表中存在某键值对，那么低级链表中必定存在，此时可调用<code>ChainList&lt;T&gt;::indexOf(T)</code>直接获取起始搜索位置，实现二分搜索降低时间复杂度</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* SkipList&lt;K, V&gt;::find(<span class="keyword">const</span> K&amp; k) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 若字典内无数对，返回空</span></span><br><span class="line"><span class="keyword">if</span> (empty()) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* ret = m_kvp[<span class="number">0</span>]-&gt;getNode(<span class="number">0</span>);<span class="comment">// 初始化为0级链表首节点</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = m_iMaxLevel - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;<span class="comment">// 由高级向低级搜索</span></span><br><span class="line"><span class="keyword">if</span> (m_kvp[i]-&gt;empty()) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> index = m_kvp[i]-&gt;indexOf(ret-&gt;getVal());<span class="comment">// 查找当前节点</span></span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt; * pn = (index == <span class="number">-1</span>)? \</span><br><span class="line">m_kvp[i]-&gt;getNode(<span class="number">0</span>): \</span><br><span class="line">            m_kvp[i]-&gt;getNode(index);                    <span class="comment">// 当前级链表开始搜索的位置</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> ((pn-&gt;getVal()-&gt;getKey() &lt; k) &amp;&amp; pn-&gt;next-&gt;ptr) &#123;</span><br><span class="line">pn = pn-&gt;next; ret = pn;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!pn-&gt;next-&gt;ptr) <span class="keyword">continue</span>;<span class="comment">// 已为尾节点</span></span><br><span class="line"><span class="keyword">if</span> (pn-&gt;next-&gt;getVal()-&gt;getKey() == ret-&gt;getVal()-&gt;getKey()) </span><br><span class="line"><span class="keyword">return</span> pn-&gt;next;<span class="comment">// 后一节点是否符合</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">V SkipList&lt;K, V&gt;::get(<span class="keyword">const</span> K&amp; k) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 查找</span></span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* found = find(k);</span><br><span class="line"><span class="keyword">if</span> (found) <span class="keyword">return</span> found-&gt;getVal()-&gt;getVal();</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>插入时，需确定插入键值对的级数<code>level()</code>，依次插入链表即可，注意<strong>有序插入</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> SkipList&lt;K, V&gt;::insert(<span class="keyword">const</span> K&amp; k, <span class="keyword">const</span> V&amp; v)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 先进行查找，若找到节点则不添加键，仅修改值</span></span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt; * found = find(k);</span><br><span class="line"></span><br><span class="line"><span class="comment">// ----- 若字典为空，直接插入键值对 -----</span></span><br><span class="line"><span class="keyword">if</span> (!found) &#123;</span><br><span class="line"><span class="comment">// 创建节点</span></span><br><span class="line">Pair&lt;K, V&gt;* pn = <span class="keyword">new</span> Pair&lt;K, V&gt;(k, v);</span><br><span class="line"><span class="comment">// 确定层级</span></span><br><span class="line"><span class="keyword">int</span> lv = level();</span><br><span class="line"><span class="comment">// 插入各级链表</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= lv; i++) </span><br><span class="line">m_kvp[i]-&gt;insert(<span class="number">0</span>, pn);</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ----- 若找到节点则不添加键，仅修改值 -----</span></span><br><span class="line"><span class="keyword">if</span> (found-&gt;getVal()-&gt;getKey() == k) &#123; found-&gt;getVal()-&gt;setVal(v); <span class="keyword">return</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ----- 若未找到，在标志节点附近有序插入 -----</span></span><br><span class="line">Pair&lt;K, V&gt;* pn = <span class="keyword">new</span> Pair&lt;K, V&gt;(k, v);</span><br><span class="line"><span class="comment">// 确定层级</span></span><br><span class="line"><span class="keyword">int</span> lv = level();</span><br><span class="line"><span class="comment">// 插入各级链表</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= lv; i++) &#123;</span><br><span class="line"><span class="comment">// 当前链表空</span></span><br><span class="line"><span class="keyword">if</span> (m_kvp[i]-&gt;empty()) &#123; m_kvp[i]-&gt;insert(<span class="number">0</span>, pn); <span class="keyword">continue</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* lvn = m_kvp[i]-&gt;getNode(<span class="number">0</span>);<span class="comment">// 当前级链表的首节点</span></span><br><span class="line"><span class="keyword">while</span> (lvn) &#123;</span><br><span class="line"><span class="keyword">if</span> (!lvn-&gt;ptr) &#123;                <span class="comment">// 尾节点</span></span><br><span class="line">m_kvp[i]-&gt;insert(index, pn);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (lvn-&gt;ptr &amp;&amp; lvn-&gt;getVal()-&gt;getKey() &gt; k) &#123;<span class="comment">// 当前节点的键大于k</span></span><br><span class="line">m_kvp[i]-&gt;insert(index, pn);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">lvn = lvn-&gt;next; index++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>删除结点时，从高级链表至低级的顺序释放，因为若从低级链表至高级的顺序释放，某节点在调用<code>ChainList&lt;T&gt;::erase(int)</code>后，内存被释放导致高级链表中<code>ChainList&lt;T&gt;::indexOf(T)</code>难以定位</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> SkipList&lt;K, V&gt;::erase(<span class="keyword">const</span> K&amp; k)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 先进行查找</span></span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* found = find(k);</span><br><span class="line"></span><br><span class="line"><span class="comment">// ----- 若未找到 ----</span></span><br><span class="line"><span class="keyword">if</span> (!found) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ----- 若找到，删除各级链中的该数对-----</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = m_iMaxLevel - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">ChainList&lt;Pair&lt;K, V&gt;*&gt;* pc = m_kvp[i];<span class="comment">// 当前级链表</span></span><br><span class="line"><span class="keyword">int</span> index = pc-&gt;indexOf(found-&gt;getVal());</span><br><span class="line"><span class="keyword">if</span> (index == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">pc-&gt;erase(index);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主函数测试及输出如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    SkipList&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; dict(<span class="number">3</span>, <span class="number">0.5</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"insert: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        dict.insert(i % <span class="number">6</span>, i);</span><br><span class="line">        dict.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"erase: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">        dict.erase(i);</span><br><span class="line">        dict.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">insert:</span><br><span class="line">Level [0]: 0: 0 -&gt;</span><br><span class="line">Level [1]:</span><br><span class="line">Level [2]:</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 0 -&gt; 1: 1 -&gt;</span><br><span class="line">Level [1]: 1: 1 -&gt;</span><br><span class="line">Level [2]:</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt;</span><br><span class="line">Level [1]: 1: 1 -&gt; 2: 2 -&gt;</span><br><span class="line">Level [2]: 2: 2 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt;</span><br><span class="line">Level [1]: 1: 1 -&gt; 2: 2 -&gt;</span><br><span class="line">Level [2]: 2: 2 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt;</span><br><span class="line">Level [1]: 1: 1 -&gt; 2: 2 -&gt; 4: 4 -&gt;</span><br><span class="line">Level [2]: 2: 2 -&gt; 4: 4 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 1: 1 -&gt; 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 6 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 1: 1 -&gt; 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 6 -&gt; 1: 7 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 1: 7 -&gt; 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 6 -&gt; 1: 7 -&gt; 2: 8 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 1: 7 -&gt; 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 0: 6 -&gt; 1: 7 -&gt; 2: 8 -&gt; 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 1: 7 -&gt; 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">erase:</span><br><span class="line">Level [0]: 1: 7 -&gt; 2: 8 -&gt; 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 1: 7 -&gt; 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 2: 8 -&gt; 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line"></span><br><span class="line">Level [0]: 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [1]: 4: 4 -&gt; 5: 5 -&gt;</span><br><span class="line">Level [2]: 4: 4 -&gt; 5: 5 -&gt;</span><br></pre></td></tr></table></figure><h2 id="散列描述"><a href="#散列描述" class="headerlink" title="散列描述"></a>散列描述</h2><h3 id="散列函数和散列表"><a href="#散列函数和散列表" class="headerlink" title="散列函数和散列表"></a>散列函数和散列表</h3><p>字典的另一种表示方法是<strong>散列</strong>(hashing)。它用一个<strong>散列函数(哈希函数)</strong>把字典的数对映射到一个<strong>散列表(哈希表)</strong>的具体位置。若关键字范围过大，可将若干个不同的关键字映射到散列表的同一位置。散列表的每个位置称作<strong>桶</strong>(bucket)，桶的数目等于散列表的长度。对于关键字为$k$的数对，$f(k)$为<strong>起始桶</strong>(home bucket)。</p><p>当两个不同关键字所对应的起始桶相同，产生<strong>冲突</strong>(collision)，但一个桶内可存放多个数位，但若存储桶内没有空间存储新数对，则发生<strong>溢出</strong>(overflow)。当冲突和溢出同时发生，可采用<a href="#%e7%ba%bf%e6%80%a7%e6%8e%a2%e6%9f%a5">线性探查法</a>、平法探查法、双重散列法等解决。</p><p>可采取<strong>均匀散列函数</strong>(uniform hash function)，使映射到散列表中各个桶的关键字数量大致相同，此时冲突和溢出发生的平均数最少。在实际应用中性能表现良好的均匀散列函数被称作<strong>良好散列函数</strong>(good hash function)。</p><ol><li>均匀散列函数输入值应依赖关键字的所有位；</li><li>要使得<strong>除法散列函数</strong>成为良好散列函数，$D$应取素数，且不能被小于$20$的整数除。<script type="math/tex; mode=display">f(k) = k \% D</script></li></ol><p>以下为STL模板类<code>hash&lt;T&gt;</code>的专业实现版本<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">hash</span>&lt;string&gt;</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="keyword">size_t</span> <span class="keyword">operator</span>() (<span class="keyword">const</span> <span class="built_in">string</span> key) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> hashVal = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; key.size(); i++)</span><br><span class="line">hashVal = <span class="number">5</span> * hashVal + key.at(i);</span><br><span class="line"><span class="keyword">return</span> hashVal;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="线性探查"><a href="#线性探查" class="headerlink" title="线性探查"></a>线性探查</h3><p><strong>线性探查</strong>(Linear Probing)是指，在某数对插入时。键所对应的桶满时，向哈希表往后找到下一个可用的桶。对于关键字为$k$的数据对，其具体插入步骤如下</p><ol><li>搜索起始桶$f(k)$，若为空，则插入并退出，否则进入2；</li><li>将散列表当作环表继续搜索下一个可用的桶，直至以下情况发生：a)存有关键字$k$的桶已找到，修改其数值；b)到达一个空桶，插入后退出；c)回到起始桶，冲突和溢出同时发生。</li></ol><p>设散列表中有$n$条数据，桶数为$b$，散列函数的除数为$D$且$D=b$</p><ul><li>散列表初始化时间为$O(b)$；</li><li>最坏情况下(关键字都对应同一起始桶)插入和查找时间均为$\Theta(n)$；</li><li>平均性能而言，散列远优于线性表。一次成功搜索和不成功搜索平均搜索桶数分别记作$U_n$与$S_n$，则有<script type="math/tex; mode=display">U_n \approxeq \frac{1}{2} \left( 1 + \frac{1}{(1 - \alpha)^2} \right)</script><script type="math/tex; mode=display">S_n \approxeq \frac{1}{2} \left( 1 + \frac{1}{(1 - \alpha)  } \right)</script>  其中$\alpha = n / b$，称作负载因子，当负载因子比较小是，线性探查使得散列的平均性能比线性表优越许多。</li></ul><p>基于线性探查实现的字典如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashTabel</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">HashTabel(<span class="keyword">int</span> length = <span class="number">50</span>, <span class="keyword">int</span> d = <span class="number">23</span>);</span><br><span class="line">~HashTabel();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> size() == <span class="number">0</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span>  <span class="title">size</span><span class="params">()</span>  <span class="keyword">const</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++)</span><br><span class="line"><span class="keyword">if</span> (m_listIsEmpty[i]) cnt++;</span><br><span class="line"><span class="keyword">return</span> cnt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> K&amp; k, <span class="keyword">const</span> V&amp; v)</span></span>;</span><br><span class="line"><span class="function">V <span class="title">get</span><span class="params">(<span class="keyword">const</span> K&amp; k)</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span><span class="params">(<span class="keyword">const</span> K&amp; k)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="keyword">size_t</span> hash(<span class="keyword">const</span> K k) <span class="keyword">const</span> &#123; <span class="keyword">return</span> k % m_iD; &#125;</span><br><span class="line">Pair&lt;K, V&gt;* find(<span class="keyword">const</span> K&amp; k, <span class="keyword">bool</span> insert = <span class="literal">true</span>) <span class="keyword">const</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">indexOf</span><span class="params">(<span class="keyword">const</span> Pair&lt;K, V&gt;* p)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++)</span><br><span class="line"><span class="keyword">if</span> (m_listTable[i] == p) <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> m_iD;</span><br><span class="line">Pair&lt;K, V&gt;** m_listTable;</span><br><span class="line"><span class="keyword">bool</span>* m_listIsEmpty;</span><br><span class="line"><span class="keyword">int</span> m_iLength;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><ol><li><p>关键的线性探查步骤如下</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">Pair&lt;K, V&gt;* HashTabel&lt;K, V&gt;::find(<span class="keyword">const</span> K&amp; k, <span class="keyword">bool</span> insert) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> hv = hash(k);<span class="comment">// 求取hash值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从初始桶开始查找</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++) &#123;</span><br><span class="line"><span class="keyword">int</span> index = (hv + i) % m_iLength;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 回到初始位置，同时发生冲突与溢出，返回空</span></span><br><span class="line"><span class="keyword">if</span> (i != <span class="number">0</span> &amp;&amp; index == hv) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在插入操作时，可返回空桶</span></span><br><span class="line"><span class="keyword">if</span> (insert &amp;&amp; m_listIsEmpty[index]) <span class="keyword">return</span> m_listTable[index];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找到相同键的桶</span></span><br><span class="line"><span class="keyword">if</span> (m_listTable[index]-&gt;getKey() == k)</span><br><span class="line"><span class="keyword">return</span> m_listTable[index];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>构造函数、析构函数、输出函数</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">HashTabel&lt;K, V&gt;::HashTabel(<span class="keyword">int</span> length, <span class="keyword">int</span> d) :</span><br><span class="line">m_iLength(length), m_iD(d)</span><br><span class="line">&#123;</span><br><span class="line">m_listTable = <span class="keyword">new</span> Pair&lt;K, V&gt;* [m_iLength];</span><br><span class="line">m_listIsEmpty = <span class="keyword">new</span> <span class="keyword">bool</span> [m_iLength];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++) &#123;</span><br><span class="line">m_listTable[i] = <span class="keyword">new</span> Pair&lt;K, V&gt;;</span><br><span class="line">m_listIsEmpty[i] = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">HashTabel&lt;K, V&gt;::~HashTabel()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++) </span><br><span class="line"><span class="keyword">delete</span> m_listTable[i];</span><br><span class="line"><span class="keyword">delete</span>[] m_listTable;</span><br><span class="line"><span class="keyword">delete</span>[] m_listIsEmpty;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> HashTabel&lt;K, V&gt;::print() <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++) &#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"|["</span> &lt;&lt;  i &lt;&lt; <span class="string">"]"</span>;</span><br><span class="line"><span class="keyword">if</span> (!m_listIsEmpty[i])</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt;  m_listTable[i]-&gt;getKey() &lt;&lt; <span class="string">": "</span> &lt;&lt; \</span><br><span class="line">m_listTable[i]-&gt;getVal() &lt;&lt; <span class="string">"| -&gt;"</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">" -&gt; "</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>插入、查询、删除</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> HashTabel&lt;K, V&gt;::insert(<span class="keyword">const</span> K&amp; k, <span class="keyword">const</span> V&amp; v)</span><br><span class="line">&#123;</span><br><span class="line">Pair&lt;K, V&gt;* found = find(k);</span><br><span class="line"><span class="keyword">if</span> (!found) &#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"哈希表已满"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 反查位置</span></span><br><span class="line"><span class="keyword">int</span> index = indexOf(found);</span><br><span class="line"><span class="comment">// 若为空，设置键</span></span><br><span class="line"><span class="keyword">if</span> (m_listIsEmpty[index]) found-&gt;setKey(k);</span><br><span class="line"><span class="comment">// 设置值</span></span><br><span class="line">found-&gt;setVal(v); </span><br><span class="line">m_listIsEmpty[index] = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">V HashTabel&lt;K, V&gt;::get(<span class="keyword">const</span> K&amp; k) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">Pair&lt;K, V&gt;* found = find(k);</span><br><span class="line"><span class="keyword">if</span> (found) <span class="keyword">return</span> found-&gt;getVal();</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> HashTabel&lt;K, V&gt;::erase(<span class="keyword">const</span> K&amp; k)</span><br><span class="line">&#123;</span><br><span class="line">Pair&lt;K, V&gt;* found = find(k, <span class="literal">false</span>);</span><br><span class="line"><span class="keyword">if</span> (!found) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 反查位置</span></span><br><span class="line"><span class="keyword">int</span> index = indexOf(found);</span><br><span class="line"><span class="comment">// 设置为空</span></span><br><span class="line">m_listIsEmpty[index] = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>主函数测试如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    HashTabel&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; dict(<span class="number">10</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"insert: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line">        </span><br><span class="line">        dict.insert(<span class="number">2</span> * i, i);</span><br><span class="line">        dict.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"erase: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">4</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line">        dict.erase(<span class="number">2</span> * i);</span><br><span class="line">        dict.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>插入过程如下图所示<br><img src="/2020/02/24/【数据结构】跳表和散列/hashtabel_insert.jpg" alt="hashtabel_insert"></p><blockquote><p>输出格式为<code>|[index] key: value| -&gt; ...</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">insert:</span><br><span class="line">|[0]0: 0| -&gt;|[1] -&gt; |[2] -&gt; |[3] -&gt; |[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1] -&gt; |[2]2: 1| -&gt;|[3] -&gt; |[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3] -&gt; |[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5]10: 5| -&gt;|[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5]10: 5| -&gt;|[6]12: 6| -&gt;|[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5]10: 5| -&gt;|[6]12: 6| -&gt;|[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;</span><br><span class="line">erase:</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5]10: 5| -&gt;|[6]12: 6| -&gt;|[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6]12: 6| -&gt;|[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6] -&gt; |[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;</span><br><span class="line">|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;</span><br></pre></td></tr></table></figure></p></blockquote><h3 id="链式散列"><a href="#链式散列" class="headerlink" title="链式散列"></a>链式散列</h3><p>链式散列是指，给散列表的每一个桶配置一个链式线性表，那么每个桶可以容纳无限多个容量，那么便不存在溢出问题。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashChains</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">HashChains(<span class="keyword">int</span> length = <span class="number">50</span>, <span class="keyword">int</span> d = <span class="number">23</span>);</span><br><span class="line">~HashChains();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> size() == <span class="number">0</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span>  <span class="title">size</span><span class="params">()</span>  <span class="keyword">const</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++)</span><br><span class="line">cnt += m_listChains[i]-&gt;size();</span><br><span class="line"><span class="keyword">return</span> cnt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> K&amp; k, <span class="keyword">const</span> V&amp; v)</span></span>;</span><br><span class="line"><span class="function">V <span class="title">get</span><span class="params">(<span class="keyword">const</span> K&amp; k)</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span><span class="params">(<span class="keyword">const</span> K&amp; k)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="keyword">size_t</span> hash(<span class="keyword">const</span> K k) <span class="keyword">const</span> &#123; <span class="keyword">return</span> k % m_iD; &#125;</span><br><span class="line">Pair&lt;K, V&gt;* find(<span class="keyword">const</span> K&amp; k, <span class="keyword">bool</span> insert = <span class="literal">true</span>) <span class="keyword">const</span>;</span><br><span class="line">ChainList&lt;Pair&lt;K, V&gt;*&gt;* findChain(<span class="keyword">const</span> K&amp; k) <span class="keyword">const</span> &#123; <span class="keyword">return</span> m_listChains[hash(k)]; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> m_iD;</span><br><span class="line">ChainList&lt;Pair&lt;K, V&gt;*&gt;** m_listChains;<span class="comment">// 也可以换成跳表</span></span><br><span class="line"><span class="keyword">int</span> m_iLength;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ol><li><p>构造函数、析构函数、打印输出</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">HashChains&lt;K, V&gt;::HashChains(<span class="keyword">int</span> length, <span class="keyword">int</span> d) :</span><br><span class="line">m_iLength(length), m_iD(d)</span><br><span class="line">&#123;</span><br><span class="line">m_listChains = <span class="keyword">new</span> ChainList&lt;Pair&lt;K, V&gt;*&gt;* [m_iLength];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++)</span><br><span class="line">m_listChains[i] = <span class="keyword">new</span> ChainList&lt;Pair&lt;K, V&gt;*&gt;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">HashChains&lt;K, V&gt;::~HashChains()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++)</span><br><span class="line"><span class="keyword">delete</span> m_listChains[i];</span><br><span class="line"><span class="keyword">delete</span>[] m_listChains;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> HashChains&lt;K, V&gt;::print() <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iLength; i++) &#123;</span><br><span class="line">ChainList&lt;Pair&lt;K, V&gt;*&gt;* chain = m_listChains[i];</span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* node = chain-&gt;getNode(<span class="number">0</span>);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"["</span> &lt;&lt; i &lt;&lt; <span class="string">"]"</span>;</span><br><span class="line"><span class="keyword">while</span> (node &amp;&amp; node-&gt;ptr) &#123;</span><br><span class="line">Pair&lt;K, V&gt;* p = node-&gt;getVal();</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; p-&gt;getKey() &lt;&lt; <span class="string">":"</span> &lt;&lt; p-&gt;getVal() &lt;&lt; <span class="string">" -&gt; "</span>;</span><br><span class="line">node = node-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">" | "</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在各桶内查找</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">Pair&lt;K, V&gt;* HashChains&lt;K, V&gt;::find(<span class="keyword">const</span> K&amp; k, <span class="keyword">bool</span> insert) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> hv = hash(k);<span class="comment">// 求取hash值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取哈希值对应的桶</span></span><br><span class="line">ChainList&lt;Pair&lt;K, V&gt;*&gt;* chain = m_listChains[hv];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在桶内依次查找</span></span><br><span class="line">ChainNode&lt;Pair&lt;K, V&gt;*&gt;* node = chain-&gt;getNode(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">while</span> (node) &#123;</span><br><span class="line"><span class="comment">// 已到达链尾</span></span><br><span class="line"><span class="keyword">if</span> (!node-&gt;next) &#123;</span><br><span class="line"><span class="keyword">if</span> (!insert) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line"><span class="comment">// 插入操作时，创建节点</span></span><br><span class="line">Pair&lt;K, V&gt;* p = <span class="keyword">new</span> Pair&lt;K, V&gt;;</span><br><span class="line">chain-&gt;insert(chain-&gt;size(), p);</span><br><span class="line"><span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 找到对应键</span></span><br><span class="line"><span class="keyword">if</span> (node-&gt;getVal()-&gt;getKey() == k)</span><br><span class="line"><span class="keyword">return</span> node-&gt;getVal();</span><br><span class="line">node = node-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>插入、查询、删除操作</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> HashChains&lt;K, V&gt;::insert(<span class="keyword">const</span> K&amp; k, <span class="keyword">const</span> V&amp; v)</span><br><span class="line">&#123;</span><br><span class="line">Pair&lt;K, V&gt;* p = find(k);</span><br><span class="line"><span class="keyword">if</span> (p-&gt;getKey() != k) p-&gt;setKey(k);</span><br><span class="line">p-&gt;setVal(v);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line">V HashChains&lt;K, V&gt;::get(<span class="keyword">const</span> K&amp; k) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">Pair&lt;K, V&gt;* p = find(k, <span class="literal">false</span>);</span><br><span class="line"><span class="keyword">if</span> (p) <span class="keyword">return</span> p-&gt;getVal();</span><br><span class="line"><span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> K, <span class="keyword">typename</span> V&gt;</span><br><span class="line"><span class="keyword">void</span> HashChains&lt;K, V&gt;::erase(<span class="keyword">const</span> K&amp; k)</span><br><span class="line">&#123;</span><br><span class="line">ChainList&lt;Pair&lt;K, V&gt;*&gt;* chain = findChain(k);</span><br><span class="line">Pair&lt;K, V&gt;* p = find(k, <span class="literal">false</span>);</span><br><span class="line">chain-&gt;erase(chain-&gt;indexOf(p));</span><br><span class="line"><span class="keyword">delete</span> p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>主函数测试如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    HashChains&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; dict(<span class="number">10</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"insert: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line">        </span><br><span class="line">        dict.insert(<span class="number">2</span> * i, i);</span><br><span class="line">        dict.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"erase: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">4</span>; i &lt; <span class="number">8</span>; i++) &#123;</span><br><span class="line">        dict.erase(<span class="number">2</span> * i);</span><br><span class="line">        dict.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>分析：插入过程如下图<br><img src="/2020/02/24/【数据结构】跳表和散列/hashchain_insert.jpg" alt="hashchain_insert"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">insert:</span><br><span class="line">[0]0:0 -&gt;  | [1] | [2] | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]0:0 -&gt;  | [1] | [2]2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]0:0 -&gt;  | [1]4:2 -&gt;  | [2]2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]6:3 -&gt; 0:0 -&gt;  | [1]4:2 -&gt;  | [2]2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]6:3 -&gt; 0:0 -&gt;  | [1]4:2 -&gt;  | [2]8:4 -&gt; 2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]6:3 -&gt; 0:0 -&gt;  | [1]10:5 -&gt; 4:2 -&gt;  | [2]8:4 -&gt; 2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt;  | [1]10:5 -&gt; 4:2 -&gt;  | [2]8:4 -&gt; 2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt;  | [1]10:5 -&gt; 4:2 -&gt;  | [2]14:7 -&gt; 8:4 -&gt; 2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">erase:</span><br><span class="line">[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt;  | [1]10:5 -&gt; 4:2 -&gt;  | [2]14:7 -&gt; 2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt;  | [1]4:2 -&gt;  | [2]14:7 -&gt; 2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]6:3 -&gt; 0:0 -&gt;  | [1]4:2 -&gt;  | [2]14:7 -&gt; 2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br><span class="line">[0]6:3 -&gt; 0:0 -&gt;  | [1]4:2 -&gt;  | [2]2:1 -&gt;  | [3] | [4] | [5] | [6] | [7] | [8] | [9] |</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】栈与队列</title>
      <link href="/2020/02/21/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97/"/>
      <url>/2020/02/21/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97/</url>
      
        <content type="html"><![CDATA[<h1 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h1><h2 id="定义及抽象数据描述"><a href="#定义及抽象数据描述" class="headerlink" title="定义及抽象数据描述"></a>定义及抽象数据描述</h2><p>定义：<strong>栈</strong>(stack)是一种特殊的线性表，其插入(aka. 入栈、压栈)和删除(aka. 出栈、弹栈)操作在表的同一端进行，即后进先出(last-in-first-out, LIFO)。两端称作栈顶(top)、栈底(bottom)。</p><p>抽象数据描述：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">抽象数据类型 <span class="built_in">stack</span></span><br><span class="line">&#123;</span><br><span class="line">    实例:</span><br><span class="line">        linearList;</span><br><span class="line">    操作：</span><br><span class="line">        empty();    <span class="comment">// 判断是否为空</span></span><br><span class="line">        size();     <span class="comment">// 返回栈内元素个数</span></span><br><span class="line">        top();      <span class="comment">// 返回栈顶元素</span></span><br><span class="line">        pop();      <span class="comment">// 出栈</span></span><br><span class="line">        push(x);    <span class="comment">// 入栈</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="链表描述"><a href="#链表描述" class="headerlink" title="链表描述"></a>链表描述</h2><p>继承自模板类<code>ChainList</code>编写模板类<code>LinkedStack</code>即可。</p><blockquote><p>注：若构造的类为模板类，那么派生类不可以直接使用继承到的基类数据和方法，需要通过<code>this</code>指针使用。子类内存释放时，将自动调用父类析构函数。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// linkedstack.h</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"arraylist.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkedStack</span>:</span> <span class="keyword">public</span> ChainList&lt;T&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    LinkedStack(): ChainList&lt;T&gt;() &#123;&#125;</span><br><span class="line">    ~LinkedStack()&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">T&amp; <span class="title">top</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        checkStack();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;get(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">T&amp; <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        checkStack();</span><br><span class="line">        T value = <span class="keyword">this</span>-&gt;get(<span class="number">0</span>); <span class="keyword">this</span>-&gt;erase(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">const</span> T&amp; value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;insert(<span class="number">0</span>, value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">checkStack</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) <span class="keyword">throw</span> <span class="string">"The stack is empty()"</span>;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>主函数测试<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LinkedStack&lt;<span class="keyword">int</span>&gt; <span class="built_in">stack</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 入栈</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        <span class="built_in">stack</span>.push(<span class="number">3</span> * i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">stack</span>.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 出栈</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++) &#123;</span><br><span class="line">        <span class="built_in">stack</span>.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">stack</span>.print();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">12 9 6 3 0</span><br><span class="line">3 0</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>例：一辆从前至后各车厢标号混乱的火车，从轨道入口驶入，出口驶出，与轨道垂直方向分布$k$个缓冲轨道(holding track)。要求利用缓冲轨道将车厢进行排序。</p><p><img src="/2020/02/21/【数据结构】栈与队列/stack.jpg" alt="stack"></p><p>实际上，此时缓冲轨道和入口轨道均可视作栈，均用<code>LinkedStack</code>表示。</p><blockquote><p>也可考虑入口至出口为双向链表，另一解决方案。</p></blockquote><p>两个功能性函数：1) 判断缓冲轨道是否为空；2) 寻找栈顶最大的缓冲轨道，返回其索引。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isHoldingTracksEmpty</span><span class="params">(<span class="keyword">const</span> LinkedStack&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (holdingTracks[i].top() != <span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findHoldingTracksWithMaxTop</span><span class="params">(<span class="keyword">const</span> LinkedStack&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">-1</span>; <span class="keyword">int</span> max = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> top = holdingTracks[i].top();</span><br><span class="line">        <span class="keyword">if</span> (top &gt; max) &#123;</span><br><span class="line">            index = i; max = top;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>算法说明如下：<br>1) 依次将火车车厢装入缓冲轨道。注意在第$i$节标号为$s_i$的车厢选择缓冲轨道时，选择<strong>栈顶车厢标号$t_j$小于$s_i$且$|t_j - s_i|$最小</strong>的缓冲轨道$j$，这样的目的是，使缓冲轨道内数字<strong>从栈顶至栈底按从大到小</strong>的顺序排列，且尽可能连续。若找不到满足要求的轨道，进入2)；<br>    <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findHoldingTrack</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> LinkedStack&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 寻找 `栈顶 &lt; 当前车厢标号num` 且 `最接近num` 的栈</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> em = INT32_MAX;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> e = holdingTracks[i].top() - num;</span><br><span class="line">        <span class="keyword">if</span> (e &lt; <span class="number">0</span> &amp;&amp; <span class="built_in">abs</span>(e) &lt; em) &#123;</span><br><span class="line">            em = <span class="built_in">abs</span>(e); index = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>2) 将所有缓冲轨道内的车厢按照<strong>从大到小</strong>的顺序，装回原车厢，进入3)；<br>    <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">putBackBins</span><span class="params">(LinkedStack&lt;<span class="keyword">int</span>&gt;&amp; bins, LinkedStack&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 注意，此时插入车辆时，按从大到小排序</span></span><br><span class="line">    <span class="keyword">while</span> (!isHoldingTracksEmpty(holdingTracks, k)) &#123;</span><br><span class="line">        <span class="keyword">int</span> itop = findHoldingTracksWithMaxTop(holdingTracks, k);</span><br><span class="line">        bins.push(holdingTracks[itop].pop());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>3) 重新按1)对车厢进行排序，如此循环，直至排序完成，进入4)；<br>4) 重复步骤2)一次，完成排序。</p><p>程序入口如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">LinkedStack&lt;<span class="keyword">int</span>&gt; reSortBins(<span class="keyword">const</span> LinkedStack&lt;<span class="keyword">int</span>&gt;&amp; bins, <span class="keyword">int</span> k = <span class="number">3</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    LinkedStack&lt;<span class="keyword">int</span>&gt;* holdingTracks = <span class="keyword">new</span> LinkedStack&lt;<span class="keyword">int</span>&gt;[k];</span><br><span class="line">    LinkedStack&lt;<span class="keyword">int</span>&gt; bins_c(bins);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化各栈，栈底置`-1`</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) holdingTracks[i].push(<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将链表内数据划入栈内，要求每个栈内从顶至底，标号从大到小</span></span><br><span class="line">    <span class="keyword">while</span>(!bins_c.empty()) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sign = bins_c.pop(); </span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">-1</span>; </span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 打印当前车厢编号信息</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Time["</span> &lt;&lt; cnt++ &lt;&lt; <span class="string">"]: "</span>;  bins_c.print();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 寻找 `栈顶 &lt; 当前车厢标号sign` 且 `最接近sign` 的栈</span></span><br><span class="line">            index = findHoldingTrack(sign, holdingTracks, k);</span><br><span class="line">            <span class="keyword">if</span> (index != <span class="number">-1</span>) <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 若未找到，将当前已拍好序的放回车辆</span></span><br><span class="line">            putBackBins(bins_c, holdingTracks, k);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入栈</span></span><br><span class="line">        holdingTracks[index].push(sign);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查看缓冲车道内排序</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"Holding tracks: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) holdingTracks[i].print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重新装回车辆</span></span><br><span class="line">    putBackBins(bins_c, holdingTracks, k);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 回收工作</span></span><br><span class="line">    <span class="keyword">delete</span>[] holdingTracks;</span><br><span class="line">    <span class="keyword">return</span> bins_c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试主函数如下，输入<code>581742936</code>，排序后应输出<code>123456789</code><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LinkedStack&lt;<span class="keyword">int</span>&gt; bins;</span><br><span class="line">    <span class="keyword">int</span> origin[<span class="number">9</span>] = &#123; <span class="number">5</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">3</span> &#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++)</span><br><span class="line">        bins.push(origin[<span class="number">9</span> - i - <span class="number">1</span>]);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"origin: "</span>; bins.print(); <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    LinkedStack&lt;<span class="keyword">int</span>&gt; bins_c = reSortBins(bins, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"sorted: "</span>; bins_c.print();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">origin: 5 8 1 7 4 2 9 6 3</span><br><span class="line"></span><br><span class="line">Time[0]: 8 1 7 4 2 9 6 3</span><br><span class="line">Time[1]: 1 7 4 2 9 6 3</span><br><span class="line">Time[2]: 7 4 2 9 6 3</span><br><span class="line">Time[3]: 4 2 9 6 3</span><br><span class="line">Time[4]: 2 9 6 3</span><br><span class="line">Time[5]: 9 6 3</span><br><span class="line">Time[6]: 1 4 5 7 8 9 6 3</span><br><span class="line">Time[7]: 4 5 7 8 9 6 3</span><br><span class="line">Time[8]: 5 7 8 9 6 3</span><br><span class="line">Time[9]: 7 8 9 6 3</span><br><span class="line">Time[10]: 8 9 6 3</span><br><span class="line">Time[11]: 9 6 3</span><br><span class="line">Time[12]: 6 3</span><br><span class="line">Time[13]: 3</span><br><span class="line">Time[14]:</span><br><span class="line"></span><br><span class="line">Holding tracks:</span><br><span class="line">9 8 7 5 4 2 -1</span><br><span class="line">6 1 -1</span><br><span class="line">3 -1</span><br><span class="line"></span><br><span class="line">sorted: 1 2 3 4 5 6 7 8 9</span><br></pre></td></tr></table></figure></p><h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><h2 id="定义及抽象数据描述-1"><a href="#定义及抽象数据描述-1" class="headerlink" title="定义及抽象数据描述"></a>定义及抽象数据描述</h2><p>定义：<strong>队列</strong>(queue)是一个线性表，其插入和删除操作分别在表的两端进行，即先进先出(first-in-first-out, FIFO)。插入的一端称队尾(back或rear)，删除元素的一端称为队首(front)。</p><p>抽象数据描述：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">抽象数据类型 <span class="built_in">queue</span></span><br><span class="line">&#123;</span><br><span class="line">    实例:</span><br><span class="line">        linearList;</span><br><span class="line">    操作：</span><br><span class="line">        empty();    <span class="comment">// 判断是否为空</span></span><br><span class="line">        size();     <span class="comment">// 返回队列内元素个数</span></span><br><span class="line">        front();    <span class="comment">// 返回队首元素</span></span><br><span class="line">        back();     <span class="comment">// 返回队尾元素</span></span><br><span class="line">        pop();      <span class="comment">// 删除队首元素</span></span><br><span class="line">        push(x);    <span class="comment">// 把元素加入队尾</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="链表描述-1"><a href="#链表描述-1" class="headerlink" title="链表描述"></a>链表描述</h2><p>同样的，继承自模板类<code>ChainList</code>编写模板类<code>LinkedQueue</code>即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// linkedqueue.h</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"arraylist.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkedQueue</span> :</span> <span class="keyword">public</span> ChainList&lt;T&gt;</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    LinkedQueue() : ChainList&lt;T&gt;() &#123;&#125;</span><br><span class="line">    ~LinkedQueue() &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">T <span class="title">front</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        checkStack();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;get(<span class="keyword">this</span>-&gt;size() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">T <span class="title">back</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        checkStack();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;get(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">T <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        checkStack();</span><br><span class="line">        T value = <span class="keyword">this</span>-&gt;get(<span class="keyword">this</span>-&gt;size() - <span class="number">1</span>); </span><br><span class="line">        <span class="keyword">this</span>-&gt;erase(<span class="keyword">this</span>-&gt;size() - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">const</span> T&amp; value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;insert(<span class="number">0</span>, value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;size(); i++)</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="keyword">this</span>-&gt;get(i) &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">checkStack</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) <span class="keyword">throw</span> <span class="string">"The queue is empty()"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>主函数测试程序:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LinkedQueue&lt;<span class="keyword">int</span>&gt; <span class="built_in">queue</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 入栈</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">        <span class="built_in">queue</span>.push(<span class="number">3</span> * i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">queue</span>.print();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"front: "</span> &lt;&lt; <span class="built_in">queue</span>.front() &lt;&lt; <span class="built_in">endl</span> &lt;&lt; </span><br><span class="line">            <span class="string">"back: "</span> &lt;&lt; <span class="built_in">queue</span>.back() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 出栈</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++) &#123;</span><br><span class="line">        <span class="built_in">queue</span>.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">queue</span>.print();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">12 9 6 3 0</span><br><span class="line">front: 0</span><br><span class="line">back: 12</span><br><span class="line">12 9</span><br></pre></td></tr></table></figure><h2 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h2><p>例：一辆从前至后各车厢标号混乱的火车，从轨道入口驶入，出口驶出，与轨道水平方向分布$k$个缓冲轨道(holding track)。要求利用缓冲轨道将车厢进行排序。</p><p><img src="/2020/02/21/【数据结构】栈与队列/queue.jpg" alt="queue"></p><p>此时，出入轨道、缓冲轨道均视作队列。</p><p>两个功能性函数：1) 判断缓冲轨道是否为空；2) 寻找队首最小的缓冲轨道，返回其索引。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isHoldingTracksEmpty</span><span class="params">(<span class="keyword">const</span> LinkedQueue&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!holdingTracks[i].empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findHoldingTracksWithMinFront</span><span class="params">(<span class="keyword">const</span> LinkedQueue&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">-1</span>; <span class="keyword">int</span> min = <span class="number">11</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (holdingTracks[i].empty()) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">int</span> front = holdingTracks[i].front();</span><br><span class="line">        <span class="keyword">if</span> (front &lt; min) &#123;</span><br><span class="line">            index = i; min = front;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>算法说明如下：<br>1) 依次将火车车厢装入缓冲轨道。注意在第$i$节标号为$s_i$的车厢选择缓冲轨道时，选择<strong>队尾标号$t_j$小于$s_i$且$|t_j - s_i|$最小</strong>的缓冲轨道$j$，这样的目的是，使缓冲轨道内数字<strong>从队首至队尾按从小到大</strong>的顺序排列，且尽可能连续。<strong>注意，必须留下一条轨道用于存放1号车厢</strong>。<br>    <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findHoldingTrack</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> LinkedQueue&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 寻找 `队尾 &lt; 当前车厢标号num` 且 `最接近num` 的栈</span></span><br><span class="line">    <span class="keyword">int</span> index = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> em = INT32_MAX;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> e = holdingTracks[i].back() - num;</span><br><span class="line">        <span class="keyword">if</span> (e &lt; <span class="number">0</span> &amp;&amp; <span class="built_in">abs</span>(e) &lt; em) &#123;</span><br><span class="line">            em = <span class="built_in">abs</span>(e); index = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>2) 将车厢放入出轨道，完成排序。<br>    <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">putIntoOutTrack</span><span class="params">(LinkedQueue&lt;<span class="keyword">int</span>&gt;&amp; outTrack, LinkedQueue&lt;<span class="keyword">int</span>&gt;* holdingTracks, <span class="keyword">int</span> k = <span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 缓冲车道的队首标记元素。</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) holdingTracks[i].pop();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注意，此时进入出轨道时时，按从小到大排序</span></span><br><span class="line">    <span class="keyword">while</span> (!isHoldingTracksEmpty(holdingTracks, k)) &#123;</span><br><span class="line">        <span class="keyword">int</span> itop = findHoldingTracksWithMinFront(holdingTracks, k);</span><br><span class="line">        outTrack.push(holdingTracks[itop].pop());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">LinkedQueue&lt;<span class="keyword">int</span>&gt; reSortBins(<span class="keyword">const</span> LinkedQueue&lt;<span class="keyword">int</span>&gt;&amp; bins, <span class="keyword">int</span> k = <span class="number">3</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    LinkedQueue&lt;<span class="keyword">int</span>&gt;* holdingTracks = <span class="keyword">new</span> LinkedQueue&lt;<span class="keyword">int</span>&gt;[k];</span><br><span class="line">    LinkedQueue&lt;<span class="keyword">int</span>&gt; bins_c(bins);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化各队列，列首置`-1`，其中一条置`11`，**用于存放`1号车`**</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">            holdingTracks[i].push(<span class="number">11</span>);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        holdingTracks[i].push(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 依次处理各车厢</span></span><br><span class="line">    <span class="keyword">while</span> (!bins_c.empty()) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sign = bins_c.pop();</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 打印当前车厢编号信息</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"Time["</span> &lt;&lt; cnt++ &lt;&lt; <span class="string">"]: "</span>;  bins_c.print();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 寻找 `队尾 &lt; 当前车厢标号sign` 且 `最接近sign` 的栈</span></span><br><span class="line">            index = findHoldingTrack(sign, holdingTracks, k);</span><br><span class="line">            <span class="keyword">if</span> (index != <span class="number">-1</span>) <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 若未找到，存入`0`号轨道</span></span><br><span class="line">            holdingTracks[<span class="number">0</span>].push(sign);</span><br><span class="line">            sign = bins_c.pop();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入栈</span></span><br><span class="line">        holdingTracks[index].push(sign);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查看缓冲车道内排序</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"Holding tracks: "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) holdingTracks[i].print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 排列到出轨道</span></span><br><span class="line">    LinkedQueue&lt;<span class="keyword">int</span>&gt; bins_o;</span><br><span class="line">    putIntoOutTrack(bins_o, holdingTracks, k);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bins_o;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    LinkedQueue&lt;<span class="keyword">int</span>&gt; bins;</span><br><span class="line">    <span class="keyword">int</span> origin[<span class="number">9</span>] = &#123; <span class="number">5</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">3</span> &#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++)</span><br><span class="line">        bins.push(origin[<span class="number">9</span> - i - <span class="number">1</span>]);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"origin: "</span>; bins.print(); <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    LinkedQueue&lt;<span class="keyword">int</span>&gt; bins_c = reSortBins(bins, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span> &lt;&lt; <span class="string">"sorted: "</span>; bins_c.print();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">origin: 5 8 1 7 4 2 9 6 3</span><br><span class="line"></span><br><span class="line">Time[0]: 5 8 1 7 4 2 9 6</span><br><span class="line">Time[1]: 5 8 1 7 4 2 9</span><br><span class="line">Time[2]: 5 8 1 7 4 2</span><br><span class="line">Time[3]: 5 8 1 7 4</span><br><span class="line">Time[4]: 5 8 1 7</span><br><span class="line">Time[5]: 5 8 1</span><br><span class="line">Time[6]: 5 8</span><br><span class="line">Time[7]: 5</span><br><span class="line">Time[8]:</span><br><span class="line"></span><br><span class="line">Holding tracks:</span><br><span class="line">5 1 11</span><br><span class="line">9 6 3 -1</span><br><span class="line">8 7 4 2 -1</span><br><span class="line"></span><br><span class="line">sorted: 9 8 7 6 5 4 3 2 1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【数据结构】线性表</title>
      <link href="/2020/02/17/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BA%BF%E6%80%A7%E8%A1%A8/"/>
      <url>/2020/02/17/%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BA%BF%E6%80%A7%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="线性表数据结构"><a href="#线性表数据结构" class="headerlink" title="线性表数据结构"></a>线性表数据结构</h1><p>定义：<strong>线性表</strong>(linear list)也称<strong>有序表</strong>(ordered list)，他的每一个实例都是元素的一个有序集合。对于形如$(e_0, e_1, \dots, e_{n-1})$的实例，$n$为有穷自然数表示线性表的长度或大小，$e_i$为线性表的元素，$i$是元素$e_i$的索引。可以认为$e_0$先于$e_1, \cdots, e_{n-1}$，<strong>除了这种先后关系外，线性表不再有其他关系</strong>。</p><p>线性表应具有以下操作：</p><ol><li>创建/删除线性表</li><li>判断是否为空</li><li>确定长度或大小</li><li>按索引查找元素</li><li>给定元素确定索引</li><li>给定索引插入/删除元素</li><li>按顺序输出线性表元素</li></ol><h1 id="描述方式"><a href="#描述方式" class="headerlink" title="描述方式"></a>描述方式</h1><h2 id="数组描述"><a href="#数组描述" class="headerlink" title="数组描述"></a>数组描述</h2><p>定义：<strong>数组描述</strong>(array representation)中，利用数组来存储线性表的元素。</p><ol><li>映射关系：用于确定一个元素在线性表中的位置，如$location(i)=i, location(i)=arrayLength-i-1$等等；</li><li>删除/插入元素：若在指定索引$i$处删除/插入元素$e_{new}$，除记录/删除元素外，需将$e_i, e_{i+1}, \cdots$前移/后移；</li><li>创建数组类：a)确定数据类型，b)确定数组长度。可使用模板类解决问题a)，使用动态数组解决问题b)。</li></ol><h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><blockquote><p>注意：实现模板类时，成员函数的声明与实现需位于同一文件，即<code>.h</code>中需包含函数的实现。否则将出现连接错误<code>LINK2019</code>错误。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// arraylist.h</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArrayList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">// 构造、拷贝构造、析构</span></span><br><span class="line">ArrayList(<span class="keyword">int</span> initSize = <span class="number">10</span>);</span><br><span class="line">ArrayList(<span class="keyword">const</span> ArrayList&lt;T&gt;&amp;);</span><br><span class="line">~ArrayList() &#123; <span class="keyword">delete</span>[] m_TElements;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组信息</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> m_iCount == <span class="number">0</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span>  <span class="title">size</span><span class="params">()</span>  <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> m_iCount; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 元素获取、查询</span></span><br><span class="line"><span class="function">T&amp;   <span class="title">get</span>    <span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> index)</span>   <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span>  <span class="title">indexOf</span><span class="params">(<span class="keyword">const</span> T&amp;  element)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> index, <span class="keyword">const</span> T&amp; e)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span> <span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> index)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123; </span><br><span class="line"><span class="keyword">if</span> (index &lt; <span class="number">0</span> || index &gt; m_iCount - <span class="number">1</span>) <span class="keyword">throw</span> <span class="string">"无效的索引"</span>; &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">expend</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">T*  m_TElements;</span><br><span class="line"><span class="keyword">int</span> m_iSize;</span><br><span class="line"><span class="keyword">int</span> m_iCount;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ************************************************</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">ArrayList&lt;T&gt;::ArrayList(<span class="keyword">int</span> initSize) :</span><br><span class="line">m_iSize(initSize), m_iCount(<span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (m_iSize &lt;= <span class="number">0</span>) <span class="keyword">throw</span> <span class="string">"无效的数组长度"</span>;</span><br><span class="line">m_TElements = <span class="keyword">new</span> T[m_iSize];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">ArrayList&lt;T&gt;::ArrayList(<span class="keyword">const</span> ArrayList&lt;T&gt;&amp; a)</span><br><span class="line">&#123;</span><br><span class="line">m_iSize = a.m_iSize;</span><br><span class="line">m_iCount = a.m_iCount;</span><br><span class="line">m_TElements = <span class="keyword">new</span> T[m_iSize];</span><br><span class="line"><span class="built_in">std</span>::copy(a.m_TElements, a.m_TElements + m_iSize, m_TElements);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">T&amp; ArrayList&lt;T&gt;::get(<span class="keyword">const</span> <span class="keyword">int</span> index) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">checkIndex(index);</span><br><span class="line"><span class="keyword">return</span> m_TElements[index];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">int</span> ArrayList&lt;T&gt;::indexOf(<span class="keyword">const</span> T&amp; element) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> index = (<span class="keyword">int</span>)(find(m_TElements, m_TElements + m_iSize, element) - m_TElements);</span><br><span class="line"><span class="comment">// 若未找到，返回-1</span></span><br><span class="line"><span class="keyword">return</span> index == m_iSize - <span class="number">1</span> ? <span class="number">-1</span> : index;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> ArrayList&lt;T&gt;::insert(<span class="keyword">const</span> <span class="keyword">int</span> index, <span class="keyword">const</span> T&amp; e)</span><br><span class="line">&#123;</span><br><span class="line">checkIndex(index);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 若数组已满，则倍增</span></span><br><span class="line"><span class="keyword">if</span> (m_iCount &gt;= m_iSize - <span class="number">1</span>) </span><br><span class="line">expend();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 元素后移</span></span><br><span class="line"><span class="built_in">std</span>::copy(m_TElements + index, m_TElements + m_iCount, m_TElements + index + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存元素，计数自增</span></span><br><span class="line">m_TElements[index] = e; m_iCount++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> ArrayList&lt;T&gt;::erase(<span class="keyword">const</span> <span class="keyword">int</span> index)</span><br><span class="line">&#123;</span><br><span class="line">checkIndex(index);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (index &gt; m_iCount - <span class="number">1</span>) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 前移元素</span></span><br><span class="line"><span class="built_in">std</span>::copy(m_TElements + index + <span class="number">1</span>, m_TElements + m_iCount, m_TElements + index);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计数自减</span></span><br><span class="line">m_iCount--;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> ArrayList&lt;T&gt;::print() <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m_iCount; i++)</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; m_TElements[i] &lt;&lt; <span class="string">" "</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> ArrayList&lt;T&gt;::expend()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> size = <span class="number">2</span> * m_iSize;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新辟内存空间，并复制</span></span><br><span class="line">T* elements = <span class="keyword">new</span> T[size];</span><br><span class="line"><span class="built_in">std</span>::copy(m_TElements, m_TElements + m_iSize, elements);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 释放原内存，重新指定</span></span><br><span class="line"><span class="keyword">delete</span> [] m_TElements;</span><br><span class="line">m_TElements = elements;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组信息</span></span><br><span class="line">m_iSize = size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主函数测试如下<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> size = <span class="number">3</span>;</span><br><span class="line">    ArrayList&lt;<span class="keyword">int</span>&gt; a1(size);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        a1.insert(i, <span class="number">2</span> * i);</span><br><span class="line">        a1.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        a1.erase(<span class="number">0</span>);</span><br><span class="line">        a1.print();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">0 2</span><br><span class="line">0 2 4</span><br><span class="line">2 4</span><br><span class="line">4</span><br></pre></td></tr></table></figure></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p><strong>优点</strong>：a)很多线性表的操作可以调用C++ 方法实现；b)时间性能良好，方法<code>indexOf</code>，<code>erase</code>，<code>insert</code>最坏时间复杂度与表达大小为线性关系。<br><strong>缺点</strong>：空间利用率低，在<code>expend</code>方法中，需开辟两倍新的内存空间并赋值后，才释放原有空间。</p><p>解决空间需求的一个方式是，把所有线性表映射到一个足够大的数组<code>array</code>中，再用数组<code>front</code>与<code>last</code>作为数组索引，构造多重表。其中<code>front[i]</code>与<code>last[i]</code>分别指定第<code>i</code>个表<code>a[i]</code>的第一个元素位置与最后一个元素位置。</p><p>需注意元素操作时，各表间元素相互干扰。</p><h2 id="链式描述"><a href="#链式描述" class="headerlink" title="链式描述"></a>链式描述</h2><p>定义：在<strong>链式描述</strong>中，对象实例的每一个元素都用一个单元或节点来描述，每个节点明确包含另一个节点的位置信息，该信息称作<strong>链(link)</strong>或<strong>指针(pointer)</strong>。</p><h3 id="具体实现-1"><a href="#具体实现-1" class="headerlink" title="具体实现"></a>具体实现</h3><p>链表操作即可实现，注意内存的申请与释放。</p><p>首先，构造双向链表的节点<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// arraylist.h</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChainNode</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">ChainNode() &#123; ptr = <span class="literal">nullptr</span>; prev = <span class="literal">nullptr</span>; next = <span class="literal">nullptr</span>; &#125;</span><br><span class="line">ChainNode(<span class="keyword">const</span> T val) &#123; ptr = <span class="keyword">new</span> T; *ptr = val; prev = <span class="literal">nullptr</span>; next = <span class="literal">nullptr</span>; &#125;</span><br><span class="line">~ChainNode() &#123; <span class="keyword">if</span> (ptr) <span class="keyword">delete</span> ptr; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setVal</span><span class="params">(T val)</span>  </span>&#123; <span class="keyword">if</span> (!ptr) ptr = <span class="keyword">new</span> T;  *ptr = val;&#125;</span><br><span class="line"><span class="function">T&amp; <span class="title">getVal</span><span class="params">()</span>       </span>&#123; <span class="keyword">if</span> (!ptr) <span class="keyword">throw</span> <span class="string">"invalid node"</span>; <span class="keyword">return</span> *ptr; &#125;</span><br><span class="line"></span><br><span class="line">ChainNode* prev;</span><br><span class="line">ChainNode* next;</span><br><span class="line">T* ptr;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// arraylist.h</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChainList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">// 构造、析构、拷贝构造</span></span><br><span class="line">ChainList() &#123; m_cnHead = <span class="keyword">new</span> ChainNode&lt;T&gt;; m_iCount = <span class="number">0</span>; &#125;</span><br><span class="line">ChainList(<span class="keyword">const</span> ChainList&lt;T&gt;&amp;);</span><br><span class="line">~ChainList();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组信息</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> !m_cnHead-&gt;ptr; &#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span>  <span class="title">size</span><span class="params">()</span>  <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> m_iCount; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 元素获取、查询</span></span><br><span class="line">ChainNode&lt;T&gt;* getNode(<span class="keyword">int</span> index) <span class="keyword">const</span>;</span><br><span class="line"><span class="function">T&amp;   <span class="title">get</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> index)</span>   <span class="keyword">const</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span>  <span class="title">indexOf</span><span class="params">(<span class="keyword">const</span> T&amp; value)</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> index, <span class="keyword">const</span> T&amp; value)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">erase</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> index)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">int</span> index)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (index &lt; <span class="number">0</span> || index &gt; m_iCount) <span class="keyword">throw</span> <span class="string">"无效的索引"</span>;&#125;</span><br><span class="line"></span><br><span class="line">ChainNode&lt;T&gt;* m_cnHead;</span><br><span class="line"><span class="keyword">int</span> m_iCount;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">ChainList&lt;T&gt;::ChainList(<span class="keyword">const</span> ChainList&lt;T&gt;&amp; <span class="built_in">list</span>)</span><br><span class="line">&#123;</span><br><span class="line">m_cnHead = <span class="keyword">new</span> ChainNode&lt;T&gt;;</span><br><span class="line">m_iCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">ChainNode&lt;T&gt;* node = <span class="built_in">list</span>.m_cnHead;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">list</span>.m_iCount &amp;&amp; node; i++) &#123;</span><br><span class="line">insert(i, node-&gt;getVal());</span><br><span class="line">node = node-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">ChainList&lt;T&gt;::~ChainList()</span><br><span class="line">&#123;</span><br><span class="line">ChainNode&lt;T&gt;* node = m_cnHead;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (!node-&gt;ptr &amp;&amp; !node) &#123;</span><br><span class="line">ChainNode&lt;T&gt;* tmp = node-&gt;next;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 释放节点空间</span></span><br><span class="line">node-&gt;~ChainNode();</span><br><span class="line"></span><br><span class="line"><span class="keyword">delete</span> node;</span><br><span class="line">node = tmp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">ChainNode&lt;T&gt;* ChainList&lt;T&gt;::getNode(<span class="keyword">const</span> <span class="keyword">int</span> index) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">checkIndex(index);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 根据索引，链式寻找节点</span></span><br><span class="line">ChainNode&lt;T&gt;* node = m_cnHead;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index; i++)node = node-&gt;next;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">T&amp; ChainList&lt;T&gt;::get(<span class="keyword">const</span> <span class="keyword">int</span> index) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">return</span> getNode(index)-&gt;getVal();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">int</span> ChainList&lt;T&gt;::indexOf(<span class="keyword">const</span> T&amp; value) <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">ChainNode&lt;T&gt;* node = m_cnHead;</span><br><span class="line"><span class="comment">// 搜索</span></span><br><span class="line"><span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (node-&gt;ptr &amp;&amp; node-&gt;getVal()!=value) &#123;</span><br><span class="line">node = node-&gt;next;</span><br><span class="line">index++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> index == m_iCount ? <span class="number">-1</span> : index;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> ChainList&lt;T&gt;::insert(<span class="keyword">const</span> <span class="keyword">int</span> index, <span class="keyword">const</span> T&amp; value)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 创建节点</span></span><br><span class="line">ChainNode&lt;T&gt;* tmp = <span class="keyword">new</span> ChainNode&lt;T&gt;(value);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取节点</span></span><br><span class="line">ChainNode&lt;T&gt;* node = getNode(index);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 连接，注意头节点的处理</span></span><br><span class="line"><span class="keyword">if</span> (node-&gt;prev) &#123;</span><br><span class="line">node-&gt;prev-&gt;next = tmp; </span><br><span class="line">tmp-&gt;prev = node-&gt;prev;</span><br><span class="line">&#125;</span><br><span class="line">tmp-&gt;next = node; </span><br><span class="line">node-&gt;prev = tmp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (index == <span class="number">0</span>) m_cnHead = tmp;</span><br><span class="line"></span><br><span class="line">m_iCount++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> ChainList&lt;T&gt;::erase(<span class="keyword">const</span> <span class="keyword">int</span> index)</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 获取节点</span></span><br><span class="line">ChainNode&lt;T&gt;* node = getNode(index);</span><br><span class="line"><span class="keyword">if</span> (index == <span class="number">0</span>) &#123;</span><br><span class="line">m_cnHead = node-&gt;next; m_cnHead-&gt;prev = <span class="literal">nullptr</span>;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 连接，释放，注意头节点的处理</span></span><br><span class="line"><span class="keyword">if</span>(node-&gt;prev) node-&gt;prev-&gt;next = node-&gt;next;</span><br><span class="line">node-&gt;~ChainNode();</span><br><span class="line"></span><br><span class="line">m_iCount--;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> ChainList&lt;T&gt;::print() <span class="keyword">const</span></span><br><span class="line">&#123;</span><br><span class="line">ChainNode&lt;T&gt;* node = m_cnHead;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (node-&gt;ptr &amp;&amp; node-&gt;next) &#123;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; node-&gt;getVal() &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">node = node-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主函数测试如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">// 创建链表</span></span><br><span class="line">ChainList&lt;<span class="keyword">int</span>&gt; c1;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"empty(1): "</span> &lt;&lt; c1.empty() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"size (1): "</span> &lt;&lt; c1.size() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 插入0, 2, 4</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++) &#123;</span><br><span class="line">c1.insert(i, <span class="number">2</span> * i);</span><br><span class="line">c1.print();</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"empty(2): "</span> &lt;&lt; c1.empty() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"size (2): "</span> &lt;&lt; c1.size() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"index of `2`: "</span> &lt;&lt; c1.indexOf(<span class="number">2</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按顺序删除2, 0</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</span><br><span class="line">c1.erase(<span class="number">1</span> - i);</span><br><span class="line">c1.print();</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"empty(3): "</span> &lt;&lt; c1.empty() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"size (3): "</span> &lt;&lt; c1.size() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">empty(1): 1</span><br><span class="line">size (1): 0</span><br><span class="line">0</span><br><span class="line">0 2</span><br><span class="line">0 2 4</span><br><span class="line">empty(2): 0</span><br><span class="line">size (2): 3</span><br><span class="line">index of `2`: 1</span><br><span class="line">0 4</span><br><span class="line">4</span><br><span class="line">empty(3): 0</span><br><span class="line">size (3): 1</span><br></pre></td></tr></table></figure></p><h3 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h3><ol><li><p>内存空间</p><p> 与数组描述的线性表不同，链式描述动态分配内存空间，故可较少<code>expend</code>操作时的内存占用。<br> 由于使用指针进行内存维护，对于<code>n</code>个元素的双向线性表，该链表共申请$n(12+s)$字节内存空间(单个元素指针占用$4 \times 3$字节，数据存储占用$s$字节)。空间需求不是决定因素。</p></li><li><p>运行时间</p><p> 链式描述的线性表时间复杂度为$\Theta(n)$，数组描述的为$\Theta(1)$。链表的插入位置是随机的，也即逻辑相邻的元素在空间上不一定相邻，这样导致很多高速缓存缺失。<br> 考虑时间复杂度，可使用<strong>双向链表</strong>(doubly linked list)，用两个数据成员<code>firstNode</code>与<code>lastNode</code>分别指向链表的头部和尾部，若查询下标<code>index</code>大于size/2，则从末端进行查找。</p></li><li><p>指针的优点</p><p> 操作灵活，且动态分配内存空间。</p></li></ol><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ol><li><p>箱子排序<br> 对n个存放在链表中的数据进行排序，假设每个箱子为一个链表，其节点数介于$0~n$之间，初始状态每个箱子都是空的。<br> a) 逐个删除输入链表的首元素，并将删除节点分配至相应箱子的链表首位；<br> b) 从最后一个箱子开始，逐个删除每个箱子的元素，并插入一个初始为空的链表首位。</p><p> 添加代码</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">ChainList&lt;<span class="keyword">int</span>&gt; ChainList&lt;T&gt;::binsort(<span class="keyword">int</span> range, T (*pFunc)(T))</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 创建箱子(存放下标，节省内存，但时间多用于查询)</span></span><br><span class="line">ChainList&lt;<span class="keyword">int</span>&gt;** bins = <span class="keyword">new</span> ChainList&lt;<span class="keyword">int</span>&gt;* [range];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; range; i++)&#123;</span><br><span class="line">bins[i] = <span class="keyword">new</span> ChainList&lt;<span class="keyword">int</span>&gt;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据划入箱子</span></span><br><span class="line"><span class="keyword">for</span> (ChainNode&lt;T&gt;* node = m_cnHead; node-&gt;ptr; node = node-&gt;next) &#123;</span><br><span class="line">T val = node-&gt;getVal();</span><br><span class="line"><span class="keyword">int</span> index = pFunc? (*pFunc)(val): val;</span><br><span class="line">bins[index]-&gt;insert(<span class="number">0</span>, indexOf(val));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; range; i++) &#123;</span><br><span class="line">bins[i]-&gt;print();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重新整合顺序，存储于链表</span></span><br><span class="line">ChainList&lt;<span class="keyword">int</span>&gt; order;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; range; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (ChainNode&lt;<span class="keyword">int</span>&gt;* node = bins[i]-&gt;m_cnHead; node-&gt;ptr; node = node-&gt;next) &#123;</span><br><span class="line">order.insert(order.size(), node-&gt;getVal());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">delete</span> bins[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">delete</span> bins;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> order;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">ChainList&lt;T&gt; ChainList&lt;T&gt;::ordered(<span class="keyword">const</span> ChainList&lt;<span class="keyword">int</span>&gt; order)</span><br><span class="line">&#123;</span><br><span class="line">ChainList&lt;T&gt; output; <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (ChainNode&lt;<span class="keyword">int</span>&gt;* node = order.m_cnHead; node-&gt;ptr; node = node-&gt;next) &#123;</span><br><span class="line">output.insert(cnt++, get(node-&gt;getVal()));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> output;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 主程序测试：</p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Hello World!"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">ChainList&lt;<span class="keyword">int</span>&gt; c;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">c.insert(<span class="number">0</span>, <span class="number">53</span> * i % <span class="number">50</span>);</span><br><span class="line">&#125;</span><br><span class="line">c.print();</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>(*pf1)(<span class="keyword">int</span>) = f1;</span><br><span class="line">ChainList&lt;<span class="keyword">int</span>&gt; order = c.binsort(<span class="number">50</span>, pf1);</span><br><span class="line">ChainList&lt;<span class="keyword">int</span>&gt; ordered = c.ordered(order);</span><br><span class="line">ordered.print();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">47 44 41 38 35 32 29 26 23 20 17 14 11 8 5 2 49 46 43 40 37 34 31 28 25 22 19 16 13 10 7 4 1 48 45 42 39 36 33 30 27 24 21 18 15 12 9 6 3 0 47 44 41 38 35 32 29 26 23 20 17 14 11 8 5 2 49 46 43 40 37 34 31 28 25 22 19 16 13 10 7 4 1 48 45 42 39 36 33 30 27 24 21 18 15 12 9 6 3 0</span><br><span class="line">0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43 44 44 45 45 46 46 47 47 48 48 49 49</span><br></pre></td></tr></table></figure><p> 对于具有$m$级大小关系的$n$个数据，该排序算法需初始化$m$个空箱子。在箱子排序基础上进行改进，可得到基数排序。</p></li><li><p>基数排序<br> 基于箱子排序，基数排序可在$\Theta(n)$时间内，对$0~n^c-1$之间的$n$个整数进行排序。具体算法为：按从低到高的位数顺序，按数据当前位数字大小进行箱子排序，整数范围位于$0~9$之间，故仅需初始化$10$个箱子即可。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【算法】程序性能分析与渐近记号</title>
      <link href="/2020/02/16/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B8%8E%E6%B8%90%E8%BF%91%E8%AE%B0%E5%8F%B7/"/>
      <url>/2020/02/16/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B8%8E%E6%B8%90%E8%BF%91%E8%AE%B0%E5%8F%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="程序性能"><a href="#程序性能" class="headerlink" title="程序性能"></a>程序性能</h1><p><strong>程序性能</strong>(performance of a program)是指运行这个程序所需要的内存和时间的多少。</p><h2 id="空间复杂度"><a href="#空间复杂度" class="headerlink" title="空间复杂度"></a>空间复杂度</h2><p><strong>定义</strong>：<strong>空间复杂度</strong>(space complexity)是该程序的运行所需内存的大小。</p><p>程序需要的空间主要以以下部分组成</p><ol><li><p>指令空间(instruction space)<br> 是指编译后的程序指令所需要的存储空间，取决于以下因素</p><ul><li>编译器；</li><li>编译选项；</li><li>目标计算机。</li></ul></li><li><p>数据空间(data space)<br>是所有常量和变量值所需要的存储空间，由</p><ul><li>常量和简单变量所需要的存储空间；</li><li>动态数组和动态类实例等动态对象所需要的空间。</li></ul></li><li><p>环境栈空间(environment stack space)<br> 保存暂停的函数和方法在恢复运行时所需要的信息：</p><ul><li>返回地址；</li><li>正在调用的函数的所有局部变量的值以及形式参数的值(仅对递归函数而言)。</li></ul></li></ol><p>程序所需要的空间可分成两部分</p><ol><li>固定部分，独立于实例特征，包括上述指令空间、简单变量空间、常量空间等；</li><li>可变部分，由动态分配空间(某种程度上依赖实力特征)和递归栈空间(依赖实力特征)构成。</li></ol><p>将空间需求的固定部分记作$c$，可变部分记作$S_p$，那么所需空间为$c + S_p(实例特征)$。</p><h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p><strong>定义</strong>：<strong>时间复杂度</strong>(time complexity)是该程序的运行所需要的时间。</p><h3 id="操作计数"><a href="#操作计数" class="headerlink" title="操作计数"></a>操作计数</h3><p>估算程序或函数的时间复杂度，一种方法是选择一种或多种<strong>关键操作</strong>，如加、乘、比较等，确定每一种操作的<strong>执行次数</strong>，注意操作计数忽视了其他操作。以选择排序和冒泡排序为例，选择排序与冒泡排序都需要$\sum_{i=1}^{n-1} i = n(n-1)/2$次比较和$3(n-1)$次移动。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">selectionSort</span><span class="params">(T a[], <span class="keyword">int</span> n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">1</span>; i--) &#123;    <span class="comment">// n - 1次</span></span><br><span class="line"><span class="keyword">int</span> index = i;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--)&#123;   <span class="comment">// i次比较</span></span><br><span class="line"><span class="keyword">if</span> (a[j] &gt; a[index]) index = j;</span><br><span class="line">        &#125;</span><br><span class="line">T temp = a[i];                      <span class="comment">// 3次移动</span></span><br><span class="line">        a[i] = a[index]; </span><br><span class="line">        a[index] = temp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubbleSort</span><span class="params">(T a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">1</span>; i--) &#123;      <span class="comment">// n - 1次循环</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123; </span><br><span class="line"><span class="keyword">if</span> (a[j] &gt; a[j + <span class="number">1</span>]) &#123;          <span class="comment">// i次比较</span></span><br><span class="line">T temp = a[j];              <span class="comment">// 3次移动</span></span><br><span class="line">                a[j] = a[j + <span class="number">1</span>]; </span><br><span class="line">                a[j + <span class="number">1</span>] = temp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>操作计数不总是由实例特征唯一确定</strong>，如冒泡排序中，交换次数不仅依赖于实例特征$n$，还依赖于数组元素的具体值，所以就要估算最好、最坏和平均操作计数。</p><h3 id="步数"><a href="#步数" class="headerlink" title="步数"></a>步数</h3><p><strong>步数</strong>(step-count)方法中，将程序/函数的所有操作部分都进行统计。<strong>步数也是实例特征的函数</strong>，为确定程序的步数，必须<strong>先确定所采用的实例特征</strong>，如输入个数、输出个数、输入或输出的大小，不仅<strong>确定步数计算表达式中的变量</strong>，还确定了<strong>一步应该包含多少次计算</strong>。<strong>一步</strong>(a step)是独立于所选定实例特征的一个计算单位，例如$100$次乘法可视作一步，但$n$次乘法包含实例特征不能视作一步。</p><p><strong>定义</strong>：<strong>程序步</strong>(a program step)可以大概定义为<strong>一个语法或语义上的程序片段</strong>，该片段执行时间独立于实例特征，例如下面语句中变量均独立于实例特征，可视作一个程序步：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> a + b + b * c + (a + b - c) / (a + b) + <span class="number">4</span>;</span><br></pre></td></tr></table></figure></p><blockquote><ul><li>故而相同步数的程序可能差别很大；</li><li>步数与时间复杂度是同义词。</li></ul></blockquote><p>用剖析法计算程序步数时，记$s/e$为每条语句每次执行所需要的步数(steps pre execution)，$f$为语句的执行次数或频率，进行列表计算，例如对于冒泡排序<br>| 程序语句 | s/e | 频率 | 总步数 |<br>| ———- | —- | —— | ——- |<br>| template\<typename t\=""> | / | / | / |<br>| void bubbleSort(T a[], int n) { | / | / | / |<br>|      for (int i = n - 1; i &gt;= 1; i—) { | $1$ | $(n - 1) + 1$ | $n$ |<br>|         for (int j = 0; j &lt; i; j++) {  | $i+1$ | $n-1$ | $n(n-1)/2 + (n-1)$ |<br>|             if (a[j] &gt; a[j + 1]) { T temp = a[j]; a[j] = a[j + 1];  a[j + 1] = temp; } | $i$ | $n-1$ | $n(n-1)/2$ |<br>|         }}} | / | / | / |<br>| 总计 | / | / | $n^2 + n - 1$ |</typename></p><h1 id="渐近记号"><a href="#渐近记号" class="headerlink" title="渐近记号"></a>渐近记号</h1><p>当<strong>实例特征$n$很大($n \rightarrow \infty$)时</strong>，使用步数可以准确地预计运行时间的增长，以比较两个程序的性能。当$n \rightarrow \infty$时，最高次项在整个多项式中占主导，可省略$c_2 n + c_3$两项。<strong>渐近分析主要确定的时复杂函数中的最大项</strong>。</p><blockquote><p>例如，对于任何具有形式$c_1 n^2 + c_3 + c_2 n + c_3$,($c_1 &gt; 0$且$c_1, c_2, c_3$为常数)的步数的程序，都有</p><script type="math/tex; mode=display">\lim_{n \rightarrow \infty} \frac{c_2 n + c_3}{c_1 n^2} = 0</script></blockquote><p><strong>定义</strong>：令$p(n)$和$q(n)$是两个<strong>非负函数</strong>，那么当且仅当</p><script type="math/tex; mode=display">\lim_{n \rightarrow \infty} \frac{q(n)}{p(n)} = 0</script><p>称$p(n)$<strong>渐近地大于</strong>$q(n)$($p(n)$渐近地优于$q(n)$)，或$q(n)$<strong>渐近地小于</strong>$p(n)$；当且仅当任何一个都不是渐近大于另一个时，称$p(n)$<strong>渐近地等于</strong>$q(n)$。</p><p>步数分析中常见的项如下标，<strong>大小关系</strong>从上往下依次减小<br>| 项 | 名称 |<br>| — | —- |<br>| $1$ | 常量 |<br>| $\log n$ | 对数 |<br>| $n$ | 线性 |<br>| $n \log n$ | $n$倍对数 |<br>| $n^2$ | 平方 |<br>| $n^3$ | 立方 |<br>| $2^n$ | 指数 |<br>| $n!$ | 阶乘 |</p><p><strong>定义</strong>：<strong>渐近记法</strong>(asymptotic notation)描述的是大实例特征的时间或空间复杂度，用步数中<strong>渐近最大</strong>的一项的<strong>单位项</strong>来描述复杂度。<strong>几种常见的渐近记法</strong>如下<br>| 表示法 | 读法 | 含义 | 例 |<br>| ——- | —— | —— | — |<br>| $f(n) = O(g(n))$ | $f(n)$ is big O of $g(n)$ | $f(n)$渐近小于或等于$f(n)$，$g(n)$是$f(n)$的上限 | $100n^2 - 20 n + 3 = O(n^2) \neq O(n)$ |<br>| $f(n) = \Omega(g(n))$ | $f(n)$ is big omega of $g(n)$ | $f(n)$渐近大于或等于$f(n)$，$g(n)$是$f(n)$的下限 | $100n^2 - 20 n + 3 = \Omega(n^2) = \Omega(n) \neq \Omega(n^3)$ |<br>| $f(n) = \Theta(g(n))$ | $f(n)$ is big theta of $g(n)$ | $f(n)$渐近等于$f(n)$，$g(n)$是$f(n)$的上限 | $100n^2 - 20 n + 3 = \Theta(n^2) \neq \Theta(n) \neq \Theta(n^3)$ | </p><blockquote><p>注意$f(n) = O(g(n))$与$O(g(n)) = f(n)$含义不同。</p></blockquote><p>仍以冒泡排序为例，总的时间复杂度为$\Theta(n^2)$。</p><div class="table-container"><table><thead><tr><th>程序语句</th><th>s/e</th><th>频率</th><th>总步数</th></tr></thead><tbody><tr><td>template\<typename t\=""></typename></td><td>/</td><td>/</td><td>/</td></tr><tr><td>void bubbleSort(T a[], int n) {</td><td>/</td><td>/</td><td>/</td></tr><tr><td>for (int i = n - 1; i &gt;= 1; i—) {</td><td>$1$</td><td>$(n - 1) + 1$</td><td>$\Theta(n)$</td></tr><tr><td>for (int j = 0; j &lt; i; j++) {</td><td>$i+1$</td><td>$n-1$</td><td>$\Theta(n^2)$</td></tr><tr><td>if (a[j] &gt; a[j + 1]) { T temp = a[j]; a[j] = a[j + 1];  a[j + 1] = temp; }</td><td>$i$</td><td>$n-1$</td><td>$\Theta(n^2)$</td></tr><tr><td>}}}</td><td>/</td><td>/</td><td>/</td></tr><tr><td>总计</td><td>/</td><td>/</td><td>$\Theta(n^2)$</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习框架darknet【五】——训练解析</title>
      <link href="/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%94%E3%80%91%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%94%E3%80%91%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>第三节介绍了如何<strong>调用函数</strong>进行网络的训练，本节扒一扒源码，详细介绍前向、反向、更新部分。查看本节内容要有BP算法的基础，以全连接网络为例详细推导过程查看<a href="https://louishsu.xyz/2019/12/08/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9CBP%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%E3%80%90%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E3%80%91/" target="_blank" rel="noopener">全连接网络BP算法推导【矩阵形式】</a>。</p><h1 id="Dataflow"><a href="#Dataflow" class="headerlink" title="Dataflow"></a>Dataflow</h1><p>运算过程中最重要的是数据的传递，在整个过程中，向系统申请的内存资源(部分)列表如下</p><div class="table-container"><table><thead><tr><th style="text-align:left">内存地址指针</th><th style="text-align:left">类型</th><th style="text-align:left">功能</th></tr></thead><tbody><tr><td style="text-align:left"><code>data.X</code></td><td style="text-align:left">matrix</td><td style="text-align:left">输入数据，以二维矩阵存储，每行表示一个样本</td></tr><tr><td style="text-align:left"><code>data.y</code></td><td style="text-align:left">matrix</td><td style="text-align:left">真实标签，以二维矩阵存储，每行表示一个样本</td></tr><tr><td style="text-align:left"><code>net-&gt;input</code></td><td style="text-align:left">float*</td><td style="text-align:left">用于层间<strong>输入数据传递</strong>的指针：(1)初始化为数据读取地址<code>d.X</code>； (2)在forward时更新为第i层输出作为第i+1层输入； (3)在backward时更新为第i+1层输出用于第i层的参数梯度计算。若为特征图，以<code>CHW</code>格式储存</td></tr><tr><td style="text-align:left"><code>net-&gt;delta</code></td><td style="text-align:left">float*</td><td style="text-align:left">用于层间<strong>输出梯度传递</strong>的指针，实现链式求导：在backward时更新为第i+1层输出梯度用于第i层的参数梯度计算</td></tr><tr><td style="text-align:left"><code>net-&gt;truth</code></td><td style="text-align:left">float*</td><td style="text-align:left"><strong>真实标签</strong>数据指针：初始化为<code>d.y</code>(groundtruth)；在损失函数层进行损失计算</td></tr><tr><td style="text-align:left"><code>net-&gt;workspace</code></td><td style="text-align:left">float*</td><td style="text-align:left">运算时可使用的<strong>临时工作区</strong>，如在卷积运算时需要将特征图展开为矩阵形式</td></tr><tr><td style="text-align:left"><code>l.output</code></td><td style="text-align:left">float*</td><td style="text-align:left">网络层的输出，若为特征图，以<code>CHW</code>格式储存</td></tr><tr><td style="text-align:left"><code>l.delta</code></td><td style="text-align:left">float*</td><td style="text-align:left">网络层<strong>输出对输入</strong>的梯度</td></tr><tr><td style="text-align:left"><code>l.weights</code></td><td style="text-align:left">float*</td><td style="text-align:left">网络层的权重，若为特征图，以<code>CHW</code>格式储存</td></tr><tr><td style="text-align:left"><code>l.weight_updates</code></td><td style="text-align:left">float*</td><td style="text-align:left">网络层权重的梯度值，由链式法则可知由<code>l.delta</code>与<code>l.weights</code>计算得到</td></tr><tr><td style="text-align:left"><code>l.biases</code></td><td style="text-align:left">float*</td><td style="text-align:left">网络层的偏置</td></tr><tr><td style="text-align:left"><code>l.bias_updates</code></td><td style="text-align:left">float*</td><td style="text-align:left">网络层偏置的梯度值</td></tr></tbody></table></div><p>示意图如下<br><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/network.jpg" alt="network"></p><h1 id="Entry"><a href="#Entry" class="headerlink" title="Entry"></a>Entry</h1><p>训练的入口函数为<code>train_netwok</code>，获取一批次数据并进行训练，数据由<code>(float*)d.X</code>、<code>(float*)d.y</code>拷贝至<code>(float*)net-&gt;input</code>、<code>(float*)net-&gt;truth</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">train_network</span><span class="params">(network *net, data d)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    assert(d.X.rows % net-&gt;batch == <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">int</span> batch = net-&gt;batch;</span><br><span class="line">    <span class="keyword">int</span> n = d.X.rows / batch;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">        get_next_batch(d, batch, i*batch, net-&gt;input, net-&gt;truth);  <span class="comment">// d.X =&gt; net-&gt;input; d.y =&gt; net-&gt;truth</span></span><br><span class="line">        <span class="keyword">float</span> err = train_network_datum(net);</span><br><span class="line">        sum += err;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">float</span>)sum/(n*batch);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">train_network_datum</span><span class="params">(network *net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    *net-&gt;seen += net-&gt;batch;</span><br><span class="line">    net-&gt;train = <span class="number">1</span>;</span><br><span class="line">    forward_network(net);       <span class="comment">// 网络前向，计算各层输出</span></span><br><span class="line">    backward_network(net);      <span class="comment">// 网络反向，计算各层参数梯度</span></span><br><span class="line">    <span class="keyword">float</span> error = *net-&gt;cost;</span><br><span class="line">    <span class="keyword">if</span>(((*net-&gt;seen)/net-&gt;batch)%net-&gt;subdivisions == <span class="number">0</span>) </span><br><span class="line">        update_network(net);    <span class="comment">// 更新各层参数</span></span><br><span class="line">    <span class="keyword">return</span> error;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h1><p>网络前向运算会依次调用各层的<code>forward</code>函数，通过指针<code>net-&gt;input</code>实现两层间数据传递，第$i$层将<code>net-&gt;input</code>作为输入进行运算，结果保存在<code>l.output</code>，之后将<code>net-&gt;input</code>更新为<code>l.output</code>完成地址传递，用作第$i+1$层的输入。</p><p>那么有疑问，<code>net-&gt;input</code>在创建网络时是分配了空间的，直接替换指针不会造成内存泄露吗？注意到<code>network* netp</code>是原始网络，在前向运算前先定义了局部变量<code>network net</code>，所以运算结束改变的是<code>net.input</code>而不是<code>netp-&gt;input</code>。</p><p>在第$1$层时，先读取<code>d.X</code>作为输入，第$i$层接受第$i-1$层输出<code>net-&gt;input</code>，与自身参数<code>l.weights</code>、<code>l.biases</code>运算得到输出，结果保存在<code>l.output</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_network</span><span class="params">(network *netp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> GPU</span></span><br><span class="line">    <span class="keyword">if</span>(netp-&gt;gpu_index &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        forward_network_gpu(netp);   </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    network net = *netp;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; net.n; ++i)&#123;</span><br><span class="line">        net.index = i;</span><br><span class="line">        layer l = net.layers[i];</span><br><span class="line">        <span class="keyword">if</span>(l.delta)&#123;</span><br><span class="line">            fill_cpu(l.outputs * l.batch, <span class="number">0</span>, l.delta, <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        l.forward(l, net);      <span class="comment">// 各层前向</span></span><br><span class="line">        net.input = l.output;   <span class="comment">// 指针传递</span></span><br><span class="line">        <span class="keyword">if</span>(l.truth) &#123;</span><br><span class="line">            net.truth = l.output;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    calc_network_cost(netp);    <span class="comment">// 获取总体损失</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/forward_layer_0.jpg" alt="forward_layer_0"></p><p><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/forward_layer_i.jpg" alt="forward_layer_i"></p><p>以全连接层为例，在线性运算后经激活函数得到最终输出，<code>gemm</code>为BLAS函数，为通用的矩阵乘法接口</p><script type="math/tex; mode=display">l.output_{l.batch \times l.outputs} = net.input_{l.batch \times l.inputs} l.weights_{l.outputs \times l.inputs}^T + l.biases_{l.outputs, broadcast}</script><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_connected_layer</span><span class="params">(layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    fill_cpu(l.outputs*l.batch, <span class="number">0</span>, l.output, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> m = l.batch;</span><br><span class="line">    <span class="keyword">int</span> k = l.inputs;</span><br><span class="line">    <span class="keyword">int</span> n = l.outputs;</span><br><span class="line">    <span class="keyword">float</span> *a = net.input;</span><br><span class="line">    <span class="keyword">float</span> *b = l.weights;</span><br><span class="line">    <span class="keyword">float</span> *c = l.output;</span><br><span class="line">    gemm(<span class="number">0</span>,<span class="number">1</span>,m,n,k,<span class="number">1</span>,a,k,b,k,<span class="number">1</span>,c,n);</span><br><span class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</span><br><span class="line">        forward_batchnorm_layer(l, net);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        add_bias(l.output, l.biases, l.batch, l.outputs, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    activate_array(l.output, l.outputs*l.batch, l.activation);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>损失函数层有所不同，运算示意图如下，除计算该层输出(损失)保存于<code>l.output</code>，最终损失<code>l.cost</code>为该批次样本损失的总和，计算<strong>输出对输入的梯度</strong>保存于<code>l.delta</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_cost_layer</span><span class="params">(cost_layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!net.truth) <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">if</span>(l.cost_type == MASKED)&#123;</span><br><span class="line">        <span class="keyword">int</span> i;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch*l.inputs; ++i)&#123;</span><br><span class="line">            <span class="keyword">if</span>(net.truth[i] == SECRET_NUM) net.input[i] = SECRET_NUM;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(l.cost_type == SMOOTH)&#123;</span><br><span class="line">        smooth_l1_cpu(l.batch*l.inputs, net.input, net.truth, l.delta, l.output);</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(l.cost_type == L1)&#123;</span><br><span class="line">        l1_cpu(l.batch*l.inputs, net.input, net.truth, l.delta, l.output);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        l2_cpu(l.batch*l.inputs, net.input, net.truth, l.delta, l.output);</span><br><span class="line">    &#125;</span><br><span class="line">    l.cost[<span class="number">0</span>] = sum_array(l.output, l.batch*l.inputs);  <span class="comment">// 各样本损失总和</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/forward_layer_cost.jpg" alt="forward_layer_cost"></p><h1 id="Backward"><a href="#Backward" class="headerlink" title="Backward"></a>Backward</h1><p>依次调用各层的<code>backward</code>函数，对于第$i$层的反向运算，需要将第$i-1$层输出<code>prev.output</code>与梯度<code>prev.delta</code>传递到<code>net.input</code>与<code>net.delta</code>。实际上，由于<strong>指针的特性</strong>，此时<code>net.input</code>即前层输出<code>prev.output</code>，<code>net.delta</code>即前层梯度<code>prev.delta</code>，<strong>在第$i$层反向时，需依靠第$i-1$层输出，并对第$i-1$层梯度进行更新</strong>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_network</span><span class="params">(network *netp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> GPU</span></span><br><span class="line">    <span class="keyword">if</span>(netp-&gt;gpu_index &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        backward_network_gpu(netp);   </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    network net = *netp;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    network orig = net;</span><br><span class="line">    <span class="keyword">for</span>(i = net.n<span class="number">-1</span>; i &gt;= <span class="number">0</span>; --i)&#123;</span><br><span class="line">        layer l = net.layers[i];</span><br><span class="line">        <span class="keyword">if</span>(l.stopbackward) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span>(i == <span class="number">0</span>)&#123;</span><br><span class="line">            net = orig;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            layer prev = net.layers[i<span class="number">-1</span>];</span><br><span class="line">            net.input = prev.output;</span><br><span class="line">            net.delta = prev.delta;</span><br><span class="line">        &#125;</span><br><span class="line">        net.index = i;</span><br><span class="line">        l.backward(l, net);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>损失函数层为反向运算时的第$1$层，运算较简单</p><script type="math/tex; mode=display">net.delta += l.scale \times l.delta</script><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_cost_layer</span><span class="params">(<span class="keyword">const</span> cost_layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    axpy_cpu(l.batch*l.inputs, l.scale, l.delta, <span class="number">1</span>, net.delta, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>示意图如下<br><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/backward_layer_cost_1.jpg" alt="backward_layer_cost_1"></p><p>如果省略指针传递，那么简化的运算如下</p><script type="math/tex; mode=display">l^{prev}.delta += l.scale \times l.delta</script><p><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/backward_layer_cost_2.jpg" alt="backward_layer_cost_2"></p><p>其余各层调用各自的反传函数，先计算输出对输入的梯度<code>l.delta</code>，利用<code>net.input</code>与<code>l.delta</code>计算<code>l.weight_updates</code>、<code>l.bias_updates</code>，之后更新<code>net.delta</code>，示意图如下<br><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/backward_layer_i_1.jpg" alt="backward_layer_i_1"></p><p>如果省略指针传递，那么简化的运算如下<br><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/backward_layer_i_2.jpg" alt="backward_layer_i_2"></p><p>以全连接层为例</p><script type="math/tex; mode=display">\begin{cases}    l.weight\_updates_{l.outputs \times l.inputs} = l.delta_{l.batch \times l.outputs}^T net.input_{l.batch \times l.inputs} \\    l.bias\_updates_{l.outputs} += \sum_{i}^{l.batch} (l.delta^{(i)}_{l.outputs}) \\    l^{prev}.delta_{l.batch \times l.inputs} = l.delta_{l.batch \times l.outputs} l.weights_{l.outputs \times l.inputs}\end{cases}</script><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_connected_layer</span><span class="params">(layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    gradient_array(l.output, l.outputs*l.batch, l.activation, l.delta);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</span><br><span class="line">        backward_batchnorm_layer(l, net);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        backward_bias(l.bias_updates, l.delta, l.batch, l.outputs, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> m = l.outputs;</span><br><span class="line">    <span class="keyword">int</span> k = l.batch;</span><br><span class="line">    <span class="keyword">int</span> n = l.inputs;</span><br><span class="line">    <span class="keyword">float</span> *a = l.delta;</span><br><span class="line">    <span class="keyword">float</span> *b = net.input;</span><br><span class="line">    <span class="keyword">float</span> *c = l.weight_updates;</span><br><span class="line">    gemm(<span class="number">1</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,m,b,n,<span class="number">1</span>,c,n);</span><br><span class="line"></span><br><span class="line">    m = l.batch;</span><br><span class="line">    k = l.outputs;</span><br><span class="line">    n = l.inputs;</span><br><span class="line"></span><br><span class="line">    a = l.delta;</span><br><span class="line">    b = l.weights;</span><br><span class="line">    c = net.delta;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(c) gemm(<span class="number">0</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,k,b,n,<span class="number">1</span>,c,n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h1><p>更新这一步比较简单，调用优化器参数，将当前已计算得到的参数梯度累加到网络层参数上即可</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">update_network</span><span class="params">(network *netp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> GPU</span></span><br><span class="line">    <span class="keyword">if</span>(netp-&gt;gpu_index &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        update_network_gpu(netp);   </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    network net = *netp;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    update_args a = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    a.batch = net.batch*net.subdivisions;</span><br><span class="line">    a.learning_rate = get_current_rate(netp);</span><br><span class="line">    a.momentum = net.momentum;</span><br><span class="line">    a.decay = net.decay;</span><br><span class="line">    a.adam = net.adam;</span><br><span class="line">    a.B1 = net.B1;</span><br><span class="line">    a.B2 = net.B2;</span><br><span class="line">    a.eps = net.eps;</span><br><span class="line">    ++*net.t;</span><br><span class="line">    a.t = *net.t;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; net.n; ++i)&#123;</span><br><span class="line">        layer l = net.layers[i];</span><br><span class="line">        <span class="keyword">if</span>(l.update)&#123;</span><br><span class="line">            l.update(l, a);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/update_layer_0.jpg" alt="update_layer_0"></p><p><img src="/2019/12/08/深度学习框架darknet【五】——训练解析/update_layer_n_1.jpg" alt="update_layer_n_1"></p><p>以全连接层为例</p><script type="math/tex; mode=display">\begin{cases}    l.biases_{l.outputs} += l.bias\_updates_{l.outputs} \times \frac{learning\_rate}{batch} \\    l.weight\_updates_{l.outputs \times l.inputs} -= l.weights_{l.outputs \times l.inputs} \times decay \times batch \\    l.weights_{l.outputs \times l.inputs} += l.weight\_updates_{l.outputs \times l.inputs} \times \frac{learning\_rate}{batch} \\    l.weight\_updates_{l.outputs \times l.inputs} *= momentum \\    l.bias\_updates_{l.outputs} *= momentum\end{cases}</script><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">update_connected_layer</span><span class="params">(layer l, update_args a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> learning_rate = a.learning_rate*l.learning_rate_scale;</span><br><span class="line">    <span class="keyword">float</span> momentum = a.momentum;</span><br><span class="line">    <span class="keyword">float</span> decay = a.decay;</span><br><span class="line">    <span class="keyword">int</span> batch = a.batch;</span><br><span class="line">    axpy_cpu(l.outputs, learning_rate/batch, l.bias_updates, <span class="number">1</span>, l.biases, <span class="number">1</span>);</span><br><span class="line">    scal_cpu(l.outputs, momentum, l.bias_updates, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</span><br><span class="line">        axpy_cpu(l.outputs, learning_rate/batch, l.scale_updates, <span class="number">1</span>, l.scales, <span class="number">1</span>);</span><br><span class="line">        scal_cpu(l.outputs, momentum, l.scale_updates, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    axpy_cpu(l.inputs*l.outputs, -decay*batch, l.weights, <span class="number">1</span>, l.weight_updates, <span class="number">1</span>);</span><br><span class="line">    axpy_cpu(l.inputs*l.outputs, learning_rate/batch, l.weight_updates, <span class="number">1</span>, l.weights, <span class="number">1</span>);</span><br><span class="line">    scal_cpu(l.inputs*l.outputs, momentum, l.weight_updates, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习框架darknet【四】——网络配置选项</title>
      <link href="/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E5%9B%9B%E3%80%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9/"/>
      <url>/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E5%9B%9B%E3%80%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一些重要网络层的参数配置可以在<code>network *parse_network_cfg(char *filename)</code>函数中查看，进入对应层的解析函数<code>layer parse_xxx(list *options, size_params params)</code>，部分参数有缺省值。</p><p>配置文件书写，以alexnet为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">[net]</span><br><span class="line">batch=1</span><br><span class="line">subdivisions=1</span><br><span class="line">height=227</span><br><span class="line">width=227</span><br><span class="line">channels=3</span><br><span class="line">momentum=0.9</span><br><span class="line">decay=0.0005</span><br><span class="line">max_crop=256</span><br><span class="line"></span><br><span class="line">learning_rate=0.01</span><br><span class="line">policy=poly</span><br><span class="line">power=4</span><br><span class="line">max_batches=800000</span><br><span class="line"></span><br><span class="line">angle=7</span><br><span class="line">hue = .1</span><br><span class="line">saturation=.75</span><br><span class="line">exposure=.75</span><br><span class="line">aspect=.75</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">filters=96</span><br><span class="line">size=11</span><br><span class="line">stride=4</span><br><span class="line">pad=0</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size=3</span><br><span class="line">stride=2</span><br><span class="line">padding=0</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">filters=256</span><br><span class="line">size=5</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size=3</span><br><span class="line">stride=2</span><br><span class="line">padding=0</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">filters=384</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">filters=384</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">filters=256</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size=3</span><br><span class="line">stride=2</span><br><span class="line">padding=0</span><br><span class="line"></span><br><span class="line">[connected]</span><br><span class="line">output=4096</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[dropout]</span><br><span class="line">probability=.5</span><br><span class="line"></span><br><span class="line">[connected]</span><br><span class="line">output=4096</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[dropout]</span><br><span class="line">probability=.5</span><br><span class="line"></span><br><span class="line">[connected]</span><br><span class="line">output=1000</span><br><span class="line">activation=linear</span><br><span class="line"></span><br><span class="line">[softmax]</span><br><span class="line">groups=1</span><br></pre></td></tr></table></figure></p><h1 id="net"><a href="#net" class="headerlink" title="net"></a>net</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>batch</td><td>int</td><td>1</td><td>单个批次数据量</td></tr><tr><td>max_batches</td><td>int</td><td>0</td><td>训练最大迭代次数</td></tr><tr><td>learning_rate</td><td>float</td><td>.001</td><td>学习率</td></tr><tr><td>momentum</td><td>float</td><td>.9</td><td>动量</td></tr><tr><td>decay</td><td>float</td><td>.0001</td><td>权重衰减</td></tr><tr><td>subdivisions</td><td>int</td><td>1</td><td>对批次进行细分</td></tr><tr><td>time_steps</td><td>int</td><td>1</td><td>循环神经网络使用，时间步数</td></tr><tr><td>notruth</td><td>int</td><td>0</td><td>未知</td></tr><tr><td>random</td><td>int</td><td>0</td><td><code>classifier.c</code>中使用，是否随机改变图像载入参数</td></tr><tr><td>adam</td><td>int</td><td>0</td><td>是否使用adam优化器</td></tr><tr><td>B1</td><td>float</td><td>.9</td><td>adam优化器参数</td></tr><tr><td>B2</td><td>float</td><td>.999</td><td>adam优化器参数</td></tr><tr><td>eps</td><td>float</td><td>.0000001</td><td>adam优化器参数</td></tr><tr><td>height</td><td>int</td><td>0</td><td>输入图像高度，必须指定</td></tr><tr><td>width</td><td>int</td><td>0</td><td>输入图像宽度，必须指定</td></tr><tr><td>channels</td><td>int</td><td>0</td><td>输入图像通道数，必须指定</td></tr><tr><td>inputs</td><td>int</td><td>height <em> width </em> channels</td><td>输入图像尺寸</td></tr><tr><td>max_crop</td><td>int</td><td>width * 2</td><td>图像裁剪参数</td></tr><tr><td>min_crop</td><td>int</td><td>width</td><td>图像裁剪参数</td></tr><tr><td>max_ratio</td><td>float</td><td>max_crop / width</td><td>图像裁剪参数</td></tr><tr><td>min_ratio</td><td>float</td><td>max_crop / width</td><td>图像裁剪参数</td></tr><tr><td>center</td><td>int</td><td>0</td><td>未知</td></tr><tr><td>clip</td><td>float</td><td>0</td><td>未知</td></tr><tr><td>angle</td><td>float</td><td>0</td><td>图像扩增参数</td></tr><tr><td>aspect</td><td>float</td><td>1</td><td>图像扩增参数</td></tr><tr><td>saturation</td><td>float</td><td>1</td><td>图像扩增参数</td></tr><tr><td>exposure</td><td>float</td><td>1</td><td>图像扩增参数</td></tr><tr><td>hue</td><td>float</td><td>0</td><td>图像扩增参数</td></tr><tr><td>policy</td><td>char*</td><td>constant</td><td>学习率调整策略，可选<code>random</code>, <code>poly</code>, <code>constant</code>, <code>step</code>, <code>exp</code>, <code>sigmoid</code>, <code>steps</code></td></tr><tr><td>burn_in</td><td>int</td><td>0</td><td>学习率调整策略参数</td></tr><tr><td>power</td><td>float</td><td>4</td><td>学习率调整策略参数</td></tr><tr><td>step</td><td>int</td><td>1</td><td>学习率调整策略参数</td></tr><tr><td>scale</td><td>float</td><td>1</td><td>学习率调整策略参数</td></tr><tr><td>steps</td><td>int,  …</td><td>/</td><td>学习率调整策略参数</td></tr><tr><td>scales</td><td>float,…</td><td>/</td><td>学习率调整策略参数</td></tr><tr><td>gamma</td><td>float</td><td>1</td><td>学习率调整策略参数</td></tr></tbody></table></div><h1 id="layer"><a href="#layer" class="headerlink" title="layer"></a>layer</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>clip</td><td>float</td><td>net-&gt;clip</td><td>图像裁剪</td></tr><tr><td>truth</td><td>int</td><td>0</td><td>是否以输出作为groundtruth</td></tr><tr><td>onlyforward</td><td>int</td><td>0</td><td>仅前向</td></tr><tr><td>stopbackward</td><td>int</td><td>0</td><td>该层不进行反向</td></tr><tr><td>dontsave</td><td>int</td><td>0</td><td>不保存该层参数</td></tr><tr><td>dontload</td><td>int</td><td>0</td><td>不读取该层参数</td></tr><tr><td>numload</td><td>int</td><td>0</td><td>卷积层使用，指定卷积核个数</td></tr><tr><td>dontloadscales</td><td>int</td><td>0</td><td>不读取batchnorm参数</td></tr><tr><td>learning_rate</td><td>float</td><td>1</td><td>学习率，在网络学习率基础上累乘</td></tr><tr><td>smooth</td><td>float</td><td>0</td><td>平滑</td></tr></tbody></table></div><h1 id="convolutional"><a href="#convolutional" class="headerlink" title="convolutional"></a>convolutional</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>filters</td><td>int</td><td>1</td><td>卷积核个数</td></tr><tr><td>size</td><td>int</td><td>1</td><td>卷积核尺寸</td></tr><tr><td>stride</td><td>int</td><td>1</td><td>卷积步长</td></tr><tr><td>pad</td><td>int</td><td>0</td><td>卷积是否填充，若填充<code>padding=size/2</code></td></tr><tr><td>padding</td><td>int</td><td>0</td><td>卷积填充值，若<code>pad=1</code>则该值无效指定</td></tr><tr><td>groups</td><td>int</td><td>1</td><td>卷积组个数</td></tr><tr><td>activation</td><td>char*</td><td>logistic</td><td>激活函数类型</td></tr><tr><td>batch_normalize</td><td>int</td><td>0</td><td>是否使用batchnorm</td></tr><tr><td>binary</td><td>int</td><td>0</td><td>是否二值化参数</td></tr><tr><td>xnor</td><td>int</td><td>0</td><td>未知</td></tr><tr><td>flipped</td><td>int</td><td>0</td><td>未知</td></tr><tr><td>dot</td><td>int</td><td>float</td><td>未知</td></tr></tbody></table></div><h1 id="deconvolutional"><a href="#deconvolutional" class="headerlink" title="deconvolutional"></a>deconvolutional</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>filters</td><td>int</td><td>1</td><td>卷积核个数</td></tr><tr><td>size</td><td>int</td><td>1</td><td>卷积核尺寸</td></tr><tr><td>stride</td><td>int</td><td>1</td><td>卷积步长</td></tr><tr><td>activation</td><td>char*</td><td>logistic</td><td>激活函数类型</td></tr><tr><td>batch_normalize</td><td>int</td><td>0</td><td>是否使用batchnorm</td></tr><tr><td>pad</td><td>int</td><td>0</td><td>卷积是否填充，若填充<code>padding=size/2</code></td></tr><tr><td>padding</td><td>int</td><td>0</td><td>卷积填充值，若<code>pad=1</code>则该值无效指定</td></tr></tbody></table></div><h1 id="rnn"><a href="#rnn" class="headerlink" title="rnn"></a>rnn</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>output</td><td>int</td><td>1</td><td>输出维数</td></tr><tr><td>activation</td><td>char*</td><td>logistic</td><td>激活函数类型</td></tr><tr><td>batch_normalize</td><td>int</td><td>0</td><td>是否使用batchnorm</td></tr><tr><td>shortcut</td><td>int</td><td>0</td><td>是否使用shortcut</td></tr></tbody></table></div><h1 id="gru"><a href="#gru" class="headerlink" title="gru"></a>gru</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>output</td><td>int</td><td>1</td><td>输出维数</td></tr><tr><td>batch_normalize</td><td>int</td><td>0</td><td>是否使用batchnorm</td></tr><tr><td>tanh</td><td>int</td><td>0</td><td>是否使用tanh</td></tr></tbody></table></div><h1 id="lstm"><a href="#lstm" class="headerlink" title="lstm"></a>lstm</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>output</td><td>int</td><td>1</td><td>输出维数</td></tr><tr><td>batch_normalize</td><td>int</td><td>0</td><td>是否使用batchnorm</td></tr></tbody></table></div><h1 id="crnn"><a href="#crnn" class="headerlink" title="crnn"></a>crnn</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>output_filters</td><td>int</td><td>1</td><td>输出特征通道数</td></tr><tr><td>hidden_filters</td><td>int</td><td>1</td><td>隐层特征通道数</td></tr><tr><td>activation</td><td>char*</td><td>logistic</td><td>激活函数类型</td></tr><tr><td>batch_normalize</td><td>int</td><td>0</td><td>是否使用batchnorm</td></tr><tr><td>shortcut</td><td>int</td><td>0</td><td>是否使用shortcut</td></tr></tbody></table></div><h1 id="connected"><a href="#connected" class="headerlink" title="connected"></a>connected</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>output</td><td>int</td><td>1</td><td>输出维数</td></tr><tr><td>activation</td><td>char*</td><td>logistic</td><td>激活函数类型</td></tr><tr><td>batch_normalize</td><td>int</td><td>0</td><td>是否使用batchnorm</td></tr></tbody></table></div><h1 id="crop"><a href="#crop" class="headerlink" title="crop"></a>crop</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>crop_height</td><td>int</td><td>1</td><td>裁剪高度</td></tr><tr><td>crop_width</td><td>int</td><td>1</td><td>裁剪宽度</td></tr><tr><td>flip</td><td>int</td><td>0</td><td>是否翻转</td></tr><tr><td>angle</td><td>float</td><td>0</td><td>旋转角度</td></tr><tr><td>saturation</td><td>float</td><td>1</td><td>饱和度</td></tr><tr><td>exposure</td><td>float</td><td>1</td><td>曝光</td></tr></tbody></table></div><h1 id="cost"><a href="#cost" class="headerlink" title="cost"></a>cost</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>type</td><td>char*</td><td>sse</td><td>损失类型</td></tr><tr><td>scale</td><td>float</td><td>1</td><td>损失倍率</td></tr><tr><td>ratio</td><td>float</td><td>0</td><td>未知</td></tr><tr><td>noobj</td><td>float</td><td>1</td><td>未知</td></tr><tr><td>thresh</td><td>float</td><td>0</td><td>未知</td></tr></tbody></table></div><h1 id="batchnorm"><a href="#batchnorm" class="headerlink" title="batchnorm"></a>batchnorm</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>无</td><td>/</td><td>/</td><td>/</td></tr></tbody></table></div><h1 id="maxpool"><a href="#maxpool" class="headerlink" title="maxpool"></a>maxpool</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>stride</td><td>int</td><td>1</td><td>步长</td></tr><tr><td>size</td><td>int</td><td>stride</td><td>尺寸</td></tr><tr><td>padding</td><td>int</td><td>size - 1</td><td>填充</td></tr></tbody></table></div><h1 id="avgpool"><a href="#avgpool" class="headerlink" title="avgpool"></a>avgpool</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>无</td><td>/</td><td>/</td><td>/</td></tr></tbody></table></div><h1 id="upsample"><a href="#upsample" class="headerlink" title="upsample"></a>upsample</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>stride</td><td>int</td><td>2</td><td>步长</td></tr><tr><td>scale</td><td>float</td><td>1</td><td>未知</td></tr></tbody></table></div><h1 id="shortcut"><a href="#shortcut" class="headerlink" title="shortcut"></a>shortcut</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>from</td><td>int</td><td>/</td><td>指定来自某层的连接，可为相对层数如<code>from=-4</code>表示从4层前连接，或绝对层数如<code>from=4</code>表示第4层连接</td></tr><tr><td>activation</td><td>char*</td><td>linear</td><td>激活函数类型</td></tr><tr><td>alpha</td><td>float</td><td>1</td><td>详情查看<code>shortcut_layer.c/backward_shortcut_layer</code></td></tr><tr><td>beta</td><td>float</td><td>1</td><td>详情查看<code>shortcut_layer.c/backward_shortcut_layer</code></td></tr></tbody></table></div><h1 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h1><div class="table-container"><table><thead><tr><th>option</th><th>dtype</th><th>default</th><th>function</th></tr></thead><tbody><tr><td>probability</td><td>float</td><td>.5</td><td>dropout概率</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习框架darknet【三】——调包大法好</title>
      <link href="/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%89%E3%80%91%E2%80%94%E2%80%94%E8%B0%83%E5%8C%85%E5%A4%A7%E6%B3%95%E5%A5%BD/"/>
      <url>/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%89%E3%80%91%E2%80%94%E2%80%94%E8%B0%83%E5%8C%85%E5%A4%A7%E6%B3%95%E5%A5%BD/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本节以YOLO为例，介绍如何调用darknet函数来进行网络训练和测试，对框架有一个整体的认识。</p><h1 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h1><p>主函数入口在<code>examples/darknet.c/main</code>，命令行指定参数<code>yolo</code>即可进入yolo的运行函数段<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./darknet yolo</span></span><br><span class="line">usage: ./darknet yolo [train/test/valid] [cfg] [weights (optional)]</span><br></pre></td></tr></table></figure></p><p>由以下代码可知，可选参数2有</p><ul><li><code>test</code>：测试</li><li><code>train</code>：训练</li><li><code>valid</code>：验证</li><li><code>recall</code>：验证，计算召回率等指标</li><li><code>demo</code>：样机，视频演示，需OpenCV支持</li></ul><details><summary><font color="darkred">void run_yolo(int argc, char **argv)</font></summary><pre><code>void run_yolo(int argc, char **argv){    char *prefix = find_char_arg(argc, argv, "-prefix", 0);    float thresh = find_float_arg(argc, argv, "-thresh", .2);    int cam_index = find_int_arg(argc, argv, "-c", 0);    int frame_skip = find_int_arg(argc, argv, "-s", 0);    if(argc < 4){        fprintf(stderr, "usage: %s %s [train/test/valid] [cfg] [weights (optional)]\n", argv[0], argv[1]);        return;    }    int avg = find_int_arg(argc, argv, "-avg", 1);    char *cfg = argv[3];    char *weights = (argc > 4) ? argv[4] : 0;    char *filename = (argc > 5) ? argv[5]: 0;    if(0==strcmp(argv[2], "test")) test_yolo(cfg, weights, filename, thresh);    else if(0==strcmp(argv[2], "train")) train_yolo(cfg, weights);    else if(0==strcmp(argv[2], "valid")) validate_yolo(cfg, weights);    else if(0==strcmp(argv[2], "recall")) validate_yolo_recall(cfg, weights);    else if(0==strcmp(argv[2], "demo")) demo(cfg, weights, thresh, cam_index, filename, voc_names, 20, frame_skip, prefix, avg, .5, 0,0,0,0);}</code></pre></details><p><strong>本节只介绍基本的训练(train)与测试(test)，其余均为拓展。</strong></p><h1 id="Training-Stage"><a href="#Training-Stage" class="headerlink" title="Training Stage"></a>Training Stage</h1><p>指定参数2为<code>train</code>，即可进入训练函数段，提示无训练数据<code>/data/voc/train.txt</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./darknet yolo train cfg/yolov3.cfg yolov3.weights</span></span><br><span class="line">yolov3</span><br><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 / 1   608 x 608 x   3   -&gt;   608 x 608 x  32  0.639 BFLOPs</span><br><span class="line">    1 conv     64  3 x 3 / 2   608 x 608 x  32   -&gt;   304 x 304 x  64  3.407 BFLOPs</span><br><span class="line">... # 略</span><br><span class="line">  105 conv    255  1 x 1 / 1    76 x  76 x 256   -&gt;    76 x  76 x 255  0.754 BFLOPs</span><br><span class="line">  106 yolo</span><br><span class="line">Loading weights from yolov3.weights...Done!</span><br><span class="line">Learning Rate: 0.001, Momentum: 0.9, Decay: 0.0005</span><br><span class="line">Couldn't open file: /data/voc/train.txt</span><br></pre></td></tr></table></figure></p><p>在训练函数段内，主要做的工作有以下几个部分，分别进行说明</p><ul><li>网络构建</li><li>数据载入</li><li>训练，包括前向、反向、参数更新</li></ul><details><summary><font color="darkred">void train_yolo(char *cfgfile, char *weightfile)</font></summary><pre><code>void train_yolo(char *cfgfile, char *weightfile){    char *train_images = "/data/voc/train.txt";    char *backup_directory = "/home/pjreddie/backup/";    srand(time(0));    char *base = basecfg(cfgfile);    printf("%s\n", base);    float avg_loss = -1;    // 网络构建    network *net = load_network(cfgfile, weightfile, 0);    printf("Learning Rate: %g, Momentum: %g, Decay: %g\n", net->learning_rate, net->momentum, net->decay);    // 数据载入    int imgs = net->batch*net->subdivisions;    int i = *net->seen/imgs;    data train, buffer;    layer l = net->layers[net->n - 1];    int side = l.side;    int classes = l.classes;    float jitter = l.jitter;    list *plist = get_paths(train_images);    //int N = plist->size;    char **paths = (char **)list_to_array(plist);    load_args args = {0};    args.w = net->w;    args.h = net->h;    args.paths = paths;    args.n = imgs;    args.m = plist->size;    args.classes = classes;    args.jitter = jitter;    args.num_boxes = side;    args.d = &buffer;    args.type = REGION_DATA;    args.angle = net->angle;    args.exposure = net->exposure;    args.saturation = net->saturation;    args.hue = net->hue;    pthread_t load_thread = load_data_in_thread(args);    // 开始训练    clock_t time;    while(get_current_batch(net) < net->max_batches){        i += 1;        time=clock();        pthread_join(load_thread, 0);        train = buffer;        load_thread = load_data_in_thread(args);        printf("Loaded: %lf seconds\n", sec(clock()-time));        time=clock();        float loss = train_network(net, train);        if (avg_loss < 0) avg_loss = loss;        avg_loss = avg_loss*.9 + loss*.1;        printf("%d: %f, %f avg, %f rate, %lf seconds, %d images\n", i, loss, avg_loss, get_current_rate(net), sec(clock()-time), i*imgs);        if(i%1000==0 || (i < 1000 && i%100 == 0)){            char buff[256];            sprintf(buff, "%s/%s_%d.weights", backup_directory, base, i);            save_weights(net, buff);        }        free_data(train);    }    char buff[256];    sprintf(buff, "%s/%s_final.weights", backup_directory, base);    save_weights(net, buff);}</code></pre></details><h2 id="Load-Network"><a href="#Load-Network" class="headerlink" title="Load Network"></a>Load Network</h2><p>通过<code>load_network</code>可快速构建网络，来看一下该函数做了哪些事情<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network *net = load_network(cfgfile, weightfile, <span class="number">0</span>);</span><br></pre></td></tr></table></figure></p><details><summary><font color="darkred"> network *load_network(char *cfg, char *weights, int clear) </font></summary><pre><code>network *load_network(char *cfg, char *weights, int clear){    network *net = parse_network_cfg(cfg);    if(weights && weights[0] != 0){        load_weights(net, weights);    }    if(clear) (*net->seen) = 0;    return net;}</code></pre></details><ol><li><p>构建框架</p> <details> <summary><font color="darkred"> network *parse_network_cfg(char *filename) </font></summary> <pre><code> network *parse_network_cfg(char *filename) {     list *sections = read_cfg(filename);     node *n = sections->front;     if(!n) error("Config file has no sections");     network *net = make_network(sections->size - 1);     net->gpu_index = gpu_index;     size_params params;     section *s = (section *)n->val;     list *options = s->options;     if(!is_network(s)) error("First section must be [net] or [network]");     parse_net_options(options, net);     params.h = net->h;     params.w = net->w;     params.c = net->c;     params.inputs = net->inputs;     params.batch = net->batch;     params.time_steps = net->time_steps;     params.net = net;     size_t workspace_size = 0;     n = n->next;     int count = 0;     free_section(s);     fprintf(stderr, "layer     filters    size              input                output\n");     while(n){         params.index = count;         fprintf(stderr, "%5d ", count);         s = (section *)n->val;         options = s->options;         layer l = {0};         LAYER_TYPE lt = string_to_layer_type(s->type);         if(lt == CONVOLUTIONAL){             l = parse_convolutional(options, params);         }else if(lt == DECONVOLUTIONAL){             l = parse_deconvolutional(options, params);         }else if(lt == LOCAL){             l = parse_local(options, params);         }else if(lt == ACTIVE){             l = parse_activation(options, params);         }else if(lt == LOGXENT){             l = parse_logistic(options, params);         }else if(lt == L2NORM){             l = parse_l2norm(options, params);         }else if(lt == RNN){             l = parse_rnn(options, params);         }else if(lt == GRU){             l = parse_gru(options, params);         }else if (lt == LSTM) {             l = parse_lstm(options, params);         }else if(lt == CRNN){             l = parse_crnn(options, params);         }else if(lt == CONNECTED){             l = parse_connected(options, params);         }else if(lt == CROP){             l = parse_crop(options, params);         }else if(lt == COST){             l = parse_cost(options, params);         }else if(lt == REGION){             l = parse_region(options, params);         }else if(lt == YOLO){             l = parse_yolo(options, params);         }else if(lt == ISEG){             l = parse_iseg(options, params);         }else if(lt == DETECTION){             l = parse_detection(options, params);         }else if(lt == SOFTMAX){             l = parse_softmax(options, params);             net->hierarchy = l.softmax_tree;         }else if(lt == NORMALIZATION){             l = parse_normalization(options, params);         }else if(lt == BATCHNORM){             l = parse_batchnorm(options, params);         }else if(lt == MAXPOOL){             l = parse_maxpool(options, params);         }else if(lt == REORG){             l = parse_reorg(options, params);         }else if(lt == AVGPOOL){             l = parse_avgpool(options, params);         }else if(lt == ROUTE){             l = parse_route(options, params, net);         }else if(lt == UPSAMPLE){             l = parse_upsample(options, params, net);         }else if(lt == SHORTCUT){             l = parse_shortcut(options, params, net);         }else if(lt == DROPOUT){             l = parse_dropout(options, params);             l.output = net->layers[count-1].output;             l.delta = net->layers[count-1].delta; #ifdef GPU             l.output_gpu = net->layers[count-1].output_gpu;             l.delta_gpu = net->layers[count-1].delta_gpu; #endif         }else{             fprintf(stderr, "Type not recognized: %s\n", s->type);         }         l.clip = net->clip;         l.truth = option_find_int_quiet(options, "truth", 0);         l.onlyforward = option_find_int_quiet(options, "onlyforward", 0);         l.stopbackward = option_find_int_quiet(options, "stopbackward", 0);         l.dontsave = option_find_int_quiet(options, "dontsave", 0);         l.dontload = option_find_int_quiet(options, "dontload", 0);         l.numload = option_find_int_quiet(options, "numload", 0);         l.dontloadscales = option_find_int_quiet(options, "dontloadscales", 0);         l.learning_rate_scale = option_find_float_quiet(options, "learning_rate", 1);         l.smooth = option_find_float_quiet(options, "smooth", 0);         option_unused(options);         net->layers[count] = l;         if (l.workspace_size > workspace_size) workspace_size = l.workspace_size;         free_section(s);         n = n->next;         ++count;         if(n){             params.h = l.out_h;             params.w = l.out_w;             params.c = l.out_c;             params.inputs = l.outputs;         }     }     free_list(sections);     layer out = get_network_output_layer(net);     net->outputs = out.outputs;     net->truths = out.outputs;     if(net->layers[net->n-1].truths) net->truths = net->layers[net->n-1].truths;     net->output = out.output;     net->input = calloc(net->inputs*net->batch, sizeof(float));     net->truth = calloc(net->truths*net->batch, sizeof(float)); #ifdef GPU     net->output_gpu = out.output_gpu;     net->input_gpu = cuda_make_array(net->input, net->inputs*net->batch);     net->truth_gpu = cuda_make_array(net->truth, net->truths*net->batch); #endif     if(workspace_size){         //printf("%ld\n", workspace_size); #ifdef GPU         if(gpu_index >= 0){             net->workspace = cuda_make_array(0, (workspace_size-1)/sizeof(float)+1);         }else {             net->workspace = calloc(1, workspace_size);         } #else         net->workspace = calloc(1, workspace_size); #endif     }     return net; } </code></pre> </details><ul><li><p>首先调用<code>read_cfg</code>读取<code>*.cfg</code>文件，解析为链表结构</p>  <details>  <summary><font color="darkred"> list *read_cfg(char *filename) </font></summary>  <pre><code>  list *read_cfg(char *filename)  {      FILE *file = fopen(filename, "r");      if(file == 0) file_error(filename);      char *line;      int nu = 0;      list *options = make_list();      section *current = 0;      while((line=fgetl(file)) != 0){          ++ nu;          strip(line);          switch(line[0]){              case '[':                  current = malloc(sizeof(section));                  list_insert(options, current);                  current->options = make_list();                  current->type = line;                  break;              case '\0':              case '#':              case ';':                  free(line);                  break;              default:                  if(!read_option(line, current->options)){                      fprintf(stderr, "Config file error line %d, could parse: %s\n", nu, line);                      free(line);                  }                  break;          }      }      fclose(file);      return options;  }  </code></pre>  </details><p>  例如，对于内容为以下的文件</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[net]</span><br><span class="line">batch=1</span><br><span class="line">height=227</span><br><span class="line">width=227</span><br><span class="line">channels=3</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">filters=96</span><br><span class="line">size=11</span><br><span class="line">stride=4</span><br><span class="line">pad=0</span><br><span class="line">activation=relu</span><br><span class="line"></span><br><span class="line">[maxpool]</span><br><span class="line">size=3</span><br><span class="line">stride=2</span><br><span class="line">padding=0</span><br></pre></td></tr></table></figure><p>  该函数将其解析为链表<code>list* sections</code>，其元素为<code>section*</code></p>  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> *type;     <span class="comment">// 记录`[xxx]`</span></span><br><span class="line">    <span class="built_in">list</span> *options;  <span class="comment">// 记录选项</span></span><br><span class="line">&#125;section;</span><br></pre></td></tr></table></figure><p>  <code>sections -&gt; s -&gt; options</code>也为链表，其元素为<code>kvp*</code>，用以存储键值对</p>  <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> *key;</span><br><span class="line">    <span class="keyword">char</span> *val;</span><br><span class="line">    <span class="keyword">int</span> used;</span><br><span class="line">&#125; kvp;</span><br></pre></td></tr></table></figure><p>  示意图如下<br>  <img src="/2019/12/08/深度学习框架darknet【三】——调包大法好/cfg.jpg" alt="cfg"></p></li><li><p>之后按上述输出的参数，构建网络</p><ul><li><p>解析网络参数</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">network *net = make_network(sections-&gt;size - <span class="number">1</span>);</span><br><span class="line">section *s = (section *)n-&gt;val;</span><br><span class="line"><span class="built_in">list</span> *options = s-&gt;options;</span><br><span class="line"><span class="keyword">if</span>(!is_network(s)) error(<span class="string">"First section must be [net] or [network]"</span>);</span><br><span class="line">parse_net_options(options, net);</span><br></pre></td></tr></table></figure><details><summary><font color="darkred"> void parse_net_options(list *options, network *net) </font></summary><pre><code>void parse_net_options(list *options, network *net){    net->batch = option_find_int(options, "batch",1);    net->learning_rate = option_find_float(options, "learning_rate", .001);    net->momentum = option_find_float(options, "momentum", .9);    net->decay = option_find_float(options, "decay", .0001);    int subdivs = option_find_int(options, "subdivisions",1);    net->time_steps = option_find_int_quiet(options, "time_steps",1);    net->notruth = option_find_int_quiet(options, "notruth",0);    net->batch /= subdivs;    net->batch *= net->time_steps;    net->subdivisions = subdivs;    net->random = option_find_int_quiet(options, "random", 0);    net->adam = option_find_int_quiet(options, "adam", 0);    if(net->adam){        net->B1 = option_find_float(options, "B1", .9);        net->B2 = option_find_float(options, "B2", .999);        net->eps = option_find_float(options, "eps", .0000001);    }    net->h = option_find_int_quiet(options, "height",0);    net->w = option_find_int_quiet(options, "width",0);    net->c = option_find_int_quiet(options, "channels",0);    net->inputs = option_find_int_quiet(options, "inputs", net->h * net->w * net->c);    net->max_crop = option_find_int_quiet(options, "max_crop",net->w*2);    net->min_crop = option_find_int_quiet(options, "min_crop",net->w);    net->max_ratio = option_find_float_quiet(options, "max_ratio", (float) net->max_crop / net->w);    net->min_ratio = option_find_float_quiet(options, "min_ratio", (float) net->min_crop / net->w);    net->center = option_find_int_quiet(options, "center",0);    net->clip = option_find_float_quiet(options, "clip", 0);    net->angle = option_find_float_quiet(options, "angle", 0);    net->aspect = option_find_float_quiet(options, "aspect", 1);    net->saturation = option_find_float_quiet(options, "saturation", 1);    net->exposure = option_find_float_quiet(options, "exposure", 1);    net->hue = option_find_float_quiet(options, "hue", 0);    if(!net->inputs && !(net->h && net->w && net->c)) error("No input parameters supplied");    char *policy_s = option_find_str(options, "policy", "constant");    net->policy = get_policy(policy_s);    net->burn_in = option_find_int_quiet(options, "burn_in", 0);    net->power = option_find_float_quiet(options, "power", 4);    if(net->policy == STEP){        net->step = option_find_int(options, "step", 1);        net->scale = option_find_float(options, "scale", 1);    } else if (net->policy == STEPS){        char *l = option_find(options, "steps");        char *p = option_find(options, "scales");        if(!l || !p) error("STEPS policy must have steps and scales in cfg file");        int len = strlen(l);        int n = 1;        int i;        for(i = 0; i < len; ++i){            if (l[i] == ',') ++n;        }        int *steps = calloc(n, sizeof(int));        float *scales = calloc(n, sizeof(float));        for(i = 0; i < n; ++i){            int step    = atoi(l);            float scale = atof(p);            l = strchr(l, ',')+1;            p = strchr(p, ',')+1;            steps[i] = step;            scales[i] = scale;        }        net->scales = scales;        net->steps = steps;        net->num_steps = n;    } else if (net->policy == EXP){        net->gamma = option_find_float(options, "gamma", 1);    } else if (net->policy == SIG){        net->gamma = option_find_float(options, "gamma", 1);        net->step = option_find_int(options, "step", 1);    } else if (net->policy == POLY || net->policy == RANDOM){    }    net->max_batches = option_find_int(options, "max_batches", 0);}</code></pre></details></li><li><p>解析各层参数<br>根据不同类型构建网络，注意不同类型的网络层需定义<code>layer parse_[lt](list *options, size_params params)</code>函数，以申请内存资源等</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">size_params params;</span><br><span class="line">...                         <span class="comment">// 尺寸参数</span></span><br><span class="line"></span><br><span class="line">n = n-&gt;next;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">free_section(s);</span><br><span class="line"><span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"layer     filters    size              input                output\n"</span>);</span><br><span class="line"><span class="keyword">while</span>(n)&#123;                   <span class="comment">// 各层遍历</span></span><br><span class="line">    params.index = count;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%5d "</span>, count);</span><br><span class="line">    s = (section *)n-&gt;val;</span><br><span class="line">    options = s-&gt;options;</span><br><span class="line"></span><br><span class="line">    layer l = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    LAYER_TYPE lt = string_to_layer_type(s-&gt;type);</span><br><span class="line">    <span class="keyword">if</span>(lt == CONVOLUTIONAL)&#123;</span><br><span class="line">        l = parse_convolutional(options, params);</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(...)&#123;</span><br><span class="line">    ...                     <span class="comment">// 判断类型并构建对应网络层</span></span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"Type not recognized: %s\n"</span>, s-&gt;type);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 一些其他参数</span></span><br><span class="line">    l.clip = net-&gt;clip;</span><br><span class="line">    ...</span><br><span class="line">    l.smooth = option_find_float_quiet(options, <span class="string">"smooth"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    option_unused(options);</span><br><span class="line">    net-&gt;layers[count] = l;</span><br><span class="line">    <span class="keyword">if</span> (l.workspace_size &gt; workspace_size) workspace_size = l.workspace_size;</span><br><span class="line">    free_section(s);</span><br><span class="line">    n = n-&gt;next;</span><br><span class="line">    ++count;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 更新特征图尺寸，用做下一层网络层的输入尺寸</span></span><br><span class="line">    <span class="keyword">if</span>(n)&#123;</span><br><span class="line">        params.h = l.out_h;</span><br><span class="line">        params.w = l.out_w;</span><br><span class="line">        params.c = l.out_c;</span><br><span class="line">        params.inputs = l.outputs;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">free_list(sections);</span><br></pre></td></tr></table></figure><p>以卷积层为例</p><details><summary><font color="darkred"> convolutional_layer parse_convolutional(list *options, size_params params) </font></summary><pre><code>convolutional_layer parse_convolutional(list *options, size_params params){    // 寻找参数，若无指定使用默认值    int n = option_find_int(options, "filters",1);    int size = option_find_int(options, "size",1);    int stride = option_find_int(options, "stride",1);    int pad = option_find_int_quiet(options, "pad",0);    int padding = option_find_int_quiet(options, "padding",0);    int groups = option_find_int_quiet(options, "groups", 1);    if(pad) padding = size/2;    char *activation_s = option_find_str(options, "activation", "logistic");    ACTIVATION activation = get_activation(activation_s);    // 输入尺寸参数    int batch,h,w,c;    h = params.h;    w = params.w;    c = params.c;    batch=params.batch;    if(!(h && w && c)) error("Layer before convolutional layer must output image.");    int batch_normalize = option_find_int_quiet(options, "batch_normalize", 0);    int binary = option_find_int_quiet(options, "binary", 0);    int xnor = option_find_int_quiet(options, "xnor", 0);    // 创建网络层    convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net->adam);    layer.flipped = option_find_int_quiet(options, "flipped", 0);    layer.dot = option_find_float_quiet(options, "dot", 0);    return layer;}</code></pre></details></li><li><p>内存指针分配<br>网络输出即最后一层<code>out</code>输出，两者共享内存</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">layer out = get_network_output_layer(net);</span><br><span class="line">net-&gt;outputs = out.outputs;</span><br><span class="line">net-&gt;truths = out.outputs;</span><br><span class="line"><span class="keyword">if</span>(net-&gt;layers[net-&gt;n<span class="number">-1</span>].truths) net-&gt;truths = net-&gt;layers[net-&gt;n<span class="number">-1</span>].truths;</span><br><span class="line">net-&gt;output = out.output;</span><br><span class="line">net-&gt;input = <span class="built_in">calloc</span>(net-&gt;inputs*net-&gt;batch, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">net-&gt;truth = <span class="built_in">calloc</span>(net-&gt;truths*net-&gt;batch, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(workspace_size)&#123;</span><br><span class="line">    net-&gt;workspace = <span class="built_in">calloc</span>(<span class="number">1</span>, workspace_size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>载入权值</p> <details> <summary><font color="darkred"> load_weights(network *net, char *filename) </font></summary> <pre><code> void load_weights(network *net, char *filename) {     load_weights_upto(net, filename, 0, net->n); } void load_weights_upto(network *net, char *filename, int start, int cutoff) {     fprintf(stderr, "Loading weights from %s...", filename);     fflush(stdout);     FILE *fp = fopen(filename, "rb");     if(!fp) file_error(filename);     int major;     int minor;     int revision;     fread(&major, sizeof(int), 1, fp);     fread(&minor, sizeof(int), 1, fp);     fread(&revision, sizeof(int), 1, fp);     if ((major*10 + minor) >= 2 && major < 1000 && minor < 1000){         fread(net->seen, sizeof(size_t), 1, fp);     } else {         int iseen = 0;         fread(&iseen, sizeof(int), 1, fp);         *net->seen = iseen;     }     int transpose = (major > 1000) || (minor > 1000);     int i;     for(i = start; i < net->n && i < cutoff; ++i){         layer l = net->layers[i];         if (l.dontload) continue;         if(l.type == CONVOLUTIONAL || l.type == DECONVOLUTIONAL){             load_convolutional_weights(l, fp);         }         if(l.type == CONNECTED){             load_connected_weights(l, fp, transpose);         }         if(l.type == BATCHNORM){             load_batchnorm_weights(l, fp);         }         if(l.type == CRNN){             load_convolutional_weights(*(l.input_layer), fp);             load_convolutional_weights(*(l.self_layer), fp);             load_convolutional_weights(*(l.output_layer), fp);         }         if(l.type == RNN){             load_connected_weights(*(l.input_layer), fp, transpose);             load_connected_weights(*(l.self_layer), fp, transpose);             load_connected_weights(*(l.output_layer), fp, transpose);         }         if (l.type == LSTM) {             load_connected_weights(*(l.wi), fp, transpose);             load_connected_weights(*(l.wf), fp, transpose);             load_connected_weights(*(l.wo), fp, transpose);             load_connected_weights(*(l.wg), fp, transpose);             load_connected_weights(*(l.ui), fp, transpose);             load_connected_weights(*(l.uf), fp, transpose);             load_connected_weights(*(l.uo), fp, transpose);             load_connected_weights(*(l.ug), fp, transpose);         }         if (l.type == GRU) {             if(1){                 load_connected_weights(*(l.wz), fp, transpose);                 load_connected_weights(*(l.wr), fp, transpose);                 load_connected_weights(*(l.wh), fp, transpose);                 load_connected_weights(*(l.uz), fp, transpose);                 load_connected_weights(*(l.ur), fp, transpose);                 load_connected_weights(*(l.uh), fp, transpose);             }else{                 load_connected_weights(*(l.reset_layer), fp, transpose);                 load_connected_weights(*(l.update_layer), fp, transpose);                 load_connected_weights(*(l.state_layer), fp, transpose);             }         }         if(l.type == LOCAL){             int locations = l.out_w*l.out_h;             int size = l.size*l.size*l.c*l.n*locations;             fread(l.biases, sizeof(float), l.outputs, fp);             fread(l.weights, sizeof(float), size, fp);         }     }     fprintf(stderr, "Done!\n");     fclose(fp); } </code></pre> </details><p> 权值载入比较简单，根据每层的网络参数格式，以二进制格式读取即可。需要注意的是，文件头需读写4个整形<code>int</code>的数据</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> major;</span><br><span class="line"><span class="keyword">int</span> minor;</span><br><span class="line"><span class="keyword">int</span> revision;</span><br><span class="line">fread(&amp;major, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), <span class="number">1</span>, fp);</span><br><span class="line">fread(&amp;minor, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), <span class="number">1</span>, fp);</span><br><span class="line">fread(&amp;revision, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), <span class="number">1</span>, fp);</span><br><span class="line"><span class="keyword">if</span> ((major*<span class="number">10</span> + minor) &gt;= <span class="number">2</span> &amp;&amp; major &lt; <span class="number">1000</span> &amp;&amp; minor &lt; <span class="number">1000</span>)&#123;</span><br><span class="line">    fread(net-&gt;seen, <span class="keyword">sizeof</span>(<span class="keyword">size_t</span>), <span class="number">1</span>, fp);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">int</span> iseen = <span class="number">0</span>;</span><br><span class="line">    fread(&amp;iseen, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), <span class="number">1</span>, fp);</span><br><span class="line">    *net-&gt;seen = iseen;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 以卷积层为例，需要读取卷积核<code>weights</code>、偏置<code>biases</code>，在无批归一<code>batchnorm</code>的情况下，其读取函数为</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">load_convolutional_weights</span><span class="params">(layer l, FILE *fp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l.numload) l.n = l.numload;</span><br><span class="line">    <span class="keyword">int</span> num = l.c/l.groups*l.n*l.size*l.size;</span><br><span class="line">    fread(l.biases, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), l.n, fp);</span><br><span class="line">    fread(l.weights, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), num, fp);</span><br><span class="line">    <span class="keyword">if</span> (l.flipped) &#123;</span><br><span class="line">        transpose_matrix(l.weights, l.c*l.size*l.size, l.n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h2><p>训练过程中关于数据载入的代码整理如下，通过多线程的方式进行数据读取。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> *train_images = <span class="string">"/data/voc/train.txt"</span>;</span><br><span class="line"><span class="keyword">int</span> imgs = net-&gt;batch*net-&gt;subdivisions;</span><br><span class="line"><span class="keyword">int</span> i = *net-&gt;seen/imgs;</span><br><span class="line">data train, buffer;</span><br><span class="line"></span><br><span class="line">layer l = net-&gt;layers[net-&gt;n - <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取文件列表</span></span><br><span class="line"><span class="built_in">list</span> *plist = get_paths(train_images);</span><br><span class="line"><span class="keyword">char</span> **paths = (<span class="keyword">char</span> **)list_to_array(plist);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据读取参数</span></span><br><span class="line">load_args args  = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">args.paths      = paths;</span><br><span class="line">args.w          = net-&gt;w;</span><br><span class="line">args.h          = net-&gt;h;</span><br><span class="line">args.n          = imgs;</span><br><span class="line">args.m          = plist-&gt;size;</span><br><span class="line">args.classes    = l.classes;</span><br><span class="line">args.jitter     = l.jitter;</span><br><span class="line">args.num_boxes  = l.side;</span><br><span class="line">args.d          = &amp;buffer;</span><br><span class="line">args.type       = REGION_DATA;</span><br><span class="line">args.angle      = net-&gt;angle;</span><br><span class="line">args.exposure   = net-&gt;exposure;</span><br><span class="line">args.saturation = net-&gt;saturation;</span><br><span class="line">args.hue        = net-&gt;hue;</span><br><span class="line"><span class="keyword">pthread_t</span> load_thread = load_data_in_thread(args);</span><br><span class="line"></span><br><span class="line"><span class="keyword">clock_t</span> time;</span><br><span class="line"><span class="keyword">while</span>(get_current_batch(net) &lt; net-&gt;max_batches)&#123;</span><br><span class="line">    i += <span class="number">1</span>;</span><br><span class="line">    time = clock();</span><br><span class="line">    pthread_join(load_thread, <span class="number">0</span>);           <span class="comment">// 主线程阻塞，等待load_thread线程结束</span></span><br><span class="line">    train = buffer;</span><br><span class="line">    load_thread = load_data_in_thread(args);<span class="comment">// 创建新线程</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Loaded: %lf seconds\n"</span>, sec(clock()-time));</span><br><span class="line">    </span><br><span class="line">    ... <span class="comment">// 网络训练</span></span><br><span class="line"></span><br><span class="line">    free_data(train);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下两个函数用于读取文本文件，每行存储对应的样本路径，<code>char **paths</code>每个内存单元指向字符串首地址。</p><details><summary><font color="darkred"> list *get_paths(char *filename) </font></summary><pre><code>list *get_paths(char *filename){    char *path;    FILE *file = fopen(filename, "r");    if(!file) file_error(filename);    list *lines = make_list();    while((path=fgetl(file))){        list_insert(lines, path);    }    fclose(file);    return lines;}</code></pre></details><details><summary><font color="darkred"> void **list_to_array(list *l) </font></summary><pre><code>void **list_to_array(list *l){    void **a = calloc(l->size, sizeof(void*));    int count = 0;    node *n = l->front;    while(n){        a[count++] = n->val;        n = n->next;    }    return a;}</code></pre></details><p>重点关注多线程数据载入的部分，先介绍Linux系统下线程控制的函数，以下资料来源于百度百科<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">函数定义：<span class="function"><span class="keyword">int</span> <span class="title">pthread_join</span><span class="params">(<span class="keyword">pthread_t</span> thread, <span class="keyword">void</span> **retval)</span></span>;</span><br><span class="line">描述：pthread_join()函数，以阻塞的方式等待thread指定的线程结束。当函数返回时，被等待线程的资源被收回。如果线程已经结束，那么该函数会立即返回。并且thread指定的线程必须是joinable的</span><br><span class="line">参数：</span><br><span class="line">    - thread: 线程标识符，即线程ID，标识唯一线程；</span><br><span class="line">    - retval: 用户定义的指针，用来存储被等待线程的返回值。</span><br><span class="line">返回值：<span class="number">0</span>代表成功，若失败则返回错误号</span><br><span class="line"></span><br><span class="line">函数定义：<span class="function"><span class="keyword">int</span> <span class="title">pthread_create</span><span class="params">(<span class="keyword">pthread_t</span> *tidp, <span class="keyword">const</span> <span class="keyword">pthread_attr_t</span> *attr, (<span class="keyword">void</span>*)(*start_rtn)(<span class="keyword">void</span>*),<span class="keyword">void</span> *arg)</span></span>;</span><br><span class="line">描述：pthread_create是类Unix操作系统（Unix、Linux、Mac OS X等）的创建线程的函数。它的功能是创建线程（实际上就是确定调用该线程函数的入口点），在线程创建以后，就开始运行相关的线程函数。</span><br><span class="line">参数：</span><br><span class="line">    - tidp: 指向线程标识符的指针；</span><br><span class="line">    - attr: 设置线程属性；</span><br><span class="line">    - start_rtn: 线程运行函数的起始地址；</span><br><span class="line">    - arg: 运行函数的参数。</span><br><span class="line">返回值：表示成功，返回<span class="number">0</span>；表示出错，返回<span class="number">-1</span>。</span><br></pre></td></tr></table></figure></p><details><summary><font color="darkred"> pthread_t load_data_in_thread(load_args args) </font></summary><pre><code>pthread_t load_data_in_thread(load_args args){    pthread_t thread;    struct load_args *ptr = calloc(1, sizeof(struct load_args));    *ptr = args;    if(pthread_create(&thread, 0, load_thread, ptr)) error("Thread creation failed");    return thread;}</code></pre></details><p>以上函数用于线程启动，在某批次数据读取结束前，主线程将被阻塞。读取结束后，<code>train = buffer</code>将缓冲区指针传递给<code>train</code>，再重新启动线程。注意在线程函数内，缓冲区<code>buffer</code>各指针将被分配新的内存地址，以防止下一批数据对当前批数据造成影响，因此当前批数据所占内存资源需要通过<code>free_data(train)</code>来释放。</p><details><summary><font color="darkred"> void *load_thread(void *ptr) </font></summary><pre><code>void *load_thread(void *ptr){    load_args a = *(struct load_args*)ptr;    if(a.exposure == 0) a.exposure = 1;    if(a.saturation == 0) a.saturation = 1;    if(a.aspect == 0) a.aspect = 1;    if (a.type == OLD_CLASSIFICATION_DATA){        *a.d = load_data_old(a.paths, a.n, a.m, a.labels, a.classes, a.w, a.h);    } else if (a.type == REGRESSION_DATA){        *a.d = load_data_regression(a.paths, a.n, a.m, a.classes, a.min, a.max, a.size, a.angle, a.aspect, a.hue, a.saturation, a.exposure);    } else if (a.type == CLASSIFICATION_DATA){        *a.d = load_data_augment(a.paths, a.n, a.m, a.labels, a.classes, a.hierarchy, a.min, a.max, a.size, a.angle, a.aspect, a.hue, a.saturation, a.exposure, a.center);    } else if (a.type == SUPER_DATA){        *a.d = load_data_super(a.paths, a.n, a.m, a.w, a.h, a.scale);    } else if (a.type == WRITING_DATA){        *a.d = load_data_writing(a.paths, a.n, a.m, a.w, a.h, a.out_w, a.out_h);    } else if (a.type == ISEG_DATA){        *a.d = load_data_iseg(a.n, a.paths, a.m, a.w, a.h, a.classes, a.num_boxes, a.scale, a.min, a.max, a.angle, a.aspect, a.hue, a.saturation, a.exposure);    } else if (a.type == INSTANCE_DATA){        *a.d = load_data_mask(a.n, a.paths, a.m, a.w, a.h, a.classes, a.num_boxes, a.coords, a.min, a.max, a.angle, a.aspect, a.hue, a.saturation, a.exposure);    } else if (a.type == SEGMENTATION_DATA){        *a.d = load_data_seg(a.n, a.paths, a.m, a.w, a.h, a.classes, a.min, a.max, a.angle, a.aspect, a.hue, a.saturation, a.exposure, a.scale);    } else if (a.type == REGION_DATA){        *a.d = load_data_region(a.n, a.paths, a.m, a.w, a.h, a.num_boxes, a.classes, a.jitter, a.hue, a.saturation, a.exposure);    } else if (a.type == DETECTION_DATA){        *a.d = load_data_detection(a.n, a.paths, a.m, a.w, a.h, a.num_boxes, a.classes, a.jitter, a.hue, a.saturation, a.exposure);    } else if (a.type == SWAG_DATA){        *a.d = load_data_swag(a.paths, a.n, a.classes, a.jitter);    } else if (a.type == COMPARE_DATA){        *a.d = load_data_compare(a.n, a.paths, a.m, a.classes, a.w, a.h);    } else if (a.type == IMAGE_DATA){        *(a.im) = load_image_color(a.path, 0, 0);        *(a.resized) = resize_image(*(a.im), a.w, a.h);    } else if (a.type == LETTERBOX_DATA){        *(a.im) = load_image_color(a.path, 0, 0);        *(a.resized) = letterbox_image(*(a.im), a.w, a.h);    } else if (a.type == TAG_DATA){        *a.d = load_data_tag(a.paths, a.n, a.m, a.classes, a.min, a.max, a.size, a.angle, a.aspect, a.hue, a.saturation, a.exposure);    }    free(ptr);    return 0;}</code></pre></details><p>那么线程启动后，执行的函数<code>void *load_thread(void *ptr)</code>做了哪些工作呢？首先初始化数据读取参数，对应不同类型的数据，分别进入对应的数据读取子函数，例如分类、检测、图像等等，读取的数据以指针的形式存放在<code>a.d</code>中，最后释放在线程创建时申请的参数指针<code>ptr</code>。</p><details><summary><font color="darkred"> data load_data_region(int n, char **paths, int m, int w, int h, int size, int classes, float jitter, float hue, float saturation, float exposure) </font></summary><pre><code>data load_data_region(int n, char **paths, int m, int w, int h, int size, int classes, float jitter, float hue, float saturation, float exposure){    char **random_paths = get_random_paths(paths, n, m);    int i;    data d = {0};    d.shallow = 0;    d.X.rows = n;    d.X.vals = calloc(d.X.rows, sizeof(float*));    d.X.cols = h*w*3;    int k = size*size*(5+classes);    d.y = make_matrix(n, k);    for(i = 0; i < n; ++i){        image orig = load_image_color(random_paths[i], 0, 0);        int oh = orig.h;        int ow = orig.w;        int dw = (ow*jitter);        int dh = (oh*jitter);        int pleft  = rand_uniform(-dw, dw);        int pright = rand_uniform(-dw, dw);        int ptop   = rand_uniform(-dh, dh);        int pbot   = rand_uniform(-dh, dh);        int swidth =  ow - pleft - pright;        int sheight = oh - ptop - pbot;        float sx = (float)swidth  / ow;        float sy = (float)sheight / oh;        int flip = rand()%2;        image cropped = crop_image(orig, pleft, ptop, swidth, sheight);        float dx = ((float)pleft/ow)/sx;        float dy = ((float)ptop /oh)/sy;        image sized = resize_image(cropped, w, h);        if(flip) flip_image(sized);        random_distort_image(sized, hue, saturation, exposure);        d.X.vals[i] = sized.data;        fill_truth_region(random_paths[i], d.y.vals[i], classes, size, flip, dx, dy, 1./sx, 1./sy);        free_image(orig);        free_image(cropped);    }    free(random_paths);    return d;}</code></pre></details><p>对于YOLO，其载入的数据类型为<code>REGION_DATA</code>，所以进入到<code>load_data_region</code>函数，这里需要关注的为数据存放的格式。可以看到，<code>d.X</code>为<code>matrix</code>类型，其行数<code>rows</code>与列数<code>cols</code>在下面函数中分别指定为<code>n</code>与<code>h*w*3</code>，即尺寸为<code>n × h*w*3</code>，每行表示一副图像数据。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">d.X.rows = n;</span><br><span class="line">d.X.cols = h*w*<span class="number">3</span>;</span><br><span class="line">d.X.vals = <span class="built_in">calloc</span>(d.X.rows, <span class="keyword">sizeof</span>(<span class="keyword">float</span>*));    <span class="comment">// 该指针用于存放内存空间的首地址，即“指针的指针”</span></span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">    image orig = load_image_color(random_paths[i], <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    ...     <span class="comment">// 数据扩增、缩放等常规操作</span></span><br><span class="line">    d.X.vals[i] = sized.data;</span><br><span class="line">    fill_truth_region(random_paths[i], d.y.vals[i], classes, size, flip, dx, dy, <span class="number">1.</span>/sx, <span class="number">1.</span>/sy); <span class="comment">// 获取groundtrurh，存储在`d.y`中</span></span><br><span class="line">    ...     <span class="comment">// 内存释放</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Forward-Backward-and-Update"><a href="#Forward-Backward-and-Update" class="headerlink" title="Forward, Backward and Update"></a>Forward, Backward and Update</h2><p>读取数据后，训练网络已被包装为函数<code>train_network</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> loss = train_network(net, train);</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">train_network</span><span class="params">(network *net, data d)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    assert(d.X.rows % net-&gt;batch == <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">int</span> batch = net-&gt;batch;</span><br><span class="line">    <span class="keyword">int</span> n = d.X.rows / batch;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</span><br><span class="line">        get_next_batch(d, batch, i*batch, net-&gt;input, net-&gt;truth);  <span class="comment">// 获取下一批次数据</span></span><br><span class="line">        <span class="keyword">float</span> err = train_network_datum(net);                       <span class="comment">// 前向、反向、更新</span></span><br><span class="line">        sum += err;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">float</span>)sum/(n*batch);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中获取数据部分，将<code>d.X.vals</code>与<code>d.y.vals</code>内容分别拷贝至<code>net-&gt;input</code>与<code>net-&gt;truth</code></p><details><summary><font color="darkred"> void get_next_batch(data d, int n, int offset, float *X, float *y) </font></summary><pre><code>void get_next_batch(data d, int n, int offset, float *X, float *y){    int j;    for(j = 0; j < n; ++j){        int index = offset + j;        memcpy(X+j*d.X.cols, d.X.vals[index], d.X.cols*sizeof(float));        if(y) memcpy(y+j*d.y.cols, d.y.vals[index], d.y.cols*sizeof(float));    }}</code></pre></details><p>训练部分，<code>net-&gt;input</code>作为输入，前向计算得到<code>net-&gt;cost</code>，再反向计算得到各层梯度，之后梯度累加到各层参数上进行参数更新</p><details><summary><font color="darkred"> float train_network_datum(network *net) </font></summary><pre><code>float train_network_datum(network *net){    *net->seen += net->batch;    net->train = 1;    forward_network(net);    backward_network(net);    float error = *net->cost;    if(((*net->seen)/net->batch)%net->subdivisions == 0) update_network(net);    return error;}</code></pre></details><p>依次调用各层的<code>l.forward</code>，<code>l.backward</code>，<code>l.update</code></p><details><summary><font color="darkred"> forward, backward, update </font></summary><pre><code>void forward_network(network *netp){    network net = *netp;    int i;    for(i = 0; i < net.n; ++i){        net.index = i;        layer l = net.layers[i];        if(l.delta){            fill_cpu(l.outputs * l.batch, 0, l.delta, 1);        }        l.forward(l, net);        net.input = l.output;        if(l.truth) {            net.truth = l.output;        }    }    calc_network_cost(netp);}void backward_network(network *netp){    network net = *netp;    int i;    network orig = net;    for(i = net.n-1; i >= 0; --i){        layer l = net.layers[i];        if(l.stopbackward) break;        if(i == 0){            net = orig;        }else{            layer prev = net.layers[i-1];            net.input = prev.output;            net.delta = prev.delta;        }        net.index = i;        l.backward(l, net);    }}void update_network(network *netp){    network net = *netp;    int i;    update_args a = {0};    a.batch = net.batch*net.subdivisions;    a.learning_rate = get_current_rate(netp);    a.momentum = net.momentum;    a.decay = net.decay;    a.adam = net.adam;    a.B1 = net.B1;    a.B2 = net.B2;    a.eps = net.eps;    ++*net.t;    a.t = *net.t;    for(i = 0; i < net.n; ++i){        layer l = net.layers[i];        if(l.update){            l.update(l, a);        }    }}</code></pre></details><p>注意在初始化网络层时，结构体<code>struct layer</code>中已初始化三个函数指针<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">layer</span>&#123;</span></span><br><span class="line">    ...             <span class="comment">// 略</span></span><br><span class="line">    <span class="keyword">void</span> (*forward)   (struct layer, struct network);</span><br><span class="line">    <span class="keyword">void</span> (*backward)  (struct layer, struct network);</span><br><span class="line">    <span class="keyword">void</span> (*update)    (struct layer, update_args);</span><br><span class="line">    ...             <span class="comment">// 略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>以卷积层为例，<code>make_convolutional_layer</code>中<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l.forward  = forward_convolutional_layer;</span><br><span class="line">l.backward = backward_convolutional_layer;</span><br><span class="line">l.update   = update_convolutional_layer;</span><br></pre></td></tr></table></figure></p><h1 id="Test-Stage"><a href="#Test-Stage" class="headerlink" title="Test Stage"></a>Test Stage</h1><p>指定参数2为<code>test</code>，可进入测试函数段，提示输入图片路径<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./darknet yolo <span class="built_in">test</span> cfg/yolov3.cfg yolov3.weights</span></span><br><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 / 1   608 x 608 x   3   -&gt;   608 x 608 x  32  0.639 BFLOPs</span><br><span class="line">    1 conv     64  3 x 3 / 2   608 x 608 x  32   -&gt;   304 x 304 x  64  3.407 BFLOPs</span><br><span class="line">... # 略</span><br><span class="line">  105 conv    255  1 x 1 / 1    76 x  76 x 256   -&gt;    76 x  76 x 255  0.754 BFLOPs</span><br><span class="line">  106 yolo</span><br><span class="line">Loading weights from yolov3.weights...Done!</span><br><span class="line">Enter Image Path:</span><br></pre></td></tr></table></figure></p><p>原始测试函数<code>examples/yolo.c/test_yolo</code>并不能实现检测功能(<code>detector.c/test_detector</code>可以使用)，所以将其修改为以下</p><details><summary><font color="darkred"> void test_yolo(char *cfgfile, char *weightfile, char *filename, float thresh) </font></summary><pre><code>void test_yolo(char *cfgfile, char *weightfile, char *filename, float thresh){    // 获取标签名称、字母表等    list *options = read_data_cfg("cfg/coco.data");    char *name_list = option_find_str(options, "names", "data/names.list");    char **names = get_labels(name_list);    image **alphabet = load_alphabet();    // 构建网络    network *net = load_network(cfgfile, weightfile, 0);    set_batch_network(net, 1);    // 一些参数。。。    srand(2222222);    double time;    char buff[256];    char *input = buff;    float nms=.45;    while(1){        if(filename){            strncpy(input, filename, 256);        } else {            printf("Enter Image Path: ");            fflush(stdout);            input = fgets(input, 256, stdin);            if(!input) return;            strtok(input, "\n");        }        image im = load_image_color(input,0,0);        image sized = letterbox_image(im, net->w, net->h);        layer l = net->layers[net->n-1];        float *X = sized.data;        time=what_time_is_it_now();        network_predict(net, X);        printf("%s: Predicted in %f seconds.\n", input, what_time_is_it_now()-time);        int nboxes = 0;        detection *dets = get_network_boxes(net, im.w, im.h, thresh, 0, 0, 1, &nboxes);        if (nms) do_nms_sort(dets, nboxes, l.classes, nms);        draw_detections(im, dets, nboxes, thresh, names, alphabet, l.classes);        free_detections(dets, nboxes);        save_image(im, "predictions");#ifdef OPENCV        make_window("predictions", 512, 512, 0);        show_image(im, "predictions", 0);#endif        free_image(im);        free_image(sized);        if(filename) break;    }}</code></pre></details><p>主要的过程如下</p><ol><li><p>读取图像、裁剪并缩放</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image im = load_image_color(input,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">image sized = letterbox_image(im, net-&gt;w, net-&gt;h);</span><br></pre></td></tr></table></figure> <details> <summary><font color="darkred"> image letterbox_image(image im, int w, int h) </font></summary> <pre><code> image letterbox_image(image im, int w, int h) {     int new_w = im.w;     int new_h = im.h;     if (((float)w/im.w) < ((float)h/im.h)) {         new_w = w;         new_h = (im.h * w)/im.w;     } else {         new_h = h;         new_w = (im.w * h)/im.h;     }     image resized = resize_image(im, new_w, new_h);     image boxed = make_image(w, h, im.c);     fill_image(boxed, .5);     //int i;     //for(i = 0; i < boxed.w*boxed.h*boxed.c; ++i) boxed.data[i] = 0;     embed_image(resized, boxed, (w-new_w)/2, (h-new_h)/2);      free_image(resized);     return boxed; } </code></pre> </details></li><li><p>前向运算</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">layer l  = net-&gt;layers[net-&gt;n<span class="number">-1</span>];</span><br><span class="line"><span class="keyword">float</span> *X = sized.data;</span><br><span class="line">time = what_time_is_it_now();</span><br><span class="line">network_predict(net, X);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%s: Predicted in %f seconds.\n"</span>, input, what_time_is_it_now()-time);</span><br></pre></td></tr></table></figure></li><li><p>根据结果生成候选框</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nboxes = <span class="number">0</span>;   </span><br><span class="line">detection *dets = get_network_boxes(net, im.w, im.h, thresh, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, &amp;nboxes);</span><br></pre></td></tr></table></figure> <details> <summary><font color="darkred"> detection *get_network_boxes(network *net, int w, int h, float thresh, float hier, int *map, int relative, int *num) </font></summary> <pre><code> detection *get_network_boxes(network *net, int w, int h, float thresh, float hier, int *map, int relative, int *num) {     detection *dets = make_network_boxes(net, thresh, num);     fill_network_boxes(net, w, h, thresh, hier, map, relative, dets);     return dets; } </code></pre> </details><p> YOLO详细算法不过多介绍。</p></li><li><p>非极大值抑制(NMS)</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (nms) do_nms_sort(dets, nboxes, l.classes, nms);</span><br></pre></td></tr></table></figure> <details> <summary><font color="darkred"> void do_nms_sort(detection *dets, int total, int classes, float thresh) </font></summary> <pre><code> void do_nms_sort(detection *dets, int total, int classes, float thresh) {     int i, j, k;     k = total-1;     for(i = 0; i <= k;="" ++i){="" if(dets[i].objectness="=" 0){="" detection="" swap="dets[i];" dets[i]="dets[k];" dets[k]="swap;" --k;="" --i;="" }="" total="k+1;" for(k="0;" k="" <="" classes;="" ++k){="" for(i="0;" i="" total;="" dets[i].sort_class="k;" qsort(dets,="" total,="" sizeof(detection),="" nms_comparator);="" if(dets[i].prob[k]="=" 0)="" continue;="" box="" a="dets[i].bbox;" for(j="i+1;" j="" ++j){="" b="dets[j].bbox;" if="" (box_iou(a,="" b)=""> thresh){                     dets[j].prob[k] = 0;                 }             }         }     } } </=></code></pre> </details></li><li><p>绘制并显示</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">draw_detections(im, dets, nboxes, thresh, names, alphabet, l.classes);</span><br><span class="line">save_image(im, <span class="string">"predictions"</span>);</span><br><span class="line">make_window(<span class="string">"predictions"</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">0</span>);</span><br><span class="line">show_image(im, <span class="string">"predictions"</span>, <span class="number">0</span>);</span><br></pre></td></tr></table></figure></li><li><p>释放资源</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">free_detections(dets, nboxes);</span><br><span class="line">free_image(im);</span><br><span class="line">free_image(sized);</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习框架darknet【二】——目录结构</title>
      <link href="/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%8C%E3%80%91%E2%80%94%E2%80%94%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/"/>
      <url>/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%8C%E3%80%91%E2%80%94%E2%80%94%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> tree -L 1</span><br><span class="line">.</span><br><span class="line">├── cfg/</span><br><span class="line">├── data/</span><br><span class="line">├── examples/</span><br><span class="line">├── include/</span><br><span class="line">├── python/</span><br><span class="line">├── scripts/</span><br><span class="line">├── src/</span><br><span class="line">├── LICENSE</span><br><span class="line">├── LICENSE.fuck</span><br><span class="line">├── LICENSE.gen</span><br><span class="line">├── LICENSE.gpl</span><br><span class="line">├── LICENSE.meta</span><br><span class="line">├── LICENSE.mit</span><br><span class="line">├── LICENSE.v1</span><br><span class="line">├── Makefile</span><br><span class="line">└── README.md</span><br><span class="line"></span><br><span class="line">7 directories, 9 files</span><br></pre></td></tr></table></figure><h1 id="cfg-：网络结构"><a href="#cfg-：网络结构" class="headerlink" title="cfg/：网络结构"></a><code>cfg/</code>：网络结构</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">darknet/cfg$ tree</span><br><span class="line">.</span><br><span class="line">├── alexnet.cfg</span><br><span class="line">├── ... # 略</span><br><span class="line">├── yolo9000.cfg</span><br><span class="line">├── yolov1.cfg</span><br><span class="line">├── yolov1-tiny.cfg</span><br><span class="line">├── yolov2.cfg</span><br><span class="line">├── yolov2-tiny.cfg</span><br><span class="line">├── yolov2-tiny-voc.cfg</span><br><span class="line">├── yolov2-voc.cfg</span><br><span class="line">├── yolov3.cfg</span><br><span class="line">├── yolov3-openimages.cfg</span><br><span class="line">├── yolov3-spp.cfg</span><br><span class="line">├── yolov3-tiny.cfg</span><br><span class="line">└── yolov3-voc.cfg</span><br><span class="line"></span><br><span class="line">0 directories, 52 files</span><br></pre></td></tr></table></figure><p><code>cfg/</code>内存放的<code>*.cfg</code>文件，用于记录网络结构，类似Caffe的<code>*.prototxt</code>文件，通过调用<code>darknet.h/network *parse_network_cfg(char *filename)</code>，可读取指定的<code>*.cfg</code>文件，快速构建深度神经网络。</p><h1 id="include-，src-：底层实现"><a href="#include-，src-：底层实现" class="headerlink" title="include/，src/：底层实现"></a><code>include/</code>，<code>src/</code>：底层实现</h1><p><code>include/</code>内仅包含全局头文件<code>darknet.h</code>，主要用于定义枚举(enum)、结构体(struct)和声明常用函数，几个重要的定义声明整理如下(不全)</p><div class="table-container"><table><thead><tr><th>名称</th><th>类型</th><th>作用</th></tr></thead><tbody><tr><td>ACTIVATION</td><td>enum</td><td>激活函数类型</td></tr><tr><td>LAYER_TYPE</td><td>enum</td><td>网络层类型</td></tr><tr><td>COST_TYPE</td><td>enum</td><td>损失类型</td></tr><tr><td>learning_rate_policy</td><td>enum</td><td>学习率调整策略类型</td></tr><tr><td>data_type</td><td>enum</td><td>数据载入类型</td></tr><tr><td>image</td><td>struct</td><td>图像结构，数据存放格式为<code>CHW</code></td></tr><tr><td>update_args</td><td>struct</td><td>优化器参数</td></tr><tr><td>layer</td><td>struct</td><td>面向网络层对象，通用的网络层结构定义，除必要的内存指针(weight, delta, output等)，还包括前向(forward)、反向(backward)、参数更新(update)等函数指针</td></tr><tr><td>network</td><td>struct</td><td>面向网络对象，通用的网络结构定义，包括网络层指针(layers)、优化参数(batch, epoch, learning_rate, momentum, decay等)、输入输出内存指针(input, truth, delta, workspace等)</td></tr><tr><td>augment_args</td><td>struct</td><td>图像扩增属性</td></tr><tr><td>box</td><td>struct</td><td>边界框，包含<code>x, y, w, h</code></td></tr><tr><td>data</td><td>struct</td><td>用于读取数据，<code>X</code>与<code>y</code>为二维矩阵<code>matrix</code>，每行表示一个样本</td></tr><tr><td>load_args</td><td>struct</td><td>数据读取参数</td></tr><tr><td>network <em>load_network(char </em>cfg, char *weights, int clear)</td><td>function</td><td>载入网络，内部调用<code>parse_network_cfg</code>与<code>load_weights</code></td></tr><tr><td>float train_networks(network **nets, int n, data d, int interval)</td><td>function</td><td>训练网络，内部调用前向、反向、更新等函数</td></tr><tr><td>network <em>parse_network_cfg(char </em>filename)</td><td>function</td><td>解析<code>*.cfg</code>文件以快速构建网络</td></tr><tr><td>list <em>read_cfg(char </em>filename)</td><td>function</td><td>读取配置文件</td></tr><tr><td>void save_weights(network <em>net, char </em>filename)</td><td>function</td><td>保存网络权重</td></tr><tr><td>void load_weights(network <em>net, char </em>filename)</td><td>function</td><td>读取网络权重</td></tr><tr><td>void save_weights_upto(network <em>net, char </em>filename, int cutoff)</td><td>function</td><td>可指定截断的网络权重保存</td></tr><tr><td>void load_weights_upto(network <em>net, char </em>filename, int start, int cutoff)</td><td>function</td><td>可指定截断的网络权重读取</td></tr><tr><td>void forward_network(network *net)</td><td>function</td><td>网络前向运算，依次调用各层前向运算函数</td></tr><tr><td>void backward_network(network *net)</td><td>function</td><td>网络反向运算求取梯度，依次调用各层反向运算函数</td></tr><tr><td>void update_network(network *net)</td><td>function</td><td>更新网络参数，依次调用各层参数更新函数</td></tr><tr><td>void forward_network_gpu(network *net)</td><td>function</td><td>网络前向运算，CPU加速</td></tr><tr><td>void backward_network_gpu(network *net)</td><td>function</td><td>网络反向运算求取梯度，CPU加速</td></tr><tr><td>void update_network_gpu(network *net)</td><td>function</td><td>更新网络参数，CPU加速</td></tr><tr><td>void cuda_set_device(int n)</td><td>function</td><td>指定GPU设备</td></tr><tr><td>void cuda_free(float *x_gpu)</td><td>function</td><td>释放指针对应的显存</td></tr><tr><td>float <em>cuda_make_array(float </em>x, size_t n)</td><td>function</td><td>申请固定字节数的显存</td></tr><tr><td>void cuda_pull_array(float <em>x_gpu, float </em>x, size_t n)</td><td>function</td><td>从显存读取数据，存放至内存</td></tr><tr><td>void cuda_push_array(float <em>x_gpu, float </em>x, size_t n)</td><td>function</td><td>从内存读取数据，存放至显存</td></tr><tr><td>float <em>network_predict(network </em>net, float *input)</td><td>function</td><td>指定输入，前向运算并输出结果</td></tr><tr><td>int resize_network(network *net, int w, int h)</td><td>function</td><td>调整网络输入尺寸，相应改变各层尺寸</td></tr><tr><td>void free_network(network *net)</td><td>function</td><td>释放网络所占资源，内部释放各层资源</td></tr><tr><td>void free_layer(layer)</td><td>function</td><td>释放网络层所占资源</td></tr><tr><td>list <em>read_data_cfg(char </em>filename)</td><td>function</td><td>读取数据参数配置文件</td></tr><tr><td>load_args get_base_args(network *net)</td><td>function</td><td>读取网络参数以确定数据读取参数</td></tr><tr><td>pthread_t load_data(load_args args)</td><td>function</td><td>创建数据读取线程</td></tr><tr><td>void free_data(data d)</td><td>function</td><td>释放网络层所占资源</td></tr><tr><td>image load_image_color(char *filename, int w, int h)</td><td>function</td><td>读取3通道图</td></tr><tr><td>image resize_image(image im, int w, int h)</td><td>function</td><td>图像尺寸调整</td></tr><tr><td>image crop_image(image im, int dx, int dy, int w, int h)</td><td>function</td><td>图像裁剪</td></tr><tr><td>image rotate_image(image m, float rad)</td><td>function</td><td>图像旋转</td></tr><tr><td>void flip_image(image a)</td><td>function</td><td>图像翻转</td></tr><tr><td>int show_image(image p, const char *name, int ms)</td><td>function</td><td>图像显示，若OpenCV可用则显示，否则保存图像</td></tr><tr><td>void free_image(image m)</td><td>function</td><td>释放图像所占资源</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table></div><h1 id="examples-：算法实现"><a href="#examples-：算法实现" class="headerlink" title="examples/：算法实现"></a><code>examples/</code>：算法实现</h1><p>调用<code>include/</code>与<code>src/</code>底层函数，实现如<code>yolo</code>、<code>darknet</code>、<code>rnn</code>等算法，如<code>yolo</code>主函数内可进行训练、测试、演示等操作。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run_yolo</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *prefix = find_char_arg(argc, argv, <span class="string">"-prefix"</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">float</span> thresh = find_float_arg(argc, argv, <span class="string">"-thresh"</span>, <span class="number">.2</span>);</span><br><span class="line">    <span class="keyword">int</span> cam_index = find_int_arg(argc, argv, <span class="string">"-c"</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">int</span> frame_skip = find_int_arg(argc, argv, <span class="string">"-s"</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span>(argc &lt; <span class="number">4</span>)&#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"usage: %s %s [train/test/valid] [cfg] [weights (optional)]\n"</span>, argv[<span class="number">0</span>], argv[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> avg = find_int_arg(argc, argv, <span class="string">"-avg"</span>, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">char</span> *cfg = argv[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">char</span> *weights = (argc &gt; <span class="number">4</span>) ? argv[<span class="number">4</span>] : <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> *filename = (argc &gt; <span class="number">5</span>) ? argv[<span class="number">5</span>]: <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"test"</span>)) test_yolo(cfg, weights, filename, thresh);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"train"</span>)) train_yolo(cfg, weights);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"valid"</span>)) validate_yolo(cfg, weights);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"recall"</span>)) validate_yolo_recall(cfg, weights);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"demo"</span>)) demo(cfg, weights, thresh, cam_index, filename, voc_names, <span class="number">20</span>, frame_skip, prefix, avg, <span class="number">.5</span>, <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="python-：提供接口"><a href="#python-：提供接口" class="headerlink" title="python/：提供接口"></a><code>python/</code>：提供接口</h1><p>调用<code>ctypes</code>模块，可在python中调用darknet库，如检测算法调用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect</span><span class="params">(net, meta, image, thresh=<span class="number">.5</span>, hier_thresh=<span class="number">.5</span>, nms=<span class="number">.45</span>)</span>:</span></span><br><span class="line">    im = load_image(image, <span class="number">0</span>, <span class="number">0</span>)    <span class="comment"># 读取图片</span></span><br><span class="line">    num = c_int(<span class="number">0</span>)</span><br><span class="line">    pnum = pointer(num)</span><br><span class="line">    predict_image(net, im)          <span class="comment"># 前向运算</span></span><br><span class="line">    dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, <span class="keyword">None</span>, <span class="number">0</span>, pnum) <span class="comment"># 获取边界框</span></span><br><span class="line">    num = pnum[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> (nms): do_nms_obj(dets, num, meta.classes, nms)  <span class="comment"># NMS</span></span><br><span class="line"></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(meta.classes):</span><br><span class="line">            <span class="keyword">if</span> dets[j].prob[i] &gt; <span class="number">0</span>:</span><br><span class="line">                b = dets[j].bbox</span><br><span class="line">                res.append((meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h)))</span><br><span class="line">    res = sorted(res, key=<span class="keyword">lambda</span> x: -x[<span class="number">1</span>])</span><br><span class="line">    free_image(im)</span><br><span class="line">    free_detections(dets, num)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure></p><h1 id="scripts-：功能脚本"><a href="#scripts-：功能脚本" class="headerlink" title="scripts/：功能脚本"></a><code>scripts/</code>：功能脚本</h1><p>一些自动化脚本，如下载数据、生成样本标签等，略。</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习框架darknet【一】——简单使用</title>
      <link href="/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%80%E3%80%91%E2%80%94%E2%80%94%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%80%E3%80%91%E2%80%94%E2%80%94%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://louishsu.xyz/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%80%E3%80%91%E2%80%94%E2%80%94%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">深度学习框架darknet【一】——简单使用</a></li><li><a href="https://louishsu.xyz/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%8C%E3%80%91%E2%80%94%E2%80%94%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/" target="_blank" rel="noopener">深度学习框架darknet【二】——目录结构</a></li><li><a href="https://louishsu.xyz/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%89%E3%80%91%E2%80%94%E2%80%94%E8%B0%83%E5%8C%85%E5%A4%A7%E6%B3%95%E5%A5%BD/" target="_blank" rel="noopener">深度学习框架darknet【三】——调包大法好</a></li><li><a href="https://louishsu.xyz/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E5%9B%9B%E3%80%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9/" target="_blank" rel="noopener">深度学习框架darknet【四】——网络配置选项</a></li><li><a href="https://louishsu.xyz/2019/12/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%94%E3%80%91%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E8%A7%A3%E6%9E%90/" target="_blank" rel="noopener">深度学习框架darknet【五】——训练解析</a></li></ul><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前可用于深度学习的著名框架有</p><ul><li>tensorflow</li><li>pytorch</li><li>keras</li><li>theano</li><li>caffe</li><li>…</li></ul><p>这些给广大调参狗带来便利，但不利于学生党理解底层实现，例如神经网络的BP算法。</p><p>darknet是C语言实现的深度学习开源代码，细细品读，收获颇丰，开源万岁！</p><p><img src="/2019/12/08/深度学习框架darknet【一】——简单使用/darknet.png" alt="darknet"></p><p>附上官网链接和<code>Github</code>地址</p><ul><li><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO: Real-Time Object Detection</a></li><li><a href="https://github.com/pjreddie/darknet" target="_blank" rel="noopener">pjreddie/darknet: Convolutional Neural Networks</a></li><li><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">AlexeyAB/darknet: Windows and Linux version of Darknet Yolo v3 &amp; v2 Neural Networks for object detection (Tensor Cores are used)</a></li></ul><h1 id="Linux安装与使用"><a href="#Linux安装与使用" class="headerlink" title="Linux安装与使用"></a>Linux安装与使用</h1><p>首先，从Github下载作者的源码<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/pjreddie/darknet.git</span></span><br><span class="line">Cloning into 'darknet'...</span><br><span class="line">remote: Enumerating objects: 5901, done.</span><br><span class="line">remote: Total 5901 (delta 0), reused 0 (delta 0), pack-reused 5901</span><br><span class="line">Receiving objects: 100% (5901/5901), 6.16 MiB | 10.00 KiB/s, done.</span><br><span class="line">Resolving deltas: 100% (3915/3915), done.</span><br></pre></td></tr></table></figure></p><p>其目录结构如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> darknet</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tree -L 1</span></span><br><span class="line">.</span><br><span class="line">├── cfg/</span><br><span class="line">├── data/</span><br><span class="line">├── examples/</span><br><span class="line">├── include/</span><br><span class="line">├── python/</span><br><span class="line">├── scripts/</span><br><span class="line">├── src/</span><br><span class="line">├── LICENSE</span><br><span class="line">├── LICENSE.fuck</span><br><span class="line">├── LICENSE.gen</span><br><span class="line">├── LICENSE.gpl</span><br><span class="line">├── LICENSE.meta</span><br><span class="line">├── LICENSE.mit</span><br><span class="line">├── LICENSE.v1</span><br><span class="line">├── Makefile</span><br><span class="line">└── README.md</span><br><span class="line"></span><br><span class="line">7 directories, 9 files</span><br></pre></td></tr></table></figure></p><p>这其中包括<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO</a>的算法实现，若需尝试检测算法先进行编译，在<strong>Makefile中指定编译选项</strong>，由于现在使用的PC显存较小(1G)，暂时不采用CUDA加速(GPU=0, CUDNN=0)；Darknet可不安装图像库，但不支持图像显示，为了便于展示，编译加入本机已安装的OpenCV(OPENCV=1)<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GPU=0</span><br><span class="line">CUDNN=0</span><br><span class="line">OPENCV=1</span><br><span class="line">OPENMP=0</span><br><span class="line">DEBUG=0</span><br></pre></td></tr></table></figure></p><p>开始编译，编译完成后，除了新建三个目录<code>obj/</code>, <code>backup/</code>, <code>results/</code>外，还生成<strong>可执行文件<code>darknet</code>, 静态连接库<code>libdarknet.a</code>, 动态链接库<code>libdarknet.so</code></strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">  make</span></span><br><span class="line">mkdir -p obj        # 新建目录 obj/</span><br><span class="line">mkdir -p backup     # 新建目录 backup/</span><br><span class="line">mkdir -p results    # 新建目录 results/</span><br><span class="line">gcc -Iinclude/ -Isrc/ -DOPENCV `pkg-config --cflags opencv`  -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -DOPENCV -c ./src/gemm.c -o obj/gemm.o</span><br><span class="line">... # 略</span><br><span class="line">gcc -Iinclude/ -Isrc/ -DOPENCV `pkg-config --cflags opencv`  -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -DOPENCV -c ./examples/darknet.c -o obj/darknet.o</span><br><span class="line">gcc -Iinclude/ -Isrc/ -DOPENCV `pkg-config --cflags opencv`  -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -DOPENCV obj/captcha.o obj/lsd.o obj/super.o obj/art.o obj/tag.o obj/cifar.o obj/go.o obj/rnn.o obj/segmenter.o obj/regressor.o obj/classifier.o obj/coco.o obj/yolo.o obj/detector.o obj/nightmare.o obj/instance-segmenter.o obj/darknet.o libdarknet.a -o darknet -lm -pthread  `pkg-config --libs opencv` -lstdc++ libdarknet.a</span><br></pre></td></tr></table></figure></p><p>尝试运行YOLO3检测算法，下载权重、选择cfg文件，并指定图片进行检测<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://pjreddie.com/media/files/yolov3.weights</span></span><br><span class="line">... # 略</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./darknet detect cfg/yolov3.cfg yolov3.weights data/fastfurious.jpeg </span></span><br><span class="line">layer     filters    size              input                output</span><br><span class="line">    0 conv     32  3 x 3 / 1   608 x 608 x   3   -&gt;   608 x 608 x  32  0.639 BFLOPs</span><br><span class="line">    1 conv     64  3 x 3 / 2   608 x 608 x  32   -&gt;   304 x 304 x  64  3.407 BFLOPs</span><br><span class="line">    2 conv     32  1 x 1 / 1   304 x 304 x  64   -&gt;   304 x 304 x  32  0.379 BFLOPs</span><br><span class="line">... # 略</span><br><span class="line">  105 conv    255  1 x 1 / 1    76 x  76 x 256   -&gt;    76 x  76 x 255  0.754 BFLOPs</span><br><span class="line">  106 yolo</span><br><span class="line">Loading weights from yolov3.weights...Done!</span><br><span class="line">data/fastfurious.jpeg: Predicted in 27.746693 seconds.</span><br><span class="line">car: 100%</span><br><span class="line">person: 100%</span><br></pre></td></tr></table></figure></p><p>放上大光头靓照，检测效果非常好，由于未使用CUDA，所以检测速度较慢(27.7s)</p><p><img src="/2019/12/08/深度学习框架darknet【一】——简单使用/fastfurious.jpeg" alt="fastfurious"></p><p><img src="/2019/12/08/深度学习框架darknet【一】——简单使用/fastfurious_predictions.jpg" alt="fastfurious_predictions"></p><h1 id="Windows安装与使用"><a href="#Windows安装与使用" class="headerlink" title="Windows安装与使用"></a>Windows安装与使用</h1><ol><li>新建项目文件夹<code>project</code>，复制<code>darknet_AlexeyAB</code>中文件到该目录下</li><li>选择无gpu版本，打开<code>project/build/darknet/darknet_no_gpu.sln</code><br> <img src="/2019/12/08/深度学习框架darknet【一】——简单使用/darknet_build_win_no_gpu_sln.png" alt="darknet_build_win_no_gpu_sln"></li><li>修改版本为<code>release</code><br> <img src="/2019/12/08/深度学习框架darknet【一】——简单使用/darknet_build_win_release.png" alt="darknet_build_win_release"></li><li>安装<code>OpenCV3</code></li><li>一键<code>build</code>！如果需要生成<code>.dll</code>动态链接库，则配置<code>项目属性 -&gt; 常规 -&gt; 目标文件扩展名</code>为<code>.dll</code><br> <img src="/2019/12/08/深度学习框架darknet【一】——简单使用/darknet_build_win_build.png" alt="darknet_build_win_build"><br> <img src="/2019/12/08/深度学习框架darknet【一】——简单使用/darknet_build_win_success.png" alt="darknet_build_win_success"></li></ol><p>从<code>https://pjreddie.com/media/files/yolov3.weights</code>下载<code>yolo_v3</code>权重，放到放在<code>project/build/darknet/x64</code>目录，执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./darknet_no_gpu.exe detect cfg/yolov3.cfg yolov3.weights data/dog.jpg</span><br><span class="line">layer     filters    size              input                output</span><br><span class="line">   0 conv     32  3 x 3 / 1   416 x 416 x   3   -&gt;   416 x 416 x  32 0.299 BF</span><br><span class="line">   1 conv     64  3 x 3 / 2   416 x 416 x  32   -&gt;   208 x 208 x  64 1.595 BF</span><br><span class="line">   2 conv     32  1 x 1 / 1   208 x 208 x  64   -&gt;   208 x 208 x  32 0.177 BF</span><br><span class="line">   3 conv     64  3 x 3 / 1   208 x 208 x  32   -&gt;   208 x 208 x  64 1.595 BF</span><br><span class="line">   4 Shortcut Layer: 1</span><br><span class="line">   5 conv    128  3 x 3 / 2   208 x 208 x  64   -&gt;   104 x 104 x 128 1.595 BF</span><br><span class="line">   6 conv     64  1 x 1 / 1   104 x 104 x 128   -&gt;   104 x 104 x  64 0.177 BF</span><br><span class="line">   7 conv    128  3 x 3 / 1   104 x 104 x  64   -&gt;   104 x 104 x 128 1.595 BF</span><br><span class="line">   </span><br><span class="line">   ...</span><br><span class="line">  </span><br><span class="line">  94 yolo</span><br><span class="line">  95 route  91</span><br><span class="line">  96 conv    128  1 x 1 / 1    26 x  26 x 256   -&gt;    26 x  26 x 128 0.044 BF</span><br><span class="line">  97 upsample            2x    26 x  26 x 128   -&gt;    52 x  52 x 128</span><br><span class="line">  98 route  97 36</span><br><span class="line">  99 conv    128  1 x 1 / 1    52 x  52 x 384   -&gt;    52 x  52 x 128 0.266 BF</span><br><span class="line"> 100 conv    256  3 x 3 / 1    52 x  52 x 128   -&gt;    52 x  52 x 256 1.595 BF</span><br><span class="line"> 101 conv    128  1 x 1 / 1    52 x  52 x 256   -&gt;    52 x  52 x 128 0.177 BF</span><br><span class="line"> 102 conv    256  3 x 3 / 1    52 x  52 x 128   -&gt;    52 x  52 x 256 1.595 BF</span><br><span class="line"> 103 conv    128  1 x 1 / 1    52 x  52 x 256   -&gt;    52 x  52 x 128 0.177 BF</span><br><span class="line"> 104 conv    256  3 x 3 / 1    52 x  52 x 128   -&gt;    52 x  52 x 256 1.595 BF</span><br><span class="line"> 105 conv    255  1 x 1 / 1    52 x  52 x 256   -&gt;    52 x  52 x 255 0.353 BF</span><br><span class="line"> 106 yolo</span><br><span class="line">Total BFLOPS 65.864</span><br><span class="line">Loading weights from yolov3.weights...Done!</span><br></pre></td></tr></table></figure></p><p><img src="/2019/12/08/深度学习框架darknet【一】——简单使用/darknet_build_win_predict_dog.png" alt="darknet_build_win_predict_dog"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://blog.csdn.net/KayChanGEEK/article/details/84979441" target="_blank" rel="noopener">Windows配置darknet - 楷尘·极客 - CSDN博客</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>全连接网络BP算法推导【矩阵形式】</title>
      <link href="/2019/12/08/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9CBP%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%E3%80%90%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E3%80%91/"/>
      <url>/2019/12/08/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9CBP%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%E3%80%90%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在<a href="https://louishsu.xyz/2018/10/20/Feedforward-Neural-Network/" target="_blank" rel="noopener">Feedforward Neural Network/5. 梯度推导</a>中介绍了前馈神经网络的BP算法，但是推导结果为<strong>对元素的梯度</strong>，不便于编程且影响计算效率。本节<strong>以3层前馈全连接神经网络为例</strong>，介绍矩阵形式的BP算法推导。</p><h1 id="网络定义"><a href="#网络定义" class="headerlink" title="网络定义"></a>网络定义</h1><p>定义多层全连接网络：<br>【输入(input)】 -&gt; 全连接层 -&gt; sigmoid -&gt; 【隐藏层(hidden)】-&gt; 全连接层 -&gt; 【输出(output)】 -&gt; 损失函数层 -&gt; 损失值(loss)。</p><p>示意图如下(已省略激活函数层)</p><p><img src="/2019/12/08/全连接网络BP算法推导【矩阵形式】/nn.svg" alt="nn"></p><p>其<strong>符号定义</strong>为</p><ul><li>$\vec{x}^{(l-1)}$: 第$l$层的输入向量，维度为$n_{l-1} \times 1$；</li><li>$\vec{x}^{(l)}$: 第$l$层的输出向量，维度为$n_l \times 1$；</li><li><p>$f^{(l)}(\vec{x})$: 第$l$层输入输出映射，即</p><script type="math/tex; mode=display">\vec{x}^{(l)} = f^{(l)}(\vec{x}_{l-1}) \tag{1}</script><ul><li><p>全连接层: </p><script type="math/tex; mode=display">f^{(l)}(\vec{x}) = W^{(l)} \vec{x} + \vec{b}^{(l)} \tag{1.1}</script><p>  其中$W^{(l)}$, $\vec{b}^{(l)}$分别表示权重与偏置，维度分别为$n_l \times n_{l-1}$, $n_l \times 1$；</p></li><li><p>激活函数层: </p><script type="math/tex; mode=display">f^{(l)}(\vec{x}) = \sigma(\vec{x}) \tag{1.2.1}</script><p>  当激活函数采用sigmoid函数时，有</p><script type="math/tex; mode=display">f^{(l)}(x_j) = \frac{1}{1 + e^{- x_j}} \tag{1.2.2}</script></li><li><p>损失函数层：</p><script type="math/tex; mode=display">f^{(l)}(\vec{x}) = L(\vec{x}, \vec{t}) \tag{1.3.1}</script><p>  取MSE损失时，有</p><script type="math/tex; mode=display">L(\vec{x}, \vec{t}) = \frac{1}{2} ||\vec{x} - \vec{t}||_2^2 = \frac{1}{2} (\vec{x} - \vec{t})^T (\vec{x} - \vec{t}) \tag{1.3.2}</script><p>  其中$\vec{t}$表示groudtruth，$\vec{x}$表示网络的输出。</p></li></ul></li></ul><p>则有</p><script type="math/tex; mode=display">\begin{aligned}    \vec{x}^{(1)} = W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)} \\    \vec{x}^{(2)} = \sigma(\vec{x}^{(1)}) \\    \vec{x}^{(3)} = W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)} \\    x^{(4)} = L(\vec{x}^{(3)}, \vec{t}) \\\end{aligned} \tag{2}</script><blockquote><script type="math/tex; mode=display">W^{(l)} \vec{x}^{(l-1)} + \vec{b}^{(l)} = \begin{bmatrix}  W^{(l)}_{11} & W^{(l)}_{12} & \cdots & W^{(l)}_{1n_{l-1}} \\  \vdots & \vdots & \ddots & \vdots \\  W^{(l)}_{n_l1} & W^{(l)}_{n_l2} & \cdots & W^{(l)}_{n_l n_{l-1}} \\\end{bmatrix}\begin{bmatrix}  x^{(l-1)}_1 \\  \vdots \\  x^{(l-1)}_{n_{l-1}} \\\end{bmatrix} + \begin{bmatrix}  \vec{b}^{(l)}_1 \\  \vdots \\  \vec{b}^{(l)}_{n_l} \\\end{bmatrix}</script></blockquote><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><blockquote><p>标量对矩阵、向量的梯度见<a href="#%e9%99%84%e5%bd%95">附录</a>。</p></blockquote><p>每层输出$\vec{x}^{(l)} = f^{(l)}(\vec{x}^{(l-1)})$对输入$\vec{x}^{(l-1)}$的梯度为$f’^{(l)}(\vec{x}^{(l-1)})$(雅可比矩阵)，记作$p^{(l)}$，即</p><script type="math/tex; mode=display">p^{(l)} = \frac{\partial f^{(l)}(\vec{x}^{(l-1)})}{\partial \vec{x}^{(l-1)T}} \tag{3}</script><p>则对于已定义的网络，有</p><ul><li><p>全连接层(1)(向量对向量的梯度)：</p><script type="math/tex; mode=display">  p^{(1)} = \frac{\partial }{\partial \vec{x}^{(0)T}} (W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)}) = \frac{\partial \vec{x}^{(1)}}{\partial \vec{x}^{(0)T}} = W^{(1)} \tag{3.1}</script></li><li><p>激活函数层(2)(向量对向量的梯度)：</p><script type="math/tex; mode=display">  p^{(2)} = \frac{\partial }{\partial \vec{x}^{(1)T}} \sigma(\vec{x}^{(1)}) = \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)T}} = \rm{diag}[\sigma(\vec{x}^{(1)}) \odot (1 - \sigma(\vec{x}^{(1)}))] \tag{3.2}</script></li><li><p>全连接层(3)(向量对向量的梯度)：</p><script type="math/tex; mode=display">  p^{(3)} = \frac{\partial }{\partial \vec{x}^{(2)T}} (W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)}) = \frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)T}} = W^{(3)} \tag{3.3}</script></li><li><p>损失函数层(4)(标量对向量的梯度)：</p><script type="math/tex; mode=display">  p^{(4)} = \frac{\partial }{\partial \vec{x}^{(3)}} [\frac{1}{2} (\vec{x}^{(3)} - \vec{t})^T (\vec{x}^{(3)} - \vec{t})] = \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} = \vec{x}^{(3)} - \vec{t} \tag{3.4}</script></li></ul><blockquote><script type="math/tex; mode=display">n_1 = n_2; \quad n_4 = 1</script></blockquote><p>待更新参数为$W^{(1)}, \vec{b}^{(1)}, W^{(3)}, \vec{b}^{(3)}$。</p><ol><li><p>损失函数$L$对第$3$层参数$W^{(3)}, \vec{b}^{(3)}$的梯度，有</p><script type="math/tex; mode=display"> \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(3)}} =  \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \frac{\partial f^{(3)}(\vec{x}^{(2)})}{\partial W^{(3)}} =  \underbrace{p^{(4)}}_{n_3 \times 1}  \underbrace{\frac{\partial}{\partial W^{(3)}} (W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)})}_{1 \times n_2} =  p^{(4)} \vec{x}^{(2)T} \tag{4.1}</script><script type="math/tex; mode=display"> \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(3)}} =  (\frac{\partial f^{(3)}(\vec{x}^{(2)})}{\partial \vec{b}^{(3)T}})^T \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} = \underbrace{\frac{\partial}{\partial \vec{b}^{(3)T}} (W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)})}_{I_{n_3 \times n_3}} \underbrace{p^{(4)}}_{n_3 \times 1} = p^{(4)} \tag{4.2}</script></li><li><p>损失函数$L$对第$1$层参数$W^{(1)}, \vec{b}^{(1)}$的梯度，有</p><script type="math/tex; mode=display"> \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(1)}} =  \frac{\partial x^{(4)}}{\partial \vec{x}^{(1)}} \frac{\partial f^{(1)}(\vec{x}^{(0)})}{\partial W^{(1)}} \tag{5.1}</script><p> 其中$\frac{\partial x^{(4)}}{\partial \vec{x}^{(1)}}$为标量对向量的梯度，由式$(7.2)$可得</p><script type="math/tex; mode=display"> \frac{\partial x^{(4)}}{\partial \vec{x}^{(1)}} =  (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(1)}})^T  \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} =  (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)}}  \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)}})^T  \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \tag{5.2}</script><p> 代入式$(5.1)$得</p><script type="math/tex; mode=display"> \begin{aligned}     \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(1)}} \\     = \left[         (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)}}          \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)}})^T          \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}}      \right]     \frac{\partial f^{(1)}(\vec{x}^{(0)})}{\partial W^{(1)}} \\     = \underbrace{p^{(2)T}}_{n_2 \times n_2}     \underbrace{p^{(3)T}}_{n_2 \times n_3}     \underbrace{p^{(4)}}_{n_3 \times 1}     \underbrace{\frac{\partial}{\partial W^{(1)}} (W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)})}_{1 \times n_0} =      p^{(2)T} p^{(3)T} p^{(4)} \vec{x}^{(0)T} \end{aligned} \tag{5.3}</script><script type="math/tex; mode=display"> \begin{aligned}     \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(1)}}      = \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \\     = \left[         (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)}}          \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)}})^T          \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}}      \right]     \frac{\partial f^{(1)}(\vec{x}^{(0)})}{\partial \vec{b}^{(1)}} \\     = \underbrace{p^{(2)T}}_{n_2 \times n_2}     \underbrace{p^{(3)T}}_{n_2 \times n_3}     \underbrace{p^{(4)}}_{n_3 \times 1}     \underbrace{\frac{\partial}{\partial \vec{b}^{(1)}} (W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)})}_{I_{n_0 \times n_0}} =     p^{(2)T} p^{(3)T} p^{(4)} \end{aligned} \tag{5.4}</script></li></ol><p>如果记</p><script type="math/tex; mode=display">\begin{aligned}    \vec{\delta}^{(4)} = p^{(4)} = \vec{x}^{(3)} - \vec{t} \\    \vec{\delta}^{(3)} = p^{(3)T} \vec{\delta}^{(4)} = W^{(3)T} \vec{\delta}^{(4)} \\    \vec{\delta}^{(2)} = p^{(2)T} \vec{\delta}^{(3)} = \rm{diag}[\sigma(\vec{x}^{(1)}) \odot (1 - \vec{x}^{(1)})]^T \vec{\delta}^{(3)} \\\end{aligned}</script><p>最终得到梯度表达式</p><script type="math/tex; mode=display">\begin{aligned}    \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(3)}} = \vec{\delta}^{(4)} x^{(2)T} \\    \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(3)}} = \vec{\delta}^{(4)} \\    \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(1)}} =         \vec{\delta}^{(2)} \vec{x}^{(0)T} \\    \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(1)}} =        \vec{\delta}^{(2)}\end{aligned} \tag{*}</script><h1 id="附录：矩阵形式的链式求导"><a href="#附录：矩阵形式的链式求导" class="headerlink" title="附录：矩阵形式的链式求导"></a>附录：矩阵形式的链式求导</h1><ol><li><p>向量对向量<br> 假设三个向量满足依赖关系$\vec{x} \rightarrow \vec{y} \rightarrow \vec{z}$，分别为$m, n, p$维向量，则满足</p><script type="math/tex; mode=display"> \underbrace{\frac{\partial \vec{z}}{\partial \vec{x}}}_{p \times m} =  \underbrace{\frac{\partial \vec{z}}{\partial \vec{y}}}_{p \times n}  \underbrace{\frac{\partial \vec{y}}{\partial \vec{x}}}_{n \times m} \tag{7.1}</script><p> 注意：若其中任意变量为矩阵时上式子不成立，如$\vec{x} \rightarrow Y \rightarrow \vec{z}$</p></li><li><p>标量对向量<br> 假设有依赖关系$\vec{\vec{x}} \rightarrow \vec{\vec{y}} \rightarrow z$，分别为$m, n$维向量与标量，则$\frac{\partial z}{\partial \vec{x}}$维度为$m \times 1$，$\frac{\partial z}{\partial \vec{y}}$维度为$n \times 1$，$\frac{\partial \vec{y}}{\partial \vec{x}}$维度为$n \times m$，为保证矩阵维度相容，需增加转置</p><script type="math/tex; mode=display"> \underbrace{\frac{\partial z}{\partial \vec{x}}}_{m \times 1} =  \underbrace{(\frac{\partial \vec{y}}{\partial \vec{x}})^T}_{m \times n} \underbrace{\frac{\partial z}{\partial \vec{y}}}_{n \times 1} \tag{7.2}</script></li><li><p>标量对矩阵<br> 矩阵对矩阵求导定义复杂，不给出整体的求导法则，仅对矩阵中某元素进行求导。假设有依赖关系$X \rightarrow Y \rightarrow z$，则满足</p><script type="math/tex; mode=display"> \frac{\partial z}{\partial X_{ij}} = \sum_{k, l} \frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}} \tag{7.3}</script></li></ol><p><strong>特别地</strong>，对于$z = f(Y), Y = AX + B$，求$\frac{\partial z}{\partial X}$时，由上式应满足</p><script type="math/tex; mode=display">\frac{\partial z}{\partial X_{ij}} = \sum_{k, l} \frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}}</script><p>其中</p><script type="math/tex; mode=display">\frac{\partial Y_{kl}}{\partial X_{ij}} = \frac{\partial \sum_s A_{ks} X_{sl}}{\partial X_{ij}} = \frac{\partial A_{ki} X_{il}}{\partial X_{ij}} = A_{ki} \delta_{lj}, \quad\delta_{lj} =   \begin{cases}      1 & l = j \\      0 & \rm{otherwise}  \end{cases}</script><p>那么</p><script type="math/tex; mode=display">\frac{\partial z}{\partial X_{ij}} = \sum_{k, l} \frac{\partial z}{\partial Y_{kl}} A_{ki} \delta_{lj} =\sum_{k} \frac{\partial z}{\partial Y_{kj}} A_{ki}</script><p>也即$A^T$第$i$行与$\frac{\partial z}{\partial Y}$第$j$列内积，即</p><script type="math/tex; mode=display">\frac{\partial z}{\partial X} = A^T \frac{\partial z}{\partial Y} \tag{8.1}</script><p>类似的，对于$z = f(\vec{y}), \vec{y} = A\vec{x} + \vec{b}$</p><script type="math/tex; mode=display">\frac{\partial z}{\partial \vec{x}} = A^T \frac{\partial z}{\partial \vec{y}} \tag{8.2}</script><p>对于$z = f(Y), Y = XA + B$</p><script type="math/tex; mode=display">\frac{\partial z}{X} = \frac{\partial z}{\partial Y} A^T \tag{8.3}</script><p>对于$z = f(\vec{y}), \vec{y} = X\vec{a} + \vec{b}$</p><script type="math/tex; mode=display">\frac{\partial z}{\partial X} = \frac{\partial z}{\partial \vec{y}} \vec{a}^T \tag{8.4}</script>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gcc/g++版本切换</title>
      <link href="/2019/12/04/gcc-g-%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2/"/>
      <url>/2019/12/04/gcc-g-%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>gcc/g++为Linux下C/C++编译器，但版本差异较大，如CUDA编译时不支持gcc-6之后的版本，故需要安装多个版本适应开发任务。</p><h1 id="详细"><a href="#详细" class="headerlink" title="详细"></a>详细</h1><p>Ubuntu 18.04预装gcc/g++ 7.3，若需要安装gcc/g++-5，详细操作步骤如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 查看当前所有已安装版本</span><br><span class="line"><span class="meta">$</span> ls /usr/bin/gcc*</span><br><span class="line">/usr/bin/gcc    /usr/bin/gcc-7   /usr/bin/gcc-ar-6  /usr/bin/gcc-nm    /usr/bin/gcc-nm-7    /usr/bin/gcc-ranlib-6</span><br><span class="line">/usr/bin/gcc-6  /usr/bin/gcc-ar  /usr/bin/gcc-ar-7  /usr/bin/gcc-nm-6  /usr/bin/gcc-ranlib  /usr/bin/gcc-ranlib-7</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> ls /usr/bin/g++*</span><br><span class="line">/usr/bin/g++    /usr/bin/g++-6   /usr/bin/g++-7</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 安装gcc-5与g++-5</span><br><span class="line"><span class="meta">$</span> sudo apt-get install gcc-5</span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree       </span><br><span class="line">Reading state information... Done</span><br><span class="line">The following packages were automatically installed and are no longer required:</span><br><span class="line">  javascript-common libjs-jquery libjs-sphinxdoc libjs-underscore python-apt python-m2crypto python-pkg-resources python-typing</span><br><span class="line">Use 'sudo apt autoremove' to remove them.</span><br><span class="line">The following additional packages will be installed:</span><br><span class="line">  cpp-5 gcc-5-base libasan2 libgcc-5-dev libisl15 libmpx0</span><br><span class="line">Suggested packages:</span><br><span class="line">  gcc-5-locales gcc-5-multilib gcc-5-doc libgcc1-dbg libgomp1-dbg libitm1-dbg libatomic1-dbg libasan2-dbg liblsan0-dbg libtsan0-dbg libubsan0-dbg libcilkrts5-dbg libmpx0-dbg</span><br><span class="line">  libquadmath0-dbg</span><br><span class="line">The following NEW packages will be installed:</span><br><span class="line">  cpp-5 gcc-5 gcc-5-base libasan2 libgcc-5-dev libisl15 libmpx0</span><br><span class="line">0 upgraded, 7 newly installed, 0 to remove and 2 not upgraded.</span><br><span class="line">Need to get 19.2 MB of archives.</span><br><span class="line">After this operation, 61.5 MB of additional disk space will be used.</span><br><span class="line">Do you want to continue? [Y/n]</span><br><span class="line">Get:1 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 gcc-5-base amd64 5.5.0-12ubuntu1 [17.1 kB]</span><br><span class="line">Get:2 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libisl15 amd64 0.18-4 [548 kB]</span><br><span class="line">Get:3 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 cpp-5 amd64 5.5.0-12ubuntu1 [7,785 kB]                                                                                         </span><br><span class="line">Get:4 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libasan2 amd64 5.5.0-12ubuntu1 [264 kB]                                                                                       </span><br><span class="line">Get:5 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libmpx0 amd64 5.5.0-12ubuntu1 [9,888 B]                                                                                       </span><br><span class="line">Get:6 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libgcc-5-dev amd64 5.5.0-12ubuntu1 [2,224 kB]                                                                                 </span><br><span class="line">Get:7 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 gcc-5 amd64 5.5.0-12ubuntu1 [8,357 kB]                                                                                         </span><br><span class="line">Fetched 19.2 MB in 9s (2,082 kB/s)                                                                                                                                                         </span><br><span class="line">Selecting previously unselected package gcc-5-base:amd64.</span><br><span class="line">(Reading database ... 183097 files and directories currently installed.)</span><br><span class="line">Preparing to unpack .../0-gcc-5-base_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking gcc-5-base:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Selecting previously unselected package libisl15:amd64.</span><br><span class="line">Preparing to unpack .../1-libisl15_0.18-4_amd64.deb ...</span><br><span class="line">Unpacking libisl15:amd64 (0.18-4) ...</span><br><span class="line">Selecting previously unselected package cpp-5.</span><br><span class="line">Preparing to unpack .../2-cpp-5_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking cpp-5 (5.5.0-12ubuntu1) ...</span><br><span class="line">Selecting previously unselected package libasan2:amd64.</span><br><span class="line">Preparing to unpack .../3-libasan2_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking libasan2:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Selecting previously unselected package libmpx0:amd64.</span><br><span class="line">Preparing to unpack .../4-libmpx0_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking libmpx0:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Selecting previously unselected package libgcc-5-dev:amd64.</span><br><span class="line">Preparing to unpack .../5-libgcc-5-dev_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Selecting previously unselected package gcc-5.</span><br><span class="line">Preparing to unpack .../6-gcc-5_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking gcc-5 (5.5.0-12ubuntu1) ...</span><br><span class="line">Setting up libisl15:amd64 (0.18-4) ...</span><br><span class="line">Processing triggers for libc-bin (2.27-3ubuntu1) ...</span><br><span class="line">Processing triggers for man-db (2.8.3-2ubuntu0.1) ...</span><br><span class="line">Setting up gcc-5-base:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Setting up libmpx0:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Setting up libasan2:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Setting up libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Setting up cpp-5 (5.5.0-12ubuntu1) ...</span><br><span class="line">Setting up gcc-5 (5.5.0-12ubuntu1) ...</span><br><span class="line">Processing triggers for libc-bin (2.27-3ubuntu1) ...</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo apt-get install g++-5</span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree       </span><br><span class="line">Reading state information... Done</span><br><span class="line">The following packages were automatically installed and are no longer required:</span><br><span class="line">  javascript-common libjs-jquery libjs-sphinxdoc libjs-underscore python-apt python-m2crypto python-pkg-resources python-typing</span><br><span class="line">Use 'sudo apt autoremove' to remove them.</span><br><span class="line">The following additional packages will be installed:</span><br><span class="line">  libstdc++-5-dev</span><br><span class="line">Suggested packages:</span><br><span class="line">  g++-5-multilib gcc-5-doc libstdc++6-5-dbg libstdc++-5-doc</span><br><span class="line">The following NEW packages will be installed:</span><br><span class="line">  g++-5 libstdc++-5-dev</span><br><span class="line">0 upgraded, 2 newly installed, 0 to remove and 2 not upgraded.</span><br><span class="line">Need to get 9,864 kB of archives.</span><br><span class="line">After this operation, 38.6 MB of additional disk space will be used.</span><br><span class="line">Do you want to continue? [Y/n]</span><br><span class="line">Get:1 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libstdc++-5-dev amd64 5.5.0-12ubuntu1 [1,415 kB]</span><br><span class="line">Get:2 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 g++-5 amd64 5.5.0-12ubuntu1 [8,450 kB]</span><br><span class="line">Fetched 9,864 kB in 45s (220 kB/s)                                                                                                                                                         </span><br><span class="line">Selecting previously unselected package libstdc++-5-dev:amd64.</span><br><span class="line">(Reading database ... 183333 files and directories currently installed.)</span><br><span class="line">Preparing to unpack .../libstdc++-5-dev_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Selecting previously unselected package g++-5.</span><br><span class="line">Preparing to unpack .../g++-5_5.5.0-12ubuntu1_amd64.deb ...</span><br><span class="line">Unpacking g++-5 (5.5.0-12ubuntu1) ...</span><br><span class="line">Processing triggers for man-db (2.8.3-2ubuntu0.1) ...</span><br><span class="line">Setting up libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...</span><br><span class="line">Setting up g++-5 (5.5.0-12ubuntu1) ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 设置版本优先级</span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 10</span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 10</span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-6 10</span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 10</span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 10</span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 10</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 选择对应版本的gcc/g++</span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --config gcc</span><br><span class="line">There are 3 choices for the alternative gcc (providing /usr/bin/gcc).</span><br><span class="line"></span><br><span class="line">  Selection    Path            Priority   Status</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">  0            /usr/bin/gcc-6   10        auto mode</span><br><span class="line">  1            /usr/bin/gcc-5   10        manual mode</span><br><span class="line">* 2            /usr/bin/gcc-6   10        manual mode</span><br><span class="line">  3            /usr/bin/gcc-7   10        manual mode</span><br><span class="line"></span><br><span class="line">Press &lt;enter&gt; to keep the current choice[*], or type selection number: 1    # 输入标签</span><br><span class="line">update-alternatives: using /usr/bin/gcc-5 to provide /usr/bin/gcc (gcc) in manual mode</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo update-alternatives --config g++</span><br><span class="line">There are 3 choices for the alternative g++ (providing /usr/bin/g++).</span><br><span class="line"></span><br><span class="line">  Selection    Path            Priority   Status</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">  0            /usr/bin/g++-6   10        auto mode</span><br><span class="line">  1            /usr/bin/g++-5   10        manual mode</span><br><span class="line">* 2            /usr/bin/g++-6   10        manual mode</span><br><span class="line">  3            /usr/bin/g++-7   10        manual mode</span><br><span class="line"></span><br><span class="line">Press &lt;enter&gt; to keep the current choice[*], or type selection number: 1    # 输入标签</span><br><span class="line">update-alternatives: using /usr/bin/g++-5 to provide /usr/bin/g++ (g++) in manual mode</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 查看当前版本号</span><br><span class="line"><span class="meta">$</span> gcc -v</span><br><span class="line">Using built-in specs.</span><br><span class="line">COLLECT_GCC=gcc</span><br><span class="line">COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper</span><br><span class="line">Target: x86_64-linux-gnu</span><br><span class="line">Configured with: ../src/configure -v --with-pkgversion='Ubuntu 5.5.0-12ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu</span><br><span class="line">Thread model: posix</span><br><span class="line">gcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1)</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> g++ -v</span><br><span class="line">Using built-in specs.</span><br><span class="line">COLLECT_GCC=g++</span><br><span class="line">COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper</span><br><span class="line">Target: x86_64-linux-gnu</span><br><span class="line">Configured with: ../src/configure -v --with-pkgversion='Ubuntu 5.5.0-12ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu</span><br><span class="line">Thread model: posix</span><br><span class="line">gcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.jianshu.com/p/f66eed3a3a25" target="_blank" rel="noopener">linux下gcc、g++不同版本的安装和切换</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>详细！MTCNN训练全过程！</title>
      <link href="/2019/11/10/%E8%AF%A6%E7%BB%86%EF%BC%81MTCNN%E8%AE%AD%E7%BB%83%E5%85%A8%E8%BF%87%E7%A8%8B%EF%BC%81/"/>
      <url>/2019/11/10/%E8%AF%A6%E7%BB%86%EF%BC%81MTCNN%E8%AE%AD%E7%BB%83%E5%85%A8%E8%BF%87%E7%A8%8B%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一下详细的MTCNN训练过程，之前损失函数的问题，训练得到网络效果一般，就拖到了最近。原理在之前写的博客<a href="https://louishsu.xyz/2019/05/05/Face-Detection-MTCNN/" target="_blank" rel="noopener">Face Detection: MTCNN</a>中已说明，本文不做详细介绍。详细代码可见本人代码仓库<a href="https://github.com/isLouisHsu/MTCNN_Darknet" target="_blank" rel="noopener">MTCNN_Darknet - Github</a>。</p><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p><img src="/2019/11/10/详细！MTCNN训练全过程！/dataset1.jpg" alt="dataset1"></p><p><img src="/2019/11/10/详细！MTCNN训练全过程！/dataset2.jpg" alt="dataset2"></p><p>数据集使用的是<a href="http://shuoyang1213.me/WIDERFACE/index.html" target="_blank" rel="noopener">WIDER FACE: A Face Detection Benchmark</a>与<a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm" target="_blank" rel="noopener">Deep Convolutional Network Cascade for Facial Point Detection</a>，前者用于生成识别分类任务、定位回归任务的数据，后者用于生成关键点回归任务的数据。</p><p>我们所需的文件有<code>WIDER_train.zip</code>，<code>WIDER_valid.zip</code>，<code>wider_face_split.zip</code>，<code>train.zip</code>，解压整理后得到文件目录如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">data/</span><br><span class="line">├─WIDER/</span><br><span class="line">│   ├── wider_face_train_bbx_gt.txt</span><br><span class="line">│   ├── wider_face_val_bbx_gt.txt</span><br><span class="line">│   ├── WIDER_train/</span><br><span class="line">│   └── WIDER_val/</span><br><span class="line">└─Align/</span><br><span class="line">    ├── lfw_5590/</span><br><span class="line">    ├── net_7876/</span><br><span class="line">    ├── testImageList.txt</span><br><span class="line">    └── trainImageList.txt</span><br></pre></td></tr></table></figure></p><p>运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/prepare_data/merge_annotations.py" target="_blank" rel="noopener">prepare_data/merge_annotations.py</a>后，合并标注文件得到<code>data/annotations.txt</code>。</p><h1 id="PNet"><a href="#PNet" class="headerlink" title="PNet"></a>PNet</h1><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size           input                output</span><br><span class="line">0 conv      10      3 x 3 / 1    12 x  12 x   3   -&gt;    10 x  10 x  10  0.000 BFLOPs</span><br><span class="line">1 max               2 x 2 / 2    10 x  10 x  10   -&gt;     5 x   5 x  10</span><br><span class="line">2 conv      16      3 x 3 / 1     5 x   5 x  10   -&gt;     3 x   3 x  16  0.000 BFLOPs</span><br><span class="line">3 conv      32      3 x 3 / 1     3 x   3 x  16   -&gt;     1 x   1 x  32  0.000 BFLOPs</span><br><span class="line">4 conv      15      1 x 1 / 1     1 x   1 x  32   -&gt;     1 x   1 x  15  0.000 BFLOPs</span><br></pre></td></tr></table></figure><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/prepare_data/pnet_12x12.py" target="_blank" rel="noopener">prepare_data/pnet_12x12.py</a>，对原始图像进行随机采样，注意以下几点：</p><ol><li>WIDER用作分类、定位任务，Align用作关键点定位任务；</li><li>滤除尺寸小于40(pixel)的人脸图像；</li><li>采样数目设置<ul><li>对每张图中任意采样$5 \times n_{faces}$份，再在每个人脸附近采样5份$iou &lt; 0.3$的人脸图像作为负样本(0)；</li><li>每个人脸附近采样10份$iou &gt; 0.65$的人脸图像作为正样本(1)；</li><li>每个人脸附近采样10份$0.4 &lt; iou &lt; 0.65$的人脸图像作为部分样本(-1)；</li><li>每个Align样本附近采样50份关键点样本(-2)。</li></ul></li><li>数据集扩增<ul><li>WIDER：随机左右镜像翻转；</li><li>Align：随机左右镜像翻转，$\pm 15$度随机旋转；</li></ul></li></ol><p>生成以下文件及文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data/</span><br><span class="line">├── 12x12/</span><br><span class="line">├── 12x12.txt</span><br><span class="line">├── 12x12_train.txt</span><br><span class="line">├── 12x12_valid.txt</span><br><span class="line">└── 12x12_test.txt</span><br></pre></td></tr></table></figure><p>其中<code>12x12/</code>包含随机采样得到的尺寸为$12 \times 12$图像，用于输入网络进行训练；<code>12x12.txt</code>包含所有图像的标注，其格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[image path] [sample label] [annotations]</span><br></pre></td></tr></table></figure><p>其中负样本(0)无annotations，正样本(1)与部分样本(-1)包含定位框左上右下角的相对偏移量(4个)，关键点样本(-2)包含5个关键点相对于图片左上角的相对偏移量(10个)，例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">../data/12x12/25219.jpg 0</span><br><span class="line">../data/12x12/25269.jpg 1 0.07142857142857142 0.03571428571428571 -0.21428571428571427 0.10714285714285714</span><br><span class="line">../data/12x12/25231.jpg -1 -0.030303030303030304 -0.12121212121212122 0.18181818181818182 0.696969696969697</span><br><span class="line">../data/12x12/3926594.jpg -2 0.29134029571831355 0.08374723013072503 0.7777450550823473 0.20460741825864107 0.2530593377292015 0.5109005869899402 0.30317271612810937 0.767843704149384 0.6327263098562724 0.8486940727227192</span><br></pre></td></tr></table></figure><p><code>12x12_*.txt</code>三个文件为对<code>12x12.txt</code>进行一定比例划分得到的标注文件，这里采用的比例为<code>0.6: 0.15: 0.25</code>。</p><p>经统计，其样本数量与比例详细信息如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">==================================================</span><br><span class="line">Totally 3397282 images</span><br><span class="line">Pos(1): Neg(0): Part(-1): Landmark(-2) = 0.3107857987650127: 0.3618628067967275: 0.12916354897827145: 0.1981878454599883</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Cls      | Pos(1)              : Neg(0)   = 0.858849798674096</span><br><span class="line">Offset   | [Pos(1) + Part(-1)] : n_samples = 0.4399493477432842</span><br><span class="line">Landmark | Landmark(-2)         : n_samples = 0.1981878454599883</span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure></p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>数据准备就绪后，可进行网络训练，运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/mtcnn_py/main_pnet.py" target="_blank" rel="noopener">mtcnn_py/main_pnet.py</a>。批次大小设置为512，初始学习率0.01，权重衰减设置为4e-5，采用Adam优化器，学习率调整使用指数衰减策略，每代衰减95%，对于PNet，进行20代训练即可，得到权重文件<code>mtcnn_py/ckptdir/PNet.pkl</code>。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p><img src="/2019/11/10/详细！MTCNN训练全过程！/1.jpg" alt="1"></p><h1 id="RNet"><a href="#RNet" class="headerlink" title="RNet"></a>RNet</h1><h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size           input                output</span><br><span class="line">0 conv     28       3 x 3 / 1    24 x  24 x   3   -&gt;    22 x  22 x  28  0.001 BFLOPs</span><br><span class="line">1 max               3 x 3 / 2    22 x  22 x  28   -&gt;    11 x  11 x  28</span><br><span class="line">2 conv     48       3 x 3 / 1    11 x  11 x  28   -&gt;     9 x   9 x  48  0.002 BFLOPs</span><br><span class="line">3 max               3 x 3 / 2     9 x   9 x  48   -&gt;     4 x   4 x  48</span><br><span class="line">4 conv     64       2 x 2 / 1     4 x   4 x  48   -&gt;     3 x   3 x  64  0.000 BFLOPs</span><br><span class="line">5 connected                                 576   -&gt;               128</span><br><span class="line">6 connected                                 128   -&gt;                15</span><br></pre></td></tr></table></figure><h2 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h2><p>运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/prepare_data/rnet_24x24.py" target="_blank" rel="noopener">prepare_data/rnet_24x24.py</a>，生成以下文件及文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data/</span><br><span class="line">├── 24x24/</span><br><span class="line">├── 24x24.txt</span><br><span class="line">├── 24x24_train.txt</span><br><span class="line">├── 24x24_valid.txt</span><br><span class="line">├── 24x24_test.txt</span><br><span class="line">└── rDets.npy</span><br></pre></td></tr></table></figure><p>RNet训练数据需要用到训练好的PNet生成候选框。首先利用PNet对每张原始图片进行检测，保存检测得到的候选框，文件为<code>data/rDets.npy</code>。</p><p>再依据真实标注框对每个候选框进行判别，由于负样本众多，每张图片保留15份负样本，保留全部正样本与部分样本。关键点样本与PNet一致，每张原始图片生成80份关键点样本。</p><p>经统计，样本数目如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">==================================================</span><br><span class="line">Totally 1954946 images</span><br><span class="line">Pos(1): Neg(0): Part(-1): Landmark(-2) = 0.05522607785585893: 0.10885466913152589: 0.2848656689238475: 0.5510535840887677</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Cls      | Pos(1)              : Neg(0)   = 0.507337703531402</span><br><span class="line">Offset   | [Pos(1) + Part(-1)] : n_samples = 0.34009174677970644</span><br><span class="line">Landmark | Landmark(-2)         : n_samples = 0.5510535840887677</span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure><h2 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h2><p>运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/mtcnn_py/main_rnet.py" target="_blank" rel="noopener">mtcnn_py/main_rnet.py</a>。批次大小设置为512，初始学习率0.01，权重衰减设置为4e-5，采用Adam优化器，学习率调整使用指数衰减策略，每代衰减95%，进行50代训练，得到权重文件<code>mtcnn_py/ckptdir/RNet.pkl</code>。。</p><h2 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h2><p><img src="/2019/11/10/详细！MTCNN训练全过程！/2.jpg" alt="2"></p><h1 id="ONet"><a href="#ONet" class="headerlink" title="ONet"></a>ONet</h1><h2 id="网络结构-2"><a href="#网络结构-2" class="headerlink" title="网络结构"></a>网络结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size              input                output</span><br><span class="line">0 conv     32       3 x 3 / 1    48 x  48 x   3   -&gt;    46 x  46 x  32  0.004 BFLOPs</span><br><span class="line">1 max               3 x 3 / 2    46 x  46 x  32   -&gt;    23 x  23 x  32</span><br><span class="line">2 conv     64       3 x 3 / 1    23 x  23 x  32   -&gt;    21 x  21 x  64  0.016 BFLOPs</span><br><span class="line">3 max               3 x 3 / 2    21 x  21 x  64   -&gt;    10 x  10 x  64</span><br><span class="line">4 conv     64       3 x 3 / 1    10 x  10 x  64   -&gt;     8 x   8 x  64  0.005 BFLOPs</span><br><span class="line">5 max               2 x 2 / 2     8 x   8 x  64   -&gt;     4 x   4 x  64</span><br><span class="line">6 conv    128       2 x 2 / 1     4 x   4 x  64   -&gt;     3 x   3 x 128  0.001 BFLOPs</span><br><span class="line">7 connected                                1152   -&gt;               256</span><br><span class="line">8 connected                                 256   -&gt;                15</span><br></pre></td></tr></table></figure><h2 id="数据准备-2"><a href="#数据准备-2" class="headerlink" title="数据准备"></a>数据准备</h2><p>运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/prepare_data/onet_48x48.py" target="_blank" rel="noopener">prepare_data/onet_48x48.py</a>，生成以下文件及文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data/</span><br><span class="line">├── 48x48/</span><br><span class="line">├── 48x48.txt</span><br><span class="line">├── 48x48_train.txt</span><br><span class="line">├── 48x48_valid.txt</span><br><span class="line">├── 48x48_test.txt</span><br><span class="line">└── oDets.npy</span><br></pre></td></tr></table></figure><p>数据生成方法与RNet一致，不同之处是用到PNet与RNet生成的候选框，保存检测得到的候选框，文件为<code>data/oDets.npy</code>。</p><p>由于负样本众多，每张图片保留10份负样本，保留全部正样本与部分样本。关键点样本与PNet一致，每张原始图片生成80份关键点样本。</p><p>经统计，样本数目如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">==================================================</span><br><span class="line">Totally 1106392 images</span><br><span class="line">Pos(1): Neg(0): Part(-1): Landmark(-2) = 0.04348278006348564: 0.12449656179726534: 0.10240583807547415: 0.7296148200637749</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Cls      | Pos(1)              : Neg(0)   = 0.34926892305905244</span><br><span class="line">Offset   | [Pos(1) + Part(-1)] : n_samples = 0.14588861813895979</span><br><span class="line">Landmark | Landmark(-2)         : n_samples = 0.7296148200637749</span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure><h2 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h2><p>运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/mtcnn_py/main_onet.py" target="_blank" rel="noopener">mtcnn_py/main_onet.py</a>，批次大小设置为512，初始学习率0.01，权重衰减设置为4e-5，采用Adam优化器，学习率调整使用指数衰减策略，每代衰减95%，进行30代训练，得到权重文件<code>mtcnn_py/ckptdir/RNet.pkl</code>。</p><h2 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h2><p><img src="/2019/11/10/详细！MTCNN训练全过程！/3.jpg" alt="3"></p><h1 id="转C语言"><a href="#转C语言" class="headerlink" title="转C语言"></a>转C语言</h1><p>C语言框架使用的是<a href="https://github.com/pjreddie/darknet" target="_blank" rel="noopener">darknet</a>，我对它进行了一些修改，在<a href="https://github.com/isLouisHsu/DarkerNet" target="_blank" rel="noopener">isLouisHsu/DarkerNet</a>，安装使用说明在仓库文档说明。</p><p>运行<a href="https://github.com/isLouisHsu/MTCNN_Darknet/blob/master/mtcnn_py/extract_weights.py" target="_blank" rel="noopener">mtcnn_py/extract_weights.py</a>生成权重<code>mtcnn_c/weights/*.weights</code>，运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd mtcnn_c/</span><br><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">cmake .. &amp;&amp; make</span><br><span class="line">cd ..</span><br><span class="line">./mtcnn -h</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1604.02878" target="_blank" rel="noopener">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks - arXiv</a></li><li><a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" target="_blank" rel="noopener">kpzhang93/MTCNN_face_detection_alignment - Github</a></li><li><a href="https://github.com/AITTSMD/MTCNN-Tensorflow" target="_blank" rel="noopener">AITTSMD/MTCNN-Tensorflow - Github</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CenterNet - Objects as Points</title>
      <link href="/2019/10/09/CenterNet-Objects-as-Points/"/>
      <url>/2019/10/09/CenterNet-Objects-as-Points/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前大多检测算法将物体视作边界框进行检测，并采用锚框(Anchor)，能很大程度提升检测算法的表现，但是Anchor难以确定参数，且增加大量网络运算量。目前的物体检测算法开始向无Anchor的方向发展，如本文的CenterNet，是把物体所在包围框作为点，并结合热图的形式进行。除物体检测外，此算法还可用于人体姿态识别、3D物体检测等。</p><p>个人理解，采用热图的形式，是将Anchor一定程度地连续化了。</p><h1 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>In this paper, we provide a much simpler and more efficient alternative. We <strong>represent objects by a single point at their bounding box center</strong> (see Figure 2). Other properties, such as object size, dimension, 3D extent, orientation, and pose are then <strong>regressed directly from image features at the center location</strong>. Object detection is then a standard keypoint estimation problem [3,39,60]. We simply feed the input image to a fully convolutional network [37, 40] that generates a heatmap. <strong>Peaks in this heatmap correspond to object centers</strong>. Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised learning [39,60]. Inference is a single network forward-pass, <strong>without non-maximal suppression for post-processing</strong>.<br><img src="/2019/10/09/CenterNet-Objects-as-Points/Fig2.png" alt="Figure 2"><br><img src="/2019/10/09/CenterNet-Objects-as-Points/Fig3.png" alt="Figure 3"></p></blockquote><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p>若有某三通道图片$I \in \mathcal{R}^{W \times H \times 3}$，经卷积神经网络后得到输出特征图$\left[\hat{Y} || \hat{O} || \hat{S} \right] \in [0, 1]^{\frac{W}{R} \times \frac{H}{R} \times (C + 4)}$。其中$R$为降采样系数，表示特征图上单个位置处的点映射回原图后网格的大小，文中默认为$R=4$；$C$可根据关键点类型指定，如人体姿态估计时指定$C=17$(17个关键点)，物体检测时指定$C=80$(80类物体)。$\hat{Y}_{x, y, c} = 1$表示对应关键点，$\hat{Y}_{x, y, c} = 0$表示为背景。</p><p><img src="/2019/10/09/CenterNet-Objects-as-Points/Fig4.png" alt="Figure 4"></p><p>ground truth设置为热图(heatmap)的形式，采用高斯核</p><script type="math/tex; mode=display">Y_{x, y, c} =     \exp\left(        - \frac{(x - \tilde{p}_x)^2 + (y - \tilde{p}_y)^2}{2 \sigma_p^2}    \right) \tag{1}</script><p>其中$\tilde{p} = \lfloor \frac{p}{R} \rfloor$，$p$为物体中心点在原图中的坐标；$\sigma_p$称对象尺寸自适应标准差(object size-adaptive standard deviation)，以解决物体尺寸不一致的问题。若两个同类别对象相距过近，其对应类别特征图上重叠部分用逐元素取大的策略(element-wise maximun)。</p><p><strong>关键点</strong>为热图的回归(keypoint regression)，损失函数参考Focal Loss，添加$(1 - Y_{x, y, z})^{\beta}$项，使其适应回归任务</p><script type="math/tex; mode=display">L_k = - \frac{1}{N} \sum_{x, y, z}\begin{cases}    (1 - \hat{Y}_{x, y, z})^{\alpha} \log (\hat{Y}_{x, y, z}) & Y_{x, y, z} = 1 \\    (1 - Y_{x, y, z})^{\beta} (\hat{Y}_{x, y, z})^{\alpha} \log (1 - \hat{Y}_{x, y, z}) & \rm{otherwsie} \\\end{cases} \tag{2.1}</script><p>其中$\alpha, \beta$为超参数，文中指定$\alpha=2, \beta=4$；$N$为图片$I$中关键点的个数。</p><blockquote><p>Focal Loss定义为</p><script type="math/tex; mode=display">FL(p_t) = - \alpha_t (1 - p_t)^{\gamma} \log p_t</script><p>其中$\gamma &gt; 0$</p><script type="math/tex; mode=display">\begin{aligned}    p_t =      \begin{cases} p      & y = 1 \\ 1 - p      & \rm{otherwsie} \end{cases} \in [0, 1] \\    \alpha_t = \begin{cases} \alpha & y = 1 \\ 1 - \alpha & \rm{otherwsie} \end{cases} \in [0, 1]\end{aligned}</script></blockquote><p>由于网络的降采样，设置物体坐标偏置(local offset)<strong>修正物体中心坐标</strong>，即特征图$\hat{O} \in \mathcal{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$，该特征图所有类别共享。采用$L_1$损失，且仅对包含关键点的位置计算损失值。</p><script type="math/tex; mode=display">L_{off} = \frac{1}{N} \sum_p \left| \hat{O}_{\tilde{p}} - \left( \frac{p}{R} - \tilde{p} \right) \right| \tag{2.2}</script><p>同样地，物体的<strong>边界框尺寸回归</strong>设置特征图$\hat{S} \in \mathcal{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$，$s_k = (x^{(k)}_2 - x^{(k)}_1, y^{(k)}_2 - y^{(k)}_1)$，也采用$L_1$损失，那么</p><script type="math/tex; mode=display">L_{size} = \frac{1}{N} \sum_k \left| \hat{S}_{p_k} - s_k \right| \tag{2.3}</script><p>总体损失为以上几项的加权和，原文中$\lambda_{off} = 1, \lambda_{size} = 0.1$。</p><script type="math/tex; mode=display">L = L_k + \lambda_{off} L_{off} + \lambda_{size} L_{size} \tag{2}</script><p>基于以上，在测试阶段时，先在类别输出特征图$\hat{Y}$上确定位置，即保留100个峰(peak)(某处值在比其8领域值都大)的坐标$(\hat{x}_i, \hat{y}_i), i = 1, \cdots, 100$，各位置处的预测输出为</p><script type="math/tex; mode=display">\begin{aligned}    \begin{cases}        \hat{O}_{\hat{x}_i, \hat{y}_i} = (\delta_{\hat{x}_i}, \delta_{\hat{y}_i}) \\        \hat{S}_{\hat{x}_i, \hat{y}_i} = (\hat{w}_i, \hat{h}_i)    \end{cases}    \Rightarrow \\    (        \hat{x}_i + \delta_{\hat{x}_i} - \hat{w}_i / 2,         \hat{y}_i + \delta_{\hat{y}_i} - \hat{h}_i / 2, \\        \hat{x}_i + \delta_{\hat{x}_i} + \hat{w}_i / 2,         \hat{y}_i + \delta_{\hat{y}_i} + \hat{h}_i / 2    )\end{aligned} \tag{3}</script><h2 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h2><p>原文中分别采用ResNet-18, ResNet101, DLA-34, and Hourglass-104作为网络框架进行实验，效果对比如下<br><img src="/2019/10/09/CenterNet-Objects-as-Points/Tab1.png" alt="Table 1"></p><p>训练细节如下</p><ol><li>输入图像分辨率为$512 \times 512$，输出特征图分辨率为$128 \times 128$；</li><li>数据集扩增：随机翻转、尺度缩放($0.6$~$1.3$)、随即裁剪、颜色抖动；</li><li>使用Adam优化器，批次大小、初始学习率、学习率调整策略与不同网络框架相关。</li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="/2019/10/09/CenterNet-Objects-as-Points/Tab2.png" alt="Table 2"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1904.07850v1" target="_blank" rel="noopener">Objects as Points - arXiv</a></li><li><a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">Focal Loss for Dense Object Detection - arXiv</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《教父》三部曲：充满腐败而不失庄重的资本主义气息</title>
      <link href="/2019/10/05/%E3%80%8A%E6%95%99%E7%88%B6%E3%80%8B%E4%B8%89%E9%83%A8%E6%9B%B2%EF%BC%9A%E5%85%85%E6%BB%A1%E8%85%90%E8%B4%A5%E8%80%8C%E4%B8%8D%E5%A4%B1%E5%BA%84%E9%87%8D%E7%9A%84%E8%B5%84%E6%9C%AC%E4%B8%BB%E4%B9%89%E6%B0%94%E6%81%AF/"/>
      <url>/2019/10/05/%E3%80%8A%E6%95%99%E7%88%B6%E3%80%8B%E4%B8%89%E9%83%A8%E6%9B%B2%EF%BC%9A%E5%85%85%E6%BB%A1%E8%85%90%E8%B4%A5%E8%80%8C%E4%B8%8D%E5%A4%B1%E5%BA%84%E9%87%8D%E7%9A%84%E8%B5%84%E6%9C%AC%E4%B8%BB%E4%B9%89%E6%B0%94%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/05/《教父》三部曲：充满腐败而不失庄重的资本主义气息/timg.jpg" alt="timg.jpg"></p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>《教父》三部曲每一部都很精彩，第二部主角是个活生生的反例，可能稍逊一筹。</p><p>第一部《教父》介绍的是柯里昂家族反对毒品交易而引起的大风波，是三部曲中唯一一部被拍成电影的书(电影《教父Ⅰ》完美还原，表现力很强，但缺少细节导致剧情不太连续)。个人认为，维托·唐·柯里昂是三部曲中唯一一位称得上是教父的人，其度量、耐心、冷静与智慧让人折服。教父因拒绝毒品交易被枪击，随后小儿子迈克·柯里昂替父报仇潜逃西西里，长子桑提诺·柯里昂性格鲁莽在之后的大战中被击毙，唐请求各大家族协商和谈停止战争，让迈克返回纽约继承教父衣钵。迈克与父亲一样沉着、冷静、精明、坚强，整本书也像是讲述他褪茧化蝶的过程，最终成为了第二代教父。每一个男人都必须像他一样，让自己的心智不断变得成熟，让自己变得足够强大，可以保护自己、家人和别人。</p><p>第二部《教父：西西里人》故事发生在黑手党之乡西西里，整本书的格调非常浪漫，结局悲惨令人唏嘘。图里·吉里安诺劫富济贫、反抗政府、藐视黑手党，是西西里的英雄人物，在政府、黑手党的阴谋之下，各种矛盾激发，最终被他的忠实追随者阿斯帕努·皮肖塔背叛而惨遭杀害。图里失败的原因有很多，他是是理想主义者，信奉个人英雄主义。第二，不具有大局观，被政府与黑手党联手利用以对抗共产党人时，不曾想到长远利益。第三，他也不善于领导下属，他以“属下”而不是“伙伴”的态度来对待和尊重他的团队，最后计划潜逃美国时，没有考虑他的好友阿斯帕努的后路，导致亲密伙伴的叛变。</p><p>第三部《教父：最后的教父》，讲述克莱里库齐尼奥家族消灭敌人库奇奥家族后发生的故事，穿插家族与好莱坞两条主线，故事内容较前两部更加丰富、情节更加复杂。全书最精彩的是结尾，唐·克莱里库齐尼奥为完成使家族产业合法化的大局，借用外孙克罗斯·德·莱纳之手除去孙子丹特·克莱里库齐尼奥。这两个人辈分相同，背景与性情却大相径庭：克罗斯·德·莱纳跟随父亲皮皮·德·莱纳长大，他彬彬有礼、勇敢果断；而丹特的父亲是库奇奥家族长子乔治·库奇奥(类似罗密欧与朱丽叶之间的家族纠纷，乔治在结婚之日被皮皮所杀)，继承了库奇奥家族残暴的性情。唐其实不愿将教父之位传与丹特，在丹特为父报仇杀害皮皮后，没有主持所谓的公道，假装不知情，任由克罗斯将丹特杀害，再将克罗斯驱逐。最终克罗斯退出了家族，但个人认为他是天生的黑手党首领，是“最后的教父”。</p><h1 id="摘录"><a href="#摘录" class="headerlink" title="摘录"></a>摘录</h1><ol><li>有个道理他早就弄清楚了，那就是你必须承受社会强加的侮辱，因为他明白，连最卑微的人，只要时刻擦亮眼睛，就迟早能抓住机会，报复最有权势的人。正是明白这个道理，唐才从不放弃他的谦逊风度，所有朋友都对此敬佩有加。</li><li>友谊就是一切。比天赋更重要，比政府更重要。和家人差不多同样重要。</li><li>“我会给他一个他无法拒绝的条件。”</li><li>知道极地探险家在通往北极的路上要沿途存放口粮，防止日后某天会需要食物吗？那就是我父亲的人情。</li><li>黑根的谈判技巧是唐亲自传授的。“永远不要动怒，”唐这么教导他，“决不要威胁，要讲道理。”用意大利语说“讲道理”听上去像“应对”。关键是忽视所有的侮辱和威胁，一边脸挨了打，就把另一边脸也凑上去。</li><li>意大利人有个玩笑话，说世界太残酷，所以一个人非得有两个父亲照看他，这就是教父的由来。</li><li>在黑根的世界里，柯里昂家族的世界里，女性的美丽肉体和性魅力对世俗事务毫无重要性可言。只要不涉及婚姻和家族的脸面，这就只是私人事务。</li><li>动作不慌不忙得吓人，像是全世界的时间都归他们支配。他们不是慌乱地瞎打一通，而是一板一眼，用上躯体的全部力量，慢镜头似的慢慢收拾他。每一拳下去都带着皮开肉绽的声音。</li><li>没有一句警告，不装腔作势，不按理出牌，不留任何余地。这种冷酷无情，这种对一切价值的全然蔑视，意味着这个人只认他自己的法律，甚至把自己视为上帝。</li><li>绝对不要让家族外的人知道你在想什么。绝对不要让他们知道你手里有什么牌。</li><li>可是，克莱门扎有个强烈的感觉，那就是遵守良好的工作习惯很重要，务必做到万无一失。论到生死问题，谁也说不准会发生什么。</li><li>将来有可能会出现有人会因为利益而出卖他的情况，要是只有一名同伙，那就是正反双方各执一词。但要是还有第二名同伙作旁证，平衡就会被打破。不，办事必须严格按照程序。</li><li>你不能说他走运，彼得，那太低估他了。我们最近太过于低估别人。</li><li>“这种事每隔十来年就要发生一次，能释放彼此的仇怨。另外，要是放任他们在小事上随便摆布我们，那他们就会想要夺走我们的一切。必须一冒头就斩断。就像他们当初在慕尼黑就该阻止希特勒，他干了那种事，怎么能随便放过他，放过他就意味着后面的大麻烦都是自找苦吃。”</li><li>有些事情非做不可，做了也不值得再次提起，不需要给自己找正当的借口。这种事情正当不起来。反正做就是了，然后忘掉。</li><li>当然了，谁都不可能以任何理由挑拨唐，唐的情感只受他自己左右。</li><li>尼诺的反应与成功之路背道而驰，无论别人怎么帮他，他都觉得受了羞辱。</li><li>一个向警方通风报信的人，一个收钱就可以不寻仇的人，肯定没有过硬的后台。</li><li>这段经验催生了他的座右铭：一个人只能有一种命运。</li><li>“我会和他讲道理。”维托·柯里昂说，这句话后来成了他的名言，致命攻击前的最后警告。后来他成为唐，每次请对手坐下来和他讲道理，对手就明白这是解决争端而不流血杀人的最后一次机会了。</li><li>当时维托·柯里昂还不知道这个笑容的威力。之所以让人毛骨悚然，正是因为毫无威胁的意思，像是听到了只有自己才明白的什么私人玩笑。可是，他只在性命攸关的事情上露出这个笑容，玩笑也并不真的私密，他的双眼毫无笑意，外在性格平时又是那么通情达理和沉默寡言，因此突然摘下面具，露出真实的自我才那么吓人。</li><li>伟大的人并非生而伟大，而是越活越伟大，维托·柯里昂就是明证。</li><li>他掂量了一下他们的威胁，发现没什么说服力，于是降低了对新伙伴的评价，因为他们太愚蠢，在毫无必要的情况下滥用威胁。</li><li>你难道不想好好上学，不想当个律师？律师拎着手提箱能偷的钱，一千个强盗戴着面具拿着枪也比不上。</li><li>除了他时常重复的“一个人只有一种命运”理论，唐还喜欢责备桑尼动不动就发脾气的毛病。唐认为威胁是最愚蠢的自我暴露，不假思索就释放怒火是最危险的任性表现。没有谁听唐发出过赤裸裸的威胁，没有谁见过他陷入无法控制的狂怒。那是难以想象的事情。他就这么尽量向桑尼传授自己的准则。他说除了朋友低估你的优点，世上最大的天然优势就是敌人高估你的缺陷。</li><li>和历史上所有伟大的统治者和立法者一样，唐·柯里昂看明白了，除非把王国的数量缩减到可控范围之内，否则就不可能缔造秩序与和平。</li><li>一个人靠汗水挣面包钱，做什么职业都值得尊敬。</li><li>时间能治好创伤。痛苦和恐惧不是死亡，还有挽回的余地，</li><li>卡洛觉察到桑尼会杀了他，明白桑尼拥有动物本性，能杀死另一名人类；而他要想杀人，却必须聚集起全部勇气和全部意志力。卡洛从没想到过，这是因为他比桑尼·柯里昂更有人情味——如果“人情味”能用在他们头上的话；他嫉妒桑尼身上那种被镀上传奇色彩的、可怕的凶残。</li><li>明白如果她有必要知道的坏事，那么马上就会有人来通知她；如果是坏事但她不知道也无所谓，那么她还是不要知道为妙。</li><li>朋友不会轻易求你帮忙，你也不能轻易拒绝。</li><li>没有理性，我们和丛林野兽还有什么分别？但是，我们毕竟有理性，能够彼此说理，能够和自己说理。</li><li>他没有详细解释，在唐这样的人看来，一个人试图自杀是多么不可思议的事情。</li><li>唉，人生又不完全是甜美的音乐，你要是愿意在医院里走一圈，就会看见什么是真正的苦难，就会给肉赘唱一首小情歌了。</li><li>面对残暴的绝对权力，苦难中的人民害怕被击垮，学会了决不泄露怒火和恨意，决不空口威胁而让自己受到伤害，因为那就等于提醒对手，会迅速遭到报复。他们学会了社会就是敌人，于是在受到冤屈而寻找救济的时候，转而求助于叛逆的地下王国——黑手党。黑手党通过缄默规则巩固权力。在西西里乡村，陌生人连问怎么去最近的村镇都得不到礼遇和回答。黑手党成员能犯下的最大罪错莫过于告诉警察，他吃了谁的枪子或者被谁以任何方式伤害了。缄默规则成了人们的宗教。一个女人就算丈夫被杀、儿子被杀、女儿被强奸，也不能向警方透露罪犯的姓名。</li><li>我经常这么和别人说，‘肉别吃那么多，否则你会死；烟别抽那么多，否则你会死；工作别那么卖力，否则你会死；酒别喝那么凶，否则你会死。’谁也不听我的。知道为什么吗？因为我说的不是‘明天你就会死’。</li><li>唐帮助不幸的人，而这些人的不幸又有一部分要归咎于他。不一定出于狡诈或计划，很可能只是因为他在各方面都有利益，也许这就是宇宙的固有性质：善与恶内在联系，宇宙本身就是这样。</li><li>她知道全世界只有她能压迈克尔一头，但也知道经常这么做只会毁掉这种能力。</li><li>忒西奥看好迈克尔。他感觉到这个年轻人没这么简单，巧妙地隐藏起了某种力量，他戒心很重，生怕把真正的实力暴露在公众视线之下，遵循唐的教诲：让朋友低估你的优点，敌人高估你的缺陷。</li><li>你不能对你爱的人说不，至少不能经常说。这就是秘诀。要是非说不可，也得听起来像是肯定。或者想办法让他们自己说。你得耐心，不怕麻烦。</li><li>他已在弥留之际。他闻着花园的香味，黄色的光球刺得眼睛生疼，他悄声说：“生活如此美丽。”</li><li>要是不赞同某个团体或个人的看法，他要么避而不谈，要么直截了当地说出他的不满。他从不礼节性地随意附和。</li><li>很多年轻人在拥抱真正的命运之前都走错过路。时间和运气会改正错误。</li><li>既然他不怕死，存心找死，那么诀窍就在于，让他唯独不想死在你的手里。他害怕的事情只有一件——不是死亡，而是他或许会死在你手里。到那个时候，他就完全属于你了。</li><li>因为背叛是不能宽恕的罪行。迈克尔可以宽恕他们，但他们永远无法宽恕自己，因此反而更危险。迈克尔真的很喜欢忒西奥，更爱自己的妹妹。可是，如果放过忒西奥和卡洛，那就是对你和孩子、对他的整个家庭、对我和我的家人的失职。他们会对我们所有人、所有人的生命构成危险。</li><li>他显然是一个“值得尊敬”的人。</li><li>西西里人害怕真相。过去几千年里，暴君和宗教法庭用酷刑逼迫他们说真话，罗马政府用法律要求人们说真话，教堂忏悔处的神父用下地狱的痛苦敦促人们说真话，可是真话是力量的源泉，是控制的杠杆，为什么要拱手交给别人呢？</li><li>天生的恶棍对天生的英雄有着天然的敌意。</li><li>穷人总是上当受骗，甚至被那些指引他们通往救赎之路的人欺骗。</li><li>我们真正的信仰在于我们相信奇迹会发生。</li><li>一个对人类过去两千年历史一无所知的人，是一个在黑暗中生活的人。</li><li>唐·克罗切最厉害的地方就是不理会别人对他的侮辱和不恭，但是他会把它牢记在心里。</li><li>我们小时候，我们年轻的时候，热爱我们的朋友，慷慨地对待他们，原谅他们的错误，这些都是很自然的。每一天都很新鲜。我们愉快地期待未来，毫无畏惧之心。世界本身并没有那么危险；那是一段欢乐的时光。可是我们长大了，要自食其力了，朋友的情谊就不那么容易维系下去。我们必须随时保持警惕。我们的长辈不再照顾我们，我们也不再对儿时简单的乐趣感到满足。我们开始有了自豪感——我们希望成为了不起的人、有权的人或者有钱的人，或者只是为了使自己免遭不幸。我知道你非常热爱图里·吉里安诺，可是现在你必须问问自己，这样的爱要付出什么代价？经过这么多年，这样的爱是否还存在？是不是只存在于记忆之中？</li><li>皮肖塔很聪明，但只是年轻人的那种聪明——也就是说，他没有充分认识到，最好的人心里也有潜在的恐怖与邪恶。</li><li>如果皮肖塔继续对吉里安诺忠心耿耿，他也会被人们所遗忘，因为那样整个传奇故事就将是关于吉里安诺一个人的。但是他犯下了弥天大罪，这样他就会与他热爱的图里永远并列在一起了。</li><li>你是不是想成为像吉里安诺一样的英雄？一个传奇人物？一个死人？我喜欢他，因为他是我最好的朋友的儿子，可是我并不羡慕他的名声。你还活着，可是他死了。永远不要忘记这一点。活着不要当英雄，只要活着就行。时过境迁之后，英雄似乎就有点儿愚蠢了。</li><li>在熟铁框架上锻造了一行字，是给那些自鸣得意的凭吊者看的：我们曾经像你们一样——你们也会像我们一样。</li><li>“我们要做的是助人为乐，”唐说道，“不是舍己为人。”</li><li>永远别回头。无论是为了找借口、为自己辩解还是找乐子，永远都不要回头。你现在是什么人，就是什么人，世界眼下是什么样，就是什么样。</li><li>自杀——如今这种事儿还属于“政治不正确”吗？</li><li>暴力这种武器太宝贵了，浪费不得，必须用以达到重要目的才行。</li><li>每个男人、女人和孩童，在压力之下、悔恨之中，或是艰难的环境面前，都要完全对自己的行为负责。决定一个人的是行为，言语只不过是个屁。</li><li>不轻易动怒，不谈论自己。要用行动而不是言语来赢得尊重。尊重家庭的每一分子。赌博是消遣，可不是营生。爱父母和妹妹，但是小心，别爱上老婆以外的女人。老婆就是给你生孩子的女人。有了妻儿，就要牺牲自己的生活去养活他们。</li><li>聪明的、理智的男人，绝大多数情况下都不必害怕女人。两种人你必须警惕：第一种是最危险的，那就是遭遇了不幸的小姑娘；第二种就是比你还有野心的女人。</li><li>显然他的行为都是让她主动离开的小伎俩，可她这么长时间都不知趣。想想真丢脸。她怎么能这么傻呢？</li><li>事实证明，谨慎地镇压自由意志对人类生存很有必要。</li><li>法兰西国王露出屁股，当众大便，以示对教皇的轻蔑。而那个红衣主教则高呼：“噢，这是天使的屁股！”然后冲上去大亲特亲。</li><li>迪尔最喜欢用“脆弱”形容他认为愚蠢的人。</li><li>皮皮大笑道：“别让任何人告诉你该怎么做，不要让别人插手你的事。他们只能告诉你他们的期望，而且我们只找最有利于我们的方法办事。最简单的就是最好的。如果你必须把事情搞复杂，那就搞到最复杂。”</li><li>唐也知道，无论爱有多么深，这种情感都不可靠。爱不一定带来感激和顺从，也不一定能在如此艰难的世界里带来和睦。所以，为了激励真正的爱，必须让人有所畏惧。爱本身一钱不值，必须包括信任和服从才有意义。</li><li>他对萝塞·玛丽耶说：“像这样的世界，谁都可以为所欲为，怎么能生活得下去呢？谁都不会受到上帝或者他人的惩罚，谁都不必养活自己？真有这种为所欲为的女人吗？真有男人如此愚蠢和软弱，任何一点欲望、梦想或者快乐都可以使他们屈服吗？那些诚实的丈夫在哪？他们为了食物而工作，想方设法保护孩子们不受命运和这个残酷的世界摆布。谁能真正明白一片奶酪、一杯酒、一个温暖的家，人生就足够了？向往虚无缥缈的幸福的都是什么人？看看他们把生活搞得多么乱七八糟、凭空制造了多少悲剧。”唐拍了拍女儿的头，冲着电视荧幕不屑地摆摆手，说，“让他们都下地狱吧。”最后，他又加了一句金玉良言，“每个人都要为自己的所作所为负责。”</li><li>任何形式的伟大都会招致嫉妒。</li><li>怪人做怪事，是为了让人们注意不到他们的真面目。他们自卑。</li><li>这就是真相。他对人类失望了。他见到了太多的背叛、太多可怜的软弱、太多争名逐利的贪婪。相爱的人之间却都是逢场作戏，夫妻也好、父子也好、母女也好，都是一样。</li><li>我没胆子跳出窗户。我想象力丰富，在落地之前就能想到一千种自己落地后摔得七零八落的样子，还有可能会砸到别人；我不敢割腕，见血就晕；我也不敢用刀、枪或者撞车、卧轨。</li><li>人可以不断犯错，但绝不能犯要命的错。</li><li>阅读是永远不会背叛他的一种快乐。</li><li>那些笔触优雅的散文，那么受人欢迎，现在看来都是无病呻吟，夸张做作而且自命不凡。</li><li>“你不能认栽，”唐对克罗斯说，“这样的话，大家都把你当傻子看，全世界的人谁也不会尊重你了。”</li><li>他是个好孩子，但还年轻，年轻人必须得承担风险。</li><li>克罗斯想知道他什么时候最爱她，是当她生气勃勃的时候，当她严肃认真的时候，还是她闷闷不乐的时候。她美丽的面容不断变化，似乎有种魔力，克罗斯自己的情绪也随她一起变化。</li><li>他急切地想牺牲自己成全自己爱的女人，很多男人都有过这种感觉，现在轮到他了。</li><li>唐说过，绝大多数悲剧都是荒诞的。</li><li>下午剩下的时间，克罗斯在片场观看拍摄。有这么一场戏，主角赤手空拳干掉了三个全副武装的敌人。这把他惹毛了。是英雄就不应该让自己陷入这么绝望的局面。这种事只能证明这家伙太蠢，根本不配当英雄。</li><li>要知道，世界眼下是什么样，就是什么样；你现在是什么人，就是什么人。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PCA and LDA with Kernel</title>
      <link href="/2019/09/05/PCA-and-LDA-with-Kernel/"/>
      <url>/2019/09/05/PCA-and-LDA-with-Kernel/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>核函数可将数据进行升维，在更高维度的空间进行数据分析，可以将线性不可分问题转换为线性可分问题，关于核函数，详细可查看<a href="https://louishsu.xyz/2019/05/27/Support-Vector-Machine/" target="_blank" rel="noopener">Support Vector Machine 2.2核技巧</a>。在<a href="https://louishsu.xyz/2018/10/22/PCA/" target="_blank" rel="noopener">PCA</a>与<a href="https://louishsu.xyz/2019/04/22/LDA/" target="_blank" rel="noopener">LDA</a>中分别介绍了主成分分析与线性鉴别分析，但这两种均为线性方法，本文利用核技巧将数据映射到高维空间，然后再用两种降维方法进行降维。</p><p><strong>先升维再降维！</strong></p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>假设有$M$个$N$维样本组成数据矩阵$X_{N \times M}$</p><script type="math/tex; mode=display">X = \begin{bmatrix}         | & | &   & | \\        x^{(1)} & x^{(2)} & \cdots & x^{(M)} \\        | & | &   & |    \end{bmatrix} \tag{1}</script><p>其中$x^{(i)} = \begin{bmatrix} x^{(i)}_1 &amp; x^{(i)}_2 &amp; \cdots &amp; x^{(i)}_N \end{bmatrix}^T$，属于$C$个类别，第$j$类的样本数为$M_j$。</p><p>核函数一般定义为$\kappa(x, y) = \Phi(x)^T \Phi(y) $，注意到，核函数是升维后数据的<strong>内积形式</strong>，利用隐式映射$\Phi(x)$将样本维度增加。</p><h2 id="Kernel-PCA"><a href="#Kernel-PCA" class="headerlink" title="Kernel PCA"></a>Kernel PCA</h2><p>同样地，求取升维后数据的协方差矩阵，需要对样本去均值化，即</p><script type="math/tex; mode=display">C = \frac{1}{M} \sum_{i=1}^M \left[\Phi(x^{(i)}) - \Phi(\mu)\right] \left[\Phi(x^{(i)}) - \Phi(\mu)\right]^T \tag{2}</script><p>其中$\Phi(\mu) = \frac{1}{M} \sum_{i=1}^M \Phi(x^{(i)})$。将其特征分解，记高维空间的第一主轴为$\Phi(u_1)$，有</p><script type="math/tex; mode=display">C \Phi(u_1) = \lambda_1 \Phi(u_1) \tag{3}</script><p>现在有个问题，由于$\Phi(x)$为隐式映射，<strong>实际上$C$是不能直接求解的</strong>。将$(2)$代入$(3)$，有</p><script type="math/tex; mode=display">\underbrace{        \frac{1}{M} \sum_{i=1}^M         \left[\Phi(x^{(i)}) - \Phi(\mu)\right]        \left[\Phi(x^{(i)}) - \Phi(\mu)\right]^T    }_C \Phi(u_1) = \lambda_1 \Phi(u_1) \tag{4}</script><p>其中$\left[\Phi(x^{(i)}) - \Phi(\mu)\right]^T \Phi(u_1)$部分为参数，可记作$a^{(i)}_1$，所以</p><script type="math/tex; mode=display">\frac{1}{M} \sum_{i=1}^M     \left[\Phi(x^{(i)}) - \Phi(\mu)\right]    a^{(i)}_1 = \lambda_1 \Phi(u_1)</script><p>记$\Psi(x^{(i)})=\Phi(x^{(i)}) - \Phi(\mu)$，也即</p><script type="math/tex; mode=display">\Phi(u_1) = \frac{1}{\lambda_1 M} \sum_{i=1}^M     a^{(i)}_1 \Psi(x^{(i)}) \tag{5}</script><p>所以可以看到，主轴$\Phi(u_1)$是由高维空间样本去均值化后的数据点$\Psi(x^{(i)})$<strong>张成</strong>的，这个式子很重要，<strong>只需求出$a^{(i)}_1$，即可表示出主轴</strong>，用于高维数据$\Phi(x^{(i)})$的降维。继续，将式$(1), (5)$代入$C \Phi(u_1) = \lambda_1 \Phi(u_1)$，得到</p><script type="math/tex; mode=display">\begin{aligned}    \underbrace{        \frac{1}{M} \sum_{i=1}^M             \Psi(x^{(i)})            \Psi(x^{(i)})^T    }_C     \underbrace{        \cancel{\frac{1}{\lambda_1 M}}         \sum_{i=1}^M \Psi(x^{(i)}) a^{(i)}_1    }_{\Phi(u_1)}     = \lambda_1     \underbrace{        \cancel{\frac{1}{\lambda_1 M}}         \sum_{i=1}^M \Psi(x^{(i)}) a^{(i)}_1    }_{\Phi(u_1)} \\    \frac{1}{M} \sum_{i=1}^M         \Psi(x^{(i)})    \sum_{j=1}^M         a^{(j)}_1 \Psi(x^{(i)})^T \Psi(x^{(j)})    = \lambda_1 \sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)})\end{aligned}</script><p>上式左右同乘$\Psi(x^{(l)})^T$，凑出核函数的形式，得到</p><script type="math/tex; mode=display">\begin{aligned}    \frac{1}{M} \Psi(x^{(l)})^T    \left[        \sum_{i=1}^M             \Psi(x^{(i)})        \sum_{j=1}^M             a^{(j)}_1 \Psi(x^{(i)})^T \Psi(x^{(j)})    \right]    = \lambda_1 \Psi(x^{(l)})^T    \left[        \sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)})    \right] \\    \frac{1}{M}        \sum_{i=1}^M             \underbrace{\Psi(x^{(l)})^T \Psi(x^{(i)})}_{\kappa(x^{(l)}, x^{(i)}))}        \sum_{j=1}^M             a^{(j)}_1 \underbrace{\Psi(x^{(i)})^T \Psi(x^{(j)})}_{\kappa(x^{(i)}, x^{(j)}))}    = \lambda_1         \sum_{i=1}^M             a^{(i)}_1 \underbrace{\Psi(x^{(l)})^T \Psi(x^{(i)})}_{\kappa(x^{(l)}, x^{(i)}))} \\    \frac{1}{M}        \sum_{i=1}^M             \kappa(x^{(l)}, x^{(i)})        \underbrace{            \sum_{j=1}^M                 a^{(j)}_1 \kappa(x^{(i)}, x^{(j)})        }    = \lambda_1         \underbrace{            \sum_{i=1}^M                 a^{(i)}_1 \kappa(x^{(l)}, x^{(i)})        }\end{aligned}</script><p>记$a_1 = \begin{bmatrix} a^{(1)}_1 &amp; a^{(2)}_1 &amp; \cdots &amp; a^{(M)}_1 \end{bmatrix}^T$，核矩阵$K = \begin{bmatrix} \kappa(x^{(i)}, x^{(j)}) \end{bmatrix}, i, j \in \{1, 2, \cdots, M\}$，则有</p><script type="math/tex; mode=display">\sum_{i=1}^M a^{(i)}_1 \kappa(x^{(j)}, x^{(i)}) = K_{j} a_1 (K的第j行) \tag{6}</script><p>代入后</p><script type="math/tex; mode=display">\sum_{i=1}^M \kappa(x^{(l)}, x^{(i)}) K_{i} a_1 = \lambda_1 M K_{l} a_1</script><p>等号左边再使用一次$(6)$，有</p><script type="math/tex; mode=display">K_l \begin{bmatrix} K_1 a_1 \\ K_2 a_1 \\ \cdots \\ K_M a_1 \end{bmatrix} = \lambda_1 M K_{l} a_1</script><p>其中$K a_1 = \begin{bmatrix} K_1 a_1 &amp; K_2 a_1 &amp; \cdots &amp; K_M a_1 \end{bmatrix}^T$，故$K_l K a_1 = \lambda_1 M K_{l} a_1$，由于$K_l \neq 0$，所以</p><script type="math/tex; mode=display">K a_1 = \lambda_1 M a_1 \tag{7}</script><p>所以<strong>对矩阵$K$进行特征分解即可计算得到$a_1$</strong>，同PCA一样，按特征值降序对特征向量排序，选取前$k$个<strong>主轴对应的权值向量</strong>$\{a_1, a_2, \cdots, a_k\}$，注意$a_k=\Psi(x^{(i)})^T \Phi(u_1)$并不是投影的轴，那么对于输入的样本点$x$进行降维时，应有</p><script type="math/tex; mode=display">y = \Phi(u_1)^T \Phi(x)</script><p>其中$\Phi(u_1) = \frac{1}{\lambda_1 M} \sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)})$，，仅考虑方向省略$\frac{1}{\lambda_1 M}$，所以</p><script type="math/tex; mode=display">\begin{aligned}    y_1 = \left[\sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)})\right]^T \Phi(x) \\    = \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(i)}, x)\end{aligned} \tag{8}</script><p>至于数据的重建，应该有$\hat{\Phi}(x) = \sum y_k \Phi(u_k)$，而$\Phi(u_k)$未知，故只能通过迭代优化的形式进行重建，最小化目标可使用MSE，即最小化</p><script type="math/tex; mode=display">J = \min \{|| \Phi(x) - \hat{\Phi}(x) ||^2\} \tag{9}</script><p><strong>整理一下上面的算法</strong></p><ol><li>选择核函数，如高斯核$\kappa(x, y) = - \frac{||x - y||^2}{2 \sigma^2}$，多项式核$\kappa(x, y) = (x^T y + c)^d$；</li><li>根据选用的核函数，计算核矩阵$K_{M \times M} = \begin{bmatrix} \kappa(x^{(i)}, x^{(j)}) \end{bmatrix}, i, j \in \{1, 2, \cdots, M\}$；</li><li>对$K$进行特征分解，根据特征值降序对特征向量进行排序，计算得到各个主轴方向上的权重参数$\{a_1, a_2, \cdots, a_k\}$；</li><li>对于输入的样本点$x$，根据$y_1 = \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(i)}, x)$进行降维，可以看到，Kernel PCA主轴的计算与训练样本$x^{(i)}$有关，需要进行保存。</li></ol><h2 id="Kernel-LDA"><a href="#Kernel-LDA" class="headerlink" title="Kernel LDA"></a>Kernel LDA</h2><p>对于<strong>映射后数据$\Phi(x)$</strong> ，其类内离差阵$S_W$与类间离差阵$S_B$为</p><script type="math/tex; mode=display">\begin{aligned}    S_W = \sum_{j=1}^C \frac{M_j}{M}         \left\{            \frac{1}{M_j} \sum_{i=1}^{M_j}                 \left[\Phi(x^{(i)}) - \Phi(\mu^{(j)})\right]                \left[\Phi(x^{(i)}) - \Phi(\mu^{(j)})\right]^T        \right\} \\    S_B = \sum_{j=1}^C \frac{M_j}{M}         \left\{                \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]                \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]^T        \right\}\end{aligned} \tag{10}</script><p>其中$\Phi(\mu^{(j)}) = \frac{1}{M_j} \sum_{i=1} \Phi(x^{(i)}), \Phi(\mu) = \frac{1}{C} \sum_{j} \Phi(\mu^{(j)})$。现以第一主轴为例，$\tilde{S_W}$与$\tilde{S_B}$分别表示<strong>映射后数据$\Phi(x)$</strong> 在第$1$主轴$\Phi(u_1)$上分布的类内离差阵与类间离差阵，即</p><script type="math/tex; mode=display">\begin{aligned}    \tilde{S_W} = \sum_{j=1}^C \frac{M_j}{M}         \left\{            \frac{1}{M_j} \sum_{i=1}^{M_j}                 \left[\tilde{\Phi}(x^{(i)}) - \tilde{\Phi}(\mu^{(j)})\right]                \left[\tilde{\Phi}(x^{(i)}) - \tilde{\Phi}(\mu^{(j)})\right]^T        \right\} \\    \tilde{S_B} = \sum_{j=1}^C \frac{M_j}{M}         \left\{                \left[\tilde{\Phi}(\mu^{(j)}) - \tilde{\Phi}(\mu)\right]                \left[\tilde{\Phi}(\mu^{(j)}) - \tilde{\Phi}(\mu)\right]^T        \right\}\end{aligned} \tag{11}</script><p>其中$\tilde{\Phi}(\mu^{(j)}) = \frac{1}{M_j} \sum_{i=1} \tilde{\Phi}(x^{(i)}), \tilde{\Phi}(\mu) = \frac{1}{C} \sum_{j} \tilde{\Phi}(\mu^{(j)})$，且由投影可知</p><script type="math/tex; mode=display">\tilde{\Phi} (x^{(i)}) = \Phi(u_1)^T \Phi(x^{(i)});\quad \tilde{\Phi} (\mu^{(j)}) = \Phi(u_1)^T \Phi(\mu^{(j)});\quad \tilde{\Phi} (\mu) = \Phi(u_1)^T \Phi(\mu)</script><p>将以上代入$\tilde{S_W}$与$\tilde{S_B}$，有</p><script type="math/tex; mode=display">\begin{aligned}    \tilde{S_W} = \Phi(u_1)^T S_W \Phi(u_1) \\    \tilde{S_B} = \Phi(u_1)^T S_B \Phi(u_1)\end{aligned} \tag{12}</script><p>与LDA思路一致，定义优化目标为</p><script type="math/tex; mode=display">J = \min\left\{            \frac{\tilde{S_W}}{\tilde{S_B}}        \right\} =     \min\left\{            \frac{\Phi(u_1)^T S_W \Phi(u_1)}{\Phi(u_1)^T S_B \Phi(u_1)}         \right\} \tag{13}</script><p>最终同LDA，有$S_B \Phi(u_1) = \lambda_1 S_W \Phi(u_1)$或$S_W^{-1} S_B \Phi(u_1) = \lambda_1 \Phi(u_1)$，但是有同样的问题，<strong>由于隐式映射$\Phi(x)$，$S_W, S_B$不能直接求解</strong>。由PCA可知，<strong>主轴$\Phi(u_1)$一定由高维数据样本点$\Phi(x^{(i)})$张成</strong>，即</p><script type="math/tex; mode=display">\Phi(u_1) = \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \tag{14}</script><p>$(10), (15)$代入$\Phi(u_1)^T S_W \Phi(u_1), \Phi(u_1)^T S_B \Phi(u_1)$，有</p><script type="math/tex; mode=display">\begin{aligned}    \Phi(u_1)^T S_W \Phi(u_1) =         \underbrace{            \left[                \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)})            \right]^T        }_{\Phi(u_1)^T}        \underbrace{            \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j}            \left\{                \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right]                \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right]^T            \right\}        }_{S_W}        \underbrace{            \left[                \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)})            \right]        }_{\Phi(u_1)} \\            \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j}            \left\{                \underbrace{                    \left[                        \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})                    \right]^T                    \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right]                }                \underbrace{                    \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right]^T                    \left[                        \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})                    \right]                }            \right\}\end{aligned}</script><p>其中</p><script type="math/tex; mode=display">\begin{aligned}    \left[        \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})    \right]^T    \left[        \Phi(x^{(i)}_j) - \Phi(\mu^{(j)})    \right] \\     = \sum_{k=1}^M a^{(k)}_1 \kappa(x^{(k)}, x^{(i)}_j) -         \left[            \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})        \right]^T         \left[            \frac{1}{M_j} \sum_{i=1}^{M_j} \Phi(x^{(i)}_j)        \right] \\    = \sum_{k=1}^M a^{(k)}_1 \kappa(x^{(k)}, x^{(i)}_j)        - \sum_{k=1}^M a^{(k)}_1 \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j) \\    = \sum_{k=1}^M a^{(k)}_1 \left[            \underbrace{                \kappa(x^{(k)}, x^{(i)}_j)            }_{K^{(j)}_{k, i}} -             \underbrace{                \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j)            }_{\mu_{\kappa^{(j)} k}}        \right]     = a_1^T (K^{(j)}_i - \mu_{\kappa^{(j)}})\end{aligned}</script><blockquote><p>$K^{(k)}_{:, i}$表示第$k$类的第$i$列，为列向量；$K^{(k)}_i$表示第$k$类的第$i$行，为行向量。</p></blockquote><p>所以</p><script type="math/tex; mode=display">\Phi(u_1)^T S_W \Phi(u_1) = \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j} a_1^T (K^{(j)}_i - \mu_{\kappa^{(j)}}) (K^{(j)}_i - \mu_{\kappa^{(j)}})^T a_1 \tag{15}</script><p>记$M = \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j}(K^{(j)}_i - \mu_{\kappa^{(j)}}) (K^{(j)}_i - \mu_{\kappa^{(j)}})^T$，则</p><script type="math/tex; mode=display">\Phi(u_1)^T S_W \Phi(u_1) = a_1^T M a_1 \tag{16}</script><p>同理</p><script type="math/tex; mode=display">\begin{aligned}    \Phi(u_1)^T S_B \Phi(u_1) =         \underbrace{            \left[                \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)})            \right]^T        }_{\Phi(u_1)^T}        \underbrace{            \sum_{j=1}^C \frac{M_j}{M}             \left\{                    \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]                    \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]^T            \right\}        }_{S_B}        \underbrace{            \left[                \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)})            \right]        }_{\Phi(u_1)} \\        \sum_{j=1}^C \frac{M_j}{M}         \left\{            \underbrace{                \left[                    \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)})                \right]^T                \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]            }            \underbrace{                \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]^T                \left[                    \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)})                \right]            }        \right\}\end{aligned}</script><p>其中</p><script type="math/tex; mode=display">\begin{aligned}    \left[\sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})\right]^T    \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right] \\    = \left[\sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})\right]^T    \left[        \frac{1}{M_j} \sum_{i=1}^{M_j} \Phi(x^{(i)}_j) - \frac{1}{M} \sum_{i=1}^M \Phi(x^{(i)})    \right] \\    = \sum_{k=1}^M a^{(k)}_1 \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j) -        \sum_{k=1}^M a^{(k)}_1 \frac{1}{M} \sum_{i=1}^M \kappa(x^{(k)}, x^{(i)}) \\    = \sum_{k=1}^M a^{(k)}_1 \left[            \underbrace{                \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j)            }_{\mu_{\kappa^{(j)} k}}            - \underbrace{                \frac{1}{M} \sum_{i=1}^M \kappa(x^{(k)}, x^{(i)})            }_{\mu_{\kappa k}}        \right] \\    = a_1^T (\mu_{\kappa^{(j)}} - \mu_{\kappa})\end{aligned}</script><p>所以</p><script type="math/tex; mode=display">\Phi(u_1)^T S_B \Phi(u_1) = \sum_{j=1}^C \frac{M_j}{M} a_1^T (\mu_{\kappa^{(j)}} - \mu_{\kappa}) (\mu_{\kappa^{(j)}} - \mu_{\kappa})^T a_1</script><p>记$N = \sum_{j=1}^C \frac{M_j}{M} (\mu_{\kappa^{(j)}} - \mu_{\kappa}) (\mu_{\kappa^{(j)}} - \mu_{\kappa})^T$，则</p><script type="math/tex; mode=display">\Phi(u_1)^T S_B \Phi(u_1) = a_1^T N a_1 \tag{17}</script><p>所以优化目标改为</p><script type="math/tex; mode=display">J = \min \left\{\frac{a_1^T M a_1} {a_1^T N a_1}\right\} \tag{18}</script><p>即求解</p><script type="math/tex; mode=display">M^{-1} N a_1 = \lambda_1 a_1 \tag{19}</script><p>对于新的投影数据，同Kernel PCA，有</p><script type="math/tex; mode=display">y_1 = \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(i)}, x)</script><p><strong>整理一下上述算法</strong></p><ol><li>选择核函数$\kappa(x, y)$；</li><li>计算整体的核矩阵$K_{M \times M}$，利用类别标签从中截取每个类别下的核矩阵$K^{(j)}_{M \times M_j}$；</li><li><p>根据下式计算$M$与$N$；</p><script type="math/tex; mode=display">M = \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j}(K^{(j)}_i - \mu_{\kappa^{(j)}}) (K^{(j)}_i - \mu_{\kappa^{(j)}})^T</script><script type="math/tex; mode=display">N = \sum_{j=1}^C \frac{M_j}{M} (\mu_{\kappa^{(j)}} - \mu_{\kappa}) (\mu_{\kappa^{(j)}} - \mu_{\kappa})^T</script></li><li><p>对矩阵$M^{-1}N$进行特征分解，将特征向量按特征值降序排序，选择$k$个主分量对于的权值$\{a_1, a_2, \cdots, a_k\}$；</p><script type="math/tex; mode=display">M^{-1} N a_k = \lambda_k a_k</script></li><li><p>对于需要降维的数据$x$，计算各维度上的坐标，同样的，主轴的计算与训练样本$x^{(i)}$有关，需要进行保存。</p><script type="math/tex; mode=display">y_k = \sum_{i=1}^M a^{(i)}_k \kappa(x^{(i)}, x)</script></li></ol><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>详细代码见<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/kernelPCA.py" target="_blank" rel="noopener">Basic-Machine-Learning-Algorithm/algorithm/kernelPCA.py</a>与<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/kernelFDA.py" target="_blank" rel="noopener">Basic-Machine-Learning-Algorithm/algorithm/kernelFDA.py</a>。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>构造下图数据，明显为非线性数据，利用线性方法不能提取有效性息用于分类。</p><p><img src="/2019/09/05/PCA-and-LDA-with-Kernel/data.png" alt="data"></p><p>利用Kernel PCA进行降维结果如下</p><p><img src="/2019/09/05/PCA-and-LDA-with-Kernel/kpca.png" alt="kpca"></p><p>利用Kernel FDA进行降维结果如下</p><p><img src="/2019/09/05/PCA-and-LDA-with-Kernel/kfda.png" alt="kfda"></p><p>相比较于PCA，FDA效果更好。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>G. Baudat and F. Anouar. Generalized discriminant analysis using a kernel approach. Neural Computation, 12:2385-2404, 2000.</li><li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=788121&amp;tag=1" target="_blank" rel="noopener">S. Mika, G. Ratsch, J. Weston, B. Scholkopf, and K. Muller. Fisher discriminant analysis with kernels. In IEEE Neural Networks for Signal Processing Workshop, pages 41-48, 1999.</a></li><li>Wang Q . Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models[J]. Computer Science, 2012.</li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD: Single Shot MultiBox Detector</title>
      <link href="/2019/09/02/SSD-Single-Shot-MultiBox-Detector/"/>
      <url>/2019/09/02/SSD-Single-Shot-MultiBox-Detector/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>SSD主要做了以下工作</p><ol><li>设计了one-stage检测算法，比同样是one-stage的YOLOv1更快更准确，相比较Faster R-CNN等two-stage检测算法，在不降低准确率的情况下速度更快；</li><li>其核心是用小卷积核，对特征图进行卷积运算，通过一系列的固定回归框进行预测；</li><li>用不同尺度的特征图产生不同尺度的预测，并且明确指定多种纵横比；</li><li>可进行端到端训练，即使是低分辨率图像输入，准确率也很高；</li><li>在PASCAL VOC、MSCOCO、ILSVRC等数据集上得到state-of-art的结果。</li></ol><h1 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h1><h2 id="多层特征图"><a href="#多层特征图" class="headerlink" title="多层特征图"></a>多层特征图</h2><p>一般来说，深层特征图感受野更大，具有更多的语义信息，而浅层特征图保留了更多的细节，因此SSD对两者都加以应用。</p><p>对于某层网络输出的特征图，需要确定用特征图上哪个位置的cell负责对应ground truth边界库那个的预测。<strong>和YOLO不同，SSD使用杰卡德相似度(jaccard overlap)来确定</strong>，也即交并比，$J(A, B) = \frac{|A \bigcap B|}{|A \bigcup B|}$，当该值超过阈值，则该位置负责边界框的回归。</p><p>例如对于下图，在$4 \times 4$、$6 \times 6$与$8 \times 8$的特征图上，指定两种anchor：$2.5: 1.5$与$1.5: 2.5$，选择IoU阈值$0.5$，<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/tree/master/2019/09/02/SSD-Single-Shot-MultiBox-Detector/ssd_anchor_visualize.py" target="_blank" rel="noopener">可视化</a>情况如下。可以看到，浅层特征分辨率较高，对细小物体的检测比较有效，而深层特征分辨率低，对大的物体比较敏感。</p><p>另外，真实框相对于锚框的计算方法如下，其中$(x_g, y_g, w_g, h_g)$与$(x_a, y_a, w_a, h_a)$分别表示真实框$g$与锚框$a$的中心位置坐标、边长</p><script type="math/tex; mode=display">\begin{aligned}    t_x = \frac{x_g - x_a}{w_a} \quad &    t_y = \frac{y_g - y_a}{h_a} \quad &    t_w = \log (\frac{w_g}{w_a}) \quad &    t_h = \log (\frac{h_g}{h_a})\end{aligned}</script><p><img src="/2019/09/02/SSD-Single-Shot-MultiBox-Detector/im4x4_6x6_8x8_anchor.jpg" alt="im4x4_6x6_8x8_anchor"></p><h2 id="关于anchor参数的设置"><a href="#关于anchor参数的设置" class="headerlink" title="关于anchor参数的设置"></a>关于anchor参数的设置</h2><p>这样带来的问题是，对于各个分辨率下的特征图，每个点处anchor参数是不同的，如上面随意设置的参数，可能实际使用时不适合，SSD是这样解决的。</p><p>对于某一层特征图，设置五种纵横比：$a_r \in \{\frac{1}{3}, \frac{1}{2}, 1, 2, 3\}$，纵横比为$1$的anchor再缩放形成另一尺度的anchor，$s_k’ = \sqrt{s_k s_{k+1}}$，那么该层每个点处包含$6$个anchor。</p><p>对于不同层间，特征图的尺度改变，anchor大小也需随之变化。设置$s_{min}=0.2, s_{max}=0.9$，若所用特征图总层数为$m$，则从浅层到深层，各层的anchor尺度为$s_k = s_{min} + \frac{k - 1}{m - 1}(s_{max} - s_{min}), k \in [1, m]$，在第$k$层，anchor尺寸为$w^a_k = h^a_k = \frac{s_k}{\sqrt{a_r}}$。</p><p>anchor的中心定于$(\frac{i + 0.5}{|f_k|}, \frac{j + 0.5}{|f_k|})$，$|f_k|$为第$k$层特征图某点对应原图中网格的边长。实际上，锚框参数是人为指定的，是需要被解决的一个问题，应根据实际情况进行修改。</p><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p><img src="/2019/09/02/SSD-Single-Shot-MultiBox-Detector/Fig2.png" alt="Fig2"></p><p>SSD接受$300 \times 300$的图像输入，以VGG-16作为backbone网络，从Conv5-3层截断，后面加上一些卷积层作为特征提取器($1 \times 1$与$3 \times 3$卷积层堆叠的形式)。SSD在多个特征图上进行候选框的预测，例如对于$m \times n \times p$的特征图，在该尺度上每个位置指定$k$种尺度的anchor，用$p \times 3 \times 3 \times [k \times (classes + 4)]$的卷积核进行运算，得到$m \times n \times [k \times (classes + 4)]$的特征图。注意，对于$C$个类别的识别，应包含背景，所以$classes = C + 1$。</p><p>如上图，共生成$38^2 \times 4 + 19^2 \times 6 + 10^2 \times 6 + 5^2 \times 6 + 3^2 \times 4 + 1^2 \times 4 = 8732$个候选框。</p><p>根据置信度，滤除一大部分候选框，再用NMS算法删除冗余框。</p><h1 id="Training-Step"><a href="#Training-Step" class="headerlink" title="Training Step"></a>Training Step</h1><p>设置参数$1^{ij_k}_l$，表示在特征图$(i, j)$处的第$k$个锚框$a^{ij_k}$，用于预测真实框$g_l$，即</p><script type="math/tex; mode=display">1^{ij_k}_l =     \begin{cases}        1 & \text{IoU}(a^{ij_k}, g_l) > \text{threshold} \\        0 & \text{otherwise}    \end{cases}</script><p>总体损失由定位损失$L_{loc}$与分类损失$L_{conf}$加权组成，定义在特征图位置$(i, j)$处，对于锚框$k$，网络输出为$\hat{y}^{ij_k} = (\hat{t}^{ij_k}_x, \hat{t}^{ij_k}_y, \hat{t}^{ij_k}_w, \hat{t}^{ij_k}_h, \hat{c}^{ij_k}_0, \hat{c}^{ij_k}_1, \hat{c}^{ij_k}_2, \cdots, \hat{c}^{ij_k}_C)$</p><script type="math/tex; mode=display">L_{total} = \frac{1}{N} \left[\alpha \underbrace{\sum_{i, j, k, l} 1^{ij_k}_l \sum_{* \in \{x, y, w, h\}} \text{SmoothL1}(\hat{t}^{ij_k}_*, t^{ij_k}_*)}_{L_{loc}} + \underbrace{\sum_{i, j, k} - \log \hat{c}^{ij_k}_{c}}_{L_{conf}}\right]</script><p>其中$\hat{c}^{ij_k}_{c} = \frac{\hat{c}^{ij_k}_{c}}{\sum \hat{c}^{ij_k}_{c}}$</p><p>另外，由于物体分布稀疏，分类负样本数目(背景)远大于正样本，所以用困难样本挖掘(hard negative mining)的方法，保留损失值最大的负样本，使分类正负样本的比例保持在$neg: pos = 3: 1$左右。</p><p>同样地，SSD训练时也使用了数据集扩增，数据来源于以下三种途径</p><ol><li>整个原始图像；</li><li>在标注框附近采样，使得杰卡德相似度系数在$0.1, 0.3, 0.5, 0.7, 0.9$；</li><li>随机采样截取。</li></ol><p>随机产生的patch边长在原始边长的$[0.1, 1]$范围内，纵横比范围为$[0.5, 2]$。此外依照$50\%$的概率进行水平翻转，并添加其余干扰。</p><p>用SGD进行优化，批次大小32，初始学习率0.001，动量0.9，权重衰减0.0005。</p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p><img src="/2019/09/02/SSD-Single-Shot-MultiBox-Detector/Tab1.png" alt="Tab1"></p><p><img src="/2019/09/02/SSD-Single-Shot-MultiBox-Detector/Fig3.png" alt="Fig3"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector - arXiv.org</a></li><li><a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">weiliu89/caffe - Github</a></li><li><a href="http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf" target="_blank" rel="noopener">ssd_eccv2016_slide - cs.unc.edu</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLO! v1, v2, v3</title>
      <link href="/2019/09/01/YOLO-v1-v2-v3/"/>
      <url>/2019/09/01/YOLO-v1-v2-v3/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>YOLO是You Only Look Once的简称，是one-stage检测算法，在2016年刚提出时风靡一时，现在看可能存在一些缺点，后续有许多one-stage检测算法都参考YOLO的思路，如SSD。目前位置，YOLO共发布了3代，论文链接在<a href="#reference">Reference</a>中。</p><h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><p>YOLO的一个关键点是，将目标检测作为回归任务，将分离的边界框回归和分类联合起来，可进行端到端的优化。</p><blockquote><p>We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.</p></blockquote><p>YOLO算法具有以下几个特性</p><ol><li><strong>Extremely fast.</strong> 将检测作为回归问题后，不需要候选框生成、分类、回归框矫正等几个步骤，可达到45FPS的速度，Fast YOLO甚至能达到155FPS；</li><li><strong>Reasons globally.</strong> YOLO接受整个图像的输入，而不是将感兴趣区域单独提取，这相比于R-CNN算法，对上下文信息的使用更加全面；</li><li><strong>Highly generalizable.</strong> 当在艺术图上进行识别时，也具有较好的识别效果。</li></ol><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>YOLO的思路是，将一张指定尺寸输入的图片分割为$S \times S$个网格，每个网格负责落在它内部的物体的检测，并且每个网格将负责$B$个回归框的检测，每个回归框的参数包括$(x, y, w, h, c)$，$x, y$表示回归框的中心点坐标，$w, h$表示回归框的边长，$c$表示回归框的置信度表示cell内是否包含真实框中心点。此外，每个网格也负责$C$类的物体分类任务，输出$C$维的分类向量。因此输入图片经过深度卷积网络后，得到的特征图尺寸应为$S \times S \times [B \times (4 + 1) + C]$，如在论文中提到，在PSCAL VOC数据集上，指定$S=7, B=2, C=20$，则特征图尺寸为$7 \times 7 \times 30$。</p><blockquote><p>这里有个疑问，是训练样本confidence真实值如何确定，难道是计算cell与真实框的IoU？这样计算得到的值很小。</p></blockquote><p>下图为论文前3个版本中的示意图。</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/prepareDataV1.png" alt="prepareDataV1"></p><p>上述说明比较抽象，用代码说明生成的target是个什么玩意。假如我们有图像如下</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/dog.jpg" alt="dog"></p><p>首先将其裁剪为正方形，并缩放至$448 \times 448$，得到</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/resized.jpg" alt="resized"></p><p>将图像分成$7 \times 7$个网格，并假设其标注位置为：狗$( 95, 295, 140, 250)$、自行车$(205, 212, 340, 214)$、汽车$(365,  92, 150,  64)$，在图上为</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/annotated.jpg" alt="annotated"></p><p>假设在$7 \times 7$特征图上，某cell的位置为$(i, j)$，其中$0 \leq i, j &lt; 7$，应有$i = \lfloor\frac{x}{cellsize}\rfloor, j = \lfloor\frac{y}{cellsize}\rfloor$。若映射回原图后，该cell内包含某真实标记框$(x, y, h, w)$的中心，则置$c=1$，且对应类别的特征图上置$(i, j, class)$处类别信息为1。关于ground truth标记框相对于cell的各个偏移量计算与可视化输出如下</p><script type="math/tex; mode=display">\begin{cases}    t_x = \frac{x - x'}{cellsize}, x' = (i + \frac{1}{2}) \times cellsize\\    t_y = \frac{y - y'}{cellsize}, y' = (j + \frac{1}{2}) \times cellsize\\    t_h = \frac{h}{cellsize} \\    t_w = \frac{w}{cellsize}\end{cases}</script><p>其中$x, y, cellsize$等都除以图像尺寸进行归一化。</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/groudtruth.jpg" alt="groudtruth"></p><p>类别特征图$(S \times S \times C)$比较好理解，在上图中用颜色表示，展开为张量即可。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/2019/09/01/YOLO-v1-v2-v3/YOLOv1-architecture.PNG" alt="YOLOv1-architecture"></p><p>网络共包含24个卷积层与2个全连接层。卷积的基本结构是先通过$1 \times 1$卷积层减少前层输出特征图的通道数，再紧接用$3 \times 3$卷积层进行运算。其使用darknet训练的配置文件如<a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov1.cfg" target="_blank" rel="noopener">yolov1.cfg</a>，PyTorch实现可查看<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/blob/master/2019/09/01/YOLO-v1-v2-v3/yolo.py" target="_blank" rel="noopener">yolo.py</a>，文中插代码很不地道。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数由三部分组成，使用sum-square error作为误差函数，因为易于优化</p><script type="math/tex; mode=display">L = \sum_i (y_i - \hat{y_i})^2</script><p>定义</p><script type="math/tex; mode=display">1^{\rm{condition}}_{\rm{index}} =     \begin{cases}        1 & \text{if meets condition} \\        0 & \text{otherwise}    \end{cases}</script><ol><li><p>坐标</p><p> 考虑到，小物体定位惩罚应该比大物体的惩罚更大。所以在边长回归时，考虑回归边长的开方值，以平衡大小物体的惩罚。</p><script type="math/tex; mode=display"> \begin{aligned}     L_{\rm{coord}} = \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right. \\     \left. + (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] \end{aligned}</script></li><li><p>置信度</p><p> 图像中具有物体的网格是非常稀疏的，导致正负样本数目不均衡，这些作为负样本的cell会将置信度分数推向$0$，影响网络模型的稳定性,通过系数$\lambda_{\rm{noobj}}$进行平衡</p><script type="math/tex; mode=display"> \begin{aligned}     L_{\rm{confid}} = \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} (C_i - \hat{C}_i)^2 \\     + \lambda_{\rm{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{noobj}}_{\rm{ij}} (C_i - \hat{C}_i)^2 \end{aligned}</script></li><li><p>类别</p><p> 注意到，分类任务仅对包含物体的cell进行惩罚。</p><script type="math/tex; mode=display">L_{\rm{classify}} = \sum_{i=0}^{S^2} 1^{\rm{obj}}_{\rm{i}} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2</script></li></ol><p>如果将定位误差和分类误差相等地加权，可能不理想。所以，通过设置系数$\lambda_{\rm{coord}}$进行平衡，增大边界框回归的惩罚，降低分类的惩罚。</p><script type="math/tex; mode=display">\begin{aligned}    \lambda_{\rm{coord}} \underbrace{\sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 + (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right]}_{L_{\rm{coord}}} \\    + \underbrace{\sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} (C_i - \hat{C}_i)^2     + \lambda_{\rm{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{noobj}}_{\rm{ij}} (C_i - \hat{C}_i)^2}_{L_{\rm{confid}}} \\    + \underbrace{\sum_{i=0}^{S^2} 1^{\rm{obj}}_{\rm{i}} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2}_{L_{\rm{classify}}}\end{aligned}</script><p>论文中设置$\lambda_{\rm{coord}}=5, \lambda_{\rm{noobj}}=0.5$。</p><h2 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h2><p>首先，将网络的前20层卷积分离，添加全局均值池化与全连接层，在ImageNet数据集上进行预训练，输入图像的尺寸为$224 \times 224$。大约训练了一周，并取得了top-5达到$88\%$的准确率。</p><p>之后，将模型用于检测的训练。Ren等人提出，全连接层可提高网络的performance，所以在预训练的20层卷积后，添加4层卷积层与2层全连接层，权值随机初始化。图像的分辨率升高到$448 \times 448$。</p><p>在测试时，每个cell将负责多个框的生成；在训练时，每个cell对每个回归框负责，也就是说，如果某个cell内出现两个重复的真实标记框，将对这两个框分别计算损失并累加。</p><p>训练超参数设置。在PASCAL VOC 2007和2012的训练集与验证集上进行训练，注意在测试2012数据集时，也包含2017测试数据。批次大小设置为$64$，动量(momentum)设置为$0.9$，权值衰减设置为$0.0005$。</p><blockquote><p>祖传参数。</p></blockquote><p>学习率调节。如果一开始设置较大的学习率，不稳定的梯度会导致网络发散。因此从第一个epoch开始，缓慢将学习率从$10^{-3}$升高到$10^{-2}$，之后在$10^{-2}$保持$75$代，然后在$10^{-3}$保持$30$代，最后在$10^{-4}$保持$30$代。</p><h2 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h2><p>测试阶段，置信度的计算包含两个部分的乘积，即框内是否含物体$\rm{Pr}(\rm{object})$与候选框与真实框之间的交并比$\rm{IoU}(truth, pred)$，计算方法如下</p><script type="math/tex; mode=display">\rm{Confidence} = \rm{Pr}(Class_i | Object) * \rm{Pr}(Object) * \rm{IoU}^{truth}_{pred} = \rm{Pr}(Class_i) * \rm{IoU}^{truth}_{pred}</script><p>根据置信度，先删除一些评分较低的候选框，之后用NMS算法删除冗余框。其最终输出可由下图说明</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/test_step.png" alt="test_step"></p><h2 id="结果对比"><a href="#结果对比" class="headerlink" title="结果对比"></a>结果对比</h2><p><img src="/2019/09/01/YOLO-v1-v2-v3/VOC2012_Leaderboard.png" alt="VOC2012_Leaderboard"></p><h2 id="缺点和限制"><a href="#缺点和限制" class="headerlink" title="缺点和限制"></a>缺点和限制</h2><ol><li>每个单元网格只能预测两个框，且两个框同属于一类，这限制了模型对相邻物体的检测；</li><li>另外，对于聚集的小物体，如鸟群等，很难检测到；</li><li>由于网络经过已有标注训练，对于新的纵横比很难适应；</li><li>网络多次进行下采样，特征较为粗略；</li><li>小物体检测比大物体检测错误率高，这是由于错误定位造成的。</li></ol><h1 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h1><p>YOLOv2即YOLO9000，在上一代YOLO的基础上进行改进。首先，它可以接受任意尺寸图像的输入而不是固定的$448 \times 448$，速度更快。此外，一个比较重要的点是，在训练YOLO9000时，联合使用分类数据，用带标记的图像进行学习精确定位，而分类图像添加到样本集中，用作分类部分的训练。</p><h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><p>YOLOv2进行了众多实验，添加别的组件，验证新模型在识别率上的表现，如下表</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/YOLOv2-Table2.png" alt="YOLOv2-Table2"></p><ol><li><p>Batch Normalization</p><p> 添加批归一化层后，mAP升高$2\%$以上。并且可减少其他形式的正则化惩罚，如移除了dropout层。</p></li><li><p>High Resolution Classifier</p><p> 在YOLOv1训练时，使用$224 \times 224$尺寸的图片进行模型预训练，之后用$448 \times 448$尺寸的图片进行检测网络的训练，也就是说，网络需要在学习检测任务的同时，适应新的输入分辨率。所以在YOLOv2训练时，先将预训练网络在$448 \times 448$尺寸的数据机上进行微调，共迭代10代。之后训练检测网络，这提升了$4\%$的mAP。</p></li><li><p>Convolutional With Anchor Boxes</p><p> YOLOv2移除了全连接层，并添加了锚框。首先，移除一层最大池化层，使得特征图的分辨率升高，这样输出的特征图尺寸为$13 \times 13$。考虑到特征图的几何意义，最终层输出的特征图每个位置代表一个网格，也就是说，每个网格中心都应对应一个位置，但是$448/13$不为整数，故将图像分辨率调整至$416 \times 416$，网络下采样倍率为$\frac{416}{13}=32$。</p><p> 在不添加锚框时，一个cell仅能输出$7 \times 7 \times 2 = 98$个候选框。而每个cell设置锚框后，对每个锚框均进行预测，而不是仅考虑cell的空间位置，可生成更多的候选框。</p><p> 没有锚框的情况下，模型指标为$69.5$mAP与$81\%$的召回率，增加后，mAP降低为$69.2$，召回率上升到$88\%$，这说明模型还有较大空间可以调整。</p><p> 对于锚框，文中没有提及，以下进行一些说明，现在每个cell的尺寸为$32\times32$，假定对单个cell设置$3$种纵横比的anchor，如$32 \times 32, 24 \times 48, 48 \times 24$，那么对于下图中包含物体的cell，锚框可视化为</p><p> <img src="/2019/09/01/YOLO-v1-v2-v3/anchor.jpg" alt="anchor"></p><p> 对于上述三个锚框，从上至下分别为$32 \times 32, 24 \times 48, 48 \times 24$的锚框对应的特征图显示如下</p><p> <img src="/2019/09/01/YOLO-v1-v2-v3/groudtruth_32x32.jpg" alt="groudtruth_32x32"></p><p> <img src="/2019/09/01/YOLO-v1-v2-v3/groudtruth_24x48.jpg" alt="groudtruth_24x48"></p><p> <img src="/2019/09/01/YOLO-v1-v2-v3/groudtruth_48x24.jpg" alt="groudtruth_48x24"></p><blockquote><p>此外，每个纵横比下可分别设置$3$种尺度。</p></blockquote></li><li><p>Dimension Clusters</p><p> 至此遇到两个问题。第一，锚框的纵横比、尺度等是人为指定的，网络可以适应指定的锚框，但不是最优的，希望能够从数据中得到锚框的参数。所以对标注框的长宽尺寸进行K-Means聚类，注意到，如果使用欧式距离作为聚类指标，框越大它造成的误差也越大，希望的是IOU评分上升，所以定义指标为</p><script type="math/tex; mode=display">d(\rm{box}, \rm{centroid}) = 1 - \rm{IoU}(\rm{box}, \rm{centroid})</script><p> 最终确定为$k=5$，在VOC和MSCOCO数据机上可能有所差异，详情查看下图<br> <img src="/2019/09/01/YOLO-v1-v2-v3/YOLOv2-Figure2.png" alt="YOLOv2-Figure2"></p></li><li><p>Direct location prediction</p><p> 对于偏移量，如果沿用一般的计算方法，则有</p><script type="math/tex; mode=display"> \begin{cases}     t_x = \frac{x - x_a}{w_a}, x_a = (\lfloor\frac{x}{cellsize}\rfloor + \frac{1}{2}) \times w_a \\     t_y = \frac{y - y_a}{h_a}, y_a = (\lfloor\frac{y}{cellsize}\rfloor + \frac{1}{2}) \times h_a \\     t_w = \frac{w}{w_a} \\     t_h = \frac{h}{h_a}  \end{cases}</script><p> 也即</p><script type="math/tex; mode=display"> \begin{cases}     x = t_x \times w_a + x_a \\     y = t_y \times h_a + y_a \\     w = t_w \times w_a \\     h = t_h \times h_a  \end{cases}</script><p> 当$t_x=1$时表示相对于锚框向右平移$w_a$的距离，$t_x=-1$时表示相对于锚框向左平移$w_a$的距离，上式中对$t_x$是没有限制作用的。现添加sigmoid函数处理，将值映射到$[0, 1]$范围内，并且直接预测坐标，而不是偏移量的方法，此外，利用指数函数$e^x$进行映射，平衡回归框尺寸对损失值的影响。如下</p><script type="math/tex; mode=display"> \begin{cases}     b_x = \sigma(t_x) + c_x \\      b_y = \sigma(t_y) + c_y \\      b_w = p_w e^{t_w} \\     b_h = p_h e^{t_h} \end{cases}</script><p> 其中$(c_x, c_y)$为特征图上相对于左上角的坐标，$\sigma(t_x), \sigma(t_y), t_w, t_h$是网络的输出，$p_*$为锚框的尺寸信息(除以图像边长归一化)。这种计算方法下，各锚框的ground truth应为</p><script type="math/tex; mode=display"> \begin{cases}     \sigma(t_x) = \frac{x}{cellsize} - \lfloor\frac{x}{cellsize}\rfloor \\     \sigma(t_y) = \frac{y}{cellsize} - \lfloor\frac{y}{cellsize}\rfloor \\     t_w = \log \frac{b_w}{p_w} \\     t_h = \log \frac{b_h}{p_h}  \end{cases}</script><p> <img src="/2019/09/01/YOLO-v1-v2-v3/groudtruth_32x32_new.jpg" alt="groudtruth_32x32_new"></p><p> <img src="/2019/09/01/YOLO-v1-v2-v3/groudtruth_24x48_new.jpg" alt="groudtruth_24x48_new"></p><p> <img src="/2019/09/01/YOLO-v1-v2-v3/groudtruth_48x24_new.jpg" alt="groudtruth_48x24_new"></p></li><li><p>Fine-Grained Features</p><p> 为获得更细粒度的特征，用于小物体的检测，如Faster-RCNN与SSD是在各层特征图上进行候选框的生成。YOLOv2添加了passthrough层，将浅层特征与深层特征堆叠起来。这提升了$1\%$的表现。</p></li><li><p>Multi-Scale Training</p><p> 由于YOLOv2中仅包含卷积层与池化层，所以可接受任意尺寸的输入，为使得网络对各尺寸均有良好的鲁棒性，在尺寸为$\{320, 352, \cdots, 608\}$时都进行训练。这样做可以使图像尺寸减小以加快计算速度，减少显存消耗，并且不降低准确率，在$288 \times 288$的尺寸时，可达到$90$FPS。</p></li></ol><h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><ol><li><p>Darknet-19</p><p> 大多数检测器用VGG-16作为特征提取网络结构，但是它参数量众多，且运算量很大，即使输入为$224 \times 224$也需要千万级别的FLOPS。新设计的Darknet-19架构，仅需要$5.58$百万级别的FLOPS，达到了72.9% top-1和91.2% top-5的准确率。</p></li><li><p>Training for classification</p><p> 首先，在ImageNet上进行1000类的图像分类模型训练，共迭代160代，初始学习率为$0.1$，多项式学习率衰减，权值衰减设置为$4e-5$，动量$0.9$。之后，将图像分辨率提高到$448 \times 448$，学习率0.001，进行10个周期的迭代，来微调网络，使其适应该分辨率。</p></li><li><p>Training for detection</p><p> 将预训练模型的最后一层卷积层删除，添加$3 \times 3 \times 1024$卷积层与$1 \times 1 \times [(C + 4 + 1) \times B]$卷积层。注意到，与YOLOv1不同的是，每个框都会输出类别信息。在VOC数据集上，$C = 20, B = 5$。在$3 \times 3 \times 512$层的输出到最终的输出间设置passthrough，以更好地使用浅层特征。</p><p> 共进行$160$个周期的迭代，初始学习率$0.001$，在第$60, 90$代时衰减$10\%$。权重衰减$5e-4$，动量$0.9$。</p></li></ol><h2 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h2><p>在YOLO9000训练过程中，用到了一种联合分类数据的训练机制。将监测数据与分类数据混合，若训练过程中遇到检测数据，则计算全部的损失，而若遇到分类数据，仅计算分类的损失。但是这样做有一些挑战，检测数据集中对物体的标号仅仅包含dog、boat等，而分类数据标签划分更细，如dog可分为Norfolk terrier、Yorkshire terrier等。分类模型使用softmax作为损失函数，只能进行单标签的分类。一种解决方法是，采用多标签的模型来组合不假设互斥的数据集，这种方法忽略了我们对数据所知的所有结构，例如，MSCOCO的类别都是互斥的。</p><ol><li><p>Hierarchical classification</p><p> ImageNet的标签来源于<a href="https://wordnet.princeton.edu/" target="_blank" rel="noopener">WordNet</a>，这是一个语言的数据集合，整理了语义关系。比如说，Norfolk terrier和Yorkshire terrier都是猎犬，都是犬等等。由于语言的复杂性，WordNet的结构是有向图，在训练YOLOv2时，并不需要完整的图结构，因此可通过这种非扁平的数据结构构建分层树形图来简化问题。</p><p> 对于ImageNet中的视觉名词，许多同义词只有一条路径，所以首先将这些路径添加到树中，然后继续迭代剩下的名词。如果某个概念通往根节点有两条路径，那么其中一条会往树中添加3条边，另一条路径添加1条边，选择尽量短的边添加方案。最终形成层级的结构WordTree，这个树可用于计算条件概率，如</p><script type="math/tex; mode=display"> \begin{aligned}     Pr(\rm{Norfolk terrier} | \rm{terrier}) \\     Pr(\rm{Yorkshire terrier} | \rm{terrier}) \\     Pr(\rm{Bedlington terrier} | \rm{terrier}) \\     \cdots \end{aligned}</script><p> 对于某个类别的概率，其通过各条件概率累乘的方法，如</p><script type="math/tex; mode=display"> \begin{aligned}     Pr(\rm{Norfolk terrier}) = \\     Pr(\rm{Norfolk terrier} | \rm{terrier}) \\     * Pr(\rm{terrier} | \rm{hunting dog}) \\     * \cdots \\     * Pr(\rm{mammal} | \rm{animal}) \\     * Pr(\rm{animal} | \rm{physical object}) \\     * Pr(\rm{physical object}) \end{aligned}</script><p> 注意$Pr(\rm{physical object})=1$。</p></li><li><p>Dataset combination with WordTree</p><p> 借助于WordTree的多层级结构，构建多标签分类模型，如下图，进行多次softmax运算</p><p> <img src="/2019/09/01/YOLO-v1-v2-v3/YOLOv2-Figure5.png" alt="YOLOv2-Figure5"></p></li><li><p>Joint classification and detection</p><p> 在MSCOCO与ImageNet数据集上，产生了9000多个类别。此外我们需要对模型进行验证。</p><blockquote><p>We also need to evaluate our method so we add in any classes from the ImageNet detection challenge that were not already included. The corresponding WordTree for this dataset has 9418 classes.</p></blockquote><p> ImageNet是一个更大的数据集，所以通过对MSCOCO进行过采样来平衡数据，使ImageNet以4：1的比例增大。使用此数据集进行训练时，仅选用3个锚框以减少内存消耗，当网络遇到带检测标签的数据，进行全部的损失计算，而对于分类数据，仅对分类损失进行反传。值得注意的是，如果标签为dog，仅对该层次进行反传，因为再向下没有细化信息，如这是德国牧羊犬还是金毛犬。</p></li></ol><h1 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h1><p>YOLOv3相比较于YOLO9000没有过大的改变：</p><ul><li>边界框回归值同YOLO9000；</li><li>仍进行多标签的分类；</li><li>关于锚框，共3种尺度$(S=3)$，类似特征金字塔的概念，则最终层输出为$N \times N \times [(4 + 1 + C) \times B \times S]$；</li><li>将多个浅层的特征层上采样后进行组合，用于候选框的生成；</li><li>仍使用K-Means确定锚框的纵横比，最终确定9种纵横比$10 \times 13, 16 \times 30, 33 \times 23, 30 \times 61, 62 \times 45, 59 \times 119, 116 \times 90, 156 \times 198, 373 \times 326$，每个纵横比下3种尺度，共27个锚框。</li></ul><p>网络仍使用$3 \times 3$与$1 \times 1$卷积层的堆叠，加入shortcut使深度大大增加，最终形成Darknet-53。</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/YOLOv3-Darknet-53.png" alt="YOLOv3-Darknet-53"></p><p>YOLOv3运算速度相当快，准确率也可以</p><p><img src="/2019/09/01/YOLO-v1-v2-v3/YOLOv3-Figure3.png" alt="YOLOv3-Figure3"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO: Real Time Object Detection</a></li><li><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></li><li><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO9000: Better, Faster, Stronger</a></li><li><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener">YOLOv3: An Incremental Improvement</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[解读] Recent Advances in Deep Learning for Object Detection</title>
      <link href="/2019/08/31/%E8%A7%A3%E8%AF%BB-Recent-Advances-in-Deep-Learning-for-Object-Detection/"/>
      <url>/2019/08/31/%E8%A7%A3%E8%AF%BB-Recent-Advances-in-Deep-Learning-for-Object-Detection/</url>
      
        <content type="html"><![CDATA[<p>论文可从<a href="https://arxiv.org/abs/1908.03673?context=cs.CV" target="_blank" rel="noopener">Recent Advances in Deep Learning for Object Detection - arXiv.org</a>下载。</p><h1 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h1><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/contents.jpg" alt="contents"></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><blockquote><p>Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. <strong>Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label.</strong> Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: <strong>(i) detection components, (ii) learning strategies, and (iii) applications &amp; benchmarks.</strong> In the survey, we cover a variety of factors affecting the detection performance in detail, <strong>such as detector architectures, feature learning, proposal generation, sampling strategies, etc.</strong> Finally, we discuss <strong>several future directions to facilitate and spur future research</strong> for visual object detection with deep learning.<br><strong>Keywords:</strong> Object Detection, Deep Learning, Deep Convolutional Neural Networks</p></blockquote><p>目标检测：在给定图像中找到目标类对象的精确位置，并分配相应的类别标签。</p><p>本文介绍的主要分为以下几个部分</p><ul><li>组件组成部分；</li><li>学习策略；</li><li>应用及数据集、测试。</li></ul><p>影响检测的几个细节：</p><ul><li>检测器结构；</li><li>特征学习；</li><li>预选框生成；</li><li>采样策略等。</li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><h2 id="1-1-基本的视觉识别问题"><a href="#1-1-基本的视觉识别问题" class="headerlink" title="1.1 基本的视觉识别问题"></a>1.1 基本的视觉识别问题</h2><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig1.jpg" alt="Fig1"></p><ul><li>图像分类(image classification)：识别给定图像中对象的语义类别；</li><li>物体检测(object detection)：    识别给定图像中对象的语义类别，并用边界框表出物体位置；</li><li>语义分割(semantic segementation)：为每个像素分配特定的类别标签；</li><li>实例分割(instance segementation)：目标检测和语义分割交集，在语义分割基础上，在同类别物体中分辨出不同实例。</li></ul><h2 id="1-2-如何设计好的检测算法"><a href="#1-2-如何设计好的检测算法" class="headerlink" title="1.2 如何设计好的检测算法"></a>1.2 如何设计好的检测算法</h2><p>好的检测算法需要对语义及空间信息有很好的理解。</p><blockquote><p>A good detection algorithm should have a strong understanding of semantic cues as well as the spatial information about the image.</p></blockquote><h2 id="1-3-早期物体检测算法"><a href="#1-3-早期物体检测算法" class="headerlink" title="1.3 早期物体检测算法"></a>1.3 早期物体检测算法</h2><p>可分成三个步骤</p><ul><li>候选框生成(proposal generation)<br>一种直观方法：滑动窗口(sliding window)，将检测图像以不同比例缩放，并使用不同尺寸的滑动窗口选择感兴趣区域(regions of interest)。</li><li>特征提取(feature vector extraction)<br>从ROI区域提取固定长度的向量作为特征，如尺度不变特征变换(SIFT)，Haar，梯度直方图(HOG)，或加速鲁棒特征(SURF)。</li><li>区域分类(region classification)<br>例如支持向量机(SVM)，配合bagging，adaboost，级联学习(cascade learning)等方法。</li></ul><p>传统方法存在的局限性</p><ul><li>在候选框生成时，大量冗余的候选框造成误正(false positive)率高。窗口尺度是人为或启发式指定的，不能很好地匹配物体；</li><li>特征提取时，基于低级视觉特征，难以捕获上下文语义信息；</li><li>以上三个步骤单独设计优化，无法获得全局最优解。</li></ul><h2 id="1-4-深度学习方法"><a href="#1-4-深度学习方法" class="headerlink" title="1.4 深度学习方法"></a>1.4 深度学习方法</h2><p>模拟生物学分层结构，利用反向传播算法更新参数，但存在局限性：</p><ul><li>训练样本过少，造成过拟合；</li><li>计算资源的限制；</li><li>相比较于SVM等传统方法，缺少理论支持。</li></ul><p>其优点是</p><ul><li>深度卷积神经网络生成从原始像素到高级语义信息的分层特征表示，其从训练数据中自动学习，并且在复杂上下文中显示出更具辨别力的表达能力；</li><li>于传统视觉描述符相比，自动学习特征表示，而不是固定的；</li><li>可以以端到端的方式进行优化。</li></ul><h2 id="1-5-基于深度学习的对象检测框架"><a href="#1-5-基于深度学习的对象检测框架" class="headerlink" title="1.5 基于深度学习的对象检测框架"></a>1.5 基于深度学习的对象检测框架</h2><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig2.jpg" alt="Fig2"></p><p>两种系列</p><ul><li>两阶段检测器(two-stage detector)：通过候选框生成器，生成数目较少的候选框，并对每个候选框内内容进行特征提取；应用特征对候选区域进行分类。如 R-CNN(CNN + SVM/FNN)；</li><li>一阶段检测器(one-stage detector)：无虚级联区域分类步骤，对特征图的每个位置上的对象进行分类预测。如 YOLO 及其变体。<br>通常来说，一阶段检测器速度更快，可用于试试物体检测；二阶段检测器精度更高，在公共数据集等测试中效果更好。</li></ul><h1 id="2-Problem-Settings"><a href="#2-Problem-Settings" class="headerlink" title="2. Problem Settings"></a>2. Problem Settings</h1><p>目标检测涉及识别(如物体分类)和定位(如坐标回归)任务。给定数据集包含$N$张带标记的图片</p><script type="math/tex; mode=display">D = \{X^{(1)}, X^{(2)}, \cdots, X^{(N)}\}</script><p>对于第$i$张图片，若其内部包含$M_i$个物体，这些物体属于$C$类，第$j$个物体的标签包含所属类别$c^{(i)}_j$，所在位置$b^{(i)}_j$，则</p><script type="math/tex; mode=display">y^{(i)}_j = (c^{(i)}_j, b^{(i)}_j)</script><p>则该图片对应标签集为</p><script type="math/tex; mode=display">y^{(i)} = \{(c^{(i)}_1, b^{(i)}_1), (c^{(i)}_2, b^{(i)}_2), \cdots, (c^{(i)}_{M_i}, b^{(i)}_{M_i})\}</script><p>对于预测输出，为</p><script type="math/tex; mode=display">\hat{y}^{(i)} = \{(\hat{c}^{(i)}_1, \hat{b}^{(i)}_1), (\hat{c}^{(i)}_2, \hat{b}^{(i)}_2), \cdots, (\hat{c}^{(i)}_{M_i}, \hat{b}^{(i)}_{M_i})\}</script><p>总体损失定义为</p><script type="math/tex; mode=display">L(X, \theta) = \frac{1}{N} \sum_{i=1}^N L(X^{(i)}, y^{(i)}, \hat{y}^{(i)}; \theta) + \frac{\lambda}{2} ||\theta||_2^2</script><p>定义指标交并比IoU(intersection-over-union)</p><script type="math/tex; mode=display">\text{IoU} (b^{(i)}_j, \hat{b}^{(i)}_j) = \frac{\text{Area} (b^{(i)}_j \bigcap \hat{b}^{(i)}_j)}{\text{Area} (b^{(i)}_j \bigcup \hat{b}^{(i)}_j)}</script><p>则预测正确与否的判决为</p><script type="math/tex; mode=display">\text{Prediction} =     \begin{cases}         \text{Positive} & c^{(i)}_j = \hat{c}^{(i)}_j \quad \text{and} \quad \text{IoU} (b^{(i)}_j, \hat{b}^{(i)}_j) > \Omega \\         \text{Negative} & \text{otherwise}     \end{cases}</script><p>另外，在$C$类物体的检测问题中，需要计算mAP(mean average precision)进行评估。通常来说，达到20FPS的检测器可用于实时检测场景。</p><h1 id="3-Detection-Components"><a href="#3-Detection-Components" class="headerlink" title="3. Detection Components"></a>3. Detection Components</h1><h2 id="3-1-Detection-Settings-bbox-level-and-mask-level-algorithms"><a href="#3-1-Detection-Settings-bbox-level-and-mask-level-algorithms" class="headerlink" title="3.1. Detection Settings: bbox-level and mask-level algorithms"></a>3.1. Detection Settings: bbox-level and mask-level algorithms</h2><ul><li>bbox-level<br>只需要边界框注释，在评估时，计算预测与实际边界框的交并比IoU进行性能衡量。</li><li>mask-level<br>即实例分割，需要通过像素级掩码而不是粗略的边界框来分割对象，对空间信息的处理要求更改。</li></ul><h2 id="3-2-Detection-Paradigms-two-stage-detectors-and-one-stage-detectors"><a href="#3-2-Detection-Paradigms-two-stage-detectors-and-one-stage-detectors" class="headerlink" title="3.2. Detection Paradigms: two-stage detectors and one-stage detectors"></a>3.2. Detection Paradigms: two-stage detectors and one-stage detectors</h2><ul><li>两阶段检测器(two-stage detector)：通过候选框生成器，生成数目较少的候选框，并对每个候选框内内容进行特征提取；应用特征对候选区域进行分类。如 R-CNN(CNN + SVM/FNN)；</li><li>一阶段检测器(one-stage detector)：无虚级联区域分类步骤，对特征图的每个位置上的对象进行分类预测。如 YOLO 及其变体。<br>通常来说，一阶段检测器速度更快，可用于试试物体检测；二阶段检测器精度更高，在公共数据集等测试中效果更好。</li></ul><h3 id="3-2-1-Two-stage-Detectors"><a href="#3-2-1-Two-stage-Detectors" class="headerlink" title="3.2.1. Two-stage Detectors"></a>3.2.1. Two-stage Detectors</h3><p>将检测任务分成两部分：</p><ul><li>候选框生成<br>基本上思想是选出高召回率(Recall)的图像区域。</li><li>对候选框内图像进行判别<br>基于深度学习的模型，可将框内图像进行分类，此外，该模型输出也用以矫正第一阶段输出的候选框，使其更契合物体位置。</li></ul><h4 id="3-2-1-1-R-CNN"><a href="#3-2-1-1-R-CNN" class="headerlink" title="3.2.1.1. R-CNN"></a>3.2.1.1. R-CNN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-CNN.png" alt="Fig3-R-CNN"></p><ol><li>组成</li></ol><ul><li>候选框生成：通过选择性搜索(Selective Search)生成，可拒绝一些明显是背景的候选框，减少误正率(false positive)；</li><li>特征提取：通过裁剪、缩放获得固定尺寸图像，输入到深度卷积网络，得到4096维的向量；</li><li>区域分类：分类器选用一对多(one-vs-all)SVM，此外边界框回归器将候选框矫正。</li></ul><ol><li>细节</li></ol><ul><li>与传统方法相比，深度卷积网络生成分层特征，捕获不同尺度信息；</li><li>利用迁移学习方法，使用ImageNet与训练的卷积网络权重，全连接层重新初始化权值；</li></ul><ol><li>缺点</li></ol><ul><li>不共享权值，造成大量冗余计算；</li><li>各部分独立，不能进行端到端的方式优化，难以获得全局最优；</li><li>选择性搜索难以适应复杂背景的图片，并且无法使用GPU加速。</li></ul><h4 id="3-2-1-2-SPP-Net"><a href="#3-2-1-2-SPP-Net" class="headerlink" title="3.2.1.2. SPP-Net"></a>3.2.1.2. SPP-Net</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-SPP-net.png" alt="Fig3-SPP-net"></p><ol><li><p>细节<br>在处理候选框内的图片时，并不将其进行裁剪缩放，而是定义空间金字塔池化层(Spatial Pyramid Pooling Layer)。<br>例如对于尺寸为$H \times W$的可见光图像，将其划分为$N \times N$个网格，在每个尺寸为$\frac{H}{N} \times \frac{W}{N}$网格内进行池化操作。选取不同的$N$重复采样，将不同划分数目$N$下得到的输出，合并为长度为$N \times N$向量。<br>SPP-layer可以接受不同尺度和纵横比的图像，故避免了信息丢失和几何失真等情况。</p></li><li><p>缺点</p></li></ol><ul><li>仍旧不能进行端到端的方式优化；</li><li>由于该层不能进行反向传播，此层之前的网络参数需要固定。</li></ul><h4 id="3-2-1-3-Fast-R-CNN"><a href="#3-2-1-3-Fast-R-CNN" class="headerlink" title="3.2.1.3. Fast R-CNN"></a>3.2.1.3. Fast R-CNN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-Fast-R-CNN.png" alt="Fig3-Fast-R-CNN"></p><ol><li><p>细节<br>计算了整个图像的特征图(同SPP-Net)，在特征图上提取固定长度的特征，这一步使用ROI Pooling Layer实现，是Spatial Pyramid Pooling Layer的特殊情况，仅在一种网格划分数目下采样，实际操作步骤如下：在<strong>整个图像</strong>计算得到的特征图中，根据实际边界框位置选取尺寸为$h \times w$的感兴趣区域(ROI)，指定网格的大小如$H \times W$，将ROI划分为$\frac{h}{H} \times \frac{w}{W}$个网格，在每个网格内进行池化操作，得到$\frac{h}{H} \times \frac{w}{W}$的向量。<br>分类与边界框回归为两个单独的全连接层网络，而不是采用SVM，分类器输出维数为$C+1$($C$种类别加背景)，回归器输出维数为$4 \times C$(各类别分别对应矫正参数)。<br>值得注意的是，该结构特征提取、区域分类与边界框回归这几个步骤可通过端到端的方式优化，</p></li><li><p>缺点<br>候选框仍使用传统方法生成，如选择性搜索(Selective Search)或边缘框(Edge Boxes)。</p></li></ol><h4 id="3-2-1-4-Faster-R-CNN"><a href="#3-2-1-4-Faster-R-CNN" class="headerlink" title="3.2.1.4. Faster R-CNN"></a>3.2.1.4. Faster R-CNN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-Faster-R-CNN.png" alt="Fig3-Faster-R-CNN"></p><ol><li><p>细节<br>提出新的候选框生成方法：Region Proposal Network(PRN)，为全卷积神经网络，接受任意大小的图像。在检测时，用大小为$n \times n$的滑动窗口在特征图上滑动，每个位置提取特征，送入分类层(cls)与回归层(reg)，最终结果用于确定候选框。<br>可将RPN插入到Fast R-CNN中从而以端到端的方式进行优化。</p></li><li><p>缺点<br>尽管Faster R-CNN在提取特征图时共享权值，但对于后续每个ROI内的特征图对应的向量，仍需要单独通过全连接计算。</p></li></ol><h4 id="3-2-1-5-R-FCN"><a href="#3-2-1-5-R-FCN" class="headerlink" title="3.2.1.5. R-FCN"></a>3.2.1.5. R-FCN</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-FCN.png" alt="Fig3-R-FCN"></p><ol><li>细节<br>提出Positive-Sensitive ROP Pooling(PSROI Polling)，生成名为Position-Sensitive Score Map的特征图，保持了空间信息(to extract spatial-aware region features by encoding each relative position of the target regions)。如对于Stage 1生成的特征图，设其尺寸为$h \times w \times c$，用$k^2(C+1)$个$1 \times 1$的卷积核，即$k^2(C+1) \times 1 \times 1 \times c$进行运算，得到$h \times w \times k^2(C+1)$的Position-Sensitive Score Map。</li></ol><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig3-R-FCN-psROI.png" alt="Fig3-R-FCN-psROI"></p><p>当取$k=3$时，表示将每个ROI划分为$3 \times 3$，在特征图平面位置为$(x, y)$处，可切片得到向量$f_{3^2 \times (C+1)}$，对于所属类别$C_j$的部分，又有$f^{(j)}_{3^2}$，表示该ROI内，左上、上、右上、左、中、右、左下、下、右下$9$个位置处，所属类别$C_j$的概率。Pooling 操作同Fast R-CNN与Faster R-CNN。</p><h4 id="3-2-1-6-FPN-MNC-Mask-R-CNN-Mask-Scoring-R-CNN-…"><a href="#3-2-1-6-FPN-MNC-Mask-R-CNN-Mask-Scoring-R-CNN-…" class="headerlink" title="3.2.1.6. FPN, MNC, Mask R-CNN, Mask Scoring R-CNN, …"></a>3.2.1.6. FPN, MNC, Mask R-CNN, Mask Scoring R-CNN, …</h4><p>略。</p><h3 id="3-2-2-One-stage-Detectors"><a href="#3-2-2-One-stage-Detectors" class="headerlink" title="3.2.2. One-stage Detectors"></a>3.2.2. One-stage Detectors</h3><p>该类检测方法没有设置单独的生成候选框的阶段，通常将图像上所有位置视为潜在对象，并尝试将每个感兴趣区域分类。</p><h4 id="3-2-2-1-YOLOv1"><a href="#3-2-2-1-YOLOv1" class="headerlink" title="3.2.2.1. YOLOv1"></a>3.2.2.1. YOLOv1</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-YOLO.png" alt="Fig4-YOLO"></p><p>YOLOv1接受$448 \times 448$大小的$3$通道图像输入，得到尺寸为$7 \times 7 \times ((4 + 1) \times n_{boxes} + n_{classes})$，$n_{boxes}$表示该网格预测输出几个回归框(YOLOv1中设置为2)，每个回归框对应$5$维特征，即$(t_x, t_y, t_w, t_h, t_c)$。可达到45FPS，或者精简网络下155FPS。但存在一些缺点</p><ul><li>每个网格处，仅能预测两个物体，对于体型较小的或是聚集的物体，难以识别；</li><li>仅通过最后一个特征图来进行预测，没有考虑多尺度和不同横比。</li></ul><h4 id="3-2-2-2-SSD"><a href="#3-2-2-2-SSD" class="headerlink" title="3.2.2.2. SSD"></a>3.2.2.2. SSD</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-SSD.png" alt="Fig4-SSD"></p><p>SSD将各层特征图均用于预测，分成固定数目的网格，每个网格设置一定尺度和纵横比的anchor。例如对于某层大小为$H \times W \times C$的特征图，经过卷积核$[n_{anchors} \times (n_{classes} + 4)] \times 3 \times 3 \times C$，得到$H \times W \times [n_{anchors} \times (n_{classes} + 4)]$的特征图，此特征图用于预测。</p><h4 id="3-2-2-3-RetinaNet"><a href="#3-2-2-3-RetinaNet" class="headerlink" title="3.2.2.3. RetinaNet"></a>3.2.2.3. RetinaNet</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-RetinaNet.png" alt="Fig4-RetinaNet"></p><p>使用Focal Loss解决了不同类别样本数目不均衡问题，此外使用Feature Pyramid Network来检测多尺度物体。</p><h4 id="3-2-2-4-CornerNet"><a href="#3-2-2-4-CornerNet" class="headerlink" title="3.2.2.4 CornerNet"></a>3.2.2.4 CornerNet</h4><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig4-CornerNet.png" alt="Fig4-CornerNet"></p><p>该网络为anchor-free类，改变以往寻找anchor框内物体的思路，而预测回归框的关键点。它将物体检测为一对角，在特征图的每个位置上，预测了类热图(Classification Heatmaps)。</p><h2 id="3-3-Backbone-Architecture"><a href="#3-3-Backbone-Architecture" class="headerlink" title="3.3. Backbone Architecture"></a>3.3. Backbone Architecture</h2><p>采用大规模图像分类的预训练模型已经成为大多物体检测网络的默认策略。但直接使用分类模型是次优的</p><ul><li>分类需要更大的感受野，并希望保持空间不变性，故应用多个下采样操作以降低特征的映射分辨率，生成的特征图是低分辨率且空间不变的。但是定位任务需要较高的空间信息；</li><li>分类使用单个特征图即可，而检测任务考虑到多尺度问题，需要在多个尺寸的特征图上进行预测。</li></ul><h3 id="3-3-1-Basic-Architecture-of-a-CNN"><a href="#3-3-1-Basic-Architecture-of-a-CNN" class="headerlink" title="3.3.1. Basic Architecture of a CNN"></a>3.3.1. Basic Architecture of a CNN</h3><p>深度卷积神经网络通常由一系列的卷积层、池化层、非线性激活层和全连接层组成。</p><ul><li>卷积层：卷积层输入输出图像可视作通道数为$C_1, C_2$的图像，卷积核参数为$C_2 \times k \times k \times C_1$；每个特征图上像素点对应原图中的大小称为感受野。</li><li>池化岑：用于扩大感受野并降低计算成本，在一定程度上增加了对图像旋转等抗干扰性。</li><li>非线性激活层：用于添加非线性信息，若无非线性层，网络再深也是线性变化。</li><li>全连接层：一般设置一系列卷积操作后，最后输出时，添加全连接层“兜底”。</li></ul><h3 id="3-3-2-CNN-Backbone-for-Object-Detection"><a href="#3-3-2-CNN-Backbone-for-Object-Detection" class="headerlink" title="3.3.2. CNN Backbone for Object Detection"></a>3.3.2. CNN Backbone for Object Detection</h3><ol><li><p>VGG16</p><p> 包括$2 + 2 + 3 + 3 + 3$层卷积层和$3$层全连接层，每组卷积层间为最大池化层。网络层数的增加可增大网络的容量，但是超过20层时，难以使用SGD进行梯度下降反传。</p></li><li><p>ResNet</p><p> 引入shortcut connection，一定程度上解决了梯度消失问题，可使网络堆叠得更深，即</p><script type="math/tex; mode=display">x_{l+1} = x_l + f_{l+1}(x_l, \theta)</script></li><li><p>ResNet-v2</p><p> ResNet-v2增加Batch Normalization层。尽管shortcut解决了训练问题，但它没有充分利用前层特征图。底层特征在逐元素操作中逐渐丢失，因此提出DenseNet，通过通道组合得方法合并特征，并且层间密集连接。</p><script type="math/tex; mode=display">x_{l+1} = x_l \circ f_{l+1}(x_l, \theta)</script></li><li><p>Dual Path Network</p><p> Dual Path Network结合两者的特点，某一层的特征图通道可分为密集连接部分和逐元素相加元素，即$C=C^d + C^r$</p><script type="math/tex; mode=display">x_{l+1} = (x^r_l + f^r_{l+1}(x^r_l, \theta^r)) \circ (x^d_l \circ f^d_{l+1}(x^d_l, \theta^d))</script></li><li><p>ResNeXt</p><p> 采用分组分离卷积的方法，大大减少了参数量与计算量。</p></li><li><p>GoogLeNet</p><p> 除了增加网络深度，还通过增加支路来扩大网络容量。</p></li></ol><h2 id="3-4-Proposal-Generation"><a href="#3-4-Proposal-Generation" class="headerlink" title="3.4. Proposal Generation"></a>3.4. Proposal Generation</h2><p>One-stage Detectors与Two-stage Detectors都产生了候选框，区别是前者在特征图对应的每个位置生成指定大小的特征图，而后者仅产生前景或背景信息的候选框，得到的结果较为稀疏。</p><h3 id="3-4-1-Traditional-Computer-Vision-Methods"><a href="#3-4-1-Traditional-Computer-Vision-Methods" class="headerlink" title="3.4.1. Traditional Computer Vision Methods"></a>3.4.1. Traditional Computer Vision Methods</h3><p>略。</p><h3 id="3-4-2-Anchor-based-Methods"><a href="#3-4-2-Anchor-based-Methods" class="headerlink" title="3.4.2. Anchor-based Methods"></a>3.4.2. Anchor-based Methods</h3><p>anchor通过人为指定的方式</p><ul><li>根据不同特征图的感受野设置锚框(SSD)；</li><li>为检测细小物体，可增大图像尺寸和减少锚框stride的方式；</li><li>基于RPN，分解锚框的维度(DeRPN)；</li><li>用K-Means确定(YOLOv2)；</li><li>先指定锚框，再通过训练结果矫正锚框(RefineNet)；</li></ul><h3 id="3-4-3-Keypoints-based-Methods"><a href="#3-4-3-Keypoints-based-Methods" class="headerlink" title="3.4.3. Keypoints-based Methods"></a>3.4.3. Keypoints-based Methods</h3><p>略。暂时没有接触过，理解不深，后续可能补上(TODO:)。</p><h3 id="3-4-4-Other-Methods"><a href="#3-4-4-Other-Methods" class="headerlink" title="3.4.4. Other Methods"></a>3.4.4. Other Methods</h3><p>略。</p><h2 id="3-5-Feature-Representation-Learning"><a href="#3-5-Feature-Representation-Learning" class="headerlink" title="3.5. Feature Representation Learning"></a>3.5. Feature Representation Learning</h2><h3 id="3-5-1-Multi-scale-Feature-Learning"><a href="#3-5-1-Multi-scale-Feature-Learning" class="headerlink" title="3.5.1. Multi-scale Feature Learning"></a>3.5.1. Multi-scale Feature Learning</h3><p>如Fast R-CNN与Faster R-CNN，仅通过单个特征图预测输出，这对应多尺度和多个纵横比的情况下具有极大难度。</p><p>底层特征图比高层特征图具有更高的分辨率与更小的感受野，因此更适合于检测小物体。而深层的特征图具有更多的语义信息，对光照、偏移等鲁棒性更好，并具有更大的感受野，更适用于检测大体系物体。</p><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig7.png" alt="Fig7"></p><p>有四种解决多尺度特征学习问题的方法</p><ol><li><p>Image Pyramid<br>通过将图像缩放为不同大小的方式，单独训练适应尺寸的网络，测试时将图片缩放为相应尺寸输入到不同的网络中，计算量较大。但是Singh等人提出，学习单个网络以适应不同尺寸比学习多个适应不同尺寸的网络更困难(SNIP)。</p></li><li><p>Integrated Features<br>通过组合多个层的特征图，以新构造的特征图来预测输出，常用的方法如跳跃连接(skip connection)。又如ION，通过ROI Pooling剪裁来自不同层的区域特征，将其组合后作为特征。又如HyperNet，用反卷积的方式，通过集成浅层及中间层的特征图，生成新的高分辨率特征用以输出候选框。又如Multi-scale Location-aware Kernel Representation (MLKP)，捕获高阶统计量，生成更多的判别特征表示，更具描述性，为分类和定位提供语义及空间信息。</p></li><li><p>Prediction Pyramid<br>如SSD，每一层特征图均用于预测输出，每个层设置一定比例的对象。Multi-Scale Deep Convolutional Neural Network(MSCNN)通过反卷积将特征图分辨率增大，用这些特征图预测输出。Reception Field Block Net(RFBNet)设置多分支，每个分支设置不同尺寸的卷积核，从而获得多尺度、不同感受野的特征图，从而用于预测输出。</p></li><li><p>Feature Pyramid<br>结合Integrated Features与Prediction Pyramid的优点。如Feature Pyramid Network(FPN)自上而下将不同尺度的特征通过逐元素相加或者组合的方式，利用深层特征丰富浅层特征的语义信息。</p></li></ol><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig8.png" alt="Fig8"></p><h3 id="3-5-2-Region-Feature-Encoding"><a href="#3-5-2-Region-Feature-Encoding" class="headerlink" title="3.5.2. Region Feature Encoding"></a>3.5.2. Region Feature Encoding</h3><ul><li>R-CNN</li><li>ROI Pooling Layer</li><li>ROI Warping Layer</li><li>ROI Align Layer</li><li>Precise ROI Pooling Layer</li><li>Position Sensitive ROI Pooling Layer</li><li>Feature Selective Network</li><li>CoupleNet</li><li>Deformable ROI Pooling Layer</li></ul><h3 id="3-5-3-Contextual-Reasoning"><a href="#3-5-3-Contextual-Reasoning" class="headerlink" title="3.5.3. Contextual Reasoning"></a>3.5.3. Contextual Reasoning</h3><p>由于物体的出现与环境相关，且需要与其他物体进行交互，故上下文信息十分重要。深度卷积网络隐式地从分层特征表示中捕获上下文信息。</p><ul><li><p>全局上下文<br>从整个图像地上下文中学习，利用图像其余部分信息，来对感兴趣区域进行分类。</p><ul><li>循环神经网络编码整个图像的四个方向信息(ION)；</li><li>学习类别分数，用于作为与检测结果连接的上下文特征(Ouyang et al.);</li><li>从整个图像提取嵌入信息，将其与局部特征组合用以改善检测结果(He et al.)；</li><li>基于语义分割的方法；</li><li>将目标检测和语义分割作为多任务进行优化(He et al. and Dai et al.)；</li><li><u>伪分段</u>语义标注(Zhao et al.)；</li><li>通过学习的方式获得语义特征图(Detection with Enriched Semantics)；</li></ul></li><li><p>局部上下文<br>对周围区域上下文进行编码，并学习对象与周围区域的交互，直接学习不同位置和带有上下文信息的类别是很困难的。</p></li></ul><blockquote><p>Directly modeling different locations and categories objects relations with the contextual is very challenging. Chen et al. proposed <strong>Spatial Memory Network (SMN)</strong> [130] which introduced a spatial memory based module. The spatial memory module captured instance-level contexts by assembling object instances back into a pseudo ”image” representations which were later used for object relations reasoning. Liu et al. proposed <strong>Structure Inference Net (SIN)</strong> [137] which formulated object detection as a graph inference problem by considering scene contextual information and object relationships. In SIN, each object was treated as a graph node and the relationship between different objects were regarded as graph edges. Hu et al. [138] proposed a <strong>lightweight framework relation network</strong> which formulated the interaction between different objects between their appearance and image locations. The new proposed framework did not need additional annotation and showed improvements in object detection performance. Based on Hu et al., Gu et al. [139] proposed a <strong>fully learnable object detector</strong> which proposed a general viewpoint that unified existing region feature extraction methods. Their proposed method removed heuristic choices in ROI pooling methods and automatically select the most significant parts, including contexts beyond proposals. <strong>Another method</strong> to encode contextual information is to implicitly encode region features by adding image features surrounding region proposals and a large number of approaches have been proposed based on this idea [131, 106, 140, 141, 142, 143]. In addition to encode features from region proposals, <strong>Gidaris et al.</strong> [131] extracted features from a number of different sub-regions of the original object proposals(border regions, central regions, contextual regions etc.) and concatenated these features with the original region features. Similar to their method, [106] extracted local contexts by enlarging the proposal window size and concatenating these features with the original ones. Zeng et al. [142] proposed <strong>Gated Bi-Directional CNN (GBDNet)</strong> which extracted features from multi-scale subregions.Notably, GBDNet learned a gated function to control the transmission of different region information because not all contextual information is helpful for detection.</p></blockquote><h3 id="3-5-4-Deformable-Feature-Learning"><a href="#3-5-4-Deformable-Feature-Learning" class="headerlink" title="3.5.4. Deformable Feature Learning"></a>3.5.4. Deformable Feature Learning</h3><p>检测器应对图像中物体的非刚性变形具有鲁棒性。传统方式有Deformable Part based Models(DPMs)，使用可变形编码方法由多个组成表示对象。深度学习方法如DeepIDNet，开发Deformable-aware Pooling Layer;Deformable Convolutional Layers自动学习辅助位置，以增强常规采样的信息。</p><h1 id="4-Learning-Strategy"><a href="#4-Learning-Strategy" class="headerlink" title="4. Learning Strategy"></a>4. Learning Strategy</h1><h2 id="4-1-Training-Stage"><a href="#4-1-Training-Stage" class="headerlink" title="4.1. Training Stage"></a>4.1. Training Stage</h2><h3 id="4-1-1-Data-Augmentation"><a href="#4-1-1-Data-Augmentation" class="headerlink" title="4.1.1. Data Augmentation"></a>4.1.1. Data Augmentation</h3><p>为解决数据量少的问题，需要进行数据扩增。在物体检测中，常用方法有：水平翻转，旋转、随机裁剪、延申、颜色抖动(亮度，对比度，饱和度和色度)。注意图像变换后，需要对标签也作对应变换。</p><h3 id="4-1-2-Imbalance-Sampling"><a href="#4-1-2-Imbalance-Sampling" class="headerlink" title="4.1.2. Imbalance Sampling"></a>4.1.2. Imbalance Sampling</h3><p>候选框内图像大多数都是背景，而不是物体，有以下两个问题：1) 类别不平衡(class imbalance)，由于只有一小部分候选框内内容为物体，故负样本占大多数，导致梯度反传时负样本占主导。2) 困难不平衡(difficulty imbalance)，类似第一点，检测器更易分辨背景，难以分辨物体。</p><p>一些二阶段检测器，如R-CNN与Fast R-CNN会先拒绝大部分负样本，Fast R-CNN从$2000$个候选框中随机采样，直到在某批次的数据中，正负样本比例达到$1:3$。随机采样能解决类别不平衡问题，但是丢失的负样本可能包含丰富的语义信息。为解决这个问题，刘等人提出困难负样本策略(hard negative sampli  Strategy)，主要保留难以判别为负的负样本，具体地说来，就是选取损失值较大的负样本。</p><p>为了解决困难不平衡问题，大部分都是通过合理设置损失函数。对于标检测来说，多类别分类器需要分辨$C+1$个类别，即$C$类物体加背景。假定某区域真实标签为$u$，$p$是网络输出的$C+1$个类别的概率分布($p = \{p_0, \cdots, p_C\}$)，那么损失定义为</p><script type="math/tex; mode=display">L_{cls}(p, u) = - \log p_u</script><p>现阶段有一种改进的交叉熵损失：<a href="https://arxiv.org/abs/1904.09048" target="_blank" rel="noopener">Focal Loss</a>，即</p><script type="math/tex; mode=display">L_{FL} = - \alpha (1 - p_u)^{\gamma} \log p_u</script><p>其中参数$\alpha$与$\gamma$为超参数，该损失函数可根据网络输出的$p_u$计算权值，错误分类样本$p_u$更低，从而使权重更大，可以更多地关注误分类样本。梯度协调机制(gradient harmonizing mechanism - GHM)采用类似的思路，不仅抑制了易分类负样本，并避免了异常负样本的影响。此外，还有如在线困难样本挖掘策略(online hard example mining strategy)，自动选取困难样本用以训练，仅关注样本的困难度而不关注类别信息，即单批正负样本比例没有被考虑，他们认为，对于检测问题，样本困难度比类别不平衡更加重要。</p><h3 id="4-1-3-Localization-Refinement"><a href="#4-1-3-Localization-Refinement" class="headerlink" title="4.1.3. Localization Refinement"></a>4.1.3. Localization Refinement</h3><p>物体检测算法需要提供一个包含物体的最小矩形框，但精确定位困难，因为预测通常集中在物体更具分辨性特征的部位，而不一定是包含物体的区域。在一定场景下，需要进行高质量预测，可用IoU作为指标。一种解决方法是生成高质量的候选框，以下介绍一些其他方法。在R-CNN中使用L2惩罚作为损失，在Fast R-CNN中使用平滑L1惩罚作为损失，即</p><script type="math/tex; mode=display">L_{reg}(t^c, v) = \sum_{i \in \{x, y, w, h\}} \text{SmoothL1}(t^c_i - v_i)</script><script type="math/tex; mode=display">\text{SmoothL1}(x) = \begin{cases} 0.5x^2 & \text{if} |x| < 1 \\ |x| - 0.5 & \end{cases}</script><p>其中每类均具有回归的偏置，即$t^c = (t^c_x, t^c_y, t^c_w, t^c_h)$，真实边界框位置为$v=(v_x, v_y, v_w, v_h)$。</p><p>基于定位校准，一些方法采用辅助模型用于更好地矫正坐标。如Gidaris等人引入一种迭代地边框回归方法，使用R-CNN反复迭代候选框内容，多次矫正候选框；另外，提出LocNet，将每个边界框的分布进行建模。这些方法都需要单独模块，不能进行联合调优。</p><p>一些其他的，侧重于设计带有修改的目标函数(modified objective function)的统一网络框架。在多路网络(Multi-Path Network)中，采用一系列的分类器，这些分类器是通过不同的指标，以整体损失进行优化的，每个分类器都适应对应的IoU阈值，所有输出组成最终的预测结果。Fitness NMS学习了一种新式的IoU计算方法，他们认为现在的检测器旨在找到合格的预测，而不是最优的预测，因此高质量、低质量的候选有同等的重要性。Fitne-IoU更看重高度重合的候选框。他们也采用了基于一组IoU上限，来到处边界框的回归损失，以最大化具有对象的预测的IoU(They also derived a bounding box regression loss based on a set of IoU upper bounds to maximum the IoU of predictions with objects)。Grid R-CNN参照CornerNet与DeNet，用角点定位的机制取代边界框的线性回归。</p><h3 id="4-1-4-Cascade-Learning"><a href="#4-1-4-Cascade-Learning" class="headerlink" title="4.1.4. Cascade Learning"></a>4.1.4. Cascade Learning</h3><p>级联学习是一种粗到细的学习策略，从给定分类器的输出中收集信息，以级联的方式构建更强的分类器，首次被应用于训练人脸检测器。在深度学习算法方面，Cascade Region-proposal-network And FasT-rcnn(CRAFT)学习PRN以及带级联策略的分类器，首先学习一个标准的RPN网络，之后用二分类的Fast R-CNN拒绝一些容易判别的错误样本，剩余样本用来构建包含两个Fast R-CNN网络的级联区域分类器。Yang等人引入层级级联分类器，用于多尺度的物体检测，不同层的特征图设置分类器，浅层的分类器将拒绝易分辨的错误样本，剩余样本将送入更深的网络层。RefineDet和Cascade R-CNN将级联用于回归框的矫正，构建多阶段的边界框回归器，边界框输出将在各个阶段进行矫正，并且这些回归器是通过不同quality的指标训练得到的。Cheng等人通过观察Faster R-CNN的误检图片，注意到即使回归器定位准确，样本分类却存在错误，他们将此归结于特征共享(sharing of features)和联合的多任务优化(joint multi-task optimization)导致的次优特征表示(sub-optimal feature representation)；此外，他们认为Faster R-CNN的较大感受野引入了较多噪声。他们建立了一个基于Faster R-CNN与R-CNN的级联检测系统以互补，即用训练好的Faster R-CNN得到一些初始的预测，这些结果用以训练R-CNN。</p><h3 id="4-1-5-Others"><a href="#4-1-5-Others" class="headerlink" title="4.1.5. Others"></a>4.1.5. Others</h3><p>有一些其他的学习策略</p><ol><li><p>Adversarial Learning</p><p> 对抗性学习显示了生成模型的重要性，最主要的应用是生成对抗网络(GAN)。生成器对数据分布进行建模，根据给定的噪声输入，得到一副假的图片，送入鉴别器判断该图片是否为真。GAN在许多领域显示其有效性，在目标检测方面也不例外。Li等人提出的Perceptual GAN可用于细小物体检测，通过对抗过程，生成器将学习细小物体的高分辨率特征表征。A-Fast-R-CNN通过生成的对抗样本训练，他们认为困难样本是在分布的长尾上，因此他们映入了两个新式模块，自动生成具有遮挡和变形的特征。</p></li><li><p>Training from Scratch</p><p> 现在的检测器大多依赖于ImageNet与训练的分类模型，但是，由于分类和检测任务损失不同、数据分布不同，这可能会对结果造成负面影响。Finetuning可能解决这个问题，但是不能完全消除偏差。另外，将此分类模型用作新的领域可能不合适，如RGB图像到MRI数据。所以有必要从空白开始训练，这样的困难主要是物体检测训练数据过少，会造成过拟合。与图像分类不同，物体检测需要边界框级别的标注，这是非常耗时耗力的(ImageNet有1000个类别，但只有200类包含检测标注)。</p><p> Deeply Supervised Object Detectors(DSOD)的设计者认为，密集连接的网络结构用于监督学习可大大减少优化难度。基于DSOD，Shen等人提出了一种门控循环特征金字塔结构，动态调整中间层的监督强度以自适应不同尺度的物体，将空间和语义信息压缩到单个预测特征图，进一步减少了参数量从而加快了收敛速度。此外，门控特征金字塔结构可根据物体大小适应不同强度的监督，这种方法比原始DSOD更有效。但是随后He等人用从头开始训练的检测器在MSCOCO数据机上进行验证，发现vanilla detector可以获得等同于$10K$标注数据训练得到的检测能力，这表明从头开始训练的模型并不依赖特定结构，<strong>这与之前的工作相矛盾</strong>。</p></li><li><p>Knowledge Distillation</p><p> 通过师生教学方案(teacher-student training scheme)，将多个模型集成为一个模型。Li等人提出轻量级检测器，它的训练通过重量级有效的检测器指导进行训练，该检测器速度更快，精准度与后者相近。Cheng等人提出了一种基于RCNN的快速探测器，优化时，R-CNN模型作为教师网络来指导培训过程，与传统的单模型优化策略相比，他们的框架检测精度提高。</p></li></ol><h2 id="4-2-Testing-Stage"><a href="#4-2-Testing-Stage" class="headerlink" title="4.2. Testing Stage"></a>4.2. Testing Stage</h2><p>目标检测算法生成了众多预测结果，但是由于大量的冗余性，输出结果不能直接使用。因此有其他策略提升检测准确率，或加速推断过程。</p><h3 id="4-2-1-Duplicate-Removal"><a href="#4-2-1-Duplicate-Removal" class="headerlink" title="4.2.1. Duplicate Removal"></a>4.2.1. Duplicate Removal</h3><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Fig10-nms.png" alt="Fig10-nms"></p><p>Non-maximum suppression(NMS)是物体检测算法的一个组成部分，用于消除重复的假阳(false positive)预测，如上图。对于一阶段检测器来说，同一物体周围的预测边界框，可能具有相同的置信度，导致较高的假阳率；而二阶段检测算法产生较为稀疏的候选框，并且回归器会将这些候选框拉向相近的位置，同样会导致较高的假阳率。</p><p>具体说来，对于指定的一类物体，其预测框将按照置信度排序，选择当前置信度最高的候选框，记作$M$，然后计算该框与其余框的IoU，如果IoU值大于某阈值$\Omega_{test}$，该框将被从候选框中删除，即</p><script type="math/tex; mode=display">\text{Score}_B =     \begin{cases}        \text{Score}_B  & \text{IoU}(B, M) < \Omega_{test} \\        0               & \text{otherwise}    \end{cases}</script><p>但是对于聚集的物体，NMS算法将会把同类物体，靠近的物体框删除，这导致回归框预测确实，因此Navaneeth等人提出Soft-NMS算法，使用指定函数将靠近的框的置信度降低，而不是直接置为0</p><script type="math/tex; mode=display">\text{Score}_B =     \begin{cases}        \text{Score}_B              & \text{IoU}(B, M) < \Omega_{test} \\        \text{F}(\text{IoU}(B, M))  & \text{otherwise}    \end{cases}</script><blockquote><p>详细的实现代码可查看<a href="https://louishsu.xyz/2019/05/26/NMS-softer-NMS/" target="_blank" rel="noopener">该文</a></p></blockquote><p>此外，Hosong等人设计了一种网络结构，基于置信度和回归框来改进NMS，这是独立于检测器单独进行有监督训练的。他们认为，重复预测的原因是，检测器故意鼓励每个物体进行多次高分检测而不是奖励单个高分。因此他们根据两个动机来进行网络的设计：1)设置损失来惩罚二次检测，使得每个物体准确预测单个精确预测；2)处理附近的检测输出，给检测器提供是否物体已被多次检测的信息。新提出的模型没有舍弃检测结果，而是将NMS用作重估，降低重复检测结果的评分。</p><h3 id="4-2-2-Model-Acceleration"><a href="#4-2-2-Model-Acceleration" class="headerlink" title="4.2.2. Model Acceleration"></a>4.2.2. Model Acceleration</h3><p>目标检测应用需要较高的实时性，因此需要用指标评估检测器的速度。虽然当前state-of-art算法可以在公开数据集上得到一个较高的检测评价，但是他们的速度限制了他应用在实时场景中。通常来说，二阶段检测器比一阶段检测器速度更慢，因为候选框生成器与分类器单独运算，使得运算量较大。尽管R-FCN提出空间敏感的特征图，通过position-sensitive ROI Pooling来共享计算，但是随着物体种类增多，通道数也随之线性增加。</p><p>从检测器的主体结构可以看到，占计算量最多的主要是网络模型部分，因此一个简单的解决方法是用高效率的主体，比如MobileNet，通过分离卷积的方式，大大减少了参数量与计算量。PVANet是一种新的网络结构，他用到了CReLU，较少了非线性计算。</p><script type="math/tex; mode=display">\text{CReLU}(x) = [\text{ReLU}(x), \text{ReLU}(-x)]</script><p>另一种方法是，将模型进行线下优化，比如模型压缩。最后呢，如NVIDIA开发了一个加速工具TensorRT，在模型部署上进行优化，以加运算速度。</p><h3 id="4-2-3-Others"><a href="#4-2-3-Others" class="headerlink" title="4.2.3. Others"></a>4.2.3. Others</h3><p>其他方法，如将输入图像进行变换，以提高检测精度。图像金字塔是将待检测图像，以一定的比例生成一系列尺寸的图像，在每个图像上进行检测，最终结果合并于每张检测结果。Zhang等人，将图像缩放到指定尺寸，在某一特定尺寸下，仅检测某一指定大小的物体。水平翻转也可用于测试阶段。这些方法都可以增加准确率但是计算量也随之增加。</p><h1 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5. Applications"></a>5. Applications</h1><h2 id="5-1-Face-Detection"><a href="#5-1-Face-Detection" class="headerlink" title="5.1. Face Detection"></a>5.1. Face Detection</h2><p>人脸检测是一个经典的计算机视觉问题，通常是人脸验证、对其、识别等算法的第一步。然而，人脸检测问题与通用检测之间存在关键差异：1)人脸检测中，目标的比例范围远大于通用物体；2)脸部对象包括强结构信息，并且仅有一类物体。考虑到这些特效，需要有一些先验来改进检测算法。</p><h2 id="5-2-Pedestrian-Detection"><a href="#5-2-Pedestrian-Detection" class="headerlink" title="5.2. Pedestrian Detection"></a>5.2. Pedestrian Detection</h2><p>行人检测是一个关键的重要任务。和通用检测不同，1)结构良好，具有几乎固定的纵横比，但也存在于较大范围内；2)行人检测是真实应用场景，有许多挑战：人群拥挤、遮挡、图像模糊等。</p><h2 id="5-3-Others"><a href="#5-3-Others" class="headerlink" title="5.3. Others"></a>5.3. Others</h2><p>其他应用如Logo检测和视频目标检测。</p><h1 id="6-Detection-Benchmarks"><a href="#6-Detection-Benchmarks" class="headerlink" title="6. Detection Benchmarks"></a>6. Detection Benchmarks</h1><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab1.png" alt="Tab1"></p><h2 id="6-1-Generic-Detection-Benchmarks"><a href="#6-1-Generic-Detection-Benchmarks" class="headerlink" title="6.1. Generic Detection Benchmarks"></a>6.1. Generic Detection Benchmarks</h2><ul><li>Pascal VOC2007</li><li>Pascal VOC2012</li><li>MSCOCO</li><li>Open Images</li><li>LVIS</li><li>ImageNet</li></ul><h2 id="6-2-Face-Detection-Benchmarks"><a href="#6-2-Face-Detection-Benchmarks" class="headerlink" title="6.2. Face Detection Benchmarks"></a>6.2. Face Detection Benchmarks</h2><ul><li>WIDER FACE</li><li>FDDB</li><li>PASCAL FACE</li></ul><h2 id="6-3-Pedestrian-Detection-Benchmarks"><a href="#6-3-Pedestrian-Detection-Benchmarks" class="headerlink" title="6.3. Pedestrian Detection Benchmarks"></a>6.3. Pedestrian Detection Benchmarks</h2><ul><li>CityPersons</li><li>Caltech</li><li>ETH</li><li>INRIA</li><li>KITTI</li></ul><h1 id="7-State-of-the-art-for-Generic-Object-Detection"><a href="#7-State-of-the-art-for-Generic-Object-Detection" class="headerlink" title="7. State-of-the-art for Generic Object Detection"></a>7. State-of-the-art for Generic Object Detection</h1><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab2.png" alt="Tab2"></p><p><img src="/2019/08/31/解读-Recent-Advances-in-Deep-Learning-for-Object-Detection/Tab3.png" alt="Tab3"></p><h1 id="8-Concluding-Remarks-and-Future-Directions"><a href="#8-Concluding-Remarks-and-Future-Directions" class="headerlink" title="8. Concluding Remarks and Future Directions"></a>8. Concluding Remarks and Future Directions</h1><p>目标检测还存在许多挑战和未来的发展方向</p><ol><li><p>候选框生成策略</p><p> 如<a href="#34-proposal-generation">3.4.节</a>中所述，当前许多检测器是基于anchor的，这些anchor主要是手动设计的，难以匹配多尺度物体，基于IoU的匹配策略也是启发式的。而对于无anchor算法，具有大的改进空间，如计算成本告等。当前无锚框算法是一个热点。</p></li><li><p>有效的上下文信息编码</p><p> 上下文信息对物体检测十分重要，但是目前所作的工作对上下文信息的使用比较局限。</p></li><li><p>基于AutoML的检测方法</p><p> 给一个特定任务设计合适的网络结构是很重要的，但是也会消耗大量时间和人力。当前一个比较有趣和重要的研究方向是，通过学习的方法，自动设计网络结构。可通过AutoML方法进行网络结构的探索，但是这种算法需要大量的计算资源(more than 100 GPU cards to train a single model)。</p></li><li><p>目标检测新的数据集</p><p> 当前MSCOCO是目标检测最常用的数据库，但是它只包含80类物体，在真实世界中，这是远远不够的。最近有一个数据集LVIS旨在收集更多类别物体的图像数据，它包含超过1000个类别的16400张图片，其中还有许多高质量的语义分割标注。此外，它模拟真实世界的场景，其中存在大量类别但是有些类别数据很少。</p></li><li><p>Low-shot目标检测</p><p> 有限标记数据训练得到的模型，称作Low-shot。对于边界框级别的图像标注非常耗时耗力，现在有一些通过半监督学习的方法，来减少数据的使用。</p></li><li><p>检测任务的骨干架构</p><p> 当前检测器很多都基于分类模型。</p></li><li><p>其余研究问题</p><p> large batch learning和增量学习(incremental learning)等。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Batch Normalization</title>
      <link href="/2019/08/27/Batch-Normalization/"/>
      <url>/2019/08/27/Batch-Normalization/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>深度学习网络可通过mini-batch随机梯度下降的方式进行优化，然而在优化过程中，存在梯度消失、收敛不稳定的问题。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>机器学习中，假定训练数据与测试数据独立同分布(independent and identical independent)，当输入数据进行白化操作后，可使模型收敛速度加快，这被称作<code>Covariate Shift</code>。</p><p>而在神经网络内部，各层的特征图数据分布不均匀，称为<code>Internal Covariate Shift</code>，有以下几点需求需要进行内部白化</p><ol><li>为防止网络发散，网络参数初始化一般服从均值为$0$的正态分布；</li><li>激活函数敏感区域一般定义在零点位置，如<code>logistic</code>函数，在零点出该处梯度最大，而其他位置容易出现梯度消失问题；</li></ol><p><img src="/2019/08/27/Batch-Normalization/emoji.jpg" alt="emoji"></p><p>内部白化后，输出数据近似服从标准正态分布。一般用于卷积层后、激活函数前。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>Batch Normalization层通常包含参数<code>mean</code>, <code>variance</code>, <code>rolling_mean</code>, <code>rolling_variance</code>，其中<code>mean</code>与<code>variance</code>为训练时根据批次数据计算得到，在训练阶段使用以归一化批次数据；<code>rolling_mean</code>与<code>rolling_variance</code>为该层实际学习的参数，在测试阶段使用以归一化批次数据。</p><p>对于某批次输入的数据$X_{\mathcal{B}}$，尺寸为$(N, H, W, C)$，计算该批次数据每个维度上的均值<code>mean</code>与方差<code>variance</code>，尺寸均为$(H, W, C)$，即</p><script type="math/tex; mode=display">\mu = \frac{1}{N} \sum_{i=1}^N X^{(i)}</script><script type="math/tex; mode=display">\sigma^2 = \frac{1}{N} \sum_{i=1}^N (X^{(i)} - \mu)^2</script><p>更新参数<code>rolling_mean</code>与<code>rolling_variance</code>，以动量方式更新，可取$\gamma=0.01$</p><script type="math/tex; mode=display">\tilde{\mu} := \gamma \cdot \mu + (1 - \gamma) \cdot \tilde{\mu}</script><script type="math/tex; mode=display">\tilde{\sigma^2} := \gamma \cdot \sigma^2 + (1 - \gamma) \cdot \tilde{\sigma^2}</script><p>值得注意的是，在训练阶段前向计算时</p><script type="math/tex; mode=display">\hat{X^{(i)}} = \frac{X^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}</script><p>而测试阶段</p><script type="math/tex; mode=display">\hat{X^{(i)}} = \frac{X^{(i)} - \tilde{\mu}}{\sqrt{\tilde{\sigma^2} + \epsilon}}</script><p>以下为<a href="https://github.com/pjreddie/darknet/blob/master/src/batchnorm_layer.c" target="_blank" rel="noopener">darknet/src/batchnorm_layer.c</a>前向与反向传播计算的具体实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_batchnorm_layer</span><span class="params">(layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l.type == BATCHNORM) copy_cpu(l.outputs*l.batch, net.input, <span class="number">1</span>, l.output, <span class="number">1</span>);</span><br><span class="line">    copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span>(net.train)&#123;</span><br><span class="line">        mean_cpu(l.output, l.batch, l.out_c, l.out_h*l.out_w, l.mean);</span><br><span class="line">        variance_cpu(l.output, l.mean, l.batch, l.out_c, l.out_h*l.out_w, l.variance);</span><br><span class="line"></span><br><span class="line">        scal_cpu(l.out_c, <span class="number">.99</span>, l.rolling_mean, <span class="number">1</span>);</span><br><span class="line">        axpy_cpu(l.out_c, <span class="number">.01</span>, l.mean, <span class="number">1</span>, l.rolling_mean, <span class="number">1</span>);</span><br><span class="line">        scal_cpu(l.out_c, <span class="number">.99</span>, l.rolling_variance, <span class="number">1</span>);</span><br><span class="line">        axpy_cpu(l.out_c, <span class="number">.01</span>, l.variance, <span class="number">1</span>, l.rolling_variance, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        normalize_cpu(l.output, l.mean, l.variance, l.batch, l.out_c, l.out_h*l.out_w);   </span><br><span class="line">        copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x_norm, <span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        normalize_cpu(l.output, l.rolling_mean, l.rolling_variance, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    &#125;</span><br><span class="line">    scale_bias(l.output, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">    add_bias(l.output, l.biases, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_batchnorm_layer</span><span class="params">(layer l, network net)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(!net.train)&#123;</span><br><span class="line">        l.mean = l.rolling_mean;</span><br><span class="line">        l.variance = l.rolling_variance;</span><br><span class="line">    &#125;</span><br><span class="line">    backward_bias(l.bias_updates, l.delta, l.batch, l.out_c, l.out_w*l.out_h);</span><br><span class="line">    backward_scale_cpu(l.x_norm, l.delta, l.batch, l.out_c, l.out_w*l.out_h, l.scale_updates);</span><br><span class="line"></span><br><span class="line">    scale_bias(l.delta, l.scales, l.batch, l.out_c, l.out_h*l.out_w);</span><br><span class="line"></span><br><span class="line">    mean_delta_cpu(l.delta, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.mean_delta);</span><br><span class="line">    variance_delta_cpu(l.x, l.delta, l.mean, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.variance_delta);</span><br><span class="line">    normalize_delta_cpu(l.x, l.mean, l.variance, l.mean_delta, l.variance_delta, l.batch, l.out_c, l.out_w*l.out_h, l.delta);</span><br><span class="line">    <span class="keyword">if</span>(l.type == BATCHNORM) copy_cpu(l.outputs*l.batch, l.delta, <span class="number">1</span>, net.delta, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - arXiv.org</a></li><li><a href="https://arxiv.org/abs/1805.11604" target="_blank" rel="noopener">How Does Batch Normalization Help Optimization? - arXiv.org</a></li><li><a href="https://github.com/pjreddie/darknet/blob/master/src/batchnorm_layer.c" target="_blank" rel="noopener">darknet/src/batchnorm_layer.c - Github</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Module Layer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Eigenface and Fisherface</title>
      <link href="/2019/08/20/Eigenface-and-Fisherface/"/>
      <url>/2019/08/20/Eigenface-and-Fisherface/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在众多人脸图像中，能否找到特一组特征脸，用于表征其他人脸呢？在<a href="https://louishsu.xyz/2018/10/22/PCA/" target="_blank" rel="noopener">PCA</a>和<a href="https://louishsu.xyz/2019/04/22/LDA/" target="_blank" rel="noopener">LDA</a>中分别介绍了两种线性降维方法，本文介绍一种使用以上两种算法的特征提取方法。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="Eigenface"><a href="#Eigenface" class="headerlink" title="Eigenface"></a>Eigenface</h2><p><code>Eigenface</code>由Sirovich与Kirby在1987年提出，认为人脸图像可由一系列特征图加权组合重构而成，即</p><script type="math/tex; mode=display">F = F_m + \sum_i w_i F_i \tag{1}</script><p><code>Eigenface</code>可通过<code>PCA</code>生成，在大量的人脸数据图像上进行分析，选取主分量作为特征脸，详细步骤如下</p><ol><li><p>假设有图像尺寸为$(H, W)$的人脸数据，共$N$张。将单张的灰度图片展开成为维数为$H \times W$的向量$x^{(i)}$，构成数据矩阵$X$</p><script type="math/tex; mode=display">X_{(H \times W) \times N} = \begin{bmatrix} x^{(1)} & x^{(2)} & \ldots & x^{(N)} \end{bmatrix} \tag{2.1}</script></li><li><p>计算各样本向量的均值$\mu$，将各样本去均值化，生成数据矩阵$\overline{X}$</p><script type="math/tex; mode=display">\mu = \frac{1}{N} \sum_{i=1}^{N} x^{(i)}; \quad \overline{x}^{(i)} = x^{(i)} - \mu \tag{2.2}</script></li><li><p>计算协方差矩阵$C$</p><script type="math/tex; mode=display">C = \frac{1}{N} \overline{X} \cdot \overline{X}^T \tag{2.3}</script></li><li><p>将协方差矩阵进行特征值分解</p><script type="math/tex; mode=display">C \alpha_i = \lambda_i \alpha_i \tag{2.4}</script><blockquote><p>注意到，在图像尺寸为$(H, W)$的情况下，协方差矩阵的尺寸为$(H \times W, H \times W)$，本文使用数据库内图像为$112 \times 92$，存储为浮点类型<code>(4 byte)</code>，也就是说，该协方差矩阵所占内存</p><script type="math/tex; mode=display">(112 \times 92)^2 \text{pixel} \times 4 \text{byte/pixel} = 441 \rm{kb} \text{(阵亡。。。)}</script><p>因此，需要转换一下求解问题，查看<a href="#eigenface%e6%b1%82%e8%a7%a3">Eigenface求解</a>。</p></blockquote></li><li><p>按特征值降序，重新排列特征对$(\lambda_i, \alpha_i)$，选取前$K$个特征向量作为<code>Eigenface</code></p><script type="math/tex; mode=display">E_{(H \times W) \times K} = \begin{bmatrix} \alpha_1 & \alpha_2 & \ldots & \alpha_K \end{bmatrix} \tag{2.5}</script></li></ol><p>此时将人脸数据$x$投影到各主分量上，可获得相应系数，该系数向量可用于表征该人脸的特征，即</p><script type="math/tex; mode=display">\vec{w} = \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix} \tag{3}</script><p>其中$w_i = x^T \alpha_i$。</p><p>将各主轴恢复原图像尺寸后，其可视化输出如下<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_30_0.png" alt="output_30_0"></p><h2 id="FisherFace"><a href="#FisherFace" class="headerlink" title="FisherFace"></a>FisherFace</h2><p><code>Fisherface</code>基本思路与<code>Eigenface</code>一致，也是寻找一组特征脸，用于表征人脸特征。可通过<code>LDA</code>生成，<code>LDA</code>考虑类内与类间散布，与<code>PCA</code>不同，为有监督学习。详细步骤如下</p><ol><li><p>同样的，获取数据矩阵$X$</p><script type="math/tex; mode=display">X_{(H \times W) \times N} = \begin{bmatrix} x^{(1)} & x^{(2)} & \ldots & x^{(N)} \end{bmatrix} \tag{4.1}</script></li><li><p>计算所有数据的均值向量$\mu$与各类别的均值向量$\mu_j$</p><script type="math/tex; mode=display">\mu = \frac{1}{N} \sum_{i=1}^{N} x^{(i)}; \quad \mu_j = \frac{1}{N_j} \sum_{i=1}^{N_j} x^{(i)}, x^{(i)} \in C_j \tag{4.2}</script></li><li><p>计算类内离散度矩阵$S_W$与类间离散度矩阵$S_B$</p><script type="math/tex; mode=display"> \begin{aligned}  S_W = \sum_{j=1}^{C} \frac{N_j}{N} \left[ \frac{1}{N_j} \sum_{i=1}^{N_j} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T \right] \\ = \frac{1}{N} \sum_{j=1}^{C} \sum_{i=1}^{N_j} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T; \quad x^{(i)} \in C_j \end{aligned} \tag{4.3}</script><script type="math/tex; mode=display">S_B = \sum_{j=1}^{C} \frac{N_j}{N} (\mu_j - \mu) (\mu_j - \mu)^T \tag{4.4}</script></li><li><p>求解广义特征值问题</p><script type="math/tex; mode=display">S_B \alpha_i = \lambda_i S_W \alpha_i \tag{4.5}</script><blockquote><p>通常该问题可通过分解矩阵$S_W^{-1} S_B$进行求解</p><script type="math/tex; mode=display">S_W^{-1} S_B \alpha_i = \lambda_i \alpha_i</script><p>但由于函数<code>numpy.linalg.eig(a)</code>问题，在求解$S_W^{-1} S_B$特征对时出现复数。更加令人疑惑的是，作为实对称矩阵$S_W^{-1}$，其特征对用该函数求解时，也会出现复数。实际上，<code>numpy</code>提供了函数<code>numpy.linalg.eigh(a)</code>专门求解实对称矩阵或<code>Hermite</code>矩阵的特征对，故需要对该特征求解进行一定处理，查看<a href="#%e5%b9%bf%e4%b9%89%e7%89%b9%e5%be%81%e5%80%bc%e9%97%ae%e9%a2%98sb-alphai--lambdai-sw-alphai">广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$</a>。<br><strong>性质1：</strong> 实对称矩阵(满足$A^T = A$)的特征值都是实数。<br><strong>性质2：</strong> 实对称矩阵(满足$A^T = A$)属于不同特征值的特征向量正交。</p></blockquote></li><li><p>按特征值降序，重新排列特征对$(\lambda_i, \alpha_i)$，选取前$K$个特征向量作为<code>FisherFace</code></p><script type="math/tex; mode=display">F_{(H \times W) \times K} = \begin{bmatrix} \alpha_1 & \alpha_2 & \ldots & \alpha_K \end{bmatrix} \tag{4.6}</script></li></ol><p>类似的，将人脸数据$x$投影到各主分量上，可获得相应系数，该系数向量可用于表征该人脸的特征，即</p><script type="math/tex; mode=display">\vec{w} = \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix} \tag{5}</script><p>其中$w_i = x^T \alpha_i$。</p><p>将各主轴恢复原图像尺寸后，其可视化输出如下<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_33_0.png" alt="output_33_0"></p><p>实际上，若输入的数据矩阵$X$不做降维处理，计算得到矩阵$S_W$与$S_B$尺寸为$(H \times W) \times (H \times W)$，也是一个令人头疼的计算量问题。而与<a href="#eigenface">上面</a>不同，这是无法避免的。所以考虑到这一点，需要对原始数据进行降维，可利用<code>PCA</code></p><script type="math/tex; mode=display">\tilde{X}_{D \times N} = \text{pca}(X_{(H \times W) \times N}) \tag{6.1}</script><p>对矩阵$\tilde{X}$进行<code>LDA</code>计算后，得到<code>Fisherface</code>序列$\{\alpha_1, \alpha_2, \ldots\}$，其中$\alpha_i$维度为$D \times 1$，若需可视化结果，利用计算得到的<code>PCA</code>模型将其重建即可</p><script type="math/tex; mode=display">A_i = \text{pca}^{-1}(\alpha_i) \tag{6.2}</script><h1 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h1><h2 id="Eigenface求解"><a href="#Eigenface求解" class="headerlink" title="Eigenface求解"></a>Eigenface求解</h2><p>矩阵$\overline{X}$可经$SVD$分解为</p><script type="math/tex; mode=display">\overline{X} = U \Sigma V^T \tag{7}</script><p>对于协方差矩阵$C_{(H \times W) \times (H \times W)} = \frac{1}{N} \overline{X} \cdot \overline{X}^T$，对其进行特征值分解如下</p><script type="math/tex; mode=display">\overline{X} \cdot \overline{X}^T u_i = \lambda_i u_i \tag{8}</script><p>实际上，$(8)$为求解矩阵$\overline{X}$左奇异向量$u_i, i = 1, 2, \ldots, H \times W$的过程，然而其计算量过大，考虑矩阵$\overline{X}$右奇异向量$v_i, i = 1, 2, \ldots, N$。</p><script type="math/tex; mode=display">\overline{X}^T \overline{X} v_i = \lambda_i v_i \tag{9}</script><p>由$(7)$可得</p><script type="math/tex; mode=display">\begin{aligned}    \overline{X} V = U \Sigma & 或 & \overline{X} v_i = \sigma_i u_i \end{aligned}</script><p>所以</p><script type="math/tex; mode=display">\begin{aligned}    u_i = \frac{1}{\sigma_i} \overline{X} v_i & 或 & U = \overline{X} V \Sigma^{-1}\end{aligned} \tag{10}</script><p>其中$V$与$\Lambda$由式$(9)$已知，奇异值$\sigma_i = \sqrt{\lambda_i}$，故</p><script type="math/tex; mode=display">\begin{aligned}    u_i = \frac{1}{\sqrt{\lambda_i}} \overline{X} v_i & 或 & U = \overline{X} V \Lambda^{-\frac{1}{2}}\end{aligned} \tag{11}</script><p>实际上，由于$\text{rank}(\overline{X}) \leq N$，故后$(H \times W) - N$个特征值均为$0$，对应特征向量无意义，不予求解。</p><h2 id="广义特征值问题-S-B-alpha-i-lambda-i-S-W-alpha-i"><a href="#广义特征值问题-S-B-alpha-i-lambda-i-S-W-alpha-i" class="headerlink" title="广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$"></a>广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$</h2><p>由于函数<code>numpy.linalg.eig(a)</code>问题，在求解$S_W^{-1} S_B$特征对时出现复数。故作如下处理</p><p>$S_W$为实对称矩阵，故可使用函数<code>numpy.linalg.eigh(a)</code>解得其特征对，即</p><script type="math/tex; mode=display">S_W = P \Lambda P^T \tag{12}</script><p>代入$S_W^{-1} S_B \alpha_i = \lambda_i \alpha_i$并作相应变换</p><script type="math/tex; mode=display">(P \Lambda P^T)^{-1} S_B \alpha_i = \lambda_i \alpha_i</script><script type="math/tex; mode=display">\underbrace{P \Lambda^{-\frac{1}{2}} \Lambda^{-\frac{1}{2}} P^T}_{S_W} \cdot S_B \cdot \underbrace{P \Lambda^{-\frac{1}{2}} \Lambda^{\frac{1}{2}} P^T}_I \cdot \alpha_i = \lambda_i \alpha_i</script><script type="math/tex; mode=display">\underbrace{\Lambda^{-\frac{1}{2}} P^T \cdot S_B \cdot P \Lambda^{-\frac{1}{2}}}_{A} \underbrace{\Lambda^{\frac{1}{2}} P^T \cdot \alpha_i}_{\beta_i} = \lambda_i \underbrace{\Lambda^{\frac{1}{2}} P^T \cdot \alpha_i}_{\beta_i} \tag{13}</script><p>其中$A = \Lambda^{-\frac{1}{2}} P^T \cdot S_B \cdot P \Lambda^{-\frac{1}{2}}$也为对称矩阵，可由用函数<code>numpy.linalg.eigh(a)</code>解得其特征对$(\lambda_i, \beta_i)$，则</p><script type="math/tex; mode=display">\beta_i = \Lambda^{\frac{1}{2}} P^T \cdot \alpha_i</script><script type="math/tex; mode=display">\alpha_i = P \Lambda^{-\frac{1}{2}} \beta_i \tag{14}</script><h1 id="实现及实验"><a href="#实现及实验" class="headerlink" title="实现及实验"></a>实现及实验</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>实验中数据集选用<a href="https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" target="_blank" rel="noopener">ORL Face</a>，包含40名人员的10份人脸数据，包括拍摄时间、光照、表情(睁/闭眼、笑/不笑)、面部细节(眼镜)的变化。图片保存为<code>.pgm</code>格式，可用<code>OpenCV</code>进行读取。</p><p>其数据预览如下</p><p><img src="/2019/08/20/Eigenface-and-Fisherface/faces.gif" alt="preview"></p><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Principal Components Analysis</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        components_: &#123;ndarray(n_components, n_features)&#125;</span></span><br><span class="line"><span class="string">        means_:      &#123;ndarray(n_components)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.components_  = <span class="keyword">None</span></span><br><span class="line">        self.means_       = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">''' train the model</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">        self.means_ = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_ = X - self.means_</span><br><span class="line">        eigval, eigvec = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> n_samples &lt; n_features:</span><br><span class="line">            eigval, u = np.linalg.eig(X_.dot(X_.T))</span><br><span class="line">            eigvec = X_.T.dot(u).dot(np.diag(<span class="number">1</span> / eigval))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            covar_ = X_.T.dot(X_)</span><br><span class="line">            eigval, eigvec = np.linalg.eig(covar_)</span><br><span class="line"></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>]</span><br><span class="line">        eigval = eigval[order]</span><br><span class="line">        eigvec = eigvec.T[order].T</span><br><span class="line"></span><br><span class="line">        self.components_ = eigvec[:, :self.n_components].T</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X_:&#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            X'_&#123;nxk'&#125; · V_&#123;kxk'&#125;^T = X''_&#123;nxk&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X - self.means_</span><br><span class="line">        X_ = X_.dot(self.components_.T)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        self.fit(X)</span><br><span class="line">        <span class="keyword">return</span> self.transform(X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X_:&#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_) + self.means_</span><br><span class="line">        <span class="keyword">return</span> X_</span><br></pre></td></tr></table></figure><h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eig</span><span class="params">(A1, A2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A1, A2: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        eigval: &#123;ndarray(n)&#125;</span></span><br><span class="line"><span class="string">        eigvec: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        A1 \alpha = \lambda A2 \alpha</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s, u = np.linalg.eigh(A2)</span><br><span class="line">    s[s &lt;= <span class="number">0</span>] = np.finfo(float).eps</span><br><span class="line">    s_sqrt = np.diag(np.sqrt(s))</span><br><span class="line">    s_sqrt_inv = np.linalg.inv(s_sqrt)</span><br><span class="line"></span><br><span class="line">    A = s_sqrt_inv.dot(u.T).dot(A1).dot(u).dot(s_sqrt_inv)</span><br><span class="line">    eigval, P = np.linalg.eigh(A)</span><br><span class="line">    eigvec = u.dot(s_sqrt_inv).dot(P)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> eigval, eigvec</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LDA</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        components_:  &#123;ndarray(n_components, n_features)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.components_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">""" train the model</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:      &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">            y:      &#123;ndarray(n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        labels = list(set(list(y)))</span><br><span class="line">        n_class = len(labels)</span><br><span class="line">        n_samples, n_feats = X.shape</span><br><span class="line"></span><br><span class="line">        S_W = np.zeros(shape=(n_feats, n_feats))</span><br><span class="line">        S_B = np.zeros(shape=(n_feats, n_feats))</span><br><span class="line">        mean_ = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> i_class <span class="keyword">in</span> range(n_class):</span><br><span class="line">            X_ = X[y==labels[i_class]]</span><br><span class="line">            means_ = np.mean(X_, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            X_ = X_ - means_</span><br><span class="line">            means_ = (means_ - mean_).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            S_W += (X_.T).dot(X_) * (<span class="number">1</span> / n_samples)</span><br><span class="line">            S_B += (means_.T).dot(means_) * (X_.shape[<span class="number">0</span>] / n_samples)</span><br><span class="line"></span><br><span class="line">        eigval, eigvec = eig(S_B, S_W)</span><br><span class="line"></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>]</span><br><span class="line">        eigval = eigval[order]</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line"></span><br><span class="line">        self.components_ = eigvec[:, :self.n_components].T</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_.T)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.fit(X, y)</span><br><span class="line">        X_ = self.transform(X)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_features</span><span class="params">(X, dsize, title=<span class="string">"features"</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Show features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(N, n_features)&#125;</span></span><br><span class="line"><span class="string">        dsize: &#123;tuple(H, W)&#125;</span></span><br><span class="line"><span class="string">        title: &#123;str&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    SUBPLOT = <span class="string">"19&#123;&#125;"</span></span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">2</span>))</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">        plt.subplot(int(SUBPLOT.format(i+<span class="number">1</span>)))</span><br><span class="line">        plt.imshow(X[i].reshape(dsize), cmap=<span class="string">"gray"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ol><li><p>Eigenface</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=n_features); pca.fit(X)</span><br><span class="line">show_features(pca.components_, DSIZE, title=<span class="string">"Eigenface &#123;&#125;"</span>.format(repr(DSIZE)))</span><br></pre></td></tr></table></figure><p> <img src="/2019/08/20/Eigenface-and-Fisherface/output_30_0.png" alt="output_30_0"></p></li><li><p>FisherFace</p><p> 为减少计算量，将原始数据<code>PCA</code>降维后，再用于<code>LDA</code>计算<code>FisherFace</code>，故维度数目选取会影响实验结果，以下分别选择不同维数时产生的<code>FisherFace</code>序列，显示前9个特征脸。可见主分量数目越少，特征脸越清晰，但包含的细节也越少。</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n_decomposed = n_samples - n_classes - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=n_decomposed)</span><br><span class="line">X_decomposed = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line">lda = LDA(n_components=n_classes - <span class="number">1</span>)</span><br><span class="line">lda.fit(X_decomposed, y)</span><br><span class="line"></span><br><span class="line">components_ = pca.transform_inv(lda.components_)</span><br><span class="line">show_features(components_, DSIZE, title=<span class="string">"Fisherface &#123;&#125; &#123;&#125;"</span>.format(repr(DSIZE), n_decomposed))</span><br></pre></td></tr></table></figure><ul><li>199<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_33_0.png" alt="output_33_0"></li><li>279<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_33_2.png" alt="output_33_2"></li><li>359<br><img src="/2019/08/20/Eigenface-and-Fisherface/output_33_4.png" alt="output_33_4"></li></ul></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" target="_blank" rel="noopener">The Database of Faces</a></li><li><a href="https://www.xuebuyuan.com/3231919.html" target="_blank" rel="noopener">人脸识别之—-FisherFace - 学步园</a></li><li><a href="https://bytefish.de/pdf/facerec_python.pdf" target="_blank" rel="noopener">Face Recognition with Python, by Philipp Wagner</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征提取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neighborhood Preserving Embedding</title>
      <link href="/2019/08/12/Neighborhood-Preserving-Embedding/"/>
      <url>/2019/08/12/Neighborhood-Preserving-Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在<a href="https://louishsu.xyz/2019/08/09/Locally-Linear-Embedding/" target="_blank" rel="noopener">Locally Linear Embedding</a>一节中介绍了非线性降维方法<code>LLE</code>，原数据到低维数据没有指定映射方法，故不适用于新数据点。本文介绍的<code>NPE</code>是在其基础上的改进。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>设有$M$个$N$维数据，构成矩阵$X$</p><script type="math/tex; mode=display">X_{N \times M} = \left[ \begin{matrix}    |             & |             &        & |              \\    \vec{x}^{(1)} & \vec{x}^{(2)} & \cdots & \vec{x}^{(M)}  \\    |             & |             &        & |\end{matrix} \right] \tag{1.1}</script><p>其中</p><script type="math/tex; mode=display">\vec{x}^{(i)}_{N \times 1} = \left[ \begin{matrix}    \vec{x}^{(i)}_1 & \vec{x}^{(i)}_2 & \cdots & \vec{x}^{(i)}_N \end{matrix} \right] ^T \tag{1.2}</script><h2 id="高维到低维的映射"><a href="#高维到低维的映射" class="headerlink" title="高维到低维的映射"></a>高维到低维的映射</h2><p>在<code>LLE</code>基础上，将数据的映射方法指定为</p><script type="math/tex; mode=display">\vec{y}^{(i)} = P^T · \vec{x}^{(i)} \tag{*1}</script><blockquote><script type="math/tex; mode=display">\vec{y}^{(i)}_j = \vec{p}_j^T · \vec{x}^{(i)}</script></blockquote><p>其中</p><script type="math/tex; mode=display">P_{N \times D} = \left[ \begin{matrix}    |         & |         &        & |          \\    \vec{p}_1 & \vec{p}_2 & \cdots & \vec{p}_D  \\    |         & |         &        & |\end{matrix} \right] \tag{2.1}</script><script type="math/tex; mode=display">\vec{p}_{i_{N \times 1}} = \left[ \begin{matrix}    \vec{p}_{i1} & \vec{p}_{i2} & \cdots & \vec{p}_{iN}\end{matrix} \right] ^T \tag{2.2}</script><script type="math/tex; mode=display">\vec{y}^{(i)}_{D \times 1} = \left[ \begin{matrix}    \vec{y}^{(i)}_1 & \vec{y}^{(i)}_2 & \cdots & \vec{y}^{(i)}_D \end{matrix} \right] ^T \tag{2.3}</script><h2 id="高维空间的空间结构特征"><a href="#高维空间的空间结构特征" class="headerlink" title="高维空间的空间结构特征"></a>高维空间的空间结构特征</h2><p>与<a href="https://louishsu.xyz/2019/08/09/Locally-Linear-Embedding/" target="_blank" rel="noopener">Locally Linear Embedding</a>一致，通过矩阵$\dot{W}$保存空间结构特征</p><script type="math/tex; mode=display">J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2 \tag{3.1}</script><script type="math/tex; mode=display">\text{s.t.} \quad \sum_{j=1}^K w_{ij} = 1 \quad \text{or} \quad \vec{w}_i^T \vec{1} = 1 \tag{3.2}</script><p>解得</p><script type="math/tex; mode=display">\vec{w}_{i_{K \times 1}} = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}</script><p>其中</p><script type="math/tex; mode=display">Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)})</script><script type="math/tex; mode=display">X^{(i)} = \left[ \begin{matrix}    |             & |             &        & |              \\    \vec{x}^{(i)} & \vec{x}^{(i)} & \cdots & \vec{x}^{(i)}  \\    |             & |             &        & |\end{matrix} \right]</script><script type="math/tex; mode=display">N^{(i)} = \left[ \begin{matrix}    |               & |               &        & |               \\    \vec{x}^{(1)}_N & \vec{x}^{(2)}_N & \cdots & \vec{x}^{(K)}_N \\    |               & |               &        & |\end{matrix} \right]</script><p>解得矩阵</p><script type="math/tex; mode=display">W_{K \times M} = \left[ \begin{matrix}    |         & |         &        & |          \\    \vec{w}_1 & \vec{w}_2 & \cdots & \vec{w}_M  \\    |         & |         &        & |\end{matrix} \right]</script><h2 id="低维空间保持同样的空间结构"><a href="#低维空间保持同样的空间结构" class="headerlink" title="低维空间保持同样的空间结构"></a>低维空间保持同样的空间结构</h2><p>在低维空间中，损失定义为</p><script type="math/tex; mode=display">J(Y) = \sum_{i} || \vec{y}^{(i)} - \sum_j w_{ij} \vec{y}^{(j)} ||_2^2 \tag{4}</script><p>由于低维空间中近邻情况未知，故将矩阵$W$扩充为$\dot{W}$</p><script type="math/tex; mode=display">\dot{W}_{M \times M} = \left[ \begin{matrix}    |               & |               &        & |               \\    \dot{\vec{w}}_1 & \dot{\vec{w}}_2 & \cdots & \dot{\vec{w}}_M \\    |               & |               &        & |\end{matrix} \right] \tag{5.1}</script><script type="math/tex; mode=display">\dot{w}_{ij} = \begin{cases}    w_{ik}  & x^{(j)} = N^{(i)}_k \\    0       & \text{otherwise}\end{cases} \tag{5.2}</script><p>相应的，$\vec{y}^{(i)}_{D \times 1}$扩充为$\dot{\vec{y}}^{(i)}_{M \times 1}$。</p><p>则式$(4)$可变换为</p><script type="math/tex; mode=display">J(\dot{Y}) = \sum_{i} || \dot{\vec{y}}^{(i)} - \sum_j \dot{w}_{ij} \dot{\vec{y}}^{(j)} ||_2^2 \tag{6.1}</script><p>增加约束条件，与<code>LLE</code>略有不同</p><script type="math/tex; mode=display">\dot{\vec{y}}^{(i)^T} \dot{\vec{y}}^{(i)} = 1 \tag{6.2}</script><p>写作矩阵形式，即</p><script type="math/tex; mode=display">J(\dot{Y}) = \text{tr} \left[ \dot{Y} (I - \dot{W}) (I - \dot{W})^T \dot{Y}^T \right] \tag{7}</script><script type="math/tex; mode=display">\text{s.t.} \dot{\vec{y}}^{(i)^T} \dot{\vec{y}}^{(i)} = 1</script><p>其中</p><script type="math/tex; mode=display">\dot{Y}_{M \times M} = \dot{P}^T_{(N \times M)^T} X_{N \times M} \tag{8.1}</script><script type="math/tex; mode=display">\dot{\vec{y}^{(i)}}_{M \times 1} = \dot{P}^T_{(N \times M)^T} \dot{\vec{x}^{(i)}}_{N \times 1} \tag{8.2}</script><p>则优化问题转换为</p><script type="math/tex; mode=display">J(\dot{P}) = \text{tr} \left[\dot{P}^T X (I - \dot{W}) (I - \dot{W})^T X^T \dot{P} \right] \tag{9}</script><script type="math/tex; mode=display">\text{s.t.} \quad \dot{\vec{x}^{(i)}}^T \dot{P} \dot{P}^T \dot{\vec{x}^{(i)}} = 1</script><p>记</p><script type="math/tex; mode=display">M = (I - \dot{W}) (I - \dot{W})^T \tag{10}</script><p>列写拉格朗日函数</p><script type="math/tex; mode=display">L(\dot{P}) = \text{tr} \left[\dot{P}^T X (I - \dot{W}) (I - \dot{W})^T X^T \dot{P} \right] + \lambda (\dot{\vec{x}^{(i)}}^T \dot{P} \dot{P}^T \dot{\vec{x}^{(i)}} - 1) \tag{11}</script><script type="math/tex; mode=display">\frac{\nabla L(\dot{P})}{\nabla \dot{P}} = 2 X M X^T \dot{P} + 2 \lambda X X^T \dot{P}</script><script type="math/tex; mode=display">\Rightarrow \quad X M X^T \dot{P} = \lambda X X^T \dot{P} \tag{*3}</script><blockquote><script type="math/tex; mode=display">\frac{\nabla b^T X^T X c}{\nabla X} = X(bc^T + cb^T)</script></blockquote><p>同<code>LLE</code>，当低维数据维度为$D$时，按特征值升序排序约化矩阵，即选择最前的$D$个特征向量组成投影矩阵</p><script type="math/tex; mode=display">P = \left[ \begin{matrix}    |               & |               &        & |               \\    \vec{\alpha}_1  & \vec{\alpha}_2  & \cdots & \vec{\alpha}_D  \\    |               & |               &        & |\end{matrix} \right] \tag{*4}</script><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eig</span><span class="params">(A1, A2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A1, A2: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        eigval: &#123;ndarray(n)&#125;</span></span><br><span class="line"><span class="string">        eigvec: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        A1 \alpha = \lambda A2 \alpha</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s, u = np.linalg.eigh(A2 + np.diag(np.ones(A2.shape[<span class="number">0</span>]))*<span class="number">1e-3</span>)</span><br><span class="line">    s_sqrt_inv = np.linalg.inv(np.diag(np.sqrt(s)))</span><br><span class="line"></span><br><span class="line">    A = s_sqrt_inv.dot(u.T).dot(A1).dot(u).dot(s_sqrt_inv)</span><br><span class="line">    eigval, P = np.linalg.eigh(A)</span><br><span class="line">    eigvec = u.dot(s_sqrt_inv).dot(P)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> eigval, eigvec</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeighborhoodPreservingEmbedding</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Neighborhood Preserving Embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_neighbors:  &#123;int&#125;</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        W_: &#123;ndarray&#125; </span></span><br><span class="line"><span class="string">        components_:    &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_neighbors, n_components=<span class="number">2</span>, k_skip=<span class="number">1</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.n_neighbors  = n_neighbors</span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.k_skip = k_skip</span><br><span class="line"></span><br><span class="line">        self.W_ = <span class="keyword">None</span></span><br><span class="line">        self.components_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KDTree</span><br><span class="line">        kdtree = KDTree(X, metric=<span class="string">'euclidean'</span>)</span><br><span class="line">        </span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        self.W_ = np.zeros((n_samples, n_samples))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line"></span><br><span class="line">            <span class="comment">## 获取近邻样本点</span></span><br><span class="line">            x = X[i]</span><br><span class="line">            idx = kdtree.query(x.reshape(<span class="number">1</span>, <span class="number">-1</span>), self.n_neighbors + <span class="number">1</span>, return_distance=<span class="keyword">False</span>)[<span class="number">0</span>][<span class="number">1</span>: ]</span><br><span class="line">            <span class="comment">## 求取矩阵 Z = (x - N).dot((x - N).T)</span></span><br><span class="line">            N = X[idx]</span><br><span class="line">            Z = (x - N).dot((x - N).T)</span><br><span class="line">            <span class="comment">## 求取权重 w_i</span></span><br><span class="line">            Z_inv = np.linalg.inv(Z + np.finfo(float).eps * np.eye(self.n_neighbors))</span><br><span class="line">            w = np.sum(Z_inv, axis=<span class="number">1</span>) / np.sum(Z_inv)</span><br><span class="line">            <span class="comment">## 保存至 W</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_neighbors):</span><br><span class="line">                self.W_[idx[j], i] = w[j]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 求取矩阵 M = (I - W)(I - W)^T</span></span><br><span class="line">        I = np.eye(n_samples)</span><br><span class="line">        M = (I - self.W_).dot((I - self.W_).T)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 求解 X M X^T \alpha = \lambda X X^T \alpha</span></span><br><span class="line">        A1 = X.T.dot(M).dot(X)</span><br><span class="line">        A2 = X.T.dot(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 求解拉普拉斯矩阵的特征分解</span></span><br><span class="line">        <span class="comment"># eps = np.finfo(float).eps * np.eye(A2.shape[0])</span></span><br><span class="line">        <span class="comment"># A  = np.linalg.inv(A2 + eps).dot(A1)</span></span><br><span class="line">        <span class="comment"># eigval, eigvec = np.linalg.eig(A)</span></span><br><span class="line">        <span class="comment"># 上三句改为</span></span><br><span class="line">        eigval, eigvec = eig(A1, A2)</span><br><span class="line"></span><br><span class="line">        eigvec = eigvec[:, np.argsort(eigval)]</span><br><span class="line">        eigval = eigval[np.argsort(eigval)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 选取 D 维</span></span><br><span class="line">        self.components_ = eigvec[:, self.k_skip: self.n_components + self.k_skip]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        Y = X.dot(self.components_)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.fit(X)</span><br><span class="line">        Y = self.transform(X)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits(n_class=<span class="number">6</span>)</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">images = digits.images</span><br><span class="line"></span><br><span class="line">npe = NeighborhoodPreservingEmbedding(<span class="number">30</span>, <span class="number">2</span>, k_skip=<span class="number">3</span>)</span><br><span class="line">X_npe = npe.fit_transform(X)</span><br><span class="line"></span><br><span class="line">plot_embedding(X_npe, y, images, title=<span class="keyword">None</span>, t=<span class="number">2e-3</span>, figsize=(<span class="number">12</span>, <span class="number">9</span>))</span><br></pre></td></tr></table></figure><p><img src="/2019/08/12/Neighborhood-Preserving-Embedding/Figure_1.png" alt="Figure_1"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
            <tag> manifold </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>挪威的森林</title>
      <link href="/2019/08/10/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97/"/>
      <url>/2019/08/10/%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/08/10/挪威的森林/image.jpg" alt="挪威的森林"></p><blockquote><p>没有人喜欢孤独，只是不愿失望。</p></blockquote><ol><li>时至今日，我才恍然领悟到直子之所以求我别忘掉她的原因。直子当然知道，知道她在我心目中的记忆迟早要被冲淡。也惟其如此，她才强调说：希望你能记住我，记住我曾这样存在过。 想到这里，我就悲哀得难以自禁。因为，直子连爱都没爱过我的。</li><li>死并非生的对立面，而作为生的一部分永存。</li><li>或许我的心包有一层硬壳，能破壳而入的东西是极其有限的。所以我才不能对人一往情深。</li><li>他也背负着他的十字架匍匐在人生征途中。</li><li>那正是同被直子盯视眼睛时所感到的同一性质的悲哀。这种莫可名状的心绪，我既不能将其排遣于外，又不能将其深藏于内。它像掠身而去的阵风一样没有轮廓，没有重量。</li><li>这种莫可名状的心绪，我既不能将其排遣于外，又不能将其深藏于内。它像掠身而去的阵风一样没有轮廓，没有重量。我甚至连把它裹在身上都不可能。</li><li>“哪里会有人喜欢孤独！不过是不乱交朋友罢了。那样只能落得失望。”我说。</li><li>“绅士就是：所做的，不是自己想做之事，而是自己应做之事。”</li><li>说不定那时我们是为相遇而相遇的。纵令那时未能相遇，也会在别的地方相遇—倒没什么根据，但我总是有这种感觉。</li><li>也许等得过久了。我追求的是十二分完美无缺的东西，所以才这么难。</li><li>孤零零一个人，觉得身体就像一点点腐烂似的。渐渐腐烂、融化，最后变成一洼黏糊糊的绿色液体，再被吸进地底下去，剩下来的只是衣服—就是这种感觉，在干等一天的时间里。</li><li>每个人无不显得很幸福。至于他们是真的幸福还是仅仅表面看上去如此，就无从得知了。</li><li>什么是美好的以及如何获得幸福之类。对我毋宁说是个十分烦琐而错综复杂的命题，从而使我转求其他的标准，诸如公正、正直、普遍性等。”</li><li>倘若我在你心中留下什么创伤，那不仅仅是你一个人的，也是我的创伤。</li><li>所以如此，是因为什么，而它又意味什么，为什么等等。至于这种分析是将世界简单化还是条理化，我却是不明不白。</li><li>普通人啊。生在普通家庭，长在普通家庭，一张普通的脸，普通的成绩，想普通的事情。</li><li>人若要在某件事上扯谎，就势必为此编造出一大堆相关的谎言。</li><li>世界上，有人喜欢查时刻表一查就整整一天；也有的人把火柴棍拼在一起，准备造一艘一米长的船。所以说，这世上有一两个要理解你的人也没什么不自然的吧？</li><li>世上是有这种人的：尽管有卓越的天赋才华，却承受不住使之系统化的训练，而终归将才华支离破碎地挥霍掉。</li><li>现实世界里，很多方面人们都在互相强加，以邻为壑，否则就活不下去。</li><li>“那不是努力，只是劳动。”永泽断然说道，“我所说的努力与这截然不同。所谓努力，指的是主动而有目的的活动。”</li><li>就在这种气势夺人的暮色当中，我猛然想起了初美，并且这时才领悟她给我带来的心灵震颤究竟是什么东西—它类似一种少年时代的憧憬，一种从来不曾实现而且永远不可能实现的憧憬。这种直欲燃烧般的天真烂漫的憧憬，我在很早以前就已遗忘在什么地方了，甚至在很长时间里我连它曾在我心中存在过都未曾记起。而初美所摇撼的恰恰就是我身上长眠未醒的“我自身的一部分”。</li><li>“可爱极了！”<br>“绿子，”她说，“要加上名字。”<br>“可爱极了，绿子。”我补充道。<br>“极了是怎么个程度？”<br>“山崩海枯那样可爱。”<br>绿子扬着脸看着我：“你用词倒还不同凡响。”<br>“给你这么一说，我心里也暖融融的。”我笑道。<br>“来句更棒的。”<br>“最最喜欢你，绿子。”<br>“什么程度？”<br>“像喜欢春天的熊一样。”<br>“春天的熊？”绿子再次扬起脸，“什么春天的熊？”<br>“春天的原野里，你正一个人走着，对面走来一只可爱的小熊，浑身的毛活像天鹅绒，眼睛圆鼓鼓的。它这么对你说道：‘你好，小姐，和我一块打滚玩好么？’接着你就和小熊抱在一起，顺着长满三叶草的山坡咕噜咕噜滚下去，整整玩了一大天。你说棒不棒？”<br>“太棒了。”<br>“我就这么喜欢你。”</li><li>“喜欢我喜欢到什么程度？”绿子问。<br>“整个世界森林里的老虎全都融化成黄油。”</li><li>我则几乎没有抬头，日复一日地打发时光。在我眼里，只有漫无边际的泥沼。往前落下右脚，拔起左脚，再拔起右脚。我判断不出我位于何处，也不具有自己是在朝正确方向前进的信心。我之所以一步步挪动步履，只是因为我必须挪动，而无论去哪里。</li><li>在我眼里，春夜里的樱花，宛如从开裂的皮肤中鼓胀出来的烂肉，整个院子都充满烂肉那甜腻而沉闷的腐臭气味。</li><li>同情自己是卑劣懦夫干的勾当。</li><li>“饼干罐不是装有各种各样的饼干，喜欢的和不大喜欢的不都在里面吗？如果先一个劲儿地挑你喜欢的吃，那么剩下的就全是不大喜欢的。每次遇到麻烦我就总这样想：先把这个应付过去，往下就好过了。人生就是饼干罐。”</li><li>纵令听其自然，世事的长河也还是要流往其应流的方向，而即使再竭尽人力，该受伤害的人也无由幸免。所谓人生便是如此。</li><li>死并非生的对立面，死潜伏在我们的生之中。</li><li>“信终归不过是信。”我说，“即使烧了，该留在心里的自然留下；就算保存在那里，留不下来的照样留不下。”</li><li>我给绿子打电话，告诉她：自己无论如何都想和她说话，有满肚子话要说，有满肚子非说不可得话。整个世界上除了她别无他求。相见她想同她说话，两人一切从头开始。<br>绿子在电话的另一头久久默然不语，如同全世界的细雨落在全世界所有的草坪上一般的沉默在持续。这时间里，我一直合着双眼，把额头顶在电话亭玻璃上。良久，绿子用沉静的声音开口道：“你现在在哪里？”<br>我现在在哪里？<br>我拿着听筒扬起脸，飞快读环视电话亭四周。我现在在哪里？我不知道这里是哪里，全然摸不着头脑。这里究竟是哪里？目力所及，无不是不知走去哪里的无数男男女女。我在哪里也不是的场所的中央，不断地呼唤着绿子。 </li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 村上春树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Locally Linear Embedding</title>
      <link href="/2019/08/09/Locally-Linear-Embedding/"/>
      <url>/2019/08/09/Locally-Linear-Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>高维数据可视化需要通过降维的方式，而PCA、LDA等线性降维算法，忽视了数据点间的空间结构特征，本文介绍的局部线性嵌入<code>(LLE)</code>算法为非线性降维方法，基于谱的降维方法，所以很靠谱 :-)。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>设有$M$个$N$维数据点，构成矩阵$X$，如下</p><script type="math/tex; mode=display">X = \left[ \begin{matrix} \vec{x}^{(1)} & \vec{x}^{(2)} & \cdots & \vec{x}^{(M)} \end{matrix} \right] \tag{1}</script><p>其中$\vec{x}^{(i)}$表示样本点向量</p><script type="math/tex; mode=display">\vec{x}^{(i)} = \left[ \begin{matrix} x^{(i)}_1 & x^{(i)}_2 & \cdots & x^{(i)}_3 \end{matrix} \right]^T \tag{2}</script><p><strong><code>LLE</code>的基本思路：</strong> 任一数据点$\vec{x}^{(i)}$可由其的$K$个近邻点$\mathcal{N}_K(\vec{x}^{(i)})$线性表出，可作为数据样本的结构特征，即</p><script type="math/tex; mode=display">\hat{\vec{x}}^{(i)} = \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} \tag{3.1}</script><p>其中</p><script type="math/tex; mode=display">\vec{w}_i = \left[ \begin{matrix} w_{i1} & w_{i2} & \cdots & w_{ik} \end{matrix} \right]^T</script><p>若$\vec{x}^{(i)}$映射到低维空间中对应特征点为$\vec{y}^{(i)}$，对相同参数$w_{ij}$，$\vec{x}^{(i)}$也可由其K个近邻点$\mathcal{N}_K(\vec{y}^{(i)})$表出，即</p><script type="math/tex; mode=display">\hat{\vec{y}}^{(i)} = \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} w_{ij} \vec{y}^{(j)} \tag{3.2}</script><h2 id="数据结构参数"><a href="#数据结构参数" class="headerlink" title="数据结构参数"></a>数据结构参数</h2><p>求解参数向量$\vec{w}_i$，可利用最小二乘法，即目标函数定义为</p><script type="math/tex; mode=display">J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2 \tag{4.1}</script><p>对参数向量$\vec{w}_i$归一化，即加入约束项</p><script type="math/tex; mode=display">\sum_{j=1}^K w_{ij} = 1 \quad \text{or} \quad \vec{w}_i^T \vec{1} = 1 \tag{4.2}</script><p>式$(4.1)$可作如下变换</p><script type="math/tex; mode=display">J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2</script><script type="math/tex; mode=display">= || \sum_{j=1}^K w_{ij} \vec{x}^{(i)} - \sum_{j=1}^K w_{ij} \vec{x}^{(j)} ||_2^2</script><script type="math/tex; mode=display">= || \sum_{j=1}^K w_{ij} (\vec{x}^{(i)} - \vec{x}^{(j)}) ||_2^2</script><script type="math/tex; mode=display">= || (X^{(i)} - N^{(i)}) \vec{w}_i ||_2^2 \tag{5.1}</script><p>其中</p><script type="math/tex; mode=display">X^{(i)} = \left[ \begin{matrix} \vec{x}^{(i)} & \vec{x}^{(i)} & \cdots & \vec{x}^{(i)} \end{matrix} \right] \tag{5.1.1}</script><script type="math/tex; mode=display">N^{(i)} = \left[ \begin{matrix} \vec{x}_N^{(1)} & \vec{x}_N^{(2)} & \cdots & \vec{x}_N^{(K)} \end{matrix} \right] \tag{5.1.2}</script><p>则</p><script type="math/tex; mode=display">J(\vec{w}_i) = || (X^{(i)} - N^{(i)}) \vec{w}_i ||_2^2 \tag{5.1}</script><script type="math/tex; mode=display">= \left[ (X^{(i)} - N^{(i)}) \vec{w}_i \right]^T \left[ (X^{(i)} - N^{(i)}) \vec{w}_i \right]</script><script type="math/tex; mode=display">= \vec{w}_i^T (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \vec{w}_i \tag{5.2}</script><p>记矩阵</p><script type="math/tex; mode=display">Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \tag{5.3}</script><p>最终优化目标为</p><script type="math/tex; mode=display">J(\vec{w}_i) = \vec{w}_i^T Z^{(i)} \vec{w}_i \tag{*1}</script><script type="math/tex; mode=display">\text{s.t.} \quad \vec{w}_i^T \vec{1} = 1</script><p>利用拉格朗日乘子法，构造拉格朗日函数，有</p><script type="math/tex; mode=display">L(\vec{w}_i) = \vec{w}_i^T Z^{(i)} \vec{w}_i + \lambda (\vec{w}_i^T \vec{1} - 1) \tag{6.1}</script><p>则</p><script type="math/tex; mode=display">\begin{cases}    \frac{\nabla L(\vec{w}_i)}{\nabla \vec{w}_i} = 2 Z^{(i)} \vec{w}_i  + \lambda \vec{1} = \vec{0} \\    \frac{\nabla L(\vec{w}_i)}{\nabla \lambda} = \vec{w}_i^T \vec{1} - 1 = 0\end{cases} \tag{6.2}</script><p>由式$1$得到</p><script type="math/tex; mode=display">\vec{w}_i = - \frac{\lambda}{2} Z^{(i)-1} \vec{1} \tag{6.3}</script><p>代入式$2$有</p><script type="math/tex; mode=display">\left( - \frac{\lambda}{2} Z^{(i)-1} \vec{1} \right)^T \vec{1} = 1 \tag{6.4}</script><p>解得</p><script type="math/tex; mode=display">\lambda = - \frac{2}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{6.5}</script><p>代回$(6.3)$得到</p><script type="math/tex; mode=display">\vec{w}_i = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}</script><h2 id="低维数据的求解"><a href="#低维数据的求解" class="headerlink" title="低维数据的求解"></a>低维数据的求解</h2><p>由式$(3.2)$，即</p><script type="math/tex; mode=display">\hat{\vec{y}}^{(i)} = \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} \dot{w}_{ij} \vec{y}^{(j)} \tag{3.2}</script><p>同样的，利用最小二乘法，构建目标函数为</p><script type="math/tex; mode=display">J(Y) = \sum_{i=1}^M || \vec{y}^{(i)} - \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} \dot{w}_{ij} \vec{y}^{(j)} ||_2^2 \tag{7.1}</script><p>对低维数据进行标准化，即加入约束</p><script type="math/tex; mode=display">\begin{cases}    \sum_{i=1}^M \vec{y}^{(i)} = \vec{0} \\    \frac{1}{M} \sum_{i=1}^M \vec{y}^{(i)} \vec{y}^{(i)T} = I\end{cases} \tag{7.2}</script><p>矩阵形式为</p><script type="math/tex; mode=display">Y Y^T = M·I \tag{7.3}</script><p>由于当前数据点具体分布未知，故$\vec{y}^{(i)}$的近邻点$\mathcal{N}_K(\vec{y}^{(i)})$无法得知，故将矩阵$W_{M \times K}$补全为矩阵$\dot{W}_{M \times M}$，即</p><script type="math/tex; mode=display">\dot{w}_{ij} = \begin{cases}    w_{ik}  & x^{(j)} = N^{(i)}_k \\    0       & \text{otherwise}\end{cases} \tag{8}</script><blockquote><p>相当于邻接矩阵。</p></blockquote><p>同样的，$\vec{y}^{(i)}$也需在维度上进行补全为$\dot{\vec{y}}^{(i)}$，记</p><script type="math/tex; mode=display">\dot{Y}_{M \times M} = \left[ \begin{matrix} \dot{\vec{y}}^{(1)} & \dot{\vec{y}}^{(2)} & \cdots & \dot{\vec{y}}^{(M)} \end{matrix} \right] \tag{9}</script><p>则</p><script type="math/tex; mode=display">J(\dot{Y}) = \sum_{i=1}^M || \dot{\vec{y}}^{(i)} - \sum_{\dot{\vec{y}}^{(j)} \in \mathcal{N}_K(\dot{\vec{y}}^{(i)})} \dot{w}_{ij} \dot{\vec{y}}^{(j)} ||_2^2 \tag{7.1}</script><script type="math/tex; mode=display">= \sum_{i=1}^M || \dot{Y} \vec{1}_i - \dot{Y} \dot{\vec{w}_i} ||_2^2 = \sum_{i=1}^M || \dot{Y} (\vec{1}_i - \dot{\vec{w}_i}) ||_2^2 \tag{10.1}</script><script type="math/tex; mode=display">= \text{tr} \left[ \dot{Y} (I - \dot{W}) (I - \dot{W})^T \dot{Y}^T \right] \tag{10.2}</script><blockquote><script type="math/tex; mode=display">(10.1) \rightarrow (10.2): \mathcal{WTF}</script></blockquote><p>实际上</p><blockquote><p>由于</p><script type="math/tex; mode=display">d_i = \sum_{j=1}^M \dot{w}_{ij} = 1</script><p>所以由拉普拉斯矩阵定义</p><script type="math/tex; mode=display">L = D - W</script><p>$I - \dot{W}$即邻接矩阵$\dot{W}$的拉普拉斯矩阵。</p></blockquote><p>其中</p><script type="math/tex; mode=display">\vec{1}_i = \left[ \begin{matrix} 0 & \cdots & 1_i & \cdots & 0 \end{matrix} \right]^T</script><script type="math/tex; mode=display">I = \left[\begin{matrix}    1 &         &   \\      & \cdots  &   \\      &         & 1 \end{matrix}\right]</script><p>记$A = (I - \dot{W}) (I - \dot{W})^T$, 最终优化目标为</p><script type="math/tex; mode=display">J(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] \tag{*3}</script><script type="math/tex; mode=display">\text{s.t.} \quad \dot{Y} \dot{Y}^T = M·I</script><p>构造拉格朗日函数</p><script type="math/tex; mode=display">L(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] + \lambda (\dot{Y} \dot{Y}^T - M · I) \tag{11}</script><script type="math/tex; mode=display">\begin{cases}    \frac{\nabla L(\dot{Y})}{\nabla \dot{Y}} = 2 A \dot{Y}^T + 2 \lambda \dot{Y}^T = 0 \\    \frac{\nabla L(\dot{Y})}{\nabla \lambda} = \dot{Y} \dot{Y}^T - M · I = 0\\\end{cases}</script><blockquote><script type="math/tex; mode=display">\frac{\nabla \text{tr}[F(\vec{x})]}{\nabla \vec{x}} = f(\vec{x})^T</script></blockquote><p>注意$1$式，可变换为</p><script type="math/tex; mode=display">A \dot{Y}^T = \hat{\lambda} \dot{Y}^T \tag{12.1}</script><p>其中$\hat{\lambda} = - \lambda$，又$\dot{Y} \dot{Y}^T = M·I$，为<strong>正交相似变换</strong>，所以</p><script type="math/tex; mode=display">\dot{Y}^T = P = \left[ \begin{matrix} \vec{\alpha}_1 & \vec{\alpha}_2 & \cdots & \vec{\alpha}_M \end{matrix} \right] \tag{12.2}</script><p>由于最小化目标为</p><script type="math/tex; mode=display">J(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] = \text{tr} (\Lambda) = \sum_{i=1}^M \lambda_i</script><p>故选择最小的特征值$\lambda_i$及其对应的特征向量$\alpha_i$，要得到$D$维数据集，将$\dot{Y}^T$进行约化，即</p><script type="math/tex; mode=display">Y^T = \left[ \begin{matrix} \vec{\alpha}_{M + 1 - D} & \cdots & \vec{\alpha}_M \end{matrix} \right] \tag{13}</script><p>由于$\vec{w}_i^T \vec{1} = 1$，即</p><script type="math/tex; mode=display">\dot{W}^T \vec{1} = \vec{1} \tag{14.1}</script><p>移项整理得</p><script type="math/tex; mode=display">(\dot{W} - I)^T \vec{1} = \vec{0}</script><p>$\vec{1} \neq \vec{0}$，所以</p><script type="math/tex; mode=display">(\dot{W} - I)^T = 0 \tag{14.2}</script><p>左边同乘$\dot{W} - I$得到</p><script type="math/tex; mode=display">(\dot{W} - I) (\dot{W} - I)^T \vec{1} = A · \vec{1} = 0 · \vec{1} \tag{14.3}</script><p>所以</p><script type="math/tex; mode=display">\lambda_M = 0, \quad \alpha_M = \vec{1} \tag{14.4}</script><p>特征值为$0$表示不能反映数据特征，故低维数据应为</p><script type="math/tex; mode=display">Y^T = \left[ \begin{matrix} \vec{\alpha}_{M - D} & \cdots & \vec{\alpha}_{M-1} \end{matrix} \right] \tag{*4}</script><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><ol><li><p>计算原始样本空间中，每个样本$x^{(i)}$的近邻点$N^{(i)}$，并求取权值$w_{ij}$作为邻接权重，存储为矩阵$\dot{W}_{M \times M}$；</p><script type="math/tex; mode=display">Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \tag{5.3}</script><script type="math/tex; mode=display">\vec{w}_i = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}</script><script type="math/tex; mode=display"> \dot{w}_{ij} =  \begin{cases}     w_{ik}  & x^{(j)} = N^{(i)}_k \\     0       & \text{otherwise} \end{cases} \tag{8}</script></li><li><p>求取矩阵$A$，并将其特征分解</p><script type="math/tex; mode=display">A = (I - \dot{W}) (I - \dot{W})^T</script><script type="math/tex; mode=display">A \dot{Y}^T = \hat{\lambda} \dot{Y}^T \tag{12.1}</script></li><li><p>选择最小的特征值$\lambda_i$及其对应的特征向量$\alpha_i$，约化为$D$维数据，作为低维特征点</p><script type="math/tex; mode=display">Y^T = \left[ \begin{matrix} \vec{\alpha}_{M - D} & \cdots & \vec{\alpha}_{M-1} \end{matrix} \right] \tag{*4}</script></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LocallyLinearEmbedding</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Locally Linear Embedding</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_neighbors:  &#123;int&#125;</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        W: &#123;ndarray&#125; </span></span><br><span class="line"><span class="string">            $$ W = \left[ \begin&#123;matrix&#125; w_1 &amp; w_2 &amp; \cdots &amp; w_&#123;n_samples&#125; \end&#123;matrix&#125; \right] $$</span></span><br><span class="line"><span class="string">            $$ w_i = \left[ \begin&#123;matrix&#125; w_&#123;i1&#125; &amp; w_&#123;i2&#125; &amp; \cdots &amp; w_&#123;i, n_samples&#125; \end&#123;matrix&#125; \right]^T $$</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_neighbors, n_components=<span class="number">2</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.n_neighbors  = n_neighbors</span><br><span class="line">        self.n_components = n_components</span><br><span class="line">        self.W = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            W: &#123;ndarray(n_samples, n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KDTree</span><br><span class="line">        kdtree = KDTree(X, metric=<span class="string">'euclidean'</span>)</span><br><span class="line">        </span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        self.W = np.zeros((n_samples, n_samples))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line"></span><br><span class="line">            <span class="comment">## 获取近邻样本点</span></span><br><span class="line">            x = X[i]</span><br><span class="line">            idx = kdtree.query(x.reshape(<span class="number">1</span>, <span class="number">-1</span>), self.n_neighbors + <span class="number">1</span>, return_distance=<span class="keyword">False</span>)[<span class="number">0</span>][<span class="number">1</span>: ]</span><br><span class="line">            <span class="comment">## 求取矩阵 Z = (x - N).dot((x - N).T)</span></span><br><span class="line">            N = X[idx]</span><br><span class="line">            Z = (x - N).dot((x - N).T)</span><br><span class="line">            <span class="comment">## 求取权重 w_i</span></span><br><span class="line">            Z_inv = np.linalg.inv(Z + np.finfo(float).eps * np.eye(self.n_neighbors))</span><br><span class="line">            w = np.sum(Z_inv, axis=<span class="number">1</span>) / np.sum(Z_inv)</span><br><span class="line">            <span class="comment">## 保存至 W</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_neighbors):</span><br><span class="line">                self.W[idx[j], i] = w[j]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.W</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 求取矩阵 A = (I - W)(I - W)^T</span></span><br><span class="line">        I = np.eye(n_samples)</span><br><span class="line">        A = (I - self.W).dot((I - self.W).T)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 对 A 进行特征分解，并按特征值升序排序</span></span><br><span class="line">        eigval, eigvec = np.linalg.eig(A)</span><br><span class="line">        eigvec = eigvec[:, np.argsort(eigval)]</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 选取 D 维</span></span><br><span class="line">        k_skip = <span class="number">1</span></span><br><span class="line">        Y = eigvec[:, k_skip: self.n_components + k_skip]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Y: &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        self.fit(X)</span><br><span class="line">        Y = self.transform(X)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在<code>scikit learn</code>官网有具体实现<a href="https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py" target="_blank" rel="noopener">基于手写数据集的几种基于谱的降维方法对比</a>。</p><p><img src="/2019/08/09/Locally-Linear-Embedding/sklearn.png" alt="sphx_glr_plot_lle_digits_008"></p><p>以下为上述代码实现的结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets  <span class="keyword">import</span> load_digits</span><br><span class="line"></span><br><span class="line">digits = load_digits(n_class=<span class="number">6</span>)</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">images = digits.images</span><br><span class="line"></span><br><span class="line">lle = LocallyLinearEmbedding(<span class="number">30</span>, <span class="number">2</span>)</span><br><span class="line">X_lle = lle.fit_transform(X)</span><br><span class="line"></span><br><span class="line">plot_embedding(X_lle, y, images, title=<span class="keyword">None</span>, t=<span class="number">2e-3</span>, figsize=(<span class="number">12</span>, <span class="number">9</span>))</span><br></pre></td></tr></table></figure></p><p><img src="/2019/08/09/Locally-Linear-Embedding/custom.png" alt="custom"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.jianshu.com/p/25a2a47bb60b" target="_blank" rel="noopener">(十二)LLE局部线性嵌入降维算法 - 简书</a></li><li><a href="https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding" target="_blank" rel="noopener">2.2.3. Locally Linear Embedding - scikit learn</a></li><li><a href="https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf" target="_blank" rel="noopener">An Introduction to Locally Linear Embedding</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
            <tag> manifold </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git工具-submodules</title>
      <link href="/2019/08/09/Git%E5%B7%A5%E5%85%B7-submodules/"/>
      <url>/2019/08/09/Git%E5%B7%A5%E5%85%B7-submodules/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>当需要在一个项目中使用另一个项目时，可以将后者作为子模块加入前者。</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="新建带子模块的仓库"><a href="#新建带子模块的仓库" class="headerlink" title="新建带子模块的仓库"></a>新建带子模块的仓库</h2><p>例如在本地新建仓库<code>Repository</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir Repository &amp;&amp; cd Repository</span><br><span class="line"><span class="meta">$</span> git init</span><br><span class="line">Initialized empty Git repository in C:/Users/islou/Desktop/Repository/.git/</span><br></pre></td></tr></table></figure></p><p>若在本仓库中，需要使用该仓库<a href="https://github.com/isLouisHsu/Games" target="_blank" rel="noopener">isLouisHsu/Games</a><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git submodule add https://github.com/isLouisHsu/Games</span><br><span class="line">Cloning into 'C:/Users/islou/Desktop/Repository/Games'...</span><br><span class="line">remote: Enumerating objects: 30, done.</span><br><span class="line">remote: Counting objects: 100% (30/30), done.</span><br><span class="line">remote: Compressing objects: 100% (27/27), done.</span><br><span class="line">remote: Total 30 (delta 5), reused 0 (delta 0), pack-reused 0</span><br><span class="line">Unpacking objects: 100% (30/30), done.</span><br></pre></td></tr></table></figure></p><p>在当前仓库<code>Repository</code>中可以看到生成了文件<code>.gitmodules</code>与子仓库<code>Games</code>，<code>.gitmodules</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[submodule &quot;Games&quot;]</span><br><span class="line">path = Games</span><br><span class="line">url = https://github.com/isLouisHsu/Games</span><br></pre></td></tr></table></figure></p><p>此时，若在当前仓库查询状态，显示更改内容<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git status</span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use "git rm --cached &lt;file&gt;..." to unstage)</span><br><span class="line"></span><br><span class="line">        new file:   .gitmodules</span><br><span class="line">        new file:   Games</span><br><span class="line"><span class="meta">$</span></span><br><span class="line"><span class="meta">$</span> cd Games</span><br><span class="line"><span class="meta">$</span> git status</span><br><span class="line">On branch master</span><br><span class="line">Your branch is up to date with 'origin/master'.</span><br><span class="line"></span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure></p><h2 id="克隆带子仓库的模块"><a href="#克隆带子仓库的模块" class="headerlink" title="克隆带子仓库的模块"></a>克隆带子仓库的模块</h2><ol><li><p>方法1</p><p> 克隆这类仓库时，默认包含该子模块目录，但其中没有文件，需要在仓库目录下运行</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git submodule init</span><br><span class="line"><span class="meta">$</span> git submodule update</span><br></pre></td></tr></table></figure><p> <code>git submodule init</code> 用来初始化本地配置文件，而 <code>git submodule update</code> 则从该项目中抓取所有数据并检出父项目中列出的合适的提交</p></li><li><p>方法2</p><p> 直接使用<code>--recursive</code>参数，自动初始化并更新仓库中的每一个子模块</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> git clone --recursive https://github.com/louishsu/Repository</span><br></pre></td></tr></table></figure></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://git-scm.com/book/zh/v2/Git-%E5%B7%A5%E5%85%B7-%E5%AD%90%E6%A8%A1%E5%9D%97" target="_blank" rel="noopener">7.11 Git工具-子模块 - git</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> github </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python生成Markdown表格</title>
      <link href="/2019/08/09/Python%E7%94%9F%E6%88%90Markdown%E8%A1%A8%E6%A0%BC/"/>
      <url>/2019/08/09/Python%E7%94%9F%E6%88%90Markdown%E8%A1%A8%E6%A0%BC/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h1 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h1><p>Markdown中编辑表格比较繁琐，如编辑下表时，需要按字符输入</p><div class="table-container"><table><thead><tr><th style="text-align:center">姓名\科目</th><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td style="text-align:center">小妖</td><td>3</td><td>4</td><td>5</td><td>3</td></tr><tr><td style="text-align:center">小怪</td><td>4</td><td>5</td><td>3</td><td>4</td></tr><tr><td style="text-align:center">小兽</td><td>5</td><td>3</td><td>4</td><td>5</td></tr></tbody></table></div><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| 姓名\科目 | A | B | C | D |</span><br><span class="line">| :------: | - | - | - | - |</span><br><span class="line">| 小妖     | 3 | 4 | 5 | 3 |</span><br><span class="line">| 小怪     | 4 | 5 | 3 | 4 |</span><br><span class="line">| 小兽     | 5 | 3 | 4 | 5 |</span><br></pre></td></tr></table></figure><p>可借助字符串操作生成表格。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_markdown_table_2d</span><span class="params">(head_name, rows_name, cols_name, data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        head_name: &#123;str&#125; 表头名， 如"count\比例"</span></span><br><span class="line"><span class="string">        rows_name, cols_name: &#123;list[str]&#125; 项目名， 如 1,2,3</span></span><br><span class="line"><span class="string">        data: &#123;ndarray(H, W)&#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        table: &#123;str&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ELEMENT = <span class="string">" &#123;&#125; |"</span></span><br><span class="line"></span><br><span class="line">    H, W = data.shape</span><br><span class="line">    LINE = <span class="string">"|"</span> + ELEMENT * W</span><br><span class="line">    </span><br><span class="line">    lines = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 表头部分</span></span><br><span class="line">    lines += [<span class="string">"| &#123;&#125; | &#123;&#125; |"</span>.format(head_name, <span class="string">' | '</span>.join(cols_name))]</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 分割线</span></span><br><span class="line">    SPLIT = <span class="string">"&#123;&#125;:"</span></span><br><span class="line">    line = <span class="string">"| &#123;&#125; |"</span>.format(SPLIT.format(<span class="string">'-'</span>*len(head_name)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(W):</span><br><span class="line">        line = <span class="string">"&#123;&#125; &#123;&#125; |"</span>.format(line, SPLIT.format(<span class="string">'-'</span>*len(cols_name[i])))</span><br><span class="line">    lines += [line]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## 数据部分</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(H):</span><br><span class="line">        d = list(map(str, list(data[i])))</span><br><span class="line">        lines += [<span class="string">"| &#123;&#125; | &#123;&#125; |"</span>.format(rows_name[i], <span class="string">' | '</span>.join(d))]</span><br><span class="line"></span><br><span class="line">    table = <span class="string">'\n'</span>.join(lines)</span><br><span class="line">    <span class="keyword">return</span> table</span><br></pre></td></tr></table></figure><p>终端中运行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; from temp import gen_markdown_table_2d</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; head_name = "姓名\\科目"</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; rows_name = ["小妖", "小怪", "小兽"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; cols_name = ["A", "B", "C", "D"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; data = np.arange(4*3).reshape(3, 4)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; table = gen_markdown_table_2d(head_name, rows_name, cols_name, data)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; table</span><br><span class="line">'| 姓名\\科目 | A | B | C | D |\n| -----: | -: | -: | -: | -: |\n| 小妖 | 0 | 1 | 2 | 3 |\n| 小怪 | 4 | 5 | 6 | 7 |\n|  小兽 | 8 | 9 | 10 | 11 |'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(table)</span><br><span class="line">| 姓名\科目 | A | B | C | D |</span><br><span class="line">| -----: | -: | -: | -: | -: |</span><br><span class="line">| 小妖 | 0 | 1 | 2 | 3 |</span><br><span class="line">| 小怪 | 4 | 5 | 6 | 7 |</span><br><span class="line">| 小兽 | 8 | 9 | 10 | 11 |</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自动化脚本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ID3, C4.5, CART, RF</title>
      <link href="/2019/08/02/ID3-C4-5-CART-RF/"/>
      <url>/2019/08/02/ID3-C4-5-CART-RF/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>决策树(Decision Tree)是一种基本的分类与回归方法，本文主要讨论分类决策树，它可被认作<code>if-then</code>的集合，也可以认作是定义在特征空间与类空间上的条件概率分布。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>决策树模型形式如下，由结点<code>(node)</code>，有向边<code>(directed edge)</code>组成，节点包含两种：内部节点<code>(internal node)</code>和叶节点<code>(leaf node)</code>，内部节点表示一个特征或属性，叶节点包含一个类。</p><p><img src="/2019/08/02/ID3-C4-5-CART-RF/model.jfif" alt="model"></p><p>决策树需要满足一个重要性质：互斥且完备。即每个实例<strong>都被且仅被</strong>一条路径或一条<code>if-then</code>规则覆盖。并且生成的决策树深度不能过大。</p><p>决策树学习主要有3个步骤： 特征选择、决策树生成、决策树修剪。</p><h2 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h2><p>现以下某贷款申请样本数据表为例进行解释说明。</p><div class="table-container"><table><thead><tr><th>ID</th><th>年龄</th><th>工作</th><th>有房</th><th>信贷情况</th><th>类别</th></tr></thead><tbody><tr><td>1</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>2</td><td>青年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>3</td><td>青年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>4</td><td>青年</td><td>是</td><td>是</td><td>一般</td><td>是</td></tr><tr><td>5</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>6</td><td>中年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>7</td><td>中年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>8</td><td>中年</td><td>是</td><td>是</td><td>好</td><td>是</td></tr><tr><td>9</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>10</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>11</td><td>老年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>12</td><td>老年</td><td>否</td><td>是</td><td>好</td><td>是</td></tr><tr><td>13</td><td>老年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>14</td><td>老年</td><td>是</td><td>否</td><td>非常好</td><td>是</td></tr><tr><td>15</td><td>老年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr></tbody></table></div><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><p>设数据集为$D$，$|D|$表示样本总数。设有$K$个类别$C_k(k=1, \cdots, K)$，$|C_k|$为类别$k$的样本数目，那么</p><script type="math/tex; mode=display">|D| = \sum_{k=1}^K |C_k|</script><p>设特征$A$有$N$个不同的取值$\{a_1, a_2, \cdots, a_N\}$，则根据不同的取值，可将数据集$D$划分为$N$组$\{D_1, D_2, \cdots, D_N\}$，也有</p><script type="math/tex; mode=display">|D| = \sum_{n=1}^N |D_n|</script><p>记子集$D_n$中属于类别$C_k$的样本集合为$D_{nk}$，即</p><script type="math/tex; mode=display">D_{nk} = D_n \bigcap C_k</script><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>在某个内部节点上，需要选择最具有代表性，或区分度最高的特征。可选用的准则有信息增益（条件熵）、基尼系数等。</p><ol><li><p>ID3：以<strong>信息增益</strong>为准则来选择最优划分属性</p><p> <strong>定义(信息增益)</strong>：特征$A$对训练数据集$D$的信息增益$g(D, A)$，定义为集合$D$的经验熵$H(D)$与特征A给定条件下$D$的经验条件熵$H(D|A)$之差，即</p><script type="math/tex; mode=display">g(D, A) = H(D) - H(D|A) \tag{1.1}</script><p> 等价于训练数据集中类与特征的<strong>互信息</strong><code>(mutual information)</code>，表示<strong>得知特征$A$的信息对数据集$D$分类的不确定性减少的程度。</strong></p><p> 其中</p><script type="math/tex; mode=display">H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} \tag{1.2}</script><p> 上式即，<strong>类别分布的经验熵</strong></p><script type="math/tex; mode=display"> H(D|A) = \sum_{n=1}^N \frac{|D_n|}{|D|} H(D_n)  = - \sum_{n=1}^N \frac{|D_n|}{|D|} \underbrace{     \sum_{k=1}^K \frac{|D_{nk}|}{|D_i|} \log_2 \frac{|D_{nk}|}{|D_i|} }_{H(D_n)} \tag{1.3}</script><p> 上式即，以特征$A$不同取值进行子集划分，<strong>每个子集的类别分布经验熵，求取加权和</strong></p></li></ol><pre><code>&gt; 注：&gt; 1. 熵&gt;   - 连续&gt;       $$ H(X) = - \int_x p(x) \log p(x) dx $$&gt;&gt;   - 离散&gt;       $$ H(X) = - \sum_x p(x) \log p(x) $$&gt;&gt; 2. 条件熵&gt; $$&gt; \begin{aligned}&gt;   H(X|Y) = H(X, Y) - H(Y) \\&gt;   = (- \sum_{x, y} p(x, y) \log p(x, y)) - (- \sum_y p(y) \log p(y)) \\&gt;   = - \sum_{x, y} p(x, y) \log p(x, y) + \sum_x (\sum_y p(x, y)) \log p(y) \\&gt;   = - \sum_{x, y} p(x, y) (\log p(x, y) - \log p(y)) \\&gt;   = - \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(y)} \\&gt;   = - \sum_{x, y} p(x, y) \log p(x | y)&gt; \end{aligned}&gt; $$&gt; &gt; 3. 互信息&gt;   一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息`(mutual information)`。&gt;   $$ I(X; Y) = H(X) - H(X | Y) $$&gt;   也即&gt;   $$&gt;   \begin{aligned}&gt;       I(X; Y) = H(X) + H(Y) - H(X, Y) \\&gt;       I(X; Y) = I(Y; X)&gt;   \end{aligned}&gt;   $$&gt;   由定义可得&gt;   $$&gt;   I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}&gt;   $$</code></pre><ol><li><p>C4.5：基于<strong>信息增益率准则</strong>选择最优分割属性的算法</p><p> 是对<code>ID3</code>的改进：</p><ul><li>以<strong>信息增益比</strong><code>(information gain ratio)</code>作为特征选择的准则，克服<code>ID3</code>会优先选择有较多属性值的特征的缺点；<blockquote><p>属性值较多的特征，可将$D$划分为更多子集，计算得信息增益更大；信息增益比引入特征数据分布的衡量，对于两个相同信息增益的特征，选择特征数据分布更为集中的，划分更有效。</p></blockquote></li><li><p>弥补不能处理特征属性值连续的问题</p><script type="math/tex; mode=display">g_R(D, A) = \frac{g(D, A)}{H_A(D)} \tag{2.1}</script><p>其中$H_A(D)$为<strong>特征$A$的分布经验熵</strong></p><script type="math/tex; mode=display">H_A(D) = - \sum_{n=1}^N \frac{|D_n|}{|D|} \log_2 \frac{|D_n|}{|D|} \tag{2.2}</script></li></ul></li><li><p>CART：以<strong>基尼系数</strong>为准则选择最优划分属性，可以应用于分类和回归</p><p> 由L.Breiman,J.Friedman,R.Olshen和C.Stone于1984年提出。ID3中根据属性值分割数据，之后该特征不会再起作用，这种快速切割的方式会影响算法的准确率。<strong>CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树</strong>。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。</p><p> <strong>定义(基尼系数)</strong>：分类问题中，假设有$K$个类别，样本点属于第$k$类的概率为$p_k$，则基尼系数定义为</p><script type="math/tex; mode=display">\text{Gini}(p) = \sum_{k=1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2 \tag{3.1}</script><ul><li><p>分类树</p><p>  特征选择准则为</p><script type="math/tex; mode=display">g_G(D, A) = \text{Gini}(D) - \text{Gini}(D|A)  \tag{3.2}</script><p>  其中</p><script type="math/tex; mode=display">\text{Gini}(D) = 1 - \sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right)^2 \tag{3.3}</script><script type="math/tex; mode=display">\text{Gini}(D|A) = \sum_{n=1}^N \frac{|D_n|}{|D|} \text{Gini}(D_n) = \sum_{n=1}^N \frac{|D_n|}{|D|} \left[ 1 - \sum_{k=1}^K \left( \frac{|D_{nk}|}{|D_n|} \right)^2 \right] \tag{3.4}</script></li><li><p>回归树</p><p>  寻找最佳切分点，将输入空间划分为多个区域。</p></li></ul></li></ol><h2 id="引例求解"><a href="#引例求解" class="headerlink" title="引例求解"></a>引例求解</h2><p>以求解在<a href="#%e5%bc%95%e4%be%8b">引例</a>的第一个内部节点为例，选用信息增益作为分类特征。</p><p>类别标签有两种</p><script type="math/tex; mode=display">D.y = \{ 是， 否 \}</script><p>则</p><script type="math/tex; mode=display">p(y = 是) = \frac{9}{15},\quad p(y = 否) = \frac{6}{15}</script><script type="math/tex; mode=display">H(D) = - \frac{9}{15} \log_2 \frac{9}{15} - \frac{6}{15} \log_2 \frac{6}{15} = 0.971</script><p>当前特征集为</p><script type="math/tex; mode=display">A = \{ 年龄， 有工作， 有自己的房子， 信贷情况 \}</script><ol><li><p>以年龄为特征<br> 年龄含青年、中年、老年，则</p><script type="math/tex; mode=display">p(青年) = \frac{5}{15}, \quad p(中年) = \frac{5}{15}, \quad p(老年) = \frac{5}{15}</script><script type="math/tex; mode=display">p(y = 是|青年) = \frac{2}{5}, \quad p(y = 否|青年) = \frac{3}{5}</script><script type="math/tex; mode=display">p(y = 是|中年) = \frac{3}{5}, \quad p(y = 否|中年) = \frac{2}{5}</script><script type="math/tex; mode=display">p(y = 是|老年) = \frac{4}{5}, \quad p(y = 否|老年) = \frac{1}{5}</script><p> 则</p><script type="math/tex; mode=display">H(D|青年) = - \frac{2}{5} \log_2 \frac{2}{5} - \frac{3}{5} \log_2 \frac{3}{5} = 0.971</script><script type="math/tex; mode=display">H(D|中年) = - \frac{3}{5} \log_2 \frac{3}{5} - \frac{2}{5} \log_2 \frac{2}{5} = 0.971</script><script type="math/tex; mode=display">H(D|老年) = - \frac{4}{5} \log_2 \frac{4}{5} - \frac{1}{5} \log_2 \frac{1}{5} = 0.722</script><p> 则</p><script type="math/tex; mode=display">H(D|年龄) = \sum_{年龄=青年}^{老年} p(年龄) H(D|年龄) = 0.888</script><p> 所以</p><script type="math/tex; mode=display">g(D, 年龄) = H(D) - H(D|年龄) = 0.083</script></li><li><p>以工作为特征</p><script type="math/tex; mode=display">g(D, 工作) = H(D) - H(D|工作) = 0.324</script></li><li><p>以房子为特征</p><script type="math/tex; mode=display">g(D, 房子) = H(D) - H(D|房子) = 0.420</script></li><li><p>以信贷为特征</p><script type="math/tex; mode=display">g(D, 信贷) = H(D) - H(D|信贷) = 0.363</script></li></ol><p>由于$g(D, 房子)$最大，故当前节点选择房子作为分类特征，然后按此特征可分成2个子集合$\{D_{有房}，D_{无房}\}$，递归即可。</p><h2 id="决策树生成"><a href="#决策树生成" class="headerlink" title="决策树生成"></a>决策树生成</h2><p>决策树生成使用递归方法，将该节点处所属数据子集$D_s$按<a href="#%e7%89%b9%e5%be%81%e9%80%89%e6%8b%a9">特征选择准则</a>，选择当期节点最优划分，然后在划分后的子节点处，继续调用该函数进行递归生成。</p><p>伪代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">输入：数据集D，特征集A = &#123;a_1, a_2, ..., a_N&#125;，阈值eps；</span><br><span class="line">输出：节点node</span><br><span class="line"></span><br><span class="line">CREAT-NODE(D, A, eps)</span><br><span class="line">    if D中所有样本属于类别C_k</span><br><span class="line">        return Node(C_k)</span><br><span class="line"></span><br><span class="line">    if A为空集</span><br><span class="line">        C = argmax(|D_k|, k = 1, ..., K)</span><br><span class="line">        return Node(C)</span><br><span class="line"></span><br><span class="line">    # 计算特征选择准则，这里采用互信息g(D, A)</span><br><span class="line">    for n = 1 upto N</span><br><span class="line">        g(D, A_n) = H(D) - H(D|A_n)</span><br><span class="line">    </span><br><span class="line">    # 选择最优特征</span><br><span class="line">    a_g = argmax(g(D, A_n), n = 1, ..., N)</span><br><span class="line"></span><br><span class="line">    # 终止条件</span><br><span class="line">    if g(D, a_g) &lt; eps</span><br><span class="line">        C = argmax(|D_k|, k = 1, ..., K)</span><br><span class="line">        return Node(C)</span><br><span class="line">    </span><br><span class="line">    node = Node(a_g)</span><br><span class="line"></span><br><span class="line">    # 递归</span><br><span class="line">    for i = 1 upto a_g.length</span><br><span class="line">        node.sub[i] = CREAT-NODE(D_i, A - a_g, eps)</span><br><span class="line"></span><br><span class="line">    return node</span><br></pre></td></tr></table></figure></p><h2 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h2><p>如果学习时过多地考虑如何提高对训练数据的正确分类，使决策树<strong>划分层次过多(也即叶节点个数过多)</strong>，构建出过于复杂的决策树，那么会引起过拟合现象。</p><p>设决策树共有$T$个叶节点，且叶节点$t$上有$N_t$个样本点、其中类别$k$的有$N_{tk}$个，$k = 1, \cdots, K$，那么<strong>叶节点$t$的类别经验熵</strong>为</p><script type="math/tex; mode=display">H_t(T) = - \sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t} \tag{4.1}</script><p>那么<strong>所有叶节点的类别经验熵之和</strong>为</p><script type="math/tex; mode=display">C(T) = \sum_{t=1}^{|T|} N_t H_t(T) \tag{4.2}</script><p>希望$C(T)$极小化，减小类别经验熵，那么会增加划分使叶子节点数目增多，考虑模型复杂度$T$与目标之间的权衡，设置正则化参数$\alpha$，那么决策树的<strong>整体结构损失函数</strong>为</p><script type="math/tex; mode=display">C_{\alpha} (T) = C(T) + \alpha |T| \tag{4.3}</script><p><strong>利用损失函数最小原则仅剪枝就是用正则化的极大似然估计进行模型选择</strong>。可以通过<strong>动态规划</strong>递归地使叶子节点向上回缩，求取损失函数最小地子树$T_{\alpha}$。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>详细查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/p110_decision_tree.py" target="_blank" rel="noopener">Github: isLouisHsu/Basic-Machine-Learning-Algorithm</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        index: 子树分类标签, 若为叶节点, 则为None</span></span><br><span class="line"><span class="string">        childNode: 子树，若为叶节点, 则为分类标签; 否则为字典</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.index = <span class="keyword">None</span></span><br><span class="line">        self.childNode = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @note:  </span></span><br><span class="line"><span class="string">        - categorical features;</span></span><br><span class="line"><span class="string">        - ID3</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.tree = <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">     </span><br><span class="line">        self.tree = self._creatNode(X, y)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_creatNode</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    </span><br><span class="line">        node = Node()</span><br><span class="line">        <span class="comment"># 若只含一种类别，则返回叶节点</span></span><br><span class="line">        <span class="keyword">if</span> len(set(y)) == <span class="number">1</span>: node.childNode = list(set(y))[<span class="number">0</span>]; <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line">        <span class="comment"># entropy: H(D)</span></span><br><span class="line">        y_encoded = OneHotEncoder().fit_transform(y.reshape(<span class="number">-1</span>, <span class="number">1</span>)).toarray()</span><br><span class="line">        p_y = np.mean(y_encoded, axis=<span class="number">0</span>); p_y[p_y==<span class="number">0.0</span>] = <span class="number">1.0</span>       <span class="comment"># 因为 0*np.log(0)结果为nan, 而不是0, 用 1*np.log(1)替代</span></span><br><span class="line">        H_D = - np.sum(p_y * np.log(p_y))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># conditional entropy: H(D|A)</span></span><br><span class="line">        H_D_A = np.zeros(shape=(X.shape[<span class="number">1</span>],))                       <span class="comment"># initialize</span></span><br><span class="line">        <span class="keyword">for</span> i_feature <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">            X_feature = X[:, i_feature]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> len(set(X_feature)) == <span class="number">1</span>: </span><br><span class="line">                H_D_A[i_feature] = float(<span class="string">'inf'</span>); <span class="keyword">continue</span>           <span class="comment"># 若该特征只有一种取值，表示已使用该列作为分类特征</span></span><br><span class="line">        </span><br><span class="line">            X_feature_encoded = OneHotEncoder().fit_transform(X_feature.reshape((<span class="number">-1</span>, <span class="number">1</span>))).toarray()</span><br><span class="line">            p_X = np.mean(X_feature_encoded, axis=<span class="number">0</span>)                <span class="comment"># 每个取值的概率</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> j_feature <span class="keyword">in</span> range(X_feature_encoded.shape[<span class="number">1</span>]):     <span class="comment"># 该特征取值有几种，编码后就有几列</span></span><br><span class="line">                y_encoded_feature = y_encoded[X_feature_encoded[:, j_feature]==<span class="number">1</span>]   <span class="comment"># 该特征某种取值下，其对应的标签值</span></span><br><span class="line">                p_y_X = np.mean(y_encoded_feature, axis=<span class="number">0</span>); p_y_X[p_y_X==<span class="number">0.0</span>] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">                H_D_feature = - np.sum(p_y_X * np.log(p_y_X))</span><br><span class="line">                H_D_A[i_feature] += p_X[j_feature] * H_D_feature    <span class="comment"># 条件熵</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># information gain: g(D, A) = H(D) - H(D|A)</span></span><br><span class="line">        g_D_A = H_D - H_D_A</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选出最大的作为分类特征</span></span><br><span class="line">        node.index = np.argmax(g_D_A)</span><br><span class="line">        X_selected = X[:, node.index]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 分类后继续建立树</span></span><br><span class="line">        node.childNode = dict()</span><br><span class="line">        <span class="keyword">for</span> val <span class="keyword">in</span> set(X_selected):</span><br><span class="line">            valIndex = (X_selected==val)</span><br><span class="line">            X_val, y_val = X[valIndex], y[valIndex]</span><br><span class="line">            node.childNode[val] = self._creatNode(X_val, y_val)     <span class="comment"># 存储在字典中，键为分类值，值为子树</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        y_pred = np.zeros(shape=(X.shape[<span class="number">0</span>],))</span><br><span class="line">        <span class="keyword">for</span> i_sample <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">            currentNode = self.tree                                 <span class="comment"># 初始化为父节点</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> currentNode.index==<span class="keyword">None</span>:                      <span class="comment"># 若为None, 表示为叶子结点</span></span><br><span class="line">                val = X[i_sample, currentNode.index]                <span class="comment"># 当前样本在分类特征上的值</span></span><br><span class="line">                currentNode = currentNode.childNode[val]            <span class="comment"># 递归</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_pred[i_sample] = currentNode.childNode</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><p>最终生成分类树如下<br><img src="/2019/08/02/ID3-C4-5-CART-RF/tree.jpg" alt="tree"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/34534004" target="_blank" rel="noopener">ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结</a></li><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">统计学习方法，李航，第5章</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>人间失格</title>
      <link href="/2019/07/21/%E4%BA%BA%E9%97%B4%E5%A4%B1%E6%A0%BC/"/>
      <url>/2019/07/21/%E4%BA%BA%E9%97%B4%E5%A4%B1%E6%A0%BC/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/07/21/人间失格/image.jpg" alt="image"></p><p>人类是群居的孤独者。</p><h1 id="第一手札"><a href="#第一手札" class="headerlink" title="第一手札"></a>第一手札</h1><ol><li>尽管如此，他们却能够不死自杀，免于疯狂，纵谈政治，竟不绝望，不屈不挠，继续与生活搏斗。他们不是并不痛苦吗？他们使自己成为彻底的利己主义者，并虔信那一些里所担任，曾几何时怀疑过自己呢？这样一来，不是很轻松惬意吗？</li></ol><blockquote><p>太平洋战争爆发后，当时的许多作家纷纷创造起了激发人们战斗欲望的小说作品，唯独太宰治不一样，他从不去碰战争题材，相反的，太宰治为了让那些为了度过困难时期而奋斗着的人们于百忙之中能得到片刻慰藉，于是发明了一种新的写作方式——私小说，并创作了令人轻松愉悦捧腹大笑的作品——《御伽草纸》。<br>一直到日本二战战败以后，许多曾经崇拜战争的作家们纷纷“临阵倒戈”。太宰治发现，人们似乎根本没有意识到自己的罪恶，并创作了以二战为背景的战争题材小说《斜阳》：“被记者追捧者，鼓吹民主主义什么的，我不干，所有日本人都参与了战争。”</p></blockquote><ol><li>这是我向人类最后的求爱。尽管我对人类极度恐惧，但似乎始终割不断对人类的缘情，于是借着装傻这一缕细丝，来维系与人类的贯联。表面上我总是笑脸迎人，暗中则是拼了死命，战战兢兢，如履薄冰般才艰难万分做出这样的奉侍。</li></ol><blockquote><p>戴上人性的面具。</p></blockquote><ol><li>我却从人们动怒的面孔中发现了比狮子、鳄鱼、巨龙更可怕的动物本性。平常他们总是隐藏起这种动物本性，可一旦遇到某个时机，他们就会像那些温文尔雅地躺在草地上歇息的牛，蓦然甩动尾巴抽死肚皮上的牛虻一般，暴露出人的这种本性。</li><li>对讨厌的事不能说讨厌，而对喜欢的事呢，也是一样。</li><li>我对受人尊敬这一状态进行了如下定义：近于完美无缺地蒙骗别人，尔后又被某个全知全能之人识破真相，最终原形毕露，被迫当众出丑，以致于比死亡更难堪更困窘。</li><li>那时，我被男女佣人教唆者做出了可悲的丑事。事到如今我认为，对年幼者干出那种事情，无疑是人类所能犯下的罪孽中最丑恶最卑劣的行径。</li><li>不公平现象是必然存在的。这点是明摆着的事实。向人诉苦不过是徒劳，与其如此，不如默默承受。</li><li>相互欺骗，却又令人惊奇地不受到任何伤害，甚至于就好像没有察觉到彼此在欺骗似的，这种不加掩饰从而显得清冽、豁达的互不信任的例子，在人类生活中比比皆是。</li></ol><h1 id="第二手札"><a href="#第二手札" class="headerlink" title="第二手札"></a>第二手札</h1><ol><li>在迄今为止的生涯中，我曾经无数次祈望过自己被杀死，却从来也没有动过杀死别人的念头。这是因为我觉得，那样做只会给可怕的对手带来幸福的缘故。</li><li>“迷恋”、“被迷恋”这些措辞本身就是粗俗不堪而又戏弄人的说法，给人一种装腔作势的感觉。无论是多么“严肃”的场合，只要让这些词语抛头露面，忧郁的伽蓝就会顷刻间分崩离析，变得索然无味。</li><li>对人感到过分恐惧的人，反倒更加迫切地希望用自己地眼睛去看更可怕的妖怪；越是容易对事物感到胆怯的神经质的人，就越是渴望暴风雨降临得更加猛烈。</li><li>人啊，明明一点儿也不了解对方，错看对方，却视彼此为独一无二的挚友，一生不解对方的真性情，待一方撒手西去，还要为其哭泣，念诵悼词。</li><li>竭力想把觉得美的东西原封不动地描绘为美是幼稚和愚蠢乃至完全谬误的。</li><li>胆小鬼连幸福都会害怕，碰到棉花都会受伤，有时还被幸福所伤。</li></ol><h1 id="第三手札"><a href="#第三手札" class="headerlink" title="第三手札"></a>第三手札</h1><ol><li>人怎么能如此轻易地变得面目全非呢？这令我感到可耻，不，毋宁说是滑稽。</li><li>世上所有人的说法，总是显得拐弯抹角，含糊不清，其中有一种试图逃避责任似的微妙性和复杂性。</li><li>那以后我也尝试过画各种各样的画，但都远远及不上那记忆中的杰作，以致于我总是被一种失落感所折磨着，恍若整个胸膛都变成了一个空洞。</li><li>“可事实上，我是多么畏惧他们啊！我越是畏惧他们，就越是博得他们的喜欢，而越是博得他们的喜欢，我就越是畏惧他们，并不得不里他们远去。”</li><li>莫非在别人眼里，我那种畏惧他人、躲避他人、搪塞他人的性格，竟然与遵从俗话所说的那种“明哲保身、得过且过”的处世训条的做法，在表现形式上是相同的吗？</li><li>所谓世间，又是什么呢？是人的复数吗？可哪儿存在着“世间”这个东西的实体呢？迄今为止，我一直以为它是一种苛烈、严酷、而且可怕的东西，并且一直生活在这种想法之中，如今被崛木一说，有一句话差点就脱口而出：“所谓的世人，不就是你吗？”……打那时候起，我开始萌发了一种可以称之为“思想”的念头：所谓世间，不就是个人吗？</li><li>第二天也重复着同一件事情/只需遵从与昨天相同的习性/倘若愿意避免狂喜狂乐/大惊大悲就不会降临/躲开前方的挡路巨石/像蟾蜍一般迂回前进。</li><li>我知道有人是爱我的，但我好像缺乏爱人的能力。</li><li>所谓世间的真相，就是个人与个人之间的争斗，而且是即时即地的斗争。人需要在那种争斗中当场取胜，人是绝不可能服从他人的。即使是当努力，也会以努力的方式进行卑屈的反击。所以，人除了当场一决胜负外，不可能有别的生存方式。虽然人们提倡大义名分，但努力的目标毕竟是属于个人的，超越了个人之后依旧还是个人，世间的不可思议其实也就是个人的不可思议。</li><li>难道纯真无瑕的信赖之心真的是罪恶之源吗？难道纯真无瑕的信赖之心也算是罪过吗？</li><li>我的不幸，恰恰在于我缺乏拒绝的能力。我害怕一旦拒绝别人，便会在彼此心里留下永远无法愈合的裂痕。</li><li>但是，被关进这所医院的人全是狂人，而逍遥在外的全都是正常人。我问神灵：难道不反抗也是一种罪过吗？</li><li>我已丧失做人的资格。我已经彻底变成一个废人了。</li></ol><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>大庭叶藏是善良的人，心思细腻敏感却怯懦敏感，性格孤独，渴望被爱，在世间选择了极端的生活方式，根源最主要还是软弱的灵魂，没有自我</p><ul><li>自小充满幻想，对幻想破灭而大觉扫兴；</li><li>见到静子母女的背影，黯然离去，不忍打扰；</li><li>良子过于信任他人，与书商发生关系，而叶藏反复认定这是因为自己欣赏良子的纯真无暇，这不是她的错；</li><li>堀木作为叶藏的“朋友”，对叶藏一直抱有轻蔑的态度，叶藏看破不说破，并从堀木家中看到东京人真实的生活状态；</li><li>在药店老板的诱导下，吸毒上瘾；</li><li>不忍拒绝他人的请求；</li><li>“我们所认识的阿叶，又诚实又乖巧，要是不喝酒的话，不，即使是喝酒……也是一个神一样的好孩子呐。”</li></ul><p>想要成为一个人，就必须接受自己会犯错，会让人讨厌的事实，不要在乎他人的眼光。</p>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SphereFace, CosFace, ArcFace</title>
      <link href="/2019/07/13/SphereFace-CosFace-ArcFace/"/>
      <url>/2019/07/13/SphereFace-CosFace-ArcFace/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/feature.gif" alt="feature"></p><blockquote><p>文中合成<code>gif</code>图不清晰，原图可在<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/tree/master/2019/07/13/SphereFace-CosFace-ArcFace/gif" target="_blank" rel="noopener">该页面</a>下载获取</p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>深度神经网络可将样本映射到超空间中的嵌入向量，然而若不在损失中增加几何约束，该超空间中的嵌入向量不具有几何意义。现对空间中嵌入向量间的余弦距离研究甚是火热，可用于<code>open-set</code>数据集的识别问题，特别是人脸验证问题。</p><blockquote><p>所谓<code>open-set</code>与<code>close-set</code>，是指训练集中是否包含测试集中的类别，如人脸问题中，训练集不可能包含所有人的人脸数据，则如何识别未出现在训练集中的样本成为一个问题。<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/open-close-set.jpg" alt="open-close-set"></p></blockquote><p>目前对嵌入向量的约束方案有如下几种</p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/intra-interstrategy.jpg" alt="intra-interstrategy"></p><ul><li>Centre Loss，惩罚样本对应特征与其对应类别中心之间的欧氏距离，以获得类内紧致性，本文不做介绍；</li><li>Triplet Loss，在训练样本中寻找三元组，组合数量爆炸，且计算量较大，本文不做介绍；</li><li>CosFace: Additive Cosine Margin；</li><li>SphereFace: Multiplicative Angular Margin；</li><li>ArcFace: Additive Angular Margin。</li></ul><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="Maltilabel-Softmax-CrossEnt"><a href="#Maltilabel-Softmax-CrossEnt" class="headerlink" title="Maltilabel: Softmax + CrossEnt"></a>Maltilabel: Softmax + CrossEnt</h2><p>在多分类问题中，网络输出层一般设置为<code>Softmax</code>层</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_j = \frac{e^{f^{(i)}_j}}{\sum_k e^{f^{(i)}_k}} \tag{1}</script><p>其中$f^{(i)}_j$表示<code>Softmax</code>层输入，一般为前一层网络的线性输出。损失函数一般采用交叉熵<code>Cross Entropy</code></p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \tilde{y}^{(i)}_{y^{(i)}} \tag{2}</script><p>其中，$N$为<code>batchsize</code>大小，$\tilde{y}^{(i)}_j$表示样本$X^{(i)}$预测为第$j$类的概率输出，$y^{(i)}$为其对应真实标签。</p><p>也即</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{f^{(i)}_{y^{(i)}}}}{\sum_k e^{f^{(i)}_k}} \tag{*}</script><blockquote><script type="math/tex; mode=display">H(p, q) = \sum_x p(x) \log \frac{1}{q(x)}</script><p>其中$p(x), q(x)$为概率分布。</p></blockquote><h2 id="Geometric-Modified-Softmax-Loss"><a href="#Geometric-Modified-Softmax-Loss" class="headerlink" title="Geometric: Modified Softmax Loss"></a>Geometric: Modified Softmax Loss</h2><p>由于<code>Softmax</code>不具有几何意义，对其进行修改，在前一层与<code>Softmax</code>层之间添加线性层，即</p><script type="math/tex; mode=display">g^{(i)}_j = W_j^T f^{(i)} + b_j \tag{3}</script><p>其中</p><script type="math/tex; mode=display">W_j^T f^{(i)} = ||W_j|| ||f^{(i)}|| \cos \theta^{(i)}_j \tag{4}</script><p>$\theta^{(i)}_j$表示$W_j$与$f^{(i)}$夹角，$f^{(i)}$为上一层输出，其维数为$D$，则参数$W, b$维数应为$C \times D, C \times 1$。</p><p>此时类别$c_1$与$c_2$间决策平面为</p><script type="math/tex; mode=display">W_{c_1}^T f^{(i)} + b_{c_1} = W_{c_2}^T f^{(i)} + b_{c_2}</script><p>令$||W_j|| = 1, b_j = 0$，有</p><script type="math/tex; mode=display">g^{(i)}_j = W_j^T f^{(i)} + b_j = ||f^{(i)}|| \cos \theta^{(i)}_j \tag{5}</script><p>则判决平面为</p><script type="math/tex; mode=display">||f^{(i)}|| \cos \theta^{(i)}_{c_1} = ||f^{(i)}|| \cos \theta^{(i)}_{c_2}</script><p>也即</p><script type="math/tex; mode=display">\cos \theta^{(i)}_{c_1} = \cos \theta^{(i)}_{c_2}\tag{6}</script><p>则此时向量夹角大小可作为判决平面，赋予角度上的几何意义，将$g^{(i)}$输入<code>Softmax</code>层与<code>CrossEnt</code>损失</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_j = \frac{e^{g^{(i)}_j}}{\sum_k e^{g^{(i)}_k}} = \frac{e^{||f^{(i)}|| \cos \theta^{(i)}_j}}{\sum_k e^{||f^{(i)}|| \cos \theta^{(i)}_k}}</script><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \tilde{y}^{(i)}_{y^{(i)}}</script><p>写成一个等式，在<code>SphereFace</code>一文中，称下式为<code>Modified Softmax Loss</code></p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{||f^{(i)}|| \cos \theta^{(i)}_{y^{(i)}}}}{\sum_k e^{||f^{(i)}|| \cos \theta^{(i)}_k}} \tag{*1}</script><p>其与<code>Softmax</code>对比如下<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/softmax-a-softmax.jpg" alt="softmax-a-softmax"></p><h2 id="CosFace-Additive-Cosine-Margin"><a href="#CosFace-Additive-Cosine-Margin" class="headerlink" title="CosFace: Additive Cosine Margin"></a>CosFace: Additive Cosine Margin</h2><p>在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin。</p><p>希望对于样本$X^{(i)}$，其对应的嵌入向量$f^{(i)}$与其所属类中心向量$W_{y^{(i)}}$间夹角$\theta^{(i)}_{y^{(i)}}$愈小愈好，由余弦函数单调性，可得</p><script type="math/tex; mode=display">\min \theta^{(i)}_{y^{(i)}} \Rightarrow \max \cos \theta^{(i)}_{y^{(i)}}</script><p>引入Margin参数$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong>，也即</p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = \arccos (\cos \theta^{(i)}_{c_{y^{(i)}}} - m) > \theta^{(i)}_{c_{y^{(i)}}}</script><p>从而</p><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos \theta^{(i)}_{c_{y^{(i)}}} - m < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{7}</script><p>所以</p><script type="math/tex; mode=display">m > 0</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">\cos \theta^{(i)}_{c_{y^{(i)}}} - m = \cos \theta^{(i)}_{c_j}</script><p>那么$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{||f^{(i)}|| (\cos \theta^{(i)}_{y^{(i)}} - m)}}{e^{||f^{(i)}|| (\cos \theta^{(i)}_{y^{(i)}} - m)} + \sum_{k \neq y^{(i)}} e^{||f^{(i)}|| \cos \theta^{(i)}_k}}, 其中m > 0 \tag{8}</script><p>由于决策平面取决于角度，与$||f^{(i)}||$分布无关，故设置为常数$s$，有<code>Additive Cosine Margin Loss</code></p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s (\cos \theta^{(i)}_{y^{(i)}} - m)}}{e^{s (\cos \theta^{(i)}_{y^{(i)}} - m)} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m > 0 \tag{*2}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>$m$典型值为</p><script type="math/tex; mode=display">m = 0.35</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cosface.jpg" alt="cosface"></p><blockquote><p>实际上，$s$可控制决策平面的陡峭程度，如在二分类问题中，采用<code>Softmax</code>进行决策时</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_1 = \frac{e^{s f^{(i)}_1}}{e^{s f^{(i)}_1} + e^{s f^{(i)}_2}} = \frac{1}{1 + e^{-s (f^{(i)}_1 - f^{(i)}_2)}}</script><p>记$z^{(i)}_1 = f^{(i)}_1 - f^{(i)}_2$，有<code>Sigmoid</code>函数</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_1 = \frac{1}{1 + e^{-s z^{(i)}_1}}</script><p>设置不同的$s$参数，得到图像如下<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/parameter_s.png" alt="parameter_s"></p><p>从另一角度理解，可设$s = \frac{1}{\sigma^2}$，则$s$也可用于描述分布的方差，本文中，这里指角度的分布方差。</p></blockquote><h2 id="SphereFace-Multiplicative-Angular-Margin"><a href="#SphereFace-Multiplicative-Angular-Margin" class="headerlink" title="SphereFace: Multiplicative Angular Margin"></a>SphereFace: Multiplicative Angular Margin</h2><p>与<a href="#CosFace-Additive-Cosine-Margin">CosFace: Additive Cosine Margin</a>思路一致，在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin，不同的是对角度增加乘法裕度。</p><p>引入乘子$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong></p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = m \theta^{(i)}_{y^{(i)}}</script><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos m \theta^{(i)}_{y^{(i)}} < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{9}</script><p>所以$m &gt; 1$，原文中去整数，即$m \geq 2$，同时，沿用<code>CosFace</code>中对$||f^{(i)}||$处理，$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos m \theta^{(i)}_{y^{(i)}}}}{e^{s \cos m \theta^{(i)}_{y^{(i)}}} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m(\text{integer}) \geq 2 \tag{*3}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">m \theta^{(i)}_{c_{y^{(i)}}} = \theta^{(i)}_{c_j}</script><p>$m$典型值为</p><script type="math/tex; mode=display">m = 2</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/sphereface.jpg" alt="sphereface"></p><h2 id="ArcFace-Additive-Angular-Margin"><a href="#ArcFace-Additive-Angular-Margin" class="headerlink" title="ArcFace: Additive Angular Margin"></a>ArcFace: Additive Angular Margin</h2><p>与<a href="#CosFace-Additive-Cosine-Margin">CosFace: Additive Cosine Margin</a>思路一致，在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin，对角度增加加法裕度。</p><p>引入$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong></p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = \theta^{(i)}_{y^{(i)}} + m</script><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos (\theta^{(i)}_{y^{(i)}} + m) < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{9}</script><p>所以$m &gt; 0$，同时，沿用<code>CosFace</code>中对$||f^{(i)}||$处理，$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos (\theta^{(i)}_{y^{(i)}} + m)}}{e^{s \cos (\theta^{(i)}_{y^{(i)}} + m)} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m > 0 \tag{*4}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>$m$典型值为</p><script type="math/tex; mode=display">m = 0.5</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">\theta^{(i)}_{c_{y^{(i)}}} + m = \theta^{(i)}_{c_j}</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arcface.jpg" alt="arcface"></p><h1 id="可改进之处"><a href="#可改进之处" class="headerlink" title="可改进之处"></a>可改进之处</h1><h2 id="CosMulFace"><a href="#CosMulFace" class="headerlink" title="CosMulFace?"></a>CosMulFace?</h2><p>好像还有一个空余的位置可以添加参数，笑:-)。</p><p>与<a href="#CosFace-Additive-Cosine-Margin">CosFace: Additive Cosine Margin</a>思路一致，在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上添加Margin，不同的是对余弦值增加乘法裕度。</p><p>引入乘子$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$<strong>惩罚更大</strong></p><script type="math/tex; mode=display">\phi^{(i)}_{c_{y^{(i)}}} = \arccos (m \cos \theta^{(i)}_{c_{y^{(i)}}}) > \theta^{(i)}_{c_{y^{(i)}}}</script><script type="math/tex; mode=display">\cos \phi^{(i)}_{c_{y^{(i)}}} = m \cos \theta^{(i)}_{c_{y^{(i)}}} < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{10}</script><p>由于函数<code>cos(x)</code>有</p><script type="math/tex; mode=display">\begin{cases}    \cos x \geq 0 & x \in [-\frac{\pi}{2} + 2k\pi, \frac{\pi}{2} + 2k\pi] \\    \cos x \leq 0 & x \in [\frac{\pi}{2} + 2k\pi, \frac{3\pi}{2} + 2k\pi]\end{cases}, 且 -1 \leq \cos x \leq 1</script><p>所以为使得对于乘子$m$，均有式$(9)$成立，令</p><script type="math/tex; mode=display">\cos \theta^{(i)}_j := \cos \theta^{(i)}_j - 1 \tag{11}</script><p>所以$m &gt; 1$，同时，沿用<code>CosFace</code>中对$||f^{(i)}||$处理，$(*1)$更新为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s m (\cos \theta^{(i)}_{y^{(i)}} - 1)}}{e^{s m (\cos \theta^{(i)}_{y^{(i)}} - 1)} + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}}, 其中 m > 1 \tag{*5}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><p>此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为</p><script type="math/tex; mode=display">m (\cos \theta^{(i)}_{c_{y^{(i)}}} - 1) = \cos \theta^{(i)}_{c_j} - 1 \tag{12}</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cosmulface.jpg" alt="cosmulface"></p><h2 id="AdaptiveFace-Adaptive-Margin"><a href="#AdaptiveFace-Adaptive-Margin" class="headerlink" title="AdaptiveFace: Adaptive Margin?"></a>AdaptiveFace: Adaptive Margin?</h2><p>Margin必须为固定常数？</p><p>设置可变参数$m_i, i = 1, \cdots, 4$</p><p>可令</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cdot m^4 \left[\cos (m^1 \theta^{(i)} + m^2) - m^3 - 1\right] }}{e^{s \cdot m^4 \left[\cos (m^1 \theta^{(i)} + m^2) - m^3 - 1\right] } + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}} \tag{*6}</script><script type="math/tex; mode=display">\begin{cases}    \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\    m_1 > 1 \\    m_2 > 0 \\    m_3 > 0 \\    m_4 > 1\end{cases}</script><p>反向传播时，同时更新$m_i, i = 1, \cdots, 4$。</p><p>注意点：参数下限，每次参数更新后，需对其限制，即剪裁。</p><script type="math/tex; mode=display">f(x) = \begin{cases}    \text{min} & x < \text{min} \\    x          & otherwise \\    \text{max} & x > \text{max}\end{cases}</script><h2 id="船新版本"><a href="#船新版本" class="headerlink" title="船新版本"></a>船新版本</h2><!-- 先保密hhh，见[实验3](#3-%E4%BC%A0%E6%96%B0%E7%89%88%E6%9C%AC)。 --><p>在<a href="#Geometric-Modified-Softmax-Loss">Modified Softmax Loss</a>基础上，增加损失项</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos \theta^{(i)}_{y^{(i)}} }}{e^{s \cos \theta^{(i)}_{y^{(i)}} } + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}} + \lambda || \sum_j W_j ||_2 \tag{*7}</script><script type="math/tex; mode=display">s.t.\begin{cases}    \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\    || W_j ||_2 = 1 \\\end{cases}</script><p>即使得各类别所属向量的<strong>矢量和</strong>长度趋向$0$，由于最小化损失项$|| \sum_j W_j ||_2$，其最优解为</p><script type="math/tex; mode=display">W_j = \vec{0}</script><p>或者说$W_j$长度衰减至$0$，所以需要加上限制，<strong>在每次参数迭代后，重新归一化该向量，使其为单位向量</strong>，即</p><script type="math/tex; mode=display">W_j := \frac{W_j}{||W_j||}</script><h1 id="CosFace-SphereFace-ArcFace-CosMulFace"><a href="#CosFace-SphereFace-ArcFace-CosMulFace" class="headerlink" title="CosFace + SphereFace + ArcFace + CosMulFace"></a>CosFace + SphereFace + ArcFace + CosMulFace</h1><p>综合<a href="#CosFace-Additive-Cosine-Margin">CosFace</a>, <a href="#SphereFace-Multiplicative-Angular-Margin">SphereFace</a>, <a href="#ArcFace-Additive-Angular-Margin">ArcFace</a>, <a href="#CosMulFace">CosMulFace</a>, 得到</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cdot m_4 \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 - 1 \right] }}{e^{s \cdot m_4 \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 - 1 \right] } + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}} \tag{**}</script><script type="math/tex; mode=display">s.t. \qquad\begin{cases}    \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\    m_1 \geq 2 \\    m_2 > 0 \\    m_3 > 0 \\    m_4 > 1\end{cases}</script><p>通过组合可能获得更好的实验结果。</p><h1 id="方法对比"><a href="#方法对比" class="headerlink" title="方法对比"></a>方法对比</h1><p><code>ArcFace</code>与<code>CosFace</code>, <code>SphereFace</code>对比有如下特点</p><ol><li><p>数值计算稳定<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arcface1.jpg" alt="arcface1"></p></li><li><p>几何意义不同，决策平面区分度更好<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arcface2.jpg" alt="arcface2"></p></li></ol><h1 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h1><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/process.jpg" alt="process"></p><ol><li>初始化各类中心，记作矩阵$W_{C \times D}$，其中$C$表示类别数目， $D$表示特征维数；</li><li>迭代过程中，某批输入的样本记作$X_N$，其中$N$表示批次数据数；</li><li>计算该批中各样本$X^{(i)}$提取的特征$f^{(i)}$到各类别中心$W_j$的余弦值$\cos \theta^{(i)}_j$，保存为矩阵$Cos_{N \times C}$；</li><li><p>各样本真实标签对应的余弦值加上相应Margin，即</p><script type="math/tex; mode=display"> \cos \phi^{(i)}_j = \begin{cases}     m_4 (\text{monocos} (m_1 \theta^{(i)}_j + m_2) - m_3 - 1) & j = y^{(i)} \\     \cos \theta^{(i)}_j - 1 & \text{otherwise} \end{cases} \tag{14}</script><blockquote><p><code>monocos</code>在<a href="#cos%E5%87%BD%E6%95%B0%E7%9A%84%E5%8D%95%E8%B0%83%E6%80%A7%E9%97%AE%E9%A2%98">cos函数的单调性问题</a>中说明</p></blockquote></li><li><p>将计算得到的矩阵代入<code>Softmax</code>层，计算该样本属于各类别的概率</p><script type="math/tex; mode=display">\tilde{y}^{(i)}_j = \frac{e^{\cos \phi^{(i)}_j}}{\sum_k e^{\cos \phi^{(i)}_k}} \tag{15}</script></li><li><p>代入<code>Cross Entropy</code>计算损失值</p><script type="math/tex; mode=display">L^{(i)} = - \log \tilde{y}^{(i)}_{y^{(i)}} \tag{16}</script></li><li><p>则该批次的损失值为</p><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i L^{(i)} \tag{17}</script></li></ol><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>详情查看<a href="https://github.com/isLouisHsu/Toy-Problem-based-on-MNIST" target="_blank" rel="noopener">isLouisHsu/Toy-Problem-based-on-MNIST - Github</a>。</p><p>在实现过程中，有两个注意点：</p><ul><li>反余弦函数<code>arccos(x)</code>在$x = \pm 1$处不可导问题；</li><li>余弦函数<code>cos(x)</code>的单调性问题；</li></ul><p>对于上述两个问题，进行以下处理</p><h2 id="arccos函数不可导点问题"><a href="#arccos函数不可导点问题" class="headerlink" title="arccos函数不可导点问题"></a>arccos函数不可导点问题</h2><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/arccos.png" alt="arccos"></p><p>由于函数$\arccos(x)$的导函数为</p><script type="math/tex; mode=display">\frac{d}{dx} \arccos(x) = - \frac{1}{\sqrt{1 - x^2}} \tag{18}</script><p>特征$f^{(i)}$与各类中心$W_j$余弦值范围为</p><script type="math/tex; mode=display">\cos \theta^{(i)}_j \in [-1, 1]</script><p>则当$\cos \theta^{(i)}_j = \pm 1$时</p><script type="math/tex; mode=display">\lim_{\theta \rightarrow \pm 1} \frac{d}{dx} \arccos(x) = \infty</script><p>则无法使用BP算法进行参数更新，因此，使用泰勒展开式近似计算$\theta^_j$</p><script type="math/tex; mode=display">\arccos x = \frac{\pi}{2} - x - \frac{1}{2} \cdot \frac{x^3}{3} - \cdots - \frac{(2n-1)!}{(2n)!} \cdot \frac{x^{2n+1}}{2n+1}- \cdots \tag{19}</script><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/taylor_arccos,_n_=_5.png" alt="taylor_arccos,_n_=_5"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">taylorArccos</span><span class="params">(x, n)</span>:</span></span><br><span class="line"></span><br><span class="line">    general_term = <span class="keyword">lambda</span> x, n: (math.factorial(<span class="number">2</span> * n - <span class="number">1</span>) /\</span><br><span class="line">                                math.factorial(<span class="number">2</span> * n)) *\</span><br><span class="line">                                (x**(<span class="number">2</span> * n + <span class="number">1</span>) / (<span class="number">2</span> * n + <span class="number">1</span>))</span><br><span class="line">    y = np.pi / <span class="number">2</span> - x</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        y -= general_term(x, i)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><h2 id="cos函数的单调性问题"><a href="#cos函数的单调性问题" class="headerlink" title="cos函数的单调性问题"></a>cos函数的单调性问题</h2><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cos.png" alt="cos"></p><p>由于特征$f^{(i)}$与各类中心$W_j$余弦值范围为</p><script type="math/tex; mode=display">\cos \theta^{(i)}_j \in [-1, 1]</script><p>利用反三角函数<code>arccos(x)</code>计算其角度后，有</p><script type="math/tex; mode=display">\theta^{(i)}_j \in [0, \pi]</script><p>仅考虑余弦函数内部部分，添加Margin后，应有</p><script type="math/tex; mode=display">\phi^{(i)}_j = m_1 \theta^{(i)}_j + m_2 \in [m_2, m_2 + m_1 \pi]</script><p>此时仍然满足</p><script type="math/tex; mode=display">\phi^{(i)}_j > \theta^{(i)}_j</script><p>然而经过余弦函数后，可能由于其单调性问题，<strong>不一定满足下式</strong></p><script type="math/tex; mode=display">\cos \phi^{(i)}_j < \cos \theta^{(i)}_j</script><p>所以定义如下函数，使其在$[0, \infty]$单调递减</p><script type="math/tex; mode=display">\text{monocos}(\theta) = \cos (\theta - n \pi) - 2n 其中 n = \lfloor{} \frac{\theta}{\pi} \rfloor{} \tag{19}</script><p>此时必满足</p><script type="math/tex; mode=display">\text{monocos} \phi^{(i)}_j < \text{monocos} \theta^{(i)}_j</script><blockquote><p>由于$\phi^{(i)}_j &gt; \theta^{(i)}_j$，所以满足$[0, \infty]$即可，实际上，该函数在$[-\infty, \infty]$均单调递减。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">monocos</span><span class="params">(x)</span>:</span></span><br><span class="line">    n = x // np.pi</span><br><span class="line">    y = np.cos(x - np.pi*n) - <span class="number">2</span>*n</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>其函数图像如下<br><img src="/2019/07/13/SphereFace-CosFace-ArcFace/cos_&amp;_monocos.png" alt="cos_&amp;_monocos"></p><h2 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        weight: &#123;Parameter(num_classes, feature_size)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, feature_size)</span>:</span></span><br><span class="line">        super(CosineLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.weights = Parameter(torch.Tensor(num_classes, feature_size))</span><br><span class="line">        nn.init.xavier_uniform_(self.weights)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            x: &#123;tensor(N, feature_size)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            \cos \theta^&#123;(i)&#125;_j = \frac&#123;W_j^T f^&#123;(i)&#125;&#125;&#123;||W_j|| ||f^&#123;(i)&#125;||&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = F.linear(F.normalize(x), F.normalize(self.weights))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NetworkMargin</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, feature_size)</span>:</span></span><br><span class="line">        super(NetworkMargin, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.pre_layers = nn.Sequential(</span><br><span class="line">            nn.Conv2d(   <span class="number">1</span>,  <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d(  <span class="number">64</span>,  <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            nn.Conv2d( <span class="number">64</span>,  feature_size, <span class="number">7</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.cosine_layer = CosineLayer(num_classes, feature_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        x = self.pre_layers(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line">        x = self.get_feature(x)</span><br><span class="line">        x = self.cosine_layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  x</span><br></pre></td></tr></table></figure><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarginProduct</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string">        \text&#123;softmax&#125; = \frac&#123;1&#125;&#123;N&#125; \sum_i -\log \frac&#123;e^&#123;\tilde&#123;y&#125;_&#123;y_i&#125;&#125;&#125;&#123;\sum_i e^&#123;\tilde&#123;y&#125;_i&#125;&#125;</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        $\text&#123;where&#125;$</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string">        \tilde&#123;y&#125; = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">            s(m4 \cos(m_1 \theta_&#123;j, i&#125; + m_2) + m_3) &amp; j = y_i \\</span></span><br><span class="line"><span class="string">            s(m4 \cos(    \theta_&#123;j, i&#125;))             &amp; j \neq y_i</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string">        $$</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, s=<span class="number">32.0</span>, m1=<span class="number">2.00</span>, m2=<span class="number">0.50</span>, m3=<span class="number">0.35</span>, m4=<span class="number">2.00</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(MarginProduct, self).__init__()</span><br><span class="line">        self.s = s</span><br><span class="line">        self.m1 = m1</span><br><span class="line">        self.m2 = m2</span><br><span class="line">        self.m3 = m3</span><br><span class="line">        self.m4 = m4</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, cosTheta, label)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            cosTheta: &#123;tensor(N, n_classes)&#125; 每个样本(N)，到各类别(n_classes)矢量的余弦值</span></span><br><span class="line"><span class="string">            label:  &#123;tensor(N)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output: &#123;tensor(N, n_classes)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        one_hot = torch.zeros(cosTheta.size(), device=<span class="string">'cuda'</span> <span class="keyword">if</span> \</span><br><span class="line">                        torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">        one_hot.scatter_(<span class="number">1</span>, label.view(<span class="number">-1</span>, <span class="number">1</span>).long(), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># theta  = torch.acos(cosTheta)</span></span><br><span class="line">        theta  = arccos(cosTheta)</span><br><span class="line">        cosPhi = self.m4 * (monocos(self.m1*theta + self.m2) - self.m3 - <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        output = torch.where(one_hot &gt; <span class="number">0</span>, cosPhi, cosTheta - <span class="number">1</span>)</span><br><span class="line">        output = self.s * output</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarginLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, s=<span class="number">32.0</span>, m1=<span class="number">2.00</span>, m2=<span class="number">0.50</span>, m3=<span class="number">0.35</span>, m4=<span class="number">2.00</span>)</span>:</span></span><br><span class="line">        super(MarginLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.margin = MarginProduct(s, m1, m2, m3, m4)</span><br><span class="line">        self.crossent = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, gt)</span>:</span></span><br><span class="line"></span><br><span class="line">        output = self.margin(pred, gt)</span><br><span class="line">        loss   = self.crossent(output, gt)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><strong>注意，以下实验网络的参数初始均相同。</strong></p><h2 id="1-设置不同参数，对比实验结果"><a href="#1-设置不同参数，对比实验结果" class="headerlink" title="1.设置不同参数，对比实验结果"></a>1.设置不同参数，对比实验结果</h2><script type="math/tex; mode=display">L = \frac{1}{N} \sum_i - \log \frac{e^{s \cdot \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 \right] }}{e^{s \cdot \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 \right] } + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}} \tag{**}</script><script type="math/tex; mode=display">s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}</script><div class="table-container"><table><thead><tr><th>Margin type</th><th>Modified</th><th>CosFace</th><th>SphereFace</th><th>ArcFace</th><th>ArcFace</th><th>ArcFace</th><th>ArcFace</th></tr></thead><tbody><tr><td>$s$</td><td>8</td><td>8</td><td>8</td><td><strong>16</strong></td><td><strong>8</strong></td><td><strong>4</strong></td><td><strong>1</strong></td></tr><tr><td>$m_1(m_1&gt;=2)$</td><td>1</td><td>1</td><td><strong>2</strong></td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>$m_2(m_2&gt;0)$</td><td>0</td><td>0</td><td>0</td><td><strong>0.5</strong></td><td><strong>0.5</strong></td><td><strong>0.5</strong></td><td><strong>0.5</strong></td></tr><tr><td>$m_3(m_3&gt;0)$</td><td>0</td><td><strong>0.35</strong></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div><ol><li>参数$s$对实验结果影响不大；</li><li><code>CosFace</code>, <code>SphereFace</code>, <code>ArcFace</code>三种损失作用下，角度的区分度均比<code>Modified Softmax</code>效果好。</li></ol><h3 id="嵌入向量维度为2"><a href="#嵌入向量维度为2" class="headerlink" title="嵌入向量维度为2"></a>嵌入向量维度为2</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp1_dim2.png" alt="exp1_dim2"></p><h3 id="嵌入向量维度为3"><a href="#嵌入向量维度为3" class="headerlink" title="嵌入向量维度为3"></a>嵌入向量维度为3</h3><blockquote><p>使用<code>imageio.mimread()</code>函数，读取出<code>.gif</code>会改变原图颜色，很迷。</p></blockquote><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp1_dim3.gif" alt="exp1_dim3"></p><h2 id="2-改进的方法实验"><a href="#2-改进的方法实验" class="headerlink" title="2. 改进的方法实验"></a>2. 改进的方法实验</h2><div class="table-container"><table><thead><tr><th>Margin type</th><th>CosMulFace</th><th>CosMulFace</th><th>CosMulFace</th><th>AdaptiveFace</th></tr></thead><tbody><tr><td>$s$</td><td>8</td><td>8</td><td>8</td><td>8</td></tr><tr><td>$m_1(m_1&gt;=2)$</td><td>1</td><td>1</td><td>1</td><td>/</td></tr><tr><td>$m_2(m_2&gt;0)$</td><td>0</td><td>0</td><td>0</td><td>/</td></tr><tr><td>$m_3(m_3&gt;0)$</td><td>0</td><td>0</td><td>0</td><td>/</td></tr><tr><td>$m_4(m_4 &gt; 1)$</td><td><strong>2.00</strong></td><td><strong>3.00</strong></td><td><strong>4.00</strong></td><td>/</td></tr></tbody></table></div><ul><li><code>CosMulFace</code>在$m=2, 4$时效果良好。</li><li><code>AdaptiveFace</code>得到自适应参数如下(?)<script type="math/tex; mode=display">  \begin{cases}      m_1 = 1 \\      m_2 = 0 \\      m_3 = 0 \\      m_4 = 6.8345 \\  \end{cases}</script></li></ul><h3 id="嵌入向量维度为2-1"><a href="#嵌入向量维度为2-1" class="headerlink" title="嵌入向量维度为2"></a>嵌入向量维度为2</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp2_dim2.png" alt="exp2_dim2"></p><h3 id="嵌入向量维度为3-1"><a href="#嵌入向量维度为3-1" class="headerlink" title="嵌入向量维度为3"></a>嵌入向量维度为3</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp2_dim3.gif" alt="exp2_dim3"></p><h2 id="3-传新版本"><a href="#3-传新版本" class="headerlink" title="3. 传新版本"></a>3. 传新版本</h2><p>可见其角度分布更加均匀，从而区分度更大。</p><h3 id="嵌入向量维度为2-2"><a href="#嵌入向量维度为2-2" class="headerlink" title="嵌入向量维度为2"></a>嵌入向量维度为2</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp3_dim2.png" alt="exp3_dim2"></p><h3 id="嵌入向量维度为3-2"><a href="#嵌入向量维度为3-2" class="headerlink" title="嵌入向量维度为3"></a>嵌入向量维度为3</h3><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/exp3_dim3.gif" alt="exp3_dim3"></p><p>上图和#$%@%一样，如下为<code>scatter_lda8</code>原图</p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda8.gif" alt="scatter_lda8"></p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda8_spherized.gif" alt="scatter_lda8_spherized"></p><p>数据类别过多可能不够明显，现选择4类，在设置$\lambda=0$与$\lambda=16$时，各类别的三维分布如下两图，区别很明显</p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda0_c4_.gif" alt="scatter_lda0_c4_"></p><p><img src="/2019/07/13/SphereFace-CosFace-ArcFace/gif/scatter_lda16_c4_.gif" alt="scatter_lda16_c4_"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1801.07698v1" target="_blank" rel="noopener">ArcFace: Additive Angular Margin Loss for Deep Face Recognition - arxiv.org</a></li><li><a href="https://arxiv.org/abs/1801.09414" target="_blank" rel="noopener">CosFace: Large Margin Cosine Loss for Deep Face Recognition - arxiv.org</a></li><li><a href="https://arxiv.org/abs/1704.08063" target="_blank" rel="noopener">SphereFace: Deep Hypersphere Embedding for Face Recognition - arxiv.org</a></li><li><a href="https://github.com/deepinsight/insightface" target="_blank" rel="noopener">deepinsight/insightface - Github</a></li><li><a href="https://github.com/wy1iu/sphereface" target="_blank" rel="noopener">wy1iu/sphereface - Github</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python迭代器与生成器</title>
      <link href="/2019/07/11/Python%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/"/>
      <url>/2019/07/11/Python%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在读取大数据量的文件时，使用迭代器和生成器可减少内存开销。</p><h1 id="迭代器-Iterator"><a href="#迭代器-Iterator" class="headerlink" title="迭代器(Iterator)"></a>迭代器(Iterator)</h1><p>迭代是Python访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。</p><h2 id="内建函数iter-与next"><a href="#内建函数iter-与next" class="headerlink" title="内建函数iter()与next()"></a>内建函数iter()与next()</h2><p>该函数将可迭代对象转换为迭代器，支持的数据容器对象如<code>string</code>、<code>list</code>、<code>dict</code>、<code>tuple</code>等，使用方法如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 创建可迭代对象</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; a = "iterator"</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; b = [_ for _ in a]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; c = dict(zip(a, range(len(a))))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; d = tuple(a)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; a</span><br><span class="line">'iterator'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; b</span><br><span class="line">['i', 't', 'e', 'r', 'a', 't', 'o', 'r']</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; c</span><br><span class="line">&#123;'i': 0, 't': 5, 'e': 2, 'r': 7, 'a': 4, 'o': 6&#125;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; d</span><br><span class="line">('i', 't', 'e', 'r', 'a', 't', 'o', 'r')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 调用`iter()`函数将可迭代对象转换为迭代器</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; all = [a, b, c, d]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; iterAll = list(map(iter, all))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; all</span><br><span class="line">['iterator', ['i', 't', 'e', 'r', 'a', 't', 'o', 'r'], &#123;'i': 0, 't': 5, 'e': 2, 'r': 7, 'a': 4, 'o': 6&#125;, ('i', 't', 'e', 'r', 'a', 't', 'o', 'r')]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; iterAll</span><br><span class="line">[&lt;str_iterator object at 0x00000216C399D6A0&gt;, &lt;list_iterator object at 0x00000216C399D6D8&gt;, &lt;dict_keyiterator object at 0x00000216C37672C8&gt;, &lt;tuple_iterator object at 0x00000216C399D710&gt;]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 反复调用`next()`函数取出迭代器值</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'i'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'t'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'e'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'r'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'a'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'t'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'o'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">'r'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 迭代结束</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(iterAll[0])</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure></p><h2 id="for语法糖"><a href="#for语法糖" class="headerlink" title="for语法糖"></a>for语法糖</h2><p>实际中对可迭代对象进行迭代时，上述这般麻烦。在Python的循环语句<code>for</code>已对其进行包装，内部调用函数<code>iter()</code>和<code>next()</code>，返回可迭代对象元素<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; a</span><br><span class="line">'iterator'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for i in a:</span><br><span class="line">...     print(i)</span><br><span class="line">...</span><br><span class="line">i</span><br><span class="line">t</span><br><span class="line">e</span><br><span class="line">r</span><br><span class="line">a</span><br><span class="line">t</span><br><span class="line">o</span><br><span class="line">r</span><br></pre></td></tr></table></figure></p><h2 id="自定义迭代器-敲黑板划重点！"><a href="#自定义迭代器-敲黑板划重点！" class="headerlink" title="自定义迭代器(敲黑板划重点！)"></a>自定义迭代器(敲黑板划重点！)</h2><p>将自定义类定义为可迭代对象的实现方法是，实现魔术方法<code>__iter__()</code>与<code>__next__()</code>。</p><p>例如，要求返回斐波那契数列前20个值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fibonacci</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, m)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.m = m</span><br><span class="line">        self.n, self.a, self.b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.n &lt; self.m:</span><br><span class="line">            t = self.b</span><br><span class="line">            self.a, self.b = self.b, self.a + self.b</span><br><span class="line">            self.n += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> t</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br></pre></td></tr></table></figure></p><p>在命令行中执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from iter import Fibonacci</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g = Fibonacci(20)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g</span><br><span class="line">&lt;iter.Fibonacci object at 0x000001F359927E10&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; while True:</span><br><span class="line">...     try:</span><br><span class="line">...             next(g)</span><br><span class="line">...     except StopIteration:</span><br><span class="line">...             break</span><br><span class="line">...</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">5</span><br><span class="line">8</span><br><span class="line">13</span><br><span class="line">21</span><br><span class="line">34</span><br><span class="line">55</span><br><span class="line">89</span><br><span class="line">144</span><br><span class="line">233</span><br><span class="line">377</span><br><span class="line">610</span><br><span class="line">987</span><br><span class="line">1597</span><br><span class="line">2584</span><br><span class="line">4181</span><br><span class="line">6765</span><br></pre></td></tr></table></figure></p><blockquote><p>在<code>Pytorch</code>中，数据集<code>Dataset</code>定义时，重写函数<code>__getitem__()</code>与<code>__len__()</code>，并不是可迭代对象，而<code>Dataloader</code>为可迭代对象，详细源码可查看<a href="https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py/" target="_blank" rel="noopener">Github: Pytorch</a></p></blockquote><h1 id="生成器-Generator"><a href="#生成器-Generator" class="headerlink" title="生成器(Generator)"></a>生成器(Generator)</h1><p>调用一个生成器函数，返回的是一个迭代器对象。在调用生成器运行的过程中，每次遇到<code>yield</code>时函数会暂停并保存当前所有的运行信息，返回<code>yield</code>的值, 并在下一次执行<code>next()</code>方法时从当前位置继续运行。</p><h2 id="推导式定义生成器"><a href="#推导式定义生成器" class="headerlink" title="推导式定义生成器"></a>推导式定义生成器</h2><p>语法类似列表推导式，不同的是将<code>[]</code>改为<code>()</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; g = (i for i in range(20))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g</span><br><span class="line">&lt;generator object &lt;genexpr&gt; at 0x00000161D16F44F8&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">0</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">2</span><br></pre></td></tr></table></figure></p><h2 id="将函数定义为生成器"><a href="#将函数定义为生成器" class="headerlink" title="将函数定义为生成器"></a>将函数定义为生成器</h2><p>利用函数打印斐波那契数列<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(m)</span>:</span></span><br><span class="line">    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; m:</span><br><span class="line">        print(b)</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        n = n + <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>若需得到生成器，将<code>print()</code>改为<code>yield()</code>即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci_gen</span><span class="params">(m)</span>:</span></span><br><span class="line">    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; m:</span><br><span class="line">        <span class="keyword">yield</span>(b)</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        n = n + <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>在命令行中执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from iter import fibonacci_gen</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g = fibonacci_gen(20)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; g</span><br><span class="line">&lt;generator object fibonacci_gen at 0x000001F036DA44F8&gt;</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">2</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">3</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; next(g)</span><br><span class="line">5</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.runoob.com/python3/python3-iterator-generator.html" target="_blank" rel="noopener">Python3 迭代器与生成器 - 菜鸟教程</a></li><li><a href="https://www.cnblogs.com/tkqasn/p/5984090.html" target="_blank" rel="noopener">python 迭代器和生成器详解 - 博客园</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WSL - Windows Subsystem for Linux</title>
      <link href="/2019/07/11/WSL-Windows-Subsystem-for-Linux/"/>
      <url>/2019/07/11/WSL-Windows-Subsystem-for-Linux/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Windows远程登陆服务器，需要借助<a href="https://xshell.en.softonic.com/" target="_blank" rel="noopener">xshell</a>等软件。其实安装完子系统后，即可使用<code>ssh</code>登录，并且子系统与原系统隔离较好，不会产生影响。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ol><li><p>呼叫小娜，打开“启用或关闭Windows功能”，勾选“适用于Linux的Windows子系统”，并重启；<br> <img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl1.jpg" alt="wsl1"><br> <img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl2.jpg" alt="wsl2"></p></li><li><p>在巨硬软件商城(Microsoft Store)中下载合适的WSL，选择喜好的WSL进行安装</p></li></ol><ul><li><a href="https://www.microsoft.com/zh-cn/p/ubuntu-1604-lts/9pjn388hp8c9?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Ubuntu 16.04 LTS</a></li><li><a href="https://www.microsoft.com/zh-cn/p/ubuntu-1804-lts/9n9tngvndl3q?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Ubuntu 18.04 LTS</a></li><li><a href="https://www.microsoft.com/zh-cn/p/opensuse-leap-15/9n1tb6fpvj8c?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">OpenSUSE Leap 15</a></li><li><a href="https://www.microsoft.com/zh-cn/p/opensuse-leap-42/9njvjts82tjx?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">OpenSUSE Leap 42</a></li><li><a href="https://www.microsoft.com/zh-cn/p/suse-linux-enterprise-server-12/9p32mwbh6cns?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">SUSE Linux Enterprise Server 12</a></li><li><a href="https://www.microsoft.com/zh-cn/p/suse-linux-enterprise-server-15/9pmw35d7fnlx?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">SUSE Linux Enterprise Server 15</a></li><li><a href="https://www.microsoft.com/zh-cn/p/kali-linux/9pkr34tncv07?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Kali Linux</a></li><li><a href="https://www.microsoft.com/zh-cn/p/debian/9msvkqc78pk6?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Debian GNU/Linux</a></li><li><a href="https://www.microsoft.com/zh-cn/p/fedora-remix-for-wsl/9n6gdm4k2hnc?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Fedora Remix for WSL</a></li><li><a href="https://www.microsoft.com/zh-cn/p/pengwin/9nv1gv1pxz6p?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Pengwin</a></li><li><a href="https://www.microsoft.com/zh-cn/p/pengwin-enterprise/9n8lp0x93vcp?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Pengwin Enterprise</a></li><li><a href="https://www.microsoft.com/zh-cn/p/alpine-wsl/9p804crf0395?rtc=1&amp;activetab=pivot:overviewtab" target="_blank" rel="noopener">Alpine WSL</a></li></ul><ol><li>启动WSL，并添加用户和密码，即可使用<br><img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl3.jpg" alt="wsl3"><br><img src="/2019/07/11/WSL-Windows-Subsystem-for-Linux/wsl4.jpg" alt="wsl4"></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10" target="_blank" rel="noopener">Windows Subsystem for Linux Installation Guide for Windows 10 - Microsoft</a></li><li><a href="https://linux.cn/article-9545-1.html" target="_blank" rel="noopener">如何在Windows 10上开启WSL之旅</a></li><li><a href="https://www.cnblogs.com/skyshalo/p/7724072.html" target="_blank" rel="noopener">关于WSL(Windows上的Linux子系统)的简单介绍及安装 - CNBLOG</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Useful Terminal Control Sequences</title>
      <link href="/2019/05/28/Useful-Terminal-Control-Sequences/"/>
      <url>/2019/05/28/Useful-Terminal-Control-Sequences/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>ANSI</code>定义了用于屏幕显示的<code>Escape</code>屏幕控制码，打印输出到终端时，可指定输出颜色、格式等。</p><h1 id="基本格式"><a href="#基本格式" class="headerlink" title="基本格式"></a>基本格式</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\033[&lt;background color&gt;;&lt;front color&gt;m string to print \033[0m</span><br></pre></td></tr></table></figure><ul><li><code>\033[ xxxx m</code>为一个句段；</li><li><code>\033[0m</code>关闭所有属性；</li></ul><h1 id="光标控制"><a href="#光标控制" class="headerlink" title="光标控制"></a>光标控制</h1><div class="table-container"><table><thead><tr><th>ANSI控制码</th><th>含义</th></tr></thead><tbody><tr><td>\033[nA</td><td>光标上移n行</td></tr><tr><td>\033[nB</td><td>光标下移n行</td></tr><tr><td>\033[nC</td><td>光标右移n行</td></tr><tr><td>\033[nD</td><td>光标左移n行</td></tr><tr><td>\033[y;xH</td><td>设置光标位置</td></tr><tr><td>\033[2J</td><td>清屏</td></tr><tr><td>\033[K</td><td>清除从光标到行尾的内容</td></tr><tr><td>\033[s</td><td>保存光标位置</td></tr><tr><td>\033[u</td><td>恢复光标位置</td></tr><tr><td>\033[?25l</td><td>隐藏光标</td></tr><tr><td>\033[?25h</td><td>显示光标</td></tr></tbody></table></div><h1 id="颜色控制"><a href="#颜色控制" class="headerlink" title="颜色控制"></a>颜色控制</h1><div class="table-container"><table><thead><tr><th>ANSI控制码</th><th>含义</th></tr></thead><tbody><tr><td>\033[m</td><td>NONE</td></tr><tr><td>\033[0;32;31m</td><td>RED</td></tr><tr><td>\033[1;31m</td><td>LIGHT RED</td></tr><tr><td>\033[0;32;32m</td><td>GREEN</td></tr><tr><td>\033[1;32m</td><td>LIGHT GREEN</td></tr><tr><td>\033[0;32;34m</td><td>BULE</td></tr><tr><td>\033[1;34m</td><td>LIGHT BLUE</td></tr><tr><td>\033[1;30m</td><td>GRAY</td></tr><tr><td>\033[0;36m</td><td>CYAN</td></tr><tr><td>\033[1;36m</td><td>LIGHT CYAN</td></tr><tr><td>\033[0;35m</td><td>PURPLE</td></tr><tr><td>\033[1;35m</td><td>LIAGHT PURPLE</td></tr><tr><td>\033[0;33m</td><td>BROWN</td></tr><tr><td>\033[1;33m</td><td>YELLO</td></tr><tr><td>\033[0;37m</td><td>LIGHT GRAY</td></tr><tr><td>\033[1;37m</td><td>WHITE</td></tr></tbody></table></div><p>背景色与字体颜色符号不同</p><div class="table-container"><table><thead><tr><th>背景色</th><th>字体色</th></tr></thead><tbody><tr><td>40: 黑</td><td>30: 黑</td></tr><tr><td>41: 红</td><td>31: 红</td></tr><tr><td>42: 绿</td><td>32: 绿</td></tr><tr><td>43: 黄</td><td>33: 黄</td></tr><tr><td>44: 蓝</td><td>34: 蓝</td></tr><tr><td>45: 紫</td><td>35: 紫</td></tr><tr><td>46: 深绿</td><td>36: 深绿</td></tr><tr><td>47: 白色</td><td>37: 白色</td></tr></tbody></table></div><h1 id="格式控制"><a href="#格式控制" class="headerlink" title="格式控制"></a>格式控制</h1><div class="table-container"><table><thead><tr><th>ANSI控制码</th><th>含义</th></tr></thead><tbody><tr><td>\033[0m</td><td>关闭所有属性</td></tr><tr><td>\033[1m</td><td>设置高亮度</td></tr><tr><td>\033[4m</td><td>下划线</td></tr><tr><td>\033[5m</td><td>闪烁</td></tr><tr><td>\033[7m</td><td>反显</td></tr><tr><td>\033[8m</td><td>消隐</td></tr></tbody></table></div><h1 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h1><p>例如用python打印输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"\007"</span>)                       <span class="comment"># 发出提示音</span></span><br><span class="line">print(<span class="string">"\033[42:31m hello! \033[0m"</span>) <span class="comment"># 绿底红字` hello! ` </span></span><br><span class="line">print(<span class="string">"\033[4m"</span>)                    <span class="comment"># 开启下划线</span></span><br><span class="line">print(<span class="string">"\033[42:31m hello! \033[0m"</span>) <span class="comment"># 下划线绿底红字` hello! ` </span></span><br><span class="line">print(<span class="string">"\033[0m"</span>)                    <span class="comment"># 关闭所有格式</span></span><br><span class="line">print(<span class="string">"\033[2J"</span>)                    <span class="comment"># 清屏</span></span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://blog.csdn.net/lzuacm/article/details/8993785" target="_blank" rel="noopener">“\\033”(ESC)的用法-ANSI的Esc屏幕控制 - CSDN</a></li><li><a href="https://www.student.cs.uwaterloo.ca/~cs452/terminal.html" target="_blank" rel="noopener">Useful Terminal Control Sequences - student.cs.uwaterloo.ca</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine</title>
      <link href="/2019/05/27/Support-Vector-Machine/"/>
      <url>/2019/05/27/Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/p97_svm.py" target="_blank" rel="noopener">Github: isLouisHsu/Basic-Machine-Learning-Algorithm/algorithm/p97_svm.py</a></p><p>补一补支持向量机的笔记。</p><p>支持向量机<code>(SVM)</code>为有监督学习算法，可用于回归、分类甚至聚类(支持向量聚类)，其主要特点为</p><ul><li>将样本表示为超空间中的点；</li><li>求取支持向量，其余样本点对超平面无影响；</li><li>对于线性不可分问题，利用核函数映射到高维空间，使其线性可分；</li></ul><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>以下介绍支持向量机的基本原理，首先对一些概念作补充</p><h2 id="概念补充"><a href="#概念补充" class="headerlink" title="概念补充"></a>概念补充</h2><ol><li><p>$n$维空间与超平面<br> 如对于$n$维数据集，其所在空间即为$n$维欧式空间，则在该空间中，余维度为$1$的线性子空间，或称$n-1$维仿射子空间，称为超平面，为$n-1$维，可由下式确定</p><script type="math/tex; mode=display">w^T x + b = 0 \tag{1}</script><p> 其中$w,x$为$n$维列向量，$x$表示超平面上的点($n$维)，$w$表示超平面的法向量，决定超平面的方向，$b$为实数。</p><script type="math/tex; mode=display">x = (x_1, x_2, \cdots, x_n)^T</script><script type="math/tex; mode=display">w = (w_1, w_2, \cdots, w_n)^T</script><p> 例如$3$维空间中的$2$维平面方程为</p><script type="math/tex; mode=display">Ax + By + Cz + D = 0</script></li><li><p>点到超平面的距离<br> <img src="/2019/05/27/Support-Vector-Machine/distance.jpg" alt="distance"></p><p> 对于样本空间中任一点$x$，到超平面$\mathcal{P}_{w,b}$的距离，可表示为</p><script type="math/tex; mode=display">\mathcal{D} = \frac{|w^T x + b|}{||w||} \tag{2}</script><p> 其中$||w||$为向量$w$的$2$范数，即</p><script type="math/tex; mode=display">||w|| = \sqrt{w_1^2 + w_2^2 + \cdots + w_n^2} \tag{3}</script><blockquote><p>证明：假设超平面上一向量点为$x_0$，则向量$x-x_0$在单位法向量$\frac{w}{||w||}$上的投影$d$，即为向量点到超平面的距离</p><script type="math/tex; mode=display">d =  |\frac{w^T}{||w||} (x-x_0)|</script><p>而</p><script type="math/tex; mode=display">w^T x + b = 0 \Rightarrow w^T = -b</script><p>所以</p><script type="math/tex; mode=display">d =  | \frac{- w^T x_0 - b}{||w||} | = \frac{|w^T x_0 + b|}{||w||}</script></blockquote></li></ol><h2 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在前言中说到支持向量机使用核函数映射低维空间到高维空间，这是如何做到的？<br>假设有两个$n$维向量$x_i, x_j$，设有映射$\mathit{\Phi}$使其维度扩张到$n’$维，即</p><script type="math/tex; mode=display">x' = \mathit{\Phi}(x)</script><p>则定义核函数为</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = \mathit{\Phi}(x_i)^T \mathit{\Phi}(x_j) \tag{4}</script><h3 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h3><ol><li>线性<code>(linear)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = x_i^T x_j</script></li><li>多项式<code>(Polynomial)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = (\gamma x_i^T x_j + c)^n</script></li><li>高斯<code>(Gaussian)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}}</script></li><li>拉普拉斯<code>(Laplace)</code>核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||}{2\sigma}}</script></li><li>Sigmoid核函数<script type="math/tex; mode=display">\kappa(x_i, x_j) = \tanh (\gamma x_i^T x_j + c)</script></li></ol><p>等等。</p><p>至于如何选取核函数，需要技术人员一定的先验知识，或者使用超参数搜索确定。</p><h3 id="实例分析核函数的作用"><a href="#实例分析核函数的作用" class="headerlink" title="实例分析核函数的作用"></a>实例分析核函数的作用</h3><ol><li><p>多项式核函数<br> 指定多项式核函数参数$ \gamma = 1, c = 0, n = 2 $，即</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = (x_i^T x_j)^2 \tag{4.a}</script><p> 设原始空间为$2$维，即</p><script type="math/tex; mode=display">x_i = (x_{i1}, x_{i2})^T</script><p> 代入核函数$(4.a)$，有</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = \left[(x_{i1}, x_{i2}) (x_{j1}, x_{j2})^T \right]^2 = (x_{i1}x_{j1} + x_{i2}x_{j2})^2</script><script type="math/tex; mode=display">= x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2</script><p> 把以上$3$项多项式表示成$3$维向量内积，即</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = \left[ \begin{matrix} x_{i1}^2 & \sqrt{2}x_{i1}x_{i2} & x_{i2}^2 \end{matrix} \right]                          \left[ \begin{matrix} x_{j1}^2 & \sqrt{2}x_{j1}x_{j2} & x_{j2}^2 \end{matrix} \right]^T</script><p> 由上式与式$(4)$可得，该核函数对应的映射函数为</p><script type="math/tex; mode=display">\mathit{\Phi}(x) = \left[ \begin{matrix} x_1^2 & \sqrt{2}x_1x_2 & x_2^2 \end{matrix} \right]^T \tag{4.b}</script><p> 这样就把$2$维空间中的点映射到了$3$维空间，作图如下。<br> <img src="/2019/05/27/Support-Vector-Machine/kernel_poly.jpg" alt="kernel_poly"></p></li></ol><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X = np.array([</span><br><span class="line">[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">1</span>],[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">5</span>,<span class="number">2</span>],</span><br><span class="line">[<span class="number">6</span>,<span class="number">1</span>],[<span class="number">6</span>,<span class="number">2</span>],[<span class="number">6</span>,<span class="number">3</span>],[<span class="number">6</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">])</span><br><span class="line">Y=np.array([<span class="number">-1</span>] * <span class="number">14</span> + [<span class="number">1</span>] * <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">0</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据映射到高维后显示</span></span><br><span class="line"><span class="comment"># 映射函数为Φ(x)=[x1^2 √2*x1*x2 x2^2]</span></span><br><span class="line">FX = np.zeros((<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">    tmp = np.array([X[i, <span class="number">0</span>]**<span class="number">2</span>, np.sqrt(<span class="number">2</span>)*X[i, <span class="number">0</span>]*X[i, <span class="number">1</span>], X[i, <span class="number">1</span>]**<span class="number">2</span>]).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    FX = np.r_[FX, tmp]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(<span class="number">1</span>)</span><br><span class="line">figAx = Axes3D(fig)</span><br><span class="line">figAx.scatter(FX[:, <span class="number">0</span>], FX[:, <span class="number">1</span>], FX[:, <span class="number">2</span>], c=Y)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></code></pre><ol><li><p>高斯核函数</p><script type="math/tex; mode=display">\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}} = e^{c||x_i - x_j||^2} \tag{4.c}</script><p> 其中包含指数函数，其级数展开式为</p><script type="math/tex; mode=display">e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!} = \sum_{i=0}^{n} \frac{x^i}{i!} + R(n)</script><p> 因此可将样本点映射到无穷维度。</p><ul><li><p>输入维度为$1$</p><script type="math/tex; mode=display">||x_i - x_j||^2 = (x_i - x_j)^2 =x_i^2 - 2x_ix_j + x_j^2</script><p>  则代入核函数$(4.c)$得到</p><script type="math/tex; mode=display">e^{c||x_i - x_j||^2}   = e^{c(x_i^2 - 2x_ix_j + x_j^2)}  = e^{c(x_i^2 + x_j^2)} e^{-2c x_i x_j}</script><p>  其中</p><script type="math/tex; mode=display">  e^{-2c x_i x_j} = \sum_{n=0}^{\infty} \frac{(-2c)^n x_i^n x_j^n}{n!} = 1 - 2cx_ix_j + \frac{4 c^2 x_i^2 x_j^2}{2} - \frac{8 c^3 x_i^3 x_j^3}{6} + \cdots</script><script type="math/tex; mode=display">  = \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_i^n}, \cdots \end{matrix} \right]  \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_j^n}, \cdots \end{matrix} \right]^T</script><p>  所以</p><script type="math/tex; mode=display">  e^{c||x_i - x_j||^2}  = e^{c(x_i^2 + x_j^2)} \sum_{n=0}^{\infty} \frac{(-2c)^n x_i^n x_j^n}{n!}</script><script type="math/tex; mode=display">  = \left\{ e^{cx_i^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_i^n}, \cdots \end{matrix} \right] \right\}  \left\{ e^{cx_j^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_j^n}, \cdots \end{matrix} \right] \right\}^T</script><p>  则可得映射函数为</p><script type="math/tex; mode=display">\mathit{\Phi}(x) = e^{cx^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x^n}, \cdots \end{matrix} \right] \tag{4.d}</script></li><li><p>输入维度为$d$</p><script type="math/tex; mode=display">||x_i - x_j||^2 = (x_i - x_j)^T (x_i - x_j) = x_i^T x_i - 2 x_i^T x_j + x_j^T x_j</script><p>  代入核函数$(4.c)$得到</p><script type="math/tex; mode=display">e^{c||x_i - x_j||^2} = e^{c(x_i^T x_i + x_j^T x_j)} e^{-2c x_i^T x_j}</script><p>  其中</p><script type="math/tex; mode=display">e^{-2c x_i^T x_j} = \sum_{n=0}^{\infty} \frac{(-2c)^n (x_i^T x_j)^n}{n!}</script><p>  故</p><script type="math/tex; mode=display">e^{c||x_i - x_j||^2} = e^{c(x_i^T x_i + x_j^T x_j)} \sum_{n=0}^{\infty} \frac{(-2c)^n (x_i^T x_j)^n}{n!}</script><p>  特殊化，对于$d=2$，展开至$3$阶，得到</p><script type="math/tex; mode=display">  e^{c||x_i - x_j||^2} = e^{c(x_{i1}^2 + x_{i2}^2 + x_{j1}^2 + x_{j2}^2)} \left[ 1 - 2c (x_{i1}x_{j1} + x_{i2}x_{j2}) + 4c^2 (x_{i1}x_{j1} + x_{i2}x_{j2})^2 -8c^3 (x_{i1}x_{j1} + x_{i2}x_{j2})^3 \right]</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      x_{i1}x_{j1} + x_{i2}x_{j2} \\      (x_{i1}x_{j1} + x_{i2}x_{j2})^2 = x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2 \\      (x_{i1}x_{j1} + x_{i2}x_{j2})^3 = x_{i1}^3 x_{j1}^3 + 3x_{i1}x_{j1}x_{i2}^2x_{j2}^2 + 3x_{i1}^2x_{j1}^2x_{i2}x_{j2} + x_{i2}^3 x_{j2}^3 \\  \end{cases}</script><p>  带入后得</p><script type="math/tex; mode=display">  e^{c||x_i - x_j||^2} = e^{c(x_{i1}^2 + x_{i2}^2 + x_{j1}^2 + x_{j2}^2)} [ 1 - 2c (x_{i1}x_{j1} + x_{i2}x_{j2}) + 4c^2 (x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2) -</script><script type="math/tex; mode=display">  8c^3 (x_{i1}^3 x_{j1}^3 + 3x_{i1}x_{j1}x_{i2}^2x_{j2}^2 + 3x_{i1}^2x_{j1}^2x_{i2}x_{j2} + x_{i2}^3 x_{j2}^3) ]</script><p>  所以映射函数为</p><script type="math/tex; mode=display">  \mathit{\Phi}(x) = e^{c(x_1^2 + x_2^2)} \left[       \begin{matrix}          1, \sqrt{-2c} x_1, \sqrt{-2c} x_2, \sqrt{4c^2} x_1^2, \sqrt{8c^2} x_1x_2, \sqrt{4c^2} x_2^2, \sqrt{-8c^3}x_1^3, \sqrt{-24c^3} x_1 x_2^2, \sqrt{-24c^3} x_1^2 x_2, \sqrt{-8c^3} x_1^3      \end{matrix} \right]^T \tag{4.e}</script><p>  即升维后维数为$10$。</p></li></ul></li></ol><h2 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h2><p>对于线性可分得情况，目标为求解一个超平面$w^T \mathit{\Phi}(x) + b = 0$使两类点落在超平面两侧。</p><p><img src="/2019/05/27/Support-Vector-Machine/svm-3-conditions.jpg" alt="svm-3-conditions"></p><p>观察以上两图，图$(c)$分割最佳，应有</p><ul><li>超平面在两类点间隔内，可平移距离最大；<script type="math/tex; mode=display">r = \max (r_+ - r_-)</script></li><li>分割平面$\mathcal{P}$到$\mathcal{P}_+, \mathcal{P}_-$的距离相等，即；<script type="math/tex; mode=display">r_- = r_+ = \frac{r}{2}</script></li><li>落在支撑超平面$\mathcal{P}_+, \mathcal{P}_-$上的点称为<strong>支持向量<code>Support Vector</code></strong>，可记作$x_{+/-}^{sup}$；</li></ul><p>设分割超平面$\mathcal{P}$为</p><script type="math/tex; mode=display">g(x) = w^T \mathit{\Phi}(x) + b = 0 \tag{5}</script><p>判别方程可定义为</p><script type="math/tex; mode=display">\hat{y} = \text{sign} \left[  w^T \mathit{\Phi}(x) + b \right] \tag{6}</script><p>则平面$\mathcal{P}$上下平移后得到平面$\mathcal{P}_+, \mathcal{P}_-$，即</p><script type="math/tex; mode=display">g_+(x) = w^T \mathit{\Phi}(x) + b - C(常数) = 0</script><script type="math/tex; mode=display">g_-(x) = w^T \mathit{\Phi}(x) + b + C(常数) = 0</script><p>作归一化处理，两边同除以$C(常数)$，则支撑超平面方程为</p><script type="math/tex; mode=display">g_+(x) = w^T \mathit{\Phi}(x) + b - 1 = 0</script><script type="math/tex; mode=display">g_-(x) = w^T \mathit{\Phi}(x) + b + 1 = 0</script><p>对于正负样本$x_{+/-}$，分别满足</p><script type="math/tex; mode=display">\begin{cases}    w^T \mathit{\Phi}(x_+) + b > 1 \\    y = 1\end{cases}\begin{cases}    w^T \mathit{\Phi}(x_-) + b < - 1 \\    y = -1\end{cases}</script><p>即</p><script type="math/tex; mode=display">y \left[ w^T \mathit{\Phi}(x) + b \right] > 1</script><p>现希望优化两个平面间的距离，使其达到最大，即优化目标为</p><script type="math/tex; mode=display">w, b = \arg \max r</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] > 1 \tag{7}</script><p><strong>那么如何求解支撑超平面间距离$r$呢？</strong>，有两种思路</p><ol><li><p>思路一<br> <img src="/2019/05/27/Support-Vector-Machine/r.jpg" alt="r"></p><p> 过超平面$\mathcal{P}$任一点$\mathit{\Phi}(x)$作垂线，分别交$\mathcal{P}_{+/-}$于向量$\mathit{\Phi}(x_{+/-})$，则</p><script type="math/tex; mode=display"> \begin{cases}     w^T \mathit{\Phi}(x_+) + b = + 1 \\     w^T \mathit{\Phi}(x_-) + b = - 1  \end{cases}</script><p> 利用距离公式求解点$\mathit{\Phi}(x)$到超平面$\mathcal{P}$的距离</p><script type="math/tex; mode=display">r_+ = r_- = \frac{r}{2} = \frac{| w^T \mathit{\Phi}(x_{+/-}) + b |}{||w||} = \frac{1}{||w||}</script><p> 所以</p><script type="math/tex; mode=display">r = \frac{2}{||w||} \tag{8.a}</script></li><li><p>思路二</p><script type="math/tex; mode=display">r_+ = r_- = \frac{r}{2} = \min_i \frac{| w^T \mathit{\Phi}(x^{(i)}) + b |}{||w||}, \quad i = 1, ..., N</script><p> 由支持向量定义</p><script type="math/tex; mode=display">| w^T \mathit{\Phi}(x_{+/-}^{sup}) + b | = \min | w^T \mathit{\Phi}(x^{(i)}) + b |</script><p> 且</p><script type="math/tex; mode=display">| w^T \mathit{\Phi}(x_{+/-}^{sup}) + b | = 1</script><p> 所以</p><script type="math/tex; mode=display">r = \frac{2}{||w||} \tag{8.b}</script></li></ol><p>所以优化目标为</p><script type="math/tex; mode=display">w, b = \arg \max \frac{2}{||w||}</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{9.a}</script><p>为方便求解，相当于</p><script type="math/tex; mode=display">w, b = \arg \min \frac{1}{2} ||w||^2</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{10.b}</script><h1 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h1><p>回归模型的目标是让训练集中每个样本点$(x^{(i)}, y^{(i)})$尽量拟合到一个线性模型上</p><script type="math/tex; mode=display">y^{(i)} = w^T \mathit{\Phi}(x^{(i)}) + b</script><p>对于一般的回归模型，一般使用$MSE$作为损失函数，但是在$y^{(i)} \neq w^T \mathit{\Phi}(x^{(i)}) + b$时就会有损失，所以$SVM$不采用。</p><p>定义一个常量$\epsilon &gt; 0$，对于某个样本点$(x^{(i)}, y^{(i)})$，其损失定义如下，即在支撑超平面间隔内的样本点是没有损失的</p><script type="math/tex; mode=display">L^{(i)} = \begin{cases}    0 & | y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) - b | \leq \epsilon \\    | y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) - b | - \epsilon & \text{otherwise}\end{cases}</script><p><img src="/2019/05/27/Support-Vector-Machine/regression.jpg" alt="regression"></p><h1 id="优化问题的求解与SMO算法"><a href="#优化问题的求解与SMO算法" class="headerlink" title="优化问题的求解与SMO算法"></a>优化问题的求解与SMO算法</h1><h2 id="带约束的优化问题求解"><a href="#带约束的优化问题求解" class="headerlink" title="带约束的优化问题求解"></a>带约束的优化问题求解</h2><ol><li><p>纯等式约束<br> 一般形式为</p><script type="math/tex; mode=display">\vec{w} = \arg \min f(\vec{w})</script><script type="math/tex; mode=display">s.t. \qquad h_j(\vec{w}) = 0, \quad j = 1, 2, \cdots, m \tag{11}</script><p> 其中$f(\vec{w}), h_j(\vec{w})$均可导</p><p> 列写拉格朗日函数</p><script type="math/tex; mode=display">L(\vec{w}, \vec{\lambda}) = f(\vec{w}) + \sum_{j=0}^m \lambda_j h_j(\vec{w}) \tag{11.a}</script><p> 求取极点</p><script type="math/tex; mode=display"> \begin{cases} \frac{\partial L}{\partial w_i} = \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^m \lambda_j \frac{\partial h_j(\vec{w})}{\partial w_i} = 0 \\ \frac{\partial L}{\partial \lambda_j} = h_j(\vec{w}) = 0 \end{cases} \tag{11.b}</script><blockquote><p>如 $f(x, y) = x^2 + 3xy + y^2, \quad s.t. \quad x + y = 100 $</p></blockquote></li><li><p>纯不等式约束<br> 一般形式为</p><script type="math/tex; mode=display">\vec{w} = \arg \min f(\vec{w})</script><script type="math/tex; mode=display">s.t. \qquad g_j(\vec{w}) \leq 0, \quad j = 1, 2, \cdots, p \tag{12}</script><p> 对于上不等式约束，引入松弛变量$\epsilon_j^2$，使其转换为等式约束</p><script type="math/tex; mode=display">s.t. \qquad g_j(\vec{w}) + \epsilon_j^2 = 0, \quad j = 1, 2, \cdots, p</script><blockquote><p>注意，这里引入的松弛变量为平方项，如此可避免增加约束$\epsilon_j \geq 0$</p></blockquote><p> 列写拉格朗日函数</p><script type="math/tex; mode=display">L(\vec{w}, \vec{\mu}, \vec{\epsilon^2}) = f(\vec{w}) + \sum_{j=0}^p \mu_j (g_j(\vec{w}) + \epsilon_j^2) \tag{12.a}</script><p> 求取极值点</p><script type="math/tex; mode=display"> \begin{cases} \frac{\partial L}{\partial w_i} = \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\ \frac{\partial L}{\partial \mu_j} = g_j(\vec{w}) + \epsilon_j^2 = 0 \\ \frac{\partial L}{\partial \epsilon_j} = 2 \mu_j \epsilon_j = 0 \\ \mu_j \geq 0 \end{cases} \tag{12.b}</script><p> 注意等式三</p><ol><li>若$\mu_j = 0$，即对应不等式$g_j(\vec{w}) \leq 0$未起到约束作用；</li><li><p>若$\mu_j \neq 0$，则$\epsilon_j = 0$，那么$ g_j(\vec{w}) = 0$；<br>则总结可得</p><script type="math/tex; mode=display">\mu_j g_j(\vec{w}) = 0</script><p>故$(12.b)$转化为</p><script type="math/tex; mode=display">\begin{cases}\frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\\mu_j g_j(\vec{w}) = 0 \\\mu_j \geq 0\end{cases} \tag{12.c}</script></li></ol></li></ol><ol><li><p>混合条件约束<br> 一般形式为</p><script type="math/tex; mode=display">\vec{w} = \arg \min f(\vec{w})</script><script type="math/tex; mode=display">s.t. \qquad h_j(\vec{w}) = 0, \quad j = 1, 2, \cdots, m</script><script type="math/tex; mode=display">\quad \qquad g_k(\vec{w}) \leq 0, \quad k = 1, 2, \cdots, p \tag{13}</script><p> 经上述内容，可得</p><script type="math/tex; mode=display">L(\vec{w}, \vec{\mu}, \vec{\epsilon^2}) = f(\vec{w}) + \sum_{j=0}^m \lambda_j h_j(\vec{w}) + \sum_{j=0}^p \mu_j (g_j(\vec{w}) + \epsilon_j^2) \tag{13.a}</script><p> 求取极值点</p><script type="math/tex; mode=display"> \begin{cases} \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^m \lambda_j \frac{\partial h_j(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\ h_j(\vec{w}) = 0 \\ \mu_j g_j(\vec{w}) = 0 \\ \mu_j \geq 0 \end{cases} \tag{13.b}</script><p> 上式即$K.K.T.$条件。</p></li></ol><h2 id="线性可分情况下求解"><a href="#线性可分情况下求解" class="headerlink" title="线性可分情况下求解"></a>线性可分情况下求解</h2><p><img src="/2019/05/27/Support-Vector-Machine/linear.jpg" alt="linear"></p><p>对于优化问题$(10.b)$</p><script type="math/tex; mode=display">w, b = \arg \max \frac{2}{||w||}</script><script type="math/tex; mode=display">s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{10.b}</script><p>利用拉格朗日乘子法</p><script type="math/tex; mode=display">L(w, b, \mu) = \frac{1}{2} ||w||^2 + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} \tag{14}</script><p>则带求解问题为</p><script type="math/tex; mode=display">w, b, \mu = \arg \min_{w, b} \max_{\mu} L(w, b, \mu) \tag{15.a}</script><p>转化为上式的<strong>对偶问题</strong></p><script type="math/tex; mode=display">w, b, \mu = \arg \min_{\mu} \max_{w,b} L(w, b, \mu) \tag{15.b}</script><blockquote><p>凸优化</p></blockquote><p>分别对参数求偏导</p><script type="math/tex; mode=display">\frac{\partial}{\partial w_j} L(w, b, \mu) = w_j - \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j)</script><script type="math/tex; mode=display">\frac{\partial}{\partial b} L(w, b, \mu) = - \sum_i \mu^{(i)} y^{(i)}</script><blockquote><p>$ \frac{\partial}{\partial w_j} \frac{1}{2} ||w||^2 = w_j; \quad \frac{\partial}{\partial w_j} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = \mathit{\Phi}(x^{(i)}_j) $，注意这里还没有用到映射函数$\mathit{\Phi}(x)$计算。</p></blockquote><p>该式为不等式约束，由$K.K.T.$条件，联立得到</p><script type="math/tex; mode=display">\begin{cases}    \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) = w_j \\    \sum_i \mu^{(i)} y^{(i)} = 0 \\    \\    1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 0 \\    \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0 \\    \mu^{(i)} \geq 0\end{cases} \tag{16}</script><p>有</p><script type="math/tex; mode=display">\tilde{L}(\mu) = \frac{1}{2} w^T w + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\}</script><script type="math/tex; mode=display">\qquad \qquad \qquad \qquad = \frac{1}{2} w^T w + \sum_i \mu^{(i)} - w^T \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) + b \sum_i \mu^{(i)} y^{(i)} \tag{17.a}</script><p>将$w = \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}), \sum_i \mu^{(i)} y^{(i)} = 0$代入，消去变量$w, b$，有</p><script type="math/tex; mode=display">\tilde{L}(\mu) = \sum_i \mu^{(i)} - \frac{1}{2} w^T w \tag{17.b}</script><p>其中</p><script type="math/tex; mode=display">w^T w = \left[ \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \right]^T \left[ \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \right]</script><script type="math/tex; mode=display">\qquad = \sum_i \left[ \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T \sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)}) \right]</script><script type="math/tex; mode=display">\quad = \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{17.c}</script><p>代回$(17.b)$得到</p><script type="math/tex; mode=display">\tilde{L}(\mu) = \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{17}</script><blockquote><p>这里出现了<strong>核函数</strong>：$\kappa(x^{(i)}, x^{(j)}) = \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})$</p></blockquote><p>那么优化问题现在转化为</p><script type="math/tex; mode=display">\mu = \arg \max_{\mu} \tilde{L}(\mu) = \arg \max_{\mu} \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})</script><script type="math/tex; mode=display">s.t.\qquad \mu^{(i)} \geq 0, \quad \sum_i \mu^{(i)} y^{(i)} = 0 \tag{18}</script><p>其中$i = 1, \cdots, N$，现在只需优化$\mu$即可，该式使用$SMO$或梯度下降法算法求解，再用下式求解参数$w, b$</p><script type="math/tex; mode=display">\begin{cases}    w = \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \\    y^{sup} \left[ w^T \mathit{\Phi}(x^{sup}) + b \right] = 1\end{cases} \tag{19}</script><p>可能存在多个支持向量，均满足等式$(19.2)$。</p><h2 id="线性不可分情况下求解"><a href="#线性不可分情况下求解" class="headerlink" title="线性不可分情况下求解"></a>线性不可分情况下求解</h2><p><img src="/2019/05/27/Support-Vector-Machine/non-linear.jpg" alt="non-linear"></p><p>如上图，存在部分样本点线性不可分，有两种方法可解决</p><ol><li>核函数</li><li>软间隔支持向量机</li></ol><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>上面介绍了核函数，其作用是将样本特征升维，添加的维度与已存在的特征是线性不相关的，例如我们有样本点</p><script type="math/tex; mode=display">x^{(i)} = \left[\begin{matrix}    x^{(i)}_1, \cdots, x^{(i)}_n\end{matrix}\right]^T</script><p>可以增加多项式维，例如$(x^{(i)}_j)^n, \prod_{k \leq n}^{k \leq K \leq N} x^{(i)}_k$等，或者添加其他形式的非线性函数，但我们知道，所有函数均可在某点$x = x_0$处展开为幂级数，本质上一致</p><script type="math/tex; mode=display">f(x) = \sum_n \frac{f^{(n)}(x - x_0)}{n!} (x - x_0)^{(n)}</script><p>核函数就是利用级数展开的概念，构造多项式维度，将样本点升维，例如高斯核函数$\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}}$将其升高到无穷维。机器学习算法求解各维度的权值系数，特征越重要，系数值占比越大。</p><h3 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h3><p><strong>本质上仍为线性支持向量机</strong>，对于线性不可分的情况，应允许部分样本点不满足条件</p><script type="math/tex; mode=display">y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1</script><p>可对每个样本引入松弛变量$\epsilon^{(i)}$，即</p><script type="math/tex; mode=display">y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 - \epsilon^{(i)} \tag{20}</script><blockquote><p>仅仅对于落在支撑超平面间的样本点满足$\epsilon^{(i)} &gt; 0$</p></blockquote><p>形象解释，即对于<strong>落在支撑超平面间的样本点$x^{(i)}$</strong>，视其为<strong>软间隔支持向量(自创)</strong>，调整$\epsilon^{(i)}$将支撑超平面进行微量的位移$d_{\epsilon^{(i)}}$，如下图<br><img src="/2019/05/27/Support-Vector-Machine/epsilon.jpg" alt="epsilon"></p><p>但是呢，也要对不满足该条件的样本个数进行限制，希望越少越好。那么加入<strong>惩罚系数$C$(超参数)</strong>，对不满足条件的样本进行惩罚，使$\sum_i \epsilon^{(i)}$越小越好，则优化目标变更为</p><script type="math/tex; mode=display">w, b = \arg \min_{w,b} \left( \frac{1}{2} ||w||^2 + C \sum_i \epsilon^{(i)} \right)</script><script type="math/tex; mode=display">s.t. \qquad y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 - \epsilon^{(i)}</script><script type="math/tex; mode=display">\qquad \epsilon^{(i)} \geq 0 \tag{21}</script><p>同样的，构造拉格朗日函数</p><script type="math/tex; mode=display">L(w, b, \epsilon, \mu_1, \mu_2) = \left( \frac{1}{2} ||w||^2 + C \sum_i \epsilon^{(i)} \right) + \sum_i \mu_1^{(i)} \left\{ 1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} + \sum_i \mu_2^{(i)} \left( - \epsilon^{(i)} \right) \tag{22}</script><p>根据$K.K.T.$条件</p><script type="math/tex; mode=display">\begin{cases}    \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) = w_j \\    \sum_i \mu_1^{(i)} y^{(i)} = 0 \\    C - \mu_1^{(i)} - \mu_2^{(i)} = 0 \\    \\    1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 0 \\    \mu_1^{(i)} \left\{ 1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0 \\    \mu_1^{(i)} \geq 0 \\    \\    - \epsilon^{(i)} \leq 0 \\    \mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0 \\    \mu_2^{(i)} \geq 0\end{cases} \tag{23}</script><p>消除$w, b, \epsilon$</p><script type="math/tex; mode=display">\tilde{L}(\mu_1, \mu_2) = \frac{1}{2} w^T w + C \sum_i \epsilon^{(i)} + \sum_i \mu_1^{(i)} \left( 1 - \epsilon^{(i)} \right) - w^T \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) - b \sum_i \mu_1^{(i)} y^{(i)} - \sum_i \mu_2^{(i)} \epsilon^{(i)}</script><p>其中$w = \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}); \quad \sum_i \mu_1^{(i)} y^{(i)} = 0$</p><script type="math/tex; mode=display">\tilde{L}(\mu_1, \mu_2) = - \frac{1}{2} w^T w  + C \sum_i \epsilon^{(i)} + \sum_i \mu_1^{(i)} - \sum_i \left( \mu_1^{(i)} + \mu_2^{(i)} \right) \epsilon^{(i)}</script><p>其中$\mu_1^{(i)} + \mu_2^{(i)} = C$，所以同$(17)$</p><script type="math/tex; mode=display">\tilde{L}(\mu_1) = \sum_i \mu_1^{(i)} - \frac{1}{2} w^T w = \sum_i \mu_1^{(i)} - \frac{1}{2} \sum_i \sum_j \mu_1^{(i)} \mu_1^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{24}</script><p>那么优化问题现在转化为</p><script type="math/tex; mode=display">\mu_1 = \arg \max_{\mu_1} \tilde{L}(\mu_1) = \arg \max_{\mu_1} \sum_i \mu_1^{(i)} - \frac{1}{2} \sum_i \sum_j \mu_1^{(i)} \mu_1^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})</script><script type="math/tex; mode=display">s.t.\qquad \mu_1^{(i)} \geq 0, \quad \mu_2^{(i)} \geq 0</script><script type="math/tex; mode=display">\sum_i \mu_1^{(i)} y^{(i)} = 0</script><script type="math/tex; mode=display">C - \mu_1^{(i)} - \mu_2^{(i)} = 0 \tag{25}</script><p>对于上式，有如下分析</p><script type="math/tex; mode=display">\begin{cases} \mu_1^{(i)} \geq 0 \\ \mu_2^{(i)} \geq 0 \\ \mu_2^{(i)} = C - \mu_1^{(i)} \end{cases} \Rightarrow C \geq \mu_1^{(i)} \geq 0 \tag{26.a}</script><ol><li>$C = \mu_1^{(i)}$时，$\mu_2^{(i)} = 0$，由$\mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0$，可得$\epsilon^{(i)} \geq 0$<ol><li>$\epsilon^{(i)} = 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1$，该点为支撑向量；</li><li>$\epsilon^{(i)} &gt; 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] &lt; 1$，该点在支撑超平面间；</li></ol></li><li>$C \neq \mu_1^{(i)}$即$C &gt; \mu_1^{(i)} \geq 0$时，$\mu_2^{(i)} \neq 0$，则由$\mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0$，可得$\epsilon^{(i)} = 0$，那么$\mu_1^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0$<ol><li>$\mu_1^{(i)} &gt; 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1$，该点为支撑向量；</li><li>$\mu_1^{(i)} = 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1$，该点分类正确，在支持超平面上或者两边</li></ol></li></ol><p>总结一下，即</p><script type="math/tex; mode=display">\begin{cases}    \mu_1^{(i)} = 0 \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 \\    0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 \\    \mu_1^{(i)} = C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1\end{cases} \tag{27}</script><p>同样利用$SMO$或梯度下降法求解$\mu_1$，然后以下式求解$w, b, \epsilon, \mu_2$</p><script type="math/tex; mode=display">\begin{cases}    \mu_2 = C - \mu_1 \\    w_j = \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) \\    y^{sup} \left[ w^T \mathit{\Phi}(x^{sup}) + b \right] = 1 \\    y^{sup'} \left[ w^T \mathit{\Phi}(x^{sup'}) + b \right] = 1 - \epsilon^{sup'}\end{cases} \tag{28}</script><blockquote><p>注：$(x^{sup’}, y^{sup’})$表示“软间隔支持向量”。<br>注：(1) 并非所有的样本点都有一个松弛变量与其对应。实际上只有“离群点”才有，所有没离群的点松弛变量都等于0<br>(2) 松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远<br>(3) 惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题<br>(4) 惩罚因子C不是一个变量，整个优化问题在解的时候，C是一个你必须事先指定的值，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个C的值，再解一次优化问题，得到另一个分类器，再看看效果，如此就是一个参数寻优的过程，但这和优化问题本身决不是一回事，优化问题在解的过程中，C一直是定值<br>(5) 尽管加了松弛变量这么一说，但这个优化问题仍然是一个优化问题（汗，这不废话么），解它的过程比起原始的硬间隔问题来说，没有任何更加特殊的地方(C≥$\mu^{(i)}$≥0)<br>(6) 完全可以给每一个离群点都使用不同的C，这时就意味着你对每个样本的重视程度都不一样，有些样本丢了也就丢了，错了也就错了，这些就给一个比较小的C；而有些样本很重要，决不能分类错误（比如中央下达的文件啥的，笑），就给一个很大的C。<br><strong>以上忘记从哪里摘抄的了:-(</strong> </p></blockquote><h2 id="Sequential-Minimal-Optimization-SMO"><a href="#Sequential-Minimal-Optimization-SMO" class="headerlink" title="Sequential Minimal Optimization(SMO)"></a>Sequential Minimal Optimization(SMO)</h2><p>以上我们得到优化目标</p><script type="math/tex; mode=display">\mu = \arg \max_{\mu} \tilde{L}(\mu) = \arg \max_{\mu} \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})</script><script type="math/tex; mode=display">s.t.\qquad \mu^{(i)} \geq 0</script><script type="math/tex; mode=display">\quad \sum_i \mu^{(i)} y^{(i)} = 0</script><script type="math/tex; mode=display">\quad C \geq \mu^{(i)} \geq 0 \tag{29}</script><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>把对整个λ向量的优化转化为对每一对$\mu^{(i)},\mu^{(j)}$的优化，如果我们把其他λ先固定，仅仅优化某一对$\mu^{(i)},\mu^{(j)}$，那么我们可以通过解析式（即通过确定的公式来计算）来优化$\mu^{(i)},\mu^{(j)}$ 。而且此时$K.K.T.$条件很重要，之前说过最优解是一定会满足$K.K.T.$条件的，所以如果我们优化使所有$\mu$都满足了$K.K.T.$条件，那么这样最优解就会找到。</p><h3 id="选择优化对的方法"><a href="#选择优化对的方法" class="headerlink" title="选择优化对的方法"></a>选择优化对的方法</h3><p>寻找两个参数时，应找那些违反$K.K.T.$条件的，具体过程可分为外层循环和内层循环，利用启发式规则寻找待优化参数对。</p><ol><li>启发式规则1<ol><li>在<strong>所有样本</strong>中选择违反$K.K.T.$条件的一个乘子$\mu^{(i)}$，用启发式规则2选择另一个乘子$\mu^{(j)}$，对这两个乘子进行优化；</li><li>接着，从<strong>所有非边界样本</strong>中，选择违反$K.K.T.$条件的一个乘子作为最外层循环，用启发式规则2选择另一个乘子进行这两个乘子的优化；</li><li>最后，若上述非边界样本中没有违反$K.K.T.$条件的样本，则<strong>再从整个样本中</strong>去找，直到所有样本中没有需要改变的乘子，或满足其他停止条件为止。</li></ol></li><li>启发式规则2<ol><li>首先在<strong>非边界乘子</strong>中获得$|E_1−E_2|$最大的样本$\mu^{(j)}$；</li><li>如果1中没有找到，则从<strong>所有样本中</strong>随机确定$\mu^{(j)}$</li></ol></li></ol><p>满足式$(27)$即满足$K.K.T.$条件</p><script type="math/tex; mode=display">\begin{cases}    \mu_1^{(i)} = 0 \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 \\    0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 \\    \mu_1^{(i)} = C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1\end{cases} \tag{27}</script><p>那么以下情况不满足$K.K.T.$条件</p><script type="math/tex; mode=display">\begin{cases}    y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 时，\mu_1^{(i)} > 0\\    y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 时，\mu_1^{(i)} = 0 或 \mu_1^{(i)} = 1 \\    y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1 时，\mu_1^{(i)} < C\end{cases} \tag{30}</script><blockquote><p><a href="https://www.cnblogs.com/xxrxxr/p/7538430.html" target="_blank" rel="noopener">第三部分：SMO算法的个人理解 - cnblogs</a><br>找第一个参数的具体过程是这样的：</p><ol><li>遍历一遍整个数据集，对每个不满足$K.K.T.$条件的参数，选作第一个待修改参数</li><li>在上面对整个数据集遍历一遍后，选择那些参数满足$0 &lt; \mu &lt; C$的子集，开始遍历，如果发现一个不满足$K.K.T.$条件的，作为第一个待修改参数，然后找到第二个待修改的参数并修改，修改完后，重新开始遍历这个子集</li><li>遍历完子集后，重新开始①②，直到在执行①和②时没有任何修改就结束<br>（为什么要这样遍历，我现在还是不太明白）</li></ol><p>找第二个参数的过程是这样的：</p><ol><li>启发式找，找能让下式最大的</li><li>寻找一个随机位置的满足下式的可以优化的参数进行修改</li><li>在整个数据集上寻找一个随机位置的可以优化的参数进行修改</li><li>都不行那就找下一个第一个参数</li></ol></blockquote><h3 id="两个变量的优化问题"><a href="#两个变量的优化问题" class="headerlink" title="两个变量的优化问题"></a>两个变量的优化问题</h3><p>以$\mu^{(1)}, \mu^{(2)}$为例</p><script type="math/tex; mode=display">\min_{\mu^{(1)}, \mu^{(2)}} \left[ \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - \sum_i \mu^{(i)} \right]</script><script type="math/tex; mode=display">= \min_{\mu^{(1)}, \mu^{(2)}} \left[ \frac{1}{2} \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T \sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)}) - \sum_i \mu^{(i)} \right] \tag{31.a}</script><p>其中</p><script type="math/tex; mode=display">\sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T = \mu^{(1)} y^{(1)} \mathit{\Phi}(x^{(1)})^T + \mu^{(2)} y^{(2)} \mathit{\Phi}(x^{(2)})^T + \sum_{i=3} \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T</script><script type="math/tex; mode=display">\sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)})^T = \mu^{(1)} y^{(1)} \mathit{\Phi}(x^{(1)})^T + \mu^{(2)} y^{(2)} \mathit{\Phi}(x^{(2)})^T + \sum_{j=3} \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)})^T</script><script type="math/tex; mode=display">\sum_i \mu^{(i)} = \mu^{(1)} + \mu^{(2)} + \sum_{i=3} \mu^{(i)}</script><p>代回$(31.a)$，多项式展开整理得到</p><script type="math/tex; mode=display">(31.b) = \min_{\mu^{(1)}, \mu^{(2)}} [\frac{1}{2} \mu^{(1)2} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(1)}) + \frac{1}{2} \mu^{(2)2} \mathit{\Phi}(x^{(2)})^T \mathit{\Phi}(x^{(2)}) + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(2)}) +</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} \sum_{i=3} \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(i)}) + \mu^{(2)} y^{(2)} \sum_{j=3} \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(2)})^T \mathit{\Phi}(x^{(j)}) +</script><script type="math/tex; mode=display">\frac{1}{2} \sum_{i=3} \sum_{j=3} \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - (\mu^{(1)} + \mu^{(2)}) - \sum_{i=3} \mu^{(i)}] \tag{31.b}</script><p>令核函数</p><script type="math/tex; mode=display">K_{ij} = \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{32.a}</script><script type="math/tex; mode=display">const = \frac{1}{2} \sum_{i=3} \sum_{j=3} \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - \sum_{i=3} \mu^{(i)} \tag{32.b}</script><p>定义</p><script type="math/tex; mode=display">f(x^{(i)}) = \sum_m \mu^{(m)} y^{(m)} K_{im} + b \tag{32.c}</script><script type="math/tex; mode=display">E^{(i)} = f(x^{(i)}) - y^{(i)}</script><script type="math/tex; mode=display">v^{(i)} = \sum_{m=3} \mu^{(m)} y^{(m)} K_{im} = f(x^{(i)}) - \sum_{m=1}^2 \mu^{(m)} y^{(m)} K_{im} - b \tag{32.e}</script><p>以上，代回$(31.b)$，优化问题转化为</p><script type="math/tex; mode=display">\min_{\mu^{(1)}, \mu^{(2)}} [\frac{1}{2} \mu^{(1)2} K_{11} + \frac{1}{2} \mu^{(2)2} K_{22} + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} K_{12} +</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} v^{(1)} + \mu^{(2)} y^{(2)} v^{(2)} -(\mu^{(1)} + \mu^{(2)}) + const]</script><script type="math/tex; mode=display">s.t.\qquad \mu^{(i)} \geq 0</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} + \mu^{(2)} y^{(2)} = - \sum_{i=3} \mu^{(i)} y^{(i)}</script><script type="math/tex; mode=display">C \geq \mu^{(i)} \geq 0 \tag{33}</script><p>记</p><script type="math/tex; mode=display">g(\mu^{(1)}, \mu^{(2)}) = \frac{1}{2} \mu^{(1)2} K_{11} + \frac{1}{2} \mu^{(2)2} K_{22} + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} K_{12} +</script><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} v^{(1)} + \mu^{(2)} y^{(2)} v^{(2)} -(\mu^{(1)} + \mu^{(2)}) + const \tag{34}</script><p>记$\epsilon = - \sum_{i=3} \mu^{(i)} y^{(i)}$，则</p><script type="math/tex; mode=display">\mu^{(1)} y^{(1)} + \mu^{(2)} y^{(2)} = \epsilon</script><p>消去$\mu^{(1)}$</p><script type="math/tex; mode=display">\mu^{(1)} = \frac{\epsilon - \mu^{(2)} y^{(2)}}{y^{(1)}} \quad 同 \quad \mu^{(1)} = y^{(1)}\epsilon - y^{(1)} y^{(2)} \mu^{(2)}</script><p>记$\gamma = y^{(1)}\epsilon, \quad s = y^{(1)} y^{(2)} $，则</p><script type="math/tex; mode=display">\mu^{(1)} = \gamma - s \mu^{(2)} \tag{35}</script><blockquote><script type="math/tex; mode=display">s^2 = 1, \quad \gamma s = y^{(2)}\epsilon</script></blockquote><p>代入函数$(34)$，有</p><script type="math/tex; mode=display">\tilde{g}(\mu^{(2)}) = \frac{1}{2} (K_{11} + K_{22} - 2 K_{12}) \mu^{(2)2} +</script><script type="math/tex; mode=display">\left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} +</script><script type="math/tex; mode=display">(\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{36}</script><p>转化为单变量$\mu^{(2)}$的二次优化问题。</p><h3 id="剪裁边界"><a href="#剪裁边界" class="headerlink" title="剪裁边界"></a>剪裁边界</h3><p>需考虑$\mu^{(2)}$的取值范围，即编程时$\mu^{(2)}$的<strong>剪裁边界</strong>，设</p><script type="math/tex; mode=display">L \leq \mu^{(2)}_{new} \leq H \tag{37}</script><p>综合条件</p><script type="math/tex; mode=display">\mu^{(1)}_{new} y^{(1)} + \mu^{(2)}_{new} y^{(2)} = \epsilon</script><script type="math/tex; mode=display">0 \leq \mu^{(i)} \leq C</script><blockquote><p>$\mu^{(1)}_{new} y^{(1)} + \mu^{(2)}_{new} y^{(2)} = \epsilon \Rightarrow \mu^{(1)}_{new} + \mu^{(2)}_{new} y^{(1)} y^{(2)} = \epsilon  y^{(1)}$ </p></blockquote><ol><li><p>$y^{(1)} \neq y^{(2)}$时，$y^{(1)} y^{(2)} = -1$</p><script type="math/tex; mode=display">\mu^{(1)}_{old} - \mu^{(2)}_{old} = \epsilon_{\neq} (常数) \quad 则 \quad \mu^{(2)}_{old} = \mu^{(1)}_{old} - \epsilon_{\neq} \tag{38.a}</script><p> 考虑</p><script type="math/tex; mode=display">0 \leq \mu^{(i)} \leq C</script><p> 则</p><script type="math/tex; mode=display"> \begin{cases}     0 \leq \mu^{(1)}_{old} \leq C \\     0 \leq \mu^{(2)}_{old} \leq C \\ \end{cases} \Rightarrow 0 - \epsilon_{\neq} \leq \mu^{(2)}_{old} = \mu^{(1)}_{old} - \epsilon_{\neq} \leq C - \epsilon_{\neq}</script><p> <img src="/2019/05/27/Support-Vector-Machine/y1neqy2.jpg" alt="y1neqy2"></p><p> 此时上下界为</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - \epsilon_{\neq} \} \\     H = \min \{ C, C - \epsilon_{\neq} \} \\ \end{cases} \tag{38.b}</script><p> $(38.a)$代入$(38.b)$，得到迭代式</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\     H = \min \{ C, C - \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\ \end{cases} \tag{38}</script></li><li><p>$y^{(1)} = y^{(2)}$时，$y^{(1)} y^{(2)} = 1$</p><script type="math/tex; mode=display">\mu^{(1)}_{old} + \mu^{(2)}_{old} = \epsilon_{=} (常数) \quad 则 \quad \mu^{(2)}_{old} = - \mu^{(1)}_{old} + \epsilon_{=} \tag{39.a}</script><p> 考虑</p><script type="math/tex; mode=display">0 \leq \mu^{(i)} \leq C</script><p> 则</p><script type="math/tex; mode=display"> \begin{cases}     0 \leq \mu^{(1)}_{old} \leq C \\     0 \leq \mu^{(2)}_{old} \leq C \\ \end{cases} \Rightarrow - C + \epsilon_{=} \leq - \mu^{(1)}_{old} + \epsilon_{=} \leq 0 + \epsilon_{=}</script><p> <img src="/2019/05/27/Support-Vector-Machine/y1eqy2.jpg" alt="y1eqy2"></p><p> 此时上下界为</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - C + \epsilon_{=} \} \\     H = \min \{ C, 0 + \epsilon_{=} \} \\ \end{cases} \tag{39.b}</script><p> $(39.a)$代入$(39.b)$，得到迭代式</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, \mu^{(1)}_{old} + \mu^{(2)}_{old} - C \} \\     H = \min \{ C, \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\ \end{cases} \tag{39}</script></li></ol><h3 id="单变量的二次优化"><a href="#单变量的二次优化" class="headerlink" title="单变量的二次优化"></a>单变量的二次优化</h3><script type="math/tex; mode=display">\tilde{g}(\mu^{(2)}) = \frac{1}{2} (K_{11} + K_{22} - 2 K_{12}) \mu^{(2)2} +</script><script type="math/tex; mode=display">\left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} +</script><script type="math/tex; mode=display">(\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{36}</script><ol><li><p>二次项系数$K_{11} + K_{22} - 2 K_{12} &gt; 0$时<br> 求极值点</p><script type="math/tex; mode=display">\frac{\partial \tilde{g}(\mu^{(2)})}{\partial \mu^{(2)}} = 0 \Rightarrow \tilde{\mu}^{(2)} \tag{40.a}</script><ol><li>若$L \leq \tilde{\mu}^{(2)} \leq H$，则最小值点即为$\mu^{(2)} = \tilde{\mu}^{(2)}$；</li><li>否则在边界处取得最小值。</li></ol></li><li><p>二次项系数$K_{11} + K_{22} - 2 K_{12} = 0$时</p><script type="math/tex; mode=display"> \tilde{g}(\mu^{(2)}) =  \left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} +  (\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{40.b}</script><p> $\tilde{g}(\mu^{(2)})$为一次函数，在边界处取得最小值。</p></li><li><p>二次项系数$K_{11} + K_{22} - 2 K_{12} &lt; 0$时<br> $\tilde{g}(\mu^{(2)})$为开口向下的二次函数，在边界处取得最小值。</p></li></ol><p>综上所述，$\mu^{(2)}$更新的解析解为</p><script type="math/tex; mode=display">\mu^{(2)}_{new, clip} = \begin{cases}    H & \mu^{(2)}_{new} > H \\    \mu^{(2)}_{new} & L \leq \mu^{(2)}_{new} \leq H \\    L & \mu^{(2)}_{new} < L \\\end{cases} \tag{40}</script><p>又因为</p><script type="math/tex; mode=display">\mu^{(1)}_{old} = \gamma - s \mu^{(2)}_{old} \tag{41.a}</script><script type="math/tex; mode=display">\mu^{(1)}_{new} = \gamma - s \mu^{(2)}_{new, clip} \tag{41.b}</script><p>两式相减，得到$\mu^{(1)}$更新的增量形式</p><script type="math/tex; mode=display">\mu^{(1)}_{new} = \mu^{(1)}_{old} + y^{(1)} y^{(2)} \left( \mu^{(2)}_{old} - \mu^{(2)}_{new, clip} \right) \tag{41}</script><p>更新$\mu^{(1)}, \mu^{(2)}$后，重新计算$b$，因为$b$影响到$E^{(i)}$的计算，由$(27)$</p><script type="math/tex; mode=display">0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1</script><p>此时对应支撑向量$x^{sup}$，上右式两边同乘$y^{(i)}$，得到</p><script type="math/tex; mode=display">w^T \mathit{\Phi}(x^{(i)}) + b = y^{(i)}</script><script type="math/tex; mode=display">\Rightarrow b = y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) = y^{(i)} - \sum_m \mu^{(m)} y^{(m)} K_{im}</script><blockquote><p>疑问：$w^T \mathit{\Phi}(x^{(i)}) = \sum_m \mu^{(m)} y^{(m)} K_{im}?$</p></blockquote><p>所以</p><script type="math/tex; mode=display">b^{(1)}_{new} = y^{(1)} - \sum_{m=3} \mu^{(m)} y^{(m)} K_{1m} - \mu^{(1)}_{new} y^{(1)} K_{11} - \mu^{(2)}_{new} y^{(2)} K_{12} \tag{42.a}</script><p>其中</p><script type="math/tex; mode=display">\sum_{m=3} \mu^{(m)} y^{(m)} K_{1m} = f(x^{(1)}) - \sum_{m=1}^2 \mu^{(m)}_{old} y^{(m)} K_{1m} - b_{old} \tag{42.b}</script><blockquote><script type="math/tex; mode=display">v^{(i)} = \sum_{m=3} \mu^{(m)} y^{(m)} K_{im} = f(x^{(i)}) - \sum_{m=1}^2 \mu^{(m)} y^{(m)} K_{im} - b \tag{32.e}</script></blockquote><p>所以</p><script type="math/tex; mode=display">b^{(1)}_{new} = y^{(1)} - f(x^{(1)}) - \mu^{(1)}_{old} y^{(1)} K_{11} + \mu^{(2)}_{old} y^{(2)} K_{12} + b_{old} - \mu^{(1)}_{new} y^{(1)} K_{11} - \mu^{(2)}_{new} y^{(2)} K_{12}</script><script type="math/tex; mode=display">= - E^{(1)} + (\mu^{(1)}_{old} - \mu^{(1)}_{new}) y^{(1)} K_{11} + (\mu^{(2)}_{old} - \mu^{(2)}_{new}) y^{(2)} K_{12} + b_{old} \tag{42}</script><p>同理</p><script type="math/tex; mode=display">b^{(2)}_{new} = - E^{(2)} + (\mu^{(1)}_{old} - \mu^{(1)}_{new}) y^{(1)} K_{21} + (\mu^{(2)}_{old} - \mu^{(2)}_{new}) y^{(2)} K_{22} + b_{old} \tag{43}</script><p>当$b^{(1)}$和$b^{(2)}$均有效时</p><script type="math/tex; mode=display">b_{new} = b^{(1)}_{new} = b^{(2)}_{new} \tag{44.a}</script><p>当两个乘子都在边界上时，则$b$阈值与$K.K.T.$条件一致时，不满足时取中点</p><script type="math/tex; mode=display">b = \begin{cases}    b^{(1)} & 0 < \mu^{(1)}_{new} < C \\    b^{(2)} & 0 < \mu^{(2)}_{new} < C \\    \frac{1}{2} (b^{(1)} + b^{(2)}) & \text{otherwise}\end{cases} \tag{44.b}</script><h3 id="梳理"><a href="#梳理" class="headerlink" title="梳理"></a>梳理</h3><ol><li><p>计算误差</p><script type="math/tex; mode=display">E^{(i)} = f(x^{(i)}) - y^{(i)} = \sum_i \mu^{(m)} y^{(m)} K_{im} + b -y^{(i)}</script></li><li><p>计算上下界</p><script type="math/tex; mode=display"> \begin{cases}     L = \max \{ 0, - \mu^{(i)}_{old} + \mu^{(j)}_{old} \};\quad H = \min \{ C, C - \mu^{(i)}_{old} + \mu^{(j)}_{old} \} & y^{(i)} \neq y^{(j)} \\     L = \max \{ 0, \mu^{(i)}_{old} + \mu^{(j)}_{old} - C \};\quad H = \min \{ C, \mu^{(i)}_{old} + \mu^{(j)}_{old} \} & y^{(i)} = y^{(j)}  \end{cases}</script></li><li><p>计算$\eta$</p><script type="math/tex; mode=display">\eta = K_{ii} + K_{jj} - 2K_{ij}</script></li><li><p>更新$\mu^{(j)}$</p><script type="math/tex; mode=display">\mu^{(j)}_{new} = \mu^{(j)}_{old} + \frac{y^{(j)}(E^{(i)} - E^{(j)})}{\eta}</script></li><li><p>修剪$\mu^{(j)}$</p><script type="math/tex; mode=display"> \mu^{(j)}_{new, clip} = \begin{cases}     H & \mu^{(j)}_{new} > H \\     \mu^{(j)}_{new} & L \leq \mu^{(j)}_{new} \leq H \\     L & \mu^{(j)}_{new} < L \\ \end{cases}</script></li><li><p>更新$\mu^{(i)}$</p><script type="math/tex; mode=display">\mu^{(i)}_{new} = \mu^{(i)}_{old} + y^{(i)} y^{(j)} \left( \mu^{(j)}_{old} - \mu^{(j)}_{new, clip} \right)</script></li><li><p>更新$b^{(i)}, b^{(j)}$</p></li></ol><script type="math/tex; mode=display">b^{(i)}_{new} = - E^{(i)} + (\mu^{(i)}_{old} - \mu^{(i)}_{new}) y^{(i)} K_{ii} + (\mu^{(j)}_{old} - \mu^{(j)}_{new}) y^{(j)} K_{ij} + b_{old}</script><script type="math/tex; mode=display">b^{(j)}_{new} = - E^{(j)} + (\mu^{(i)}_{old} - \mu^{(i)}_{new}) y^{(i)} K_{ji} + (\mu^{(j)}_{old} - \mu^{(j)}_{new}) y^{(j)} K_{jj} + b_{old}</script><ol><li>修剪$b$<script type="math/tex; mode=display">b = \begin{cases}    b^{(i)} & 0 < \mu^{(i)}_{new} < C \\    b^{(j)} & 0 < \mu^{(j)}_{new} < C \\    \frac{b^{(i)} + b^{(j)}}{2} & \text{otherwise}\end{cases}</script></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA" target="_blank" rel="noopener">支持向量机 - 维基百科，自由的百科全书</a></li><li><a href="https://scikit-learn.org/stable/modules/svm.html" target="_blank" rel="noopener">1.4. Support Vector Machine - scikit learn</a></li><li><a href="https://www.jiqizhixin.com/articles/2017-02-06-3" target="_blank" rel="noopener">详解支持向量机 - 机器之心</a></li><li><a href="https://blog.csdn.net/lijil168/article/details/69395023" target="_blank" rel="noopener">深入理解拉格朗日乘子法（Lagrange Multiplier) 和KKT条件 - CSDN</a></li><li><a href="https://zhuanlan.zhihu.com/p/26514613" target="_blank" rel="noopener">浅谈最优化问题的KKT条件 - 知乎</a></li><li><a href="https://www.cnblogs.com/xxrxxr/p/7538430.html" target="_blank" rel="noopener">第三部分：SMO算法的个人理解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mAP</title>
      <link href="/2019/05/26/mAP/"/>
      <url>/2019/05/26/mAP/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文介绍一种目标检测任务的评价指标<code>mean Average Precison(mAP)</code>，有以下几个重点</p><ul><li>如何绘制<code>Precision-Recall</code>曲线；</li><li>如何用插值方法计算<code>Average Precison</code>；</li><li>理解目标检测中<code>mAP</code>的计算；</li></ul><h1 id="Precision-Recall-Average-Precision"><a href="#Precision-Recall-Average-Precision" class="headerlink" title="Precision, Recall, Average Precision"></a>Precision, Recall, Average Precision</h1><h2 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h2><p>以上定义查看<a href="https://louishsu.xyz/2018/11/21/Metrics/" target="_blank" rel="noopener">Metrics/分类(classification)评估指标</a>，计算方式如下</p><h3 id="精确率-Precision"><a href="#精确率-Precision" class="headerlink" title="精确率(Precision)"></a>精确率<code>(Precision)</code></h3><p>即所有真实正样本中，被预测为正样本的比例</p><script type="math/tex; mode=display">P = \frac{TP}{TP + FN}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precision_score</span><span class="params">(gt, pred)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        gt:     &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">        pred:   &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">    Returns：</span></span><br><span class="line"><span class="string">        p:      &#123;float&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    index = pred == <span class="number">1</span></span><br><span class="line">    _gt = gt[index]</span><br><span class="line">    tp = _gt[_gt == <span class="number">1</span>].shape[<span class="number">0</span>]</span><br><span class="line">    pp = _gt.shape[<span class="number">0</span>]</span><br><span class="line">    p = tp / pp <span class="keyword">if</span> pp != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><h3 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率(Recall)"></a>召回率<code>(Recall)</code></h3><p>即所有预测的正样本中，真正为正样本的比例</p><script type="math/tex; mode=display">R = \frac{TP}{TP + NP}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recall_score</span><span class="params">(gt, pred)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        gt:     &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">        pred:   &#123;ndarray(N)&#125; `0` or `1`</span></span><br><span class="line"><span class="string">    Returns：</span></span><br><span class="line"><span class="string">        r:      &#123;float&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    index = gt == <span class="number">1</span></span><br><span class="line">    _pred = pred[index]</span><br><span class="line">    tp = _pred[_pred == <span class="number">1</span>].shape[<span class="number">0</span>]</span><br><span class="line">    gp = _pred.shape[<span class="number">0</span>]</span><br><span class="line">    r = tp / gp <span class="keyword">if</span> gp != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure><h3 id="平均精确度-Average-Precision"><a href="#平均精确度-Average-Precision" class="headerlink" title="平均精确度(Average Precision)"></a>平均精确度<code>(Average Precision)</code></h3><p>依次设定不同的阈值，根据预测评分，得到不同的预测标签结果，那么就可以计算得到不同的$P$和$R$，做出$P-R$曲线，其与坐标轴面积即平均精确度。</p><h4 id="Average-Precision"><a href="#Average-Precision" class="headerlink" title="Average Precision"></a>Average Precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall.png" alt="2-class-precision-recall"></p><blockquote><p>该图来自<a href="https://arleyzhang.github.io/articles/c521a01c/" target="_blank" rel="noopener">目标检测评价标准-AP mAP</a>，侵删。</p></blockquote><p>那么AP计算公式为</p><script type="math/tex; mode=display">AP = \int_0^1 P(r) dr \tag{1}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        AP = \int_0^1 P(r) dr</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = len(p)</span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        deltaR = r[i] - r[i<span class="number">-1</span>]</span><br><span class="line">        P_inter = (p[i] + p[i<span class="number">-1</span>]) / <span class="number">2</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 梯形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h4 id="Approximated-Average-Precision"><a href="#Approximated-Average-Precision" class="headerlink" title="Approximated Average Precision"></a>Approximated Average Precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall-approximated.png" alt="2-class-precision-recall-approximated"></p><p>注意图中折线上每个点代表一个样本，则将$(1)$离散化，累积<strong>每个样本点</strong>带来的面积变化</p><script type="math/tex; mode=display">AP_{approx} = \sum_{k=1}^N P(k) \Delta r(k) \tag{2}</script><p>其中$N$为样本总数，$k$为样本数，且</p><script type="math/tex; mode=display">\Delta r(k) = r(k) - r(k-1)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_approximated</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        AP_&#123;approx&#125; = \sum_&#123;k=1&#125;^N P(k) \Delta r(k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = len(p)</span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        deltaR = r[i] - r[i<span class="number">-1</span>]</span><br><span class="line">        P_inter = p[i]          <span class="comment"># 每个点处的Precision</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 矩形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h4 id="Interpolated-average-precision"><a href="#Interpolated-average-precision" class="headerlink" title="Interpolated average precision"></a>Interpolated average precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall-interpolated.png" alt="2-class-precision-recall-interpolated"></p><p>换一种插值方法计算$P(k)$</p><script type="math/tex; mode=display">P_{inter}(k) = \max_{\hat{k} \geq k} P(\hat{k})</script><script type="math/tex; mode=display">AP_{inter} = \sum_{k=1}^{N} P_{inter}(k) \Delta r(k) \tag{3}</script><p>其中$\hat{k}$为第$k$个样本点后的样本索引，也即，<strong>$P_{inter}(k)$为第$k$个样本点后，最大的<code>Precision</code>值</strong>。</p><p>因为正样本影响<code>Recall</code>阈值，故式$(3)$也可写作</p><script type="math/tex; mode=display">P_{inter}(k) = \max_{\hat{k} \geq k} P(\hat{k})</script><script type="math/tex; mode=display">AP_{inter} = \sum_{k=1}^{K} P_{inter}(k) \Delta r(k) \tag{4}</script><p>其中$K$表示正样本的个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_interpolated</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        P_&#123;inter&#125;(k) = \max_&#123;\hat&#123;k&#125; \geq k&#125; P(\hat&#123;k&#125;)</span></span><br><span class="line"><span class="string">        AP_&#123;inter&#125; = \sum_&#123;k=1&#125;^&#123;N&#125; P_&#123;inter&#125;(k) \Delta r(k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n = len(p)</span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        deltaR = r[i] - r[i<span class="number">-1</span>]</span><br><span class="line">        P_inter = max(p[i:])    <span class="comment"># 每个点后最大的Precision</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 矩形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h4 id="11-points-Interpolated-Average-Precision"><a href="#11-points-Interpolated-Average-Precision" class="headerlink" title="11 points Interpolated Average Precision"></a>11 points Interpolated Average Precision</h4><p><img src="/2019/05/26/mAP/2-class-precision-recall-11points.png" alt="2-class-precision-recall-11points"></p><p>固定选取$\{0, 0.1, \cdots, 1.0\}$ 11个<code>Recall</code>阈值，<strong>选取$P_{inter}(k)$为每个阈值点后最大<code>Precision</code>值</strong>，进行计算</p><script type="math/tex; mode=display">P_{inter}(k) = \max_{r(\hat{k}) \geq R(k)} P(\hat{k}), R \in \{0.0, 0.1, \cdots, 1.0\}</script><script type="math/tex; mode=display">AP_{inter} = \sum_{k=1}^{K} P_{inter}(k) \Delta r(k) \tag{4}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_11_points</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        P_&#123;inter&#125;(k) = \max_&#123;r(\hat&#123;k&#125;) \geq R(k)&#125; P(\hat&#123;k&#125;), R \in \&#123;0.0, 0.1, \cdots, 1.0\&#125;</span></span><br><span class="line"><span class="string">        AP_&#123;inter&#125; = \sum_&#123;k=1&#125;^&#123;K&#125; P_&#123;inter&#125;(k) \Delta r(k)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    deltaR = <span class="number">0.1</span></span><br><span class="line">    p = np.array(p); r = np.array(r)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">11</span>):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">10</span>: </span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        R = i*deltaR</span><br><span class="line">        P_inter = np.max(p[r&gt;R])<span class="comment"># 每个阈值点后最大的Precision</span></span><br><span class="line">        AP += P_inter * deltaR  <span class="comment"># 矩形面积</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>以一个二分类任务为例，假设我们得到样本真实标签与预测评分如下</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">Ground Truth</th><th style="text-align:center">Score</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.4</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0.7</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0.6</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0.45</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">1</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">1</td><td style="text-align:center">0.3</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">1</td><td style="text-align:center">0.6</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">1</td><td style="text-align:center">0.9</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">1</td><td style="text-align:center">0.8</td></tr></tbody></table></div><p>可依次设定阈值</p><script type="math/tex; mode=display">thresh = 0.2, 0.2, 0.3, 0.4, 0.45, 0.6, 0.6, 0.7, 0.8, 0.9</script><p>特别注意以下几个正样本Score对应阈值点，得表格如下</p><div class="table-container"><table><thead><tr><th style="text-align:center">Index</th><th style="text-align:center">Ground Truth</th><th style="text-align:center">Score</th><th style="text-align:center">thresh=0.2</th><th style="text-align:center">thresh=0.3</th><th style="text-align:center">thresh=0.6</th><th style="text-align:center">thresh=0.8</th><th style="text-align:center">thresh=0.9</th><th style="text-align:center">thresh=1.0</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0.4</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0.7</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0.6</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">0.45</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0</td><td style="text-align:center">0.2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">1</td><td style="text-align:center">0.2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">1</td><td style="text-align:center">0.3</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">1</td><td style="text-align:center">0.6</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">1</td><td style="text-align:center">0.9</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">1</td><td style="text-align:center">0.8</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center"><strong>recall</strong></td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">1</td><td style="text-align:center">0.8</td><td style="text-align:center">0.6</td><td style="text-align:center">0.4</td><td style="text-align:center">0.2</td><td style="text-align:center">0.0</td></tr><tr><td style="text-align:center"><strong>precision</strong></td><td style="text-align:center">/</td><td style="text-align:center">/</td><td style="text-align:center">5/10</td><td style="text-align:center">4/8</td><td style="text-align:center">3/5</td><td style="text-align:center">2/2</td><td style="text-align:center">1/1</td><td style="text-align:center">1.0</td></tr></tbody></table></div><p>$P-R$曲线如下<br><img src="/2019/05/26/mAP/P-R.png" alt="P-R"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 准备数据</span></span><br><span class="line">gt = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=<span class="string">'int'</span>)</span><br><span class="line">pred = np.array([<span class="number">0.4</span>, <span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.45</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.9</span>, <span class="number">0.8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## 作P-R曲线，每个点均需计算</span></span><br><span class="line">thresh = list(np.sort(pred))[::<span class="number">-1</span>]</span><br><span class="line">p = []; r = []</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> thresh:</span><br><span class="line">    label = score2label(pred, t)</span><br><span class="line">    n = pred[pred==t].shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        p += [precision_score(gt, label)]</span><br><span class="line">        r += [recall_score(gt, label)]</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="string">"P-R"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"recall"</span>); plt.ylabel(<span class="string">"precision"</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">1.2</span>])</span><br><span class="line">plt.grid()</span><br><span class="line">plt.plot(r, p)</span><br><span class="line">plt.scatter(r, p)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h1><p>对每一类计算<code>AP</code>，求其均值</p><script type="math/tex; mode=display">mAP = \frac{1}{C} \sum_{i=0}^{C} AP_i</script><h1 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h1><p>原文<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/devkit_doc.pdf" target="_blank" rel="noopener">devkit_doc/3.4.1</a>如下</p><blockquote><p>3.4.1 Average Precision (AP) The computation of the average precision (AP) measure was changed in 2010 to improve precision and ability to measure differences between methods with low AP. It is computed as follows: </p><ol><li>Compute a version of the measured precision/recall curve with precision monotonically decreasing, by setting the precision for recall r to the maximum precision obtained for any recall $r′ \geq r$.</li><li>Compute the AP as the area under this curve by numerical integration. No approximation is involved since the curve is piecewise constant.</li></ol><p>Note that prior to 2010 the AP is computed by sampling the monotonically decreasing curve at a fixed set of uniformly-spaced recall values $0, 0.1, 0.2, \cdots, 1.0$. By contrast, VOC2010–2012 effectively samples the curve at all unique recall values.</p></blockquote><p>记真实回归框为$B_{gt}$，预测回归框为$B_p$，则当预测框与真实框<code>IoU</code>大于阈值$0.5$时，认定物体被检测到，即</p><script type="math/tex; mode=display">IoU_{gt, p} = \frac{area(B_p \bigcap B_{gt})}{area(B_p \bigcup B_{gt})} \geq 0.5</script><p>其计算方法更加粗暴，计算每个<code>Recall</code>阈值点处最大<code>Precision</code>的均值</p><script type="math/tex; mode=display">P_{inter}(r) = \max_{\hat{r}: \hat{r} \geq r} P(\hat{r})</script><script type="math/tex; mode=display">AP_{voc} = \frac{1}{11} \sum_{r \in \{0.0, 0.1, \cdots, 1.0\}} P_{inter}(r) \tag{5}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_voc</span><span class="params">(p, r)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n)&#125; Precision</span></span><br><span class="line"><span class="string">        r: &#123;ndarray(n)&#125; Recall</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        AP:&#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        P_&#123;inter&#125;(r) = \max_&#123;\hat&#123;r&#125;: \hat&#123;r&#125; \geq r&#125; P(\hat&#123;r&#125;)</span></span><br><span class="line"><span class="string">        AP_&#123;voc&#125; = \frac&#123;1&#125;&#123;11&#125; \sum_&#123;r \in \&#123;0.0, 0.1, \cdots, 1.0\&#125;&#125; P_&#123;inter&#125;(r)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    AP = <span class="number">0</span></span><br><span class="line">    deltaR = <span class="number">0.1</span></span><br><span class="line">    p = np.array(p); r = np.array(r)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">11</span>):</span><br><span class="line">        R = i*deltaR</span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">10</span>: </span><br><span class="line">            P_inter = np.max(p[r&gt;R])<span class="comment"># 每个阈值点后最大的Precision</span></span><br><span class="line">        AP += P_inter           <span class="comment"># 矩形面积</span></span><br><span class="line">    AP = AP / <span class="number">11</span></span><br><span class="line">    <span class="keyword">return</span> AP</span><br></pre></td></tr></table></figure><h1 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h1><p>COCO评估指标较多，共有12项，详细查看<a href="http://cocodataset.org/#detection-eval" target="_blank" rel="noopener">Detection Evaluation</a>，以下仅介绍其<code>AP</code>计算方法，原文如下。</p><blockquote><p>Average Precision (AP):</p><ol><li><p>primary challenge metric($AP$)</p><script type="math/tex; mode=display">IoU=.50:.05:.95</script></li><li><p>PASCAL VOC metric($AP^{IoU}=.50$)</p><script type="math/tex; mode=display">IoU=.50</script></li><li><p>strict metric($AP^{IoU}=.75$) </p><script type="math/tex; mode=display">IoU=.75</script></li></ol></blockquote><p>与<code>VOC</code>类似，但选取更多阈值的<code>IoU</code>，从中标记预测正确的样本进行计算。当$IoU = 0.5$时，即<code>VOC</code>计算方法。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/devkit_doc.pdf" target="_blank" rel="noopener">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit</a></li><li><a href="https://arleyzhang.github.io/articles/c521a01c/" target="_blank" rel="noopener">目标检测评价标准-AP mAP</a></li><li><a href="https://www.zhihu.com/question/41540197" target="_blank" rel="noopener">mean average precision（MAP）在计算机视觉中是如何计算和应用的？</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NMS &amp; soft-NMS</title>
      <link href="/2019/05/26/NMS-softer-NMS/"/>
      <url>/2019/05/26/NMS-softer-NMS/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在目标检测中，若多个回归框<code>(Bounding Box)</code>重叠内容较多，则可以保留其中几个，删除其余冗余的框，非极大值抑制<code>(Non-Maximum Suppression, NMS)</code>算法可实现该功能。</p><p>那么用什么指标评价两个回归框重叠过多呢，以下介绍图像的交并比<code>IoU(Intersection over Union)</code>。</p><h1 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><code>IoU</code>的思路非常简单，即计算两个回归框交集部分面积与并集面积的比值，如下图，其计算公式为</p><script type="math/tex; mode=display">IoU_{a, b} = \frac{Area_{inter}}{Area_{union}}</script><p>其中</p><script type="math/tex; mode=display">Area_{union} = Area_{a} + Area_{b} - Area_{inter}</script><script type="math/tex; mode=display">Area_{inter} = w_{inter}*h_{inter}</script><script type="math/tex; mode=display">w_{inter} = x^a_2 - x^b_1</script><script type="math/tex; mode=display">h_{inter} = y^a_2 - y^b_1</script><p><img src="/2019/05/26/NMS-softer-NMS/IoU1.jpg" alt="iou1"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>算法实现中，比较关键的一点是计算交集部分的坐标点，记两个回归框$a, b$左上角坐标为$(x^<em>_1, y^</em>_1)$，右下角为$(x^<em>_2, y^</em>_2)$</p><p>则应有</p><script type="math/tex; mode=display">x^{inter}_1 = \max \{x^a_1, x^b_1\}, y^{inter}_1 = \max \{y^a_1, y^b_1\}</script><script type="math/tex; mode=display">x^{inter}_2 = \min \{x^a_2, x^b_2\}, y^{inter}_2 = \min \{y^a_2, y^b_2\}</script><p><img src="/2019/05/26/NMS-softer-NMS/IoU2.jpg" alt="IoU2"></p><p>使用C/C++实现如下<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="keyword">float</span> x1, y1, x2, y2;</span><br><span class="line">&#125;bbox;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_overlap</span><span class="params">(<span class="keyword">float</span> ax1, <span class="keyword">float</span> ax2, <span class="keyword">float</span> bx1, <span class="keyword">float</span> bx2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> left  = _max(ax1, bx1);</span><br><span class="line">    <span class="keyword">float</span> right = _min(ax2, bx2);</span><br><span class="line">    <span class="keyword">float</span> gap = right - left;</span><br><span class="line">    <span class="keyword">return</span> gap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_intersection</span><span class="params">(bbox a, bbox b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> w = bbox_overlap(a.x1, a.x2, b.x1, b.x2);</span><br><span class="line">    <span class="keyword">float</span> h = bbox_overlap(a.y1, a.y2, b.y1, b.y2);</span><br><span class="line">    <span class="keyword">if</span>(w &lt; <span class="number">0</span> || h &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> area = w*h;</span><br><span class="line">    <span class="keyword">return</span> area;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_area</span><span class="params">(bbox a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> w = a.x2 - a.x1;</span><br><span class="line">    <span class="keyword">float</span> h = a.y2 - a.y1;</span><br><span class="line">    <span class="keyword">return</span> w*h;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_union</span><span class="params">(bbox a, bbox b, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> u = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">float</span> area_a = bbox_area(a);</span><br><span class="line">    <span class="keyword">float</span> area_b = bbox_area(b);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mode == <span class="number">0</span>)&#123;</span><br><span class="line">        u = area_a + area_b - bbox_intersection(a, b);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (mode == <span class="number">1</span>)&#123;</span><br><span class="line">        u = _min(area_a, area_b);</span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> u;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">bbox_iou</span><span class="params">(bbox a, bbox b, <span class="keyword">int</span> mode)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> i = bbox_intersection(a, b);</span><br><span class="line">    <span class="keyword">float</span> u = bbox_union(a, b, mode);</span><br><span class="line">    <span class="keyword">return</span> i/u;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a>NMS</h1><p>以下图为例，其中三个候选框$a,b,c$，其评分依次为0.8, 0.7, 0.9，设定<code>IoU</code>阈值</p><script type="math/tex; mode=display">thresh=0.4</script><p>计算步骤如下<br><img src="/2019/05/26/NMS-softer-NMS/NMS1.jpg" alt="NMS1"></p><ol><li>将其排序，如降序排序， 得到结果$c,a,b$；</li><li>保存当前评分最高的回归框，即$c$；</li><li>计算$c$与剩余框，即$a,b$的<code>IoU</code>，即<script type="math/tex; mode=display">IoU_{c,a} = \frac{1×8}{10×9 + 9×11 - 1×8} = 0.044</script><script type="math/tex; mode=display">IoU_{c,b} = \frac{4×6}{10×9 + 10×11 - 4×6} = 0.136</script></li><li>无大于阈值的框，故无框被删除；</li><li>保存当前评分最高的回归框，即$a$；</li><li>计算$a$与剩余框即$b$的<code>IoU</code>，即<script type="math/tex; mode=display">IoU_{a,b} = \frac{7×9}{9×11 + 10×11 - 7×9} = 0.432</script></li><li>大于阈值，故删除$b$；</li><li>最终保留框$a,c$；</li></ol><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><ol><li><p>Python</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_nms</span><span class="params">(dets, thresh, mode=<span class="string">"Union"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        dets:   &#123;ndarray(n_boxes, 5)&#125; x1, y1, x2, y2 score</span></span><br><span class="line"><span class="string">        thresh: &#123;float&#125; retain overlap &lt;= thresh</span></span><br><span class="line"><span class="string">        mode:   &#123;str&#125; 'Union' or 'Minimum'</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        idx:   &#123;list[int]&#125; indexes to keep</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        greedily select boxes with high confidence</span></span><br><span class="line"><span class="string">        idx boxes overlap &lt;= thresh</span></span><br><span class="line"><span class="string">        rule out overlap &gt; thresh</span></span><br><span class="line"><span class="string">        if thresh==1.0, keep all</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x1 = dets[:, <span class="number">0</span>]</span><br><span class="line">    y1 = dets[:, <span class="number">1</span>]</span><br><span class="line">    x2 = dets[:, <span class="number">2</span>]</span><br><span class="line">    y2 = dets[:, <span class="number">3</span>]</span><br><span class="line">    scores = dets[:, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line">    order = scores.argsort()[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    idx = []</span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</span><br><span class="line">        i = order[<span class="number">0</span>]</span><br><span class="line">        idx.append(i)</span><br><span class="line"></span><br><span class="line">        xx1 = np.maximum(x1[i], x1[order[<span class="number">1</span>:]])</span><br><span class="line">        yy1 = np.maximum(y1[i], y1[order[<span class="number">1</span>:]])</span><br><span class="line">        xx2 = np.minimum(x2[i], x2[order[<span class="number">1</span>:]])</span><br><span class="line">        yy2 = np.minimum(y2[i], y2[order[<span class="number">1</span>:]])</span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)</span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        inter = w * h</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"Union"</span>:</span><br><span class="line">            ovr = inter / (areas[i] + areas[order[<span class="number">1</span>:]] - inter)</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">"Minimum"</span>:</span><br><span class="line">            ovr = inter / np.minimum(areas[i], areas[order[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">        inds = np.where(ovr &lt;= thresh)[<span class="number">0</span>]</span><br><span class="line">        order = order[inds + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure></li><li><p>C/C++</p> <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">detect</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">float</span> score;    <span class="comment">/* 该框评分 */</span></span><br><span class="line">    bbox bx;        <span class="comment">/* 回归方框 */</span></span><br><span class="line">    bbox offset;    <span class="comment">/* 偏置 */</span></span><br><span class="line">    landmark mk;    <span class="comment">/* 位置 */</span></span><br><span class="line">&#125; detect;</span><br><span class="line"></span><br><span class="line"><span class="comment">// decending order, bubble</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bsort</span><span class="params">(detect** dets, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i++ )&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n - <span class="number">1</span> - i; j++ )&#123;</span><br><span class="line">            <span class="keyword">float</span> a = (*dets)[j].score;</span><br><span class="line">            <span class="keyword">float</span> b = (*dets)[j+<span class="number">1</span>].score;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (a &lt; b)&#123;</span><br><span class="line">                detect tmp = (*dets)[j];</span><br><span class="line">                (*dets)[j] = (*dets)[j+<span class="number">1</span>];</span><br><span class="line">                (*dets)[j+<span class="number">1</span>] = tmp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> _nms(detect* dets, <span class="keyword">int</span> n, <span class="keyword">float</span> thresh, <span class="keyword">int</span> mode)</span><br><span class="line">&#123;</span><br><span class="line">    bsort(&amp;dets, n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (dets[i].score == <span class="number">0</span>) <span class="keyword">continue</span>;   <span class="comment">// 表示该框已被删除</span></span><br><span class="line">        bbox a = dets[i].bx;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            bbox b = dets[j].bx;</span><br><span class="line">            <span class="keyword">if</span> (bbox_iou(a, b, mode) &gt; thresh)</span><br><span class="line">                dets[j].score = <span class="number">0</span>;          <span class="comment">// 删除该框</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h1 id="Soft-NMS"><a href="#Soft-NMS" class="headerlink" title="Soft-NMS"></a>Soft-NMS</h1><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>对于两个相邻较近的同类物体，如下图，<code>NMS</code>可能将左框删除，这是由于在删除同类别框时，只考虑了<code>IoU</code>重叠大小，并没有将两个框的评分引入计算，当两个框相邻很近但评分都较高时，不能简单地删除。</p><p><img src="/2019/05/26/NMS-softer-NMS/soft-NMS1.jpg" alt="soft-NMS1"></p><p>改进后的<code>soft-NMS</code>伪代码如下<br><img src="/2019/05/26/NMS-softer-NMS/NMS2.jpg" alt="NMS2"></p><p>也可统一写作</p><script type="math/tex; mode=display">s_i \leftarrow s_i f(iou(\mathcal{M}, b_i))</script><p>其中<code>NMS</code>算法引入<code>hard threshold</code>，即</p><script type="math/tex; mode=display">f(iou(\mathcal{M}, b_i)) = \begin{cases}    1 & iou(\mathcal{M}, b_i) < N_t \\    0 & iou(\mathcal{M}, b_i) \geq N_t\end{cases}</script><p><img src="/2019/05/26/NMS-softer-NMS/f1.png" alt="f1"></p><p>应考虑以下因素</p><ul><li>相邻检测的分数应该降低到它们具有增加假阳性率<code>(false positive rate)</code>的可能性较小的程度，同时在排序的检测列表中高于明显的假阳性。</li><li>完全去除具有低NMS阈值的相邻检测将是次优的并且当在高重叠阈值处执行评估时将增加未命中率。</li><li>当使用高NMS阈值时，在一系列重叠阈值上测量的平均精度将下降。</li></ul><p>基于以上分析，改进$f(iou(\mathcal{M}, b_i))$为</p><script type="math/tex; mode=display">f(iou(\mathcal{M}, b_i)) = \begin{cases}    1 & iou(\mathcal{M}, b_i) < N_t \\    1 - iou(\mathcal{M}, b_i) & iou(\mathcal{M}, b_i) \geq N_t\end{cases}</script><p>其函数图像如下，在超过阈值时，为线性函数，且重叠越多，其抑制效果越大或称惩罚越多<br><img src="/2019/05/26/NMS-softer-NMS/f2.png" alt="f2"></p><p>但该函数不连续，可能导致NMS结果的突然改变，故修改为非线性连续函数如下</p><script type="math/tex; mode=display">f(iou(\mathcal{M}, b_i)) = \exp (-\frac{iou(\mathcal{M}, b_i)^2}{\sigma}), \forall b_i \notin \mathcal{D}</script><blockquote><p>注意到$x = \sqrt{\frac{\sigma}{2}}$为函数$f(x)$的拐点</p></blockquote><p><img src="/2019/05/26/NMS-softer-NMS/f3.png" alt="f3"></p><h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><p>对上面代码作如下修改</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> _f(<span class="keyword">float</span> x, <span class="keyword">float</span> sigma, <span class="keyword">int</span> soft)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">float</span> y = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (soft == <span class="number">0</span>)&#123;</span><br><span class="line">        y = x &lt; sigma? <span class="number">1</span>: <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        y = <span class="built_in">exp</span>(- <span class="built_in">pow</span>(x, <span class="number">2</span>) / sigma);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> _nms(detect* dets, <span class="keyword">int</span> n, <span class="keyword">float</span> sigma, <span class="keyword">int</span> mode, <span class="keyword">int</span> soft)</span><br><span class="line">&#123;</span><br><span class="line">    bsort(&amp;dets, n);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do nms</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (dets[i].score == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">        bbox a = dets[i].bx;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            bbox b = dets[j].bx;</span><br><span class="line">            dets[j].score *= _f(bbox_iou(a, b, mode), sigma, soft);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://ieeexplore.ieee.org/abstract/document/1699659" target="_blank" rel="noopener">Efficient Non-Maximum Suppression</a></li><li><a href="https://zhuanlan.zhihu.com/p/37489043" target="_blank" rel="noopener">非极大值抑制(Non-Maximum Suppression)</a></li><li><a href="https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/" target="_blank" rel="noopener">Non-Maximum Suppression for Object Detection in Python</a></li><li><a href="https://arxiv.org/abs/1704.04503" target="_blank" rel="noopener">Improving Object Detection With One Line of Code</a></li><li><a href="https://blog.csdn.net/u014380165/article/details/79502197" target="_blank" rel="noopener">Soft NMS算法笔记</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github清除commit记录</title>
      <link href="/2019/05/22/Github%E6%B8%85%E9%99%A4commit%E8%AE%B0%E5%BD%95/"/>
      <url>/2019/05/22/Github%E6%B8%85%E9%99%A4commit%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>当commit记录过多时，仓库会过大难以下载，本文介绍删除commit记录的方法。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>新建无任何文件的分支</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout --orphan new</span><br></pre></td></tr></table></figure></li><li><p>添加文件并提交</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add -A</span><br><span class="line">git commit -am "recommit"</span><br></pre></td></tr></table></figure></li><li><p>删除主分支</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -D master</span><br></pre></td></tr></table></figure></li><li><p>重命名当前分支</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -m master</span><br></pre></td></tr></table></figure></li><li><p>强制更新远程仓库</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push -f origin master</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> github </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Install nVidia drivers on Ubuntu</title>
      <link href="/2019/05/08/Install-nVidia-drivers-on-Ubuntu/"/>
      <url>/2019/05/08/Install-nVidia-drivers-on-Ubuntu/</url>
      
        <content type="html"><![CDATA[<h1 id="PPA安装"><a href="#PPA安装" class="headerlink" title="PPA安装"></a>PPA安装</h1><ol><li>禁用<code>nouveau</code>驱动<br>先将<code>Ubuntu</code>系统集成的显卡驱动程序<code>nouveau</code>从<code>linux</code>内核卸载</li></ol><p>查看当前驱动状态<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> lsmod | grep nouveau</span><br><span class="line">nouveau              1851392  1</span><br><span class="line">mxm_wmi                16384  1 nouveau</span><br><span class="line">i2c_algo_bit           16384  2 i915,nouveau</span><br><span class="line">ttm                   110592  1 nouveau</span><br><span class="line">drm_kms_helper        172032  2 i915,nouveau</span><br><span class="line">drm                   458752  8 drm_kms_helper,i915,ttm,nouveau</span><br><span class="line">wmi                    24576  3 wmi_bmof,mxm_wmi,nouveau</span><br><span class="line">video                  45056  3 thinkpad_acpi,i915,nouveau</span><br></pre></td></tr></table></figure></p><p>添加黑名单<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ll /etc/modprobe.d/blacklist.conf</span><br><span class="line">-rw-r--r-- 1 root root 1667 11月 13 05:54 /etc/modprobe.d/blacklist.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo chmod 666 /etc/modprobe.d/blacklist.conf</span><br><span class="line">[sudo] password for louishsu: </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo vim /etc/modprobe.d/blacklist.conf </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo chmod 644 /etc/modprobe.d/blacklist.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo update-initramfs -u</span><br><span class="line">update-initramfs: Generating /boot/initrd.img-4.18.0-17-generic</span><br><span class="line">...</span><br><span class="line">I: Set the RESUME variable to override this.</span><br></pre></td></tr></table></figure></p><p>重启后查看驱动状态，无输出表示禁用成功<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> lsmod | grep nouveau</span><br></pre></td></tr></table></figure></p><ol><li>安装驱动</li></ol><p>这里使用<code>PPA</code>方式安装，首先添加源<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line"><span class="meta">$</span> sudo apt-get update</span><br></pre></td></tr></table></figure></p><p>查看合适的驱动版本，如下<code>recommended</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ubuntu-drivers devices</span><br></pre></td></tr></table></figure></p><p>按<code>ctrl+alt+F1</code>进入<code>tty</code>模式，关闭图形桌面显示管理器<code>LightDM</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> service lightdm stop</span><br></pre></td></tr></table></figure></p><p>安装驱动<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo apt-get install nvidia-418</span><br><span class="line"><span class="meta">$</span> sudo reboot</span><br></pre></td></tr></table></figure></p><ol><li>查看安装情况<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo nvidia-smi</span><br><span class="line">Wed May  8 20:22:55 2019       </span><br><span class="line">+------------------------------------------------------+                       </span><br><span class="line">| NVIDIA-SMI 340.107    Driver Version: 340.107        |                       </span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GT 730M     Off  | 0000:04:00.0     N/A |                  N/A |</span><br><span class="line">| N/A   46C    P0    N/A /  N/A |    185MiB /  1023MiB |     N/A      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Compute processes:                                               GPU Memory |</span><br><span class="line">|  GPU       PID  Process name                                     Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    0            Not Supported                                               |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo nvidia-settings</span><br></pre></td></tr></table></figure></li></ol><h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><p>查看<a href="https://louishsu.xyz/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/" target="_blank" rel="noopener">Ubuntu编译安装Tensorflow</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://blog.csdn.net/10km/article/details/61191230" target="_blank" rel="noopener">ubuntu16.04下NVIDIA GTX965M显卡驱动PPA安装</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Face Detection: MTCNN</title>
      <link href="/2019/05/05/Face-Detection-MTCNN/"/>
      <url>/2019/05/05/Face-Detection-MTCNN/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>MTCNN即Multi-task Cascaded Convolutional Networks，利用深度学习方法进行人脸识别、检测与关键点定位，可谓结合机器视觉领域三大任务为一体。</p><p>该算法中，人脸检测与识别视作分类任务，即判别框内图像是否包含人脸；定位视作回归任务，共需确定7个坐标点，依次为：回归框左上、右下坐标，左眼、右眼、鼻尖、左嘴角、右嘴角坐标。</p><p>在设计损失函数时，分类任务采用交叉熵<code>(Cross Entropy)</code>，回归任务采用均方误差<code>(MSE)</code>，并且三个任务的损失，可给定不同的系数进行网络训练，使各网络侧重点不同，即<code>PNet</code>与<code>RNet</code>侧重于人脸识别与回归框定位，<code>ONet</code>侧重于关键点定位。</p><h1 id="算法对比"><a href="#算法对比" class="headerlink" title="算法对比"></a>算法对比</h1><p><img src="/2019/05/05/Face-Detection-MTCNN/detect_algorithm.jpg" alt="detect_algorithm"></p><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>共设计3个卷积网络，每个网络均可输出识别概率(1)、回归框坐标(2×2)、关键点坐标(5×2)，三个网络级联以获得良好的预测结果。各网络结构图如下</p><p><img src="/2019/05/05/Face-Detection-MTCNN/mtcnn.png" alt="mtcnn"></p><p>分类任务输出层可采用<code>softmax</code>，即视作多分类任务；或者采用<code>sigmoid</code>，视作二分类任务。</p><h2 id="1-P-Net：-Proposal-Network"><a href="#1-P-Net：-Proposal-Network" class="headerlink" title="1. P-Net： Proposal Network"></a>1. P-Net： Proposal Network</h2><p>检测任务比较重要的一步是产生数目足够多的候选框，<code>PNet</code>设计全卷积网络，可接受任意大小的图片输入，利用输出的特征图生成候选框，具体生成算法在检测算法中说明。该网络在训练时接受$12×12×3$的图像输入，各层参数如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size           input                output</span><br><span class="line">0 conv      10      3 x 3 / 1    12 x  12 x   3   -&gt;    10 x  10 x  10  0.000 BFLOPs</span><br><span class="line">1 max               2 x 2 / 2    10 x  10 x  10   -&gt;     5 x   5 x  10</span><br><span class="line">2 conv      16      3 x 3 / 1     5 x   5 x  10   -&gt;     3 x   3 x  16  0.000 BFLOPs</span><br><span class="line">3 conv      32      3 x 3 / 1     3 x   3 x  16   -&gt;     1 x   1 x  32  0.000 BFLOPs</span><br><span class="line">4 conv      15      1 x 1 / 1     1 x   1 x  32   -&gt;     1 x   1 x  15  0.000 BFLOPs</span><br></pre></td></tr></table></figure><h2 id="2-R-Net：-Refine-Network"><a href="#2-R-Net：-Refine-Network" class="headerlink" title="2. R-Net： Refine Network"></a>2. R-Net： Refine Network</h2><p><code>PNet</code>产生候选框后，将这些候选框内的图像数据分割并缩放到统一大小，输入到<code>RNet</code>改善识别结果。该网络最后增加全连接层，仅接受$24×24×3$的图像输入，各层参数如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size           input                output</span><br><span class="line">0 conv     28       3 x 3 / 1    24 x  24 x   3   -&gt;    22 x  22 x  28  0.001 BFLOPs</span><br><span class="line">1 max               3 x 3 / 2    22 x  22 x  28   -&gt;    11 x  11 x  28</span><br><span class="line">2 conv     48       3 x 3 / 1    11 x  11 x  28   -&gt;     9 x   9 x  48  0.002 BFLOPs</span><br><span class="line">3 max               3 x 3 / 2     9 x   9 x  48   -&gt;     4 x   4 x  48</span><br><span class="line">4 conv     64       2 x 2 / 1     4 x   4 x  48   -&gt;     3 x   3 x  64  0.000 BFLOPs</span><br><span class="line">5 connected                                 576   -&gt;               128</span><br><span class="line">6 connected                                 128   -&gt;                15</span><br></pre></td></tr></table></figure><h2 id="3-O-Net：-Output-Network"><a href="#3-O-Net：-Output-Network" class="headerlink" title="3. O-Net： Output Network"></a>3. O-Net： Output Network</h2><p>该网络功能与<code>RNet</code>相同，不同的是分辨率更高，网络层次更深，且训练过程中，损失的设置更偏重于关键点的回归。接受$48×48×3$的图像输入，各层参数如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer     filters    size              input                output</span><br><span class="line">0 conv     32       3 x 3 / 1    48 x  48 x   3   -&gt;    46 x  46 x  32  0.004 BFLOPs</span><br><span class="line">1 max               3 x 3 / 2    46 x  46 x  32   -&gt;    23 x  23 x  32</span><br><span class="line">2 conv     64       3 x 3 / 1    23 x  23 x  32   -&gt;    21 x  21 x  64  0.016 BFLOPs</span><br><span class="line">3 max               3 x 3 / 2    21 x  21 x  64   -&gt;    10 x  10 x  64</span><br><span class="line">4 conv     64       3 x 3 / 1    10 x  10 x  64   -&gt;     8 x   8 x  64  0.005 BFLOPs</span><br><span class="line">5 max               2 x 2 / 2     8 x   8 x  64   -&gt;     4 x   4 x  64</span><br><span class="line">6 conv    128       2 x 2 / 1     4 x   4 x  64   -&gt;     3 x   3 x 128  0.001 BFLOPs</span><br><span class="line">7 connected                                1152   -&gt;               256</span><br><span class="line">8 connected                                 256   -&gt;                15</span><br></pre></td></tr></table></figure><h1 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h1><h2 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1. 数据集"></a>1. 数据集</h2><ul><li><p>人脸检测<br>  <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" rel="noopener">WIDER FACE</a>是目前最常用的训练集，也是目前最大的公开训练集，人工标注的风格比较友好，适合训练。总共32203图像，393703标注人脸，目前难度最大，各种难点比较全面：尺度，姿态，遮挡，表情，化妆，光照等。</p><p>  <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" rel="noopener">WIDER FACE</a>有以下特点：</p><ul><li>图像分辨率普遍偏高，所有图像的宽都缩放到1024，最小标注人脸10*10，都是彩色图像；</li><li>每张图像的人脸数据偏多，平均12.2人脸/图，密集小人脸非常多；</li><li>分训练集train/验证集val/测试集test，分别占40%/10%/50%，而且测试集的标注结果(ground truth)没有公开，需要提交结果给官方比较，更加公平公正，而且测试集非常大，结果可靠性极高；</li><li>根据EdgeBox的检测率情况划分为三个难度等级：Easy, Medium, Hard。</li></ul></li></ul><p><img src="/2019/05/05/Face-Detection-MTCNN/detect_demo.png" alt="detect_demo"></p><ul><li><p>关键点定位<br>  <a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm" target="_blank" rel="noopener">CNN FACE POINT</a>，包含5个关键点位置。</p><blockquote><p>Training set: <a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN/data/train.zip" target="_blank" rel="noopener">Download</a><br>  It contains 5,590 LFW images and 7,876 other images downloaded from the web. The training set and validation set are defined in trainImageList.txt and testImageList.txt, respectively. Each line of these text files starts with the image name, followed by the boundary positions of the face bounding box retured by our face detector, then followed by the positions of the five facial points.<br>Testing set: <a href="http://mmlab.ie.cuhk.edu.hk/archive/CNN/data/test.zip" target="_blank" rel="noopener">Download</a><br>  It contains the 1,521 BioID images, 781 LFPW training images, and 249 LFPW test images used in our testing, together with the text files recording the boundary positions of the face bounding box retured by our face detector for each dataset. A few images that our face detector failed are not listed in the text files. LFPW images are renamed for the convenience of processing.</p></blockquote><p>  <img src="/2019/05/05/Face-Detection-MTCNN/landmark_demo.png" alt="landmark_demo"></p></li></ul><h2 id="2-训练数据生成"><a href="#2-训练数据生成" class="headerlink" title="2. 训练数据生成"></a>2. 训练数据生成</h2><p>采用了<code>Hard Example Mining</code>，后一网络的训练数据由前一网络结果生成，即先用训练好的前一网络进行数据评估，在评分较低、难以检测的数据中继续采样。在生成数据时，使用数据增广。</p><p>七个关键点坐标转换为偏移量<code>(offset)</code>，使其不受图像缩放影响，且数值较小便于网络收敛，计算方法如下</p><p><img src="/2019/05/05/Face-Detection-MTCNN/offsets.jpg" alt="offsets"></p><ul><li><p>Bounding Box</p><script type="math/tex; mode=display">  offset_{x_1'} = \frac{x_1' - x_1}{size}</script><script type="math/tex; mode=display">  offset_{x_2'} = \frac{x_2' - x_2}{size}</script><ul><li>$y_n$同$x_n$;</li><li>$(x_1, y_1)$表示左上角点位置，$(x_2, y_2)$表示右下角点位置;</li><li>$(x_1’, y_1’)$表示<code>ground true</code>矩形方框位置;</li><li>$size$表示方形回归框边长，即$size = x_2 - x_1 = y_2 - y_1$;</li></ul></li><li><p>Landmark<br>  均以<strong>正方形回归框左上方点</strong>坐标作为基准</p><script type="math/tex; mode=display">  offset_{x_n''} = \frac{x_n'' - x_1}{size}</script><script type="math/tex; mode=display">  offset_{y_n''} = \frac{y_n'' - y_1}{size}</script><ul><li>$(x_n’’, y_n’’)$表示五个关键点坐标;</li><li>$(x_1’, y_1’)$表示回归框左上角点位置;</li><li>$size$表示方形回归框边长，即$size = x_2 - x_1 = y_2 - y_1$;</li></ul></li></ul><p>根据<code>groudtruth</code>随机偏移和旋转，切割人脸数据，根据<code>iou</code>评分，共记作3种标签的样本，其标签分别为</p><ul><li><p><code>Positive(1)</code>:<br>  $iou &gt; 0.65$，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/positive/404031.jpg 1.0 0.18 -0.09 0.15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br></pre></td></tr></table></figure></li><li><p><code>Negative(0)</code>:<br>  $iou &lt; 0.3$，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/negative/92073.jpg 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br></pre></td></tr></table></figure></li><li><p><code>Part(-1)</code>:<br>  $0.4 \leq iou \leq 0.65$，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/part/749569.jpg -1.0 -0.04 0.07 -0.15 0.36 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br></pre></td></tr></table></figure></li><li><p>此外，关键点数据标签记作<code>Landmark(-2)</code>，其数据格式样例如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xxx/train_PNet_landmark_aug/4.jpg -2.0 0.0 0.0 0.0 0.0 0.24367088607594936 0.17405063291139242 0.7563291139240507 0.2310126582278481 0.48417721518987344 0.6170886075949367 0.2310126582278481 0.8069620253164557 0.6677215189873418 0.8575949367088608</span><br></pre></td></tr></table></figure></li></ul><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="1-Classification-loss"><a href="#1-Classification-loss" class="headerlink" title="1. Classification loss"></a>1. Classification loss</h2><p>人脸识别为分类任务，采用二分类交叉熵损失函数，即</p><script type="math/tex; mode=display">L^{(i)}_{cls} = - y^{(i)} \log (p^{(i)}) - (1 - y^{(i)}) \log (1 - p^{(i)})</script><p>注意，在计算分类损失时，将<code>Negative(0)</code>, <code>Part(-1)</code>, <code>Landmark(-2)</code>标签均视作<code>0</code>。</p><h2 id="2-Regression-loss"><a href="#2-Regression-loss" class="headerlink" title="2. Regression loss"></a>2. Regression loss</h2><p>回归任务，采用最小方差准则，即</p><script type="math/tex; mode=display">L^{(i)}_{bbox} = \frac{1}{4} \sum_{j=1}^4 (pred^{(i)}_j - gt^{(i)}_j)^2</script><script type="math/tex; mode=display">L^{(i)}_{landmark} = \frac{1}{10} \sum_{j=1}^{10} (pred^{(i)}_j - gt^{(i)}_j)^2</script><p>注意，在计算回归框损失时，仅对<code>Positive(1)</code>, <code>Part(-1)</code>样本进行，计算关键点损失时，仅对<code>Landmark(-2)</code>样本进行。</p><h2 id="3-OHEM"><a href="#3-OHEM" class="headerlink" title="3. OHEM"></a>3. OHEM</h2><p><code>OHEM</code>即<code>Online Hard Example Mining</code>，在线硬样本数据挖掘，即对于当前批次数据，计算损失时，选取总损失值最大的<code>k</code>个样本，计算其均值作为该批次的损失值。</p><h2 id="4-Total-Loss"><a href="#4-Total-Loss" class="headerlink" title="4. Total Loss"></a>4. Total Loss</h2><p>三个网络赋予各损失的系数不同， 使其偏重于其中某个任务</p><script type="math/tex; mode=display">L_{total} = \frac{1}{topk} \sum_{i=0}^{topk} coef_{cls} L^{(i)}_{cls} + coef_{bbox} L^{(i)}_{bbox} + coef_{landmark} L^{(i)}_{landmark}</script><p>其中$topk$即<code>OHEM</code>计算得到的样本数。</p><details><summary>PyTorch实现</summary><pre><code>class MtcnnLoss(nn.Module):    def __init__(self, cls, bbox, landmark, ohem=0.7):        super(MtcnnLoss, self).__init__()        self.cls = cls        self.bbox = bbox        self.landmark = landmark        self.ohem = ohem        self.bce = nn.BCEWithLogitsLoss(reduction='none')        self.mse = nn.MSELoss(reduction='none')    def forward(self, pred, gt):        """        Params:            pred:   {tensor(N, n) or tensor(N, n, 1, 1)}            gt:     {tensor(N, n)}        Notes:            y_true        """        N = pred.shape[0]        pred = pred.view(N, -1)        ## origin label        gt_labels = gt[:, 0]        ## pos -> 1, neg -> 0, others -> 0        pred_cls = pred[:, 0]        gt_cls = gt_labels.clone(); gt_cls[gt_labels!=1.0] = 0.0        loss_cls = self.bce(pred_cls, gt_cls)        # ohem        n_keep = int(self.ohem * loss_cls.shape[0])        loss_cls = torch.mean(torch.topk(loss_cls, n_keep)[0])        ## label=1 or label=-1 then do regression        idx = (gt_labels==1)^(gt_labels==-1)        pred_bbox = pred[idx, 1: 5]        gt_bbox = gt[idx, 1: 5]        loss_bbox = self.mse(pred_bbox, gt_bbox)        loss_bbox = torch.mean(loss_bbox, dim=1)        # ohem        n_keep = int(self.ohem * loss_bbox.shape[0])        loss_bbox = torch.mean(torch.topk(loss_bbox, n_keep)[0])            ## keep label =-2  then do landmark detection        idx = gt_labels==-2        pred_landmark = pred[idx, 5:]        gt_landmark = gt[idx, 5:]        loss_landmark = self.mse(pred_landmark, gt_landmark)        loss_landmark = torch.mean(loss_landmark, dim=1)        # ohem        n_keep = int(self.ohem * loss_landmark.shape[0])        loss_landmark = torch.mean(torch.topk(loss_landmark, n_keep)[0])        ## total loss        loss_total = self.cls*loss_cls + self.bbox*loss_bbox + self.landmark*loss_landmark        return loss_total, loss_cls, loss_bbox, loss_landmarkloss_coef = {    'PNet': [1.0, 0.5, 0.5],    'RNet': [1.0, 0.5, 0.5],    'ONet': [1.0, 0.5, 1.0],}</code></pre></details><h1 id="检测算法"><a href="#检测算法" class="headerlink" title="检测算法"></a>检测算法</h1><p>检测算法以功能区分，主要分成两个部分：候选框生成与候选框筛除。</p><h2 id="1-候选框生成"><a href="#1-候选框生成" class="headerlink" title="1. 候选框生成"></a>1. 候选框生成</h2><p><img src="/2019/05/05/Face-Detection-MTCNN/pnet1.jpg" alt="pnet1"></p><p><img src="/2019/05/05/Face-Detection-MTCNN/pnet2.jpg" alt="pnet2"></p><p><img src="/2019/05/05/Face-Detection-MTCNN/pnet_gen_box.jpg" alt="pnet_gen_box"></p><p>该步骤使用的网络为<code>PNet</code>。指定超参数<code>minface</code>，对于任意尺寸输入的图像<code>H × W</code>，先将其缩放</p><script type="math/tex; mode=display">H_c = H × \frac{12}{minface}</script><script type="math/tex; mode=display">W_c = W × \frac{12}{minface}</script><p>指定超参数<code>factor</code>，更新缩小尺度，将图片缩小，在每个尺度上进行计算，即</p><script type="math/tex; mode=display">H_c := H_c × factor</script><script type="math/tex; mode=display">W_c := W_c × factor</script><p>对于某一尺度下的图片得到的运算特征图，</p><script type="math/tex; mode=display">Feat_{h×w×15} = PNet(input)</script><p>提取其分类层输出特征图$Feat_{cls}$，设定阈值<code>thresh</code>，对于大于阈值的点，按下式生成候选框</p><script type="math/tex; mode=display">x_1 = stride × i × \frac{1}{scale}</script><script type="math/tex; mode=display">y_1 = stride × j × \frac{1}{scale}</script><script type="math/tex; mode=display">x_2 = x_1 + \frac{cellsize}{scale}</script><script type="math/tex; mode=display">y_2 = y_1 + \frac{cellsize}{scale}</script><p>其中<code>cellsize</code>为超参数，一般指定为<code>cellsize=12</code></p><h2 id="2-候选框筛除"><a href="#2-候选框筛除" class="headerlink" title="2. 候选框筛除"></a>2. 候选框筛除</h2><p><img src="/2019/05/05/Face-Detection-MTCNN/rnet.jpg" alt="rnet"></p><p>改步使用网络<code>PNet</code>与<code>ONet</code>，依次对上一层网络进行<code>refine</code>，计算方法一致。</p><ul><li>获取上一层网络输出回归框，截取图片中相应位置的图像数据，并缩放到对应尺寸；</li><li>前向计算，得到特征输出；</li><li>设定阈值，只保留分类评估大于阈值的结果；</li><li>对剩余结果进行<code>NMS</code>，输出结果；</li></ul><h2 id="3-NMS"><a href="#3-NMS" class="headerlink" title="3. NMS"></a>3. NMS</h2><p><img src="/2019/05/05/Face-Detection-MTCNN/nms.jpg" alt="nms"></p><p>例如，有中三个候选框a, b, c，其评分依次为0.8, 0.7, 0.9，设定NMS阈值</p><script type="math/tex; mode=display">thresh=0.4</script><ol><li>先将其排序，以降序排序为c, a, b；</li><li>保存当前评分最高的回归框，即c；</li><li><p>计算c与a, b的IoU，计算方法如下</p><p> <img src="/2019/05/05/Face-Detection-MTCNN/iou.jpg" alt="iou"></p><script type="math/tex; mode=display"> IoU = \frac{Intersection}{Union}</script><p> 其中</p><script type="math/tex; mode=display"> Intersection = w_{inter} × h_{inter}</script><script type="math/tex; mode=display"> w_{inter} = \min (x^a_2, x^b_2) - \max (x^a_1, x^b_1)</script><script type="math/tex; mode=display"> h_{inter} = \min (y^a_2, y^b_2) - \max (y^a_1, y^b_1)</script><p> 而</p><script type="math/tex; mode=display"> Union = Area_a + Area_b - Intersection</script></li><li><p>则c与a, b的IoU为</p><script type="math/tex; mode=display"> IoU_{a,c} = \frac{1×8}{10×9+9×11-1×8}=0.044</script><script type="math/tex; mode=display"> IoU_{a,b} = \frac{4×6}{10×9+10×11-4×6}=0.136</script><p> 均小于阈值，故无框被删除。</p></li><li><p>保存当前评分最高的回归框，即a；</p></li><li><p>计算a与b的IoU</p><script type="math/tex; mode=display"> IoU_{a,b} = \frac{7×9}{9×11+10×11-7×9} = 0.432</script><p> 大于阈值，删除候选框b</p></li><li>最终保留a，c。</li></ol><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>详情查看<a href="https://github.com/isLouisHsu/MTCNN_Darknet/tree/master/torch_mtcnn/detector.py" target="_blank" rel="noopener">isLouisHsu/MTCNN_Darknet/torch_mtcnn/detector.py - Github</a>。</p><h1 id="附：利用关键点进行图像对齐"><a href="#附：利用关键点进行图像对齐" class="headerlink" title="附：利用关键点进行图像对齐"></a>附：利用关键点进行图像对齐</h1><ol><li><p>变换矩阵$M$的求解<br> 例如现有$n$个关键点</p><script type="math/tex; mode=display"> xy = \left[\begin{matrix}     x_1 & y_1 \\     x_2 & y_2 \\     ... & ... \\     x_n & y_n \\ \end{matrix}\right]</script><p> 希望对齐后的坐标点为</p><script type="math/tex; mode=display"> \hat{xy} = \left[\begin{matrix}     \hat{x_1} & \hat{y_1} \\     \hat{x_2} & \hat{y_2} \\     ...  & ...  \\     \hat{x_n} & \hat{y_n} \\ \end{matrix}\right]</script><p> 构造矩阵</p><script type="math/tex; mode=display"> X_{2n\times4} = \left[\begin{matrix}     \vec{x} &  \vec{y} & \vec{1} & \vec{0} \\     \vec{y} & -\vec{x} & \vec{0} & \vec{1} \end{matrix}\right]</script><script type="math/tex; mode=display"> b_{2n} = \left[\begin{matrix}     \hat{x_1} & \hat{x_2} & \cdots & \hat{x_n} &     \hat{y_1} & \hat{y_2} & \cdots & \hat{y_n} \end{matrix}\right]^T</script><p> 其中</p><script type="math/tex; mode=display"> \vec{x} = \left[\begin{matrix}     x_1 & x_2 & \cdots & x_n \end{matrix}\right]^T</script><script type="math/tex; mode=display"> \vec{y} = \left[\begin{matrix}     y_1 & y_2 & \cdots & y_n \end{matrix}\right]^T</script><script type="math/tex; mode=display"> \vec{1} = \left[\begin{matrix}     1 & 1 & \cdots & 1 \end{matrix}\right]^T</script><script type="math/tex; mode=display"> \vec{0} = \left[\begin{matrix}     0 & 0 & \cdots & 0 \end{matrix}\right]^T</script><p> 求解下式解向量$r_{4\times1}$</p><script type="math/tex; mode=display"> X \cdot r = b</script><p> 注意增广矩阵的秩</p><script type="math/tex; mode=display">\text{rank}(X) < rank([X | b])</script><p> 上式无解，可使用伪逆求解</p><script type="math/tex; mode=display"> r = (X^T X + \lambda I)^{-1} X^T b</script><p> 构造矩阵</p><script type="math/tex; mode=display"> R = \left[\begin{matrix}     r_1 & -r_2 & 0 \\     r_2 &  r_1 & 0 \\     r_3 & -r_4 & 1 \\ \end{matrix}\right]</script><p> 则变换矩阵$M$可由下式求解</p><script type="math/tex; mode=display"> \left[\begin{matrix}     M^T & \begin{matrix}         0 \\ 0 \\ 1     \end{matrix} \end{matrix}\right] = R^{-1}</script><p> 即$M$为$R^{-1}$的前两列。</p></li><li><p>坐标变换</p><script type="math/tex; mode=display"> M = \left[\begin{matrix}     m_{11} & m_{12} & m_{13} \\ m_{21} & m_{22} & m_{23} \end{matrix}\right]</script><p> 对于坐标$(x, y)$，其变换后的坐标$(\hat{x}, \hat{y})$为</p><script type="math/tex; mode=display"> \left[\begin{matrix}     \hat{x} \\ \hat{y} \\ \end{matrix}\right] = M \left[\begin{matrix}     x \\ y \\ 1 \end{matrix}\right]</script></li></ol><p>几个关键的函数，C/C++实现如下，详情可查看<a href="https://github.com/isLouisHsu/MobileFaceNet_Darknet/blob/master/src/cp2form.c" target="_blank" rel="noopener">isLouisHsu/MobileFaceNet_Darknet/src/cp2form.c</a><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * @param</span></span><br><span class="line"><span class="comment"> *      uv: [u, v]， Nx2</span></span><br><span class="line"><span class="comment"> *      xy: [x, y]， Nx2</span></span><br><span class="line"><span class="comment"> * @return</span></span><br><span class="line"><span class="comment"> * @notes</span></span><br><span class="line"><span class="comment"> * -    Xr = Y   ===&gt;  r = (X^T X + \lambda I)^&#123;-1&#125; X^T Y</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">CvMat* _findNonreflectiveSimilarity(<span class="keyword">const</span> CvMat* uv, <span class="keyword">const</span> CvMat* xy)</span><br><span class="line">&#123;</span><br><span class="line">    CvMat* X = _stitch(xy);                         <span class="comment">// 2N x  4</span></span><br><span class="line"></span><br><span class="line">    CvMat* XT = cvCreateMat(X-&gt;cols, X-&gt;rows, CV_32F);  </span><br><span class="line">    cvTranspose(X, XT);                             <span class="comment">//  4 x 2N</span></span><br><span class="line"></span><br><span class="line">    CvMat* XTX = cvCreateMat(XT-&gt;rows, X-&gt;cols, CV_32F);</span><br><span class="line">    cvMatMul(XT, X, XTX);                           <span class="comment">//  4 x  4</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; XTX-&gt;rows; i++) XTX-&gt;data.fl[i*XTX-&gt;rows + i] += <span class="number">1e-15</span>;</span><br><span class="line"></span><br><span class="line">    CvMat* XTXi = cvCreateMat(XTX-&gt;rows, XTX-&gt;cols, CV_32F); </span><br><span class="line">    cvInvert(XTX, XTXi, CV_LU);                     <span class="comment">//  4 x  4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// -----------------------------------------------------------------------</span></span><br><span class="line">    </span><br><span class="line">    CvMat* uvT = cvCreateMat(uv-&gt;cols, uv-&gt;rows, CV_32F); </span><br><span class="line">    cvTranspose(uv, uvT);                           <span class="comment">//  2 x  N</span></span><br><span class="line">    CvMat header; </span><br><span class="line">    CvMat* YT = cvReshape(uvT, &amp;header, <span class="number">0</span>, <span class="number">1</span>);      <span class="comment">//  1 x 2N    TODO</span></span><br><span class="line">    CvMat* Y = cvCreateMat(YT-&gt;cols, YT-&gt;rows, CV_32F);  </span><br><span class="line">    cvTranspose(YT, Y);                             <span class="comment">// 2N x  1</span></span><br><span class="line">    </span><br><span class="line">    CvMat* XTXiXT = cvCreateMat(XTXi-&gt;rows, XT-&gt;cols, CV_32F);</span><br><span class="line">    CvMat* r = cvCreateMat(XTXiXT-&gt;rows, Y-&gt;cols, CV_32F);</span><br><span class="line">    cvMatMul(XTXi, XT, XTXiXT); cvMatMul(XTXiXT, Y, r);       <span class="comment">//  4 x  1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// -----------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;X); cvReleaseMat(&amp;XT); </span><br><span class="line">    cvReleaseMat(&amp;XTX); cvReleaseMat(&amp;XTXi); cvReleaseMat(&amp;XTXiXT);</span><br><span class="line">    cvReleaseMat(&amp;uvT); cvReleaseMat(&amp;Y);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// =======================================================================</span></span><br><span class="line"></span><br><span class="line">    CvMat* R = cvCreateMat(<span class="number">3</span>, <span class="number">3</span>, CV_32F);</span><br><span class="line">    R-&gt;data.fl[<span class="number">0</span> * <span class="number">3</span> + <span class="number">0</span>] = r-&gt;data.fl[<span class="number">0</span>]; R-&gt;data.fl[<span class="number">0</span> * <span class="number">3</span> + <span class="number">1</span>] = -r-&gt;data.fl[<span class="number">1</span>]; R-&gt;data.fl[<span class="number">0</span> * <span class="number">3</span> + <span class="number">2</span>] = <span class="number">0.</span>;</span><br><span class="line">    R-&gt;data.fl[<span class="number">1</span> * <span class="number">3</span> + <span class="number">0</span>] = r-&gt;data.fl[<span class="number">1</span>]; R-&gt;data.fl[<span class="number">1</span> * <span class="number">3</span> + <span class="number">1</span>] =  r-&gt;data.fl[<span class="number">0</span>]; R-&gt;data.fl[<span class="number">1</span> * <span class="number">3</span> + <span class="number">2</span>] = <span class="number">0.</span>;</span><br><span class="line">    R-&gt;data.fl[<span class="number">2</span> * <span class="number">3</span> + <span class="number">0</span>] = r-&gt;data.fl[<span class="number">2</span>]; R-&gt;data.fl[<span class="number">2</span> * <span class="number">3</span> + <span class="number">1</span>] =  r-&gt;data.fl[<span class="number">3</span>]; R-&gt;data.fl[<span class="number">2</span> * <span class="number">3</span> + <span class="number">2</span>] = <span class="number">1.</span>;</span><br><span class="line">    </span><br><span class="line">    CvMat* Ri = cvCreateMat(R-&gt;cols, R-&gt;rows, CV_32F);</span><br><span class="line">    cvInvert(R, Ri, CV_LU);</span><br><span class="line"></span><br><span class="line">    CvMat* MT = cvCreateMat(<span class="number">3</span>, <span class="number">2</span>, CV_32F);</span><br><span class="line">    cvGetSubRect(Ri, MT, cvRect(<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>));</span><br><span class="line">    </span><br><span class="line">    CvMat* M = cvCreateMat(MT-&gt;cols, MT-&gt;rows, CV_32F);</span><br><span class="line">    cvTranspose(MT, M);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// -----------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;r); cvReleaseMat(&amp;R); cvReleaseMat(&amp;Ri); cvReleaseMat(&amp;MT);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * @param</span></span><br><span class="line"><span class="comment"> *      uv: [u, v]， Nx2</span></span><br><span class="line"><span class="comment"> *      xy: [x, y]， Nx2</span></span><br><span class="line"><span class="comment"> * @return</span></span><br><span class="line"><span class="comment"> * @notes</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">CvMat* _findReflectiveSimilarity(<span class="keyword">const</span> CvMat* uv, <span class="keyword">const</span> CvMat* xy)</span><br><span class="line">&#123;</span><br><span class="line">    CvMat* xyR = cvCloneMat(xy);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> r = <span class="number">0</span>; r &lt; xyR-&gt;rows; r++) xyR-&gt;data.fl[r*xyR-&gt;cols] *= <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    CvMat* M1 = _findNonreflectiveSimilarity(uv, xy);</span><br><span class="line">    CvMat* M2 = _findNonreflectiveSimilarity(uv, xyR);</span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;xyR);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> r = <span class="number">0</span>; r &lt; M2-&gt;rows; r++) M2-&gt;data.fl[r*M2-&gt;cols] *= <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    CvMat* xy1 = _tformfwd(M1, uv);</span><br><span class="line">    CvMat* xy2 = _tformfwd(M2, uv);</span><br><span class="line">    cvSub(xy1, xy, xy1, <span class="literal">NULL</span>); cvSub(xy2, xy, xy2, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> norm1 = _matrixNorm2(xy1);</span><br><span class="line">    <span class="keyword">float</span> norm2 = _matrixNorm2(xy2);</span><br><span class="line"></span><br><span class="line">    cvReleaseMat(&amp;xy1); cvReleaseMat(&amp;xy2);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (norm1 &lt; norm2)&#123;</span><br><span class="line">        cvReleaseMat(&amp;M2);</span><br><span class="line">        <span class="keyword">return</span> M1;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cvReleaseMat(&amp;M1);</span><br><span class="line">        <span class="keyword">return</span> M2;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://arxiv.org/abs/1604.02878" target="_blank" rel="noopener">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</a></li><li><a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" target="_blank" rel="noopener">kpzhang93/MTCNN_face_detection_alignment</a></li><li><a href="https://github.com/AITTSMD/MTCNN-Tensorflow" target="_blank" rel="noopener">AITTSMD/MTCNN-Tensorflow</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>无所畏</title>
      <link href="/2019/04/30/%E6%97%A0%E6%89%80%E7%95%8F/"/>
      <url>/2019/04/30/%E6%97%A0%E6%89%80%E7%95%8F/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/04/30/无所畏/1.jpg" alt="1"></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>世间没有佛，但是有带着佛性的人。</p><h1 id="精选"><a href="#精选" class="headerlink" title="精选"></a>精选</h1><h2 id="壹-成功十要素"><a href="#壹-成功十要素" class="headerlink" title="壹 成功十要素"></a>壹 成功十要素</h2><center>一命二运三风水<br>四积阴德五读书<br>六名七相八敬神<br>九交贵人十养生<br></center><ol><li>曾几何时，我们除了未来一无所有，我们充满好奇，我们有使不完的力气，我们不怕失去，我们眼里有光，我们为建设祖国而读书，我们下身肿胀，我们激素吱吱作响，我们热爱姑娘，我们万物生长。</li><li><p>如何避免成为油腻中年男</p><ul><li>不要成为胖子，曾经玉树临风，现在风狂树残；</li><li>不要停止学习，吹牛能让我们有瞬间快感，但不能改变我们对一些事物所知甚少的事实；</li><li>不要待着不动，说走就走，去散步，去旅行，也好；</li><li>不要当众谈性，关于眼神(盯着女生看)的告诫，也适用于权、钱等其他领域；</li><li>不要追忆从前，积攒唠叨从前的力气，再创业，再创造，再恋爱，我们还能攻城略地，杀伐战取；</li><li>不要教育晚辈，不愤不启；</li><li>不要给别人添麻烦；</li><li>不要停止购物，完全没了欲望，失去对美好事物的贪心，生命也就没有乐趣；</li><li>不要脏兮兮，少年时代的脏是不羁，中年时代的脏是真脏；</li><li><p>不要鄙视和年龄无关的人类习惯，所有的世道变坏，都是从鄙视文艺开始的；</p><p>因为苦逼而牛逼，因为逗逼而二逼，因为装逼而傻逼。愿我们原理油腻和猥琐，敬爱女生，过好余生，让世界更美好。</p></li></ul></li><li><p>更可怕的是成为了油腻青年</p><ul><li>装懂，多学习，多研究，对真正热爱之事，真正投入精力，向那些可以就防晒美白详细说出八种不同方法的女性好好学习；</li><li>着急，“夫水之己也不厚，其负大舟也无力”；</li><li>逐利，只有对钱的热忱却没有理想，即使站在了风口，也不会成为那只“会飞的猪”；</li><li>不要迷恋肉身；</li><li>迷恋手机，比起摸不到心爱的姑娘的手，摸不到自己的手机似乎要严重百倍；</li><li>不靠谱，将来总有一天，你会明白，困境、死境都是自己曾经立起又自己放倒的目标；</li><li>不敢真，对爱的人不敢说“爱”，对不爽的事不敢说“不”，不敢承认自己的处境，不敢承认失败然后从头再来。时过境迁，回过头来，要拿真心对世界的时候，大抵已经找不到心在哪儿了；</li><li>假佛系，假装自己无欲无求，其实只是懒得追求，你就不厌倦自己吗？</li><li>审美差，如今担心会跟审美不好的人撞了女朋友的脸(整容)，哈哈哈哈；</li><li>不要“脸”，所谓“相由心生”，脸上的油光，就是心里的油渣；</li></ul></li><li><p>“极品”男人如何极致装逼</p><ul><li>写信，比如冯唐、比如曾巩；</li><li>跑步；</li><li>喝茶；</li><li>古物，从骨子里明白拥有只是暂时，“欣于所遇，暂得于己，快然自足”；</li><li>言语，极致地吹牛逼也是极致装逼的一种，立言也是立德、立功、立言三不朽的一种；</li><li>读书；</li><li>情怀，极致装逼如下，“为天地立心，为生民立命，为往圣继绝学，为万世开太平”；</li><li>喝酒，和好玩儿的人喝，喝完能背出很多唐诗和楚辞；</li><li>养生，高逼格的养生是乐生，是在乐生的基础上长生。我老爸抽烟，从十二岁开始抽，现在八十三岁了，他的口头禅是：“天亮了，又赚了。”</li><li><p>修佛，高逼格的修佛是在日常的劳作里、阳光里、花花草草里、众生皆苦里、生命终极无意义里，试图体会到蹦蹦跳跳的快活；</p><p>其实，如果志存高远，“三观”正，逼格正，装逼装久了，就是身、心、灵的一部分了。装逼装极致了，就得大成就了。装逼的过程就是学习的过程，就是感受活着的过程，就是实现理想的过程。</p></li></ul></li><li><p>那些爱我们或者爱过我们的女生，在她们的一生中要花很多时间陪护我们这些装腔犯，安静地、积极地、有创造力地陪我们装逼好多年。……。如果爱不在了，那就不用管上面说的一切了，让他找别的姑娘配他装逼陪他飞吧。</p></li><li><p>天天临深履薄，这辈子好惨，而且睡眠毁了、人毁了，也就什么都没了。我不想这样一辈子，我不想总梦见那些提心吊胆的事儿，我还想梦见我以前那些美丽的女朋友以及那些被梨花照过的时光，我提笔在笔记本的扉页上，郑重地写下了我的九字真言：“不着急，不害怕，不要脸。”</p><ul><li>对时间的态度：不着急。有时候，关切是不问；有时候，不做比做什么都强；</li><li>对结果的态度：不害怕；</li><li><p>对他评的态度：不要脸。“是非审之于己，毁誉听之于人，得失安之于数”；</p><blockquote><p>补充一点变成四点吧：不着急，不害怕，不要脸，不抱怨。</p></blockquote></li></ul></li><li><p>“这是最好的时代，这是最坏的时代；这是智慧的时代，这是愚蠢的时代；这是信仰的时期，这是怀疑的时期；这是光明的季节，这是黑暗的季节；这是希望之春，这是失望之冬；人们面前有着各样事物，人们面前一无所有；人们正在直登天堂，人们正在直下地狱。”</p></li><li>女生把自己整修得越来越像孪生姐妹，男生把自己禅修得越来越无聊。菜越来越没有菜味儿，肉越来越没有肉味儿，街上早就没有野花可以摘了，街上早就没有板砖可以拍了。</li><li>面对我们阻止不了的时代变化，多使用肉体，多去狂喜与伤心，多去创造，活出更多人样儿。</li><li>人类改变不了人性中的恶，创造完成后保护，保护不住后破坏，破坏后再创造，永陷轮回。</li><li>亲爱的，给我写首情诗好吗？越虐心越好。</li><li>降维攻击定义：你有道德我没道德，你死，我活；你我都是人你还要做人，我自降为禽兽，你死，我活。</li></ol><h2 id="贰-爱情如何对抗时间"><a href="#贰-爱情如何对抗时间" class="headerlink" title="贰 爱情如何对抗时间"></a>贰 爱情如何对抗时间</h2><center>女人还是要自强<br>不容易生病的身体<br>够用的收入<br>养心的爱好<br>强大到浑蛋的小宇宙</center><blockquote><p>男人同~</p></blockquote><ol><li>再过一些年，或许宇宙这盆火也会最终熄灭，世界彻底安静下来，时间也瘫倒在空间里，仿佛一只死狗瘫倒在地板上。</li><li>爱情大概始于一些及其美妙的刹那。……。在刹那间，希望时间停滞，甚至无疾而终，在刹那间，就此死去。……。幸或者不幸的是，人想死的时候很难死掉，梦幻泡影、闪电烟花之后，生活继续。爱情如何对抗那些璀璨一刹那之外的漫长时间？</li><li>你看他起高楼，你看他楼塌了。起高楼时，这个男的不一定能守得住底线；楼塌时，这个男的不一定能跑得了。</li><li>自己穿暖，才是真暖；自己真暖，才有资格相互温暖。</li><li>梦里三月桃花，二人一马。</li><li>身体极累的时候，心极伤的时候，身外有酒，白、黄、红，心里有姑娘，小鸟、小兽、小妖。白、黄、红流进身体，小鸟、小兽、小妖踏着云彩从心里溜达出来。身体更累，心更伤。风住了，风又起了。沿着伤口，就着酒，往下，再往下，潜水一样，掘井一样，运气好的时候，会看到世界里从来没有的景象。</li><li>男性在修炼成功之前(绝大多数在死前都没成功)，似乎总是有种不知进退而成为二逼的风险，过分执着到死拧，过分淡定到麻木，过分较真儿到迂腐，过分邋遢到鼻毛过唇。</li><li>只有克服了对于牛逼的过分追求，才能真正避免成为一个傻逼，特别是，随着年纪的增长，避免成为一个老傻逼。</li></ol><h2 id="叁-想起一生中后悔的事儿"><a href="#叁-想起一生中后悔的事儿" class="headerlink" title="叁 想起一生中后悔的事儿"></a>叁 想起一生中后悔的事儿</h2><center>只花时间给三类人：<br>好看的人，<br>好玩的人，<br>又好看又好玩的人。</center><ol><li>同样吃一串葡萄，有人先从最好的一颗吃起，好处时每次都吃到可得的最好的一颗；有人先从最差的一颗吃起，好处是每次都能吃到比之前更好的一颗。</li><li>前半生认识的朋友来看我，是因为想看我而来看我，而不是因为我在某大机构任职或者刚得了一个世界第一、宇宙无敌的文艺大奖。</li><li>一个人在二十岁之前呆过十年的地方，就是他真正的故乡。之后无论他活多久，去过多少地方，故乡都在骨头和血液里，挥之不去。</li><li>其实，人一起生活过一段时间，就没了生死的界限，除非彼此的爱意已经被彻底忘记。</li><li>我回到您面前，您总会给我一杯热茶，然后也不说话，手指一下，茶在那儿。您走了之后我才明白，一杯热茶之前，要有被子、茶、热水，要问很久、很多次：我儿子什么时候回来啊？</li><li>人皆草木不用成材。</li><li>万事都如甘蔗，哪有两头甜？</li><li>因为手写有人味儿。……。手写信，给心里真正放不下的人，贴张邮票，去邮局寄了。</li><li>连续七天，口袋里，书包里，我天天带着这只鸟(玉)，手没事儿的时候就摸着它，睡觉的时候也攥着。……。到了第八个晚上，一模那只鸟不见了。我的酒一下醒了，我把行李拆了，没有；我把全身衣服拆了，没有；我把房间拆了，没有；我沿着进房间的路，原路返回到下出租车的那块砖，没有。……。我度过了一个非常清醒、哲学而又精疲力竭的夜晚，和初恋分手的第一晚也比这一晚好过很多。……。我醒来的时候，觉得比睡着之前还要累。我洗把脸，阳光从窗帘缝隙间洒下来，那只玉鸟就安静地待在酒店书桌地一个角落，栖息在酒店的便笺上 —— 应该是我脱裤子之前无意识地把它放到了最安全的地方。……。如果我把那只玉鸟抓过来摔碎，我就成佛了。实际发生的是，在一刹那，我找了根结实的绳儿，穿过玉鸟翅膀上面古老的打眼儿，把玉鸟牢牢地栓在我裤子的皮带扣上。<blockquote><ol><li>人生喜悦，失而复得；</li><li>太过珍惜，却弄丢了，是人性的矛盾；</li><li>愿意被找到的东西，一直在那；不愿意被找到的东西，丢了就丢了把；</li><li>为外物而悲喜，这是人性的桎梏。</li></ol></blockquote></li></ol><h2 id="肆-天用云作字"><a href="#肆-天用云作字" class="headerlink" title="肆 天用云作字"></a>肆 天用云作字</h2><center>在此刻，天用云作字。<br>在未来某处，在未来某刻，<br>天也用我作字，<br>用我的手蘸着墨作字。</center><ol><li>你耐心再看看，再看看，再看看。</li><li>如果你有一个期望，长年挥之不去，而且需要别人来满足，这个期望就是妄念。</li><li>自责是负能量最大的一种情绪。<blockquote><p>任性是被低估的美德。</p></blockquote></li><li>“事情过去好久了，话也没啥可说的了，但是有时想起你，还是真他妈的难过啊。”</li><li><p>饮酒到微醺，脸红脖子粗，脚下多了一截弹簧，整个人一蹦一跳的，似乎手不抓牢栏杆，身体就随着灵魂飞离地面。</p><blockquote><p>饮酒的一个好处是，用肉的迷失换取灵的觉悟。放不下的、看不开的，几口下肚，眼清目明，仿佛都与自己无关了，不自觉地，脸上堆满了笑。</p></blockquote></li><li><p>看看就得了，不要临。字写得漂亮的人太多了，万一你写得漂亮了，再写丑就太难了，你就不是你了，老天给你手上的那一丁丁点独特的东西就没了。</p></li><li>佛界易入，魔界难入。佛界和魔界都入入，人更知道什么是佛、什么是魔，人更容易平衡一点儿，在世上能走的更远点儿。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 冯唐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[REPRODUCTION]A Recipe for Training Neural Networks</title>
      <link href="/2019/04/29/REPRODUCTION-A-Recipe-for-Training-Neural-Networks/"/>
      <url>/2019/04/29/REPRODUCTION-A-Recipe-for-Training-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<p><strong>转载自<a href="http://karpathy.github.io" target="_blank" rel="noopener">Andrej Karpathy blog</a></strong></p><h2 id="The-recipe"><a href="#The-recipe" class="headerlink" title="The recipe"></a>The recipe</h2><p>In light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.</p><h4 id="1-Become-one-with-the-data"><a href="#1-Become-one-with-the-data" class="headerlink" title="1. Become one with the data"></a>1. Become one with the data</h4><p>The first step to training a neural net is to not touch any neural net code at all and instead begin by <u>thoroughly inspecting your data</u>. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels?</p><blockquote><ol><li>data contained duplicate examples</li><li>corrupted images &amp; labels</li><li>data imbalances and biases</li><li>local features v.s. global context</li><li>quantity &amp; form of variation</li><li>preprocess out some variation</li><li>spatial position v.s. average pool</li><li>detail v.s. downsample</li><li>labels are noisy?</li></ol></blockquote><p>In addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to <u>look at your network (mis)predictions and understand where they might be coming from</u>. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.</p><p>Once you get a qualitative sense it is also a good idea to write some simple code to <u>search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis.</u> The outliers especially almost always uncover some bugs in data quality or preprocessing.</p><h4 id="2-Set-up-the-end-to-end-training-evaluation-skeleton-get-dumb-baselines"><a href="#2-Set-up-the-end-to-end-training-evaluation-skeleton-get-dumb-baselines" class="headerlink" title="2. Set up the end-to-end training/evaluation skeleton + get dumb baselines"></a>2. Set up the end-to-end training/evaluation skeleton + get dumb baselines</h4><p>Now that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. <u>Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments.</u> At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We’ll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way.</p><p>Tips &amp; tricks for this stage:</p><ul><li><strong>fix random seed</strong>. Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome. This removes a factor of variation and will help keep you sane.</li><li><strong>simplify</strong>. Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage. Data augmentation is a regularization strategy that we may incorporate later, but for now it is just another opportunity to introduce some dumb bug.</li><li><strong>add significant digits to your eval</strong>. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.</li><li><strong>verify loss @ init</strong>. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure <code>-log(1/n_classes)</code> on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc.</li><li><strong>init well</strong>. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias.</li><li><strong>human baseline</strong>. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth.</li><li><strong>input-indepent baseline</strong>. Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all?</li><li><strong>overfit one batch</strong>. Overfit a single batch of only a few examples (e.g. as little as two). To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). I also like to visualize in the same plot both the label and the prediction and ensure that they end up aligning perfectly once we reach the minimum loss. If they do not, there is a bug somewhere and we cannot continue to the next stage.</li><li><strong>verify decreasing training loss</strong>. At this stage you will hopefully be underfitting on your dataset because you’re working with a toy model. Try to increase its capacity just a bit. Did your training loss go down as it should?</li><li><strong>visualize just before the net</strong>. The unambiguously correct place to visualize your data is immediately before your <code>y_hat = model(x)</code> (or <code>sess.run</code> in tf). That is - you want to visualize <em>exactly</em> what goes into your network, decoding that raw tensor of data and labels into visualizations. This is the only “source of truth”. I can’t count the number of times this has saved me and revealed problems in data preprocessing and augmentation.</li><li><strong>visualize prediction dynamics</strong>. I like to visualize model predictions on a fixed test batch during the course of training. The “dynamics” of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network “struggle” to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter.</li><li><strong>use backprop to chart dependencies</strong>. Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I’ve come across a few times is that people get this wrong (e.g. they use <code>view</code> instead of <code>transpose/permute</code> somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss for some example <strong>i</strong> to be 1.0, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the <strong>i-th</strong> example. More generally, gradients give you information about what depends on what in your network, which can be useful for debugging.</li><li><strong>generalize a special case</strong>. This is a bit more of a general coding tip but I’ve often seen people create bugs when they bite off more than they can chew, writing a relatively general functionality from scratch. I like to write a very specific function to what I’m doing right now, get that to work, and then generalize it later making sure that I get the same result. Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time.</li></ul><blockquote><ol><li>固定随机种子，消除随机带来的误差</li><li>简单出发，先不使用数据集扩增</li><li>测试集不要画曲线，不然会疯的</li><li>评估起始损失值，<code>-log(1/n_classes)</code>，各类初始概率应大致相等；<strong>思路一致</strong>, $p\approx$</li><li>初始化最后一层权重很重要；<strong>一般都是采用随机初始化方法？？</strong></li><li>评估人的准确性并与模型比较</li><li>设置一个独立于输入的<code>baseline</code>，观察网络是否提取了想要的特征</li><li>反复训练同一个批次的数据，使网络过拟合，查看损失最低能到多少；<strong>确定网络结构没有问题</strong></li><li>试着增加模型<code>capacity</code>，观察训练集损失是否下降，以确定合适的网络容量；<strong>确定模型参数量</strong></li><li>输入网络前，可视化数据，查看数据是否正确；<strong>确定输入数据没有问题</strong></li><li>可视化一些相同数据的输出，观察输出波动；<strong>观察网络收敛情况</strong></li><li>用反向传播debug网络；<strong>高级技能？技能点还不够</strong></li><li>全循环慢慢改成矢量化代码；<strong>一些复杂的计算可参考</strong></li></ol></blockquote><h4 id="3-Overfit"><a href="#3-Overfit" class="headerlink" title="3. Overfit"></a>3. Overfit</h4><p>At this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.</p><p>The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.</p><p>A few tips &amp; tricks for this stage:</p><ul><li><strong>picking the model</strong>. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: <strong>Don’t be a hero</strong>. I’ve seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don’t be a hero and just copy paste a ResNet-50 for your first run. You’re allowed to do something more custom later and beat this.</li><li><strong>adam is safe</strong>. In the early stages of setting baselines I like to use Adam with a learning rate of <a href="https://twitter.com/karpathy/status/801621764144971776?lang=en" target="_blank" rel="noopener">3e-4</a>. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.)</li><li><strong>complexify only one at a time</strong>. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you’d expect. Don’t throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc.</li><li><strong>do not trust learning rate decay defaults</strong>. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you’re not training ImageNet then you almost certainly do not want this. If you’re not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.</li></ul><blockquote><ol><li>不要逞强，搭建各种奇奇怪怪的模型。先从相近任务效果良好的网络结构出发，慢慢改进再击败它；</li><li><code>Adam</code>对参数敏感性低，<code>SGD</code>往往效果更好；</li><li>慢慢提高输入数据的复杂性，如输入数据的特征数、图像尺寸；</li><li>根据自己的学习任务，调整学习率衰减参数；</li></ol></blockquote><h4 id="4-Regularize"><a href="#4-Regularize" class="headerlink" title="4. Regularize"></a>4. Regularize</h4><p>Ideally, we are now at a place where we have a large model that is fitting at least the training set. Now it is time to regularize it and gain some validation accuracy by giving up some of the training accuracy. Some tips &amp; tricks:</p><ul><li><strong>get more data</strong>. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.</li><li><strong>data augment</strong>. The next best thing to real data is half-fake data - try out more aggressive data augmentation.</li><li><strong>creative augmentation</strong>. If half-fake data doesn’t do it, fake data may also do something. People are finding creative ways of expanding datasets; For example, <a href="https://openai.com/blog/learning-dexterity/" target="_blank" rel="noopener">domain randomization</a>, use of <a href="http://vladlen.info/publications/playing-data-ground-truth-computer-games/" target="_blank" rel="noopener">simulation</a>, clever <a href="https://arxiv.org/abs/1708.01642" target="_blank" rel="noopener">hybrids</a> such as inserting (potentially simulated) data into scenes, or even GANs.</li><li><strong>pretrain</strong>. It rarely ever hurts to use a pretrained network if you can, even if you have enough data.</li><li><strong>stick with supervised learning</strong>. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).</li><li><strong>smaller input dimensionality</strong>. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don’t matter much try to input a smaller image.</li><li><strong>smaller model size</strong>. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.</li><li><strong>decrease the batch size</strong>. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale &amp; offset “wiggles” your batch around more.</li><li><strong>drop</strong>. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout <a href="https://arxiv.org/abs/1801.05134" target="_blank" rel="noopener">does not seem to play nice</a> with batch normalization.</li><li><strong>weight decay</strong>. Increase the weight decay penalty.</li><li><strong>early stopping</strong>. Stop training based on your measured validation loss to catch your model just as it’s about to overfit.</li><li><strong>try a larger model</strong>. I mention this last and only after early stopping but I’ve found a few times in the past that larger models will of course overfit much more eventually, but their “early stopped” performance can often be much better than that of smaller models.</li></ul><p>Finally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network’s first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems.</p><blockquote><p>略微升高一点训练数据的损失，换取验证集损失的下降。</p><ol><li>扩大数据集；<strong>拼的就是算力和数据量</strong></li><li>数据集扩增；<strong>来了</strong></li><li>使用一些生成的虚假数据；</li><li>预训练模型，即使有足够的数据量；</li><li>监督性学习比非监督好；</li><li>减少数据特征的冗余性，如删减特征、降低图像分辨率；<strong>数据冗余容易过拟合</strong></li><li>减少模型参数容量；<strong>过大可能过拟合</strong></li><li>增大批数据量；<strong>梯度下降方向更准确</strong></li><li><code>Dropout</code>；<strong><code>BatchNorm</code>效果更好？</strong></li><li>权重衰减；<strong>正则惩罚</strong></li><li><code>Early stopping</code>；<strong>提前中断训练防止过拟合</strong></li><li>尝试更大的模型，有时候更大容易过拟合，但是提前中断训练效果会更好；</li><li>第一层网络的权值是否有可解释性；<strong>WTF?</strong></li></ol></blockquote><h4 id="5-Tune"><a href="#5-Tune" class="headerlink" title="5. Tune"></a>5. Tune</h4><p>You should now be “in the loop” with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step:</p><ul><li><strong>random over grid search</strong>. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is <a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener">best to use random search instead</a>. Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter <strong>a</strong> matters but changing <strong>b</strong> has no effect then you’d rather sample <strong>a</strong> more throughly than at a few fixed points multiple times.</li><li><strong>hyper-parameter optimization</strong>. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding.</li></ul><blockquote><ol><li>随机搜索超参数，对重要参数调整更多；</li><li>招募一个实习生帮助自己调参hhhhhh；</li></ol></blockquote><h4 id="6-Squeeze-out-the-juice"><a href="#6-Squeeze-out-the-juice" class="headerlink" title="6. Squeeze out the juice"></a>6. Squeeze out the juice</h4><p>Once you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system:</p><ul><li><strong>ensembles</strong>. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can’t afford the computation at test time look into distilling your ensemble into a network using <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">dark knowledge</a>.</li><li><strong>leave it training</strong>. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).</li></ul><blockquote><ol><li>集成方法；</li><li>坚信模型能收敛并能取得良好的效果，不要手贱中断他。</li></ol></blockquote><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Once you make it here you’ll have all the ingredients for success: You have a deep understanding of the technology, the dataset and the problem, you’ve set up the entire training/evaluation infrastructure and achieved high confidence in its accuracy, and you’ve explored increasingly more complex models, gaining performance improvements in ways you’ve predicted each step of the way. You’re now ready to read a lot of papers, try a large number of experiments, and get your SOTA results. Good luck!</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>在宇宙间不易被风吹散</title>
      <link href="/2019/04/28/%E5%9C%A8%E5%AE%87%E5%AE%99%E9%97%B4%E4%B8%8D%E6%98%93%E8%A2%AB%E9%A3%8E%E5%90%B9%E6%95%A3/"/>
      <url>/2019/04/28/%E5%9C%A8%E5%AE%87%E5%AE%99%E9%97%B4%E4%B8%8D%E6%98%93%E8%A2%AB%E9%A3%8E%E5%90%B9%E6%95%A3/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/04/28/在宇宙间不易被风吹散/1.jpg" alt="1"><br><img src="/2019/04/28/在宇宙间不易被风吹散/3.jpg" alt="3"></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>喜欢冯唐的文集，用他自己文章里写的句子描述，“毫不掩饰的小说”，露骨但是真实，文字如锦如绣，文字间有墨香、有美人、有Kindle、有古玉和瓷器。</p><blockquote><p>你迷恋什么，什么就是你的障碍。有个笃定的核，在宇宙间不易被风吹散。</p></blockquote><h1 id="精选"><a href="#精选" class="headerlink" title="精选"></a>精选</h1><ol><li>人是需要有点精神的，有点通灵的精神，否则很容易出溜成行尸走肉，任由人性中暗黑的一面驱使自己禽兽一样的肉身，在世间做一些腐朽不堪的事情。</li><li>一杆进洞，四下无人，人生悲惨莫过于此。</li><li>一日茶，一夜酒，一部毫不掩饰的小说，一次没有目的的见面，一群不谈正经事的朋友，用美好的器物消磨必定留不住的时间。</li><li>做一个人性的矿工，挖一挖，再挖一挖，看看下面的下面还有什么。</li><li>我觉得眼睛看到的一切似乎想要告诉我世界是什么但是又不明说到底是什么。</li><li>在纸书里，在啤酒里，在阳光里，在暖气里，宅着，屌着，无所事事，随梦所之。</li><li>我的、我的、我的、我的，一瞬间的我执爆棚，真好。</li><li>宇宙间大多数现象超越人类的知识范围，不可解释的例子比比皆是，比如人骨骼为啥是206块骨头，比如我爱你你为什么不爱我。</li><li>人生苦短，不如不管，继续任性。</li><li>时间在床边和鬓边一路小跑，有些事物在不知不觉中浅吟低唱，明生暗长。</li><li>人又不是黄金，怎么能让所有人都喜欢？任何事做到顶尖，都是政治，都会被人妒忌；即使是黄金，也会被某些人说成是臭狗屎。</li><li>既然死了的人都没睡醒过，活着时候睡觉就是很吃亏的一件事。</li><li>白白的，小小的，紧紧的，香香的，佛说第一次触摸最接近佛。——《初恋》</li><li>有时候，人会因为一两个微不足道的美好，安安渴望一个巨大的负面，比如因为像有机会用一下图案撩骚的Zippo打火机而渴望抽烟，比如因为一把好乳或者一头长发而舍不得一个三观凌乱的悍妇，比如因为一个火炉而期待北京一个漫长而寒冷的冬天。</li><li>脱离长期背在身上的人的羁绊，让身体里的禽兽和仙人在山林里和酒里渐渐增加比例，裸奔、裸泳，在池塘里带着猴子捞月亮，在山顶问神仙：人到底是个什么东西？</li><li>想起后半生最不靠谱的事儿，结论是：最靠谱的还是买个酒庄。</li><li>天大理比不过“我喜欢”。</li><li>涉及终极的事儿，听天，听命，让自己和身体尽人力，其他不必去想，多想无益，徒增烦恼。</li><li>全球化了，各国的建筑师都到处串了，各种时装杂志都到处发行了，各地的楼宇和姑娘越来越像，像到面目模糊，天下一城。</li><li>最后一个能想到的原因，是随身佩带之后，无时无刻不提醒自己一些必须珍惜的事物和必须坚守的品质。君子无辜，玉不去身，时刻提醒自己，不要吃喝嫖赌抽、坑蒙拐骗偷。</li><li>科技的快速进步让很多人变得过时，也让很多器物变得多余。</li><li>我会老到有一天，不需要手表告诉我，时间是如何自己消失，也不需要靠名牌手表告诉周围人类我的品味、格调、富裕程度和牛逼等级。我会根据四季里的光线的变化，大致推断现在是几点了，根据肠胃的叫声决定是否该去街口的小馆儿了。</li><li>男人要有些士的精神，有所不为，有所必为，活着不是唯一的追求和最终的底线。</li><li>女人一头长发，骑匹大马，很迷人，非常迷人，而且，她是来救你的，就无比迷人。无论她要带你去哪，你都不要拒绝，先上马，然后闭嘴，什么都不要问。</li><li>买件立领风衣，浓个眉大个眼，一直走，不要往两边看，还能再混几十年。</li><li>家庭太复杂，涉及太多硬件和软件、生理和心理、现在和未来，一篇文章不容易讲透。</li><li>上天下地，背山面海，每天看看不一样的云，想想昨晚的梦，和自己聊一会天，日子容易丰盛起来。</li><li>朋友们就散住在附近几个街区，不用提前约，菜香升起时，几个电话就能聚起几个人，酒量不同，酒品相近，术业不同，三观接近。菜一般，就多喝点酒；酒不好，就再多喝点，很快就能高兴起来。</li><li>一生中，除了做自己喜欢的事儿，剩下最重要的就是和相看两不厌的人待在一起。</li><li>不和这个世界争，也不和别人争，更不要和自己争。争的结果可能是一时牛逼，也可能是心脑血管意外，后者造成的持续影响大很多。</li><li>尽管没去过南极，但是也见过了风雨，俗事已经懒得分析，不如一起一边慢跑，一边咒骂彼此生活中奇葩一样摇曳的傻逼。</li><li>世界这么多凶狠，他人心里那么多地狱，内心没有一点混蛋，如何走的下去？</li><li>其实我们跟鱼、植物甚至草履虫有很多相近的地方，人或如草木，人可以甚至应该偶尔禽兽。</li><li>如果没有真的存在，所谓的善只能是伪善，所谓的美也只能是妄美。</li><li>因为人是要死的，所以要常常叨念冯唐说的九字箴言：不着急，不害怕，不要脸。</li><li>钱超过一定数目就不是用来个人消费的了，个人能温饱就好。多处的个人欲望需要靠修行来消灭，而不能靠多花钱来满足。</li><li>有帽子是一种相，没帽子也是一种相。内心不必太执着于无帽子的相，也不必太执着于有帽子的相。有帽子，无帽子，都需要亲尝，皆为玩耍。</li><li>既然戴帽子是相，投射到不同人的心识里就是不同的相，何必强求赞美？何必强调一致？何必消除噪音？</li><li>“宇戴王冠，必承其重。不要低头，王冠会掉。不要哭泣，有人会笑。”这个态度也太励志、太谋权，放松，戴戴耍耍，不留神，王冠掉了，掉就掉了，掉了就索性长发飘飘。</li><li>能做到实事求是地自恋其实是自信和自尊。任何领域做到最好之后，人只能相信自己的判断，只能自恋。……。从这个意义上讲，自恋不应该是被诟病的对象，不能实事求是的傻逼才应该是被诟病的对象。……。实事求是地修炼，实事求是地恋他和自恋，让别人闹心去吧。</li><li>矮子更爱居高临下，傻子更容易认为自己充满道理。</li><li>非让矮子明白自己是矮子，非让傻子明白自己是傻子，也是很耗神费时的事儿。对付世间闹心的事儿，只需要搞清楚两件事，一件是“关我屁事”，另一件是“关你屁事”。</li><li>小孩在天地间疯跑，不知道名利为何物，学习基本常识，食蔬饮水，应付无聊的课程，傻愣愣地杀无聊的时间，骂所有看不上的人“傻逼”。本身近佛，不需要佛。</li><li>有多少似乎过不起的事儿过不去一年？有多少看上去的大事最后真是大事？名片上印不下的名头，敌不过左图且书、右琴与壶，抵不过不得不褪去时一颗好心脏、一个好女生。</li><li>这样简单下去，再简单下去，脑子没弯儿了，手脚有劲儿了，山顶慢慢低于脚面了，拉萨就在眼前了。你我竟然像山、云、湖水和星空一样，一直在老去，一直在变化，一直没问题。再简单下去，在这样下去，你我都是佛了。</li><li>我常年劳碌，尽管热爱妇女，但没时间，无法让任何妇女满意。情伤之后，“得不到”，“留不住”，“无可奈何，奈何奈何”，唯一疗伤的方式就是拿伤口当笔头，写几行诗，血干了，诗出了，心里放下了。</li><li>如果去一座荒岛，没电，没电视，没电脑，一片蛮荒。我想了想，如果只能带一个活物。我就带一个和我能聊很多天的女人；如果只能带一本书，我就带一本《唐诗三百首》。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 冯唐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LDA</title>
      <link href="/2019/04/22/LDA/"/>
      <url>/2019/04/22/LDA/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在<a href="https://louishsu.xyz/2018/10/22/PCA/" target="_blank" rel="noopener">PCA</a>中，介绍了无监督降维方法，主成分分析(Principal Components Analysis)。从投影后数据方差最大的角度出发，选取主轴。下面介绍一种有监督的降维方法，线性判别分析(Linear Discriminant Analysis)。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><p>设有$n$维样本集$D=\{x^{(1)}, …, x^{(i)}, …, x^{(m)}\}$，所属类别数目为$C$，现求其第一投影轴$u_1$，即</p><script type="math/tex; mode=display">\tilde{x}^{(i)}_1 = u_1^Tx^{(i)}</script><p>现希望投影后，<strong>类内距离越小越好，类间距离越大越好</strong>，定义衡量指标</p><ol><li><p>类内离差阵</p><script type="math/tex; mode=display"> S_W = \sum_{j=1}^C \frac{m_j}{m} \left[ \frac{1}{m_j} \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \right]</script><p> 即</p><script type="math/tex; mode=display"> S_W = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \tag{1}</script></li><li><p>类间离差阵</p><script type="math/tex; mode=display"> S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T \tag{2}</script></li></ol><p>则投影到第一主轴$u_1$后数据的类内离差阵和类间离差阵为</p><script type="math/tex; mode=display">\tilde{S_W} = \sum_{j=1}^C \frac{m_j}{m} \left[ \frac{1}{m_j} \sum_{i=1}^{m_j} (\tilde{x}^{(i)}_1 - \tilde{\mu}^{(j)}_1) (\tilde{x}^{(i)}_1 - \tilde{\mu}^{(j)}_1)^T \right]</script><script type="math/tex; mode=display">\tilde{S_B} = \sum_{j=1}^C \frac{m_j}{m} (\tilde{\mu}^{(j)}_1 - \tilde{\mu}_1) (\tilde{\mu}^{(j)}_1 - \tilde{\mu}_1)^T</script><blockquote><p>此时$\tilde{S_W}, \tilde{S_B}$均为标量。</p></blockquote><p>其中</p><script type="math/tex; mode=display">\tilde{x}^{(i)}_1 = u_1^T x^{(i)}</script><script type="math/tex; mode=display">\tilde{\mu}^{(j)}_1 = u_1^T \mu^{(j)}</script><script type="math/tex; mode=display">\tilde{\mu}_1 = u_1^T \mu</script><p>带入后得到</p><script type="math/tex; mode=display">\tilde{S_W} = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (u_1^T x^{(i)} - u_1^T \mu^{(j)}) (u_1^T x^{(i)} - u_1^T \mu^{(j)})^T</script><script type="math/tex; mode=display">= \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} u_1^T(x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T u_1</script><script type="math/tex; mode=display">= u_1^T S_W u_1</script><p>同理</p><script type="math/tex; mode=display">\tilde{S_B} = u_1^T S_B u_1</script><p>定义优化目标</p><script type="math/tex; mode=display">J = \min \left\{ \frac{\tilde{S_W}}{\tilde{S_B}} \right\}= \min \left\{ \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \right\} \tag{3}</script><p>取</p><script type="math/tex; mode=display">L(u_1) = \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \tag{4}</script><p>则其极值为</p><script type="math/tex; mode=display">\frac{∂L}{∂u_1} = \frac{2(u_1^T S_B u_1) S_W u_1 - 2(u_1^T S_W u_1) S_B u_1}{(u_1^T S_B u_1)^2} = 0</script><p>得到</p><script type="math/tex; mode=display">(u_1^T S_B u_1) S_W u_1 = (u_1^T S_W u_1) S_B u_1</script><p>令$\lambda_1 = \frac{u_1^T S_B u_1}{u_1^T S_W u_1}$，有</p><script type="math/tex; mode=display">S_B u_1 = \lambda_1 S_W u_1 \tag{5}</script><p>当$m$较大时，$S_W$一般非奇异，故</p><script type="math/tex; mode=display">S_W^{-1} S_B u_1 = \lambda_1 u_1 \tag{*1}</script><p>即$\{\lambda_1, u_1\}$为矩阵$S_W^{-1} S_B$的特征对。</p><h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>特别地，对于二分类问题，有</p><script type="math/tex; mode=display">S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} (\mu^{(1)} - \mu) (\mu^{(1)} - \mu)^T + \frac{m_2}{m} (\mu^{(2)} - \mu) (\mu^{(2)} - \mu)^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - 2 \mu \left( \frac{m_1}{m} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)T} \right) + \mu \mu^T</script><p>其中</p><script type="math/tex; mode=display">\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)} = \mu \tag{6}</script><p>所以$(6)$代入$S_B$化简得</p><script type="math/tex; mode=display">S_B = \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - \mu \mu^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - (\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)}) (\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)})^T</script><script type="math/tex; mode=display">= \frac{m_1}{m} \left(1-\frac{m_1}{m}\right) \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \left(1-\frac{m_2}{m}\right) \mu^{(2)} \mu^{(2)T} - \frac{m_1}{m} \frac{m_2}{m} \mu^{(2)} \mu^{(1)T} - \frac{m_1}{m} \frac{m_2}{m} \mu^{(1)} \mu^{(2)T}</script><script type="math/tex; mode=display">= \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T</script><p>上式代入$S_W^{-1} S_B u_1$，得</p><script type="math/tex; mode=display">左边 = S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T u_1</script><p>其中$\left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T u_1$为常数，记作$\alpha$，所以$\alpha S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) = \lambda_1 u_1$，也即</p><script type="math/tex; mode=display">u_1 = \frac{\alpha}{\lambda_1} S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)</script><p>其中常数$\frac{\alpha}{\lambda_1}$不影响投影结果，如取$1$，则得到</p><script type="math/tex; mode=display">u_1 = S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \tag{*2}</script><p>同理，可求得第二、三主成分轴</p><h1 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h1><p>给定数据集矩阵</p><script type="math/tex; mode=display">X = \left[ \begin{matrix}    ... \\    x^{(i)T} \\    ... \end{matrix} \right]</script><p>其中$x^{(i)} = \left[ x^{(i)}_1, …, x^{(i)}_j, … x^{(i)}_n \right]^T$</p><ol><li><p>计算类内离差阵$S_W$与类间离差阵$S_B$；</p><script type="math/tex; mode=display"> S_W = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \tag{1}</script><script type="math/tex; mode=display"> S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T \tag{2}</script></li><li><p>计算矩阵$S_W^{-1}S_B$的特征对$(\lambda_i, u_i)$；</p><script type="math/tex; mode=display">S_W^{-1}S_B u_i = \lambda_i u_i \tag{*1}</script></li><li><p>按特征值从大到小排序，选取最大的特征值作为第一主轴；</p></li><li>将数据投影到主轴上；</li></ol><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>LDA可用于分类，以二分类为例，在求取主轴$u_1$后，将各类中心投影到主轴上，即</p><script type="math/tex; mode=display">\begin{cases}    \tilde{\mu}^{(1)}_1 = u_1^T \mu^{(1)} \\    \tilde{\mu}^{(2)}_1 = u_1^T \mu^{(2)}\end{cases}</script><p>选取阈值，如</p><script type="math/tex; mode=display">\tilde{x}_1 = \frac{\tilde{\mu}^{(1)}_1 + \tilde{\mu}^{(2)}_1}{2}</script><p>则预测时，判决方程为</p><script type="math/tex; mode=display">\begin{cases}    u_1^T x^{(i)} < \tilde{x}_1 \Rightarrow x^{(i)} \in \omega_1 \\    u_1^T x^{(i)} > \tilde{x}_1 \Rightarrow x^{(i)} \in \omega_2\end{cases}</script><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>详情查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/lda.py" target="_blank" rel="noopener">Github</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eig</span><span class="params">(A1, A2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        A1, A2: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        eigval: &#123;ndarray(n)&#125;</span></span><br><span class="line"><span class="string">        eigvec: &#123;ndarray(n, n)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        A1 \alpha = \lambda A2 \alpha</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s, u = np.linalg.eigh(A2 + np.diag(np.ones(A2.shape[<span class="number">0</span>]))*<span class="number">1e-3</span>)</span><br><span class="line">    s_sqrt_inv = np.linalg.inv(np.diag(np.sqrt(s)))</span><br><span class="line"></span><br><span class="line">    A = s_sqrt_inv.dot(u.T).dot(A1).dot(u).dot(s_sqrt_inv)</span><br><span class="line">    eigval, P = np.linalg.eigh(A)</span><br><span class="line">    eigvec = u.dot(s_sqrt_inv).dot(P)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> eigval, eigvec</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LDA</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_components: &#123;int&#125;</span></span><br><span class="line"><span class="string">        means_: &#123;ndarray(n_classes, n_features)&#125;</span></span><br><span class="line"><span class="string">        components_:  &#123;ndarray(n_components, n_features)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components)</span>:</span></span><br><span class="line">        self.n_components = n_components</span><br><span class="line"></span><br><span class="line">        self.means_ = <span class="keyword">None</span></span><br><span class="line">        self.components_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">""" train the model</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:      &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">            y:      &#123;ndarray(n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        labels = list(set(list(y)))</span><br><span class="line">        n_classes = len(labels)</span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">        self.means_ = np.zeros((n_classes, n_features))</span><br><span class="line">        S_W = np.zeros(shape=(n_features, n_features))</span><br><span class="line">        S_B = np.zeros(shape=(n_features, n_features))</span><br><span class="line">        mean_ = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> i_class <span class="keyword">in</span> range(n_classes):</span><br><span class="line">            X_ = X[y==labels[i_class]]</span><br><span class="line">            </span><br><span class="line">            means_ = np.mean(X_, axis=<span class="number">0</span>)</span><br><span class="line">            self.means_[i_class] = means_</span><br><span class="line"></span><br><span class="line">            X_ = X_ - means_</span><br><span class="line">            means_ = (means_ - mean_).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            S_W += (X_.T).dot(X_) * (<span class="number">1</span> / n_samples)</span><br><span class="line">            S_B += (means_.T).dot(means_) * (X_.shape[<span class="number">0</span>] / n_samples)</span><br><span class="line"></span><br><span class="line">        eigval, eigvec = eig(S_B, S_W)</span><br><span class="line"></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>]</span><br><span class="line">        eigval = eigval[order]</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line"></span><br><span class="line">        self.components_ = eigvec[:, :self.n_components].T</span><br><span class="line">        self.components_ /= np.linalg.norm(self.components_, axis=<span class="number">1</span>).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_.T)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.fit(X, y)</span><br><span class="line">        X_ = self.transform(X)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_components)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X_ = X.dot(self.components_)</span><br><span class="line">        <span class="keyword">return</span> X_</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            X:  &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            y:  &#123;ndarray(n_samples)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        y = np.zeros(n_samples, dtype=np.int)</span><br><span class="line">        </span><br><span class="line">        X_ = self.transform(X)</span><br><span class="line">        means_ = self.transform(self.means_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            y[i] = np.argmin(np.linalg.norm(means_ - X_[i], axis=<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>与PCA投影结果对比如下</p><ul><li>PCA<br>  <img src="/2019/04/22/LDA/PCA.png" alt="PCA"></li><li>LDA<br>  <img src="/2019/04/22/LDA/LDA.png" alt="LDA"></li></ul><p>上图不明显，下图生成了两簇高斯分布的样本点，作出主轴显示，红色为LDA第一主轴方向，蓝色为PCA第一主轴方向<br><img src="/2019/04/22/LDA/Figure_1.png" alt="Figure_1"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html" target="_blank" rel="noopener">机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>N-dim Array PCA</title>
      <link href="/2019/04/18/n-dim-Array-PCA/"/>
      <url>/2019/04/18/n-dim-Array-PCA/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在<a href="https://louishsu.xyz/2018/10/22/PCA/" target="_blank" rel="noopener">PCA</a>中介绍了1维数据的主成分分析，那么对于多维数据，如何进行处理呢？有一种做法是，将单份的样本数据展开为1维向量，再进行PCA，例如著名的<a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E8%84%B8" target="_blank" rel="noopener">Eigenface</a>，如图所示</p><p><img src="/2019/04/18/n-dim-Array-PCA/Eigenfaces.png" alt="Eigenfaces"></p><p>这有一个缺点是，忽略了多维数据的空间信息，且计算时，若展开后维度过大，协方差矩阵的求逆过程非常耗时间，以下介绍多维主成分分析。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>对于单维$n$维度数据$X = \left[\begin{matrix} x^{(1)} \\ x^{(2)} \\ … \\ x^{(N)} \end{matrix}\right]$，我们有</p><script type="math/tex; mode=display">X' = XU</script><p>其中，$x^{(i)}$为$n$维行向量，组成样本矩阵$X_{N×n}$，$U_{n×n_1}$为投影矩阵，$X’_{N×n_1}$为降维后的样本矩阵。</p><p>对于多(n)维样本张量数据$X_{N×n_{d_1}×n_{d_2}×…×n_{d_n}}$，指定各维度降维顺序，在$d_i$维度上，我们将张量在该维度上展开为$1$维向量</p><script type="math/tex; mode=display">X_{N_{d_i}×n_{d_i}}</script><p>其中</p><script type="math/tex; mode=display">N_{d_i} = \prod_{j=0,j≠i}^n n_{d_j}</script><p>然后在$d_i$维度上进行降维，即</p><script type="math/tex; mode=display">X'_{N_{d_i}×n'_{d_i}} = X_{N_{d_i}×n_{d_i}} U^{d_i}</script><p>其中$U^{d_i}$表示$d_i$维度的投影矩阵，其大小为$n_{d_i}×n’_{d_i}$，然后，将该张量其余维度恢复，得到</p><script type="math/tex; mode=display">X^{d_i}_{N×n_{d_1}×..×n'_{d_i}×...×n_{d_n}}</script><p>如此循环，在各维度进行降维，得到张量</p><script type="math/tex; mode=display">X^{d_1, ..., d_n}_{N×n'_{d_1}×..×n'_{d_i}×...×n'_{d_n}}</script><p><strong>注意，不同的降维顺序得到的参数会存在差异。</strong></p><p>以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$，其降维过程表示如图</p><ul><li><p>在$H$维度上</p><p>  <img src="/2019/04/18/n-dim-Array-PCA/d1.JPG" alt="d1"></p></li><li><p>在$W$维度上</p><p>  <img src="/2019/04/18/n-dim-Array-PCA/d2.JPG" alt="d2"></p></li><li><p>在$C$维度上</p><p>  <img src="/2019/04/18/n-dim-Array-PCA/d3.JPG" alt="d3"></p></li></ul><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p>同样的，以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$</p><ol><li><p>在$H$维度上</p><ol><li>将该张量转置，得到$X^{T_{d_1d_3}}(C, W, H)$</li><li>将其展开为$X_{flatten}(N_H×H)$，其中$N_H=C×W$</li><li>降维得到$X’_{flatten}(N_H×H’)$</li><li>重新将另外两维恢复，得到$X^{T_{d_1d_3}’}(C, W, H’)$</li><li>将张量转置，得到$X^{d_1}(H’, W, C)$</li></ol></li><li><p>在$W$维度上</p><ol><li>将$X^{d_1}(H’, W, C)$转置，得到$X^{T_{d_2d_3}}(H’, C, W)$</li><li>将其展开为$X_{flatten}(N_W×W)$，其中$N_W=H’×C$</li><li>降维得到$X’_{flatten}(N_W×W’)$</li><li>重新将另外两维恢复，得到$X^{T_{d_2d_3}’}(H’, C, W’)$</li><li>将张量转置，得到$X^{d_1d_2}(H’, W’, C)$</li></ol></li><li><p>在$C$维度上</p><ol><li>将$X^{d_1d_2}(H’, W’, C)$展开为$X_{flatten}(N_C×C)$，其中$N_C=H’×W’$</li><li>降维得到$X’_{flatten}(N_C×C’)$</li><li>重新将另外两维恢复，得到$X^{d_1d_2d_3}(H’, W’, C’)$</li></ol></li></ol><p>详细查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/tensor_pca.py" target="_blank" rel="noopener">GitHub</a>，其核心代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Params:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples, d0, d1, d2, ..., dn-1)&#125; n-dim array</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples, d0, d1, d2, ..., dn-1)&#125; n-dim array</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> self.n_dims == len(X.shape) - <span class="number">1</span>, <span class="string">'please check input dimension! '</span></span><br><span class="line">    idx = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X.shape))]   <span class="comment"># index of dimensions</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i_dim <span class="keyword">in</span> range(self.n_dims):</span><br><span class="line">            </span><br><span class="line">        <span class="comment">## transpose tensor</span></span><br><span class="line">        idx[<span class="number">-1</span>], idx[i_dim + <span class="number">1</span>] = idx[i_dim + <span class="number">1</span>], idx[<span class="number">-1</span>]</span><br><span class="line">        X = X.transpose(idx)</span><br><span class="line">        shape = list(X.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1-dim pca</span></span><br><span class="line">        X = X.reshape((<span class="number">-1</span>, shape[<span class="number">-1</span>]))</span><br><span class="line">        X = self.decomposers[i_dim].transform(X)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## transpose tensor</span></span><br><span class="line">        X = X.reshape(shape[:<span class="number">-1</span>]+[X.shape[<span class="number">-1</span>]])</span><br><span class="line">        X = X.transpose(idx)</span><br><span class="line">        idx[<span class="number">-1</span>], idx[i_dim + <span class="number">1</span>] = idx[i_dim + <span class="number">1</span>], idx[<span class="number">-1</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h1 id="实验显示"><a href="#实验显示" class="headerlink" title="实验显示"></a>实验显示</h1><p>将2维数据降维，指定通道维度数目从$3$降至$1$，得到实验结果如下</p><ul><li><p>原始数据<br>  <img src="/2019/04/18/n-dim-Array-PCA/origin_data.png" alt="origin_data"></p></li><li><p>降维后数据<br>  <img src="/2019/04/18/n-dim-Array-PCA/transform.png" alt="transform"></p></li><li><p>重建数据<br>  <img src="/2019/04/18/n-dim-Array-PCA/inv_transform.png" alt="inv_transform"></p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E8%84%B8" target="_blank" rel="noopener">特征脸 - 维基百科，自由的百科全书</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python字符串格式化</title>
      <link href="/2019/02/19/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96/"/>
      <url>/2019/02/19/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Python操作字符串行云流水，当然也支持格式化字符串。</p><h1 id="通过格式符"><a href="#通过格式符" class="headerlink" title="通过格式符"></a>通过格式符</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;我叫%s, 今年%d岁&quot; % (&apos;Louis Hsu&apos;, 18))</span><br></pre></td></tr></table></figure><p>或者使用字典进行值传递<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;我叫%(name), 今年%(age)岁&quot; % &#123;&apos;name&apos;: &apos;Louis Hsu&apos;, &apos;age&apos;: 18&#125;)</span><br></pre></td></tr></table></figure></p><p><strong>typecode</strong><br>|  格式符 | 含义 |<br>| ——— | ———- |<br>| %s | 字符串 (采用str()的显示) |<br>| %r | 字符串 (采用repr()的显示) |<br>| %c | 单个字符 |<br>| %b | 二进制整数 |<br>| %d | 十进制整数 |<br>| %i | 十进制整数 |<br>| %o | 八进制整数 |<br>| %x | 十六进制整数 |<br>| %e | 指数 (基底写为e) |<br>| %E | 指数 (基底写为E) |<br>| %f | 浮点数 |<br>| %F | 浮点数，与上相同 |<br>| %g | 指数(e)或浮点数 (根据显示长度) |<br>| %G | 指数(E)或浮点数 (根据显示长度) |<br>| %% | 字符”%” | </p><p><strong>高阶</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">% [flags] [width].[precision] typecode</span><br><span class="line">- flags:        &apos;+&apos;(右对齐), &apos;-&apos;(左对齐), &apos; &apos;(左侧填充一个空格，与负数对齐), &apos;0&apos;(用0填充)</span><br><span class="line">- width:        显示宽度</span><br><span class="line">- precision:    小数精度位数，可使用&apos;*&apos;进行动态代入</span><br><span class="line">- typecode:     格式符</span><br></pre></td></tr></table></figure></p><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(&apos;pi is %+2.2f&apos; % (3.1415926)) -&gt; pi is +3.14</span><br><span class="line">print(&apos;pi is %-2.2f&apos; % (3.1415926)) -&gt; pi is 3.14</span><br><span class="line">print(&apos;pi is % 2.2f&apos; % (3.1415926)) -&gt; pi is  3.14</span><br><span class="line">print(&apos;pi is %02.2f&apos; % (3.1415926)) -&gt; pi is 3.14</span><br><span class="line"></span><br><span class="line"># 同</span><br><span class="line">print(&apos;pi is %+*.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is %-*.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is % *.*f&apos; % (2, 2, 3.1415926))</span><br><span class="line">print(&apos;pi is %0*.*f&apos; % (2, 2, 3.1415926))</span><br></pre></td></tr></table></figure></p><h1 id="通过format"><a href="#通过format" class="headerlink" title="通过format"></a>通过format</h1><h2 id="位置传递"><a href="#位置传递" class="headerlink" title="位置传递"></a>位置传递</h2><ol><li><p>使用位置参数</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; li = [&apos;hoho&apos;,18]</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;&#125; ,age &#123;&#125;&apos;.format(&apos;hoho&apos;,18)</span><br><span class="line">&apos;my name is hoho ,age 18&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;1&#125; ,age &#123;0&#125;&apos;.format(10,&apos;hoho&apos;)</span><br><span class="line">&apos;my name is hoho ,age 10&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;1&#125; ,age &#123;0&#125; &#123;1&#125;&apos;.format(10,&apos;hoho&apos;)</span><br><span class="line">&apos;my name is hoho ,age 10 hoho&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;&#125; ,age &#123;&#125;&apos;.format(*li)</span><br><span class="line">&apos;my name is hoho ,age 18&apos;</span><br></pre></td></tr></table></figure></li><li><p>使用关键字参数</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; hash = &#123;&apos;name&apos;:&apos;hoho&apos;,&apos;age&apos;:18&#125;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;name&#125;,age is &#123;age&#125;&apos;.format(name=&apos;hoho&apos;,age=19)</span><br><span class="line">&apos;my name is hoho,age is 19&apos;</span><br><span class="line">&gt;&gt;&gt; &apos;my name is &#123;name&#125;,age is &#123;age&#125;&apos;.format(**hash)</span><br><span class="line">&apos;my name is hoho,age is 18&apos;</span><br></pre></td></tr></table></figure></li></ol><h2 id="格式限定"><a href="#格式限定" class="headerlink" title="格式限定"></a>格式限定</h2><p>基本格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;[:pad][align][sign][typecode]&#125;</span><br><span class="line">- :pad :    填充字符，空白位用该字符填充</span><br><span class="line">- align:    &apos;^&apos;, &apos;&lt;&apos;, &apos;&gt;&apos; 分别表示 &apos;居中&apos;, &apos;左对齐&apos;, &apos;右对齐&apos;(默认)，后面加宽度</span><br><span class="line">- sign :    &apos;+&apos;, &apos;-&apos; , &apos; &apos; 分别表示 &apos;正&apos;, &apos;负&apos;, &apos;正数前加空格&apos;</span><br><span class="line">- typecode: &apos;b&apos;, &apos;d&apos;, &apos;o&apos;, &apos;x&apos;, &apos;f&apos;, &apos;,&apos;, &apos;%&apos;, &apos;e&apos; 分别表示 &apos;二进制&apos;, &apos;十进制&apos;, &apos;八进制&apos;, &apos;十六进制&apos;, &apos;浮点数&apos;, &apos;逗号分隔&apos;, &apos;百分比格式&apos;,  &apos;指数记法&apos;</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>python之字符串格式化(format) - benric - 博客园 <a href="https://www.cnblogs.com/benric/p/4965224.html" target="_blank" rel="noopener">https://www.cnblogs.com/benric/p/4965224.html</a><br>Python format 格式化函数 | 菜鸟教程 <a href="http://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener">http://www.runoob.com/python/att-string-format.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python命令行参数解析</title>
      <link href="/2019/02/18/Python%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/02/18/Python%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="C-C-的参数传递"><a href="#C-C-的参数传递" class="headerlink" title="C/C++的参数传递"></a>C/C++的参数传递</h1><p>我们知道C/C++主函数形式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc,char * argv[],char * envp[])</span><br><span class="line">&#123;</span><br><span class="line">    // do something</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中各参数含义如下</p><ul><li><code>argc</code>：<code>argument count</code>，表示参数数量</li><li><code>argv</code>：<code>argument value</code>，表示参数值<br>  最后一个元素存放了一个NULL的指针</li><li><code>envp</code>：系统环境变量<br>  最后一个元素存放了一个NULL的指针</li></ul><p>例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">int main(int argc,char * argv[],char * envp[])</span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;argc is %d \n&quot;, argc);</span><br><span class="line"> </span><br><span class="line">    int i;</span><br><span class="line">    for (i=0; i&lt;argc; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;arcv[%d] is %s\n&quot;, i, argv[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for (i=0; envp[i]!=NULL; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;envp[%d] is %s\n&quot;, i, envp[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试平台为Windows10，执行编译和运行操作，结果如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; gcc main.c -o main.exe</span><br><span class="line">&gt; ./main.exe param1 param2 param3 param4</span><br><span class="line">argc is 5</span><br><span class="line">arcv[0] is C:\OneDrive\▒ĵ▒\Louis&apos; Blog\source\_drafts\Python▒▒▒▒▒в▒▒▒▒▒▒▒\test.exe</span><br><span class="line">arcv[1] is param1</span><br><span class="line">arcv[2] is param2</span><br><span class="line">arcv[3] is param3</span><br><span class="line">arcv[4] is param4</span><br><span class="line">envp[0] is ACLOCAL_PATH=C:\MyApplications\Git\mingw64\share\aclocal;C:\MyApplica                                                                                                                tions\Git\usr\share\aclocal</span><br><span class="line">envp[1] is ALLUSERSPROFILE=C:\ProgramData</span><br><span class="line">...(省略)</span><br><span class="line">envp[71] is WINDIR=C:\WINDOWS</span><br><span class="line">envp[72] is _=./main.exe</span><br></pre></td></tr></table></figure></p><h1 id="Python的参数传递"><a href="#Python的参数传递" class="headerlink" title="Python的参数传递"></a>Python的参数传递</h1><p>可以使用sys模块得到命令行参数，主函数文件<code>main.py</code>如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    print(sys.argv)</span><br></pre></td></tr></table></figure></p><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; python main.py param1 param2 pram3</span><br><span class="line">[&apos;main.py&apos;, &apos;param1&apos;, &apos;param2&apos;, &apos;pram3&apos;]</span><br></pre></td></tr></table></figure></p><h1 id="getopt"><a href="#getopt" class="headerlink" title="getopt"></a>getopt</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import getopt</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    argv = sys.argv</span><br><span class="line">    </span><br><span class="line">    if len(argv) == 1:</span><br><span class="line">        print(</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            Usage: python main.py [option]</span><br><span class="line">            -h or --help:    显示帮助信息</span><br><span class="line">            -v or --version: 显示版本</span><br><span class="line">            -i or --input:   指定输入文件路径</span><br><span class="line">            -o or --output:  指定输出文件路径</span><br><span class="line"></span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        opts, args = getopt.getopt(args=argv[1:],</span><br><span class="line">                                   shortopts=&apos;hvi:o:&apos;,</span><br><span class="line">                                   longopts=[&apos;help&apos;, &apos;version&apos;, &apos;input=&apos;, &apos;output=&apos;]</span><br><span class="line">                                )</span><br><span class="line">    except getopt.GetoptError:</span><br><span class="line">        print(&quot;argv error,please input&quot;)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    for cmd, arg in opts:</span><br><span class="line"></span><br><span class="line">        if cmd in [&apos;-h&apos;, &apos;--help&apos;]:</span><br><span class="line">            print(&quot;help info&quot;)</span><br><span class="line">            sys.exit(0)</span><br><span class="line">        elif cmd in [&apos;-v&apos;, &apos;--version&apos;]:</span><br><span class="line">            print(&quot;main 1.0&quot;)</span><br><span class="line">            sys.exit(0)</span><br><span class="line"></span><br><span class="line">        if cmd in [&apos;-i&apos;, &apos;--input&apos;]:</span><br><span class="line">            input = arg</span><br><span class="line">        if cmd in [&apos;-o&apos;, &apos;--output&apos;]:</span><br><span class="line">            output = arg</span><br></pre></td></tr></table></figure><p>说明</p><ul><li><p><code>args=sys.argv[1:]</code><br>  传入的参数，除去<code>sys.argv[0]</code>，即主函数文件路径</p></li><li><p><code>shortopts=&#39;hvi:o:&#39;</code><br>  字符串，支持形如<code>-h</code>的选项</p><ul><li>若无需指定参数，形如<code>c</code>；</li><li>若必须指定参数，则需为<code>c:</code>；</li></ul></li><li><p><code>longopts=[&#39;help&#39;, &#39;version&#39;, &#39;input=&#39;, &#39;output=&#39;]</code><br>  字符串列表，可选参数，是否支持形如<code>--help</code>的选项</p><ul><li>若无需指定参数，形如<code>cmd</code>；</li><li>若必须指定参数，则需为<code>cmd=</code>；</li></ul></li></ul><p>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt; python main.py</span><br><span class="line"></span><br><span class="line">            Usage: python main.py [option]</span><br><span class="line">            -h or --help:    显示帮助信息</span><br><span class="line">            -v or --version: 显示版本</span><br><span class="line">            -i or --input:   指定输入文件路径</span><br><span class="line">            -o or --output:  指定输出文件路径</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt; python main.py -h</span><br><span class="line">help info</span><br><span class="line"></span><br><span class="line">&gt; python main.py -v</span><br><span class="line">main 1.0</span><br><span class="line"></span><br><span class="line">&gt; python main.py -i</span><br><span class="line">argv error,please input</span><br><span class="line"></span><br><span class="line">&gt; python main.py -i a.txt -o b.txt</span><br></pre></td></tr></table></figure></p><h1 id="argsparse"><a href="#argsparse" class="headerlink" title="argsparse"></a>argsparse</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">def show_args(args):</span><br><span class="line">    if args.opencv:</span><br><span class="line">        print(&quot;opencv is used &quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;opencv is not used &quot;)</span><br><span class="line"></span><br><span class="line">    print(args.steps)</span><br><span class="line">    print(args.file)</span><br><span class="line">    print(args.data)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = argparse.ArgumentParser(description=&quot;learn to use `argparse`&quot;)</span><br><span class="line"></span><br><span class="line">    # 标志位</span><br><span class="line">    parser.add_argument(&apos;--opencv&apos;, &apos;-cv&apos;, action=&apos;store_true&apos;, help=&apos;use opencv if set &apos;)</span><br><span class="line">    # 必需参数</span><br><span class="line">    parser.add_argument(&apos;--steps&apos;, &apos;-s&apos;, required=True, type=int, help=&apos;number of steps&apos;)</span><br><span class="line">    # 默认参数</span><br><span class="line">    parser.add_argument(&apos;--file&apos;, &apos;-f&apos;, default=&apos;a.txt&apos;)</span><br><span class="line">    # 候选参数</span><br><span class="line">    parser.add_argument(&apos;--data&apos;, &apos;-d&apos;, choices=[&apos;data1&apos;, &apos;data2&apos;])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    show_args(args)</span><br></pre></td></tr></table></figure><p>说明</p><ul><li>帮助信息<br>  参数<code>help</code>，用于显示在<code>-h</code>帮助信息中</li><li>标志位参数<br>  参数<code>action=&#39;store_true&#39;</code>，即保存该参数为<code>True</code></li><li>必需参数<br>  置位<code>required</code>，即运行该程序必须带上该参数，否则报错</li><li>默认参数<br>  参数<code>default</code>填写默认参数</li><li>候选参数<br>  参数<code>choices</code>填写候选参数列表</li></ul><p>运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># 显示帮助信息</span><br><span class="line">&gt; python main.py -h</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line"></span><br><span class="line">learn to use `argparse`</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help            show this help message and exit</span><br><span class="line">  --opencv, -cv         use opencv if set</span><br><span class="line">  --steps STEPS, -s STEPS</span><br><span class="line">                        number of steps</span><br><span class="line">  --file FILE, -f FILE</span><br><span class="line">  --data &#123;data1,data2&#125;, -d &#123;data1,data2&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 测试必须参数</span><br><span class="line">&gt; python main.py</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line">main.py: error: the following arguments are required: --steps/-s</span><br><span class="line"></span><br><span class="line">&gt; python main.py -s 100</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试标志位参数</span><br><span class="line">&gt; python main.py -s 100 -cv</span><br><span class="line">opencv is used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试默认参数</span><br><span class="line">&gt; python main.py -s 100 -f b.txt</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">b.txt</span><br><span class="line">None</span><br><span class="line"></span><br><span class="line"># 测试可选参数</span><br><span class="line">&gt; python main.py -s 100 -d data1</span><br><span class="line">opencv is not used</span><br><span class="line">100</span><br><span class="line">a.txt</span><br><span class="line">data1</span><br><span class="line"></span><br><span class="line">&gt; python main.py -s 100 -d data0</span><br><span class="line">usage: main.py [-h] [--opencv] --steps STEPS [--file FILE]</span><br><span class="line">               [--data &#123;data1,data2&#125;]</span><br><span class="line">main.py: error: argument --data/-d: invalid choice: &apos;data0&apos; (choose from &apos;data1&apos;, &apos;data2&apos;)</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Python命令行参数解析：getopt和argparse - 死胖子的博客 - CSDN博客 <a href="https://blog.csdn.net/lanzheng_1113/article/details/77574446" target="_blank" rel="noopener">https://blog.csdn.net/lanzheng_1113/article/details/77574446</a><br>Python模块之命令行参数解析 - 每天进步一点点！！！ - 博客园 <a href="https://www.cnblogs.com/madsnotes/articles/5687079.html" target="_blank" rel="noopener">https://www.cnblogs.com/madsnotes/articles/5687079.html</a><br>Python解析命令行读取参数 — argparse模块 - Arkenstone - 博客园 <a href="https://www.cnblogs.com/arkenstone/p/6250782.html" target="_blank" rel="noopener">https://www.cnblogs.com/arkenstone/p/6250782.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python生成词云图</title>
      <link href="/2019/02/17/Python%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91%E5%9B%BE/"/>
      <url>/2019/02/17/Python%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一个没什么用的小技能</p><h1 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h1><blockquote><p>wordcloud · PyPI <a href="https://pypi.org/project/wordcloud/" target="_blank" rel="noopener">https://pypi.org/project/wordcloud/</a></p></blockquote><p>安装该模块<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> pip install wordcloud</span><br></pre></td></tr></table></figure></p><p>主要用到的为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wordcloud.WordCloud(font_path=<span class="keyword">None</span>, width=<span class="number">400</span>, height=<span class="number">200</span>, margin=<span class="number">2</span>,</span><br><span class="line">                ranks_only=<span class="keyword">None</span>, prefer_horizontal=<span class="number">.9</span>, mask=<span class="keyword">None</span>, scale=<span class="number">1</span>,</span><br><span class="line">                color_func=<span class="keyword">None</span>, max_words=<span class="number">200</span>, min_font_size=<span class="number">4</span>,</span><br><span class="line">                stopwords=<span class="keyword">None</span>, random_state=<span class="keyword">None</span>, background_color=<span class="string">'black'</span>,</span><br><span class="line">                max_font_size=<span class="keyword">None</span>, font_step=<span class="number">1</span>, mode=<span class="string">"RGB"</span>,</span><br><span class="line">                relative_scaling=<span class="string">'auto'</span>, regexp=<span class="keyword">None</span>, collocations=<span class="keyword">True</span>,</span><br><span class="line">                colormap=<span class="keyword">None</span>, normalize_plurals=<span class="keyword">True</span>, contour_width=<span class="number">0</span>,</span><br><span class="line">                contour_color=<span class="string">'black'</span>, repeat=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><h1 id="使用例程"><a href="#使用例程" class="headerlink" title="使用例程"></a>使用例程</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"></span><br><span class="line">font = <span class="string">'C:/Windows/Fonts/SIMYOU.TTF'</span>    <span class="comment"># 幼圆</span></span><br><span class="line">string = <span class="string">'LouisHsu 单键 小叔叔 想静静 95后 傲娇 skrrrrrrr 大猫座 佛了 要秃 嘤嘤嘤 真香'</span></span><br><span class="line"></span><br><span class="line">mask = cv2.imread(<span class="string">'./mask.jpg'</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">thresh, mask = cv2.threshold(mask, <span class="number">127</span>, <span class="number">255</span>, cv2.THRESH_BINARY)</span><br><span class="line"></span><br><span class="line">wc = WordCloud(</span><br><span class="line">        font_path=font, </span><br><span class="line">        background_color=<span class="string">'white'</span>,</span><br><span class="line">        color_func=<span class="keyword">lambda</span> *args, **kwargs: (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>),</span><br><span class="line">        mask=mask,</span><br><span class="line">        max_words=<span class="number">500</span>,</span><br><span class="line">        min_font_size=<span class="number">4</span>,</span><br><span class="line">        max_font_size=<span class="keyword">None</span>,</span><br><span class="line">        contour_width=<span class="number">1</span>,</span><br><span class="line">        repeat=<span class="keyword">True</span>                     <span class="comment"># 允许词重复</span></span><br><span class="line">    )</span><br><span class="line">wc.generate_from_text(string)</span><br><span class="line">wc.to_file(<span class="string">'./wc.jpg'</span>)                  <span class="comment">#保存图片</span></span><br></pre></td></tr></table></figure><p>输入原图为<br><img src="/2019/02/17/Python生成词云图/mask.jpg" alt="mask"></p><p>生成图像<br><img src="/2019/02/17/Python生成词云图/wc.jpg" alt="wc"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>python WordCloud 简单实例 - 博客 - CSDN博客 <a href="https://blog.csdn.net/cy776719526/article/details/80171790" target="_blank" rel="noopener">https://blog.csdn.net/cy776719526/article/details/80171790</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Makefile简单教程</title>
      <link href="/2019/01/05/makefile%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B/"/>
      <url>/2019/01/05/makefile%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h1 id="准备源文件"><a href="#准备源文件" class="headerlink" title="准备源文件"></a>准备源文件</h1><p>新建目录<code>demo/</code>，其结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">demo</span><br><span class="line">├─bin# 二进制文件，即可执行文件</span><br><span class="line">├─include# 头文件`.h`</span><br><span class="line">│      hello.h</span><br><span class="line">│</span><br><span class="line">├─obj# 目标文件`.o`</span><br><span class="line">└─src# 源文件`.c`</span><br><span class="line">        hello.c</span><br><span class="line">        test.c</span><br></pre></td></tr></table></figure></p><p>编辑<code>hello.h</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __HELLO_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __HELLO_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> __hello();</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></p><p>编辑<code>hello.c</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"hello.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> __hello()</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Hello world!\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>编辑<code>test.h</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"hello.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">__hello();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="命令行编译"><a href="#命令行编译" class="headerlink" title="命令行编译"></a>命令行编译</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test.c</span><br><span class="line"><span class="meta">$</span> gcc test.c hello.c -o test</span><br><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test  test.c</span><br><span class="line"><span class="meta">$</span> ./test</span><br><span class="line">Hello world!</span><br></pre></td></tr></table></figure><p>但是这样编译会每次都重新编译整个工程，时间比较长，所以可以先生成<code>.o</code>文件，当<code>test.c</code>代码改动后，重新生成<code>test.o</code>即可，<code>hello.o</code>不用重新编译<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  makefile  README.md  test.c</span><br><span class="line"><span class="meta">$</span> gcc test.c hello.c -c</span><br><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  hello.o  makefile  README.md  test.c  test.o</span><br><span class="line"><span class="meta">$</span> gcc test.o hello.o -o test</span><br><span class="line"><span class="meta">$</span> ls</span><br><span class="line">hello.c  hello.h  hello.o  makefile  README.md  test  test.c  test.o</span><br><span class="line"><span class="meta">$</span> ./test</span><br><span class="line">Hello world!</span><br></pre></td></tr></table></figure></p><h1 id="Makefile"><a href="#Makefile" class="headerlink" title="Makefile"></a>Makefile</h1><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;target&gt;: &lt;dependencies&gt;</span></span><br><span class="line">&lt;command&gt;<span class="comment"># [TAB]&lt;command&gt;</span></span><br></pre></td></tr></table></figure><p>例如<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">test: test.c</span></span><br><span class="line">gcc test.c -o test</span><br></pre></td></tr></table></figure></p><p>编辑<code>Makefile</code><br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># directories &amp; target name</span></span><br><span class="line">DIR_INC = ./<span class="keyword">include</span>  </span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line">DIR_BIN = ./bin</span><br><span class="line">TARGET  = test</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile macro  </span></span><br><span class="line">CC = gcc</span><br><span class="line">CFLAGS = -g -Wall -I$&#123;DIR_INC&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">all: $&#123;TARGET&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="section">$&#123;TARGET&#125;: $&#123;DIR_OBJ&#125;/hello.o $&#123;DIR_OBJ&#125;/test.o</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(DIR_OBJ)</span>/hello.o <span class="variable">$(DIR_OBJ)</span>/test.o -o $&#123;TARGET&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">$&#123;DIR_OBJ&#125;/hello.o: <span class="variable">$(DIR_SRC)</span>/hello.c</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(DIR_SRC)</span>/hello.c -o $&#123;DIR_OBJ&#125;/hello.o</span><br><span class="line"></span><br><span class="line"><span class="section">$&#123;DIR_OBJ&#125;/test.o: <span class="variable">$(DIR_SRC)</span>/test.c</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(DIR_SRC)</span>/test.c -o $&#123;DIR_OBJ&#125;/test.o</span><br></pre></td></tr></table></figure></p><p>或通用性格式<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># directories &amp; target name</span></span><br><span class="line">DIR_INC = ./<span class="keyword">include</span>  </span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line">DIR_BIN = ./bin</span><br><span class="line">TARGET  = test</span><br><span class="line"></span><br><span class="line"><span class="comment"># compile macro  </span></span><br><span class="line">CC      = gcc</span><br><span class="line">CFLAGS  = -g -Wall -I$&#123;DIR_INC&#125;                 <span class="comment"># `-g`表示调试选项，`-Wall`表示编译后显示所有警告</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load source files</span></span><br><span class="line">SRC = <span class="variable">$(<span class="built_in">wildcard</span> $&#123;DIR_SRC&#125;/*.c)</span>                         <span class="comment"># 匹配目录中所有的`.c`文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build target</span></span><br><span class="line">OBJ = <span class="variable">$(<span class="built_in">patsubst</span> %.c, $&#123;DIR_OBJ&#125;/%.o, $&#123;<span class="built_in">notdir</span> $&#123;SRC&#125;&#125;)</span>  <span class="comment"># 由`SRC`字符串内容，指定生成`.o`文件的名称与目录</span></span><br><span class="line">BIN = $&#123;DIR_BIN&#125;/$&#123;TARGET&#125;                               <span class="comment"># 指定可执行文件名称与目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># build</span></span><br><span class="line"><span class="section">$&#123;BIN&#125;: $&#123;OBJ&#125;</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(OBJ)</span> -o <span class="variable">$@</span>                               <span class="comment"># 即 `$ gcc ./obj/*.o -o ./bin/test`</span></span><br><span class="line"><span class="section">$&#123;DIR_OBJ&#125;/%.o: $&#123;DIR_SRC&#125;/%.c</span></span><br><span class="line"><span class="variable">$(CC)</span> <span class="variable">$(CFLAGS)</span> -c <span class="variable">$&lt;</span> -o <span class="variable">$@</span>                      <span class="comment"># 即 `$ gcc ./src/*.c -g -Wall -I./include -c ./obj/*.o`</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># clean</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">.PHONY</span>: clean                                            # 伪目标</span></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">find $&#123;DIR_OBJ&#125; -name *.o -exec rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></figure></p><p>执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> make</span><br><span class="line">gcc -g -Wall -I./include   -c src/hello.c -o obj/hello.o</span><br><span class="line">gcc -g -Wall -I./include   -c src/test.c -o obj/test.o</span><br><span class="line">gcc  ./obj/hello.o  ./obj/test.o  -o bin/test</span><br><span class="line"><span class="meta">$</span> ls obj/</span><br><span class="line">hello.o  test.o</span><br><span class="line"><span class="meta">$</span> ls bin/</span><br><span class="line">test</span><br><span class="line"><span class="meta">$</span> ./bin/test </span><br><span class="line">Hello world!</span><br><span class="line"><span class="meta">$</span> make clean</span><br><span class="line">find ./obj -name *.o -exec rm -rf &#123;&#125; \;</span><br><span class="line"><span class="meta">$</span> ls obj/</span><br><span class="line"><span class="meta">$</span> ls bin/</span><br><span class="line">test</span><br></pre></td></tr></table></figure></p><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><ol><li>符号 <code>$@</code>, <code>$^</code>, <code>$&lt;</code>，<code>$?</code><ul><li><code>$@</code>: 表示目标文件</li><li><code>$^</code>: 表示所有的依赖文件</li><li><code>$&lt;</code>: 表示第一个依赖文件</li><li><code>$?</code>: 表示比目标还要新的依赖文件列表</li></ul></li><li><code>wildcard</code>，<code>notdir</code>，<code>patsubst</code><ul><li><code>wildcard</code>    : 扩展通配符<br>  <code>SOURCES = $(wildcard *.c)</code>: 产生一个所有以 ’.c’ 结尾的文件的列表，然后存入变量 SOURCES 里。 </li><li><code>notdir</code>    : 去除路径，可以在使用<code>wildcard</code>函数后，再配合使用<code>notdir</code>函数只得到文件名（不含路径）。</li><li><code>patsubst</code>    : 替换通配符，需要３个参数，第一个是个需要匹配的式样，第二个表示用什么来替换他，第三个是个需要被处理的由空格分隔的字列。<br>  <code>OBJS = $(patsubst %.c,%.o,$(SOURCES))</code><pre><code>  - 将处理所有在 SOURCES 字列中的字（一列文件名），如果他的 结尾是 `.c` ，就用 `.o` 把 `.c`取代  - 这里的 % 符号将匹配一个或多个字符，而他每次所匹配的字串叫做一个‘柄’(stem)   - 在第二个参数里， %被解读成用第一参数所匹配的那个柄。</code></pre></li></ul></li><li><code>-I</code>，<code>-L</code>，<code>-l</code><ul><li><code>-I</code>: 将指定目录作为第一个寻找头文件的目录</li><li><code>-L</code>: 将指定目录作为第一个寻找库文件的目录</li><li><code>-l</code>: 在库文件路径中寻找<code>.so</code>动态库文件（如果gcc编译选项中加入了<code>-static</code>表示寻找<code>.a</code>静态库文件）</li></ul></li><li><code>.PHONY</code>后面的<code>target</code>表示的也是一个伪造的<code>target</code>, 而不是真实存在的文件<code>target</code>，注意<code>Makefile</code>的<code>target</code>默认是文件。</li></ol><h2 id="关于三个函数的使用"><a href="#关于三个函数的使用" class="headerlink" title="关于三个函数的使用"></a>关于三个函数的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DIR_INC = ./include</span><br><span class="line">DIR_SRC = ./src</span><br><span class="line">DIR_OBJ = ./obj</span><br><span class="line"></span><br><span class="line">SRC = $(wildcard $&#123;DIR_SRC&#125;/*.c)# 指定编译当前目录下所有`.c`文件，全路径`./src/*.c`</span><br><span class="line">DIR = $(notdir $&#123;SRC&#125;)# 去除路径名，只留下文件名`*.c`</span><br><span class="line">OBJ = $(patsubst %.c, $&#123;DIR_OBJ&#125;/%.o, $&#123;DIR&#125;)# 将`DIR`中匹配到的`%.c`，替换为`$&#123;DIR_OBJ&#125;/%.o`</span><br><span class="line"></span><br><span class="line">ALL:</span><br><span class="line">@echo $(SRC)</span><br><span class="line">@echo $(DIR)</span><br><span class="line">@echo $(OBJ)</span><br></pre></td></tr></table></figure><p>执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> make</span><br><span class="line">./src/hello.c ./src/test.c</span><br><span class="line">hello.c test.c</span><br><span class="line">./obj/hello.o ./obj/test.o</span><br></pre></td></tr></table></figure></p><blockquote><p>注：若<code>./src</code>目录下还有子目录<code>./src/inc</code>，则<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SRC = <span class="variable">$(<span class="built_in">wildcard</span> $&#123;DIR_SRC&#125;/*.c)</span> <span class="variable">$(<span class="built_in">wildcard</span> $&#123;DIR_SRC&#125;/inc/*.c)</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Makefile 使用总结 - wang_yb - 博客园 <a href="https://www.cnblogs.com/wang_yb/p/3990952.html" target="_blank" rel="noopener">https://www.cnblogs.com/wang_yb/p/3990952.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CMake编译库文件</title>
      <link href="/2019/01/05/Cmake%E7%BC%96%E8%AF%91%E5%BA%93%E6%96%87%E4%BB%B6/"/>
      <url>/2019/01/05/Cmake%E7%BC%96%E8%AF%91%E5%BA%93%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>库文件即源代码的二进制文件，我们通常把一些公用函数制作成函数库，供其它程序使用。函数库分为静态库和动态库两种。静态库在程序编译时会被连接到目标代码中，程序运行时将不再需要该静态库；动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此在程序运行时还需要动态库存在。</p><h1 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h1><p>以DarkNet为例，我们将其源代码编译成<code>.a</code>静态库文件。</p><ol><li><p>下载源码</p><blockquote><p>YOLO: Real-Time Object Detection <a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">https://pjreddie.com/darknet/yolo/</a></p></blockquote> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/pjreddie/darknet</span><br></pre></td></tr></table></figure></li><li><p>整理文件<br> 我们将<code>include/</code>与<code>src/</code>目录复制到新建文件夹<code>darknet/</code>。目录结构如下</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">darknet</span><br><span class="line">├── include</span><br><span class="line">│   └── darknet.h</span><br><span class="line">└── src</span><br><span class="line">    ├── activation_kernels.cu</span><br><span class="line">    ├── activation_layer.c</span><br><span class="line">    ├── activation_layer.h</span><br><span class="line">    ├── ...</span><br><span class="line">    ├── utils.c</span><br><span class="line">    ├── utils.h</span><br><span class="line">    ├── yolo_layer.c</span><br><span class="line">    └── yolo_layer.h</span><br></pre></td></tr></table></figure></li><li><p>在<code>darknet/</code>目录下编写<code>CmakeLists.txt</code>文件，内容如下</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">CMAKE_MINIMUM_REQUIRED(VERSION 2.8)                                # cmake需要的最小版本号</span><br><span class="line">PROJECT(darknet)                                        # 项目名</span><br><span class="line"></span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;----------------------------------------------------------------------&quot;</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;project name:      &quot; $&#123;PROJECT_NAME&#125;           # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;source directory:  &quot; $&#123;PROJECT_SOURCE_DIR&#125;     # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;binary directory:  &quot; $&#123;PROJECT_BINARY_DIR&#125;     # cmake默认参数</span><br><span class="line">)</span><br><span class="line">MESSAGE(STATUS </span><br><span class="line">    &quot;----------------------------------------------------------------------&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">INCLUDE_DIRECTORIES(                                # 头文件目录</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/include</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/src</span><br><span class="line">)                                                   </span><br><span class="line">AUX_SOURCE_DIRECTORY(                               # 源文件</span><br><span class="line">    $&#123;PROJECT_SOURCE_DIR&#125;/src </span><br><span class="line">    lib_srcfile</span><br><span class="line">)                                                   </span><br><span class="line">SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib)  # 设置保存`.a`的目录</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">ADD_LIBRARY(                                        # 生成库文件，可选择`.a`或`.so`</span><br><span class="line">    $&#123;PROJECT_NAME&#125;</span><br><span class="line">    STATIC                                          # `.a`</span><br><span class="line">    # SHARED                                        # `.so`</span><br><span class="line">    $&#123;lib_srcfile&#125;</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">SET_TARGET_PROPERTIES(</span><br><span class="line">    $&#123;PROJECT_NAME&#125;</span><br><span class="line">    PROPERTIES</span><br><span class="line">    LINKER_LANGUAGE C</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li><p>执行命令<br> 我们在<code>darknet/</code>目录打开终端</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir build</span><br><span class="line"><span class="meta">$</span> cd build</span><br><span class="line">/build$ cmake ..</span><br><span class="line">-- The C compiler identification is GNU 7.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 7.4.0</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/louishsu/Work/Codes/MTCNN_Darknet/darknet/build</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> make</span><br><span class="line">Scanning dependencies of target darknet</span><br><span class="line">[  2%] Building C object CMakeFiles/darknet.dir/src/activation_layer.c.o</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">[ 97%] Building C object CMakeFiles/darknet.dir/src/yolo_layer.c.o</span><br><span class="line"><span class="meta">[100%</span>] Linking C shared library lib/libdarknet.so</span><br><span class="line"><span class="meta">[100%</span>] Built target darknet</span><br></pre></td></tr></table></figure></li></ol><p>在<code>darknet/build/lib</code>目录下即可找到<code>libdarknet.a</code>库文件，<code>build</code>目录结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">build</span><br><span class="line">└── lib</span><br><span class="line">    └── libdarknet.so (*)</span><br></pre></td></tr></table></figure></p><h1 id="调用库函数"><a href="#调用库函数" class="headerlink" title="调用库函数"></a>调用库函数</h1><p>为测试该库函数是否编译成功，编写测试代码，新建目录<code>/test/</code>，其文件结构为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test</span><br><span class="line">├── include</span><br><span class="line">│   └── test.h</span><br><span class="line">├── src</span><br><span class="line">│   └── test.c</span><br><span class="line">├── build</span><br><span class="line">│   └── test</span><br><span class="line">└── CMakeLists.txt</span><br></pre></td></tr></table></figure></p><p>头文件<code>/include/test.h</code>内容为<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> TEST_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TEST_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"darknet.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></p><p>源文件<code>/src/test.c</code>内容如下<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"test.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"Hello! Darknet!\n"</span>);</span><br><span class="line">matrix M = make_matrix(<span class="number">4</span>, <span class="number">5</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"The size of matrix M is %ld bytes\n"</span>, <span class="keyword">sizeof</span>(M));</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>编译文件<code>CMakeLists.txt</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">CMAKE_MINIMUM_REQUIRED(VERSION 2.8)                        # cmake需要的最小版本号</span><br><span class="line">PROJECT(Test)                               # 项目名</span><br><span class="line"></span><br><span class="line">ADD_DEFINITIONS(-DOPENCV=1)</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">SET(DARKNET ../darknet)</span><br><span class="line">INCLUDE_DIRECTORIES(                                            # 头文件目录</span><br><span class="line">$&#123;DARKNET&#125;/include</span><br><span class="line">$&#123;DARKNET&#125;/src</span><br><span class="line">)          </span><br><span class="line">LINK_DIRECTORIES(                                               # 库文件目录</span><br><span class="line">    $&#123;DARKNET&#125;/build/lib</span><br><span class="line">)                  </span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">INCLUDE_DIRECTORIES(./include)                                # 当前项目头文件目录</span><br><span class="line">AUX_SOURCE_DIRECTORY(./src SRC_FILES)                          # 当前项目源文件目录</span><br><span class="line"></span><br><span class="line"># ----------------------------------------------------------------------------------</span><br><span class="line">ADD_EXECUTABLE($&#123;PROJECT_NAME&#125; $&#123;SRC_FILES&#125;)                 # 添加可执行文件</span><br><span class="line">TARGET_LINK_LIBRARIES(# 引用库</span><br><span class="line">$&#123;PROJECT_NAME&#125;</span><br><span class="line">darknet# darknet</span><br><span class="line">m# 数学函数库</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>执行指令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir build</span><br><span class="line"><span class="meta">$</span> cd build</span><br><span class="line"><span class="meta">$</span> cmake ..</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">-- Configuring done</span><br><span class="line">-- Generating done</span><br><span class="line">-- Build files have been written to: /home/louishsu/Work/Codes/makefile/test/build</span><br><span class="line"><span class="meta">$</span> make</span><br><span class="line">Scanning dependencies of target Test</span><br><span class="line">[ 50%] Building C object CMakeFiles/Test.dir/src/test.c.o</span><br><span class="line"><span class="meta">[100%</span>] Linking C executable Test</span><br><span class="line"><span class="meta">[100%</span>] Built target Test</span><br></pre></td></tr></table></figure></p><p>运行可执行文件<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> ./Test</span><br><span class="line">Hello! Darknet!</span><br><span class="line">The size of matrix M is 16 bytes</span><br></pre></td></tr></table></figure></p><p>查看结构体<code>matrix</code>定义<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">matrix</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> rows, cols;</span><br><span class="line">    <span class="keyword">float</span> **vals;</span><br><span class="line">&#125; matrix;</span><br></pre></td></tr></table></figure></p><p><code>int</code>占<code>32bit</code>，<code>float*</code>占<code>64bit</code>，故<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(32bit * 2 + 64bit) / 8 = 16byte</span><br></pre></td></tr></table></figure></p><p>运行成功。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>静态库和动态库的优缺点 - 默默淡然 - 博客园 <a href="https://www.cnblogs.com/liangxiaofeng/p/3228145.html" target="_blank" rel="noopener">https://www.cnblogs.com/liangxiaofeng/p/3228145.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu编译安装Tensorflow</title>
      <link href="/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/"/>
      <url>/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow/</url>
      
        <content type="html"><![CDATA[<h1 id="非常重要"><a href="#非常重要" class="headerlink" title="非常重要"></a>非常重要</h1><p>如果中途出现错误，<code>xxxx</code>文件找不到，不要怀疑！就是大天朝的网络问题！推荐科学上网！</p><h1 id="安装CUDA与CUDNN"><a href="#安装CUDA与CUDNN" class="headerlink" title="安装CUDA与CUDNN"></a>安装CUDA与CUDNN</h1><p>首先查看显卡是否支持<code>CUDA</code>加速，输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure></p><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/nvidia-smi.png" alt="nvidia-smi"></p><p>在<code>Ubuntu16.04 LTS</code>下，推荐安装<code>CUDA9.0</code>和<code>CUDNN 7</code>。</p><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn1.png" alt="cuda_cudnn1"></p><ul><li><p>CUDA</p><blockquote><p>CUDA Toolkit 9.0 Downloads | NVIDIA Developer <a href="https://developer.nvidia.com/cuda-90-download-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-90-download-archive</a></p></blockquote><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda.png" alt="cuda"></p><p>  下载<code>.run</code>版本，安装方法如下</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chmod +x cuda_9.0.176_384.81_linux.run </span><br><span class="line">$ sudo sh ./cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure><p>  服务条款很长。。。。</p></li></ul><ul><li><p>CUDNN</p><blockquote><p>NVIDIA cuDNN | NVIDIA Developer <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a></p></blockquote><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cudnn1.png" alt="cudnn1"></p><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cudnn2.png" alt="cudnn2"></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz</span><br><span class="line">$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</span><br><span class="line">$ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>  安装后进行验证</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cp -r /usr/src/cudnn_samples_v7/ $HOME</span><br><span class="line">$ cd  $HOME/cudnn_samples_v7/mnistCUDNN</span><br><span class="line">$ make clean &amp;&amp; make</span><br><span class="line">$ ./mnistCUDNN</span><br></pre></td></tr></table></figure><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying.png" alt="cuda_cudnn_verifying"></p><p>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/cuda_cudnn_verifying2.png" alt="cuda_cudnn_verifying2"></p></li></ul><h1 id="编译Tensorflow-CPU-version"><a href="#编译Tensorflow-CPU-version" class="headerlink" title="编译Tensorflow(CPU version)"></a>编译Tensorflow(CPU version)</h1><p>由于训练代码使用<code>Python</code>实现，故<code>C++</code>版本的<code>Tensorflow</code>不使用<code>GPU</code>，仅实现预测代码即可。</p><h2 id="bazel"><a href="#bazel" class="headerlink" title="bazel"></a>bazel</h2><blockquote><p>Installing Bazel on Ubuntu - Bazel <a href="https://docs.bazel.build/versions/master/install-ubuntu.html" target="_blank" rel="noopener">https://docs.bazel.build/versions/master/install-ubuntu.html</a><br>一定要用源码安装！！！</p></blockquote><p>download the Bazel binary installer named <code>bazel-&lt;version&gt;-installer-linux-x86_64.sh</code> from the <a href="https://github.com/bazelbuild/bazel/releases" target="_blank" rel="noopener">Bazel releases page on GitHub</a>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python</span><br><span class="line">$ chmod +x bazel-&lt;version&gt;-installer-linux-x86_64.sh</span><br><span class="line">$ ./bazel-&lt;version&gt;-installer-linux-x86_64.sh --user</span><br><span class="line">$ sudo nano ~/.bashrc # export PATH=&quot;$PATH:$HOME/bin&quot;</span><br><span class="line">$ source ~/.bashrc </span><br><span class="line">$ bazel version</span><br></pre></td></tr></table></figure><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/bazel.png" alt="bazel"></p><h2 id="编译CPU版本的CPU"><a href="#编译CPU版本的CPU" class="headerlink" title="编译CPU版本的CPU"></a>编译CPU版本的CPU</h2><p>查看<code>java</code>版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br><span class="line">openjdk version &quot;1.8.0_191&quot;</span><br><span class="line">OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)</span><br><span class="line">OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br></pre></td></tr></table></figure></p><h2 id="安装依赖软件包环境"><a href="#安装依赖软件包环境" class="headerlink" title="安装依赖软件包环境"></a>安装依赖软件包环境</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install python3-dev</span><br><span class="line">$ pip3 install six</span><br><span class="line">$ pip3 install numpy</span><br><span class="line">$ pip3 instal wheel</span><br></pre></td></tr></table></figure><h2 id="下载Tensorflow源码"><a href="#下载Tensorflow源码" class="headerlink" title="下载Tensorflow源码"></a>下载<code>Tensorflow</code>源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/tensorflow/tensorflow</span><br></pre></td></tr></table></figure><h2 id="编译与安装"><a href="#编译与安装" class="headerlink" title="编译与安装"></a>编译与安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd tensorflow</span><br><span class="line">$ ./configure</span><br></pre></td></tr></table></figure><p>配置选项如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command "bazel shutdown".</span><br><span class="line">INFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5</span><br><span class="line">You have bazel 0.20.0 installed.</span><br><span class="line">Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Found possible Python library paths:</span><br><span class="line">  /usr/local/lib/python3.5/dist-packages</span><br><span class="line">  /usr/lib/python3/dist-packages</span><br><span class="line">Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n</span><br><span class="line">No XLA JIT support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n</span><br><span class="line">No OpenCL SYCL support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with ROCm support? [y/N]: n</span><br><span class="line">No ROCm support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N]: n</span><br><span class="line">No CUDA support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Do you wish to download a fresh release of clang? (Experimental) [y/N]: n</span><br><span class="line">Clang will not be downloaded.</span><br><span class="line"></span><br><span class="line">Do you wish to build TensorFlow with MPI support? [y/N]: n</span><br><span class="line">No MPI support will be enabled for TensorFlow.</span><br><span class="line"></span><br><span class="line">Please specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -march=native -Wno-sign-compare]: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n</span><br><span class="line">Not configuring the WORKSPACE for Android builds.</span><br><span class="line"></span><br><span class="line">Preconfigured Bazel build configs. You can use any of the below by adding "--config=&lt;&gt;" to your build command. See .bazelrc for more details.</span><br><span class="line">--config=mkl         # Build with MKL support.</span><br><span class="line">--config=monolithic  # Config for mostly static monolithic build.</span><br><span class="line">--config=gdr         # Build with GDR support.</span><br><span class="line">--config=verbs       # Build with libverbs support.</span><br><span class="line">--config=ngraph      # Build with Intel nGraph support.</span><br><span class="line">--config=dynamic_kernels# (Experimental) Build kernels into separate shared objects.</span><br><span class="line">Preconfigured Bazel build configs to DISABLE default on features:</span><br><span class="line">--config=noaws       # Disable AWS S3 filesystem support.</span><br><span class="line">--config=nogcp       # Disable GCP support.</span><br><span class="line">--config=nohdfs      # Disable HDFS support.</span><br><span class="line">--config=noignite    # Disable Apacha Ignite support.</span><br><span class="line">--config=nokafka     # Disable Apache Kafka support.</span><br><span class="line">--config=nonccl      # Disable NVIDIA NCCL support.</span><br><span class="line">Configuration finished</span><br></pre></td></tr></table></figure></p><p>使用<code>bazel</code>编译<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bazel build --config=opt //tensorflow:libtensorflow_cc.so</span><br></pre></td></tr></table></figure></p><p>出现错误</p><blockquote><p>TF failing to build on Bazel CI · Issue #19464 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/issues/19464" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/19464</a><br>Failure to build TF 1.12 from source - multiple definitions in grpc · Issue #23402 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197</a><br>Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/pull/23583" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/pull/23583</a><br>Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow <a href="https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5</a></p></blockquote><p>解决方法</p><ul><li>方法1<br>  <img src="/2019/01/04/Ubuntu编译安装Tensorflow/tools_bazel.rc.png" alt="tools_bazel.rc"></li><li><p>方法2<br>  将<code>tools/bazel.rc</code>中内容粘到<code>.tf_configure.bazelrc</code>中，每次重新配置后需要重新粘贴一次。</p></li><li><p>源码安装<code>protobuf3.6.0</code></p><blockquote><p><a href="https://github.com/protocolbuffers/protobuf" target="_blank" rel="noopener">https://github.com/protocolbuffers/protobuf</a></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></blockquote></li><li><p>下载其他文件</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./tensorflow/contrib/makefile/download_dependencies.sh</span><br><span class="line">mkdir /tmp/eigen</span><br></pre></td></tr></table></figure><ul><li>值得注意，<code>download_dependencies.sh</code>中下载依赖包时，需要用到<code>curl</code>，但是默认方式安装  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install curl</span><br></pre></td></tr></table></figure></li></ul></li></ul><pre><code>    &gt; 现在是2018/12/19/02:48，被这个问题折腾了3个小时。时不支持`https`协议，故需要安装`OpenSSL`，并源码安装，详细资料见[curl提示不支持https协议解决方法 - 标配的小号 - 博客园](https://www.cnblogs.com/biaopei/p/8669810.html)- 执行`./autogen.sh`时，发生错误`autoreconf: not found`，则安装    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install autoconf aotomake libtool</span><br><span class="line">$ sudo apt install libffi-dev</span><br></pre></td></tr></table></figure></code></pre><ul><li>源码安装<code>Eigen</code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd tensorflow/contrib/makefile/Downloads/eigen</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></li></ul><h1 id="调用C-版本的Tensorflow"><a href="#调用C-版本的Tensorflow" class="headerlink" title="调用C++版本的Tensorflow"></a>调用C++版本的Tensorflow</h1><p>创建文件目录如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">|-- tf_test</span><br><span class="line">    |-- build</span><br><span class="line">    |-- main.cpp</span><br><span class="line">    |-- CMakeLists.txt</span><br></pre></td></tr></table></figure></p><p><code>main.cpp</code>文件内容如下<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/cc/client/client_session.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/cc/ops/standard_ops.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tensorflow/core/framework/tensor.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> tensorflow;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> tensorflow::ops;</span><br><span class="line">    Scope root = Scope::NewRootScope();</span><br><span class="line">    <span class="comment">// Matrix A = [3 2; -1 0]</span></span><br><span class="line">    <span class="keyword">auto</span> A = Const(root, &#123; &#123;<span class="number">3.f</span>, <span class="number">2.f</span>&#125;, &#123;<span class="number">-1.f</span>, <span class="number">0.f</span>&#125;&#125;);</span><br><span class="line">    <span class="comment">// Vector b = [3 5]</span></span><br><span class="line">    <span class="keyword">auto</span> b = Const(root, &#123; &#123;<span class="number">3.f</span>, <span class="number">5.f</span>&#125;&#125;);</span><br><span class="line">    <span class="comment">// v = Ab^T</span></span><br><span class="line">    <span class="keyword">auto</span> v = MatMul(root.WithOpName(<span class="string">"v"</span>), A, b, MatMul::TransposeB(<span class="literal">true</span>));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Tensor&gt; outputs;</span><br><span class="line">    <span class="function">ClientSession <span class="title">session</span><span class="params">(root)</span></span>;</span><br><span class="line">    <span class="comment">// Run and fetch v</span></span><br><span class="line">    TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs));</span><br><span class="line">    <span class="comment">// Expect outputs[0] == [19; -3]</span></span><br><span class="line">    LOG(INFO) &lt;&lt; outputs[<span class="number">0</span>].matrix&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>CMakeLists.txt</code>内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required (VERSION 2.8.8)</span><br><span class="line">project (tf_example)</span><br><span class="line"></span><br><span class="line">set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -g -std=c++11 -W&quot;)</span><br><span class="line"></span><br><span class="line">set(EIGEN_DIR /usr/local/include/eigen3)</span><br><span class="line">set(PROTOBUF_DIR/usr/local/include/google/protobuf)</span><br><span class="line">set(TENSORFLOW_DIR /home/louishsu/install/tensorflow-1.12.0)</span><br><span class="line"></span><br><span class="line">include_directories(</span><br><span class="line">$&#123;EIGEN_DIR&#125;</span><br><span class="line">$&#123;PROTOBUF_DIR&#125;</span><br><span class="line">   $&#123;TENSORFLOW_DIR&#125;</span><br><span class="line">$&#123;TENSORFLOW_DIR&#125;/bazel-genfiles</span><br><span class="line">$&#123;TENSORFLOW_DIR&#125;/tensorflow/contrib/makefile/downloads/absl</span><br><span class="line">)</span><br><span class="line">link_directories(</span><br><span class="line">/usr/local/lib</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">add_executable(</span><br><span class="line">tf_test</span><br><span class="line">main.cpp</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">target_link_libraries(</span><br><span class="line">tf_test</span><br><span class="line">tensorflow_cc</span><br><span class="line">tensorflow_framework</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir build &amp;&amp; cd build</span><br><span class="line">$ cmake .. &amp;&amp; make</span><br><span class="line">$ ./tf_test</span><br></pre></td></tr></table></figure><h1 id="install-tensorflow-gpu-for-python"><a href="#install-tensorflow-gpu-for-python" class="headerlink" title="install tensorflow-gpu for python"></a>install tensorflow-gpu for python</h1><p>可使用<code>pip</code>指令安装，推荐下载安装包，</p><blockquote><p>tensorflow · PyPI <a href="https://pypi.org/project/tensorflow/" target="_blank" rel="noopener">https://pypi.org/project/tensorflow/</a></p></blockquote><p><img src="/2019/01/04/Ubuntu编译安装Tensorflow/tensorflow_for_python.png" alt="tensorflow_for_python"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd ~/Downloads</span><br><span class="line">$ pip3 --default-timeout=1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user</span><br></pre></td></tr></table></figure><p>安装后进行验证<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> python3</span><br><span class="line">Python 3.5.2 (default, Nov 12 2018, 13:43:14) </span><br><span class="line">[GCC 5.4.0 20160609] on linux</span><br><span class="line">Type "help", "copyright", "credits" or "license" for more information.</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; import tensorflow as tf</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess = tf.Session()</span><br><span class="line">2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: </span><br><span class="line">name: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758</span><br><span class="line">pciBusID: 0000:04:00.0</span><br><span class="line">totalMemory: 983.44MiB freeMemory: 177.19MiB</span><br><span class="line">2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0</span><br><span class="line">2018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 </span><br><span class="line">2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N </span><br><span class="line">2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; a = tf.Variable([233])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; init = tf.initialize_all_variables()</span><br><span class="line">WARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.</span><br><span class="line">Instructions for updating:</span><br><span class="line">Use `tf.global_variables_initializer` instead.</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess.run(init)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess.run(a)</span><br><span class="line">array([233], dtype=int32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess.close()</span><br></pre></td></tr></table></figure></p><p>注意，如果异常中断程序，显存不会被释放，需要自行<code>kill</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br></pre></td></tr></table></figure></p><p>获得<code>PID</code>序号，使用指令结束进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -9 pid</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>TensorFlow C++动态库编译 - 简书 <a href="https://www.jianshu.com/p/d46596558640" target="_blank" rel="noopener">https://www.jianshu.com/p/d46596558640</a><br>Tensorflow C++ 从训练到部署(1)：环境搭建 | 技术刘 <a href="http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" target="_blank" rel="noopener">http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu编译安装OpenCV</title>
      <link href="/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV/"/>
      <url>/2019/01/04/Ubuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV/</url>
      
        <content type="html"><![CDATA[<h1 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h1><blockquote><p>OpenCV library <a href="https://opencv.org/" target="_blank" rel="noopener">https://opencv.org/</a></p></blockquote><h1 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h1><h2 id="依赖软件包"><a href="#依赖软件包" class="headerlink" title="依赖软件包"></a>依赖软件包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install cmake</span><br><span class="line">$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev</span><br></pre></td></tr></table></figure><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ unzip opencv-3.4.4.zip</span><br><span class="line">$ cd opencv-3.4.4</span><br><span class="line">$ mkdir build &amp;&amp; cd build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make -j4</span><br></pre></td></tr></table></figure><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo make install</span><br><span class="line">$ sudo nano /etc/ld.so.conf.d/opencv.conf # add `/usr/local/lib`</span><br><span class="line">$ sudo ldconfig</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p><code>OpenCV</code>自带验证程序，在<code>opencv-3.4.4/samples/cpp/example_cmake</code>中可以找到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cd opencv-3.4.4/samples/cpp/example_cmake</span><br><span class="line">$ cmake .</span><br><span class="line">$ make</span><br><span class="line">$ ./opencv_example</span><br></pre></td></tr></table></figure><p>如果没问题，可以看到你的大脸了~</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>Ubuntu16.04安装openCV3.4.4 - 辣屁小心的学习笔记 - CSDN博客 <a href="https://blog.csdn.net/weixin_39992397/article/details/84345197" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39992397/article/details/84345197</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python读写配置文件</title>
      <link href="/2019/01/04/Python%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"/>
      <url>/2019/01/04/Python%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<p>在深度学习中，有许多运行参数需要指定，有几种方法可以解决</p><ul><li>定义<code>.py</code>文件存储变量</li><li>定义命名元组<code>collections.namedtuple()</code></li><li>创建<code>.config</code>，<code>.ini</code>等配置文件</li></ul><p>Python 读取写入配置文件很方便，使用内置模块<code>configparser</code>即可</p><h1 id="读出"><a href="#读出" class="headerlink" title="读出"></a>读出</h1><p>首先创建文件<code>test.config</code>或<code>test.ini</code>，写入如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[db]</span><br><span class="line">db_port = 3306</span><br><span class="line">db_user = root</span><br><span class="line">db_host = 127.0.0.1</span><br><span class="line">db_pass = test</span><br><span class="line"></span><br><span class="line">[concurrent]</span><br><span class="line">processor = 20</span><br><span class="line">thread = 10</span><br></pre></td></tr></table></figure></p><p>读取操作如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; import configparser</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; configfile = &quot;./test.config&quot;</span><br><span class="line">&gt;&gt;&gt; inifile = &quot;./test.ini&quot;</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf = configparser.ConfigParser()</span><br><span class="line">&gt;&gt;&gt; cf.read(configfile)                     # 读取文件内容</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; sections = cf.sections()                # 所有的section，以列表的形式返回</span><br><span class="line">&gt;&gt;&gt; sections</span><br><span class="line">[&apos;db&apos;, &apos;concurrent&apos;]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; options = cf.options(&apos;db&apos;)              # 该section的所有option</span><br><span class="line">&gt;&gt;&gt; options</span><br><span class="line">[&apos;db_port&apos;, &apos;db_user&apos;, &apos;db_host&apos;, &apos;db_pass&apos;]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; items = cf.items(&apos;db&apos;)                  # 该section的所有键值对</span><br><span class="line">&gt;&gt;&gt; items</span><br><span class="line">[(&apos;db_port&apos;, &apos;3306&apos;), (&apos;db_user&apos;, &apos;root&apos;), (&apos;db_host&apos;, &apos;127.0.0.1&apos;), (&apos;db_pass&apos;, &apos;test&apos;)]</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; db_user = cf.get(&apos;db&apos;, &apos;db_user&apos;)       # section中option的值，返回为string类型</span><br><span class="line">&gt;&gt;&gt; db_user</span><br><span class="line">&apos;root&apos;</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; db_port = cf.getint(&apos;db&apos;, &apos;db_port&apos;)    # 得到section中option的值，返回为int类型</span><br><span class="line">&gt;&gt;&gt;                                         # 类似的还有getboolean()与getfloat()</span><br><span class="line">&gt;&gt;&gt; db_port</span><br><span class="line">3306</span><br></pre></td></tr></table></figure></p><h1 id="写入"><a href="#写入" class="headerlink" title="写入"></a>写入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import os</span><br><span class="line">&gt;&gt;&gt; import configparser</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf = configparser.ConfigParser()</span><br><span class="line">&gt;&gt;&gt; cf.add_section(&apos;test1&apos;)                 # 新增section</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1)              # 新增option：错误示范</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;pyshell#7&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    cf.set(&quot;test&quot;, &quot;count&quot;, 1)</span><br><span class="line">  File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1192, in set</span><br><span class="line">    self._validate_value_types(option=option, value=value)</span><br><span class="line">  File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1177, in _validate_value_types</span><br><span class="line">    raise TypeError(&quot;option values must be strings&quot;)</span><br><span class="line">TypeError: option values must be strings</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, &apos;1&apos;)            # 新增option</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.set(&quot;test1&quot;, &quot;opt1&quot;, &apos;ok&apos;)           # 新增option</span><br><span class="line">&gt;&gt;&gt; cf.remove_option(&quot;test1&quot;, &quot;opt1&quot;)       # 删除option</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; cf.add_section(&apos;test2&apos;)                 # 新增section</span><br><span class="line">&gt;&gt;&gt; cf.remove_section(&apos;test2&apos;)              # 删除section</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; with open(&quot;./test_wr.config&quot;, &apos;w+&apos;) as f:</span><br><span class="line">        cf.write(f)                         # 写入文件test_wr.config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>现在目录已创建文件<code>test_wr.config</code>，打开可以看到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[test1]</span><br><span class="line">count = 1</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python更新安装的包</title>
      <link href="/2019/01/04/Python%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85/"/>
      <url>/2019/01/04/Python%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<p><code>pip</code>不提供升级全部已安装模块的方法，以下指令可查看更新信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip list --outdate</span><br></pre></td></tr></table></figure></p><p>得到输出信息如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Package           Version   Latest     Type</span><br><span class="line">----------------- --------- ---------- -----</span><br><span class="line">absl-py           0.3.0     0.6.1      sdist</span><br><span class="line">autopep8          1.3.5     1.4.2      sdist</span><br><span class="line">bleach            2.1.4     3.0.2      wheel</span><br><span class="line">certifi           2018.8.24 2018.10.15 wheel</span><br><span class="line">dask              0.20.0    0.20.1     wheel</span><br><span class="line">grpcio            1.14.1    1.16.0     wheel</span><br><span class="line">ipykernel         5.0.0     5.1.0      wheel</span><br><span class="line">ipython           7.0.1     7.1.1      wheel</span><br><span class="line">jedi              0.12.1    0.13.1     wheel</span><br><span class="line">jupyter-console   5.2.0     6.0.0      wheel</span><br><span class="line">Markdown          2.6.11    3.0.1      wheel</span><br><span class="line">MarkupSafe        1.0       1.1.0      wheel</span><br><span class="line">matplotlib        2.2.2     3.0.2      wheel</span><br><span class="line">mistune           0.8.3     0.8.4      wheel</span><br><span class="line">numpy             1.14.5    1.15.4     wheel</span><br><span class="line">opencv-python     3.4.2.17  3.4.3.18   wheel</span><br><span class="line">Pillow            5.2.0     5.3.0      wheel</span><br><span class="line">prometheus-client 0.3.1     0.4.2      sdist</span><br><span class="line">pyparsing         2.2.0     2.3.0      wheel</span><br><span class="line">python-dateutil   2.7.3     2.7.5      wheel</span><br><span class="line">pytz              2018.5    2018.7     wheel</span><br><span class="line">urllib3           1.23      1.24.1     wheel</span><br></pre></td></tr></table></figure></p><p>以下提供一键升级的方法，可能比较久hhhh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pip._internal.utils.misc import get_installed_distributions</span><br><span class="line">from subprocess import call</span><br><span class="line"> </span><br><span class="line">for dist in get_installed_distributions():</span><br><span class="line">    modulename = dist.project_name</span><br><span class="line">    print(&apos;start processing module &apos; + modulename)</span><br><span class="line">    call(&quot;pip install --upgrade &quot; + modulename, shell=True)</span><br><span class="line">    print(&apos;module &apos; + modulename + &apos;done!&apos;)</span><br></pre></td></tr></table></figure></p><p>另外，从已有的安装列表，安装所需要的包，可使用以下指令</p><ul><li><p>在已安装的机器中，生成列表</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; xxx.list</span><br></pre></td></tr></table></figure></li><li><p>在未安装的机器中，使用列表安装</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r xxx.list</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python记录日志</title>
      <link href="/2019/01/04/Python%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/"/>
      <url>/2019/01/04/Python%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>日志可以用来记录应用程序的状态、错误和信息消息，也经常作为调试程序的工具。<br><code>Python</code>提供了一个标准的日志接口，就是<code>logging</code>模块。日志级别有<code>DEBUG</code>、<code>INFO</code>、<code>WARNING</code>、<code>ERROR</code>、<code>CRITICAL</code>五种。</p><p><a href="https://docs.python.org/3/library/logging.html" target="_blank" rel="noopener">logging — Logging facility for Python — Python 3.7.1 documentation</a></p><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><h2 id="logger对象"><a href="#logger对象" class="headerlink" title="logger对象"></a><code>logger</code>对象</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import logging</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger = logging.getLogger(__name__)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger</span><br><span class="line">&lt;Logger __main__ (WARNING)&gt;</span><br></pre></td></tr></table></figure><h2 id="日志级别"><a href="#日志级别" class="headerlink" title="日志级别"></a>日志级别</h2><p>可输出五种不同的日志级别，分别为有<code>DEBUG</code>、<code>INFO</code>、<code>WARNING</code>、<code>ERROR</code>、<code>CRITICAL</code><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.debug('test log')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.info('test log')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.warning('test log')</span><br><span class="line">test log</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.error('test log')</span><br><span class="line">test log</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.critical('test log')</span><br><span class="line">test log</span><br></pre></td></tr></table></figure></p><p>可以看到只有<code>WARNING</code>及以上级别日志被输出，这是由于默认的日志级别是<code>WARNING</code> ，所以低于此级别的日志不会记录。</p><h2 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.basicConfig(**kwarg)</span><br></pre></td></tr></table></figure><p><code>**kwarg</code>中部分参数如下</p><ul><li><p><code>format</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%(levelname)：日志级别的名字格式</span><br><span class="line">%(levelno)s：日志级别的数字表示</span><br><span class="line">%(name)s：日志名字</span><br><span class="line">%(funcName)s：函数名字</span><br><span class="line">%(asctime)：日志时间，可以使用datefmt去定义时间格式，如上图。</span><br><span class="line">%(pathname)：脚本的绝对路径</span><br><span class="line">%(filename)：脚本的名字</span><br><span class="line">%(module)：模块的名字</span><br><span class="line">%(thread)：thread id</span><br><span class="line">%(threadName)：线程的名字</span><br></pre></td></tr></table></figure></li><li><p><code>datefmt</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;%Y-%m-%d %H:%M:%S&apos;</span><br></pre></td></tr></table></figure></li><li><p><code>level</code><br>  默认为<code>ERROR</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">logging.DEBUG</span><br><span class="line">logging.INFO</span><br><span class="line">logging.WARNING</span><br><span class="line">logging.ERROR</span><br><span class="line">logging.CRITICAL</span><br></pre></td></tr></table></figure></li></ul><p>例如<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 未输出debug</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger = logging.getLogger()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.debug('test log')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 修改配置</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; log_format = '%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; log_datefmt = '%Y-%m-%d %H:%M:%S'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; log_level = logging.DEBUG</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logging.basicConfig(format=log_format, </span><br><span class="line">                        datefmt=log_datefmt, </span><br><span class="line">                        level=log_level)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 输出debug</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger = logging.getLogger()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; logger.debug('test log')</span><br><span class="line">&lt;pyshell#8&gt; [2018-11-13 11:59:52] [DEBUG] test log</span><br></pre></td></tr></table></figure></p><h2 id="输出到日志文件"><a href="#输出到日志文件" class="headerlink" title="输出到日志文件"></a>输出到日志文件</h2><p>保存代码为文件<code>log_test.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">log_format = <span class="string">'%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'</span></span><br><span class="line">log_datefmt = <span class="string">'%Y-%m-%d %H:%M:%S'</span></span><br><span class="line">log_level = logging.DEBUG</span><br><span class="line">log_filename = <span class="string">'./test.log'</span></span><br><span class="line">log_filemode = <span class="string">'a'</span>  <span class="comment"># 也可以为'w', 'w+'等</span></span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=log_format,</span><br><span class="line">                    datefmt=log_datefmt, </span><br><span class="line">                    level=log_level,</span><br><span class="line">                    filename=log_filename, </span><br><span class="line">                    filemode=log_filemode)</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line">logger.debug(<span class="string">'test log'</span>)</span><br><span class="line">logger.info(<span class="string">'test log'</span>)</span><br><span class="line">logger.warning(<span class="string">'test log'</span>)</span><br><span class="line">logger.error(<span class="string">'test log'</span>)</span><br><span class="line">logger.critical(<span class="string">'test log'</span>)</span><br></pre></td></tr></table></figure></p><p>运行完毕，打开<code>log_test.log</code>文件可以看到<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log_test.py [2018-11-13 12:11:04] [DEBUG] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [INFO] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [WARNING] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [ERROR] test log</span><br><span class="line">log_test.py [2018-11-13 12:11:04] [CRITICAL] test log</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+Github博客搭建</title>
      <link href="/2019/01/04/Github-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/01/04/Github-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>那么问题来了，现有的博客还是现有的这篇文章呢？</p><h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><p>安装<a href="https://nodejs.org/en/" target="_blank" rel="noopener">node.js</a>, <a href="https://git-scm.com/" target="_blank" rel="noopener">git</a>, <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a></p><h1 id="博客搭建"><a href="#博客搭建" class="headerlink" title="博客搭建"></a>博客搭建</h1><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>推荐使用<code>git</code>命令窗口，执行如下指令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir Blog</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> Blog</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo init</span></span><br><span class="line">INFO  Cloning hexo-starter to ~\Desktop\Blog</span><br><span class="line">Cloning into 'C:\Users\LouisHsu\Desktop\Blog'...</span><br><span class="line">remote: Enumerating objects: 68, done.</span><br><span class="line">remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68</span><br><span class="line">Unpacking objects: 100% (68/68), done.</span><br><span class="line">Submodule 'themes/landscape' (https://github.com/hexojs/hexo-theme-landscape.git) registered for path 'themes/landscape'</span><br><span class="line">Cloning into 'C:/Users/LouisHsu/Desktop/Blog/themes/landscape'...</span><br><span class="line">remote: Enumerating objects: 1, done.</span><br><span class="line">remote: Counting objects: 100% (1/1), done.</span><br><span class="line">remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866</span><br><span class="line">Receiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.</span><br><span class="line">Resolving deltas: 100% (459/459), done.</span><br><span class="line">Submodule path 'themes/landscape': checked out '73a23c51f8487cfcd7c6deec96ccc7543960d350'</span><br><span class="line">Install dependencies</span><br><span class="line">npm WARN deprecated titlecase@1.1.2: no longer maintained</span><br><span class="line">npm WARN deprecated postinstall-build@5.0.3: postinstall-build's behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> nunjucks@3.1.6 postinstall C:\Users\LouisHsu\Desktop\Blog\node_modules\nunjucks</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> node postinstall-build.js src</span></span><br><span class="line"></span><br><span class="line">npm notice created a lockfile as package-lock.json. You should commit this file.</span><br><span class="line">npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):</span><br><span class="line">npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)</span><br><span class="line"></span><br><span class="line">added 422 packages from 501 contributors and audited 4700 packages in 59.195s</span><br><span class="line">found 0 vulnerabilities</span><br><span class="line"></span><br><span class="line">INFO  Start blogging with Hexo!</span><br></pre></td></tr></table></figure></p><p>生成目录结构如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\-- scaffolds</span><br><span class="line">\-- source</span><br><span class="line">    \-- _posts</span><br><span class="line">\-- themes</span><br><span class="line">|-- _config.yml</span><br><span class="line">|-- package.json</span><br></pre></td></tr></table></figure></p><p>继续<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> npm install</span></span><br><span class="line">npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):</span><br><span class="line">npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)</span><br><span class="line"></span><br><span class="line">audited 4700 packages in 5.99s</span><br><span class="line">found 0 vulnerabilities</span><br></pre></td></tr></table></figure></p><p>现在该目录执行指令，开启<code>hexo</code>服务器<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo s</span></span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure></p><p><img src="/2019/01/04/Github-Hexo博客搭建/hexo_server.png" alt="hexo_server"></p><h2 id="生成目录和标签"><a href="#生成目录和标签" class="headerlink" title="生成目录和标签"></a>生成目录和标签</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo n page about</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo n page archives</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo n page categories</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo n page tags</span></span><br></pre></td></tr></table></figure><p>修改<code>/source/tags/index.md</code>，其他同理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">01| ---</span><br><span class="line">02| title: tags</span><br><span class="line">03| date: 2019-01-04 17:34:15</span><br><span class="line">04| ---</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">01| ---</span><br><span class="line">02| title: tags</span><br><span class="line">03| date: 2019-01-04 17:34:15</span><br><span class="line">04| type: &quot;tags&quot;</span><br><span class="line">05| comments: false</span><br><span class="line">06| ---</span><br></pre></td></tr></table></figure></p><h2 id="关联Github"><a href="#关联Github" class="headerlink" title="关联Github"></a>关联<code>Github</code></h2><p>在<code>Github</code>新建一个仓库，命名为<code>username.github.io</code>，例如<code>isLouisHsu.github.io</code>，新建时勾选<code>Initialize this repository with a README</code>，因为这个仓库必须不能为空。<br><img src="/2019/01/04/Github-Hexo博客搭建/github_io.png" alt="github_io"></p><p>打开博客目录下的<code>_config.yml</code>配置文件，定位到最后的<code>deploy</code>选项，修改如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repository: git@github.com:isLouisHsu/isLouisHsu.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure></p><p>安装插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>现在就可以将该目录内容推送到<code>Github</code>新建的仓库中了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure></p><h2 id="使用个人域名"><a href="#使用个人域名" class="headerlink" title="使用个人域名"></a>使用个人域名</h2><ol><li>在<code>source</code>目录下新建文件<code>CNAME</code>，输入解析后的个人域名</li><li>在<code>Github</code>主页修改域名</li></ol><h1 id="备份博客"><a href="#备份博客" class="headerlink" title="备份博客"></a>备份博客</h1><blockquote><p>没。没什么用<br>我。我不备份了<br>可以新建一个仓库专门保存文件试试</p></blockquote><p>现在博客的源文件仅保存在<code>PC</code>上， 我们对它们进行备份，并将仓库作为博客文件夹</p><ol><li>在仓库新建分支<code>hexo</code>，设置为默认分支<br> <img src="/2019/01/04/Github-Hexo博客搭建/create_branch_hexo.png" alt="create_branch_hexo"><br> <img src="/2019/01/04/Github-Hexo博客搭建/change_branch_hexo.png" alt="change_branch_hexo"></li><li><p>将仓库克隆至本地</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/isLouisHsu/isLouisHsu.github.io.git</span><br></pre></td></tr></table></figure></li><li><p>克隆文件<br> 将之前的Hexo文件夹中的</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scffolds/</span><br><span class="line">source/</span><br><span class="line">themes/</span><br><span class="line">.gitignore</span><br><span class="line">_config.yml</span><br><span class="line">package.json</span><br></pre></td></tr></table></figure><p> 复制到克隆下来的仓库文件夹<code>isLouisHsu.github.io</code><br> <img src="/2019/01/04/Github-Hexo博客搭建/backup_blog.png" alt="backup_blog"></p></li><li><p>安装包</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ npm install</span><br><span class="line">$ npm install hexo --save</span><br><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p> 备份博客使用以下指令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;backup&quot;</span><br><span class="line">$ git push origin hexo</span><br></pre></td></tr></table></figure></li><li><p>部署博客指令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g -d</span><br></pre></td></tr></table></figure></li><li><p><code>单键</code>提交<br> 编写脚本<code>commit.bat</code>，双击即可</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &apos;backup&apos;</span><br><span class="line">git push origin hexo</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure></li></ol><h1 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h1><ul><li><p>目录结构</p><ul><li><code>public</code>  生成的网站文件，发布的站点文件。</li><li><code>source</code>  资源文件夹，用于存放内容。</li><li><code>tag</code>     标签文件夹。</li><li><code>archive</code> 归档文件夹。</li><li><code>category</code>分类文件夹。</li><li><code>downloads/code include code</code>文件夹。</li><li><code>:lang i18n_dir</code> 国际化文件夹。</li><li><code>_config.yml</code> 配置文件</li></ul></li><li><p>指令</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ hexo help</span><br><span class="line">Usage: hexo &lt;command&gt;</span><br><span class="line"></span><br><span class="line">Commands:</span><br><span class="line">    clean     Remove generated files and cache.</span><br><span class="line">    config    Get or set configurations.</span><br><span class="line">    deploy    Deploy your website.</span><br><span class="line">    generate  Generate static files.</span><br><span class="line">    help      Get help on a command.</span><br><span class="line">    init      Create a new Hexo folder.</span><br><span class="line">    list      List the information of the site</span><br><span class="line">    migrate   Migrate your site from other system to Hexo.</span><br><span class="line">    new       Create a new post.</span><br><span class="line">    publish   Moves a draft post from _drafts to _posts folder.</span><br><span class="line">    render    Render files with renderer plugins.</span><br><span class="line">    server    Start the server.</span><br><span class="line">    version   Display version information.</span><br><span class="line"></span><br><span class="line">Global Options:</span><br><span class="line">    --config  Specify config file instead of using _config.yml</span><br><span class="line">    --cwd     Specify the CWD</span><br><span class="line">    --debug   Display all verbose messages in the terminal</span><br><span class="line">    --draft   Display draft posts</span><br><span class="line">    --safe    Disable all plugins and scripts</span><br><span class="line">    --silent  Hide output on console</span><br><span class="line"></span><br><span class="line">For more help, you can use &apos;hexo help [command]&apos; for the detailed information or you can check the docs: http://hexo.io/docs/</span><br></pre></td></tr></table></figure></li></ul><!-- # 修改主题 --><h1 id="拓展功能支持"><a href="#拓展功能支持" class="headerlink" title="拓展功能支持"></a>拓展功能支持</h1><h2 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><p>修改文件<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure></p><p>在执行<code>$ hexo n [layout] &lt;title&gt;</code>时会生成同名文件夹，把图片放在这个文件夹内，在<code>.md</code>文件中插入图片<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![image_name](/title/image_name.png)</span><br></pre></td></tr></table></figure></p><h2 id="搜索功能"><a href="#搜索功能" class="headerlink" title="搜索功能"></a>搜索功能</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-searchdb --save</span><br><span class="line">$ npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><p>站点配置文件<code>_config.yml</code>中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure></p><p>修改主题配置文件<code>/themes/xxx/_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure></p><h2 id="带过滤功能的首页插件"><a href="#带过滤功能的首页插件" class="headerlink" title="带过滤功能的首页插件"></a>带过滤功能的首页插件</h2><p>在首页只显示指定分类下面的文章列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-index2 --save</span><br><span class="line">$ npm uninstall hexo-generator-index --save</span><br></pre></td></tr></table></figure></p><p>修改<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">index_generator:</span><br><span class="line">  per_page: 10</span><br><span class="line">  order_by: -date</span><br><span class="line">  include:</span><br><span class="line">    - category Web  # 只包含Web分类下的文章</span><br><span class="line">  exclude:</span><br><span class="line">    - tag Hexo      # 不包含标签为Hexo的文章</span><br></pre></td></tr></table></figure></p><h2 id="数学公式支持"><a href="#数学公式支持" class="headerlink" title="数学公式支持"></a>数学公式支持</h2><p><code>hexo</code>默认的渲染引擎是<code>marked</code>，但是<code>marked</code>不支持<code>mathjax</code>。<code>kramed</code>是在<code>marked</code>的基础上进行修改。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-math --save              # 停止使用 hexo-math</span><br><span class="line">$ npm install hexo-renderer-mathjax --save    # 安装hexo-renderer-mathjax包：</span><br><span class="line">$ npm uninstall hexo-renderer-marked --save   # 卸载原来的渲染引擎</span><br><span class="line">$ npm install hexo-renderer-kramed --save     # 安装新的渲染引擎</span><br></pre></td></tr></table></figure></p><p>修改<code>/node_modules/kramed/lib/rules/inline.js</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">11| escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">...</span><br><span class="line">20| em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">11| escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">...</span><br><span class="line">20| em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><p>修改<code>/node_modules/hexo-renderer-kramed/lib/renderer.js</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">64| // Change inline math rule</span><br><span class="line">65| function formatText(text) &#123;</span><br><span class="line">66|   // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">67|   return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">68| &#125;</span><br><span class="line"></span><br><span class="line">-&gt;</span><br><span class="line"></span><br><span class="line">64| // Change inline math rule</span><br><span class="line">65| function formatText(text) &#123;</span><br><span class="line">66|   // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">67|   // return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">68|   return text;</span><br><span class="line">69| &#125;</span><br></pre></td></tr></table></figure></p><p>在主题中开启<code>mathjax</code>开关，例如<code>next</code>主题中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br></pre></td></tr></table></figure></p><p>在文章中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: title.md</span><br><span class="line">date: 2019-01-04 12:47:37</span><br><span class="line">categories:</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">top:</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>测试</p><script type="math/tex; mode=display">A = \left[\begin{matrix}    a_{11} & a_{12} \\    a_{21} & a_{22}\end{matrix}\right]</script><h2 id="背景图片更换"><a href="#背景图片更换" class="headerlink" title="背景图片更换"></a>背景图片更换</h2><p>在主题配置文件夹中，如<code>next</code>主题，打开文件<code>hexo-theme-next/source/css/_custom/custom.styl</code>，修改为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// Custom styles.</span><br><span class="line"></span><br><span class="line">// 添加背景图片</span><br><span class="line">body &#123;</span><br><span class="line">  background: url(/images/background.jpg);</span><br><span class="line">  background-size: cover;</span><br><span class="line">  background-repeat: no-repeat;</span><br><span class="line">  background-attachment: fixed;</span><br><span class="line">  background-position: 50% 50%;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 修改主体透明度</span><br><span class="line">.main-inner &#123;</span><br><span class="line">  background: #fff;</span><br><span class="line">  opacity: 0.95;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 修改菜单栏透明度</span><br><span class="line">.header-inner &#123;</span><br><span class="line">  opacity: 0.95;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="背景音乐"><a href="#背景音乐" class="headerlink" title="背景音乐"></a>背景音乐</h2><p>首先生成外链</p><p><img src="/2019/01/04/Github-Hexo博客搭建/bgm1.jpg" alt="bgm1"></p><p><img src="/2019/01/04/Github-Hexo博客搭建/bgm2.jpg" alt="bgm2"></p><p>添加到合适位置，如<code>Links</code>一栏后</p><p><img src="/2019/01/04/Github-Hexo博客搭建/bgm3.jpg" alt="bgm3"></p><h2 id="鼠标特效"><a href="#鼠标特效" class="headerlink" title="鼠标特效"></a>鼠标特效</h2><ol><li><p><a href="https://github.com/hustcc/canvas-nest.js" target="_blank" rel="noopener">hustcc/canvas-nest.js</a></p></li><li><p>点击文本特效<br>新建<code>hexo-theme-next/source/js/click_show_text.js</code></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a_idx = <span class="number">0</span>;</span><br><span class="line">jQuery(<span class="built_in">document</span>).ready(<span class="function"><span class="keyword">function</span>(<span class="params">$</span>) </span>&#123;</span><br><span class="line">    $(<span class="string">"body"</span>).click(<span class="function"><span class="keyword">function</span>(<span class="params">e</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> a = <span class="keyword">new</span> <span class="built_in">Array</span></span><br><span class="line">        (<span class="string">"for"</span>, <span class="string">"while"</span>, <span class="string">"catch"</span>, <span class="string">"except"</span>, <span class="string">"if"</span>, <span class="string">"range"</span>, </span><br><span class="line">        <span class="string">"class"</span>, <span class="string">"min"</span>, <span class="string">"max"</span>, <span class="string">"sort"</span>, <span class="string">"map"</span>, <span class="string">"filter"</span>, </span><br><span class="line">        <span class="string">"lambda"</span>, <span class="string">"switch"</span>, <span class="string">"case"</span>, <span class="string">"iter"</span>, <span class="string">"next"</span>, <span class="string">"enum"</span>, <span class="string">"struct"</span>,  </span><br><span class="line">        <span class="string">"void"</span>, <span class="string">"int"</span>, <span class="string">"float"</span>, <span class="string">"double"</span>, <span class="string">"char"</span>, <span class="string">"signed"</span>, <span class="string">"unsigned"</span>);</span><br><span class="line">        <span class="keyword">var</span> $i = $(<span class="string">"&lt;span/&gt;"</span>).text(a[a_idx]);</span><br><span class="line">        a_idx = (a_idx + <span class="number">3</span>) % a.length;</span><br><span class="line">        <span class="keyword">var</span> x = e.pageX, </span><br><span class="line">        y = e.pageY;</span><br><span class="line">        $i.css(&#123;</span><br><span class="line">            <span class="string">"z-index"</span>: <span class="number">5</span>,</span><br><span class="line">            <span class="string">"top"</span>: y - <span class="number">20</span>,</span><br><span class="line">            <span class="string">"left"</span>: x,</span><br><span class="line">            <span class="string">"position"</span>: <span class="string">"absolute"</span>,</span><br><span class="line">            <span class="string">"font-weight"</span>: <span class="string">"bold"</span>,</span><br><span class="line">            <span class="string">"color"</span>: <span class="string">"#333333"</span></span><br><span class="line">        &#125;);</span><br><span class="line">        $(<span class="string">"body"</span>).append($i);</span><br><span class="line">        $i.animate(&#123;</span><br><span class="line">            <span class="string">"top"</span>: y - <span class="number">180</span>,</span><br><span class="line">            <span class="string">"opacity"</span>: <span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line"><span class="number">3000</span>,</span><br><span class="line"><span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    $i.remove();</span><br><span class="line">&#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">    setTimeout(<span class="string">'delay()'</span>, <span class="number">2000</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">delay</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    $(<span class="string">".buryit"</span>).removeAttr(<span class="string">"onclick"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>在文件<code>hexo-theme-next/layout/_layout.swig</code>中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">...</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/click_show_text.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p><h2 id="看板娘"><a href="#看板娘" class="headerlink" title="看板娘"></a>看板娘</h2><p><a href="https://github.com/xiazeyu/live2d-widget-models" target="_blank" rel="noopener">xiazeyu/live2d-widget-models</a>，预览效果见<a href="https://huaji8.top/post/live2d-plugin-2.0/" target="_blank" rel="noopener">作者博客</a>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-helper-live2d</span><br><span class="line">npm install live2d-widget-model-hijiki</span><br></pre></td></tr></table></figure><p>站点配置文件添加<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">live2d:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">scriptFrom:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">model:</span> </span><br><span class="line"><span class="attr">use:</span> <span class="string">live2d-widget-model-hijiki</span> <span class="comment">#模型选择</span></span><br><span class="line"><span class="attr">display:</span> </span><br><span class="line"><span class="attr">position:</span> <span class="string">right</span>  <span class="comment">#模型位置</span></span><br><span class="line"><span class="attr">width:</span> <span class="number">150</span>       <span class="comment">#模型宽度</span></span><br><span class="line"><span class="attr">height:</span> <span class="number">300</span>      <span class="comment">#模型高度</span></span><br><span class="line"><span class="attr">mobile:</span> </span><br><span class="line"><span class="attr">show:</span> <span class="literal">false</span>      <span class="comment">#是否在手机端显示</span></span><br></pre></td></tr></table></figure></p><h2 id="人体时钟"><a href="#人体时钟" class="headerlink" title="人体时钟"></a>人体时钟</h2><p>新建<code>hexo-theme-next/source/js/honehone_clock_tr.js</code></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/******************************************************************************</span></span><br><span class="line"><span class="comment">初期設定</span></span><br><span class="line"><span class="comment">******************************************************************************/</span></span><br><span class="line"><span class="keyword">var</span> swfUrl = <span class="string">"http://chabudai.sakura.ne.jp/blogparts/honehoneclock/honehone_clock_tr.swf"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> swfTitle = <span class="string">"honehoneclock"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 実行</span></span><br><span class="line">LoadBlogParts();</span><br><span class="line"></span><br><span class="line"><span class="comment">/******************************************************************************</span></span><br><span class="line"><span class="comment">入力なし</span></span><br><span class="line"><span class="comment">出力document.writeによるHTML出力</span></span><br><span class="line"><span class="comment">******************************************************************************/</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">LoadBlogParts</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line"><span class="keyword">var</span> sUrl = swfUrl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> sHtml = <span class="string">""</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;object classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://fpdownload.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=8,0,0,0" width="160" height="70" id="'</span> + swfTitle + <span class="string">'" align="middle"&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="allowScriptAccess" value="always" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="movie" value="'</span> + sUrl + <span class="string">'" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="quality" value="high" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="bgcolor" value="#ffffff" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;param name="wmode" value="transparent" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;embed wmode="transparent" src="'</span> + sUrl + <span class="string">'" quality="high" bgcolor="#ffffff" width="160" height="70" name="'</span> + swfTitle + <span class="string">'" align="middle" allowScriptAccess="always" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer" /&gt;'</span>;</span><br><span class="line">sHtml += <span class="string">'&lt;/object&gt;'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">document</span>.write(sHtml);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script charset=&quot;Shift_JIS&quot; src=&quot;/js/honehone_clock_tr.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><h2 id="代码雨"><a href="#代码雨" class="headerlink" title="代码雨"></a>代码雨</h2><p>新建<code>hexo-theme-next/source/js/digital_rain.js</code><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.onload = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="comment">//获取画布对象</span></span><br><span class="line">    <span class="keyword">var</span> canvas = <span class="built_in">document</span>.getElementById(<span class="string">"canvas"</span>);</span><br><span class="line">    <span class="comment">//获取画布的上下文</span></span><br><span class="line">    <span class="keyword">var</span> context =canvas.getContext(<span class="string">"2d"</span>);</span><br><span class="line">    <span class="keyword">var</span> s = <span class="built_in">window</span>.screen;</span><br><span class="line">    <span class="keyword">var</span> W = canvas.width = s.width;</span><br><span class="line">    <span class="keyword">var</span> H = canvas.height;</span><br><span class="line">    <span class="comment">//获取浏览器屏幕的宽度和高度</span></span><br><span class="line">    <span class="comment">//var W = window.innerWidth;</span></span><br><span class="line">    <span class="comment">//var H = window.innerHeight;</span></span><br><span class="line">    <span class="comment">//设置canvas的宽度和高度</span></span><br><span class="line">    canvas.width = W;</span><br><span class="line">    canvas.height = H;</span><br><span class="line">    <span class="comment">//每个文字的字体大小</span></span><br><span class="line">    <span class="keyword">var</span> fontSize = <span class="number">12</span>;</span><br><span class="line">    <span class="comment">//计算列</span></span><br><span class="line">    <span class="keyword">var</span> colunms = <span class="built_in">Math</span>.floor(W /fontSize);</span><br><span class="line">    <span class="comment">//记录每列文字的y轴坐标</span></span><br><span class="line">    <span class="keyword">var</span> drops = [];</span><br><span class="line">    <span class="comment">//给每一个文字初始化一个起始点的位置</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>;i&lt;colunms;i++)&#123;</span><br><span class="line">        drops.push(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//运动的文字</span></span><br><span class="line">    <span class="keyword">var</span> str =<span class="string">"WELCOME TO WWW.ITRHX.COM"</span>;</span><br><span class="line">    <span class="comment">//4:fillText(str,x,y);原理就是去更改y的坐标位置</span></span><br><span class="line">    <span class="comment">//绘画的函数</span></span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">draw</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        context.fillStyle = <span class="string">"rgba(238,238,238,.08)"</span>;<span class="comment">//遮盖层</span></span><br><span class="line">        context.fillRect(<span class="number">0</span>,<span class="number">0</span>,W,H);</span><br><span class="line">        <span class="comment">//给字体设置样式</span></span><br><span class="line">        context.font = <span class="string">"600 "</span>+fontSize+<span class="string">"px  Georgia"</span>;</span><br><span class="line">        <span class="comment">//给字体添加颜色</span></span><br><span class="line">        context.fillStyle = [<span class="string">"#33B5E5"</span>, <span class="string">"#0099CC"</span>, <span class="string">"#AA66CC"</span>, <span class="string">"#9933CC"</span>, <span class="string">"#99CC00"</span>, <span class="string">"#669900"</span>, <span class="string">"#FFBB33"</span>, <span class="string">"#FF8800"</span>, <span class="string">"#FF4444"</span>, <span class="string">"#CC0000"</span>][<span class="built_in">parseInt</span>(<span class="built_in">Math</span>.random() * <span class="number">10</span>)];<span class="comment">//randColor();可以rgb,hsl, 标准色，十六进制颜色</span></span><br><span class="line">        <span class="comment">//写入画布中</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>;i&lt;colunms;i++)&#123;</span><br><span class="line">            <span class="keyword">var</span> index = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * str.length);</span><br><span class="line">            <span class="keyword">var</span> x = i*fontSize;</span><br><span class="line">            <span class="keyword">var</span> y = drops[i] *fontSize;</span><br><span class="line">            context.fillText(str[index],x,y);</span><br><span class="line">            <span class="comment">//如果要改变时间，肯定就是改变每次他的起点</span></span><br><span class="line">            <span class="keyword">if</span>(y &gt;= canvas.height &amp;&amp; <span class="built_in">Math</span>.random() &gt; <span class="number">0.99</span>)&#123;</span><br><span class="line">                drops[i] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            drops[i]++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">randColor</span>(<span class="params"></span>)</span>&#123;<span class="comment">//随机颜色</span></span><br><span class="line">        <span class="keyword">var</span> r = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">256</span>);</span><br><span class="line">        <span class="keyword">var</span> g = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">256</span>);</span><br><span class="line">        <span class="keyword">var</span> b = <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">256</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"rgb("</span>+r+<span class="string">","</span>+g+<span class="string">","</span>+b+<span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    draw();</span><br><span class="line">    setInterval(draw,<span class="number">35</span>);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><code>hexo-theme-next/source/css/main.styl</code>添加<br><figure class="highlight styl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">canvas</span> &#123;</span><br><span class="line">  <span class="attribute">position</span>: fixed;</span><br><span class="line">  <span class="attribute">right</span>: <span class="number">0px</span>;</span><br><span class="line">  <span class="attribute">bottom</span>: <span class="number">0px</span>;</span><br><span class="line">  <span class="attribute">min-width</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">min-height</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">height</span>: auto;</span><br><span class="line">  <span class="attribute">width</span>: auto;</span><br><span class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><code>hexo-theme-next/layout/_layout.swig</code>添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;canvas id=&quot;canvas&quot; width=&quot;1440&quot; height=&quot;900&quot; &gt;&lt;/canvas&gt;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/DigitalRain.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h2 id="留言板"><a href="#留言板" class="headerlink" title="留言板"></a>留言板</h2><p>用<a href="https://www.livere.com/" target="_blank" rel="noopener">来比力</a>作为后台系统。</p><p>打开主题配置文件<code>hexo-theme-next/_config.yml</code>，修改<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Support for LiveRe comments system.</span></span><br><span class="line"><span class="comment"># You can get your uid from https://livere.com/insight/myCode (General web site)</span></span><br><span class="line"><span class="attr">livere_uid:</span> <span class="string">your</span> <span class="string">uid</span></span><br></pre></td></tr></table></figure></p><p>在<code>hexo-theme-next/layout/_scripts/third-party/comments/</code> 目录中添加<code>livere.swig</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id and not theme.gentie_productKey %&#125;</span><br><span class="line"></span><br><span class="line">  &#123;% if theme.livere_uid %&#125;</span><br><span class="line">    &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">    (function(d, s) &#123;</span><br><span class="line">        var j, e = d.getElementsByTagName(s)[0];</span><br><span class="line"></span><br><span class="line">        if (typeof LivereTower === &apos;function&apos;) &#123; return; &#125;</span><br><span class="line"></span><br><span class="line">        j = d.createElement(s);</span><br><span class="line">        j.src = &apos;https://cdn-city.livere.com/js/embed.dist.js&apos;;</span><br><span class="line">        j.async = true;</span><br><span class="line"></span><br><span class="line">        e.parentNode.insertBefore(j, e);</span><br><span class="line">    &#125;)(document, &apos;script&apos;);</span><br><span class="line">    &lt;/script&gt;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></p><p>在<code>hexo-theme-next/layout/_scripts/third-party/comments.swig</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include &apos;./comments/livere.swig&apos; %&#125;</span><br></pre></td></tr></table></figure></p><p><strong>评论无法保留？？？换成<code>Gitment</code>。</strong></p><p>安装模块<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i --save gitment</span><br></pre></td></tr></table></figure></p><p>在<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">New OAuth App</a>为博客应用一个密钥<br><img src="/2019/01/04/Github-Hexo博客搭建/new_oauth_app.png" alt="new_oauth_app"></p><p>定位到主题配置文件，填写<code>`enable</code>，<code>github_user</code>，<code>github_repo</code>，<code>client_id</code>，<code>client_secret</code><br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gitment</span></span><br><span class="line"><span class="comment"># Introduction: https://imsun.net/posts/gitment-introduction/</span></span><br><span class="line"><span class="attr">gitment:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  mint:</span> <span class="literal">true</span> <span class="comment"># RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway</span></span><br><span class="line"><span class="attr">  count:</span> <span class="literal">true</span> <span class="comment"># Show comments count in post meta area</span></span><br><span class="line"><span class="attr">  lazy:</span> <span class="literal">false</span> <span class="comment"># Comments lazy loading with a button</span></span><br><span class="line"><span class="attr">  cleanly:</span> <span class="literal">false</span> <span class="comment"># Hide 'Powered by ...' on footer, and more</span></span><br><span class="line"><span class="attr">  language:</span> <span class="comment"># Force language, or auto switch by theme</span></span><br><span class="line"><span class="attr">  github_user:</span> <span class="comment"># MUST HAVE, Your Github Username</span></span><br><span class="line"><span class="attr">  github_repo:</span> <span class="comment"># MUST HAVE, The name of the repo you use to store Gitment comments</span></span><br><span class="line"><span class="attr">  client_id:</span> <span class="comment"># MUST HAVE, Github client id for the Gitment</span></span><br><span class="line"><span class="attr">  client_secret:</span> <span class="comment"># EITHER this or proxy_gateway, Github access secret token for the Gitment</span></span><br><span class="line"><span class="attr">  proxy_gateway:</span> <span class="comment"># Address of api proxy, See: https://github.com/aimingoo/intersect</span></span><br><span class="line"><span class="attr">  redirect_protocol:</span> <span class="comment"># Protocol of redirect_uri with force_redirect_protocol when mint enabled</span></span><br></pre></td></tr></table></figure></p><p>如果遇到登陆不上的问题，转到<a href="https://gh-oauth.imsun.net/" target="_blank" rel="noopener">gh-oauth.imsun.net</a>页面，点高级-&gt;继续访问就可以了。</p><p><strong>服务器问题不能解决，换成<code>Gitalk</code>。</strong></p><p>定位到路径 themes/next/layout/_third-party/comments下面，创建一个叫做 gitalk.swig的文件，写入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125;</span><br><span class="line">  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt;</span><br><span class="line">  &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;script src=&quot;https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">   &lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">        var gitalk = new Gitalk(&#123;</span><br><span class="line">          clientID: &apos;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&apos;,</span><br><span class="line">          clientSecret: &apos;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&apos;,</span><br><span class="line">          repo: &apos;&#123;&#123; theme.gitalk.repo &#125;&#125;&apos;,</span><br><span class="line">          owner: &apos;&#123;&#123; theme.gitalk.githubID &#125;&#125;&apos;,</span><br><span class="line">          admin: [&apos;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&apos;],</span><br><span class="line">          id: md5(window.location.pathname),</span><br><span class="line">          distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos;</span><br><span class="line">        &#125;)</span><br><span class="line">        gitalk.render(&apos;gitalk-container&apos;)</span><br><span class="line">       &lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>在 上面的同级目录下的 index.swig 里面加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% include &apos;gitalk.swig&apos; %&#125;</span><br></pre></td></tr></table></figure><p>在使能化之前，我们还需要修改或者说是美化一下gitalk的默认样式，如果你不进行这一步也没有影响，可能结果会丑一点。<br>定位到： themes/next/source/css/_common/components/third-party. 然后你需要创建一个 gitalk.styl 文件。</p><p>这个文件里面写入：<br><figure class="highlight styl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.gt-header</span> <span class="selector-tag">a</span>, <span class="selector-class">.gt-comments</span> <span class="selector-tag">a</span>, <span class="selector-class">.gt-popup</span> a</span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line"><span class="selector-class">.gt-container</span> <span class="selector-class">.gt-popup</span> <span class="selector-class">.gt-action</span><span class="selector-class">.is--active</span>:before</span><br><span class="line">  <span class="attribute">top</span>: <span class="number">0.7em</span>;</span><br></pre></td></tr></table></figure></p><p>然后同样的，在 third-party.styl里面导入一下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@import "gitalk";</span><br></pre></td></tr></table></figure></p><p>在 layout/_partials/comments.swig 里面加入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% elseif theme.gitalk.enable %&#125;</span><br><span class="line">  &lt;div id=&quot;gitalk-container&quot;&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure></p><p>在主题配置文件<code>_config.yml</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">gitalk:</span><br><span class="line">  enable: true</span><br><span class="line">  githubID:   # MUST HAVE, Your Github Username    </span><br><span class="line">  repo:       # MUST HAVE, The name of the repo you use to store Gitment comments</span><br><span class="line">  ClientID:   # MUST HAVE, Github client id for the Gitment</span><br><span class="line">  ClientSecret: # EITHER this or proxy_gateway, Github access secret token for the Gitment</span><br><span class="line">  adminUser: isLouisHsu</span><br><span class="line">  distractionFreeMode: true</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><blockquote><p>基于hexo+github搭建一个独立博客 - 牧云云 - 博客园 <a href="https://www.cnblogs.com/MuYunyun/p/5927491.html" target="_blank" rel="noopener">https://www.cnblogs.com/MuYunyun/p/5927491.html</a><br>hexo+github pages轻松搭博客(1) | ex2tron’s Blog <a href="http://ex2tron.wang/hexo-blog-with-github-pages-1/" target="_blank" rel="noopener">http://ex2tron.wang/hexo-blog-with-github-pages-1/</a><br>hexo下LaTeX无法显示的解决方案 - crazy_scott的博客 - CSDN博客 <a href="https://blog.csdn.net/crazy_scott/article/details/79293576" target="_blank" rel="noopener">https://blog.csdn.net/crazy_scott/article/details/79293576</a><br>在Hexo中渲染MathJax数学公式 - 简书 <a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a><br>怎么去备份你的Hexo博客 - 简书 <a href="https://www.jianshu.com/p/baab04284923" target="_blank" rel="noopener">https://www.jianshu.com/p/baab04284923</a><br>Hexo中添加本地图片 - 蜕变C - 博客园 <a href="https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral" target="_blank" rel="noopener">https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referral</a><br>hexo 搜索功能 - 阿甘的博客 - CSDN博客 <a href="https://blog.csdn.net/ganzhilin520/article/details/79047983" target="_blank" rel="noopener">https://blog.csdn.net/ganzhilin520/article/details/79047983</a><br>为 Hexo 博客主题 NexT 添加 LiveRe 评论支持 <a href="https://blog.smoker.cc/web/add-comments-livere-for-hexo-theme-next.html" target="_blank" rel="noopener">https://blog.smoker.cc/web/add-comments-livere-for-hexo-theme-next.html</a><br>终于！！！记录如何在hexo next主题下配置gitalk评论系统 <a href="https://jinfagang.github.io/2018/10/07/%E7%BB%88%E4%BA%8E%EF%BC%81%EF%BC%81%EF%BC%81%E8%AE%B0%E5%BD%95%E5%A6%82%E4%BD%95%E5%9C%A8hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E9%85%8D%E7%BD%AEgitalk%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/" target="_blank" rel="noopener">https://jinfagang.github.io/2018/10/07/%E7%BB%88%E4%BA%8E%EF%BC%81%EF%BC%81%EF%BC%81%E8%AE%B0%E5%BD%95%E5%A6%82%E4%BD%95%E5%9C%A8hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E9%85%8D%E7%BD%AEgitalk%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>scikit-learn: 处理特征</title>
      <link href="/2018/11/24/scikit-learn-%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81/"/>
      <url>/2018/11/24/scikit-learn-%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<h1 id="数据预处理-preprocessing"><a href="#数据预处理-preprocessing" class="headerlink" title="数据预处理(preprocessing)"></a>数据预处理(preprocessing)</h1><h2 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import sklearn.preprocessing as preprocessing</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; dir(preprocessing)</span><br><span class="line">['Binarizer', 'CategoricalEncoder', 'FunctionTransformer', 'Imputer', 'KBinsDiscretizer', </span><br><span class="line">'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MaxAbsScaler', 'MinMaxScaler', </span><br><span class="line">'MultiLabelBinarizer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PolynomialFeatures', </span><br><span class="line">'PowerTransformer', 'QuantileTransformer', 'RobustScaler', 'StandardScaler', </span><br><span class="line">'__all__', '__builtins__', '__cached__', '__doc__', '__file__', </span><br><span class="line">'__loader__', '__name__', '__package__', '__path__', '__spec__', </span><br><span class="line">'_discretization', '_encoders', '_function_transformer', </span><br><span class="line">'add_dummy_feature', 'base', 'binarize', 'data', </span><br><span class="line">'imputation', 'label', 'label_binarize', 'maxabs_scale', </span><br><span class="line">'minmax_scale', 'normalize', 'power_transform', </span><br><span class="line">'quantile_transform', 'robust_scale', 'scale']</span><br></pre></td></tr></table></figure><h1 id="特征抽取-feature-extraction"><a href="#特征抽取-feature-extraction" class="headerlink" title="特征抽取(feature extraction)"></a>特征抽取(feature extraction)</h1><h2 id="Module-1"><a href="#Module-1" class="headerlink" title="Module"></a>Module</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import sklearn.feature_extraction as feature_extraction</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; dir(feature_extraction)</span><br><span class="line">['DictVectorizer', 'FeatureHasher', </span><br><span class="line">'__all__', '__builtins__', '__cached__', '__doc__', '__file__', </span><br><span class="line">'__loader__', '__name__', '__package__', '__path__', '__spec__', </span><br><span class="line">'_hashing', 'dict_vectorizer', 'grid_to_graph', 'hashing', </span><br><span class="line">'image', 'img_to_graph', 'stop_words', 'text']</span><br></pre></td></tr></table></figure><h1 id="特征选择-feature-selection"><a href="#特征选择-feature-selection" class="headerlink" title="特征选择(feature selection)"></a>特征选择(feature selection)</h1><p>当数据预处理完成后，我们需要选择有意义的特征，将其输入到模型中训练，主要从两个方面考虑</p><ul><li><strong>特征是否发散</strong><br>  若一个特征不发散，其方差接近$0$，则表示该特征在各个样本上没有差别，对于样本的区分没什么用；</li><li><strong>特征与目标的相关性</strong><br>  与目标<code>(target)</code>相关性高的特征，应当优先选择。</li></ul><p>特征选择的方法可以根据特征选择的形式分为$3$种</p><ul><li><strong>过滤法<code>(Filter)</code></strong><br>  按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li><li><strong>包装法<code>(Wrapper)</code></strong><br>  根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。</li><li><strong>嵌入法<code>(Embedded)</code></strong><br>  先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于<code>Filter</code>方法，但是是通过训练来确定特征的优劣。</li></ul><h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><h3 id="移除低方差"><a href="#移除低方差" class="headerlink" title="移除低方差"></a>移除低方差</h3><blockquote><p><code>Removing features with low variance</code></p></blockquote><p>即移除那些方差较小的特征，当特征的取值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。</p><p>现实中这种方法作用不大，可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> class sklearn.feature_selection.VarianceThreshold(threshold=0.0)</span><br></pre></td></tr></table></figure><p>调用例程如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = [</span><br><span class="line">            [0, 0, 1], </span><br><span class="line">            [0, 1, 0], </span><br><span class="line">            [1, 0, 0], </span><br><span class="line">            [0, 1, 1], </span><br><span class="line">            [0, 1, 0], </span><br><span class="line">            [0, 1, 1],</span><br><span class="line">    ]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; feature_selection.VarianceThreshold(threshold=(.8 * (1 - .8))).fit_transform(X) </span><br><span class="line">array([[0, 1],</span><br><span class="line">       [1, 0],</span><br><span class="line">       [0, 0],</span><br><span class="line">       [1, 1],</span><br><span class="line">       [1, 0],</span><br><span class="line">       [1, 1]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; # 选出了第2, 3列特征</span><br></pre></td></tr></table></figure></p><h3 id="单变量特征选择"><a href="#单变量特征选择" class="headerlink" title="单变量特征选择"></a>单变量特征选择</h3><blockquote><p><code>Univariate feature selection</code></p></blockquote><p>分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。</p><p>指标适用情况：</p><ol><li>对于分类问题(<code>target</code>离散)<br> 卡方检验，f_classif, mutual_info_classif，互信息</li><li>对于回归问题(<code>target</code>连续)</li></ol><blockquote><p>注：分类与回归在一定程度上可以互相转换，</p></blockquote><h2 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h2><h2 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h2><h2 id="Module-2"><a href="#Module-2" class="headerlink" title="Module"></a>Module</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import sklearn.feature_selection as feature_selection</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; dir(feature_selection)</span><br><span class="line">['GenericUnivariateSelect', 'RFE', 'RFECV', </span><br><span class="line">'SelectFdr', 'SelectFpr', 'SelectFromModel', 'SelectFwe', </span><br><span class="line">'SelectKBest', 'SelectPercentile', 'VarianceThreshold', </span><br><span class="line">'__all__', '__builtins__', '__cached__', '__doc__', '__file__', </span><br><span class="line">'__loader__', '__name__', '__package__', '__path__', '__spec__', </span><br><span class="line">'base', 'chi2', 'f_classif', 'f_oneway', </span><br><span class="line">'f_regression', 'from_model', 'mutual_info_', 'mutual_info_classif', </span><br><span class="line">'mutual_info_regression', 'rfe', 'univariate_selection', 'variance_threshold']</span><br></pre></td></tr></table></figure><h1 id="参考博客-reference"><a href="#参考博客-reference" class="headerlink" title="参考博客(reference)"></a>参考博客(reference)</h1><blockquote><p>使用sklearn优雅地进行数据挖掘 - jasonfreak - 博客园 <a href="http://www.cnblogs.com/jasonfreak/p/5448462.html" target="_blank" rel="noopener">http://www.cnblogs.com/jasonfreak/p/5448462.html</a><br>特征选择 (feature_selection) - 会飞的蝸牛 - 博客园 <a href="https://www.cnblogs.com/stevenlk/p/6543628.html" target="_blank" rel="noopener">https://www.cnblogs.com/stevenlk/p/6543628.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Metrics</title>
      <link href="/2018/11/21/Metrics/"/>
      <url>/2018/11/21/Metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="回归-regression-评估指标"><a href="#回归-regression-评估指标" class="headerlink" title="回归(regression)评估指标"></a>回归(regression)评估指标</h1><h2 id="解释方差-Explained-Variance"><a href="#解释方差-Explained-Variance" class="headerlink" title="解释方差(Explained Variance)"></a>解释方差(Explained Variance)</h2><script type="math/tex; mode=display">EV(\hat{y}, y)= 1 - \frac{Var(y-\hat{y})}{Var(y)}</script><p>解释方差越接近$1$表示回归效果越好。</p><h2 id="平均绝对误差-Mean-Absolute-Error-MAE"><a href="#平均绝对误差-Mean-Absolute-Error-MAE" class="headerlink" title="平均绝对误差(Mean Absolute Error - MAE)"></a>平均绝对误差(Mean Absolute Error - MAE)</h2><script type="math/tex; mode=display">MAE(\hat{y}, y) = E(||\hat{y} - y||_1)= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} |\hat{y}^{(i)} - y^{(i)}|</script><p>$MAE$越小表示回归效果越好。</p><h2 id="平均平方误差-Mean-Squared-Error-MSE"><a href="#平均平方误差-Mean-Squared-Error-MSE" class="headerlink" title="平均平方误差(Mean Squared Error - MSE)"></a>平均平方误差(Mean Squared Error - MSE)</h2><p>在<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">线性回归</a>一节，使用的损失函数即$MSE$</p><script type="math/tex; mode=display">MSE(\hat{y}, y) = E(||\hat{y} - y||_2^2)= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2</script><p>其中$y$与$\hat{y}$均为$1$维向量，$MSE$越小表示回归效果越好。</p><p>其含义比较直观，即偏差的平方和。也可以从最小化方差的角度解释，定义误差向量</p><script type="math/tex; mode=display">e = \hat{y} - y</script><p>我们假定其期望为$0$，即</p><script type="math/tex; mode=display">E(e) = 0　或　\overline{e} = 0</script><p>那么误差的方差为</p><script type="math/tex; mode=display">Var(e) = E[(e - \overline{e})^T (e - \overline{e})] = E(||e||_2^2)</script><p>也即$MSE$。</p><h2 id="均方根误差-Root-Mean-Squared-Error-RMSE"><a href="#均方根误差-Root-Mean-Squared-Error-RMSE" class="headerlink" title="均方根误差(Root Mean Squared Error - RMSE)"></a>均方根误差(Root Mean Squared Error - RMSE)</h2><script type="math/tex; mode=display">RMSE(\hat{y}, y) = \sqrt{\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2}</script><p>实质与$MSE$是一样的。只不过用于数据更好的描述，使计算得损失的值较小。$RMSE$越小表示回归效果越好。</p><h2 id="均方对数误差-Mean-Squard-Logarithmic-Error-MSLE"><a href="#均方对数误差-Mean-Squard-Logarithmic-Error-MSLE" class="headerlink" title="均方对数误差(Mean Squard Logarithmic Error - MSLE)"></a>均方对数误差(Mean Squard Logarithmic Error - MSLE)</h2><script type="math/tex; mode=display">MSLE(\hat{y}, y) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} \left[\log (1+y^{(i)}) - \log (1+\hat{y}^{(i)})\right]^2</script><p>通常用于输出指数增长的模型，如，人口统计，商品的平均销售量，以及一段时间内的平均销售量等。注意，由对数性质，这一指标对过小的预测的惩罚大于预测过大的预测的惩罚。</p><h2 id="中值绝对误差-Median-Absolute-Error-MedAE"><a href="#中值绝对误差-Median-Absolute-Error-MedAE" class="headerlink" title="中值绝对误差(Median Absolute Error - MedAE)"></a>中值绝对误差(Median Absolute Error - MedAE)</h2><script type="math/tex; mode=display">MedAE(\hat{y}, y) = median(|y - \hat{y}|)</script><h2 id="R决定系数-R2"><a href="#R决定系数-R2" class="headerlink" title="R决定系数(R2)"></a>R决定系数(R2)</h2><p>又称拟合优度，提供了一个衡量未来样本有多好的预测模型。最佳可能的分数是$1.0$，它可以是负的(因为模型可以任意恶化)。一个常数模型总是预测$y$的期望值，而不考虑输入特性，则得到$R^2$分数为$0.0$。</p><script type="math/tex; mode=display">R^2(\hat{y}, y) = 1 - \frac{\sum_{i=1}^{n_{samples}} (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{n_{samples}} (y^{(i)} - \overline{y})^2}</script><p>其中</p><script type="math/tex; mode=display">\overline{y} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} y^{(i)}</script><h1 id="分类-classification-评估指标"><a href="#分类-classification-评估指标" class="headerlink" title="分类(classification)评估指标"></a>分类(classification)评估指标</h1><p>先作如下定义<br><img src="/2018/11/21/Metrics/terminology_and_derivations_1.png" alt="terminology_and_derivations_1"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_2.png" alt="terminology_and_derivations_2"><br><img src="/2018/11/21/Metrics/terminology_and_derivations_3.png" alt="terminology_and_derivations_3"></p><p><img src="/2018/11/21/Metrics/metrics_classification2.png" alt="metrics_classification2"></p><h2 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率(Accuracy)"></a>准确率(Accuracy)</h2><script type="math/tex; mode=display">Accuracy(y, \hat{y})= \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} 1(y^{(i)}=\hat{y}^{(i)})</script><p>也即</p><script type="math/tex; mode=display">Accuracy= \frac{TN+TP}{TN+TP+FN+FP}</script><p>精度只是简单地计算出比例，但是没有对不同类别进行区分。因为不同类别错误代价可能不同。例如：判断这个病人是不是病危，如果不是病危错误判断为病危，那只是损失一点医务人员的时间和精力，如果是把病危的人判断为非病危状态，那损失的就是一条人命。他们之间存在重要性差异，这时候就不能用精度。对于样本不均衡的情况，也不是用精度来衡量。例如：有A类1000个，B类5个，如果我把这1005个样本都预测成A类，正确率=1000/1005=99.5%。</p><h2 id="精确率-Precision-与召回率-Recall"><a href="#精确率-Precision-与召回率-Recall" class="headerlink" title="精确率(Precision)与召回率(Recall)"></a>精确率(Precision)与召回率(Recall)</h2><ul><li><p>精确率<code>(Precision)</code><br>  即预测正样本中，实际为正样本的百分比，度量了分类器不会将真正的负样本错误地分为正样本的能力。</p><script type="math/tex; mode=display">  Precision = \frac{TP}{TP+FP}</script></li><li><p>召回率<code>(Recall)</code><br>  又称查全率，即实际正样本中，被预测为正样本的百分比，度量了分类器找到所有正样本的能力。</p><script type="math/tex; mode=display">  Recall = \frac{TP}{TP + FN}</script><p><img src="/2018/11/21/Metrics/precision_recall.png" alt="precision_recall"></p></li></ul><h2 id="F度量"><a href="#F度量" class="headerlink" title="F度量"></a>F度量</h2><blockquote><p><a href="https://en.wikipedia.org/wiki/F1_score" target="_blank" rel="noopener">F1 score - Wikipedia</a></p></blockquote><ul><li><p>$F_1$<br>  为精确率<code>(Precision)</code>与召回率<code>(Recall)</code>的调和均值<code>(harmonic mean)</code>。</p><script type="math/tex; mode=display">  \frac{1}{F_1}   = \frac{1}{2} (\frac{1}{Precision} + \frac{1}{Recall})</script><p>  也即</p><script type="math/tex; mode=display">  F_1 = 2 · \frac{Precision·Recall}{Precision + Recall}</script></li><li><p>$F_{\beta}$<br>  在$F_1$度量的基础上增加权值$\beta$，$\beta$越大，$Recall$的权重越大，否则$Precision$的权重越大。</p><script type="math/tex; mode=display">  \frac{1}{F_{\beta}} = \frac{1}{1+\beta^2} \frac{1}{Precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{Recall}</script><p>  也即</p><script type="math/tex; mode=display">  F_{\beta} = (1+\beta^2)·\frac{Precision·Recall}{(\beta^2·Precision) + Recall}</script></li></ul><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><code>Confusion matrix</code>，也被称作错误矩阵<code>(Error matrix)</code>，是一个特别的表。无监督学习中，通常称作匹配矩阵<code>(Matching matrix)</code>。每一列表达了分类器对样本的类别预测，每一行表达了样本所属的真实类别。</p><p>例如我们有$27$个待分类样本，将其划分为<code>Cat</code>，<code>Dog</code>，<code>Rabbit</code>，讲实际标签与预测标签数目统计后填入混淆矩阵。</p><p><img src="/2018/11/21/Metrics/confusion_matrix.png" alt="confusion_matrix"></p><p>例如实际上有$8$个样本为<code>Cat</code>，而该分类器将其中$3$个划分为<code>Dog</code>，将$2$个为<code>Dog</code>的样本划分为<code>Cat</code>。我们可以根据上述混淆矩阵得出结论，该分类器对<code>Dog</code>和<code>Cat</code>分类能力较弱，而对<code>Rabbit</code>分类能力较强。而且正确预测的样本数目都在对角线上，很容易直观地检查表中的预测错误。</p><p>以下为<code>scikit-learn</code>中混淆矩阵的<code>API</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.metrics import confusion_matrix</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]</span><br><span class="line">&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred)</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]</span><br><span class="line">&gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]</span><br><span class="line">&gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])</span><br><span class="line">array([[2, 0, 0],</span><br><span class="line">       [0, 0, 1],</span><br><span class="line">       [1, 0, 2]])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; # In the binary case, we can extract true positives, etc as follows:</span><br><span class="line">&gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()</span><br><span class="line">&gt;&gt;&gt; (tn, fp, fn, tp)</span><br><span class="line">(0, 2, 1, 1)</span><br></pre></td></tr></table></figure></p><h2 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p><code>Receiver Operating Characteristic</code>，是根据一系列不同的二分类方式(分界值或决定阈)，以召回率(真正率<code>TPR</code>、灵敏度)为纵坐标，<code>fall-out</code>(假正率<code>FPR</code>、$1$-特异度)为横坐标绘制的曲线。</p><ul><li><p><code>true positive rate - TPR</code><br>  所有阳性样本中有多少正确的阳性结果。</p><script type="math/tex; mode=display">  TPR = \frac{TP}{P} = \frac{TP}{TP + FN}</script></li><li><p><code>false positive rate - FPR</code><br>  所有阴性样本中有多少不正确的阳性结果。</p><script type="math/tex; mode=display">  FPR = \frac{FP}{N} = \frac{FP}{FP + TN}</script></li></ul><h3 id="ROC-space"><a href="#ROC-space" class="headerlink" title="ROC space"></a>ROC space</h3><p><img src="/2018/11/21/Metrics/模型的评估指标/ROC_space.png" alt="ROC_space"></p><ul><li>分别以<code>FPR</code>与<code>TPR</code>作为横纵轴(又称灵敏度-$1$特异度曲线<code>sensitivity vs (1 − specificity) plot</code>)；</li><li>每次预测结果或混淆矩阵的实例代表了<code>ROC</code>空间中的一个点；<br>  例如上图中$A, B, C, C’$是以下表数据计算得到的点。<br>  <img src="/2018/11/21/Metrics/ROC_space_samples.png" alt="ROC_space_samples"></li><li>在<code>ROC</code>空间中最左上方的点$(0, 1)$称作完美分类器<code>(perfect classification)</code>；</li><li>随机分类器的结果分布在<code>ROC space</code>对角线$(0, 0)-(1, 1)$上，当实验次数足够多，其分区趋向$(0.5, 0.5)$;</li><li>对角线以上的点代表好的分类结果(比随机的好)；线下的点代表坏的结果(比随机的差)；</li><li>注意，持续不良分类器的输出可以简单地反转以获得一个好的分类器，反转后的分类器与原分类器在平面上关于对角线对称，例如点$C’$。</li></ul><h3 id="ROC曲线的绘制"><a href="#ROC曲线的绘制" class="headerlink" title="ROC曲线的绘制"></a>ROC曲线的绘制</h3><p>若训练集样本中，正样本与负样本以正态分布的形式分布在样本平面上，如下图，左峰为负样本，右峰为正样本，存在部分重叠(不然就不用搞这么多分类算法了)。</p><p><img src="/2018/11/21/Metrics/ROC_curves.svg.png" alt="ROC_curves.svg"></p><p>若假设正样本概率密度为$f_1(x)$，负样本的概率密度为$f_0(x)$，给定阈值$T$，则右</p><script type="math/tex; mode=display">TPR(T) = \int_T^{\infty} f_1(x) dx</script><script type="math/tex; mode=display">FPR(T) = \int_T^{\infty} f_0(x) dx</script><p>选取不同的阈值划分分类器输出，就能得到<code>ROC</code>曲线。</p><p>在基于有限样本作<code>ROC</code>图时，可以看到曲线每次都是一个“爬坡”，遇到正例往上爬一格$(1/m+)$，错了往右爬一格$(1/m-)$，显然往上爬对于算法性能来说是最好的。<br><img src="/2018/11/21/Metrics/ROC_curves_up_right.png" alt="ROC_curves_up_right"></p><h3 id="Area-Under-the-Curve-AUC"><a href="#Area-Under-the-Curve-AUC" class="headerlink" title="Area Under the Curve - AUC"></a>Area Under the Curve - AUC</h3><p><code>ROC</code>曲线下的面积<code>AUC</code>物理意义为，任取一对正负样本，正样本的预测值大于负样本的预测值的概率。</p><script type="math/tex; mode=display">A = \int_{-\infty}^{\infty} TPR(T) dFPR(T)</script><script type="math/tex; mode=display">= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}I(T'> T)f_1(T') f_0(T)dT' dT</script><script type="math/tex; mode=display">= P(X_1 > X_0)</script><p>同样的，在有限个样本下，其面积用累加的方法计算(梯形面积)</p><p><img src="/2018/11/21/Metrics/ROC_curves_AUC.png" alt="ROC_curves_AUC"></p><script type="math/tex; mode=display">AUC = \sum_{i=1}^{m-1} \frac{1}{2} (y_{i+1} + y_i)(x_{i+1} - x_i)</script><ul><li>$AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>$0.5 &lt; AUC &lt; 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li><li>$AUC = 0.5$，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li><li>$AUC &lt; 0.5$，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li></ul><h3 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h3><p>以下为<code>scikit-learn</code>中混淆矩阵的<code>ROC</code>曲线<code>API</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; y = np.array([1, 1, 2, 2])</span><br><span class="line">&gt;&gt;&gt; scores = np.array([0.1, 0.4, 0.35, 0.8])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)</span><br><span class="line">&gt;&gt;&gt; fpr</span><br><span class="line">array([ 0. ,  0.5,  0.5,  1. ])</span><br><span class="line">&gt;&gt;&gt; tpr</span><br><span class="line">array([ 0.5,  0.5,  1. ,  1. ])</span><br><span class="line">&gt;&gt;&gt; thresholds</span><br><span class="line">array([ 0.8 ,  0.4 ,  0.35,  0.1 ])</span><br><span class="line">&gt;&gt;&gt; </span><br><span class="line">&gt;&gt;&gt; metrics.auc(fpr, tpr)</span><br><span class="line">0.75</span><br></pre></td></tr></table></figure></p><h1 id="聚类-clustering-评估指标"><a href="#聚类-clustering-评估指标" class="headerlink" title="聚类(clustering)评估指标"></a>聚类(clustering)评估指标</h1><blockquote><ul><li><a href="https://blog.csdn.net/darkrabbit/article/details/80378597" target="_blank" rel="noopener">AI（005） - 笔记 - 聚类性能评估（Clustering Evaluation） - DarkRabbit的专栏 - CSDN博客 </a></li><li><a href="https://en.wikipedia.org/wiki/Main_Page" target="_blank" rel="noopener">Wikipedia, the free encyclopedia</a></li></ul></blockquote><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>聚类性能比较好，就是聚类结果簇内相似度<code>(intra-cluster similarity)</code>高，而簇间相似度<code>(inter-cluster similarity)</code>低，即同一簇的样本尽可能的相似，不同簇的样本尽可能不同。</p><p>聚类性能的评估（度量）分为两大类：</p><ul><li>外部评估<code>(external evaluation)</code>：将结果与某个参考模型<code>(reference model)</code>进行比较；</li><li>内部评估<code>(internal evaluation)</code>：直接考虑聚类结果而不利用任何参考模型。</li></ul><p>将$n_{samples}$个样本$\{x^{(1)}, …, x^{(n_{samples})}\}$用待评估聚类算法划分为$K$个类$\{X_1, …, X_K\}$，假定参考模型将其划分为$L$类$\{Y_1, …, Y_L\}$，将样本两辆匹配</p><script type="math/tex; mode=display">\begin{cases}    a = |SS| &  SS = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)}, x^{(j)} \in Y_l\} \\    b = |SD| &  SD = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \\    c = |DS| &  DS = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)}, x^{(j)} \in Y_l\} \\    d = |DD| &  DD = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\}\end{cases}</script><p>其中$k = 1, …, K; l = 1, …, L$</p><script type="math/tex; mode=display">a + b + c + d=   \left(        \begin{matrix}            n \\ 2        \end{matrix}    \right)= \frac{n(n-1)}{2}</script><blockquote><ul><li>$SS$包含两种划分中均属于同一类的样本对；</li><li>$SD$包含用待评估聚类算法划分中属于同一类，而在参考模型中属于不同类的样本对；</li><li>$DS$包含用待评估聚类算法划分中属于不同类，而在参考模型中属于同一类的样本对；</li><li>$DD$包含两种划分中均不属于同一类的样本对。</li></ul></blockquote><h2 id="常用外部评估-external-evaluation"><a href="#常用外部评估-external-evaluation" class="headerlink" title="常用外部评估(external evaluation)"></a>常用外部评估(external evaluation)</h2><h3 id="Rand-Index-RI"><a href="#Rand-Index-RI" class="headerlink" title="Rand Index(RI)"></a>Rand Index(RI)</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/Rand_index" target="_blank" rel="noopener">Rand index - Wikipedia</a></p></blockquote><script type="math/tex; mode=display">RI = \frac{a+d}{a + b + c + d} = \frac{a+d}{\left(\begin{matrix} n \\ 2 \end{matrix}\right)}</script><p>显然，结果值在$[0,1]$之间，且值越大越好。</p><ul><li>当为$0$时，两个聚类无重叠；</li><li>当为$1$时，两个聚类完全重叠。</li></ul><h3 id="Adjust-Rand-Index-ARI"><a href="#Adjust-Rand-Index-ARI" class="headerlink" title="Adjust Rand Index(ARI)"></a>Adjust Rand Index(ARI)</h3><p>让$RI$有了修正机会<code>(corrected-for-chance)</code>，在取值上从$[0,1]$变成$[-1, 1]$</p><p>对于$X$与$Y$的重叠可以用一个列联表<code>(contingency table)</code>表示，记作$[n_{ij}]$，$n_{ij} = |X_i \bigcap Y_j|$<br><img src="/2018/11/21/Metrics/聚类/ARI.svg" alt="ARI"></p><p>则定义$ARI$如下<br><img src="/2018/11/21/Metrics/聚类/ARI_Def.svg" alt="ARI_Def"></p><h3 id="互信息与调整互信息-Adjusted-Mutual-Information-AMI"><a href="#互信息与调整互信息-Adjusted-Mutual-Information-AMI" class="headerlink" title="互信息与调整互信息(Adjusted Mutual Information - AMI)"></a>互信息与调整互信息(Adjusted Mutual Information - AMI)</h3><blockquote><p>关于互信息可查看<a href="">熵</a>一节说明。</p></blockquote><p>$X_i$类别的概率定义为</p><script type="math/tex; mode=display">P(k) = \frac{|X_k|}{N}</script><p>则划分结果的熵定义为</p><script type="math/tex; mode=display">H(X) = - \sum_k P(k) \log P(k)</script><p>类似的</p><script type="math/tex; mode=display">P'(l) = \frac{|Y_l|}{N}</script><script type="math/tex; mode=display">H(Y) = - \sum_j P'(l) \log P'(l)</script><p>另外</p><script type="math/tex; mode=display">P(k, l) = \frac{|X_k, Y_l|}{N}</script><p>那么两种划分的互信息定义为</p><script type="math/tex; mode=display">MI(X, Y) = \sum_{k, l} P(k, l) \log \frac{P(k, l)}{P(k) P'(l)}</script><p>和$ARI$一样，我们对它进行调整。</p><script type="math/tex; mode=display">E[MI(X, Y)] = \sum_k \sum_l \sum_{n_{kl} = \max\{1, a_k + b_l - N\}}^{\min \{a_k, b_l\}}\frac{n_{kl}}{N}\log \left( \frac{N·n_{kl}}{a_k b_l} \right) ×</script><script type="math/tex; mode=display">\frac{a_k!b_l!(N-a_k)!(N-b_l)!}{N!n_{kl}!(a_k-n_{kl})!(b_l-n_{kl})!(N-a_k-b_l+n_{kl})!}</script><p>最终$AMI$表达式为</p><script type="math/tex; mode=display">AMI(X, Y) = \frac{MI(X, Y) - E[MI(X, Y)]}{\max \{H(X), H(Y)\} - E[MI(X, Y)]}</script><h3 id="同质性-Homogeneity-与完整性-Completeness"><a href="#同质性-Homogeneity-与完整性-Completeness" class="headerlink" title="同质性(Homogeneity)与完整性(Completeness)"></a>同质性(Homogeneity)与完整性(Completeness)</h3><p>这两个类似分类种的的准确率<code>(accuracy)</code>与召回率<code>(recall)</code>。</p><ul><li><p>同质性<code>(Homogeneity)</code><br>  即一个簇仅包含一个类别的样本</p><script type="math/tex; mode=display">  H = 1 - \frac{H(X|Y)}{H(X)}</script><p>  其中$H(X|Y)$为条件熵</p><script type="math/tex; mode=display">  H(X|Y) = \sum_k \sum_l P(X_k, Y_l) \log \frac{P(Y_l)}{P(X_k, Y_l)}  = \sum_k \sum_l \frac{n_{kl}}{N} \log \frac{n_{kl}}{N}</script></li><li><p>完整性<code>(Completeness)</code><br>  同类别样本被归类到相同簇中</p><script type="math/tex; mode=display">  C = 1 - \frac{H(Y|X)}{H(Y)}</script></li><li><p>$V-measure$<br>  <code>Homogeneity</code>和<code>Completeness</code>的调和平均</p><script type="math/tex; mode=display">  V = \frac{1}{\frac{1}{2} \left(\frac{1}{H} + \frac{1}{C}\right)} = \frac{2HC}{H + C}</script></li></ul><h3 id="Fowlkes-Mallows-index-FMI"><a href="#Fowlkes-Mallows-index-FMI" class="headerlink" title="Fowlkes-Mallows index(FMI)"></a>Fowlkes-Mallows index(FMI)</h3><p>成对精度和召回率的几何均值</p><blockquote><p><a href="https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index" target="_blank" rel="noopener">Fowlkes–Mallows index - Wikipedia</a></p></blockquote><p>定义</p><ul><li>$TP$ as the number of points that are present in the same cluster in both $A_1$ and $A_2$.</li><li>$FP$ as the number of points that are present in the same cluster in $A_1$ but not in $A_2$.</li><li>$FN$ as the number of points that are present in the same cluster in $A_2$ but not in $A_1$.</li><li>$TN$ as the number of points that are in different clusters in both $A_1$ and $A_2$.</li></ul><p>则</p><script type="math/tex; mode=display">TP + FP + TN + FN = \frac{n(n-1)}{2}</script><p>定义</p><script type="math/tex; mode=display">FMI = \sqrt{\frac{TP}{TP + FP} · \frac{TP}{TP + FN}}</script><h3 id="杰卡德系数-Jaccard-Coefficient-JC"><a href="#杰卡德系数-Jaccard-Coefficient-JC" class="headerlink" title="杰卡德系数(Jaccard Coefficient - JC)"></a>杰卡德系数(Jaccard Coefficient - JC)</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="noopener">Jaccard index - Wikipedia</a></p></blockquote><p>给定两个具有$n$个元素的集合$A, B$，定义</p><ul><li>$M_{11}$ represents the total number of attributes where $A$ and $B$ both have a value of $1$.</li><li>$M_{01}$ represents the total number of attributes where the attribute of $A$ is $0$ and the attribute of $B$ is $1$.</li><li>$M_{10}$ represents the total number of attributes where the attribute of $A$ is $1$ and the attribute of $B$ is $0$.</li><li>$M_{00}$ represents the total number of attributes where $A$ and $B$ both have a value of $0$.</li></ul><p>则有</p><script type="math/tex; mode=display">M_{11} + M_{01} + M_{10} + M_{00} = n</script><ul><li><p><code>Jaccard</code>相似度系数</p><script type="math/tex; mode=display">  J = \frac{M_{11}}{M_{01} + M_{10} + M_{11}}</script><blockquote><p>也即$J=\frac{A \cap B}{A \cup B}$</p></blockquote></li><li><p><code>Jaccard</code>距离</p><script type="math/tex; mode=display">  D_J = 1 - J</script></li></ul><h2 id="常用内部评估-internal-evaluation"><a href="#常用内部评估-internal-evaluation" class="headerlink" title="常用内部评估(internal evaluation)"></a>常用内部评估(internal evaluation)</h2><h3 id="轮廓系数-Silhouette-coefficient"><a href="#轮廓系数-Silhouette-coefficient" class="headerlink" title="轮廓系数(Silhouette coefficient)"></a>轮廓系数(Silhouette coefficient)</h3><p>又称侧影法，适用于实际类别信息未知的情况，对其中一个样本点$x^{(i)}$，记</p><ul><li>$a(i)$：到本簇其他样本点的距离的平均值</li><li>$b(i)$：该点到其他各个簇的样本点的平均距离的最小值</li></ul><p>定义轮廓系数</p><script type="math/tex; mode=display">S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}</script><p>或者</p><script type="math/tex; mode=display">S(i) = \begin{cases}    1 - \frac{a(i)}{b(i)} & a(i) < b(i) \\    0 & a(i) = b(i) \\    \frac{b(i)}{a(i)} - 1 & a(i) > b(i)\end{cases}</script><p>其含义如下</p><ul><li>当$a(i) \ll b(i)$时，无限接近于$1$，则意味着聚类合适；</li><li>当$a(i) \gg b(i)$时，无限接近于$-1$，则意味着把样本i聚类到相邻簇中更合适；</li><li>当$a(i)\approxeq b(i)$时，无限接近于$0$，则意味着样本在两个簇交集处。</li></ul><p>一般再对各个点的轮廓系数求均值</p><script type="math/tex; mode=display">\overline{S} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} S(i)</script><ul><li>当$\overline{S} &gt; 0.5$，表示聚类合适；</li><li>当$\overline{S} &lt; 0.2$，表示表明数据不存在聚类特征</li></ul><h3 id="Calinski-Harabaz-CH"><a href="#Calinski-Harabaz-CH" class="headerlink" title="Calinski-Harabaz(CH)"></a>Calinski-Harabaz(CH)</h3><p>也适用于实际类别信息未知的情况，以$K$分类为例</p><ul><li><p>类内散度$W$</p><script type="math/tex; mode=display">  W(K) = \sum_k \sum_{C(j)=k} ||x_j - \overline{x_k}||^2</script></li><li><p>类间散度$B$</p><script type="math/tex; mode=display">  B(K) = \sum_k a_k ||\overline{x_k} - \overline{x}||^2</script></li><li><p>$CH$</p><script type="math/tex; mode=display">  CH(K) = \frac{B(K)(N-K)}{W(K)(K-1)}</script></li></ul><h3 id="Davies-Bouldin-Index-DBI"><a href="#Davies-Bouldin-Index-DBI" class="headerlink" title="Davies-Bouldin Index(DBI)"></a>Davies-Bouldin Index(DBI)</h3><p>定义</p><ul><li>$c_k$：簇$C_k$的中心点</li><li>$\sigma_k$：簇$C_k$中所有元素到$c_k$的距离的均值</li><li>$d(c_i, c_j)$：簇中心$c_i$与$c_j$之间的距离</li></ul><p>则</p><script type="math/tex; mode=display">DBI = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)</script><p>$DBI$越小越好</p><h3 id="Dunn-index-DI"><a href="#Dunn-index-DI" class="headerlink" title="Dunn index(DI)"></a>Dunn index(DI)</h3><p>定义</p><ul><li>$d(i,j)$：两类簇的距离，定义方法多样，例如两类簇中心的距离；</li><li>$d’(k)$：簇$C_k$的类内距离，同样的，可定义多种，例如簇$C_k$中任意两点距离的最大值。</li></ul><p>则</p><script type="math/tex; mode=display">DI = \frac{\min_{1 \leq i < j \leq K} d(i, j)}{\max_{1 \leq k \leq K} d'(k)}</script><h1 id="sklearn中的评价指标"><a href="#sklearn中的评价指标" class="headerlink" title="sklearn中的评价指标"></a>sklearn中的评价指标</h1><blockquote><p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/model_evaluation.html" target="_blank" rel="noopener">3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.19.0 documentation - ApacheCN</a></p></blockquote><p><img src="/2018/11/21/Metrics/sklearn_metrics.png" alt="sklearn_metrics"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import metrics</span><br><span class="line">&gt;&gt;&gt; dir(metrics)</span><br><span class="line">[&apos;SCORERS&apos;, &apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__cached__&apos;, &apos;__doc__&apos;, </span><br><span class="line">&apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;, &apos;__path__&apos;, &apos;__spec__&apos;,</span><br><span class="line"> &apos;accuracy_score&apos;, &apos;adjusted_mutual_info_score&apos;, &apos;adjusted_rand_score&apos;, </span><br><span class="line"> &apos;auc&apos;, &apos;average_precision_score&apos;, &apos;balanced_accuracy_score&apos;, </span><br><span class="line"> &apos;base&apos;, &apos;brier_score_loss&apos;, &apos;calinski_harabaz_score&apos;, &apos;check_scoring&apos;, </span><br><span class="line"> &apos;classification&apos;, &apos;classification_report&apos;, &apos;cluster&apos;, &apos;cohen_kappa_score&apos;, </span><br><span class="line"> &apos;completeness_score&apos;, &apos;confusion_matrix&apos;, &apos;consensus_score&apos;, </span><br><span class="line"> &apos;coverage_error&apos;, &apos;davies_bouldin_score&apos;, &apos;euclidean_distances&apos;, </span><br><span class="line"> &apos;explained_variance_score&apos;, &apos;f1_score&apos;, &apos;fbeta_score&apos;, </span><br><span class="line"> &apos;fowlkes_mallows_score&apos;, &apos;get_scorer&apos;, &apos;hamming_loss&apos;, &apos;hinge_loss&apos;, </span><br><span class="line"> &apos;homogeneity_completeness_v_measure&apos;, &apos;homogeneity_score&apos;, </span><br><span class="line"> &apos;jaccard_similarity_score&apos;, &apos;label_ranking_average_precision_score&apos;, </span><br><span class="line"> &apos;label_ranking_loss&apos;, &apos;log_loss&apos;, &apos;make_scorer&apos;, &apos;matthews_corrcoef&apos;, </span><br><span class="line"> &apos;mean_absolute_error&apos;, &apos;mean_squared_error&apos;, &apos;mean_squared_log_error&apos;, </span><br><span class="line"> &apos;median_absolute_error&apos;, &apos;mutual_info_score&apos;, </span><br><span class="line"> &apos;normalized_mutual_info_score&apos;, &apos;pairwise&apos;, &apos;pairwise_distances&apos;, </span><br><span class="line"> &apos;pairwise_distances_argmin&apos;, &apos;pairwise_distances_argmin_min&apos;, </span><br><span class="line"> &apos;pairwise_distances_chunked&apos;, &apos;pairwise_fast&apos;, &apos;pairwise_kernels&apos;, </span><br><span class="line"> &apos;precision_recall_curve&apos;, &apos;precision_recall_fscore_support&apos;, </span><br><span class="line"> &apos;precision_score&apos;, &apos;r2_score&apos;, &apos;ranking&apos;, &apos;recall_score&apos;, &apos;regression&apos;, </span><br><span class="line"> &apos;roc_auc_score&apos;, &apos;roc_curve&apos;, &apos;scorer&apos;, &apos;silhouette_samples&apos;, </span><br><span class="line"> &apos;silhouette_score&apos;, </span><br><span class="line"> &apos;v_measure_score&apos;, </span><br><span class="line"> &apos;zero_one_loss&apos;]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Entropy</title>
      <link href="/2018/11/21/Entropy/"/>
      <url>/2018/11/21/Entropy/</url>
      
        <content type="html"><![CDATA[<h1 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h1><p>概率$p$是对确定性的度量，那么信息量就是对不确定性的度量，公式定义为</p><script type="math/tex; mode=display">I(x) = - \log p(x) \tag{1}</script><p>信息量也被称为随机变量$x$的自信息<code>(self-information)</code></p><blockquote><p>底数为$2$时，单位为<code>bit</code>，底数为$e$时，单位为<code>nat</code></p></blockquote><p><img src="/2018/11/21/Entropy/信息量.png" alt="信息量"></p><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>信息熵<code>(information entropy)</code>定义为</p><script type="math/tex; mode=display">H(X) = - \sum_{x} p(x) \log p(x) \tag{2}</script><p>可看作<strong>信息量的期望</strong>,在$0-1$分布的信息熵为</p><script type="math/tex; mode=display">H(p) = - p \log p - (1 - p) \log (1 - p)</script><p>图像如下，可见在$p=0.5$时，熵最大。<br><img src="/2018/11/21/Entropy/entropy_of_01.png" alt="entropy_of_01"></p><blockquote><p>函数$y=x \log x$的图像<br><img src="/2018/11/21/Entropy/xlogx.png" alt="xlogx"><br>有</p><script type="math/tex; mode=display">\lim_{x \rightarrow 0} y = \lim_{x \rightarrow 1} y = 0</script></blockquote><h1 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h1><p>根据信息熵的定义，推广到多维随机变量，就得到联合熵的定义式，以$2$维随机变量为例</p><script type="math/tex; mode=display">H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y) \tag{3}</script><p>可推广至多维。</p><!-- 机器学习笔记十：各种熵总结 - 谢小小XH - CSDN博客 https://blog.csdn.net/xierhacker/article/details/53463567 --><h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>现在有关于样本集的两个概率分布$p(x)$和$q(x)$，其中$p(x)$为真实分布，$q(x)$非真实分布。</p><p>如果用真实分布$p(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p) = - \sum_x p(x) \log p(x)</script><p>如果用非真实分布$q(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为:</p><script type="math/tex; mode=display">H(p, q) = - \sum_x p(x) \log q(x) \tag{4}</script><p>注意</p><script type="math/tex; mode=display">H(p, q) - H(p)= \sum_x p(x) \log \frac{p(x)}{q(x)}= D_{KL}(p||q)</script><p>当用非真实分布$q(x)$得到的平均码长比真实分布$p(x)$得到的平均码长多出的比特数就是相对熵。我们希望通过最小化相对熵$D_{KL}(p||q)$使$q(x)$尽量趋近$p(x)$，即</p><script type="math/tex; mode=display">q(x) = \arg \min_{q(x)} D_{KL} (p||q)</script><p>而$H(p)$是样本集的熵，为固定的值，故</p><script type="math/tex; mode=display">q(x) = \arg \min_{q(x)} H(p, q)</script><p>即等价于最小化交叉熵。</p><h1 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h1><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。定义为在给定$X$下$Y$的条件概率分布的熵对$X$的期望，即</p><script type="math/tex; mode=display">H(Y|X) = E_{p(x)} H(Y|X=x)= \sum_x p(x) H(Y|X=x) \tag{5}</script><p>其中</p><script type="math/tex; mode=display">H(Y|X=x) = - \sum_y p(y|x) \log p(y|x)</script><p>故</p><script type="math/tex; mode=display">H(Y|X) = \sum_x p(x) \left[- \sum_y p(y|x) \log p(y|x)\right]</script><script type="math/tex; mode=display">= - \sum_x \sum_y p(x, y) \log p(y|x)</script><p>即</p><script type="math/tex; mode=display">H(Y|X) = - \sum_{x, y} p(x, y) \log p(y|x) \tag{6}</script><p>实际上，条件熵满足</p><script type="math/tex; mode=display">H(Y|X) = H(X, Y) - H(X) \tag{7}</script><blockquote><p>证明：<br>已知</p><script type="math/tex; mode=display">H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y)</script><script type="math/tex; mode=display">H(X) = - \sum_{x} p(x) \log p(x)</script><p>则</p><script type="math/tex; mode=display">H(X, Y) - H(X)</script><script type="math/tex; mode=display">= - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x} p(x) \log p(x)</script><script type="math/tex; mode=display">= - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x, y} p(x, y) \log p(x)</script><script type="math/tex; mode=display">= \sum_{x, y} p(x, y) \log \frac{p(x)}{p(x, y)}</script><script type="math/tex; mode=display">= \sum_{x, y} p(x, y) \log p(y|x)</script><script type="math/tex; mode=display">= H(Y|X)</script></blockquote><h1 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h1><p>相对熵<code>(relative entropy)</code>，又称<code>KL</code>散度<code>(Kullback–Leibler divergence)</code>。可以用来衡量两个概率分布之间的差异，就是求$p(x)$与$q(x)$之间的对数差在 pp 上的期望值。</p><script type="math/tex; mode=display">D_{KL} (p||q) = E_{p(x)} \log \frac{p(x)}{q(x)}= \sum_x p(x) \log \frac{p(x)}{q(x)} \tag{8}</script><p>注意</p><ul><li><p>相对熵不具有对称性，即</p><script type="math/tex; mode=display">  D_{KL} (p||q) \neq D_{KL} (q||p)</script></li><li><p>$D_{KL} (p||q) \geq 0$</p><blockquote><p>证明：</p><script type="math/tex; mode=display">D_{KL} (p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = - \sum_x p(x) \log \frac{q(x)}{p(x)}</script><p>由<code>Jensen inequality</code></p><script type="math/tex; mode=display">\sum_x p(x) \log \frac{q(x)}{p(x)}\leq \log \sum_x p(x) \frac{q(x)}{p(x)}= \log \sum_x q(x)</script><p>所以</p><script type="math/tex; mode=display">D_{KL} (p||q) \geq - \log \sum_x q(x)</script><p>而$0 \leq q(x) \leq 1$，故</p><script type="math/tex; mode=display">D_{KL} (p||q) \geq 0</script></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Non-parameter Estimation</title>
      <link href="/2018/11/19/Non-parameter-Estimation/"/>
      <url>/2018/11/19/Non-parameter-Estimation/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>若参数估计时我们不知道样本的分布形式，那么就无法确定需要估计的概率密度函数，无法用<a href="https://louishsu.xyz/2018/10/22/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">最大似然估计、贝叶斯估计等参数估计方法</a>，应该用非参数估计方法。</p><p>需要知道的是，作为非参数方法的共同问题是对样本数量需求较大，只要样本数目足够大众可以保证收敛于任何复杂的位置密度，但是计算量和存储量都比较大。当样本数很少时，如果能够对密度函数有先验认识，则参数估计能取得更好的估计效果。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>若有$M$个样本$x^{(1)}, …, x^{(M)}$，依概率密度函数$p(x)$独立同分布抽样得到。</p><p>一个样本$x$落在区域$R$中的概率$P$可表示为</p><script type="math/tex; mode=display">P = \int_R p(x) dx \tag{1}</script><p>我们通过计算$P$来估计概率密度$p(x)$。</p><p>$K$个样本落入区域$R$的概率$P_K$为二项分布，即$K \sim B(M, P)$</p><script type="math/tex; mode=display">P_K = \left(\begin{matrix} M\\K \end{matrix}\right) P^K (1-P)^{M-K} \tag{2}</script><p>则$K$的期望与方差分别为</p><script type="math/tex; mode=display">E(K) = MP;　D(K) = MP(1-P)</script><p>样本个数$M$越多，$D(K)$越大，即$K$在期望附近的波峰越明显，因此样本足够多时，用$K/M$作为$P$的一个估计非常准确，即</p><script type="math/tex; mode=display">P \approx \frac{K}{M} \tag{3}</script><p>若我们假设$p(x)$是连续的，且区域$R$足够小，记其体积为$V$，那么有</p><script type="math/tex; mode=display">P = \int_R p(x)dx \approx p(x) V \tag{4}</script><p>所以根据$(3)(4)$，得到</p><script type="math/tex; mode=display">p(x) \approx \frac{K/M}{V} \tag{*}</script><p>但是我们获得的其实为平滑后的概率密度函数</p><script type="math/tex; mode=display">\frac{P}{V} = \frac{\int_R p(x)dx}{\int_R dx}</script><p>我们希望其尽可能地趋近$p(x)$，那么必须要求$V \rightarrow 0$，但是这样就可能不包含任何样本，那么$p(x)\approx 0$，这样估计的结果毫无意义。</p><p>所以在实际中，一般构造多个包含样本$x$的区域$R_1, …, R_i, …, R_n$，第$i$个区域使用$i$个样本，记$V_i$为$R_i$的体积，$M_i$为落在$R_i$中的样本个数，则对$p(x)$第$i$次估计$p_i(x)$表示为</p><script type="math/tex; mode=display">p_i(x) \approx \frac{M_i / M}{V_i} \tag{5}</script><p>若要求$p_i(x)$收敛到$p(x)$，则必须满足</p><ul><li>$\lim_{i\rightarrow \infty} V_i = 0$</li><li>$\lim_{i\rightarrow \infty} M_i = 0$</li><li>$\lim_{i\rightarrow \infty} \frac{M_i}{M} = 0$</li></ul><h1 id="直方图法"><a href="#直方图法" class="headerlink" title="直方图法"></a>直方图法</h1><p>记不记得小学时的直方图统计，直方图方法的思想就是这样，以$1$维样本为例，我们将$x$的取值范围平均等分为$K$个区间，统计每个区间内样本的个数，由此计算区间的概率密度。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>若共有$N$维样本$M$组，在每个维度上$K$等分，就有$K^N$个小空间，每个小空间的体积$V_i$可以定义为</p><script type="math/tex; mode=display">V_i = \prod_{n=1}^N d_n,　i=1,...,K^N</script><p>其中</p><script type="math/tex; mode=display">d_n = \frac{\max x_n - \min x_n}{K}</script><p>假设样本落到各个小空间的概率相同，若第$i$个小空间包含$M_i$个样本，则该空间的概率密度$\hat{p_i}$为</p><script type="math/tex; mode=display">\hat{p_i} = \frac{M_i / M}{V_i} \tag{6}</script><p>估计的效果与小区间的大小密切相连，如果区域选择过大，会导致最终估计出来的概率密度函数非常粗糙；如果区域的选择过小，可能会导致有些区域内根本没有样本或者样本非常少，这样会导致估计出来的概率密度函数很不连续。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/algorithm/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p><p>我们可以用<code>matplotlib.pyplot.hist()</code>或<code>numpy.histogram()</code>实现</p><ul><li><p><code>matplotlib</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n, bins, patches = plt.hist(arr, bins=10, normed=0, facecolor=&apos;black&apos;, edgecolor=&apos;black&apos;,alpha=1，histtype=&apos;bar&apos;)</span><br></pre></td></tr></table></figure><ul><li><code>Args</code><br>  参数很多，选几个常用的讲解<ul><li>arr: 需要计算直方图的一维数组</li><li>bins: 直方图的柱数，可选项，默认为10</li><li>normed: 是否将得到的直方图向量归一化。默认为0</li><li>facecolor: 直方图颜色</li><li>edgecolor: 直方图边框颜色</li><li>alpha: 透明度</li><li>histtype: 直方图类型，‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’</li></ul></li><li><code>Returns</code><ul><li>n: 直方图向量，是否归一化由参数normed设定</li><li>bins: 返回各个bin的区间范围</li><li>patches: 返回每个bin里面包含的数据，是一个list</li></ul></li></ul></li><li><p><code>numpy</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hist, bin_edges = histogram(a, bins=10, range=None, normed=None, weights=None, density=None)</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def histEstimate(X, n_bins, showfig=False):</span><br><span class="line">    &quot;&quot;&quot; 直方图密度估计</span><br><span class="line">    Args:</span><br><span class="line">        n_bins: &#123;int&#125; 直方图的条数</span><br><span class="line">    Returns:</span><br><span class="line">        hist: &#123;ndarray(n_bins,)&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    n, bins, patches = plt.hist(X, bins=n_bins, normed=1, facecolor=&apos;lightblue&apos;, edgecolor=&apos;white&apos;)</span><br><span class="line">    if showfig: plt.show()</span><br><span class="line">    return n, bins, patches</span><br></pre></td></tr></table></figure><p><code>matplotlib</code>直方图显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_matplotlib.png" alt="hist_matplotlib"></p><p>拟合各中心点显示如下<br><img src="/2018/11/19/Non-parameter-Estimation/hist_ploy.png" alt="hist_ploy"></p><h1 id="K-n-近邻估计法"><a href="#K-n-近邻估计法" class="headerlink" title="$K_n$近邻估计法"></a>$K_n$近邻估计法</h1><p>随着样本数的增加，区域的体积应该尽可能小，同时又必须保证区域内有充分多的样本，但是每个区域的样本数有必须是总样本数的很小的一部分，而不是与直方图估计那样体积不变。</p><p>那么我们想，能否根据样本的分布调整分区大小呢，$K$近邻估计法就是一种采用可变大小区间的密度估计方法。</p><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>根据总样本确定参数$K_n$，在求样本$x$处的密度估计$\hat{p}(x)$时，调整区域体积$V(x)$，直到区域内恰好落入$K_n$个样本，估计公式为</p><script type="math/tex; mode=display">\hat{p}(x) = \frac{K_n/M}{V(x)} \tag{7}</script><p>一般指定超参数$a$，取</p><script type="math/tex; mode=display">K_n = a × \sqrt{M} \tag{8}</script><blockquote><script type="math/tex; mode=display">\hat{p}(x) = \frac{a × \sqrt{M} /M}{V(x)} = \frac{K_n'/M}{V'(x)}</script><p>其中$K_n’ = a,V’(x) = V(x)×\frac{1}{\sqrt{M}}$</p></blockquote><p>在样本密度比较高的区域的体积就会比较小，而在密度低的区域的体积则会自动增大，这样就能够较好的兼顾在高密度区域估计的分辨率和在低密度区域估计的连续性。</p><h1 id="Parzen窗法"><a href="#Parzen窗法" class="headerlink" title="Parzen窗法"></a>Parzen窗法</h1><p>又称核密度估计。</p><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>我们暂时假设待估计点$x$的附近区间$R$为一个$N$维的<strong>超立方体</strong>，用$h$表示边的长度，那么</p><script type="math/tex; mode=display">V_i = h^N</script><p>即<br><img src="/2018/11/19/Non-parameter-Estimation/Parzen_window.jpg" alt="Parzen_window"><br>定义窗函数$\varphi(·)$，表示落入以$x$为中心的超立方体的区域的点</p><script type="math/tex; mode=display">\varphi \left(\frac{x_i-x}{h}\right) = \begin{cases}    1 & \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2},　n=1,...,N \\    0 & otherwise\end{cases} \tag{9}</script><blockquote><script type="math/tex; mode=display">\frac{|x_{in}-x_n|}{h} \leq \frac{1}{2}　即　(x_i-x)_n \leq \frac{h}{2}</script><p>这里的$h$起到单位化的作用，便于推广</p></blockquote><p>那么落入以$x$为中心的<strong>超立方体</strong>的区域的点的个数为</p><script type="math/tex; mode=display">M_i = \sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right) \tag{10}</script><p>代入$p(x) \approx \frac{M_i/M}{V_i}$，我们得到</p><script type="math/tex; mode=display">p(x) \approx \frac{\sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right)/M}{V_i}= \frac{1}{M} \sum_{i=1}^M \frac{1}{V_i} \varphi \left(\frac{x_i-x}{h}\right) \tag{11}</script><p>我们定义核函数(或称“窗函数”)</p><script type="math/tex; mode=display">\kappa(z) = \frac{1}{V_i} \varphi(z) \tag{12}</script><p>核函数反应了一个观测样本$x_i$对在$x$处的概率密度估计的贡献，与样本$x_i$和$x$的距离有关。而概率密度估计就是在这一点上把所有观测样本的贡献进行平均</p><script type="math/tex; mode=display">p(x) \approx \frac{1}{M} \sum_{i=1}^M \kappa\left(\frac{x_i-x}{h}\right) \tag{13}</script><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>核函数应满足概率密度的要求，即</p><script type="math/tex; mode=display">\kappa(z) \geq 0　\And　\int \kappa(z)dz = 1</script><p>通常有以下几种核函数</p><ul><li><p>均匀核</p><script type="math/tex; mode=display">  \kappa(z)  = \begin{cases}      1 & |z_n| \leq \frac{1}{2},　n=1,...,N \\      0 & otherwise  \end{cases}</script></li><li><p>高斯核(正态核)<br>  高斯核是将窗放大到整个空间，各个观测样本$x_i$对待观测点$x$的加权和(越远权值越小)。</p><script type="math/tex; mode=display">  \kappa(z)  = \frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}}  \exp \left(-\frac{1}{2} (z - \mu)^T \Sigma^{-1} (z - \mu)\right)</script></li><li><p>超球窗</p><script type="math/tex; mode=display">  \kappa(z)  = \begin{cases}      V^{-1} & ||z|| \leq 1 \\      0 & otherwise  \end{cases}</script><blockquote><p>$z=\frac{x_i-x}{h}$，故$||z||\leq 1$即$||x_i-x||^2\leq h^2$<br>此时$h$表示超球体的半径</p></blockquote></li></ul><h2 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h2><p><a href="http://sklearn.apachecn.org/en/0.19.0/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" target="_blank" rel="noopener">sklearn.neighbors.KernelDensity — scikit-learn 0.19.0 documentation - ApacheCN</a><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.neighbors import KernelDensity</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kde.score_samples(X)</span><br><span class="line">array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,</span><br><span class="line">       -0.41076071])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kde.sample(10)</span><br><span class="line">array([[ 1.80042291,  1.1030739 ],</span><br><span class="line">       [ 0.87299669,  1.0762352 ],</span><br><span class="line">       [-2.40180586, -1.19554374],</span><br><span class="line">       [-1.97985919, -1.19361193],</span><br><span class="line">       [-2.95866231, -2.1972637 ],</span><br><span class="line">       [-1.12739556, -0.80851063],</span><br><span class="line">       [ 1.03756706,  1.24855099],</span><br><span class="line">       [ 1.21729703,  1.02345815],</span><br><span class="line">       [-2.11816867, -1.0486257 ],</span><br><span class="line">       [-1.04875537, -0.89928711]])</span><br></pre></td></tr></table></figure></p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><p>具体代码见<br><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p64_non_parametric_estmation.py" target="_blank" rel="noopener">@Github: Non-parametric Estmation</a></p><p>定义核函数如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 高斯核</span></span><br><span class="line">gaussian = <span class="keyword">lambda</span> z: np.exp(<span class="number">-0.5</span>*(np.linalg.norm(z)**<span class="number">2</span>)) / np.sqrt(<span class="number">2</span>*np.pi)</span><br><span class="line"><span class="comment"># 均匀核</span></span><br><span class="line">square = <span class="keyword">lambda</span> z: <span class="number">1</span> <span class="keyword">if</span> (np.linalg.norm(z) &lt;= <span class="number">0.5</span>) <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p><p>密度估计函数如下，需要对连续范围内的各个点，即$x \in [min(X), max(X)]$进行估计获得<code>p</code>，作图显示$x-p$即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parzenEstimate</span><span class="params">(X, kernel, h, n_num=<span class="number">50</span>)</span>:</span></span><br><span class="line">    <span class="string">""" 核参数估计</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples,)&#125;</span></span><br><span class="line"><span class="string">        kernel: &#123;function&#125; 可调用的核函数</span></span><br><span class="line"><span class="string">        h: &#123;float&#125; 核函数的参数</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        p: &#123;ndarray(n_num,)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        - 一维，故`V_i = h`</span></span><br><span class="line"><span class="string">        - p(x) = \frac&#123;1&#125;&#123;M&#125; \sum_&#123;i=1&#125;^M \kappa \left( \frac&#123;x_i - x&#125;&#123;h&#125; \right)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = np.linspace(np.min(X), np.max(X), num=n_num)</span><br><span class="line">    p = np.zeros(shape=(x.shape[<span class="number">0</span>],))</span><br><span class="line">    z = <span class="keyword">lambda</span> x, x_i, h: (x - x_i) / h</span><br><span class="line">    V_i = h; n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">            p[idx] += kernel(z(x[idx], X[i], h)) / V_i</span><br><span class="line">        p[idx] /= n_samples</span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure></p><h3 id="均匀核"><a href="#均匀核" class="headerlink" title="均匀核"></a>均匀核</h3><ul><li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.5.png" alt="01_h_0.5"></p></li><li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_0.8.png" alt="01_h_0.8"></p></li><li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_1.0.png" alt="01_h_1.0"></p></li><li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/01_h_2.0.png" alt="01_h_2.0"></p></li></ul><h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><ul><li><p>$h=0.5$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.5.png" alt="gaussian_h_0.5"></p></li><li><p>$h=0.8$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_0.8.png" alt="gaussian_h_0.8"></p></li><li><p>$h=1.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_1.0.png" alt="gaussian_h_1.0"></p></li><li><p>$h=2.0$<br>  <img src="/2018/11/19/Non-parameter-Estimation/gaussian_h_2.0.png" alt="gaussian_h_2.0"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Parameter Estimation</title>
      <link href="/2018/11/19/Parameter-Estimation/"/>
      <url>/2018/11/19/Parameter-Estimation/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/20587681/answer/17435552" target="_blank" rel="noopener">贝叶斯学派与频率学派有何不同？ - 任坤的回答 - 知乎</a></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>参数估计<code>(parameter estimation)</code>，统计推断的一种。根据从总体中抽取的随机样书．来估计总体分布中未知参数的过程。主要介绍最大似然估计<code>(MLE: Maximum Likelihood Estimation)</code>，最大后验概率估计<code>(MAP: Maximum A Posteriori Estimation)</code>，贝叶斯估计<code>(Bayesian Estimation)</code>。</p><blockquote><p>解释一下“似然函数”和“后验概率”，在<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>一节，给出定义如下</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><p>上式中$ k=1,…,K  $，各部分定义如下<br>$P(c_k|x)$——<code>后验概率(posteriori probability)</code><br>$P(c_k)$——<code>先验概率(priori probability)</code><br>$p(x|c_k)$——$c_k$关于$x$的<code>似然函数(likelihood)</code><br>$p(x)$——<code>证据因子(evidence)</code></p></blockquote><h1 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h1><p>以最经典的掷硬币实验为例，假设有一枚硬币，投掷一次出现正面记$”1”$，投掷$10$次的实验结果如下</p><script type="math/tex; mode=display">\{ 0， 1， 1， 1， 1， 0， 1， 1， 1，0 \}</script><p>记硬币投掷结果为随机变量$X$，且$ x \in \{0, 1\}$，硬币投掷一次服从二项分布，估计二项分布的参数$\theta$</p><h1 id="最大似然估计-MLE"><a href="#最大似然估计-MLE" class="headerlink" title="最大似然估计(MLE)"></a>最大似然估计(MLE)</h1><h2 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h2><blockquote><p><a href="https://en.wikipedia.org/wiki/Likelihood_function#Definition" target="_blank" rel="noopener">Likelihood function - Wikipedia</a></p></blockquote><ul><li><p>离散型</p><script type="math/tex; mode=display">L(x | \theta) = p_{\theta}(x)=P_{\theta}(X = x)</script></li><li><p>连续型</p><script type="math/tex; mode=display">L(x | \theta) = f_{\theta}(x)</script></li></ul><blockquote><p>很多人能讲出一大堆哲学理论来阐明这一对区别。<br>但我觉得，从工程师角度来讲，这样理解就够了:<br>频率 $vs$ 贝叶斯 = $P(X; w)$ $vs$ $P(X|w)$ 或 $P(X,w)$</p><p>作者：许铁-巡洋舰科技<br>链接：<a href="https://www.zhihu.com/question/20587681/answer/122348889" target="_blank" rel="noopener">https://www.zhihu.com/question/20587681/answer/122348889</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>有数据集$D = \{x_1, x_2, …, x_N\}$，按$c$个类别分成$\{D_1, D_2, …, D_C\}$，各个类别服从的概率分布密度函数模型已给出，估计参数$\hat{\Theta} = \{\hat{\theta}_{c_1}, \hat{\theta}_{c_2}, …, \hat{\theta}_{c_C}\} $</p><p><strong>假定</strong></p><ul><li>类别间独立，且各自服从概率分布密度函数为$p(x|c_j)$</li><li>各类别的概率密度$p(x|c_j)$以参数$\theta_{c_j}$确定，即$p(x|c_j; \theta_{c_j})$</li></ul><p>故似然函数为</p><script type="math/tex; mode=display">L(D | \Theta) = P(x_1, x_2, ..., x_N | \Theta) = \prod_{i=1}^N p(x_i | \theta_{x_i \in c_j})</script><blockquote><p>理解为，在参数$\Theta$为何值的条件下，实验结果出现数据集$D$的概率最大</p></blockquote><p>求取其极大值对应的参数即可</p><ul><li>一般取对数似然函数<script type="math/tex; mode=display">\log L(D | \Theta) = \sum_{i=1}^N \log p(x_i | \theta_{x_i \in c_j})</script></li><li>极大值即对应梯度为$\vec{0}$的位置，即<script type="math/tex; mode=display">∇_\Theta  \log L(D | \Theta) = \vec{0}\Rightarrow\hat{\Theta}</script></li></ul><blockquote><p>Some comments about ML</p><ul><li>ML estimation is usually simpler than alternative methods. </li><li>Has good convergence properties as the number of training samples increases. </li><li>If the model chosen for p(x|θ) is correct, and independence assumptions among variables are true, ML will give very good results.</li><li>If the model is wrong, ML will give poor results.<div style="text-align: right"> —— Zhao Haitao. Maximum Likelihood and Bayes Estimation </div></li></ul></blockquote><h2 id="例：正态分布的最大似然估计"><a href="#例：正态分布的最大似然估计" class="headerlink" title="例：正态分布的最大似然估计"></a>例：正态分布的最大似然估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的的最大似然估计</p><script type="math/tex; mode=display">P(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">L(D | \mu, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}=\left( \frac{1}{\sqrt{2\pi} \sigma } \right)^N \prod_{i=1}^N e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><p>取对数似然</p><script type="math/tex; mode=display">\log L(D | \mu, \sigma^2) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2</script><h3 id="1-参数-mu-的估计"><a href="#1-参数-mu-的估计" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><script type="math/tex; mode=display">\frac{∂}{∂\mu} L(D | \mu, \sigma^2) = \frac{1}{\sigma^2} (\sum_{i=1}^N x_i - N\mu)= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\mu}= \frac{1}{N}\sum_{i=1}^N x_i</script><h3 id="2-参数-sigma-2-的估计"><a href="#2-参数-sigma-2-的估计" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2} \log L(D | \mu, \sigma^2) = - \frac{N}{2\sigma^2} +\frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\sigma^2} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2</script><blockquote><p>参数$\hat{\mu}, \hat{\sigma}^2$的值与样本均值和样本方差相等</p></blockquote><h1 id="最大后验概率估计-MAP"><a href="#最大后验概率估计-MAP" class="headerlink" title="最大后验概率估计(MAP)"></a>最大后验概率估计(MAP)</h1><!-- > [高斯混合模型(GMM)与最大期望算法(EM)]() --><h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>最大似然估计是求参数$\theta$, 使似然函数$P(D | \theta)$最大，最大后验概率估计则是求$\theta$使$P(\theta | D)$最大</p><blockquote><p>理解为，在已出现的实验样本$D$上，参数$\theta$取何值的概率最大</p></blockquote><p>且注意到</p><script type="math/tex; mode=display">P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}</script><p>故$MAP$不仅仅使似然函数$P(D | \theta)$最大，而且使$P(\theta)$最大，即</p><script type="math/tex; mode=display">\theta = argmax L(\theta | D)</script><script type="math/tex; mode=display">L(\theta | D) = P(\theta) P(D | \theta)= P(\theta) \prod_{i=1}^N p(x_i | \theta)</script><blockquote><p>比$ML$多了一项$P(\theta)$</p></blockquote><ul><li><p>取对数后</p><script type="math/tex; mode=display">\log L(\theta | D) = \sum_{i=1}^N \log p(x_i | \theta) + \log P(\theta)</script></li><li><p>求取极大值</p><script type="math/tex; mode=display">∇_\theta L(\theta | D) = 0\Rightarrow\hat{\theta}</script></li></ul><blockquote><p>$MAP$和$MLE$的区别：<br>$MAP$允许我们把先验知识加入到估计模型中，这在<strong>样本很少</strong>的时候是很有用的，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如<code>beta</code>分布的$\alpha, \beta$，我们还可以调节把估计的结果“拉”向先验的幅度，$\alpha, \beta$越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。<br><a href="https://blog.csdn.net/hustlx/article/details/51144710" target="_blank" rel="noopener">极大似然估计，最大后验概率估计(MAP)，贝叶斯估计 - 李鑫o_O - CSDN博客</a></p></blockquote><h2 id="例：正态分布的最大后验概率估计"><a href="#例：正态分布的最大后验概率估计" class="headerlink" title="例：正态分布的最大后验概率估计"></a>例：正态分布的最大后验概率估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的最大后验概率估计</p><script type="math/tex; mode=display">p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><blockquote><script type="math/tex; mode=display">\log p(x_i | \mu, \sigma^2)= - \frac{1}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (x_i - \mu)^2</script></blockquote><h3 id="1-参数-mu-的估计-1"><a href="#1-参数-mu-的估计-1" class="headerlink" title="1. 参数$\mu$的估计"></a>1. 参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p><script type="math/tex; mode=display">p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><blockquote><script type="math/tex; mode=display">\log p(\mu)= - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script></blockquote><p>则</p><script type="math/tex; mode=display">\log L(\mu, \sigma^2 | D)= - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2</script><p>则</p><script type="math/tex; mode=display">\frac{∂}{∂\mu} \log L(\mu, \sigma^2 | D)= \frac{1}{\sigma^2} \sum_{i=0}^N (x_i - \mu) - \frac{1}{\sigma_{\mu_0}^2} (\mu - \mu_0)= 0</script><script type="math/tex; mode=display">\Rightarrow\hat{\mu}= \frac{\mu_0 \sigma^2 + \sigma_{\mu_0}^2 \sum_{i=0}^N x_i}{\sigma^2 + N \sigma_{\mu_0}^2}= \frac{\mu_0 + \frac{\sigma_{\mu_0}^2}{\sigma^2} \sum_{i=0}^N x_i}{1 + \frac{\sigma_{\mu_0}^2}{\sigma^2} N }</script><h3 id="2-参数-sigma-2-的估计-1"><a href="#2-参数-sigma-2-的估计-1" class="headerlink" title="2. 参数$\sigma^2$的估计"></a>2. 参数$\sigma^2$的估计</h3><p>给定先验条件：$\sigma^2$服从正态分布$N(\sigma_0^2, \sigma_{\sigma_0^2}^2)$，即</p><script type="math/tex; mode=display">p(\sigma^2) = \frac{1}{\sqrt{2\pi} \sigma_{\sigma_0^2}} e^ {-\frac{(\sigma^2- \sigma_0^2)^2}{2 \sigma_{\sigma_0^2} ^2}}</script><blockquote><script type="math/tex; mode=display">\log p(\sigma^2) = - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script></blockquote><p>则</p><script type="math/tex; mode=display">\log L(\mu, \sigma^2 | D)= - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2</script><p>则</p><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2} \log L(\mu, \sigma^2 | D)= - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2\sigma_{\sigma_0}^2} \frac{\sigma - \sigma_0}{\sigma}\Rightarrow\hat{\sigma^2}(略)</script><blockquote><script type="math/tex; mode=display">\frac{∂}{∂\sigma^2}(\sigma - \sigma_0)^2= 2(\sigma - \sigma_0)\frac{∂}{∂\sigma^2} (\sigma - \sigma_0)= \frac{\sigma - \sigma_0}{\sigma}</script></blockquote><h1 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h1><h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><!-- 贝叶斯公式：$$P(c_i|x) = \frac{p(x|c_i)p(c_i)}{\sum_j p(x|c_j)p(c_j)}$$在给定数据集$D=\{x_1, x_2, ..., x_N\}$的情况下，可从数据中估计先验概率和似然函数，即$$P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i, D)}{\sum_j p(x|c_j, D)p(c_j, D)}$$**假定**- 先验概率密度函数为$p(c_i)$已知- 抽样结果几乎与真实分布一致，即$ p(c_i, D) \approx p(c_i) $则$$P(c_i|x, D) = \frac{p(x|c_i, D)p(c_i)}{\sum_j p(x|c_j, D)p(c_j)}$$只需从样本中，估计每个类别的似然函数$p(x|c_i, D)$即可>-----------------------------------------------现考虑**单个类别**中抽取的数据集$D$，如何估计该类别的似然函数$p(x | \theta)$参数$\theta$呢？若概率密度函数为$p(x | \theta)$，记从数据集中估计得到的似然函数为$p(x | D)$，有$$p(x | \theta) \approx p(x | D)$$> $p(x | D)$ would be the estimate of $p(x | \theta)$ given $D$且$$p(x | D) = \int p(x, \theta | D) d \theta= \int p(x | \theta, D) p(\theta | D) d \theta$$> Links $p(x | D)$ with $p(θ | D)$其中- $p(x | \theta, D) \approx p(x | \theta) $- $p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)}= \frac{p(\theta) \prod_{i=1}^N p(x_i | \theta)}{P(D)}$故$$p(x | D) = \int p(x | \theta) p(\theta | D) d \theta$$总的来说 --><script type="math/tex; mode=display">p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)}= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)</script><p>其中$a$是使</p><script type="math/tex; mode=display">\int p(\theta | D)  = 1</script><p>利用“质心公式”求解贝叶斯的点估计</p><script type="math/tex; mode=display">θ_{Bayes} = \int θ·p(θ|D) d θ</script><h2 id="例：正态分布的贝叶斯估计"><a href="#例：正态分布的贝叶斯估计" class="headerlink" title="例：正态分布的贝叶斯估计"></a>例：正态分布的贝叶斯估计</h2><p>数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的贝叶斯估计</p><script type="math/tex; mode=display">p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><h3 id="参数-mu-的估计"><a href="#参数-mu-的估计" class="headerlink" title="参数$\mu$的估计"></a>参数$\mu$的估计</h3><p>给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即</p><script type="math/tex; mode=display">p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}</script><p>则</p><script type="math/tex; mode=display">P(\mu | D) = a · p(\mu) \prod_{i=1}^N p(x_i | \mu)= a · \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}\prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}</script><script type="math/tex; mode=display">= a · \left( \frac{1}{\sqrt{2\pi}} \right)^{N + 1}\frac{1}{\sigma_{\mu_0} \sigma^N}e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2} -\sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2}}</script><p>易证</p><blockquote><p></p><p align="right">我已经想到了一个绝妙的证明,但是这台电脑的硬盘太小了,写不下。</p><br><!-- > <p align="right">诶嘿，显然成立，2333333</p> --><p></p></blockquote><script type="math/tex; mode=display">p(\mu | D) = \frac{1}{\sqrt{2\pi}\sigma_N} e^ {-\frac{(\mu - \mu_N)^2}{2\sigma_N^2}}</script><p>其中</p><script type="math/tex; mode=display">\mu_N = \frac{N \sigma_0^2}{N \sigma_0^2 + \sigma^2}\frac{1}{N} \sum_{i=1}^N x_i+\frac{\sigma^2}{N \sigma_0^2 + \sigma^2}\mu_0</script><script type="math/tex; mode=display">\sigma_N^2 = \frac{\sigma_0^2 \sigma^2}{N \sigma_0^2 + \sigma^2}</script><blockquote><p><strong>与$MLE$，$MAP$的区别</strong></p><ul><li>相比较$MLE$与$MAP$的点估计，贝叶斯估计得到的结果是参数$\theta$的密度函数$p(\theta | D)$</li><li><p>最大后验概率估计为求取对应最大后验概率的点</p><script type="math/tex; mode=display">\theta = argmax_\theta p(\theta | D)</script></li><li><p>贝叶斯估计为求取整个取值范围的概率密度$p(\theta | D)$，既然如此，必有</p><script type="math/tex; mode=display">\int p(\theta | D) d\theta = 1</script></li></ul><p><a href="https://www.cnblogs.com/zjh225901/p/7495505.html" target="_blank" rel="noopener">统计学习方法学习笔记（一）—极大似然估计与贝叶斯估计原理及区别 - YJ-20 - 博客园</a></p><script type="math/tex; mode=display">p(\theta | D) = \frac{p(\theta) \prod_{i=1}^N p(x_i | \theta)}{\int_\theta p(\theta) \prod_{i=1}^N p(x_i | \theta) d\theta}</script><p>由于$\theta$是满足一定概率分布的变量，所以在计算的时候需要将考虑所有$\theta$取值的情况，在计算过程中不可避免地高复杂度。所以计算时并不把所有地后验概率$p(\theta | D)$都找出来，而是采用类似于极大似然估计地思想，来极大化后验概率，得到这种有效的叫做$MAP$</p></blockquote><h1 id="引例的求解"><a href="#引例的求解" class="headerlink" title="引例的求解"></a>引例的求解</h1><p>已知硬币投掷结果服从$Bernoulli$分布</p><table>  <tr>    <th>X</th>    <th>0</th>    <th>1</th>  </tr>  <tr>    <td>P</td>    <td>1-θ</td>    <td>θ</td>  </tr></table><p>或者</p><script type="math/tex; mode=display">P(X_i) = \theta ^{X_i} (1 - \theta) ^{1 - X_i}</script><h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><p>实验结果中正面出现$7$次，反面出现$3$次，似然函数为</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{10} \theta ^{X_i} (1 - \theta) ^{1 - X_i} = \theta ^7 (1 - \theta) ^3</script><p>取对数似然函数并求极大值</p><script type="math/tex; mode=display">\log L(\theta) = 7 \log \theta + 3 \log (1 - \theta)</script><p>令</p><script type="math/tex; mode=display">\frac{∂}{∂ \theta} \log L(\theta)= \frac{7}{\theta} - \frac{3}{1-\theta} = 0</script><p>解得</p><script type="math/tex; mode=display">\theta = 0.7</script><p>即硬币服从$B(1, 0.7)$的概率分布</p><blockquote><p>做出$L(\theta)$图像验证，如下<br><img src="/2018/11/19/Parameter-Estimation/最大似然估计.png" alt="最大似然估计"></p></blockquote><h2 id="最大后验概率估计"><a href="#最大后验概率估计" class="headerlink" title="最大后验概率估计"></a>最大后验概率估计</h2><p>给定先验条件</p><script type="math/tex; mode=display">\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><p>则最大化</p><script type="math/tex; mode=display">L(\theta | D) = \theta ^7 (1 - \theta) ^3 · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}}</script><p>取对数</p><script type="math/tex; mode=display">\log L(\theta | D)= 7 \log \theta + 3 \log (1 - \theta) - \frac{1}{2} \log(2\pi \sigma_{\theta_0}^2) - \frac{1}{2\sigma_{\theta_0}^2} (\theta - \theta_0)^2</script><p>求取极大值点</p><script type="math/tex; mode=display">\frac{∂}{∂\theta} \log L(\theta | D) = \frac {7}{\theta} - \frac{3}{1-\theta} - \frac{\theta - \theta_0}{\sigma_{\theta_0}^2} = 0</script><p>得到</p><script type="math/tex; mode=display">\theta^3 - (\theta_0 + 1) \theta^2 + (\theta_0 - 10\sigma_{\theta_0}^2) \theta + 7\sigma_{\theta_0}^2 = 0</script><p>以下为选取不同先验条件时的$L(\theta | D)$图像，用于对比</p><blockquote><ul><li>第一张图为极大似然估计$L(D|\theta)$</li><li>第二张图为先验概率密度函数$P(\theta)$</li><li>第三张图为最大后验概率估计$L(\theta | D)$，$\hat{\theta}$由查表法求解<br>代码见<a href="https://github.com/isLouisHsu/isLouisHsu.github.io/tree/Hexo/source/_posts//参数估计的几种方法/temp.py" target="_blank" rel="noopener">仓库</a></li></ul></blockquote><ul><li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.42$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.56$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\Rightarrow$ $\hat{\theta} = 0.50$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$ $\Rightarrow$ $\hat{\theta} = 0.70$<br><img src="/2018/11/19/Parameter-Estimation/MAP_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p></li></ul><blockquote><p>结论</p><ul><li>由图$1, 2, 3$，可以看到当$\theta_0$偏移$0.7$时，$MAP$结果也相应偏移；</li><li>由图$2, 4, 5$，可以看到当$\sigma_{\theta_0}^2$越小，即越确定先验概率分布时，$MAP$结果也越趋向于先验概率分布。</li></ul></blockquote><h2 id="贝叶斯估计-1"><a href="#贝叶斯估计-1" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>先验条件为正态分布</p><script type="math/tex; mode=display">\theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)</script><script type="math/tex; mode=display">p(\theta | D)= a · p(\theta) \prod_{i=1}^N p(x_i | \theta)= a · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}} · \theta ^7 (1 - \theta) ^3</script><blockquote><p>参数$a$使用<code>scipy.integrate.quad</code>求解</p></blockquote><p>选取不同先验条件时的$L(\theta | D)$图像，用于对比</p><ul><li><p>$\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.3&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.7&#32;sigma0_0.1.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_0.01.png" alt="对比图"></p></li><li><p>$\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$<br><img src="/2018/11/19/Parameter-Estimation/BE_theta0_0.5&#32;sigma0_1.0.png" alt="对比图"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Clustering</title>
      <link href="/2018/11/16/Clustering/"/>
      <url>/2018/11/16/Clustering/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。</p><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="距离度量方法"><a href="#距离度量方法" class="headerlink" title="距离度量方法"></a>距离度量方法</h2><p>机器学习中距离度量方法有很多，以下简单介绍几种。</p><blockquote><p><a href="https://blog.csdn.net/taotiezhengfeng/article/details/80492128" target="_blank" rel="noopener">机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客</a><br><a href="https://blog.csdn.net/u014782458/article/details/58180885" target="_blank" rel="noopener">算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦123的博客 - CSDN博客 </a></p></blockquote><p>定义两个$n$维向量</p><script type="math/tex; mode=display">x = [x_1, x_2, ..., x_n]^T</script><script type="math/tex; mode=display">y = [y_1, y_2, ..., y_n]^T</script><ul><li><p>曼哈顿距离<code>(Manhattan Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_1 = \sum_i |x_i - y_i|</script></li><li><p>欧氏距离<code>(Euclidean Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_2 = \sqrt{\sum_i (x_i - y_i)^2}</script></li><li><p>闽可夫斯基距离<code>(Minkowski Distance)</code></p><script type="math/tex; mode=display">  d = || x - y ||_p = \left(\sum_i | x_i - y_i |^{p} \right)^{\frac{1}{p}}</script><p>  当$p$取$1$时为曼哈顿距离，取$2$时为欧式距离。</p></li><li><p>余弦距离<code>(Cosine)</code></p><script type="math/tex; mode=display">  d = \frac{x^T y}{||x||_2 ||y||_2} = \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}}</script><blockquote><p>突然想到为什么向量的夹角余弦是怎么来的，高中学习一直背的公式，现在给一下证明。<br>证明：向量的夹角公式<br><img src="/2018/11/16/Clustering/cosine_distance.png" alt="cosine_distance"></p><p>从余弦定理(余弦定理用几何即可)出发，有</p><script type="math/tex; mode=display">\cos \theta = \frac{a^2+b^2-c^2}{2ab}</script><p>其中</p><script type="math/tex; mode=display">||\vec{a}|| = \sqrt{x_1^2 + y_1^2}</script><script type="math/tex; mode=display">||\vec{b}|| = \sqrt{x_2^2 + y_2^2}</script><script type="math/tex; mode=display">||\vec{c}|| = \sqrt{(x_1 - x_2)^2 + (x_2 - y_2)^2}</script><p>故</p><script type="math/tex; mode=display">\cos \theta = \frac  {(\sqrt{x_1^2 + y_1^2})^2 + (\sqrt{x_2^2 + y_2^2})^2 - (\sqrt{(x_1 - x_2)^2 + (x_2 - y_2))^2}}  {2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}</script><script type="math/tex; mode=display">= \frac  {x_1 x_2 + y_1 y_2}  {\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}  = \frac{a^T b}{||a||·||b||}</script></blockquote></li></ul><h2 id="hard-vs-soft-clustering"><a href="#hard-vs-soft-clustering" class="headerlink" title="hard vs. soft clustering"></a>hard vs. soft clustering</h2><ul><li>硬聚类<code>(hard clustering)</code><br>  计算的是一个硬分配<code>(hard ssignment)</code>过程,即每个样本仅仅属于一个簇。</li><li>软聚类<code>(soft clustering)</code><br>  分配过程是软的，即一个样本的分配结果是在所有簇上的一个分布，在软分配结果中，一个样本可能对多个簇都具有隶属度。</li></ul><h2 id="聚类方法的分类"><a href="#聚类方法的分类" class="headerlink" title="聚类方法的分类"></a>聚类方法的分类</h2><ul><li>划分方法<br>  <code>K-means</code>，<code>K-medoids</code>，<code>GMM</code>等。</li><li>层次方法<br>  <code>AGNES</code>，<code>DIANA</code>，<code>BIRCH</code>，<code>CURE</code>和<code>CURE-NS</code>等。</li><li>基于密度的方法<br>  <code>DBSCAN</code>，<code>OPTICS</code>，<code>DENCLUE</code>等。</li><li>其他<br>  如<code>STING</code>等。</li></ul><h1 id="常用聚类方法"><a href="#常用聚类方法" class="headerlink" title="常用聚类方法"></a>常用聚类方法</h1><h2 id="K均值-K-means"><a href="#K均值-K-means" class="headerlink" title="K均值(K-means)"></a>K均值(K-means)</h2><p>是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一，通常用于寻找次优解，再通过其他算法(如<code>GMM</code>)寻找更优的聚类结果。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>给定$N$维数据集</p><script type="math/tex; mode=display">X = [x^{(1)}, x^{(2)}, ..., x^{(M)}]</script><p>指定类别数$K$与初始中心点$\mu^{(0)}$，将样本划分到中心点距离其最近的簇中，再根据本次划分更新各簇的中心$\mu^{(t)}$，如此迭代直至得到最好的聚类结果。预测测试样本时，将其划分到中心点距其最近的簇，也可通过<code>KNN</code>等方法。</p><p>一般使用欧式距离度量样本到各中心点的距离，也可选择余弦距离等，这也是<code>K-means</code>算法的关键</p><script type="math/tex; mode=display">D(x^{(i)}, \mu_k) = || x^{(i)} - \mu_k ||_2^2</script><p>定义损失函数为</p><script type="math/tex; mode=display">J(\Omega) = \sum_i \sum_k r^{(i)}_k D(x^{(i)}, \mu_k)</script><p>其中</p><script type="math/tex; mode=display">r^{(i)}_k = \begin{cases}    1 & x^{(i)} \in C_k \\    0 & otherwise\end{cases}</script><p>或表示为</p><script type="math/tex; mode=display">r^{(i)} = [0, ..., 1_k, ..., 0]^T</script><p>在迭代过程中，损失函数的值不断下降，优化目标为</p><script type="math/tex; mode=display">\min J(\Omega)</script><h3 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h3><ol><li>随机选取$K$个中心点；</li><li>遍历所有数据，计算每个点到各中心点的距离；</li><li>将每个数据划分到最近的中心点中；</li><li>计算每个聚类的平均值，作为新的中心点；</li><li>重复步骤2-步骤4，直到这k个中线点不再变化(收敛)，或执行了足够多的迭代；</li></ol><p><code>K-means</code>更新迭代过程如下图<br><img src="/2018/11/16/Clustering/kmeans_example.gif" alt="kmeans_example"></p><h3 id="缺点与部分解决方法"><a href="#缺点与部分解决方法" class="headerlink" title="缺点与部分解决方法"></a>缺点与部分解决方法</h3><ul><li>局部最优</li><li>初值敏感<br>  初始点的选择会影响<code>K-means</code>聚类的结果，即可能会陷入局部最优解，如下图<br>  <img src="/2018/11/16/Clustering/k_means_init.png" alt="k_means_init"><br>  可通过如下方法解决<ul><li>多次选择初始点运行<code>K-means</code>算法，选择最优的作为输出结果；</li><li><code>K-means++</code></li></ul></li><li>需要定义<code>mean</code>，对于标称型<code>(categorical)</code>数据不适用</li><li>需要给定聚类簇数目$K$<br>  这里给出一种选择簇数目的方法，选择多个$K$值进行聚类，计算代价函数，做成折线图后如下，可以看到在$K=3$处损失值的变化率出现较大变化，则可选择簇的数目为$3$。<br>  <img src="/2018/11/16/Clustering/k_means_choose_K.png" alt="k_means_choose_K"></li><li>噪声数据干扰大</li><li>对于非凸集<code>(non-convex)</code>数据无能为力<br>  谱聚类可解决非凸集数据的聚类问题。</li></ul><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><ul><li><code>K-means++</code><br>  改进初始点选择方法，第$1$个中心点随机选择；之后的初始中心点根据前面选择的中心点决定，若已选取$n$个初始聚类中心$(0&lt;n&lt;K)$，选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。</li><li><code>ISODATA</code><br>  思想：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别.</li><li><code>Kernel K-means</code><br>  参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类。</li></ul><h3 id="类似的算法"><a href="#类似的算法" class="headerlink" title="类似的算法"></a>类似的算法</h3><p>与<code>K-means</code>类似的算法有很多，例如</p><ul><li><code>K-medoids</code><br>  <code>K-means</code>的取值范围可以是连续空间中的任意值，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。<code>K-medoids</code>的取值是数据样本范围中的样本，且可应用在非数值型数据样本上。</li><li><code>k-medians</code><br>  $K$中值，选择中位数更新各簇的中心点。</li><li><code>K-centers</code><br>  <a href="https://www.bjdxs.com/xueshu/28151.html" target="_blank" rel="noopener">混合类型数据的K-Centers聚类算法/The K-Centers Clustering Algorithm for Categorical and Mixe</a></li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-1-kmeans/KMeans.py" target="_blank" rel="noopener">@Github: K-Means</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">class KMeans():</span><br><span class="line">    def __init__(self, n_cluster, mode):</span><br><span class="line">        self.n_cluster = n_cluster  # 簇的个数</span><br><span class="line">        self.mode = mode            # 距离度量方式</span><br><span class="line">        self.centroids = None       # 簇的中心</span><br><span class="line">        self.loss = float(&apos;inf&apos;)    # 优化目标值</span><br><span class="line">        plt.ion()</span><br><span class="line">    def fit(self, X, max_iter=5, min_move=0.1, display=False):</span><br><span class="line">        def initializeCentroids():</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            选择初始点</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroid = np.zeros(shape=(self.n_cluster, X.shape[1])) # 保存选出的点</span><br><span class="line">            pointIdx = []                                           # 保存已选出的点的索引</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                idx = np.random.randint(0, X.shape[0])              # 随机选择一个点</span><br><span class="line">                while idx in pointIdx:                              # 若该点已选出，则丢弃重新选择</span><br><span class="line">                    idx = np.random.randint(0, X.shape[0])</span><br><span class="line">                pointIdx.append(idx)</span><br><span class="line">                centroid[n] = X[idx]</span><br><span class="line">            return centroid</span><br><span class="line">        def dist2Centroids(x, centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            返回向量x到k个中心点的距离值</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            d = np.zeros(shape=(self.n_cluster,))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                d[n] = mathFunc.distance(x, centroids[n], mode)</span><br><span class="line">            return d</span><br><span class="line">        def nearestInfo(centroids, mode):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            每个点最近的簇中心索引、距离</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            ctIdx = -np.ones(shape=(X.shape[0],), dtype=np.int8)    # 每个点最近的簇中心索引，初始化为-1，可作为异常条件</span><br><span class="line">            ctDist = np.ones(shape=(X.shape[0],), dtype=np.float)   # 每个点到最近簇中心的距离</span><br><span class="line">            for i in range(X.shape[0]):</span><br><span class="line">                dists = dist2Centroids(X[i], centroids, mode)</span><br><span class="line">                if mode == &apos;Euclidean&apos;: ctIdx[i] = np.argmin(dists)</span><br><span class="line">                elif mode == &apos;Cosine&apos;:  ctIdx[i] = np.argmax(dists)</span><br><span class="line">                ctDist[i] = dists[ctIdx[i]]             # 保存最相似的距离度量，用于计算loss</span><br><span class="line">            return ctIdx, ctDist</span><br><span class="line">        def updateCentroids(ctIdx):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            更新簇中心</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            centroids = np.zeros(shape=(self.n_cluster, X.shape[1]))</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                X_ = X[ctIdx == n]                      # 筛选出离簇中心Cn最近的样本点</span><br><span class="line">                centroids[n] = np.mean(X_, axis=0)      # 根据筛选出的样本点更新中心值</span><br><span class="line">            return centroids</span><br><span class="line">        def loss(dist):</span><br><span class="line">            return np.mean(dist**2)</span><br><span class="line">        # -----------------------------------------</span><br><span class="line">        loss_min = float(&apos;inf&apos;)                         # 最优分类时的损失值，最小</span><br><span class="line">        n_iter = 0     </span><br><span class="line">        while n_iter &lt; max_iter:                        # 每次迭代选择不同的初始点</span><br><span class="line">            n_iter += 1; isDone = False                 # 表示本次迭代是否已收敛</span><br><span class="line">            centroids_tmp = initializeCentroids()       # 选择本次迭代的初始点</span><br><span class="line">            loss_last = float(&apos;inf&apos;)                    # 本次迭代中，中心点更新前的损失值</span><br><span class="line">            n_update = 0                                # 本次迭代的更新次数计数</span><br><span class="line">            while not isDone:</span><br><span class="line">                n_update += 1</span><br><span class="line">                ctIdx, ctDist = nearestInfo(centroids_tmp, mode=self.mode)</span><br><span class="line">                centroids_tmp = updateCentroids(ctIdx)  # 更新簇中心</span><br><span class="line">                # --- 可视化 ---</span><br><span class="line">                if (display==True) and (X.shape[1] == 2):</span><br><span class="line">                    plt.ion()</span><br><span class="line">                    plt.figure(n_iter); plt.cla()</span><br><span class="line">                    plt.scatter(X[:, 0], X[:, 1], c=ctIdx)</span><br><span class="line">                    plt.scatter(centroids_tmp[:, 0], centroids_tmp[:, 1], c=&apos;r&apos;)</span><br><span class="line">                    plt.pause(0.5)</span><br><span class="line">                # -------------</span><br><span class="line">                loss_now = loss(ctDist); moved = np.abs(loss_last - loss_now)</span><br><span class="line">                if moved &lt; min_move:                    # 若移动过小，则本次迭代收敛</span><br><span class="line">                    isDone = True</span><br><span class="line">                    print(&apos;第%d次迭代结束，中心点更新%d次&apos; % (n_iter, n_update))</span><br><span class="line">                else: loss_last = loss_now</span><br><span class="line">            if loss_now &lt; loss_min:</span><br><span class="line">                self.centroids = centroids_tmp          # 保存损失最小的模型(最优)</span><br><span class="line">                loss_min = loss_now</span><br><span class="line">                # print(&apos;聚类结果已更新&apos;)</span><br><span class="line">        self.loss = loss_min</span><br><span class="line">        print(&apos;=========== 迭代结束 ===========&apos;)</span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        各个样本的最近簇中心索引</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        labels = -np.ones(shape=(X.shape[0],), dtype=np.int)    # 初始化为-1，可用作异常条件</span><br><span class="line">        for i in range(X.shape[0]):</span><br><span class="line">            dists_i = np.zeros(shape=(self.n_cluster,))         # 保存X[i]到中心点Cn的距离</span><br><span class="line">            for n in range(self.n_cluster):</span><br><span class="line">                dists_i[n] = mathFunc.distance(X[i], self.centroids[n], mode=self.mode)</span><br><span class="line">            if self.mode == &apos;Euclidean&apos;:</span><br><span class="line">                labels[i] = np.argmin(dists_i)</span><br><span class="line">            elif self.mode == &apos;Cosine&apos;:</span><br><span class="line">                labels[i] = np.argmax(dists_i)</span><br><span class="line">        return labels</span><br></pre></td></tr></table></figure></p><p>簇数的选择代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def chooseBestK(X, start, stop, step=1, mode=&apos;Euclidean&apos;):</span><br><span class="line">    Ks = np.arange(start, stop + 1, step, dtype=np.int) # 待选择的K</span><br><span class="line">    Losses = np.zeros(shape=Ks.shape)                   # 保存不同K值时的最小损失值</span><br><span class="line">    for k in range(1, Ks.shape[0] + 1):                 # 对于不同的K，训练模型，计算损失</span><br><span class="line">        print(&apos;K = %d&apos;, k)</span><br><span class="line">        estimator = KMeans(n_cluster=k, mode=mode)</span><br><span class="line">        estimator.fit(X, max_iter=10, min_move=0.01, display=False)</span><br><span class="line">        Losses[k - 1] = estimator.loss</span><br><span class="line">    plt.ioff()</span><br><span class="line">    plt.figure(); plt.xlabel(&apos;n_clusters&apos;); plt.ylabel(&apos;loss&apos;)</span><br><span class="line">    plt.plot(Ks, Losses)                                # 做出loss-K曲线</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="均值漂移-Meanshift"><a href="#均值漂移-Meanshift" class="headerlink" title="均值漂移(Meanshift)"></a>均值漂移(Meanshift)</h2><p>本质是一个迭代的过程，能够在一组数据的密度分布中寻找到局部极值，比较稳定，而且是无参密度估计(不需要事先知道样本数据的概率密度分布函数，完全依靠对样本点的计算)，而且在采样充分的情况下，一定会收敛，即可以对服从任意分布的数据进行密度估计。</p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>有一个滑动窗口的思想，即利用当前中心点一定范围内(通常为球域)的点迭代更新中心点，重复移动窗口，直到满足收敛条件。简单的说，<code>Meanshift</code>就是沿着密度上升的方向寻找同属一个簇的数据点。</p><p>定义点$x_0$的$\epsilon$球域如下</p><script type="math/tex; mode=display">S_h(x_0) = \{ x | (x - x_0)^T (x - x_0) \leq \epsilon \}</script><p>若有$n$个点$(x_1, …, x_n)$落在中心点$ptCentroid$的邻域内，其分布如图<br><img src="/2018/11/16/Clustering/DBSCAN4.jpg" alt="DBSCAN4"></p><p>则偏移向量计算方式为</p><script type="math/tex; mode=display">vecShift = \frac{1}{n} \sum_{i=1}^n (x_i - ptCentroid)</script><p>中心点更新公式为</p><script type="math/tex; mode=display">ptCentroid := ptCentroid + vecShift</script><blockquote><p>展开后可发现，其更新公式即</p><script type="math/tex; mode=display">vecShift= \frac{1}{n} \sum_{i=1}^n x_i - ptCentroid</script><script type="math/tex; mode=display">ptCentroid :=  \frac{1}{n} \sum_{i=1}^n x_i</script></blockquote><p><img src="/2018/11/16/Clustering/DBSCAN3.jpg" alt="DBSCAN3"></p><p>一个滑动窗口的动态更新过程如下图<br><img src="/2018/11/16/Clustering/meanshift_example1.gif" alt="meanshift_example1"><br>初始化多个滑动窗口进行<code>MeanShift</code>算法，其更新过程如下，其中每个黑点代表滑动窗口的质心，每个灰点代表一个数据点<br><img src="/2018/11/16/Clustering/meanshift_example2.gif" alt="meanshift_example2"></p><h3 id="高斯权重"><a href="#高斯权重" class="headerlink" title="高斯权重"></a>高斯权重</h3><p>基本思想是，距离当前中心点近的向量对更新结果权重大，而远的权重小，可减小远点的干扰，如下图，$vecShift_2$为高斯权重下的偏移向量<br><img src="/2018/11/16/Clustering/DBSCAN5.jpg" alt="DBSCAN5"></p><p>其偏移向量计算方式为</p><script type="math/tex; mode=display">vecShift = \frac{1}{n} \sum_{i=1}^n w_i · (x_i - ptCentroid)</script><script type="math/tex; mode=display">w_i = \frac{\kappa(x_i - ptCentroid)}{\sum_j \kappa(x_j - ptCentroid)}</script><p>其中</p><script type="math/tex; mode=display">\kappa(z) = \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{||z||^2}{2\sigma^2} \right)</script><p>中心点更新公式仍然为</p><script type="math/tex; mode=display">ptCentroid := ptCentroid + vecShift</script><blockquote><p>展开也可得到</p><script type="math/tex; mode=display">ptCentroid := \frac{\sum_{i=1}^n w_i x_i}{\sum_j w_j}</script></blockquote><h3 id="计算步骤-1"><a href="#计算步骤-1" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$\epsilon_0$，终止条件参数$\epsilon_1$，簇合并参数$\epsilon_2$，并指定样本距离度量方式，目标为将其划分为$K$个簇。</p><ol><li>初始化：<ul><li>在样本集中随机选择$K_0(K_0 \gg K)$个样本作为初始中心点，以邻域大小为$\epsilon_0$建立滑动窗口；</li><li>各个样本初始化一个标记向量，用于记录被各类别访问的次数；</li></ul></li><li>以单个滑动窗口分析，记其中心点为$ptCentroid$，找到滑动窗口内的所有点，记作集合$M$，认为这些点属于该滑动窗口所属的簇类别，同时，这些点被该簇访问的次数$+1$；</li><li>以$ptCentroid$为中心，计算其到集合$M$中各个元素的向量，以这些向量计算得到偏移向量$vecShift$；</li><li>更新中心点：$ptCentroid = ptCentroid + vecShift$，即滑动窗口沿着$vecShift$方向移动，距离为$||vecShift||$；</li><li>重复步骤$2-4$，直到$||vecShift||&lt;\epsilon_1$，保存当前中心点；</li><li>如果收敛时当前簇$ptCentroid$与其它已经存在的簇的中心的距离小于阈值$\epsilon_2$，那么这两个簇合并。否则，把当前簇作为新的簇类，增加$1$类；</li><li>重复迭代直到所有的点都被标记访问；</li><li>根据每个样本被各簇的访问频率，取访问频率最大的那个簇类别作为当前点集的所属类。</li></ol><blockquote><p>即不同类型的滑窗沿着密度上升的方向进行移动，对各样本点进行标记，最后将样本划分为标记最多的类别；当两类非常接近时，合并为一类。</p></blockquote><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p71_meanshift.py" target="_blank" rel="noopener">@Github: MeanShift</a></p><p>先定义了窗格对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SlidingWindow</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        centroid: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        epsilon: &#123;float&#125; 滑动窗格大小，为半径的平方</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125; 高斯核函数的参数</span></span><br><span class="line"><span class="string">        label: &#123;int&#125; 该窗格的标记</span></span><br><span class="line"><span class="string">        X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        containIdx: &#123;ndarray(n_contain,)&#125; 窗格内包含点的索引</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, centroid, epsilon, sigma, label, X)</span>:</span></span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.label = label</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">k</span><span class="params">(self, z)</span>:</span></span><br><span class="line">        <span class="string">""" 高斯核函数</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            z: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - \kappa(z) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \exp \left( - \frac&#123;||z||^2&#125;&#123;2\sigma^2&#125; \right)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        norm = np.linalg.norm(z)</span><br><span class="line">        <span class="keyword">return</span> np.exp(- <span class="number">0.5</span> * (norm / self.sigma)**<span class="number">2</span>) / np.sqrt(<span class="number">2</span>*np.pi)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 更新滑动窗格的中心点和所包含点</span></span><br><span class="line"><span class="string">        Returns: &#123;float&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dshift = self.shift(X)</span><br><span class="line">        self.containIdx = self.updateContain(X)</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shift</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 移动窗格</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vecShift: &#123;ndarray(n_features,)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dshift: &#123;float&#125; 移动的距离</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        n_contain = self.containIdx.shape[<span class="number">0</span>]</span><br><span class="line">        contain_weighted_sum = np.zeros(shape=(n_features, ))</span><br><span class="line">        weight_sum = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 按包含的点进行移动</span></span><br><span class="line">        <span class="keyword">for</span> i_contain <span class="keyword">in</span> range(n_contain):</span><br><span class="line">            vector = X[self.containIdx[i_contain]] - self.centroid</span><br><span class="line">            weight = self.k(vector)</span><br><span class="line">            contain_weighted_sum += weight*X[self.containIdx[i_contain]]</span><br><span class="line">            weight_sum += weight</span><br><span class="line">        centroid = contain_weighted_sum / weight_sum </span><br><span class="line">        <span class="comment"># 计算移动的距离   </span></span><br><span class="line">        dshift = np.linalg.norm(self.centroid - centroid)</span><br><span class="line">        self.centroid = centroid</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateContain</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 更新窗格内的点索引</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - 用欧式距离作为度量</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        d = <span class="keyword">lambda</span> x_i, x_j: np.linalg.norm(x_i - x_j)</span><br><span class="line">        n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">        containIdx = np.array([], dtype=<span class="string">'int'</span>)</span><br><span class="line">        <span class="keyword">for</span> i_samples <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">if</span> d(X[i_samples], self.centroid) &lt; self.epsilon:</span><br><span class="line">                containIdx = np.r_[containIdx, i_samples]</span><br><span class="line">        <span class="keyword">return</span> containIdx</span><br></pre></td></tr></table></figure></p><p>聚类算法如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanShift</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_clusters: &#123;int&#125; 划分簇的个数</span></span><br><span class="line"><span class="string">        n_windows: &#123;int&#125; 滑动窗格的个数</span></span><br><span class="line"><span class="string">        epsilon: &#123;float&#125; 滑动窗格的大小</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125; &#123;float&#125; 高斯核参数</span></span><br><span class="line"><span class="string">        thresh: &#123;float&#125; 若两个窗格中心距离小于thresh，则合并两类簇</span></span><br><span class="line"><span class="string">        min_move: &#123;float&#125; 终止条件</span></span><br><span class="line"><span class="string">        windows: &#123;list[class SlidingWindow()]&#125;</span></span><br><span class="line"><span class="string">    Note:</span></span><br><span class="line"><span class="string">        - 假设所有点均被窗格划过</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_clusters, n_windows=<span class="number">-1</span>, epsilon=<span class="number">0.5</span>, sigma=<span class="number">2</span>, thresh=<span class="number">1e-2</span>, min_move=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.n_windows = <span class="number">5</span>*n_clusters <span class="keyword">if</span> (n_windows == <span class="number">-1</span>) <span class="keyword">else</span> n_windows</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.thresh = thresh</span><br><span class="line">        self.min_move = min_move</span><br><span class="line">        self.windows = []</span><br><span class="line">        self.centroids = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        <span class="comment"># 创建窗格</span></span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            idx = np.random.randint(n_samples)</span><br><span class="line">            window = SlidingWindow(X[idx], self.epsilon,</span><br><span class="line">                            self.sigma, i_windows, X)</span><br><span class="line">            <span class="comment"># 将各窗格包含的点标记</span></span><br><span class="line">            n_contain = window.containIdx.shape[<span class="number">0</span>]</span><br><span class="line">            self.windows.append(window)</span><br><span class="line"></span><br><span class="line">        dshift = float(<span class="string">'inf'</span>)   <span class="comment"># 初始化为无穷大</span></span><br><span class="line">        plt.figure(); plt.ion()</span><br><span class="line">        <span class="keyword">while</span> dshift &gt; self.min_move:</span><br><span class="line">            <span class="comment"># ------ 做图显示 ------</span></span><br><span class="line">            plt.cla()</span><br><span class="line">            plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=<span class="string">'b'</span>)</span><br><span class="line">            <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">                centroid = self.windows[i_windows].centroid</span><br><span class="line">                plt.scatter(centroid[<span class="number">0</span>], centroid[<span class="number">1</span>], c=<span class="string">'r'</span>)</span><br><span class="line">            plt.pause(<span class="number">0.5</span>)</span><br><span class="line">            <span class="comment"># ---------------------</span></span><br><span class="line">            dshift = self.step(X)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 合并窗格</span></span><br><span class="line">        dists = np.zeros(shape=(self.n_windows, self.n_windows))</span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            <span class="keyword">for</span> j_windows <span class="keyword">in</span> range(i_windows):</span><br><span class="line">                centroid_i = self.windows[i_windows].centroid</span><br><span class="line">                centroid_j = self.windows[j_windows].centroid</span><br><span class="line">                dists[i_windows, j_windows] = np.linalg.norm(centroid_i-centroid_j)</span><br><span class="line">                dists[j_windows, i_windows] = dists[i_windows, j_windows]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获得距离相近索引</span></span><br><span class="line">        index = np.where(dists&lt;self.thresh)</span><br><span class="line">        <span class="comment"># 用于标记类别</span></span><br><span class="line">        winlabel = np.zeros(shape=(self.n_windows,), dtype=<span class="string">'int'</span>)</span><br><span class="line">        label = <span class="number">1</span>; winlabel[<span class="number">0</span>] = label</span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            idx_row = index[<span class="number">0</span>][i_windows]</span><br><span class="line">            idx_col = index[<span class="number">1</span>][i_windows]</span><br><span class="line">            <span class="comment"># 若其中一个点被标记，则将令一个点并入该类</span></span><br><span class="line">            <span class="keyword">if</span> winlabel[idx_row]!=<span class="number">0</span>:</span><br><span class="line">                winlabel[idx_col] = winlabel[idx_row]</span><br><span class="line">            <span class="keyword">elif</span> winlabel[idx_col]!=<span class="number">0</span>:</span><br><span class="line">                winlabel[idx_row] = winlabel[idx_col]</span><br><span class="line">            <span class="comment"># 否则新创建类别</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label += <span class="number">1</span></span><br><span class="line">                winlabel[idx_row] = label</span><br><span class="line">                winlabel[idx_col] = label</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将标签一样的窗格合并</span></span><br><span class="line">        labels = list(set(winlabel))                            <span class="comment"># 去重后的标签</span></span><br><span class="line">        n_labels = len(labels)                                  <span class="comment"># 标签种类数</span></span><br><span class="line">        self.centroids = np.zeros(shape=(n_labels, n_features)) <span class="comment"># 记录最终聚类中心</span></span><br><span class="line">        <span class="keyword">for</span> i_labels <span class="keyword">in</span> range(n_labels):</span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">                <span class="keyword">if</span> winlabel[i_windows] == labels[i_labels]:</span><br><span class="line">                    self.centroids[i_labels] += self.windows[i_windows].centroid</span><br><span class="line">                    cnt += <span class="number">1</span></span><br><span class="line">            self.centroids[i_labels] /= cnt                    <span class="comment"># 取同类窗格中心点的均值</span></span><br><span class="line">        <span class="keyword">return</span> self.centroids</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" update all sliding windows</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dshift: \sum_i^&#123;n_windows&#125; dshift_&#123;i&#125;</span></span><br><span class="line"><span class="string">        """</span> </span><br><span class="line">        dshift = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i_windows <span class="keyword">in</span> range(self.n_windows):</span><br><span class="line">            dshift += self.windows[i_windows].step(X)</span><br><span class="line">            <span class="comment"># label the points</span></span><br><span class="line">            n_contain = self.windows[i_windows].containIdx.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> dshift</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" 简单的用近邻的方法求</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        dists = np.zeros(shape=(n_samples, self.n_clusters))</span><br><span class="line">        <span class="keyword">for</span> i_samples <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> i_clusters <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                dists[i_samples, i_clusters] = np.linalg.norm(X[i_samples]-self.centroids[i_clusters])</span><br><span class="line">        <span class="keyword">return</span> np.argmin(dists, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h2 id="谱聚类-Spectral-Clustering"><a href="#谱聚类-Spectral-Clustering" class="headerlink" title="谱聚类(Spectral Clustering)"></a>谱聚类(Spectral Clustering)</h2><p>谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用，比起传统的<code>K-Means</code>算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多。</p><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><blockquote><p><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">谱聚类（spectral clustering）原理总结 - 刘建平Pinard - 博客园 </a></p></blockquote><h4 id="无向权重图"><a href="#无向权重图" class="headerlink" title="无向权重图"></a>无向权重图</h4><p>我们用点的集合$V$和边的集合$E$描述一个图，即$G(V, E)$，其中$V$即数据集中的点</p><script type="math/tex; mode=display">V = [v_1, v_2, ..., v_n]</script><p>而点$v_i, v_j$间连接权值$w_{ij}$组成邻接矩阵$W$，由于为无向图，故满足$w_{ij}=w_{ji}$</p><script type="math/tex; mode=display">W = \left[    \begin{matrix}        w_{11} & ... & w_{1n} \\        ... & ... & ... \\        w_{n1} & ... & w_{nn} \\    \end{matrix}    \right]</script><p>对于图中的任意一个点$v_i$，定义其度$d_i$为</p><script type="math/tex; mode=display">d_i = \sum_{j=1}^n w_{ij}</script><p>则我们可以得到一个度矩阵$D=diag(d_1, …, d_n)$</p><script type="math/tex; mode=display">D = \left[        \begin{matrix}            d_1 & & \\            & ... & \\             & & d_n\\        \end{matrix}    \right]</script><p>除此之外，对于$V$中子集$V_{sub} \subset V$，定义子集$V_{sub}$点的个数为</p><script type="math/tex; mode=display">|V_{sub}| := n_{sub}</script><p>另外，定义该子集中点的度之和为</p><script type="math/tex; mode=display">vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script><h4 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h4><p>上面讲到的邻接矩阵$W$可以指定权值，但对于数据量庞大的数据集，这显然不是一个$wise$的选择。我们可以用相似矩阵$S$来获得邻接矩阵$W$，基本思想是，距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高。</p><p>构建邻接矩阵$W$的方法有三类：$\epsilon$-邻近法，$K$邻近法和全连接法，定义距离</p><script type="math/tex; mode=display">d_{ij} = ||x^{(i)} - x^{(j)}||_2^2</script><ul><li><p>$\epsilon$-邻近法<br>  设置距离阈值$\epsilon$，用欧式距离度量两点的距离$d_{ij}$，然后通过下式确定邻接权值$w_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      0 & d_{ij} > \epsilon \\      \epsilon & otherwise  \end{cases}</script><blockquote><p>两点间的权重要不就是$\epsilon$，要不就是0，距离远近度量很不精确，因此在实际应用中，我们很少使用$\epsilon$-邻近法。</p></blockquote></li><li><p>$K$邻近法</p><ul><li><p>第一种<br>  只要一个点在另一个点的$K$近邻中，则保留$d_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　or　x^{(j)} \in KNN(x^{(i)}) \\      0 & otherwise  \end{cases}</script></li><li><p>第二种<br>  互为$K$近邻时保留$d_{ij}$</p><script type="math/tex; mode=display">  w_{ij} = \begin{cases}      \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)})　and　x^{(j)} \in KNN(x^{(i)}) \\      0 & otherwise  \end{cases}</script></li></ul></li><li><p>全连接法<br>  可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和<code>Sigmoid</code>核函数。最常用的是高斯核函数<code>RBF</code>，此时相似矩阵和邻接矩阵相同</p><script type="math/tex; mode=display">  w_{ij} = \exp \left( -\frac{d_{ij}}{2\sigma^2} \right)</script></li></ul><h4 id="拉普拉斯矩阵-Graph-Laplacians"><a href="#拉普拉斯矩阵-Graph-Laplacians" class="headerlink" title="拉普拉斯矩阵(Graph Laplacians)"></a>拉普拉斯矩阵(Graph Laplacians)</h4><p>定义</p><script type="math/tex; mode=display">L = D - W</script><p>正则化的拉普拉斯矩阵为</p><script type="math/tex; mode=display">L = D^{-1} (D - W)</script><p>具有的性质如下</p><ol><li>$L^T = L$</li><li>其特征值均为实数，即$\lambda_i \in \mathbb{R}$</li><li>正定性：$\lambda_i \geq 0$</li><li><p>对于任意向量$x$，都有</p><script type="math/tex; mode=display">x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script><blockquote><p>证明：</p><script type="math/tex; mode=display">x^T L x = x^T D x - x^T W x = \sum_i d_i x_i^2 - \sum_{ij} w_{ij} x_i x_j</script><script type="math/tex; mode=display">= \frac{1}{2} \left[ \sum_i d_i x_i^2 - 2\sum_{ij} w_{ij} x_i x_j + \sum_j d_j x_j^2 \right]</script><p>其中$ d_i = \sum_j w_{ij} $，所以</p><script type="math/tex; mode=display">x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2</script></blockquote></li></ol><h4 id="无向图的切图"><a href="#无向图的切图" class="headerlink" title="无向图的切图"></a>无向图的切图</h4><h5 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h5><p>我们希望把一张无向图$G(V, E)$按一定方法切成多个子图，各个子图间无连接，每个子图的点集为$V_1, …, V_K$，满足</p><ul><li>$\bigcup_{k=1}^K V_k = V$</li><li>$V_i \cap V_j = \emptyset$</li></ul><p>定义两个子图点集合$A, B$之间的切图权重为</p><script type="math/tex; mode=display">W(A, B) = \sum_{i \in A, j \in B} w_{ij}</script><blockquote><p>共有$n_A × n_B$个权值作累加</p></blockquote><p>那么对于$K$个子图点的集合$V_1, …, V_K$，定义切图为</p><script type="math/tex; mode=display">cut(V_1, ..., V_K) = \frac{1}{2} \sum_{i=1}^K cut(V_i, \overline{V_i})</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><p>其中$\overline{V_i}$表示$V_i$的补集，或者</p><script type="math/tex; mode=display">\overline{V_i} = \bigcup_{k \neq i} V_k</script><p>通过最小化$cut(V_1, …, V_K)$使子图内权重和大，而子图间权重和小。但是这种方法存在问题，如下图<br><img src="/2018/11/16/Clustering/cut.jpg" alt="cut"></p><p>选择一个权重最小的边缘的<strong>点</strong>，比如$C$和$H$之间进行$cut$，这样可以最小化$cut(V_1, …, V_K)$，但是却不是最优的切图。</p><p>为解决上述问题，需要对每个子图的规模做出限定，以下介绍两种切图方式。</p><h5 id="Ratio-Cut"><a href="#Ratio-Cut" class="headerlink" title="Ratio Cut"></a>Ratio Cut</h5><p>不仅考虑最小化$cut(V_1, …, V_K)$，而且最大化每个子图的点个数，即</p><script type="math/tex; mode=display">RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{|V_k|}</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote><ul><li>$W(V_k, \overline{V_k}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}$</li><li>$|V_k| = n_k$</li></ul></blockquote><p>如果按照遍历的方法求解，由前面分析，$W(V_k, \overline{V_k})$需计算$n_{V_k} × n_{\overline{V_k}}$次累加，计算量庞大，那么如何求解呢？</p><p>定义指示向量$h_k$，其构成矩阵$H$</p><script type="math/tex; mode=display">H = [ h_1, ..., h_k, ..., h_K]</script><p>其中</p><script type="math/tex; mode=display">h_k = \left[h_{k1}, h_{k2}, , ..., h_{kM} \right]^T</script><script type="math/tex; mode=display">h_{ki} = \begin{cases}    \frac{1}{\sqrt{|V_k|}} & x^{(i)}\in V_k \\    0 & otherwise\end{cases}</script><blockquote><p>$h_k$为单位向量，且两两正交</p><script type="math/tex; mode=display">h_i^T h_j =       \begin{cases}          \sum_{|V_i|} \frac{1}{|V_i|} = |V_i| · \frac{1}{|V_i|} = 1 & i = j \\          0 & i \neq j      \end{cases}</script></blockquote><p>那么由拉式矩阵性质$4$</p><script type="math/tex; mode=display">h_k^T L h_k = \frac{1}{2} \sum_{i,j} w_{ij} (h_{ki} - h_{kj})^2</script><script type="math/tex; mode=display">= \frac{1}{2}     [        \sum_{i \in V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 +         \sum_{i \notin V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 +</script><script type="math/tex; mode=display">        \sum_{i \in V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 +         \sum_{i \notin V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2    ]</script><script type="math/tex; mode=display">= \frac{1}{2}    [        \sum_{i \in V_k, j \in V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - \frac{1}{\sqrt{|V_k|}})^2 +         \sum_{i \notin V_k, j \in V_k} w_{ij} (0 - \frac{1}{\sqrt{|V_k|}})^2 +</script><script type="math/tex; mode=display">        \sum_{i \in V_k, j \notin V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - 0)^2 +        \sum_{i \notin V_k, j \notin V_k} w_{ij} (0 - 0)^2    ]</script><script type="math/tex; mode=display">= \frac{1}{2}    [        \sum_{i \notin V_k, j \in V_k} w_{ij} \frac{1}{|V_k|} +        \sum_{i \in V_k, j \notin V_k} w_{ij} \frac{1}{|V_k|}    ]</script><blockquote><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}</script></blockquote><script type="math/tex; mode=display">h_k^T L h_k = \frac{1}{2} [\frac{1}{|V_k|} cut(V_k, \overline{V_k}) + \frac{1}{|V_k|} cut(V_k, \overline{V_k})]</script><script type="math/tex; mode=display">= \frac{1}{|V_k|} cut(V_k, \overline{V_k})</script><p>推到这里就能理解为什么要定义$h_k$了</p><script type="math/tex; mode=display">RatioCut(V_1, ..., V_K)= \frac{1}{2} \sum_k h_k^T L h_k</script><p>并且</p><script type="math/tex; mode=display">h_k^T L h_k = tr(H^T L H)</script><blockquote><script type="math/tex; mode=display">H^T L H = \left[      \begin{matrix}          — & h_1^T & — \\           & ... &  \\          — & h_K^T & — \\      \end{matrix}\right]L\left[          \begin{matrix}            | & & | \\            h_1 & ... & h_K \\            | & & |        \end{matrix}    \right]</script><script type="math/tex; mode=display">= \left[      \begin{matrix}          h_1^T L h_1 & ... & h_1^T L h_K \\          ... & ... & ... \\          h_K^T L h_K & ... & h_K^T L h_K \\      \end{matrix}\right]</script></blockquote><p>所以最终优化目标为</p><script type="math/tex; mode=display">\min_H tr(H^T L H)</script><script type="math/tex; mode=display">s.t.　H^T H = I</script><blockquote><script type="math/tex; mode=display">H^T H = \left[      \begin{matrix}          h_1^T h_1 & ... & h_1^T h_K \\          ... & ... & ... \\          h_K^T h_K & ... & h_K^T h_K \\      \end{matrix}\right]</script></blockquote><p>而矩阵的正交相似变换$A = P \Lambda P^{-1}$满足</p><script type="math/tex; mode=display">tr(A) = tr(\Lambda) = \sum_i \lambda_i</script><p>故</p><script type="math/tex; mode=display">tr(H^T L H) = tr(L) = \sum_{i=1}^M \lambda_i</script><p>$\lambda_i$为矩阵$L$的特征值。</p><p>我们再进行维度规约，将维度从$M$降到$k_1$，即找到$k_1$个最小的特征值之和。</p><h5 id="N-Cut"><a href="#N-Cut" class="headerlink" title="N Cut"></a>N Cut</h5><p>推导过程与<code>RatioCut</code>完全一致，只是将分母$|V_i|$换成$vol(V_i)$</p><script type="math/tex; mode=display">NCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{vol(V_i)}</script><script type="math/tex; mode=display">cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})</script><blockquote><script type="math/tex; mode=display">vol(V_{sub}) = \sum_{i \in V_{sub}} d_i</script></blockquote><h3 id="计算步骤-2"><a href="#计算步骤-2" class="headerlink" title="计算步骤"></a>计算步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，将其划分为$K$类$(C_1, …, C_K)$</p><ol><li>根据输入的相似矩阵的生成方式构建样本的相似矩阵$S_{M×M}$；</li><li>根据相似矩阵$S$构建邻接矩阵$W_{M×M}$；</li><li>构建度矩阵$D_{M×M}$；</li><li>计算拉普拉斯矩阵$L_{M×M}$，可进行规范化$ L := D^{-1}L $；</li><li>对$L$进行特征值分解<code>(EVD)</code>，得到特征对$ (\lambda_i, \alpha_i), i=1,…,M $；</li><li>指定超参数$K_1$，选取$K_1$个最小特征值对应的特征向量组成矩阵$F_{M×K_1}$，并将其按行标准化；</li><li>以$F$的行向量作为新的样本数($k_1$维，这里也有降维操作)进行聚类，划分为$K$类，可使用<code>K-means</code>；</li><li>聚类结果即为输出结果</li></ol><p>注意是$K_1$个最小特征值对应的特征向量，别问我为什么知道。。。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p86_spectral_clustering.py" target="_blank" rel="noopener">@Github: Spectral Clustering</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpectralClustering</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        k: &#123;int&#125;, k &lt; n_samples</span></span><br><span class="line"><span class="string">        sigma: &#123;float&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        Steps:</span></span><br><span class="line"><span class="string">            - similarity matrix [W_&#123;n×n&#125;]</span></span><br><span class="line"><span class="string">            - diagonal matrix [D_&#123;n×n&#125;] is defined as</span></span><br><span class="line"><span class="string">                    D_&#123;ii&#125; = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">                                \sum_j W_&#123;ij&#125; &amp; i \neq j \\</span></span><br><span class="line"><span class="string">                                0 &amp; i = j</span></span><br><span class="line"><span class="string">                            \end&#123;cases&#125;</span></span><br><span class="line"><span class="string">            - Laplacian matrix [L_&#123;n×n&#125;], Laplacian matrix is defined as</span></span><br><span class="line"><span class="string">                    L = D - W or L = D^&#123;-1&#125; (D - W)</span></span><br><span class="line"><span class="string">            - EVD: L \alpha_i = \lambda_i \alpha_i</span></span><br><span class="line"><span class="string">            - takes the eigenvector corresponding to the largest eigenvalue as</span></span><br><span class="line"><span class="string">                    B_&#123;n×k&#125; = [\beta_1, \beta_2, ..., \beta_k]</span></span><br><span class="line"><span class="string">            - apply K-Means to the row vectors of matrix B</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k, n_clusters=<span class="number">2</span>, sigma=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">        self.kmeans = KMeans(n_clusters=n_clusters)</span><br><span class="line">        self.k = k</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        n_samples = X.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># step 1</span></span><br><span class="line">        kernelGaussian = <span class="keyword">lambda</span> z, sigma: np.exp(<span class="number">-0.5</span> * np.square(z/sigma))</span><br><span class="line">        W = np.zeros((n_samples, n_samples))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                W[i, j] = kernelGaussian(np.linalg.norm(X[i] - X[j]), self.sigma)</span><br><span class="line">                W[j, i] = W[i, j]</span><br><span class="line">        <span class="comment"># step 2</span></span><br><span class="line">        D = np.diag(np.sum(W, axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># step 3</span></span><br><span class="line">        L = D - W</span><br><span class="line">        L = np.linalg.inv(D).dot(L)</span><br><span class="line">        <span class="comment"># step 4</span></span><br><span class="line">        eigval, eigvec = np.linalg.eig(L)</span><br><span class="line">        <span class="comment"># step 5</span></span><br><span class="line">        order = np.argsort(eigval)</span><br><span class="line">        eigvec = eigvec[:, order]</span><br><span class="line">        beta = eigvec[:, :self.k]</span><br><span class="line">        <span class="comment"># step 6</span></span><br><span class="line">        self.kmeans.fit(beta)</span><br><span class="line">        <span class="keyword">return</span> self.kmeans.labels_</span><br></pre></td></tr></table></figure></p><p><img src="/2018/11/16/Clustering/spectral_clustering.png" alt="spectral_clustering"></p><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><p><code>DBSCAN(Density-Based Spatial Clustering of Applications with Noise)</code>，具有噪声的基于密度的聚类方法。假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的。</p><blockquote><p><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">DBSCAN密度聚类算法 - 刘建平Pinard - 博客园</a></p></blockquote><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>先介绍几个关于密度的概念</p><ul><li><p>$\epsilon$-邻域<br>  对于样本$x^{(i)}$，其$\epsilon$-邻域包含样本集中与$x^{(i)}$距离不大于$\epsilon$的子样本集，其样本个数记作$|N_{\epsilon}(x^{(i)})|$。</p><script type="math/tex; mode=display">  N_{\epsilon}(x^{(i)}) = \{ x^{(j)} | d_{ij} \leq \epsilon \}</script></li><li><p>核心对象<br>  对于任一样本$x^{(i)}$，若其$\epsilon$-邻域$N_{\epsilon}(x^{(i)})$至少包含$minPts$个样本，则该样本为核心对象。如图，选择若选取$\epsilon=5$，则红点均为核心对象</p></li><li>密度直达<br>  若样本$x^{(j)} \in N_{\epsilon}(x^{(i)})$，且$x^{(i)}$为核心对象，则称$x^{(j)}$由$x^{(i)}$密度直达。不满足对称性，即反之不一定成立，除非$x^{(j)}$也为核心对象。如图，$x^{(8)}$可由$x^{(6)}$密度直达，而反之$x^{(6)}$不可由$x^{(8)}$密度直达，因为$x^{(8)}$不为核心对象。</li><li>密度可达<br>  若存在样本序列$p_1, p_2, …, p_T$，满足$p_1 = x^{(i)}, p_T = x^{(j)}$，且$p_{t+1}$可由$p_t$密度直达，也就是说$p_1, p_2, …, p_{T-1}$均为核心对象，则称$x^{(j)}$由$x^{(i)}$密度可达。也不满足对称性。如图，$x^{(4)}$可由$x^{(1)}$密度可达，而$x^{(2)}$不可由$x^{(4)}$密度可达，因为$x^{(4)}$不为核心对象。</li><li>密度相连<br>  存在核心对象$x^{(k)}$，使得$x^{(i)}$与$x^{(j)}$均由$x^{(k)}$密度可达，则称$x^{(i)}$与$x^{(j)}$密度相连。注意密度相连满足对称性。如图，$x^{(8)}$与$x^{(4)}$均可由$x^{(1)}$密度可达，则$x^{(8)}$与$x^{(4)}$密度相连。</li></ul><p><img src="/2018/11/16/Clustering/DBSCAN1.jpg" alt="DBSCAN1"></p><h3 id="计算思想"><a href="#计算思想" class="headerlink" title="计算思想"></a>计算思想</h3><p><code>DBSCAN</code>的聚类思想是，由<strong>密度可达关系</strong>导出的最大密度相连的样本集合，即为我们最终聚类的一个簇，这个簇里可能只有一个核心对象，也可能有多个核心对象，若有多个，则簇里的任意一个核心对象的$\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。</p><p>另外，考虑以下三个问题</p><ul><li>噪音点<br>  一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，这些样本点标记为噪音点，<code>with Noise</code>就是这个意思。</li><li>距离的度量<br>  一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和<code>KNN</code>算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用<code>KDTree</code>或者球树来快速的搜索最近邻。</li><li>类别重复时的判别<br>  某些样本可能到两个核心对象的距离都小于$\epsilon$，但是这两个核心对象如下图所示，不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？<br>  <img src="/2018/11/16/Clustering/DBSCAN2.jpg" alt="DBSCAN2"><br>  一般来说，此时<code>DBSCAN</code>采用<strong>先来后到</strong>，先进行聚类的类别簇会标记这个样本为它的类别。也就是说<code>BDSCAN</code>不是完全稳定的算法。</li></ul><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$(\epsilon, minPts)$与样本距离度量方式，将其划分为$K$类。</p><ol><li>检测数据库中尚未检查过的对象$p$，如果$p$未被处理(归为某个簇或者标记为噪声)，则检查其邻域：<ul><li>若包含的对象数不小于$minPts$，建立新簇$C$，将其中的所有点加入候选集$N$；</li></ul></li><li>对候选集$N$中所有尚未被处理的对象$q$，检查其邻域：<ul><li>若至少包含$minPts$个对象，则将这些对象加入$N$；</li><li>如果$q$未归入任何一个簇，则将$q$加入$C$；</li></ul></li><li>重复步骤$2$，继续检查$N$中未处理的对象，直到当前候选集$N$为空；</li><li>重复步骤$1$-$3$，直到所有对象都归入了某个簇或标记为噪声。</li></ol><h2 id="高斯混合模型-GMM"><a href="#高斯混合模型-GMM" class="headerlink" title="高斯混合模型(GMM)"></a>高斯混合模型(GMM)</h2><p>详情查看<a href="https://louishsu.xyz/2018/11/12/EM%E7%AE%97%E6%B3%95-GMM%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">EM算法 &amp; GMM模型</a>。</p><h2 id="层次聚类-Hierarchical-Clustering"><a href="#层次聚类-Hierarchical-Clustering" class="headerlink" title="层次聚类(Hierarchical Clustering)"></a>层次聚类(Hierarchical Clustering)</h2><p>层次聚类更多的是一种思想，而不是方法，通过从下往上不断合并簇，或者从上往下不断分离簇形成嵌套的簇。例如上面讲到的<code>DBSCAN</code>最后簇的合并就有这种思想。</p><p>层次的类通过“树状图”来表示，如下<br><img src="/2018/11/16/Clustering/层次聚类.png" alt="层次聚类"></p><p>主要的思想或方法有两种</p><ul><li>自底向上的凝聚方法<code>(agglomerative hierarchical clustering)</code><br>  如<code>AGNES</code>。</li><li>自上向下的分裂方法<code>(divisive hierarchical clustering)</code><br>  如<code>DIANA</code>。</li></ul><h2 id="图团体检测-Graph-Community-Detection"><a href="#图团体检测-Graph-Community-Detection" class="headerlink" title="图团体检测(Graph Community Detection)"></a>图团体检测(Graph Community Detection)</h2><p>略</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EM &amp; GMM</title>
      <link href="/2018/11/12/EM-GMM/"/>
      <url>/2018/11/12/EM-GMM/</url>
      
        <content type="html"><![CDATA[<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p><code>Expectation Maximization Algorithm</code>，是 Dempster, Laind, Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 <code>MLE</code> 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据。</p><h2 id="引例：先挖个坑"><a href="#引例：先挖个坑" class="headerlink" title="引例：先挖个坑"></a>引例：先挖个坑</h2><p>给出李航《统计学习方法》的三硬币模型例子，假设有$3$枚硬币$A, B, C$，各自出现正面的概率分别为$\pi, p, q$，先进行如下实验：先投掷硬币$A$，若结果为正面，则选择硬币$B$投掷一次，否则选择$C$，记录投掷结果如下</p><script type="math/tex; mode=display">1, 1, 0, 1, 0, 0, 1, 0, 1, 1</script><p>只能观测到实验结果，而投掷过程未知，即硬币$A$的投掷结果未知，现欲估计三枚硬币的参数$\pi, p, q$。</p><p><strong>解</strong>：根据题意可以得到三个随机变量$X_1, X_2, X_3$的概率分布如下</p><script type="math/tex; mode=display">P(X_1) = \pi ^ {X_1} (1 - \pi) ^ {1 - X_1}</script><script type="math/tex; mode=display">P(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2}</script><script type="math/tex; mode=display">P(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}</script><p>定义随机变量$X$表示观测结果为正面，由全概率公式可以得到</p><script type="math/tex; mode=display">P(X) = P(X|X_1)P(X_1) + P(X|\overline{X_1})P(\overline{X_1})= \pi p + (1 - \pi) q</script><script type="math/tex; mode=display">P(\overline{X}) = P(\overline{X}|X_1)P(X_1) + P(\overline{X}|\overline{X_1})P(\overline{X_1})= \pi (1 - p) + (1 - \pi) (1 - q)</script><p>即</p><script type="math/tex; mode=display">P(X) = [\pi p + (1 - \pi) q] ^ {X} [\pi (1 - p) + (1 - \pi) (1 - q)] ^ {1 - X}</script><p>利用最大似然估计，有</p><script type="math/tex; mode=display">\log L(D | \theta) = 6 \log [\pi p + (1 - \pi) q] + 4 \log [\pi (1 - p) + (1 - \pi) (1 - q)]</script><p>至此，我们一定能想到通过求似然函数极值来求解参数</p><script type="math/tex; mode=display">\frac{∂ }{∂ \pi} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><script type="math/tex; mode=display">\frac{∂ }{∂ p} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><script type="math/tex; mode=display">\frac{∂ }{∂ q} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0</script><p>但是好像出了问题，并不能求解，所以我们引入<code>EM算法</code>迭代求解。</p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>以$x^{(i)}$表示训练数据，$w_k$表示类别，设当前迭代参数为$\theta^{(t)}$，则下一次迭代应有</p><script type="math/tex; mode=display">\theta^{(t+1)} = \arg \max \sum_i \log P(x^{(i)}|\theta) \tag{1}</script><p>由边缘概率公式</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \tag{2}</script><blockquote><p>$P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)}|x^{(i)}, \theta)$<br>至此已得出引例中的表达式，其中$P(w_k^{(i)}|x^{(i)}, \theta)$与$P(x^{(i)} | w_k^{(i)}, \theta)$均未知，而通过求极值不能解得参数。</p></blockquote><p>我们引入迭代参数$\theta^{(t)}$，即第$t$次迭代时的参数$\theta$，该参数为已知变量</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)} | \theta^{(t)})}{P(w_k^{(i)} | \theta^{(t)})}</script><blockquote><p>$P(w_k^{(i)}|\theta^{(t)})$表示样本$x^{(i)}$类别为$w_k^{(i)}$的概率，注意上标。</p></blockquote><p>引入<code>Jensen不等式</code>：</p><blockquote><p>For a real convex function $\varphi$, numbers $x_1, …, x_n$ in its domain, and positive weights $a_i$, Jensen’s inequality can be stated as:</p><script type="math/tex; mode=display">\varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right)\leq \frac{\sum a_i \varphi(x_i)}{\sum a_i}</script><p>and the inquality is reversed if $\varphi$ is concave, which is</p><script type="math/tex; mode=display">\varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right)\geq \frac{\sum a_i \varphi(x_i)}{\sum a_i}</script><p>Equality holds if and only if $x_1 = … = x_n$ or $\varphi$ is linear.</p></blockquote><p>$\log(·)$为凹函数<code>(concave)</code>，且满足</p><script type="math/tex; mode=display">\sum_k P(w_k^{(i)} | \theta^{(t)}) = 1</script><p>所以有</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">\geq \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{3}</script><p>此时我们得到似然函数$\sum_i \log P(x^{(i)}|\theta)$的一个下界，但必须保证这个下界是紧的，也就是至少有点能使等号成立</p><blockquote><p>由<code>Jensen不等式</code>，当且仅当$　P(x^{(i)}, w_k^{(i)}|\theta)=C　$时取等号</p></blockquote><p>定义</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) - P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)})</script><p>其中第一项即期望</p><script type="math/tex; mode=display">E_w\left[    \log P(X, w|\theta) | X, \theta^{(t)}\right] = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) \tag{4}</script><p>第二项为$P(w | X, \theta^{(t)})$的信息熵</p><script type="math/tex; mode=display">H[P(w | X, \theta^{(t)})] = - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5}</script><p>即</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= E_w\left[    \log P(X, w|\theta) | X, \theta^{(t)}\right] +H[P(w | X, \theta^{(t)})] \tag{E-step}</script><blockquote><p>注意到$H[P(w | X, \theta^{(t)})]$项为常数，故也可设</p><script type="math/tex; mode=display">Q(\theta|\theta^{(t)}) = E_w\left[\log P(X, w|\theta) | X, \theta^{(t)} \right]</script></blockquote><p>代回$(1)$，得到优化目标</p><script type="math/tex; mode=display">\theta^{(t+1)} = \arg \max L(\theta|\theta^{(t)}) \tag{M-step}</script><p>我们需要不断最大化$L(\theta | \theta^{(t)})$来不断优化，这就是所谓的<code>EM算法</code>，<code>E-step</code>是指求出期望，<code>M-step</code>是指迭代更新参数<br><img src="/2018/11/12/EM-GMM/EM算法图解.png" alt="EM算法图解"></p><p>伪代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">According to prior knowledge set </span><br><span class="line">    $\theta$</span><br><span class="line">Repeat until convergence&#123;</span><br><span class="line">    E-step: The expectation of hidden variables</span><br><span class="line">    M-step: Finding the maximum of likelihood function</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>实际上，从边缘概率与条件概率入手</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)= \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta)</script><script type="math/tex; mode=display">= \sum_i \log \sum_k P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta)</script><script type="math/tex; mode=display">\geq \sum_i \sum_k P(w_k^{(i)} | \theta) \log P(x^{(i)} | w_k^{(i)}, \theta) \tag{Jensen inequality}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)}</script><p>而由$(3)$，引入迭代变量可以得到</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta)\geq L(\theta|\theta^{(t)})</script><p>其中</p><script type="math/tex; mode=display">L(\theta|\theta^{(t)})= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><p>则</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)})</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} -\sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})}</script><script type="math/tex; mode=display">= \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta)}</script><p>而由<code>KL散度( Kullback–Leibler divergence)</code>(又称<code>相对熵(relative entropy)</code>)定义</p><blockquote><script type="math/tex; mode=display">D(P||Q) = \sum P(x) \log \frac{P(x)}{Q(x)}</script></blockquote><p>可知</p><script type="math/tex; mode=display">\sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = D\left[ P(w_k^{(i)}|\theta^{(t)}) || P(w_k^{(i)}|\theta) \right]</script><p>即迭代的$P(w_k^{(i)}|\theta^{(t)})$与真实的$P(w_k^{(i)}|\theta)$之间的相对熵！</p><blockquote><p>这里关于<code>K-L散度</code>的困扰了$N$久，终于搞出来了。</p></blockquote><!-- 另外，附上证明一则> 证明对数似然函数$\sum_i \log P(x^{(i)} | \theta)$满足> $$> \sum_i \log P(x^{(i)} | \theta^{(t+1)}) > \geq \sum_i \log P(x^{(i)} | \theta^{(t)})> $$> > 证明：由$(M-step)$> $$\sum_i \log P(x^{(i)} | \theta^{(t+1)})> = \max L(\theta | \theta^{(t)})$$> > 而$\theta^{(t+1)}为函数L(\theta|\theta^{(t)})极值点$，所以> $$\max L(\theta | \theta^{(t)})> \geq L(\theta | \theta^{(t)})$$> > 其中> $$> L(\theta | \theta^{(t)})> = \sum_i \log P(x^{(i)} | \theta^{(t)})> $$> > 故> $$> \sum_i \log P(x^{(i)} | \theta^{(t+1)}) > \geq \sum_i \log P(x^{(i)} | \theta^{(t)})> $$ --><h2 id="引例的求解"><a href="#引例的求解" class="headerlink" title="引例的求解"></a>引例的求解</h2><blockquote><p>$Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$</p></blockquote><p>此题中</p><script type="math/tex; mode=display">P(w_k|\pi) = \pi^{w_k}(1-\pi)^{1-w_k}</script><script type="math/tex; mode=display">P(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}}</script><script type="math/tex; mode=display">P(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}}</script><ul><li><p>$E-step$</p><script type="math/tex; mode=display">  Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)})  = \sum_i \sum_k P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})   \log P(x^{(i)}, w_k^{(i)} | \pi, p, q)</script><p>  先求$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$，即第一次投掷结果为$w_k$的概率</p><script type="math/tex; mode=display">  P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})  = \frac  {\left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_k}}  {\sum_j \left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_j}  \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_j}}</script><p>  即</p><script type="math/tex; mode=display">  \begin{cases}      P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}}          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} +           (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\      P(w_2^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac          {(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}          {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} +           (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}}  \end{cases}</script><p>  记</p><script type="math/tex; mode=display">\mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})</script><script type="math/tex; mode=display">\mu_2^{(i)} = 1 - \mu_1^{(i)}</script><blockquote><p>注意$w^{(i)}_k$上标<code>^{(i)}</code></p></blockquote><p>  再求$P(x^{(i)}, w_k^{(i)} | \pi, p, q)$，已知</p><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)} | \pi, p, q)  = P(x^{(i)} | w_k^{(i)}, \pi, p, q)  P(w_k^{(i)} | \pi, p, q)</script><p>  所以</p><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)} | \pi, p,q)  = \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k}</script><p>  综上</p><script type="math/tex; mode=display">  Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)})  = \sum_i \sum_{k=1}^2 \mu^{(i)}_k \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k}  \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k}</script><script type="math/tex; mode=display">  = \sum_i \mu_1^{(i)} \log \pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \mu_1^{(i)}) \log (1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}</script></li><li><p>$M-step$</p><ul><li><p>$\frac{∂Q}{∂\pi} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂\pi}  = \sum_i \mu_1^{(i)}   \frac  {p^{x^{(i)}}(1-p)^{1-x^{(i)}}}  {\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} +   (1 - \mu_1^{(i)})  \frac  {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}}  {(1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}}</script><script type="math/tex; mode=display">  = \sum_i \frac{\mu_1^{(i)}}{\pi} + \frac{\mu_1^{(i)} - 1}{1 - \pi}  = \sum_i \frac{\mu_1^{(i)} - \pi}{\pi(1 - \pi)}  = \frac{\sum_i \mu_1^{(i)} - n\pi}{\pi(1 - \pi)} = 0</script><script type="math/tex; mode=display">  \Rightarrow \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)}</script></li><li><p>$\frac{∂Q}{∂p} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂p}  = \sum_i \mu_1^{(i)}   \left[      \frac{x^{(i)}}{p} - \frac{1 - x^{(i)}}{1 - p}  \right]</script><script type="math/tex; mode=display">  = \frac{1}{p(1 - p)}   \sum_i \mu_1^{(i)} (x^{(i)} - p)  = \frac{1}{p(1 - p)}  \left[      \sum_i \mu_1^{(i)} x^{(i)} - p \sum_i \mu_1^{(i)}  \right] = 0</script><script type="math/tex; mode=display">  \Rightarrow p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}}</script></li><li><p>$\frac{∂Q}{∂q} = 0$</p><script type="math/tex; mode=display">  \frac{∂Q}{∂q}  = \sum_i (1 - \mu_1^{(i)})   \left[      \frac{x^{(i)}}{q} - \frac{1 - x^{(i)}}{1 - q}  \right]</script><script type="math/tex; mode=display">  = \frac{1}{q(1 - q)}  \sum_i (1 - \mu_1^{(i)})   (x^{(i)} - q)</script><script type="math/tex; mode=display">  = \frac{1}{q(1 - q)}  \left[      \sum_i (1 - \mu_1^{(i)}) x^{(i)} -      q \sum_i (1 - \mu_1^{(i)})  \right] = 0</script><script type="math/tex; mode=display">  \Rightarrow q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})}</script></li></ul></li></ul><p>多次迭代即可求解，终止条件可设置为</p><script type="math/tex; mode=display">|| \theta^{(t+1)} - \theta^{(t)} || < \epsilon</script><p>或</p><script type="math/tex; mode=display">||Q(\theta^{(t+1)} | \theta^{(t)}) - Q(\theta^{(t)} |\theta^{(t)})|| < \epsilon</script><h1 id="GMM模型"><a href="#GMM模型" class="headerlink" title="GMM模型"></a>GMM模型</h1><p><code>Gaussian Mixture Model</code>，是一种无监督学习算法，常用于聚类。当聚类问题中各个类别的尺寸不同、聚类间有相关关系的时候，往往使用<code>GMM</code>更合适。对一个样本来说，<code>GMM</code>得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被成为软聚类。一般说来， 任意形状的概率分布都可以用多个高斯分布函数去近似，因而，<code>GMM</code>的应用也比较广泛。</p><p>高斯混合模型，指具有如下形式的概率分布模型：</p><script type="math/tex; mode=display">P(x|\mu_k, \Sigma_k)= \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)</script><p>其中</p><ul><li>$\pi_k(0 \leq \pi_k \leq 1)$是系数，且$\sum_k \pi_k = 1$</li><li>$N(x|\mu_k, \Sigma_k)$为高斯密度函数</li></ul><script type="math/tex; mode=display">N(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[    -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right]</script><blockquote><ul><li>即多个高斯分布叠加出来的玩意；</li><li>现在我们需要求取系数$\pi_k$及高斯模型的参数$(\mu_k, \Sigma_k)$；</li><li>与<code>K-Means</code>等聚类方法区别是，<code>GMM</code>求出的是连续的分布模型，可计算出“归属于”哪一类的概率。</li></ul></blockquote><h2 id="推导-1"><a href="#推导-1" class="headerlink" title="推导"></a>推导</h2><script type="math/tex; mode=display">\log P(X|\pi, \mu, \Sigma)= \sum_i \log \sum_k \pi_k N(x|\mu_k, \Sigma_k)</script><script type="math/tex; mode=display">s.t.　\sum_k \pi_k = 1</script><h3 id="暴力求解"><a href="#暴力求解" class="headerlink" title="暴力求解"></a>暴力求解</h3><p>以$1$维高斯分布为例</p><script type="math/tex; mode=display">N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}}</script><p>构造拉格朗日<code>(Lagrange)</code>函数</p><script type="math/tex; mode=display">L(\pi, \mu, \sigma^2) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \sigma_k^2) + \lambda \left(\sum_k \pi_k - 1 \right) \tag{5}</script><script type="math/tex; mode=display">\begin{cases}    \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2)         = \sum_i        \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\    \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\mu_k}N(x^{(i)}|\mu_k, \sigma_k^2) \\    \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\sigma_k^2}N(x^{(i)}|\mu_k, \sigma_k^2)\end{cases} \tag{6}</script><p>其中</p><script type="math/tex; mode=display">\frac{∂}{∂\mu_k} N(x|\mu_k, \sigma_k^2)= \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{x-\mu_k}{\sigma_k^2}= N(x|\mu_k, \sigma_k^2) · \frac{x-\mu_k}{\sigma_k^2}</script><script type="math/tex; mode=display">\frac{∂}{∂\sigma_k^2} N(x|\mu_k, \sigma_k^2)= \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right)</script><blockquote><p>$\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) = - \frac{\sigma_k^{-3}}{2};　\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) = - \frac{1}{\sigma_k^4}$</p></blockquote><script type="math/tex; mode=display">= \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(- \frac{\sigma_k^{-3}}{2}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \left(- \frac{1}{\sigma_k^4}\right)</script><script type="math/tex; mode=display">= N(x|\mu_k, \sigma_k^2) \left[    \frac{(x - \mu_k)^2}{\sigma_k^2} - 1\right] \frac{1}{2 \sigma_k^2}</script><p>代回$(6)$可以得到</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\    \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{x^{(i)}-\mu_k}{\sigma_k^2} \\    \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2)        = \sum_i        \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \left[    \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1\right] \frac{1}{2 \sigma_k^2}\end{cases} \tag{7}</script><p>令</p><script type="math/tex; mode=display">\gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \tag{8}</script><blockquote><p>通俗理解：$\gamma^{(i)}_k$表示样本$x^{(i)}$中来自类别$w_k$的“贡献百分比”</p></blockquote><ul><li><p>令$\frac{∂}{∂\mu_k} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到</p><script type="math/tex; mode=display">  \sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) = 0  \Rightarrow   \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k}</script></li><li><p>令$\frac{∂}{∂\sigma_k^2} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到</p><script type="math/tex; mode=display">  \sum_i      \gamma^{(i)}_k       \left[          \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1      \right] = 0  \Rightarrow  \sigma_k^2 = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k)^2}{\sum_i \gamma^{(i)}_k}</script></li><li><p>对于$\frac{∂}{∂\pi_k} \log P(X|\pi, \mu, \sigma^2) = 0$，需要做一点处理<br>  两边同乘$\pi_k$，得到</p><script type="math/tex; mode=display">  \sum_i \gamma^{(i)}_k = - \lambda \pi_k \tag{9}</script><p>  然后两边对$k$作累加</p><script type="math/tex; mode=display">  \sum_k \sum_i \gamma^{(i)}_k = - \lambda \sum_k \pi_k</script><blockquote><p>$\sum_k \sum_i \gamma^{(i)}_k = \sum_i \sum_k \gamma^{(i)}_k = N,　\sum_k \pi_k = 1$</p></blockquote><script type="math/tex; mode=display">  N = - \lambda　或　\lambda = -N \tag{10}</script><p>  代回$(9)$，得到</p><script type="math/tex; mode=display">  \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}</script></li></ul><p>综上，我们得到$4$个用于迭代的计算式，将其推广至多维即</p><script type="math/tex; mode=display">\gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)}</script><script type="math/tex; mode=display">\mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k}</script><script type="math/tex; mode=display">\Sigma_k = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)}_k}</script><script type="math/tex; mode=display">\pi_k = \frac{\sum_i \gamma^{(i)}_k}{N}</script><h3 id="用EM算法求解"><a href="#用EM算法求解" class="headerlink" title="用EM算法求解"></a>用<code>EM算法</code>求解</h3><blockquote><p>$Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta)$</p></blockquote><script type="math/tex; mode=display">Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})\log P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k)</script><ul><li><p>$ M-step $</p><script type="math/tex; mode=display">  P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})  = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)}  = \gamma^{(i)}_k</script><script type="math/tex; mode=display">  P(x^{(i)}, w_k^{(i)}|\mu_k, \Sigma_k)  = P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k)   P(w_k^{(i)}|\mu_k, \Sigma_k)  = \pi_k N(x^{(i)}|\mu_k, \Sigma_k)</script><p>  故</p><script type="math/tex; mode=display">  Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)})   = \sum_i \sum_k \gamma^{(i)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k)</script><p>  通过求解极值可得到与$\underline{暴力求解}$一样的等式，即</p><script type="math/tex; mode=display">  \gamma^{(i)(t)}_k = \frac{\pi^{(t)}_k N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})}</script><script type="math/tex; mode=display">  \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k}</script><script type="math/tex; mode=display">  \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k}</script><script type="math/tex; mode=display">  \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N}</script><p>  伪代码为</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">According to prior knowledge set</span><br><span class="line">    \pi^&#123;(t)&#125;(n_clusters,)</span><br><span class="line">    \mu^&#123;(t)&#125;(n_clusters, n_features)</span><br><span class="line">    \Sigma^&#123;(t)&#125;(n_clusters, n_features, n_features)</span><br><span class="line">Repeat until convergence&#123;</span><br><span class="line">    # E-step: calculate \gamma^&#123;(t)&#125;</span><br><span class="line">        \gamma(n_samples, n_clusters)</span><br><span class="line">    # M-step: update \pi, \mu, \Sigma</span><br><span class="line">        \pi^&#123;(t+1)&#125;(n_clusters,)</span><br><span class="line">        \mu^&#123;(t+1)&#125;(n_clusters, n_features)</span><br><span class="line">        \Sigma^&#123;(t+1)&#125;(n_clusters, n_features, n_features)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>初始点的选择可以随机选择，也可使用<code>K-Means</code></p></blockquote><p>  <code>GMM</code>算法收敛过程如下<br>  <img src="/2018/11/12/EM-GMM/gmm.gif" alt="gmm"></p></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p82_gmm.py" target="_blank" rel="noopener">@Github: GMM</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GMM</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Gaussian Mixture Model</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        n_clusters &#123;int&#125;</span></span><br><span class="line"><span class="string">        prior &#123;ndarray(n_clusters,)&#125;</span></span><br><span class="line"><span class="string">        mu &#123;ndarray(n_clusters, n_features)&#125;</span></span><br><span class="line"><span class="string">        sigma &#123;ndarray(n_clusters, n_features, n_features)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_clusters)</span>:</span></span><br><span class="line">        self.n_clusters = n_clusters</span><br><span class="line">        self.prior = <span class="keyword">None</span></span><br><span class="line">        self.mu = <span class="keyword">None</span></span><br><span class="line">        self.sigma = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, delta=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">            delta &#123;float&#125;</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - Initialize with k-means</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># initialize with k-means</span></span><br><span class="line">        clf = KMeans(n_clusters=self.n_clusters)</span><br><span class="line">        clf.fit(X)</span><br><span class="line">        self.mu = clf.cluster_centers_ </span><br><span class="line">        self.prior = np.zeros(self.n_clusters)</span><br><span class="line">        self.sigma = np.zeros((self.n_clusters, n_features, n_features))</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">            X_ = X[clf.labels_==k]</span><br><span class="line">            self.prior[k] = X_.shape[<span class="number">0</span>] / X_.shape[<span class="number">0</span>]</span><br><span class="line">            self.sigma[k] = np.cov(X_.T)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            mu_ = self.mu.copy()</span><br><span class="line">            <span class="comment"># E-step: updata gamma</span></span><br><span class="line">            gamma = np.zeros((n_samples, self.n_clusters))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                    denominator = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                        post = self.prior[k] *\</span><br><span class="line">                                    multiGaussian(X[i], self.mu[j], self.sigma[j])</span><br><span class="line">                        denominator += post</span><br><span class="line">                        <span class="keyword">if</span> j==k: numerator = post</span><br><span class="line">                    gamma[i, k] = numerator/denominator</span><br><span class="line">            <span class="comment"># M-step: updata prior, mu, sigma</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                sum1 = <span class="number">0</span></span><br><span class="line">                sum2 = <span class="number">0</span></span><br><span class="line">                sum3 = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">                    sum1 += gamma[i, k]</span><br><span class="line">                    sum2 += gamma[i, k] * X[i]</span><br><span class="line">                    x_ = np.reshape(X[i] - self.mu[k], (n_features, <span class="number">1</span>))</span><br><span class="line">                    sum3 += gamma[i, k] * x_.dot(x_.T)</span><br><span class="line">                self.prior[k]  = sum1 / n_samples</span><br><span class="line">                self.mu[k]     = sum2 / sum1</span><br><span class="line">                self.sigma[k]  = sum3 / sum1</span><br><span class="line">            <span class="comment"># to stop</span></span><br><span class="line">            mu_delta = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                mu_delta += nl.norm(self.mu[k] - mu_[k])</span><br><span class="line">            print(mu_delta)</span><br><span class="line">            <span class="keyword">if</span> mu_delta &lt; delta: <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> self.prior, self.mu, self.sigma</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_proba</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            y_pred_proba &#123;ndarray(n_samples, n_clusters)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (n_samples, n_features) = X.shape</span><br><span class="line">        y_pred_proba = np.zeros((n_samples, self.n_clusters))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(self.n_clusters):</span><br><span class="line">                y_pred_proba[i, k] = self.prior[k] *\</span><br><span class="line">                                multiGaussian(X[i], self.mu[k], self.sigma[k])</span><br><span class="line">        <span class="keyword">return</span> y_pred_proba</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(n_samples, n_features)&#125;</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            y_pred_proba &#123;ndarray(n_samples,)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        y_pred_proba = self.predict_proba(X)</span><br><span class="line">        <span class="keyword">return</span> np.argmax(y_pred_proba, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Data Augmentation</title>
      <link href="/2018/11/02/Data-Augmentation/"/>
      <url>/2018/11/02/Data-Augmentation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>“有时候不是由于算法好赢了。而是由于拥有很多其它的数据才赢了。”</p></blockquote><h1 id="数据集扩增"><a href="#数据集扩增" class="headerlink" title="数据集扩增"></a>数据集扩增</h1><p>在深度学习中,很多训练数据意味着能够用更深的网络，训练出更好的模型。既然这样，收集很多其它的数据不即可啦？假设能够收集很多其它能够用的数据当然好，比如<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>上图像数据量已达到$1400$万张，可是非常多时候，收集很多其它的数据意味着须要耗费很多其它的人力物力，这就需要使用一定的方法扩增数据集。</p><h1 id="图像扩增"><a href="#图像扩增" class="headerlink" title="图像扩增"></a>图像扩增</h1><p>大部分借助<code>OpenCV</code>库，这里推荐一位学长的博客，整理了大量的<code>OpenCV</code>使用方法.</p><blockquote><p><a href="http://ex2tron.wang/" target="_blank" rel="noopener">Ex2tron’s Blog</a></p></blockquote><p><code>TensorFlow</code>也提供相应图像处理方法<br><a href="https://tensorflow.google.cn/api_docs/python/tf/image" target="_blank" rel="noopener">Module: tf.image | TensorFlow </a></p><p>需要注意的是，扩增过程中，需注意图像数据类型，可以将数据归一化到$(0, 1)$间再进行处理</p><h2 id="翻转"><a href="#翻转" class="headerlink" title="翻转"></a>翻转</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flip</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    rand_var = np.random.random()</span><br><span class="line">    image = image[:, ::<span class="number">-1</span>, :] <span class="keyword">if</span> rand_var &gt; <span class="number">0.5</span> <span class="keyword">else</span> image</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(image, degree)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        degree &#123;float&#125;</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    (h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line">    center = (w // <span class="number">2</span>, h // <span class="number">2</span>)</span><br><span class="line">    random_angel = np.random.randint(-degree, degree)</span><br><span class="line">    M = cv2.getRotationMatrix2D(center, random_angel, <span class="number">1.0</span>)</span><br><span class="line">    image = cv2.warpAffine(image, M, (w, h))</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="噪声"><a href="#噪声" class="headerlink" title="噪声"></a>噪声</h2><p>可手动实现，如椒盐噪声代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saltnoise</span><span class="params">(image, salt=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">    <span class="string">""" add salt &amp; pepper and gaussian noise</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        salt &#123;float(0, 1)&#125; number of salt pixel = salt*h*w</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        TODO: gaussain noise</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    (h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line">    n_salt = int(salt * h * w)</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(n_salt):</span><br><span class="line">        hr = np.random.randint(<span class="number">0</span>, h)</span><br><span class="line">        wr = np.random.randint(<span class="number">0</span>, w)</span><br><span class="line">        issalt = (np.random.rand(<span class="number">1</span>) &gt; <span class="number">0.5</span>)</span><br><span class="line">        image[hr, wr] = <span class="number">255</span> <span class="keyword">if</span> issalt <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure></p><p>也可调用<code>scikit-image</code>库，需要注意的是，<code>skimage.util.random_noise()</code>会将原图数据转换为$(0, 1)$间的浮点数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">noise</span><span class="params">(image, gaussian, salt, seed=None)</span>:</span></span><br><span class="line">    <span class="string">""" add noise to image TODO</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        gaussian &#123;bool&#125;: </span></span><br><span class="line"><span class="string">        salt &#123;bool&#125;: </span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        Function to add random noise of various types to a floating-point image.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dtype = image.dtype</span><br><span class="line">    <span class="keyword">if</span> gaussian:</span><br><span class="line">        image = skimage.util.random_noise(image, mode=<span class="string">'gaussian'</span>, seed=seed)</span><br><span class="line">    <span class="keyword">if</span> salt:</span><br><span class="line">        image = skimage.util.random_noise(image, mode=<span class="string">'s&amp;p'</span>, seed=seed)</span><br><span class="line"></span><br><span class="line">    image = (image * <span class="number">255</span>).astype(dtype)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure></p><h2 id="亮度与对比度调整"><a href="#亮度与对比度调整" class="headerlink" title="亮度与对比度调整"></a>亮度与对比度调整</h2><p>考虑到数据溢出，先转换为整形数据，再限制其值到$[0, 255]$</p><blockquote><p>注意数据类型</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">brightcontrast</span><span class="params">(image, brtadj=<span class="number">0</span>, cstadj=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">""" adjust bright and contrast value</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        brtadj &#123;int&#125;    if true, adjust bright</span></span><br><span class="line"><span class="string">        cstadj &#123;float&#125;  if true, adjust contrast</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dtype = image.dtype</span><br><span class="line">    image = image.astype(<span class="string">'int'</span>)*cstadj + brtadj</span><br><span class="line">    image = np.clip(image, <span class="number">0</span>, <span class="number">255</span>).astype(dtype)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure><h2 id="投射变换"><a href="#投射变换" class="headerlink" title="投射变换"></a>投射变换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perspective</span><span class="params">(image, prop)</span>:</span></span><br><span class="line">    <span class="string">""" 透射变换</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        image &#123;ndarray(H, W, C)&#125;</span></span><br><span class="line"><span class="string">        prop &#123;float&#125;: 在四个顶点多大的方格内选取新顶点，方格大小为(H*prop, W*prop)</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        在四个顶点周围随机选取新的点进行仿射变换，四个点对应左上、右上、左下、右下</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    (h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    ptsrc = np.zeros(shape=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line">    ptdst = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, w], [h, <span class="number">0</span>], [h, w]])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        hr = np.random.randint(<span class="number">0</span>, int(h*prop))</span><br><span class="line">        wr = np.random.randint(<span class="number">0</span>, int(w*prop))</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            ptsrc[i] = np.array([hr, wr])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">            ptsrc[i] = np.array([hr, w - wr])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">2</span>:</span><br><span class="line">            ptsrc[i] = np.array([h - hr, wr])</span><br><span class="line">        <span class="keyword">elif</span> i == <span class="number">3</span>:</span><br><span class="line">            ptsrc[i] = np.array([h - hr, w - wr])</span><br><span class="line">    M = cv2.getPerspectiveTransform(ptsrc.astype(<span class="string">'float32'</span>), ptdst.astype(<span class="string">'float32'</span>))</span><br><span class="line">    image = cv2.warpPerspective(image, M, (w, h))</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>二次入坑raspberry-pi</title>
      <link href="/2018/10/29/%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi/"/>
      <url>/2018/10/29/%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>距上一次搭建树莓派平台已经两年了，保存的镜像出了问题，重新搭建一下。</p><h1 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h1><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>从官网下载树莓派系统镜像，有以下几种可选</p><blockquote><p><a href="https://www.raspberrypi.org/" target="_blank" rel="noopener">Raspberry Pi — Teach, Learn, and Make with Raspberry Pi </a></p><ol><li>Raspbian &amp; Raspbian Lite，基于Debian</li><li>Noobs &amp; Noobs Lite</li><li>Ubuntu MATE</li><li>Snappy Ubuntu Core</li><li>Windows 10 IOT</li></ol></blockquote><p><del>其余不太了解，之前安装的是Raspbian，对于Debian各种不适，换上界面优雅的Ubuntu Mate玩一下</del><br>老老实实玩Raspbian，笑脸:-)</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>比较简单，准备micro-SD卡，用Win32 Disk Imager烧写镜像</p><blockquote><p><a href="https://sourceforge.net/projects/win32diskimager/" target="_blank" rel="noopener">Win32 Disk Imager download | SourceForge.net</a></p><p><img src="/2018/10/29/二次入坑raspberry-pi/Win32DiskImager.jpg" alt="Win32DiskImager"></p></blockquote><p>安装完软件后可点击<code>Read</code>备份自己的镜像。</p><p>注意第二次开机前需要配置<code>config.txt</code>文件，否则<code>hdmi</code>无法显示</p><blockquote><p><a href="http://shumeipai.nxez.com/2015/11/23/raspberry-pi-configuration-file-config-txt-nstructions.html" target="_blank" rel="noopener">树莓派配置文档 config.txt 说明 | 树莓派实验室</a></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">disable_overscan=1 </span><br><span class="line">hdmi_force_hotplug=1</span><br><span class="line">hdmi_group=2    # DMT</span><br><span class="line">hdmi_mode=32    # 1280x960</span><br><span class="line">hdmi_drive=2</span><br><span class="line">config_hdmi_boost=4</span><br></pre></td></tr></table></figure><h2 id="修改交换分区"><a href="#修改交换分区" class="headerlink" title="修改交换分区"></a>修改交换分区</h2><h3 id="Ubuntu-Mate"><a href="#Ubuntu-Mate" class="headerlink" title="Ubuntu Mate"></a>Ubuntu Mate</h3><p>查看交换分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>未设置时如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:            0        0        0</span><br></pre></td></tr></table></figure></p><p>创建和挂载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 获取权限</span><br><span class="line">$ sudo -i</span><br><span class="line"></span><br><span class="line"># 创建目录</span><br><span class="line">$ mkdir /swap</span><br><span class="line">$ cd /swap</span><br><span class="line"></span><br><span class="line"># 指定一个大小为1G的名为“swap”的交换文件</span><br><span class="line">$ dd if=/dev/zero of=swap bs=1M count=1k</span><br><span class="line"># 创建交换文件</span><br><span class="line">$ mkswap swap</span><br><span class="line"># 挂载交换分区</span><br><span class="line">$ swapon swap</span><br><span class="line"></span><br><span class="line"># 卸载交换分区</span><br><span class="line"># $ swapoff swap</span><br></pre></td></tr></table></figure></p><p>查看交换分区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>未设置时如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:         1023        0     1023</span><br></pre></td></tr></table></figure></p><h3 id="Raspbian"><a href="#Raspbian" class="headerlink" title="Raspbian"></a>Raspbian</h3><p>We will change the configuration in the file <code>/etc/dphys-swapfile</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nano /etc/dphys-swapfile</span><br></pre></td></tr></table></figure></p><p>The default value in Raspbian is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONF_SWAPSIZE=100</span><br></pre></td></tr></table></figure></p><p>We will need to change this to:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONF_SWAPSIZE=1024</span><br></pre></td></tr></table></figure></p><p>Then you will need to stop and start the service that manages the swapfile own Rasbian:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /etc/init.d/dphys-swapfile stop</span><br><span class="line">$ sudo /etc/init.d/dphys-swapfile start</span><br></pre></td></tr></table></figure></p><p>You can then verify the amount of memory + swap by issuing the following command:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br></pre></td></tr></table></figure></p><p>The output should look like:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total     used     free   shared  buffers   cached</span><br><span class="line">Mem:           435       56      379        0        3       16</span><br><span class="line">-/+ buffers/cache:       35      399</span><br><span class="line">Swap:         1023        0     1023</span><br></pre></td></tr></table></figure></p><h1 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h1><h2 id="安装指令"><a href="#安装指令" class="headerlink" title="安装指令"></a>安装指令</h2><ul><li><p><code>apt-get</code></p><ul><li>安装软件<br><code>apt-get install softname1 softname2 softname3 ...</code></li><li>卸载软件<br><code>apt-get remove softname1 softname2 softname3 ...</code></li><li>卸载并清除配置<br><code>apt-get remove --purge softname1</code></li><li>更新软件信息数据库<br><code>apt-get update</code></li><li>进行系统升级<br><code>apt-get upgrade</code></li><li>搜索软件包<br><code>apt-cache search softname1 softname2 softname3 ...</code></li><li>修正（依赖关系）安装：<br><code>apt-get -f insta</code></li></ul></li><li><p><code>dpkg</code></p><ul><li>安装<code>.deb</code>软件包<br><code>dpkg -i xxx.deb</code></li><li>删除软件包<br><code>dpkg -r xxx.deb</code></li><li>连同配置文件一起删除<br><code>dpkg -r --purge xxx.deb</code></li><li>查看软件包信息<br><code>dpkg -info xxx.deb</code></li><li>查看文件拷贝详情<br><code>dpkg -L xxx.deb</code></li><li>查看系统中已安装软件包信息<br><code>dpkg -l</code></li><li><p>重新配置软件包<br><code>dpkg-reconfigure xx</code></p></li><li><p>卸载软件包及其配置文件，但无法解决依赖关系！<br><code>sudo dpkg -p package_name</code></p></li><li>卸载软件包及其配置文件与依赖关系包<br><code>sudo aptitude purge pkgname</code></li><li>清除所有已删除包的残馀配置文件<br><code>dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P</code></li></ul></li></ul><h2 id="软件源"><a href="#软件源" class="headerlink" title="软件源"></a>软件源</h2><ol><li><p>备份原始文件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup</span><br></pre></td></tr></table></figure></li><li><p>修改文件并添加国内源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/apt/sources.list</span><br></pre></td></tr></table></figure></li><li><p>注释元文件内的源并添加如下地址</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#Mirror.lupaworld.com 源更新服务器（浙江省杭州市双线服务器，网通同电信都可以用，亚洲地区官方更新服务器）：</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse</span><br><span class="line">deb http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirror.lupaworld.com/ubuntu gutsy-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#Ubuntu 官方源 </span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse</span><br><span class="line">deb http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-security main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-updates main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu/ gutsy-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><p> 或者</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#阿里云</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#网易163</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.163.com/ubuntu/ trusty-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure></li><li><p>放置非官方源的包不完整，可在为不添加官方源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deb http://archive.ubuntu.org.cn/ubuntu-cn/ feisty main restricted universe multiverse</span><br></pre></td></tr></table></figure></li><li><p>更新源</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure></li><li><p>更新软件</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get dist-upgrade</span><br></pre></td></tr></table></figure></li><li><p>常见的修复安装命令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get -f install</span><br></pre></td></tr></table></figure></li></ol><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>主要是<code>Python</code>和相关依赖包的安装，使用以下指令可导出已安装的依赖包<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure></p><p>并使用指令安装到树莓派<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install -r requirements.txt</span><br></pre></td></tr></table></figure></p><p>注意<code>pip</code>更新<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install --upgrade pip</span><br></pre></td></tr></table></figure></p><p>最新版本会报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: cannot import name main</span><br></pre></td></tr></table></figure></p><p>修改文件<code>/usr/bin/pip</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pip <span class="keyword">import</span> main</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sys.exit(main())</span><br></pre></td></tr></table></figure></p><p>改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pip <span class="keyword">import</span> __main__</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sys.exit(__main__._main())</span><br></pre></td></tr></table></figure></p><hr><p><del>成功!!!</del><br>失败了，笑脸:-)，手动安装吧。。。</p><ul><li><p>部分包可使用<code>pip3</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install numpy</span><br><span class="line">$ pip3 install pandas</span><br><span class="line">$ pip3 install sklearn</span><br></pre></td></tr></table></figure><blockquote><p>若需要权限，加入<code>--user</code></p></blockquote></li><li><p>部分包用<code>apt-get</code>，但是优先安装到<code>Python2.7</code>版本，笑脸:-)</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install python-scipy</span><br><span class="line">$ sudo apt-get install python-matplotlib</span><br><span class="line">$ sudo apt-get install python-opencv</span><br></pre></td></tr></table></figure></li><li><p>部分从<code>PIPY</code>下载<code>.whl</code>或<code>.tar.gz</code>文件</p><blockquote><p><a href="https://pypi.org/" target="_blank" rel="noopener">PyPI – the Python Package Index · PyPI</a></p><ul><li>tensorboardX-1.4-py2.py3-none-any.whl</li><li>visdom-0.1.8.5.tar.gz</li></ul></blockquote><p>  安装指令为</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install xxx.whl</span><br></pre></td></tr></table></figure>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf xxx.tar.gz</span><br><span class="line">$ python setup.py install</span><br></pre></td></tr></table></figure></li><li><p><code>Pytorch</code>源码安装</p><blockquote><p><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration </a></p></blockquote><p>  安装方法<a href="https://github.com/pytorch/pytorch#from-source" target="_blank" rel="noopener">Installation - From Source</a></p><p>  需要用到<code>miniconda</code>，安装方法如下，注意中间回车按慢一点，有两次输入。。。。。(行我慢慢看条款不行么。。笑脸:-))</p><ul><li>第一次是是否同意条款，<code>yes</code></li><li><p>第二次是添加到环境变量，<code>yes</code>，否则自己修改<code>/home/pi/.bashrc</code>添加到环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://repo.continuum.io/miniconda/Miniconda3-latest-Linux-armv7l.sh</span><br><span class="line">$ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5</span><br><span class="line">$ sudo /bin/bash Miniconda3-latest-Linux-armv7l.sh </span><br><span class="line"># -&gt; change default directory to /home/pi/miniconda3</span><br><span class="line">$ sudo nano /home/pi/.bashrc </span><br><span class="line"># -&gt; add: export PATH=&quot;/home/pi/miniconda3/bin:$PATH&quot;</span><br><span class="line">$ sudo reboot -h now</span><br><span class="line"></span><br><span class="line">$ conda </span><br><span class="line">$ python --version</span><br><span class="line">$ sudo chown -R pi miniconda3</span><br></pre></td></tr></table></figure><p><del>然后就可以安装了</del>没有对应版本的<code>mkl</code>，笑脸:-)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; # [anaconda root directory]</span><br><span class="line"></span><br><span class="line"># Disable CUDA</span><br><span class="line">export NO_CUDA=1</span><br><span class="line"></span><br><span class="line"># Install basic dependencies</span><br><span class="line">conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing</span><br><span class="line">conda install -c mingfeima mkldnn</span><br><span class="line"></span><br><span class="line"># Install Pytorch</span><br><span class="line">git clone --recursive https://github.com/pytorch/pytorch</span><br><span class="line">cd pytorch</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>tensorflow</code><br>  安装tensorflow需要的一些依赖和工具</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line"></span><br><span class="line"># For Python 2.7</span><br><span class="line">$ sudo apt-get install python-pip python-dev</span><br><span class="line"></span><br><span class="line"># For Python 3.3+</span><br><span class="line">$ sudo apt-get install python3-pip python3-dev</span><br></pre></td></tr></table></figure><p>  安装<code>tensorflow</code></p><blockquote><p>若下载失败，手动打开下面网页下载<code>.whl</code>包</p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># For Python 2.7</span><br><span class="line">$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp27-none-linux_armv7l.whl</span><br><span class="line">$ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl</span><br><span class="line"></span><br><span class="line"># For Python 3.4</span><br><span class="line">$ wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl</span><br><span class="line">$ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl</span><br></pre></td></tr></table></figure><p>  卸载，重装mock</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># For Python 2.7</span><br><span class="line">$ sudo pip uninstall mock</span><br><span class="line">$ sudo pip install mock</span><br><span class="line"></span><br><span class="line"># For Python 3.3+</span><br><span class="line">$ sudo pip3 uninstall mock</span><br><span class="line">$ sudo pip3 install mock</span><br></pre></td></tr></table></figure><p>  安装的版本<code>tensorflow v1.1.0</code>没有<code>models</code>，因为1.0版本以后models就被<code>Sam Abrahams</code>独立出来了，例如<code>classify_image.py</code>就在<code>models/tutorials/image/imagenet/</code>里</p><blockquote><p><a href="https://github.com/tensorflow/models" target="_blank" rel="noopener">tensorflow/models</a></p></blockquote></li></ul><h2 id="其余"><a href="#其余" class="headerlink" title="其余"></a>其余</h2><ol><li><p>输入法 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install fcitx fcitx-googlepinyin </span><br><span class="line">$ fcitx-module-cloudpinyin fcitx-sunpinyin</span><br></pre></td></tr></table></figure></li><li><p><code>git</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install git</span><br></pre></td></tr></table></figure><p>配置<code>git</code>和<code>ssh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name &quot;Louis Hsu&quot;</span><br><span class="line">$ git config --global user.email is.louishsu@foxmail.com</span><br><span class="line"></span><br><span class="line">$ ssh-keygen -t rsa -C &quot;is.louishsu@foxmail.com&quot;</span><br><span class="line">$ cat ~/.ssh/id_rsa.pub  # 添加到github</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Underfitting &amp; Overfitting</title>
      <link href="/2018/10/26/Underfitting-Overfitting/"/>
      <url>/2018/10/26/Underfitting-Overfitting/</url>
      
        <content type="html"><![CDATA[<h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>放上一张非常经典的图，以下分别表示二分类模型中的欠拟合(underfit)、恰好(just right)、过拟合(overfit)，来自吴恩达课程笔记。<br><img src="/2018/10/26/Underfitting-Overfitting/underfit_justright_overfit.png" alt="underfit_justright_overfit"></p><ul><li>欠拟合的成因大多是模型不够复杂、拟合函数的能力不够；</li><li>过拟合成因是给定的数据集相对过于简单，使得模型在拟合函数时过分地考虑了噪声等不必要的数据间的关联，或者说相对于给定数据集，模型过于复杂、拟合能力过强。</li></ul><h1 id="判别方法"><a href="#判别方法" class="headerlink" title="判别方法"></a>判别方法</h1><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>可通过学习曲线<code>(Learning curve)</code>进行欠拟合与过拟合的判别。</p><p>学习曲线就是通过画出<strong>不同训练集大小</strong>时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。</p><h2 id="绘制"><a href="#绘制" class="headerlink" title="绘制"></a>绘制</h2><p>横轴为训练样本的数量，纵轴为损失或其他<a href="">评估准则</a>。<br><code>sklearn</code>中学习曲线绘制例程如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"></span><br><span class="line">digits = load_digits(); X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">100</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">estimator = GaussianNB()</span><br><span class="line">train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=<span class="number">4</span>, train_sizes=np.linspace(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">"Learning Curves (Naive Bayes)"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Training examples"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Score"</span>)</span><br><span class="line"></span><br><span class="line">train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.fill_between(train_sizes, </span><br><span class="line">                train_scores_mean - train_scores_std,</span><br><span class="line">                train_scores_mean + train_scores_std,</span><br><span class="line">                alpha=<span class="number">0.1</span>, color=<span class="string">"r"</span>)</span><br><span class="line">plt.fill_between(train_sizes,</span><br><span class="line">                test_scores_mean - test_scores_std,</span><br><span class="line">                test_scores_mean + test_scores_std,</span><br><span class="line">                alpha=<span class="number">0.1</span>, color=<span class="string">"g"</span>)</span><br><span class="line">plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>, label=<span class="string">"Training score"</span>)</span><br><span class="line">plt.plot(train_sizes, test_scores_mean,  <span class="string">'o-'</span>, color=<span class="string">"g"</span>, label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line"></span><br><span class="line">plt.grid(); plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/2018/10/26/Underfitting-Overfitting/learning_curve_nb.png" alt="learning_curve_nb"></p><h2 id="判别"><a href="#判别" class="headerlink" title="判别"></a>判别</h2><ul><li><strong>欠拟合</strong>，即高偏差<code>(high bias)</code>，训练集和测试集的误差收敛但却很高；</li><li><strong>过拟合</strong>，即高方差<code>(high variance)</code>，训练集和测试集的误差之间有大的差距。</li></ul><p><img src="/2018/10/26/Underfitting-Overfitting/learning_curve.png" alt="learning_curve"></p><h1 id="欠拟合解决方法"><a href="#欠拟合解决方法" class="headerlink" title="欠拟合解决方法"></a>欠拟合解决方法</h1><ul><li>增加迭代次数继续训练</li><li>增加模型复杂度</li><li>增加特征</li><li>减少正则化程度</li><li>采用Boosting等集成方法</li></ul><p>此时增加数据集并不能改善欠拟合问题。</p><h1 id="过拟合解决方法"><a href="#过拟合解决方法" class="headerlink" title="过拟合解决方法"></a>过拟合解决方法</h1><ul><li>提前停止训练</li><li>获取更多样本或数据扩增<ul><li>重采样</li><li>上采样</li><li>增加随机噪声</li><li><code>GAN</code></li><li>图像数据的空间变换（平移旋转镜像）</li><li>尺度变换（缩放裁剪）</li><li>颜色变换</li><li>改变分辨率</li><li>对比度</li><li>亮度</li></ul></li><li>降低模型复杂度</li><li>减少特征</li><li>增加正则化程度</li><li>神经网络可采用<code>Dropout</code></li><li>多模型投票方法</li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Cross Validation &amp; Hyperparameter</title>
      <link href="/2018/10/26/Cross-Validation-Hyperparameter/"/>
      <url>/2018/10/26/Cross-Validation-Hyperparameter/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉验证与超参数选择"><a href="#交叉验证与超参数选择" class="headerlink" title="交叉验证与超参数选择"></a>交叉验证与超参数选择</h1><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>以下简称交叉验证<code>(Cross Validation)</code>为<code>CV</code>.<code>CV</code>是用来验证分类器的性能一种统计分析方法,基本思想是把在某种意义下将原始数据<code>(dataset)</code>进行分组,一部分做为训练集<code>(train set)</code>,另一部分做为验证集<code>(validation set)</code>,首先用训练集对分类器进行训练,在利用验证集来测试训练得到的模型<code>(model)</code>,以此来做为评价分类器的性能指标。</p><h3 id="交叉验证的几种方法"><a href="#交叉验证的几种方法" class="headerlink" title="交叉验证的几种方法"></a>交叉验证的几种方法</h3><ul><li><p>k折交叉验证(K-fold)</p><ol><li>将全部训练集$S$分成$k$个不相交的子集，假设$S$中的训练样例个数为$m$，则每个子集中有$(\frac{m}{k})$个训练样例，相应子集称作$\{s_1, s_2, …, s_k\}$；</li><li>每次从分好的子集中，拿出$1$个作为测试集，其他$k-1$个作为训练集；</li><li>在$k-1$个训练集上训练出学习器模型，将模型放到测试集上，得到分类率；</li><li>计算k次求得的分类率平均值，作为该模型或者假设函数的真实分类率<br><img src="/2018/10/26/Cross-Validation-Hyperparameter/k-fold.jpg" alt="k-fold"></li></ol></li><li><p>留一法交叉验证(Leave One Out - LOO)<br>  假设有$N$个样本，将每个样本作为测试样本，其他$(N-1)$个样本作为训练样本。这样得到$N$个分类器，$N$个测试结果。用这$N$个结果的平均值衡量模型的性能。</p></li><li><p>留P法交叉验证(Leave P Out - LPO)<br>  将$P$个样本作为测试样本，其他$(N-P)$个样本作为训练样本。这样得到$\left(\begin{matrix}</p><pre><code>  P \\ N</code></pre><p>  \end{matrix}\right)$个训练测试对。当$P＞1$时，测试集会发生重叠。当$P=1$时，变成$LOO$。<br>  <img src="/2018/10/26/Cross-Validation-Hyperparameter/LPO.jpg" alt="LPO"></p></li></ul><h3 id="scikit-learn中的交叉验证"><a href="#scikit-learn中的交叉验证" class="headerlink" title="scikit-learn中的交叉验证"></a><code>scikit-learn</code>中的交叉验证</h3><p><img src="/2018/10/26/Cross-Validation-Hyperparameter/cross_validation_sklearn.png" alt="cross_validation_sklearn"></p><ul><li><p>K-fold</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.model_selection import KFold</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = ["a", "b", "c", "d"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; kf = KFold(n_splits=2)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for train, test in kf.split(X):</span><br><span class="line">... print("%s %s" % (train, test))</span><br><span class="line">[2 3] [0 1]</span><br><span class="line">[0 1] [2 3]</span><br></pre></td></tr></table></figure></li><li><p>Leave One Out (LOO)</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.model_selection import LeaveOneOut</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = [1, 2, 3, 4]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; loo = LeaveOneOut()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for train, test in loo.split(X):</span><br><span class="line">... print("%s %s" % (train, test))</span><br><span class="line">[1 2 3] [0]</span><br><span class="line">[0 2 3] [1]</span><br><span class="line">[0 1 3] [2]</span><br><span class="line">[0 1 2] [3]</span><br></pre></td></tr></table></figure></li><li><p>Leave P Out (LPO)</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.model_selection import LeavePOut</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; X = np.ones(4)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; lpo = LeavePOut(p=2)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for train, test in lpo.split(X):</span><br><span class="line">... print("%s %s" % (train, test))</span><br><span class="line">[2 3] [0 1]</span><br><span class="line">[1 3] [0 2]</span><br><span class="line">[1 2] [0 3]</span><br><span class="line">[0 3] [1 2]</span><br><span class="line">[0 2] [1 3]</span><br><span class="line">[0 1] [2 3]</span><br></pre></td></tr></table></figure></li></ul><h2 id="使用交叉验证调整超参数"><a href="#使用交叉验证调整超参数" class="headerlink" title="使用交叉验证调整超参数"></a>使用交叉验证调整超参数</h2><p>超参数的定义：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。<br>超参数例如</p><ul><li>模型（<code>SVM</code>，<code>Softmax</code>，<code>Multi-layer Neural Network</code>,…)；</li><li>迭代算法（<code>Adam</code>, <code>SGD</code>, …)(不同的迭代算法还有各种不同的超参数，如<code>beta1</code>,<code>beta2</code>等等，但常见的做法是使用默认值，不进行调参）；</li><li>学习率（<code>learning rate</code>)；</li><li>正则化方程的选择(<code>L0</code>,<code>L1</code>,<code>L2</code>)，正则化系数；</li><li><code>dropout</code>的概率</li><li>…</li></ul><h3 id="确定调节范围"><a href="#确定调节范围" class="headerlink" title="确定调节范围"></a>确定调节范围</h3><p>超参数的种类多，调节范围大，需要先进行简单的测试确定调参范围。</p><ul><li><p>模型选择<br>  模型的选择很大程度上取决于具体的实际问题，但必须通过几项基本测试。 </p><ul><li>可以通过第一个epoch的loss，观察模型能否无BUG运行，注意此过程需要设置正则项系数为0，因为正则项引入的loss难以估算。 </li><li>模型必须可以对于小数据集过拟合，否则应该尝试其他或者更复杂的模型。</li><li><p>若训练集与验证集loss均较大，则应该尝试其他或者更复杂的模型。</p><blockquote><p>模型选择的方法为：</p><ol><li>使用训练集训练出 10 个模型</li><li>用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li><li>选取代价函数值最小的模型</li><li>用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）<p align="right"> —— Andrew Ng, Stanford University </p></li></ol></blockquote></li></ul></li><li><p>学习率</p><ul><li>loss基本不变：学习率过低 </li><li>loss波动明显或者溢出：学习率过高 </li></ul></li><li><p>正则项系数</p><ul><li>val_acc与acc相差较大：正则项系数过小 </li><li>loss逐渐增大：正则项系数过大 </li></ul></li></ul><h3 id="超参数的确定"><a href="#超参数的确定" class="headerlink" title="超参数的确定"></a>超参数的确定</h3><ul><li><p>先粗调，再细调<br> 先通过数量少，间距大的粗调确定细调的大致范围。然后在小范围内部进行间距小，数量大的细调。</p></li><li><p>尝试在对数空间内进行调节<br>  即在对数空间内部随机生成测试参数，而不是在原空间生成，通常用于学习率以及正则项系数等的调节。出发点是该超参数的指数项对于模型的结果影响更显著；而同阶的数据之间即便原域相差较大，对于模型结果的影响反而不如不同阶的数据差距大。</p></li><li><p>超参数搜索<br>  随机搜索参数值，而不是网格搜索。</p></li></ul><h3 id="超参数搜索"><a href="#超参数搜索" class="headerlink" title="超参数搜索"></a>超参数搜索</h3><p><code>scikit-learn</code>提供超参数搜索方法，可参考官方文档</p><ul><li>网格搜索<br>  <a href="http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#exhaustive-grid-search" target="_blank" rel="noopener">3.2.1. Exhaustive Grid Search</a><br>  调用例程如下  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># get some data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># build a classifier</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function to report best scores</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span><span class="params">(results, n_top=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_top + <span class="number">1</span>):</span><br><span class="line">        candidates = np.flatnonzero(results[<span class="string">'rank_test_score'</span>] == i)</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            print(<span class="string">"Model with rank: &#123;0&#125;"</span>.format(i))</span><br><span class="line">            print(<span class="string">"Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)"</span>.format(</span><br><span class="line">                results[<span class="string">'mean_test_score'</span>][candidate],</span><br><span class="line">                results[<span class="string">'std_test_score'</span>][candidate]))</span><br><span class="line">            print(<span class="string">"Parameters: &#123;0&#125;"</span>.format(results[<span class="string">'params'</span>][candidate]))</span><br><span class="line">            print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use a full grid over all parameters</span></span><br><span class="line">param_grid = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="keyword">None</span>],</span><br><span class="line">            <span class="string">"max_features"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">            <span class="string">"min_samples_split"</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">            <span class="string">"min_samples_leaf"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>],</span><br><span class="line">            <span class="string">"bootstrap"</span>: [<span class="keyword">True</span>, <span class="keyword">False</span>],</span><br><span class="line">            <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run grid search</span></span><br><span class="line">grid_search = GridSearchCV(clf, param_grid=param_grid)</span><br><span class="line">start = time()</span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"GridSearchCV took %.2f seconds for %d candidate parameter settings."</span></span><br><span class="line">    % (time() - start, len(grid_search.cv_results_[<span class="string">'params'</span>])))</span><br><span class="line">report(grid_search.cv_results_)</span><br></pre></td></tr></table></figure></li></ul><ul><li>随机搜索<br>  <a href="http://sklearn.apachecn.org/en/0.19.0/modules/grid_search.html#randomized-parameter-optimization" target="_blank" rel="noopener">3.2.2. Randomized Parameter Optimization</a><br>  调用例程如下  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint <span class="keyword">as</span> sp_randint</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># get some data</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># build a classifier</span></span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Utility function to report best scores</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span><span class="params">(results, n_top=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n_top + <span class="number">1</span>):</span><br><span class="line">        candidates = np.flatnonzero(results[<span class="string">'rank_test_score'</span>] == i)</span><br><span class="line">        <span class="keyword">for</span> candidate <span class="keyword">in</span> candidates:</span><br><span class="line">            print(<span class="string">"Model with rank: &#123;0&#125;"</span>.format(i))</span><br><span class="line">            print(<span class="string">"Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)"</span>.format(</span><br><span class="line">                results[<span class="string">'mean_test_score'</span>][candidate],</span><br><span class="line">                results[<span class="string">'std_test_score'</span>][candidate]))</span><br><span class="line">            print(<span class="string">"Parameters: &#123;0&#125;"</span>.format(results[<span class="string">'params'</span>][candidate]))</span><br><span class="line">            print(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># specify parameters and distributions to sample from</span></span><br><span class="line">param_dist = &#123;<span class="string">"max_depth"</span>: [<span class="number">3</span>, <span class="keyword">None</span>],</span><br><span class="line">            <span class="string">"max_features"</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">            <span class="string">"min_samples_split"</span>: sp_randint(<span class="number">2</span>, <span class="number">11</span>),</span><br><span class="line">            <span class="string">"min_samples_leaf"</span>: sp_randint(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">            <span class="string">"bootstrap"</span>: [<span class="keyword">True</span>, <span class="keyword">False</span>],</span><br><span class="line">            <span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># run randomized search</span></span><br><span class="line">n_iter_search = <span class="number">20</span></span><br><span class="line">random_search = RandomizedSearchCV(clf, param_distributions=param_dist,</span><br><span class="line">                                n_iter=n_iter_search)</span><br><span class="line"></span><br><span class="line">start = time()</span><br><span class="line">random_search.fit(X, y)</span><br><span class="line">print(<span class="string">"RandomizedSearchCV took %.2f seconds for %d candidates"</span></span><br><span class="line">    <span class="string">" parameter settings."</span> % ((time() - start), n_iter_search))</span><br><span class="line">report(random_search.cv_results_)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spam Classification</title>
      <link href="/2018/10/26/Spam-Classification/"/>
      <url>/2018/10/26/Spam-Classification/</url>
      
        <content type="html"><![CDATA[<blockquote><p>踩坑？？？全部给我踩平！！！</p></blockquote><p>来自<a href="https://www.lintcode.com/ai/spam-message-classification/overview" target="_blank" rel="noopener">LintCode垃圾短信分类</a><br><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/ZhaoHaitao%2C%20ECUST/spam%20or%20ham" target="_blank" rel="noopener">@Github: spam or ham</a></p><h1 id="垒代码"><a href="#垒代码" class="headerlink" title="垒代码"></a>垒代码</h1><h2 id="预处理及向量化"><a href="#预处理及向量化" class="headerlink" title="预处理及向量化"></a>预处理及向量化</h2><p>观察各文本后，发现各文本中包含的单词多种多样，包含标点、数字等，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</span><br><span class="line">- XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http://wap. </span><br><span class="line">- 07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder.</span><br></pre></td></tr></table></figure></p><p>且按空格分词后，部分单词中仍包含<code>whitespace</code>，故选择的预处理方案是，<strong>去除分词后文本中的标点、数字、空格等，并将单词中字母全部转为小写</strong>。</p><blockquote><p>中文分词可采用<code>jieba</code>(街霸？)</p></blockquote><p>预处理后，按当前的文本内容建立字典，并统计各样本的词数向量，详细代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Words2Vector</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    建立字典，将输入的词列表转换为向量，表示各词出现的次数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.dict = <span class="keyword">None</span></span><br><span class="line">        self.n_word = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        self.fit(words)</span><br><span class="line">        <span class="keyword">return</span> self.transform(words)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        words = _flatten(words)                                                 <span class="comment"># 展开为1维列表</span></span><br><span class="line">        words = self.filt(words)                                                <span class="comment"># 滤除空格、数字、标点</span></span><br><span class="line"></span><br><span class="line">        self.word = list(set(words))                                            <span class="comment"># 去重</span></span><br><span class="line">        self.n_word = len(set(words))                                           <span class="comment"># 统计词的个数</span></span><br><span class="line">        self.dict = dict(zip(self.word, [_ <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.n_word)]))       <span class="comment"># 各词在字典中的位置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">        @return &#123;ndarray&#125; retarray: vector</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        retarray = np.zeros(shape=(len(words), self.n_word))                    <span class="comment"># 返回的词数向量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</span><br><span class="line">            words[i] = self.filt(words[i])                                      <span class="comment"># 滤除空格、数字、标点</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words)):</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> words[i]:</span><br><span class="line">                <span class="keyword">if</span> w <span class="keyword">in</span> self.word:                                              <span class="comment"># 是否在训练集生成的字典中</span></span><br><span class="line">                    retarray[i, self.dict[w]] += <span class="number">1</span>                              <span class="comment"># 查询字典，找到对应特征的下标</span></span><br><span class="line">        <span class="keyword">return</span> retarray</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filt</span><span class="params">(self, flattenWords)</span>:</span></span><br><span class="line">        retWords = []</span><br><span class="line">        en_stops = set(stopwords.words(<span class="string">'english'</span>))                              <span class="comment"># 停用词列表</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> flattenWords:</span><br><span class="line">            word = word.translate(str.maketrans(<span class="string">''</span>, <span class="string">''</span>, string.whitespace))     <span class="comment"># 去除空白</span></span><br><span class="line">            word = word.translate(str.maketrans(<span class="string">''</span>, <span class="string">''</span>, string.punctuation))    <span class="comment"># 去除标点</span></span><br><span class="line">            word = word.translate(str.maketrans(<span class="string">''</span>, <span class="string">''</span>, string.digits))         <span class="comment"># 去除数字</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> en_stops <span class="keyword">and</span> (len(word) &gt; <span class="number">1</span>):                        <span class="comment"># 删除停用词，并除去长度小于等于2的词</span></span><br><span class="line">                retWords.append(word.lower())</span><br><span class="line">        <span class="keyword">return</span> retWords</span><br></pre></td></tr></table></figure></p><h2 id="TF-IDF方法"><a href="#TF-IDF方法" class="headerlink" title="TF-IDF方法"></a>TF-IDF方法</h2><p>由词数向量可计算词频，但只用词频忽略了各文本在不同文档中的重要程度，关于<code>TF-IDF</code>，在<a href="https://louishsu.xyz/2018/10/25/TF-IDF/" target="_blank" rel="noopener">另一篇博文</a>中详细说明。</p><p>由于剔除了停用词等，部分向量不包含任何内容，即词数向量为$\vec{0}$，这时计算词频和单位化时，会出现<code>nan</code>的运算结果，故只对非空向量进行计算。</p><p>训练后需要保存的是<code>IDF</code>向量，<code>TF</code>向量在新样本输入后重新计算，故无需保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TfidfVectorizer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.idf = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, num_vec)</span>:</span></span><br><span class="line">        self.fit(num_vec)</span><br><span class="line">        <span class="keyword">return</span> self.transform(num_vec)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, num_vec)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_vec[num_vec&gt;<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        n_doc = num_vec.shape[<span class="number">0</span>]</span><br><span class="line">        n_term = np.sum(num_vec, axis=<span class="number">0</span>)    <span class="comment"># 各词出现过的文档次数</span></span><br><span class="line">        self.idf = np.log((n_doc + <span class="number">1</span>) / (n_term + <span class="number">1</span>)) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.idf</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, num_vec)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 求解词频向量，由于部分向量为空，故下句会出现问题</span></span><br><span class="line">        <span class="comment"># tf = num_vec / np.sum(num_vec, axis=1).reshape(-1, 1) =&gt; nan</span></span><br><span class="line">        <span class="comment"># 解决方法：只对非空向量进行词频计算</span></span><br><span class="line">        tf = np.zeros(shape=num_vec.shape)</span><br><span class="line">        n_terms = np.sum(num_vec, axis=<span class="number">1</span>); idx = (n_terms!=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        tf[idx] = num_vec[idx] / n_terms[idx].reshape(<span class="number">-1</span>, <span class="number">1</span>)            <span class="comment"># 计算词频，只对非空向量进行</span></span><br><span class="line">        </span><br><span class="line">        tfidf = tf * self.idf</span><br><span class="line">        tfidf[idx] /= np.linalg.norm(tfidf, axis=<span class="number">1</span>)[idx].reshape(<span class="number">-1</span>, <span class="number">1</span>) <span class="comment"># 单位化，只对非空向量进行</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> tfidf</span><br></pre></td></tr></table></figure><h2 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h2><p>各文本向量化后，就可通过机器学习算法进行模型的训练和预测，这里采用的是贝叶斯决策的方法，需要注意的有以下几点</p><ul><li>似然函数$p(x|c_k)$与<a href="https://louishsu.xyz/2018/10/18/Bayes-Decision/" target="_blank" rel="noopener">贝叶斯决策</a>文中例不同，这里宜采用高斯分布作为分布模型；</li><li><p>按朴素贝叶斯计算$p(x|c_k)$，但注意此处不能将各维特征单独训练$1$维高斯分布模型，然后计算预测样本似然函数值时进行累乘，如下</p><script type="math/tex; mode=display">p(x|c_k) = \prod_{j=1}^{N_feature} p(x_j|c_k)</script><p>因为特征维度特别高，各个特征单独用$1$维高斯分布描述，累乘计算会下溢，故这里采用多元高斯分布</p><script type="math/tex; mode=display">p(x|c_k) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}} · e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)}</script><ul><li>且经主成分分析后，各维度间线性相关性降低，故假定<script type="math/tex; mode=display">\Sigma_k = diag\{\sigma_{k1}, ..., \sigma_{kn}\}</script></li><li><p>但分母$(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}$在计算时不稳定，且各特征标准差大小相差无几，故这里假定</p><script type="math/tex; mode=display">\Sigma_k = I</script></li><li><p>最终简化后的似然函数计算方法为</p><script type="math/tex; mode=display">p(x|c_k) =  e^{-\frac{1}{2} (x - \mu_k)^T (x - \mu_k)}</script></li></ul></li></ul><h3 id="贝叶斯决策模型训练"><a href="#贝叶斯决策模型训练" class="headerlink" title="贝叶斯决策模型训练"></a>贝叶斯决策模型训练</h3><p>基于上述假设，只需训练多元高斯分布的各维均值$\mu_j$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, labels, text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; labels: shape(N_samples, ), labels[i] \in &#123;0, 1&#125;</span></span><br><span class="line"><span class="string">    @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    labels = self.encodeLabel(labels); words = self.text2words(self.clean(text))</span><br><span class="line"></span><br><span class="line">    vecwords = self.numvectorizer.fit_transform(words)              <span class="comment"># 向量化</span></span><br><span class="line">    vecwords = self.tfidfvectorizer.fit_transform(vecwords)         <span class="comment"># tfidf, shape(N_samples, N_features)</span></span><br><span class="line"></span><br><span class="line">    isnotEmpty = (np.sum(vecwords, axis=<span class="number">1</span>)!=<span class="number">0</span>)                      <span class="comment"># 去掉空的样本</span></span><br><span class="line">    vecwords = vecwords[isnotEmpty]; labels = labels[isnotEmpty]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># vecwords = self.reduce_dim.fit_transform(vecwords)              # 降维，计算量太大</span></span><br><span class="line">    self.n_features = vecwords.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    labels = OneHotEncoder().fit_transform(labels.reshape((<span class="number">-1</span>, <span class="number">1</span>))).toarray()</span><br><span class="line">        self.priori = np.mean(labels, axis=<span class="number">0</span>)                           <span class="comment"># 先验概率</span></span><br><span class="line"></span><br><span class="line">    self.likelihood_mu = np.zeros(shape=(<span class="number">2</span>, vecwords.shape[<span class="number">1</span>]))    <span class="comment"># 设似然函数p(x|c)为高斯分布</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        vec = vecwords[labels[:, i]==<span class="number">1</span>]</span><br><span class="line">        self.likelihood_mu[i] = np.mean(vec, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="贝叶斯决策模型预测"><a href="#贝叶斯决策模型预测" class="headerlink" title="贝叶斯决策模型预测"></a>贝叶斯决策模型预测</h3><p>决策函数为</p><script type="math/tex; mode=display">if　p(x|c_i)P(c_i) > p(x|c_j)P(c_j),　then　x \in c_i</script><p>但实际效果显示，等先验概率$P(c_j)$结果更好$(???)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multigaussian</span><span class="params">(self, x, mu)</span>:</span></span><br><span class="line">    <span class="string">""" 简化</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = x - mu</span><br><span class="line">    a = np.exp(<span class="number">-0.5</span> * x.T.dot(x))</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param &#123;list[list[str]]&#125; words</span></span><br><span class="line"><span class="string">    @note:</span></span><br><span class="line"><span class="string">                      p(x|c)P(c)</span></span><br><span class="line"><span class="string">            P(c|x) = ------------</span></span><br><span class="line"><span class="string">                         p(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pred_porba = np.ones(shape=(len(self.clean(text)), <span class="number">2</span>))      </span><br><span class="line">        </span><br><span class="line">    words = self.text2words(text)</span><br><span class="line">    vecwords = self.tfidfvectorizer.transform(</span><br><span class="line">                                self.numvectorizer.transform(words))    <span class="comment"># 向量化</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(vecwords.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            <span class="comment"># pred_porba[i, c] = self.priori[c] * self.multigaussian(vecwords[i], self.likelihood_mu[c])</span></span><br><span class="line">            pred_porba[i, c] = self.multigaussian(vecwords[i], self.likelihood_mu[c])</span><br><span class="line"></span><br><span class="line">    pred = np.argmax(pred_porba, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> self.decodeLabel(pred)</span><br></pre></td></tr></table></figure><h1 id="调包"><a href="#调包" class="headerlink" title="调包"></a>调包</h1><p>主要用到了<code>scikit-learn</code>机器学习包以下几个功能</p><ul><li><code>sklearn.feature_extraction.text.TfidfVectorizer()</code></li><li><code>sklearn.decomposition.PCA()</code></li><li><code>sklearn.naive_bayes.BernoulliNB()</code></li></ul><p>最终准确率在$97\%$左右，代码比较简单，不进行说明。</p><blockquote><p>采用<code>sklearn.linear_model import.LogisticRegressionCV()</code>效果更佳</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    trainfile = <span class="string">"./data/train.csv"</span></span><br><span class="line">    testfile = <span class="string">"./data/test.csv"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 读取原始数据</span></span><br><span class="line">    data_train = pd.read_csv(trainfile, names=[<span class="string">'Label'</span>, <span class="string">'Text'</span>])</span><br><span class="line">    txt_train  = list(data_train[<span class="string">'Text'</span>])[<span class="number">1</span>: ]; label_train = list(data_train[<span class="string">'Label'</span>])[<span class="number">1</span>: ]</span><br><span class="line">    drop(txt_train)                                             <span class="comment"># 删除数字和标点</span></span><br><span class="line">    txt_test   = list(pd.read_csv(testfile, names=[<span class="string">'Text'</span>])[<span class="string">'Text'</span>])[<span class="number">1</span>: ]</span><br><span class="line">    drop(txt_test)                                              <span class="comment"># 删除数字和标点</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    vectorizer = TfidfVectorizer(stop_words=<span class="string">'english'</span>)          <span class="comment"># 删除英文停用词</span></span><br><span class="line">    vec_train = vectorizer.fit_transform(txt_train).toarray()   <span class="comment"># 提取文本特征向量</span></span><br><span class="line">    <span class="comment"># reduce_dim = PCA(n_components = 4096)</span></span><br><span class="line">    <span class="comment"># vec_train = reduce_dim.fit_transform(vec_train)</span></span><br><span class="line">    estimator = BernoulliNB()</span><br><span class="line">    estimator.fit(vec_train, label_train)                       <span class="comment"># 训练朴素贝叶斯模型</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试</span></span><br><span class="line">    label_train_pred = estimator.predict(vec_train)</span><br><span class="line">    acc = np.mean((label_train_pred==label_train).astype(<span class="string">'float'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    vec_test = vectorizer.transform(txt_test).toarray()</span><br><span class="line">    <span class="comment"># vec_test = reduce_dim.transform(vec_test)</span></span><br><span class="line">    label_test_pred = estimator.predict(vec_test)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/sampleSubmission.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(label_test_pred.shape[<span class="number">0</span>]):</span><br><span class="line">            f.write(label_test_pred[i] + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TF-IDF</title>
      <link href="/2018/10/25/TF-IDF/"/>
      <url>/2018/10/25/TF-IDF/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>正在做<a href="https://www.lintcode.com/" target="_blank" rel="noopener">LintCode</a>上的垃圾邮件分类，使用<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">朴素贝叶斯</a>方法解决，涉及到文本特征的提取。<br>TF-IDF（词频-逆文档频率）算法是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p><h1 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h1><h2 id="词频-TF"><a href="#词频-TF" class="headerlink" title="词频(TF)"></a>词频(TF)</h2><p><code>Term Frequency</code>，就是某个关键字出现的频率，具体来讲，就是词库中的<strong>某个词</strong>在<strong>当前文章</strong>中出现的频率。那么我们可以写出它的计算公式：</p><script type="math/tex; mode=display">TF_{ij} = \frac{n_{ij}}{\sum_k n_{i, k}}</script><p>其中，$n_{ij}$表示关键词$j$在文档$i$中的出现次数。</p><p>单纯使用TF来评估关键词的重要性忽略了常用词的干扰。常用词就是指那些文章中大量用到的，但是不能反映文章性质的那种词，比如：因为、所以、因此等等的连词，在英文文章里就体现为and、the、of等等的词。这些词往往拥有较高的TF，所以仅仅使用TF来考察一个词的关键性，是不够的。</p><h2 id="逆文档频率-IDF"><a href="#逆文档频率-IDF" class="headerlink" title="逆文档频率(IDF)"></a>逆文档频率(IDF)</h2><p><code>Inverse Document Frequency</code>，文档频率就是一个词在整个文库词典中出现的频率，逆文档频率用下式计算</p><script type="math/tex; mode=display">IDF_j = \log \frac{|D|}{|D_j| + 1}</script><p>其中，$|D|$表示总的文档数目，$|D_j|$表示关键词$j$出现过的文档数目</p><p><code>scikit-learn</code>内为</p><script type="math/tex; mode=display">IDF_j = \log \frac{|D| + 1}{|D_j| + 1} + 1</script><p><img src="/2018/10/25/TF-IDF/sklearn.jpg" alt="sklearn_tfidf"></p><h2 id="词频-逆文档频率-TF-IDF"><a href="#词频-逆文档频率-TF-IDF" class="headerlink" title="词频-逆文档频率(TF-IDF)"></a>词频-逆文档频率(TF-IDF)</h2><script type="math/tex; mode=display">TF-IDF_{i} = TF_i × IDF</script><h1 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h1><p>例如有如下$3$个文本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：My dog ate my homework.</span><br><span class="line">文本2：My cat ate the sandwich.</span><br><span class="line">文本3：A dolphin ate the homework.</span><br></pre></td></tr></table></figure></p><p>提取字典，一般需要处理大小写、去除停用词<code>a</code>，处理结果为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ate, cat, dog, dolphin, homework, my, sandwich, the</span><br></pre></td></tr></table></figure></p><p>故各个文本的词数向量为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：[1, 0, 1, 0, 1, 2, 0, 0]</span><br><span class="line">文本2：[1, 1, 0, 0, 0, 1, 1, 1]</span><br><span class="line">文本3：[1, 0, 0, 1, 1, 0, 0, 1]</span><br></pre></td></tr></table></figure></p><p>各个文本的词频向量(TF)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文本1：[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ]</span><br><span class="line">文本2：[0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ]</span><br><span class="line">文本3：[0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]</span><br></pre></td></tr></table></figure></p><p>各词出现过的文档次数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3, 1, 1, 1, 2, 2, 1, 2]</span><br></pre></td></tr></table></figure></p><p>总文档数为$3$，各词的逆文档频率(IDF)向量</p><blockquote><p>这里使用<code>scikit-learn</code>内的方法求解</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207,  1.28768207, 1.69314718, 1.28768207]</span><br></pre></td></tr></table></figure><p>故各文档的TF-IDF向量为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文本1：</span><br><span class="line">[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ]</span><br><span class="line">文本2：</span><br><span class="line">[0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641]</span><br><span class="line">文本3：</span><br><span class="line">[0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]</span><br></pre></td></tr></table></figure></p><p>经单位化后，有<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文本1：</span><br><span class="line">[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ]</span><br><span class="line">文本2：</span><br><span class="line">[0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178]</span><br><span class="line">文本3：</span><br><span class="line">[0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]</span><br></pre></td></tr></table></figure></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; import numpy as np</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_num = np.array([</span><br><span class="line">[1, 0, 1, 0, 1, 2, 0, 0],</span><br><span class="line">[1, 1, 0, 0, 0, 1, 1, 1],</span><br><span class="line">[1, 0, 0, 1, 1, 0, 0, 1]</span><br><span class="line">])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tf = vec_num / np.sum(vec_num, axis=1).reshape(-1, 1)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tf</span><br><span class="line">array([[0.2 , 0.  , 0.2 , 0.  , 0.2 , 0.4 , 0.  , 0.  ],</span><br><span class="line">       [0.2 , 0.2 , 0.  , 0.  , 0.  , 0.2 , 0.2 , 0.2 ],</span><br><span class="line">       [0.25, 0.  , 0.  , 0.25, 0.25, 0.  , 0.  , 0.25]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_num[vec_num&gt;0] = 1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; n_showup = np.sum(vec_num, axis=0)</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; n_showup</span><br><span class="line">array([3, 1, 1, 1, 2, 2, 1, 2])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; d = 3</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_idf = np.log((d + 1) / (n_showup + 1)) + 1</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_idf</span><br><span class="line">array([1.        , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf = vec_tf * vec_idf</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf</span><br><span class="line">array([[0.2       , 0.        , 0.33862944, 0.        , 0.25753641, 0.51507283, 0.        , 0.        ],</span><br><span class="line">       [0.2       , 0.33862944, 0.        , 0.        , 0.        , 0.25753641, 0.33862944, 0.25753641],</span><br><span class="line">       [0.25      , 0.        , 0.        , 0.4232868 , 0.32192052, 0.        , 0.        , 0.32192052]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf = vec_tfidf / np.linalg.norm(vec_tfidf, axis=1).reshape((-1, 1))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vec_tfidf</span><br><span class="line">array([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805, 0.73861611, 0.        , 0.        ],</span><br><span class="line">       [0.31544415, 0.53409337, 0.        , 0.        , 0.        , 0.40619178, 0.53409337, 0.40619178],</span><br><span class="line">       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 , 0.        , 0.        , 0.4804584 ]])</span><br></pre></td></tr></table></figure><h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><p>使用<code>scikit-learn</code>机器学习包计算结果<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vectorizer = TfidfVectorizer()</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; text = [</span><br><span class="line">"My dog ate my homework",</span><br><span class="line">"My cat ate the sandwich",</span><br><span class="line">"A dolphin ate the homework"]</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vectorizer.fit_transform(text).toarray()</span><br><span class="line">array([[0.28680065, 0.        , 0.48559571, 0.        , 0.36930805,  0.73861611, 0.        , 0.        ],</span><br><span class="line">       [0.31544415, 0.53409337, 0.        , 0.        , 0.        ,  0.40619178, 0.53409337, 0.40619178],</span><br><span class="line">       [0.37311881, 0.        , 0.        , 0.63174505, 0.4804584 ,  0.        , 0.        , 0.4804584 ]])</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; vectorizer.get_feature_names()</span><br><span class="line">['ate', 'cat', 'dog', 'dolphin', 'homework', 'my', 'sandwich', 'the']</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Practice </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVD</title>
      <link href="/2018/10/23/SVD/"/>
      <url>/2018/10/23/SVD/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>奇异值分解<code>Singular Value Decomposition</code>是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="从特征值分解-EVD-讲起"><a href="#从特征值分解-EVD-讲起" class="headerlink" title="从特征值分解(EVD)讲起"></a>从特征值分解(EVD)讲起</h2><p>我们知道对于一个$n$阶方阵$A_{n×n}$，有</p><script type="math/tex; mode=display">A\alpha_i = \lambda_i \alpha_i　i = 1, ..., n</script><p>取</p><script type="math/tex; mode=display">P = \left[\alpha_1, \alpha_2, ..., \alpha_n\right]</script><p>有下式成立</p><script type="math/tex; mode=display">AP = P\Lambda</script><p>其中</p><script type="math/tex; mode=display">\Lambda = \left[        \begin{matrix}            \lambda_1 & & \\            & ... & \\            & & \lambda_n \\        \end{matrix}\right]</script><blockquote><p>特征值一般从大到小排列</p></blockquote><p>利用该式可将方阵$A_{n×n}$化作对角阵$\Lambda_{n×n}$</p><script type="math/tex; mode=display">\Lambda = P^{-1}AP</script><p>或者</p><script type="math/tex; mode=display">A = P \Lambda P^{-1} = \sum_{i=1}^n \lambda_i (P_{,i})(P_{,i})^{-1}</script><blockquote><p>“$_{i}$”表示第$i$行，“$_{,i}$”表示第$i$列</p></blockquote><p>这样我们就可以理解为，矩阵$A$是由$n$个$n$阶矩阵$P_{,i}P^{-1}_{i}$加权组成，特征值$\lambda_i$即为权重。</p><blockquote><p>以上为个人理解，不妥之处可以指出。</p></blockquote><h2 id="奇异值分解-SVD"><a href="#奇异值分解-SVD" class="headerlink" title="奇异值分解(SVD)"></a>奇异值分解(SVD)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>对于长方阵$A_{m×n}$，不能进行特征值分解，可进行如下分解</p><script type="math/tex; mode=display">A_{m×n} = U_{m×m} \Sigma_{m×n} V_{n×n}^T</script><p>其中$U \in \mathbb{R}^{m×m}, V \in \mathbb{R}^{n×n}$，均为正交矩阵。矩阵$\Sigma_{m×n}$如下</p><ul><li><p>对于$m&gt;n$</p><script type="math/tex; mode=display">  \Sigma_{m×n} = \left[          \begin{matrix}              S_{n×n} \\              --- \\              O_{(m-n)×n}          \end{matrix}  \right]</script></li><li><p>对于$m&lt;n$</p><script type="math/tex; mode=display">  \Sigma_{m×n} = \left[          \begin{matrix}              S_{m×m} & | & O_{m×(n-m)}          \end{matrix}  \right]</script></li></ul><p>矩阵$S_{n×n}$为对角阵，对角元素从大到小排列</p><script type="math/tex; mode=display">S_{n×n} = \left[    \begin{matrix}        \sigma_1 & & \\         & ... & \\         & & \sigma_n\\    \end{matrix}\right]</script><p>直观表示<code>SVD</code>分解如下<br><img src="/2018/10/23/SVD/直观表示SVD.jpg" alt="直观表示SVD"></p><p>当取$r&lt;n$时，有部分奇异值分解，可用于降维</p><script type="math/tex; mode=display">A_{m×n} = U_{m×r} \Sigma_{r×r} V_{r×n}^T</script><h3 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h3><blockquote><p>以下仅考虑$m&gt;n$的情况</p></blockquote><ol><li><p>令矩阵$A^T$与$A$相乘，有</p><script type="math/tex; mode=display"> A^TA = (U \Sigma V^T)^T (U \Sigma V^T)</script><script type="math/tex; mode=display"> = V \Sigma^T U^T U \Sigma V^T</script><script type="math/tex; mode=display"> A^TA = V \Sigma^T \Sigma V^T</script><blockquote><p>矩阵$U$为正交阵，即满足$U^TU=I$</p></blockquote><p> 其中</p><script type="math/tex; mode=display"> \Sigma^T \Sigma =          \left[             \begin{matrix}                 S^T_{n×n} & | & O^T_{n×(m-n)}             \end{matrix}         \right]         \left[             \begin{matrix}                 S_{n×n} \\                 --- \\                 O_{(m-n)×n}             \end{matrix}         \right]</script><script type="math/tex; mode=display"> = S_{n×n}^2  = \left[     \begin{matrix}         \sigma_1^2 & & \\         & ... & \\         & & \sigma_n^2\\     \end{matrix} \right]</script><p> 则</p><script type="math/tex; mode=display"> A^T A = V S^2  V^T</script><p> 即矩阵$A^T A$相似对角化为$S^2$，对角元素$\sigma_i^2$与矩阵$V$的列向量$v_i(i=1, …, n)$为矩阵$A^T A$的特征对。</p><p> 那么对矩阵$A^T A$进行特征值分解，有</p><script type="math/tex; mode=display"> (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i</script><p> 则</p><script type="math/tex; mode=display"> v_i = \alpha^{(1)}_i　\sigma_i = \sqrt{\lambda^{(1)}_i}</script><blockquote><p>注：对于二次型$x^T (A^T A) x$</p><script type="math/tex; mode=display">x^T (A^T A) x = (Ax)^T(Ax) \geq 0</script><p>故矩阵$A^T A$半正定，$\sigma_i = \sqrt{\lambda_i}$有解</p></blockquote></li></ol><ol><li><p>同理，令矩阵$A$与$A^T$相乘，可证得</p><script type="math/tex; mode=display"> A A^T = U \Sigma \Sigma^T U^T</script><p> 其中</p><script type="math/tex; mode=display"> \Sigma \Sigma^T =          \left[             \begin{matrix}                 S_{n×n} \\                 --- \\                 O_{(m-n)×n}             \end{matrix}         \right]         \left[             \begin{matrix}                 S^T_{n×n} & | & O^T_{n×(m-n)}             \end{matrix}         \right]</script><script type="math/tex; mode=display"> = \left[     \begin{matrix}         S^2_{n×n} & O_{n×(m-n)} \\         O_{(m-n)×n} & O_{(m-n)×(m-n)}     \end{matrix} \right]</script><p> 即矩阵$A A^T$相似对角化，对角元素$\sigma_i^2$与矩阵$U$的列向量$u_i(i=1, …, m)$为矩阵$A A^T$的特征对。</p><p> 对矩阵$A A^T$进行特征值分解，有</p><script type="math/tex; mode=display"> (A^T A) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i</script><p> 则</p><script type="math/tex; mode=display"> u_i = \alpha^{(2)}_i　\sigma_i = \sqrt{\lambda^{(2)}_i}</script><blockquote><p>同理可证得$A A^T$半正定，略。</p></blockquote></li></ol><p>一般来说，为减少计算量，计算奇异值分解只进行一次特征值分解，如对于矩阵$X_{m×n}(m&gt;n)$，选取$n$阶矩阵$X^T X$进行特征值分解计算$v_i$，计算$u_i$方法下面介绍。</p><p>根据前面推导，我们有特征值分解</p><script type="math/tex; mode=display">(A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i</script><script type="math/tex; mode=display">(A A^T) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i</script><p>其中$\lambda^{(1)}_i = \lambda^{(2)}_i = \sigma_i^2$，$v_i = \alpha^{(1)}_i$，$u_i = \alpha^{(2)}_i$，即</p><script type="math/tex; mode=display">A^T A v_i = \sigma_i^2 v_i \tag{1}</script><script type="math/tex; mode=display">A A^T u_i = \sigma_i^2 u_i \tag{2}</script><p>$(1)$式左右乘$A$，有</p><script type="math/tex; mode=display">A A^T A v_i = \sigma_i^2 A v_i</script><p>发现什么？这是另一个特征值分解的表达式！</p><script type="math/tex; mode=display">(A A^T) (A v_i) = \sigma_i^2 (A v_i)</script><p>故</p><script type="math/tex; mode=display">u_i \propto A v_i　或　u_i = k · A v_i \tag{3}</script><p>现在求解系数$k$，根据定义</p><script type="math/tex; mode=display">A = U \Sigma V^T　\Rightarrow　AV = U \Sigma</script><p>则</p><script type="math/tex; mode=display">A v_i = \sigma_i u_i　\Rightarrow　u_i = \frac{1}{\sigma_i} A v_i</script><p>或者</p><script type="math/tex; mode=display">U = A V \Sigma^{-1}</script><blockquote><p>注：只能求前$n$个$u_i$，之后的需要列写方程求解</p></blockquote><h1 id="举栗"><a href="#举栗" class="headerlink" title="举栗"></a>举栗</h1><p>将矩阵$A$进行分解</p><script type="math/tex; mode=display">A = \left[    \begin{matrix}        0 & 1 \\        1 & 1 \\        1 & 0    \end{matrix}\right]</script><p>为减少计算量，取$A^T A$计算</p><script type="math/tex; mode=display">A^T A = \left[    \begin{matrix}        2 & 1 \\        1 & 2     \end{matrix}\right]</script><p>特征值分解，有</p><script type="math/tex; mode=display">A\left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]= \left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]\left[    \begin{matrix}        3 &  \\          & 1    \end{matrix} \right]</script><p>故</p><script type="math/tex; mode=display">\Sigma = \left[    \begin{matrix}        \sqrt{3} &  \\          & 1    \end{matrix} \right]　V = \left[    \begin{matrix}        \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}    \end{matrix} \right]</script><script type="math/tex; mode=display">U = A V \Sigma^{-1} = \left[    \begin{matrix}        \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\        \frac{2}{\sqrt{6}} & 0 \\        \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}}    \end{matrix} \right]</script><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import numpy as np</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; A = np.array([</span></span><br><span class="line">[0, 1], [1, 1], [1, 0]</span><br><span class="line">])</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; ATA = A.T.dot(A)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; eigval, eigvec= np.linalg.eig(ATA)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; V = eigvec.copy()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; S = np.diag(np.sqrt(eigval))</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; U = A.dot(V).dot(np.linalg.inv(S))</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; U</span></span><br><span class="line">array([[ 0.40824829,  0.70710678],</span><br><span class="line">       [ 0.81649658,  0.        ],</span><br><span class="line">       [ 0.40824829, -0.70710678]])</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; S</span></span><br><span class="line">array([[1.73205081, 0.        ],</span><br><span class="line">       [0.        , 1.        ]])</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; V</span></span><br><span class="line">array([[ 0.70710678, -0.70710678],</span><br><span class="line">       [ 0.70710678,  0.70710678]])</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="comment"># 验证</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; U.dot(S).dot(V.T)</span></span><br><span class="line">array([[-2.23711432e-17,  1.00000000e+00],</span><br><span class="line">       [ 1.00000000e+00,  1.00000000e+00],</span><br><span class="line">       [ 1.00000000e+00, -2.23711432e-17]])</span><br></pre></td></tr></table></figure><h1 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h1><p>展开表达式，取$r \leq n$时，</p><script type="math/tex; mode=display">A = U_{m×r} \Sigma_{r×r} V_{r×n}^T = \sum_{i=1}^r \sigma_i (U_{,i}) (V_{,i})^T</script><p>就得到与<code>PCA</code>相同的结论，矩阵$A$可由$r$个$m×n$的矩阵$(U_{,i}) (V_{,i})^T$加权组成。一般来说，前$10\%$甚至$1\%$的奇异值就占了全部奇异值之和的$99\%$，极大地保留了信息，而大大减少了存储空间。</p><blockquote><p>以图片为例，若原有<code>24bit</code>图片，其大小为<code>(1024, 768)</code>，则不计图片信息，仅仅数据共占<code>1024×768×3 B</code>，或<code>2.25 MB</code>。用奇异值分解进行压缩，保留$60\%$的奇异值，可达到几乎无损的程度，此时需要保存向量矩阵$U_{1024×60}$，$V_{60×768}$以及$60$个奇异值，以浮点数<code>float32</code>存储，一共占<code>420 KB</code>即可。</p><script type="math/tex; mode=display">(1024 × 60 + 60 × 768 + 60) × 4 / 2^{10} = 420.23</script><p>说句题外话，存储量的压缩必然以计算量的增大为代价，相反亦然，所以需要协调好<code>RAM</code>与<code>ROM</code>容量，考虑计算机的计算速度。换句话说，空间和时间上必然是互补的，哲学的味道hhhh。</p></blockquote><h1 id="分解结果的信息保留"><a href="#分解结果的信息保留" class="headerlink" title="分解结果的信息保留"></a>分解结果的信息保留</h1><p>分解后各样本间的欧式距离与角度信息应不变，给出证明如下<br>设有$m$组$n$维样本样本</p><script type="math/tex; mode=display">X_{n×m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]</script><p>经奇异值分解，有</p><script type="math/tex; mode=display">X_{n×m} = U_{n×r} \Sigma_{r×r} V_{r×m}^T</script><p>记</p><script type="math/tex; mode=display">Z_{r×m} = \Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]</script><p>有</p><script type="math/tex; mode=display">X = U Z</script><ul><li><p>欧式距离</p><script type="math/tex; mode=display">  || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2</script><script type="math/tex; mode=display">  = \left[ U (Z^{(i)} - Z^{(j)}) \right]^T \left[ U (Z^{(i)} - Z^{(j)}) \right]</script><script type="math/tex; mode=display">  = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)})</script><script type="math/tex; mode=display">  = || Z^{(i)} - Z^{(j)} ||_2^2</script><p>  即</p><script type="math/tex; mode=display">  || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2</script></li><li><p>角度信息</p><script type="math/tex; mode=display">  \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2}</script><script type="math/tex; mode=display">  = \frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2}</script><script type="math/tex; mode=display">  = \frac{(UZ^{(i)})^T(UZ^{(j)})}{\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \sqrt{(UZ^{(j)})^T(UZ^{(j)})}}</script><script type="math/tex; mode=display">  = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}</script><p>  即</p><script type="math/tex; mode=display">  \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} =   \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2}</script></li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-Pattern-Recognition-Course/blob/master/examples/p15_svd.py" target="_blank" rel="noopener">@Github: Code of SVD</a><br>对图片进行了分解<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVD</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" Singular Value Decomposition</span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        m &#123;int&#125;</span></span><br><span class="line"><span class="string">        n &#123;int&#125;</span></span><br><span class="line"><span class="string">        r &#123;int&#125;: if r == -1, then r = n</span></span><br><span class="line"><span class="string">        isTrains &#123;bool&#125;: isTrains = True if input.shape[0] &lt; input.shape[1]</span></span><br><span class="line"><span class="string">        U &#123;ndarray(m, r)&#125;</span></span><br><span class="line"><span class="string">        S &#123;ndarray(r, )&#125;</span></span><br><span class="line"><span class="string">        V &#123;ndarray(n, r)&#125;</span></span><br><span class="line"><span class="string">    Notes:</span></span><br><span class="line"><span class="string">        - Transpose input matrix if m &lt; n, and m, n := n, m</span></span><br><span class="line"><span class="string">        - Reassign r if eigvals contains zero</span></span><br><span class="line"><span class="string">        - Singular values are stored in a 1-dim array `S`</span></span><br><span class="line"><span class="string">        - X' = U S V^T</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, r=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        self.m = <span class="keyword">None</span></span><br><span class="line">        self.n = <span class="keyword">None</span></span><br><span class="line">        self.r = r</span><br><span class="line">        self.isTrans = <span class="keyword">False</span></span><br><span class="line">        self.U = <span class="keyword">None</span></span><br><span class="line">        self.S = <span class="keyword">None</span></span><br><span class="line">        self.V = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">""" calculate components</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - Transpose input matrix if m &lt; n, and m, n := n, m</span></span><br><span class="line"><span class="string">            - reassign self.r if eigvals contains zero</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        (self.m, self.n) = X.shape</span><br><span class="line">        <span class="keyword">if</span> self.m &lt; self.n:</span><br><span class="line">            X = X.T</span><br><span class="line">            self.m, self.n = self.n, self.m</span><br><span class="line">            self.isTrans = <span class="keyword">True</span></span><br><span class="line">        self.r = self.n <span class="keyword">if</span> (self.r == <span class="number">-1</span>) <span class="keyword">else</span> self.r</span><br><span class="line"></span><br><span class="line">        XTX = X.T.dot(X)</span><br><span class="line">        eigval, eigvec = np.linalg.eig(X.T.dot(X))</span><br><span class="line">        eigval, eigvec = np.real(eigval), np.real(eigvec)</span><br><span class="line">        </span><br><span class="line">        self.S = np.sqrt(np.clip(eigval, <span class="number">0</span>, float(<span class="string">'inf'</span>)))</span><br><span class="line">        self.S = self.S[self.S &gt; <span class="number">0</span>]</span><br><span class="line">        self.r = min(self.r, self.S.shape[<span class="number">0</span>])               <span class="comment"># reassign self.r</span></span><br><span class="line">        order = np.argsort(eigval)[::<span class="number">-1</span>][: self.r]          <span class="comment"># sort eigval from large to small</span></span><br><span class="line">        eigval = eigval[order]; eigvec = eigvec[:, order]</span><br><span class="line">        self.V = eigvec.copy()</span><br><span class="line">        self.U = X.dot(self.V).dot(</span><br><span class="line">                    np.linalg.inv(np.diag(self.S)))</span><br><span class="line">        <span class="keyword">return</span> self.U, self.S, self.V</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compose</span><span class="params">(self, r=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        <span class="string">""" merge first r components</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            r &#123;int&#125;: if r==-1, merge all components</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            X &#123;ndarray(m, n)&#125;</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> r == <span class="number">-1</span>:</span><br><span class="line">            X = self.U.dot(np.diag(self.S)).dot(self.V.T)</span><br><span class="line">            X = X.T <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            (m, n) = (self.n, self.m) <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> (self.m, self.n)</span><br><span class="line">            X = np.zeros(shape=(m, n))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(r):</span><br><span class="line">                X += self.__getitem__(i)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">""" get a component</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            index &#123;int&#125;: range from (0, self.r)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        u = self.U[:, idx]</span><br><span class="line">        v = self.V[:, idx]</span><br><span class="line">        s = self.S[idx]</span><br><span class="line">        x = s * u.reshape(self.m, <span class="number">1</span>).\</span><br><span class="line">                    dot(v.reshape(<span class="number">1</span>, self.n))</span><br><span class="line">        x = x.T <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">showComponets</span><span class="params">(self, r=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        <span class="string">""" display components</span></span><br><span class="line"><span class="string">        Notes:</span></span><br><span class="line"><span class="string">            - Resize components' shape into (40, 30)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        m, n = self.m, self.n</span><br><span class="line">        r = self.r <span class="keyword">if</span> r==<span class="number">-1</span> <span class="keyword">else</span> r</span><br><span class="line">        n_images = <span class="number">10</span>; m_images = r // n_images + <span class="number">1</span></span><br><span class="line">        m_size, n_size = <span class="number">40</span>, <span class="number">30</span></span><br><span class="line">        showfig = np.zeros(shape=(m_images*m_size, n_images*n_size))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(r):</span><br><span class="line">            m_pos = i // n_images</span><br><span class="line">            n_pos = i %  n_images</span><br><span class="line">            component = self.__getitem__(i)</span><br><span class="line">            component = component.T <span class="keyword">if</span> self.isTrans <span class="keyword">else</span> component</span><br><span class="line">            component = cv2.resize(component, (<span class="number">30</span>, <span class="number">40</span>))</span><br><span class="line">            showfig[m_pos*m_size: (m_pos+<span class="number">1</span>)*m_size, n_pos*n_size: (n_pos+<span class="number">1</span>)*n_size] = component</span><br><span class="line">        plt.figure(<span class="string">'components'</span>)</span><br><span class="line">        plt.imshow(showfig)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure></p><p>用上面的代码进行实验<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 读取一张图片</span></span><br><span class="line">X = load_images()[0].reshape((32, 32))</span><br><span class="line">showmat2d(X)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 对图片进行奇异值分解</span></span><br><span class="line">decomposer = SVD(r=-1)</span><br><span class="line">decomposer.fit(X)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示一下分量</span></span><br><span class="line">decomposer.showComponets(r=-1)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将全部分量组合，并显示</span></span><br><span class="line">X_ = decomposer.compose(r=-1)</span><br><span class="line">showmat2d(X_)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将前5个分量组合，并显示</span></span><br><span class="line">X_ = decomposer.compose(r=5)</span><br><span class="line">showmat2d(X_)</span><br></pre></td></tr></table></figure></p><ul><li><p>载入原图如下<br><img src="/2018/10/23/SVD/source.png" alt="source"></p></li><li><p>分量显示如下<br><img src="/2018/10/23/SVD/components.png" alt="components"></p></li><li><p>组合分量显示如下</p><ul><li>组合全部<br>  <img src="/2018/10/23/SVD/merge_all.png" alt="merge_all"></li><li>组合前5个分量<br>  <img src="/2018/10/23/SVD/merge_5.png" alt="merge_5"></li></ul></li></ul><h1 id="应用：推荐系统"><a href="#应用：推荐系统" class="headerlink" title="应用：推荐系统"></a>应用：推荐系统</h1><blockquote><p>详情查看<a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/blob/master/ZhaoHaitao%2C%20ECUST/recommend.py" target="_blank" rel="noopener">Basic-Machine-Learning-Algorithm/ZhaoHaitao, ECUST/recommend.py</a></p></blockquote><p>数据可从<a href="http://files.grouplens.org/datasets/movielens/" target="_blank" rel="noopener">MoiveLens</a>下载，实验使用<a href="http://files.grouplens.org/datasets/movielens/ml-100k.zip" target="_blank" rel="noopener">ml-100k.zip</a>。假设有用户$N_{user}$人，电影$N_{item}$部，先每个人对其看过的部分电影已进行评分，及其稀疏，现希望从这些数据预测出未知数据。</p><p>存储数据为矩阵</p><script type="math/tex; mode=display">M_{N_{user} \times N_{item}} = \left[\begin{matrix}    r_{ij}\end{matrix}\right]</script><p>现希望从用户相似度的角度，找出臭味相投的一些用户，用他们的评分均值作为该用户的评分，先对上述矩阵进行SVD分解</p><script type="math/tex; mode=display">M_{N_{user} \times N_{item}} = U_{N_{user} \times N_{user}} \cdot \Sigma_{N_{user} \times N_{item}} \cdot (V_{N_{item} \times N_{item}})^T \tag{4}</script><p>其中</p><script type="math/tex; mode=display">U_{N_{user} \times N_{user}} = \left[\begin{matrix}    \vec{u_{1}} & \vec{u_{2}} & \cdots & \vec{u_{N_{user}}}\end{matrix}\right]</script><script type="math/tex; mode=display">V_{N_{item} \times N_{item}} = \left[\begin{matrix}    \vec{v_{1}} & \vec{v_{2}} & \cdots & \vec{v_{N_{item}}}\end{matrix}\right]</script><p>利用矩阵$V$提取用户的特征，设特征维度为$D_u$</p><script type="math/tex; mode=display">M^u = M_{N_{user} \times N_{item}} \cdot V_{N_{item} \times D_u} \tag{5}</script><p>而</p><script type="math/tex; mode=display">V_{N_{item} \times N_{item}} = \left[\begin{matrix}    V_{N_{item} \times D_u} & | & V_{N_{item} \times (N_{item} - D_u)}\end{matrix}\right]</script><script type="math/tex; mode=display">(V_{N_{item} \times N_{item}})^T \cdot V_{N_{item} \times D_u} = \left[\begin{matrix}    I_{D_u \times D_u} \\ --- \\ O_{(N_{item} - D_u) \times D_u}\end{matrix}\right] \tag{6}</script><p>所以$(4)(6)$代入$(5)$，亦可化简为</p><script type="math/tex; mode=display">M^u = U_{N_{user} \times N_{user}} \cdot \Sigma_{N_{user} \times N_{item}} \cdot (V_{N_{item} \times N_{item}})^T \cdot V_{N_{item} \times D_u}</script><script type="math/tex; mode=display">= U_{N_{user} \times D_u} \cdot \Sigma_{D_u \times D_u} \tag{7}</script><p>现计算用户间的相似度矩阵，可用余弦度量，即</p><script type="math/tex; mode=display">s_{ij} = \frac{M^{u_iT} M^u_j}{||M^u_i|| ||M^u_j||}</script><p>或者</p><script type="math/tex; mode=display">M^u_i := \frac{M^u_i}{||M^u_i||}</script><script type="math/tex; mode=display">S_{N_{user} \times N_{user}} = M^u \cdot M^{uT} \tag{8}</script><p>对于某部电影$\text{item}^{(j)}$，先找到该用户$\text{user}^{(i)}$的最近似的几个用户$\text{user}^{(k)}, k \in \{1, \cdots, N_{user} \}, k \neq i$，取其均值作为该用户的评分。</p><p>若利用电影的相似度，只需</p><script type="math/tex; mode=display">M^i = (U_{N_{item} \times D_i})^T \cdot M_{N_{user} \times N_{item}} \tag{9}</script><p>以下同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">according_to_user</span><span class="params">(train_matrix, test_matrix, cols=<span class="number">80</span>, n_keep=<span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将矩阵SVD分解</span></span><br><span class="line">    _, _, vh = np.linalg.svd(train_matrix)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 压缩原矩阵，A' = A V[:, :k]</span></span><br><span class="line">    train_compressed_col = train_matrix.dot(vh[: cols].T)   <span class="comment"># N_USERS x cols</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算相似度矩阵</span></span><br><span class="line">    similarity_user = get_cosine_similarity_matrix(train_compressed_col)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    pred_matrix = np.zeros_like(test_matrix)        <span class="comment"># 保存预测结果</span></span><br><span class="line">    to_pred = np.array(np.where(test_matrix != <span class="number">0</span>))  <span class="comment"># 需要预测的数据位置, (2, n)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(to_pred.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        r, c = to_pred[:, i]                        <span class="comment"># r为用户索引，c为电影索引</span></span><br><span class="line"></span><br><span class="line">        id = np.argsort(similarity_user[r])[::<span class="number">-1</span>]   <span class="comment"># 将用户以相似度从大到小排序</span></span><br><span class="line">        id = id[<span class="number">1</span>: n_keep + <span class="number">1</span>]                      <span class="comment"># 获取相似度最大的几个用户，除自身</span></span><br><span class="line">        rates = train_matrix[id, c]                 <span class="comment"># 获取这几个用户对该电影的评分</span></span><br><span class="line">        rates = rates[rates!=<span class="number">0</span>]                     <span class="comment"># 已评价的数据</span></span><br><span class="line"></span><br><span class="line">        rate = np.mean(rates) <span class="keyword">if</span> rates.shape[<span class="number">0</span>] != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        pred_matrix[r, c] = rate</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred_matrix</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>删除停用词</title>
      <link href="/2018/10/23/%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D/"/>
      <url>/2018/10/23/%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.yiibai.com/python_text_processing/python_remove_stopwords.html" target="_blank" rel="noopener">删除停用词 - Python文本处理教程™</a></p></blockquote><p>停用词是对句子没有多大意义的英语单词。 在不牺牲句子含义的情况下，可以安全地忽略它们。 例如，the, he, have等等的单词已经在名为语料库的语料库中捕获了这些单词。</p><h1 id="下载语料库"><a href="#下载语料库" class="headerlink" title="下载语料库"></a>下载语料库</h1><ul><li><p>安装<code>nltk</code>模块</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install nltk</span><br></pre></td></tr></table></figure></li><li><p>下载语料库</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line">nltk.download(&apos;stopwords&apos;)</span><br></pre></td></tr></table></figure></li></ul><h1 id="使用库料库"><a href="#使用库料库" class="headerlink" title="使用库料库"></a>使用库料库</h1><ul><li><p>验证停用词</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; stopwords.words('english')</span><br><span class="line">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', </span><br><span class="line">'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', </span><br><span class="line">'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',</span><br><span class="line">"she's", 'her', 'hers', 'herself', 'it', "it's", 'its', </span><br><span class="line">'itself', 'they', 'them', 'their', 'theirs', 'themselves', </span><br><span class="line">'what', 'which', 'who', 'whom', 'this', 'that', "that'll", </span><br><span class="line">'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', </span><br><span class="line">'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', </span><br><span class="line">'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', </span><br><span class="line">'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', </span><br><span class="line">'with', 'about', 'against', 'between', 'into', 'through', </span><br><span class="line">'during', 'before', 'after', 'above', 'below', 'to', 'from', </span><br><span class="line">'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', </span><br><span class="line">'again', 'further', 'then', 'once', 'here', 'there', 'when',</span><br><span class="line">'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', </span><br><span class="line">'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', </span><br><span class="line">'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', </span><br><span class="line">'can', 'will', 'just', 'don', "don't", 'should', "should've", </span><br><span class="line">'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', </span><br><span class="line">"aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', </span><br><span class="line">"doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', </span><br><span class="line">"haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn',</span><br><span class="line">"mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', </span><br><span class="line">"shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', </span><br><span class="line">"won't", 'wouldn', "wouldn't"]</span><br></pre></td></tr></table></figure><p>  除了英语之外，具有这些停用词的各种语言如下。</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; stopwords.fileids()</span><br><span class="line">['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', </span><br><span class="line">'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', </span><br><span class="line">'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian',</span><br><span class="line">'spanish', 'swedish', 'turkish']</span><br></pre></td></tr></table></figure></li><li><p>示例<br>  从单词列表中删除停用词。</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>&gt;&gt; from nltk.corpus import stopwords</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; en_stops = set(stopwords.words('english'))</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; </span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; all_words = ['There', 'is', 'a', 'tree','near','the','river']</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; for word in all_words:</span><br><span class="line">if word not in en_stops:</span><br><span class="line">print(word)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">There</span><br><span class="line">tree</span><br><span class="line">near</span><br><span class="line">river</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PCA</title>
      <link href="/2018/10/22/PCA/"/>
      <url>/2018/10/22/PCA/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><code>PCA</code>全称<code>Principal Component Analysis</code>，即主成分分析，是一种常用的数据降维方法。它可以通过线性变换将原始数据变换为一组各维度线性无关的表示，以此来提取数据的主要线性分量。</p><h1 id="向量的投影"><a href="#向量的投影" class="headerlink" title="向量的投影"></a>向量的投影</h1><p>现有两个任意不共线向量$\vec{u}, \vec{v}$，将$\vec{u}$投射到$\vec{v}$上<br><img src="/2018/10/22/PCA/向量投影.jpg" alt="向量投影"></p><p>投影后，可以得到两个正交向量</p><script type="math/tex; mode=display">\vec{u}' · (\vec{u} - \vec{u}') = 0</script><p>我们设</p><script type="math/tex; mode=display">\vec{u}' = \mu \vec{v} \tag{1}</script><p>代入后有</p><script type="math/tex; mode=display">\mu \vec{v} · (\vec{u} - \mu \vec{v}) = 0</script><p>引入矩阵运算，即</p><script type="math/tex; mode=display">(\mu v)^T (u - \mu v) = 0</script><p>有</p><script type="math/tex; mode=display">v^T u = \mu v^T v</script><p>则得到$u’$以$v$为基向量的坐标</p><script type="math/tex; mode=display">\mu  = (v^T v)^{-1} v^T u \tag{2}</script><p>所以得到</p><script type="math/tex; mode=display">u' = v (v^T v)^{-1} v^T u \tag{*}</script><blockquote><ul><li><p>坐标变换求解投影向量：$u’$可视作$u$经坐标变换$u’ = P u$得到，所以</p><script type="math/tex; mode=display">P = v (v^T v)^{-1} v^T</script></li><li><p>推广至多个向量的投影，即得到</p><script type="math/tex; mode=display">P = X (X^T X)^{-1} X^T</script><p>这与<a href="https://louishsu.xyz/2018/10/18/Linear-Regression/" target="_blank" rel="noopener">线性回归</a>中得到的结论一致。</p></li></ul></blockquote><p>实际上</p><script type="math/tex; mode=display">u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T u</script><p>记单位向量$\frac{v}{||v||}$为$v_0$，得到</p><script type="math/tex; mode=display">u' = v_0 v_0^T u</script><p>由几何关系，可以计算得投影后的长度为</p><script type="math/tex; mode=display">d = ||u|| \cos \theta = ||u|| \frac{v^T u}{||u||||v||}= v_0^T u</script><p>所以在向量投影中，$u^T v_0$表示以$v_0$为基向量的坐标。</p><h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><p>现在有$N$维数据集$D=\{x^{(1)}, x^{(2)}, …, x^{(M)}\}$，其中$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, …, x^{(i)}_N\right]^T$，各维特征$D_{j}$间存在线性相关性，利用主成分分析可使</p><ul><li>数据维度降低；</li><li>提取主成分，且各成分间不相关。</li></ul><blockquote><p>说明</p><ul><li>由于选取的特征轴是正交的，所以计算结果线性无关；</li><li>提取了方差较大的几个特征，为主要线性分量。</li></ul></blockquote><p>以二维空间中的数据$x^{(i)} = \left[\begin{matrix}<br>    x^{(i)}_1 \\ x^{(i)}_2<br>\end{matrix}\right]$为例，维度可降至一维，如下图所示。<br><img src="/2018/10/22/PCA/PCA动态图.gif" alt="PCA动态图"></p><p>主轴可有无穷多种选择，那么问题就是<strong>如何选取最优的主轴</strong>。先给出<code>PCA</code>的计算步骤。</p><h2 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h2><p>输入的$M$个$N$维样本，有样本矩阵</p><script type="math/tex; mode=display">X_{N×M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right]= \left[    \begin{matrix}        x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\        x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\        ... \\        x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\    \end{matrix}\right]</script><h3 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h3><ol><li><p>对每个维度(行)进行去均值化</p><script type="math/tex; mode=display">X_j := X_j - \mu_j</script><p> 其中$\mu_j = \overline{X_j}$，$j = 1, 2, …, N$</p></li><li><p>求各维度间的协方差矩阵$\Sigma_{N×N}$</p><script type="math/tex; mode=display">\Sigma_{ij} = Cov(x_i, x_j)</script><p> 或</p><script type="math/tex; mode=display"> \Sigma = \frac{1}{M} X X^T</script></li></ol><blockquote><p>注：</p><ol><li><script type="math/tex; mode=display">X X^T = \left[           \begin{matrix}     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\     ... &     ... &     ... &     ... \\     \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 &      \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 &     ... &     \sum_{i=1}^M x^{(i)}_N x^{(i)}_N \end{matrix}\right]</script><script type="math/tex; mode=display">= \sum_{i=1}^M \left[           \begin{matrix}         x^{(i)}_1 x^{(i)}_1 &          x^{(i)}_1 x^{(i)}_2 &         ... &         x^{(i)}_1 x^{(i)}_N \\         x^{(i)}_2 x^{(i)}_1 &          x^{(i)}_2 x^{(i)}_2 &         ... &         x^{(i)}_2 x^{(i)}_N \\         ... &         ... &         ... &         ... \\         x^{(i)}_N x^{(i)}_1 &          x^{(i)}_N x^{(i)}_2 &         ... &         x^{(i)}_N x^{(i)}_N \end{matrix}\right]</script><script type="math/tex; mode=display">= \sum_{i=1}^M x^{(i)} x^{(i)T}</script></li><li><p>协方差定义式</p><script type="math/tex; mode=display">   Cov(x,y)≝\frac{1}{n-1} ∑_{i=1}^n (x_i−\overline{x})^T(y_i−\overline{y})</script><p>其中$x=[x_1, x_2, …, x_n]^T, y=[y_1, y_2, …, y_n]^T$</p></li></ol></blockquote><ol><li>求协方差矩阵$\Sigma$的特征值$λ_i$及其对应特征向量$α_i$，$i=1, …, N$；</li><li><p>按照特征值从大到小排列特征对$(λ_i,α_i)$，选取$K$个最大特征值对应的特征向量作为降维后的主轴$ \beta_1, \beta_2, …, \beta_K $，其中$\beta_k$为单位向量</p><script type="math/tex; mode=display">\beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^T</script><p>记</p><script type="math/tex; mode=display">B_{N×K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]</script><p>$K$的选取方法有如下两种：</p><ul><li>指定选取$K$个主轴</li><li>保留$99\%$的方差<script type="math/tex; mode=display">\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99</script></li></ul></li></ol><ol><li><p>将样本点投射到$K$维坐标系上<br> 样本$X^{(i)}$投射到主成分轴$\beta_k$上，其坐标表示为向量，为</p><script type="math/tex; mode=display"> S^{(i)}_k = X^{(i)T}\beta_k</script><blockquote><p>注意此时的基座标为$\beta_k$，或者说$X’^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$</p></blockquote><p> 所有样本在主轴$\beta_k$上的投影坐标即</p><script type="math/tex; mode=display"> S = B^T X</script><p> 其中$S_{K×M}$，$B_{N×K}$，$X_{N×M}$</p></li></ol><blockquote><p>注：若取$K=N$，可重建数据，如下<br><img src="/2018/10/22/PCA/pca_restructure1.png" alt="pca_restructure1"><br><img src="/2018/10/22/PCA/pca_restructure2.png" alt="pca_restructure2"></p></blockquote><h3 id="复原"><a href="#复原" class="headerlink" title="复原"></a>复原</h3><p>第$5$步中，样本点向量$X^{(i)}$的主要分量投射到$K$个$N$维向量上，投影坐标为$S^{(i)}_k$，即</p><script type="math/tex; mode=display">X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_k</script><p>以上就是样本点的复原公式，矩阵形式即</p><script type="math/tex; mode=display">\hat{X} = BS</script><p>其中$\hat{X}_{N×M}$，$B_{N×K}$，$S_{K×M}$</p><p>考虑到已去均值化，故</p><script type="math/tex; mode=display">\hat{X}_j \approx \hat{X}_j + \mu_j</script><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><blockquote><p>投影向量的$2$范数最大，或者说，投影后的坐标平方和(方差)最大</p></blockquote><p>当所有样本$X$投射到第一主轴$\beta_1$上，其坐标为</p><script type="math/tex; mode=display">S_1 = X^T \beta_1</script><p>所有元素的平方和，或向量$S_1$的$2$范数为</p><script type="math/tex; mode=display">||S_1||_2^2 = S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}</script><p>即优化目标为</p><script type="math/tex; mode=display">\max ||S_1||_2^2</script><script type="math/tex; mode=display">s.t.　||\beta_1||_2^2 = 1</script><p>矩阵$C=XX^T$为对称矩阵，故可单位正交化</p><script type="math/tex; mode=display">C = W \Lambda W^T</script><script type="math/tex; mode=display">W = \left[\begin{matrix}    | & & |\\    w_1 & ... & w_M\\    | & & |\\\end{matrix}\right]　\Lambda = \left[\begin{matrix}    \lambda_1 &  & \\     & ... & \\     &  & \lambda_M\\\end{matrix}\right]</script><p>其中$\lambda_1 &gt; …&gt; \lambda_M$，$w_i(i=1,…,M)$为矩阵$C$的特征向量(单位向量，互相正交)</p><blockquote><p>实际上$R(C) \leq (n-1)$，即最多有$(n-1)$个特征值大于$0$。</p></blockquote><script type="math/tex; mode=display">||S_1||_2^2= \beta_1^T W \Lambda W^T \beta_1 \tag{2}</script><p>令$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$，可得</p><script type="math/tex; mode=display">||S_1||_2^2= \alpha_1^T \Lambda \alpha_1 \tag{3}</script><p>即</p><script type="math/tex; mode=display">||S_1||_2^2 = \sum_{i=1}^M \lambda_i \alpha_{1i}^2 \tag{4}</script><p>假设特征值已降序排序，那么进一步</p><script type="math/tex; mode=display">\sum_{i=1}^M \lambda_i \alpha_{1i}^2\leq \lambda_1 \sum_{i=1}^M \alpha_{1i}^2 \tag{5}</script><p>且由于$\beta_1^T\beta_1 = 1$，故</p><script type="math/tex; mode=display">1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = \sum_{i=1}^M \alpha_{1i}^2</script><p>可得</p><script type="math/tex; mode=display">||S_1||_2^2= \sum_{i=1}^M \lambda_i \alpha_{1i}^2\leq \lambda_1  \tag{6}</script><p>为使$(6)$取等号，即达最大值，可使</p><script type="math/tex; mode=display">\begin{cases}    \alpha_{11} = 1 \\    \alpha_{12} = ... = \alpha_{1M} = 0\end{cases}</script><p>即令</p><script type="math/tex; mode=display">\beta_1 = W \alpha_1 = w_1</script><blockquote><p>$\alpha_1 = [1, 0, …, 0]^T$</p></blockquote><p>所以$\beta_1$对应矩阵$C=XX^T$的特征向量$w_1$，且有</p><script type="math/tex; mode=display">||S_1||_2^2 = \lambda_1</script><blockquote><p>或者第一主成分的证明也可以这样，建立优化目标</p><script type="math/tex; mode=display">\beta_1 = \arg \max　||S_1||_2^2</script><script type="math/tex; mode=display">s.t.　||\beta_1||_2^2 = 1</script><p>构造拉格朗日函数</p><script type="math/tex; mode=display">L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)</script><p>也即</p><script type="math/tex; mode=display">L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)</script><p>求其极值点</p><script type="math/tex; mode=display">▽_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0</script><p>有</p><script type="math/tex; mode=display">X X^T \beta_1 = \lambda_1 \beta_1</script><p>可见$\beta_1$即方阵$X X^T$的特征向量</p></blockquote><p>当我们希望用更多的主成分刻画数据，如已经求得主成分$\beta_1, …, \beta_{r-1}$，现需求解$\beta_r$，引入正交约束$\beta_r^T \beta_i = 0$，即目标函数为</p><script type="math/tex; mode=display">||S_r||_2^2 = \beta_r^T C \beta_r</script><script type="math/tex; mode=display">s.t.　\beta_r^T \beta_i = 0, i = 1, ..., r-1</script><script type="math/tex; mode=display">||\beta_r||_2^2 = 1</script><p>令$\beta_r = W \alpha_r$，则</p><script type="math/tex; mode=display">||S_r||_2^2= \alpha_r^T \Lambda \alpha_r= \sum_i \lambda_i \alpha_{ri}^2 \tag{7}</script><p>而根据正交约束，$\beta_r = W \alpha_r, \beta_i = w_i$代入后有</p><script type="math/tex; mode=display">0 = \beta_r^T \beta_i = (\alpha_r^T W^T) w_i = \alpha_{ri},　i = 1, ..., r-1 \tag{8}</script><blockquote><p>$ W^T w_i = \left[0, …, 1_i, …, 0\right]^T$</p></blockquote><p>$(8)$代入$(7)$后得到</p><script type="math/tex; mode=display">||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{9}</script><p>又因为$\beta_r^T \beta_r = 1$(单位向量)，故</p><script type="math/tex; mode=display">\beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1 \tag{10}</script><p>那么类似的，为使$(9)$取最大，取</p><script type="math/tex; mode=display">\begin{cases}    \alpha_{rr} = 1\\    \alpha_{ri} = 0,　i = 1, ..., M, i \neq r\end{cases}</script><blockquote><p>$\alpha_r = [0, …, 1_r, …, 0]$</p></blockquote><p>则此时</p><script type="math/tex; mode=display">\beta_r = W \alpha_r = w_r</script><p>且有</p><script type="math/tex; mode=display">||S_r||_2^2 = \lambda_r</script><p>证毕。</p><h2 id="白化-whitening"><a href="#白化-whitening" class="headerlink" title="白化(whitening)"></a>白化(whitening)</h2><p><code>whitening</code>的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。</p><p>数据的<code>whitening</code>必须满足两个条件：</p><ol><li>不同特征间相关性最小，接近$0$；</li><li>所有特征的方差相等（不一定为$1$）。</li></ol><p>常见的白化操作有<code>PCA whitening</code>和<code>ZCA whitening</code>。</p><blockquote><p><a href="http://deeplearning.stanford.edu/wiki/index.php/Whitening" target="_blank" rel="noopener">Whitening - Ufldl</a></p></blockquote><ul><li>PCA whitening<br>  <code>PCA whitening</code>指将数据$X$经过<code>PCA</code>降维为$S$后，可以看出$S$中每一维是独立的，满足<code>whitening</code>的第一个条件，这是只需要将$S$中的每一维都除以标准差就得到了每一维的方差为$1$，也就是说方差相等。<script type="math/tex; mode=display">  X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}}</script></li></ul><ul><li>ZCA whitening<br>  <code>ZCA whitening</code>是指数据$X$先经过<code>PCA</code>变换为$S$，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足<code>whtienning</code>的第一个条件，特征间相互独立。然后同样进行方差为$1$的操作，最后将得到的矩阵左乘一个特征向量矩阵$U$即可。<script type="math/tex; mode=display">  X_{ZCAwhite} = U · X_{PCAwhite}</script></li></ul><h1 id="Kernel-PCA"><a href="#Kernel-PCA" class="headerlink" title="Kernel PCA"></a>Kernel PCA</h1><p><code>Kernel PCA</code>的思想是在高维的特征空间中求解协方差矩阵</p><script type="math/tex; mode=display">\Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><p>其中$\Phi(X^{(i)})$表示将样本$i$映射到高维空间后中的向量，即</p><script type="math/tex; mode=display">\Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^T</script><p>其中$N’ &gt; N$，由于$\Phi(X^{(i)})$为隐式的，故设置核函数求解，记</p><script type="math/tex; mode=display">\kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T</script><blockquote><p>关于核技巧，移步<a href="">非线性支持向量机</a></p></blockquote><p><img src="/2018/10/22/PCA/kernel_pca.jpg" alt="kernel_pca"></p><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>可利用<code>PCA</code>与线性回归求解$3$维空间中平面的法向量</p><ol><li>利用<code>PCA</code>重建数据(不降维，此时为$3$维)，此时第$1, 2$主成分轴可张成所求平面，即该平面可表示为<script type="math/tex; mode=display"> \Pi = span \{ \beta_1, \beta_2 \}</script></li></ol><blockquote><p>就是说，第一、二主成分是这些点“拉伸”最大的方向 :-)，好懂不？</p></blockquote><ol><li><p>由<a href="https://louishsu.xyz/2018/10/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">正交投影</a>可知，平面外一点$y$可通过最小二乘(线性回归)的方法投射到平面上，向量运算，不考虑偏置项，即</p><script type="math/tex; mode=display"> \hat{y}  = \theta_1 x_1 + \theta_2 x_2 \tag{*}</script><p> 其中$x_1, x_2$表示第一、第二主成分$\beta_1, \beta_2$，为$3$维向量</p><script type="math/tex; mode=display"> \hat{y} = \left[     \begin{matrix}         \hat{y_1} \\         \hat{y_2} \\         \hat{y_3} \\     \end{matrix} \right]　 x_i = \left[     \begin{matrix}         x_{i1} \\         x_{i2} \\         x_{i3} \\     \end{matrix} \right]</script><p> 可利用公式求解回归参数$\theta$</p><script type="math/tex; mode=display"> \theta = (X^TX+\lambda I)^{-1} X^T y</script><blockquote><p>注意：$X(n_samples, n_features)$，这里把$(x_{1j}, x_{2j}, y_{j})作为一组样本$</p></blockquote><p> 此时该参数表示在主轴上的坐标$(\theta_1, \theta_2)$，带回$(*))$即可解得$\hat{y}$</p><script type="math/tex; mode=display"> \hat{y}  = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*}</script><p> 通俗理解，一掌把$y$拍平在了平面$\Pi$上，变成了$\hat{y}$，但是哪有这么好拍。。。这个时候刺在掌心里一定有一个垂直的向量分量，即为该平面的法向量</p><script type="math/tex; mode=display"> \vec{n} = y - \hat{y}</script><p> <strong>也可使用粗暴一点的方法，直接将第三主成分作为法向量。</strong></p><blockquote><p>或者直接上投影公式：</p><script type="math/tex; mode=display">\hat{y} = Py</script><script type="math/tex; mode=display">　P = X (X^TX+\lambda I)^{-1} X^T</script></blockquote></li></ol><pre><code>![projection](/PCA/projection.jpg)总体的运算流程如下- 利用所有样本点(近似平面)计算主成分，第一、二主成分张成平面$\Pi$；- 选出其中一个样本点，将平行于平面$\Pi$的成分投射到$\Pi$上；- 该样本点剩余分量即法向量；- 一般来说，取所有点法向量的均值。</code></pre><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex7-3-pca" target="_blank" rel="noopener">@Github: PCA</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PrincipalComponentAnalysis</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_component=<span class="number">-1</span>)</span>:</span></span><br><span class="line">        self.n_component = n_component</span><br><span class="line">        self.meanVal = <span class="keyword">None</span></span><br><span class="line">        self.axis = <span class="keyword">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, prop=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        the parameter 'prop' is only for 'n_component = -1'</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 第一步: 归一化</span></span><br><span class="line">        self.meanVal = np.mean(X, axis=<span class="number">0</span>)                   <span class="comment"># 训练样本每个特征上的的均值</span></span><br><span class="line">        X_normalized = (X - self.meanVal)                   <span class="comment"># 归一化训练样本</span></span><br><span class="line">        <span class="comment"># 第二步：计算协方差矩阵</span></span><br><span class="line">        <span class="comment"># cov = X_normalized.T.dot(X_normalized)</span></span><br><span class="line">        cov = np.cov(X_normalized.T)                        <span class="comment"># 协方差矩阵</span></span><br><span class="line">        eigVal, eigVec = np.linalg.eig(cov)                 <span class="comment"># EVD</span></span><br><span class="line">        order = np.argsort(eigVal)[::<span class="number">-1</span>]                    <span class="comment"># 从大到小排序</span></span><br><span class="line">        eigVal = eigVal[order]</span><br><span class="line">        eigVec = eigVec.T[order].T</span><br><span class="line">        <span class="comment"># 选择主成分的数量</span></span><br><span class="line">        <span class="keyword">if</span> self.n_component == <span class="number">-1</span>:</span><br><span class="line">            sumOfEigVal = np.sum(eigVal)</span><br><span class="line">            sum_tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(eigVal.shape[<span class="number">0</span>]):</span><br><span class="line">                sum_tmp += eigVal[k]</span><br><span class="line">                <span class="keyword">if</span> sum_tmp &gt; prop * sumOfEigVal:            <span class="comment"># 平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值</span></span><br><span class="line">                    self.n_component = k + <span class="number">1</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 选择投影坐标轴</span></span><br><span class="line">        self.axis = eigVec[:, :self.n_component]            <span class="comment"># 选择前n_component个特征向量作为投影坐标轴</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># 第一步：归一化</span></span><br><span class="line">        X_normalized = (X - self.meanVal)                   <span class="comment"># 归一化测试样本</span></span><br><span class="line">        <span class="comment"># 第二步：投影 X_nxk · V_kxk' = X'_nxk'</span></span><br><span class="line">        X_transformed = X_normalized.dot(self.axis)</span><br><span class="line">        <span class="keyword">return</span> X_transformed</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_transform</span><span class="params">(self, X, prop=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        self.fit(X, prop=prop)</span><br><span class="line">        <span class="keyword">return</span> self.transform(X)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform_inv</span><span class="params">(self, X_transformed)</span>:</span></span><br><span class="line">        <span class="comment"># 视投影向量长度为一个单位长度，投影结果为投影向量上的坐标</span></span><br><span class="line">        <span class="comment"># X'_nxk' · V_kxk'.T = X''_nxk</span></span><br><span class="line">        X_restructed = X_transformed.dot(self.axis.T)</span><br><span class="line">        <span class="comment"># 还原数据</span></span><br><span class="line">        X_restructed = X_restructed + self.meanVal</span><br><span class="line">        <span class="keyword">return</span> X_restructed</span><br></pre></td></tr></table></figure><p>实验结果</p><ul><li><p>Demo1: PCA applied on 2-d datasets<br>  <img src="/2018/10/22/PCA/2d_restructed.png" alt="2d_restructed"></p></li><li><p>Demo2: PCA applied on wild face</p><ul><li>origin<br><img src="/2018/10/22/PCA/face_origin.png" alt="origin"></li><li>reduced<br><img src="/2018/10/22/PCA/face_reduced.png" alt="reduced"></li><li>restructured<br><img src="/2018/10/22/PCA/face_restructed.png" alt="restructured"></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Activate Functions</title>
      <link href="/2018/10/20/Activate-Functions/"/>
      <url>/2018/10/20/Activate-Functions/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247483977&amp;idx=1&amp;sn=401b211bf72bc70f733d6ac90f7352cc&amp;chksm=fdb69fdecac116c81aad9e5adae42142d67f50258106f501af07dc651d2c1473c52fad8678c3&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">SigAI 理解神经网络的激活函数</a><br><a href="https://www.cnblogs.com/silence-tommy/p/7113405.html" target="_blank" rel="noopener">机器学习笔记：形象的解释神经网络激活函数的作用是什么？ - 不说话的汤姆猫 - 博客园</a></p></blockquote><h1 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h1><h2 id="复合函数"><a href="#复合函数" class="headerlink" title="复合函数"></a>复合函数</h2><p>神经网络可以看作一个多层复合函数，以下图隐含层的激活函数为例，讲解其非线性作用。<br><img src="/2018/10/20/Activate-Functions/激活函数的非线性作用.png" alt="激活函数的非线性作用"></p><p>记激活函数为$\sigma(·)$，上图神经网络各层间具有如下关系</p><script type="math/tex; mode=display">a = \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1)</script><script type="math/tex; mode=display">b = \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2)</script><script type="math/tex; mode=display">c = \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3)</script><p>输出层采用线性单元</p><script type="math/tex; mode=display">A = w^{(2)}_{1}a + w^{(2)}_{2}b + w^{(2)}_{3}c + b^{(2)}</script><!-- 或者写作复合函数$$A = w^{(2)}_{1} \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1) +     w^{(2)}_{2} \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2) +     w^{(2)}_{3} \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3) +     b^{(2)}$$ --><p>为便于作图，固定参数</p><script type="math/tex; mode=display">W^{(1)} = \left[    \begin{matrix}        1   &  1 \\        0.1 & -1 \\        1   & -1    \end{matrix}\right],b^{(1)} = \left[    \begin{matrix}        -2  \\        1.5 \\        -1    \end{matrix}\right]W^{(2)} = \left[    \begin{matrix}        1 & 2 & 3    \end{matrix}\right],b^{(2)} = \left[    \begin{matrix}        -1    \end{matrix}\right]</script><ul><li><p>线性单元作为激活函数<br>  此时神经网络的输出为</p><script type="math/tex; mode=display">  A = (x + y - 2) +       2 (0.1x - y + 1.5) +       3 (x - y - 1)- 1</script><p>  可见仍为线性函数，做出图像如下所示<br>  <img src="/2018/10/20/Activate-Functions/Linear.png" alt="Linear"></p></li><li><p>非线性单元作为激活函数<br>  此时神经网络的输出为</p><script type="math/tex; mode=display">  A = \sigma(x + y - 2) +       2 \sigma(0.1x - y + 1.5) +       3 \sigma(x - y - 1)- 1</script><p>  激活函数选择<code>Sigmoid</code>，做出图像如下所示<br>  <img src="/2018/10/20/Activate-Functions/nonLinear.png" alt="nonLinear"></p></li></ul><h2 id="分割平面"><a href="#分割平面" class="headerlink" title="分割平面"></a>分割平面</h2><p>神经网络可实现逻辑运算，各个神经元视作分割超平面时，可分割出不同形状的平面，在线性和非线性激活函数时分割效果如图。当神经元组合的情况更复杂时，表达能力就会更强。<br><img src="/2018/10/20/Activate-Functions/激活函数的非线性作用.jpg" alt=""></p><h1 id="激活函数的性质"><a href="#激活函数的性质" class="headerlink" title="激活函数的性质"></a>激活函数的性质</h1><p>已经证明，只要激活函数选择得当，神经元个数足够多，使用3层即包含一个隐含层的神经网络就可以实现对任何一个从输入向量到输出向量的连续映射函数的逼近，这个结论称为万能逼近（universal approximation）定理。</p><blockquote><p>如果$\varphi(x)$是一个非常数、有界、单调递增的连续函数，$I_{m}$是$m$维的单位立方体，$I_{m}$中的连续函数空间为$C(I_{m})$。对于任意$\varepsilon&gt;0$以及函数$f\in C(I_{m})$，存在整数$N$，实数$v_{i},b_{i}$，实向量$w_{i}\in R^{m}$，通过它们构造函数$F(x)$作为函数$f$的逼近：</p><script type="math/tex; mode=display">F(x) = \sum_{i=1}^N v_i \varphi(w_i^T x + b_i)</script><p>对任意的$X\in I_{m}$满足：</p><script type="math/tex; mode=display">| F(x) - f(x) | < \varepsilon</script><p>Cybenko, G. Approximation by superpositions of a sigmoid function. Mathematics of Control, Signals, and Systems, 2, 303-314, 1989.</p></blockquote><p>这个定理对激活函数的要求是<strong>必须非常数、有界、单调递增，并且连续</strong>。</p><p>神经网络的训练使用梯度下降法进行求解，需要计算损失函数对参数的梯度值，涉及到计算激活函数的导数，因此激活函数必须是可导的。实际应用时并不要求它在定义域内处处可导，只要是几乎处处可导即可。</p><blockquote><p>定义$R$为一维欧氏空间，$E\subset R$是它的一个子集，$mE$为点集$E$的<strong>Lebesgue测度</strong>。如果$E$为$R$中的可测集，$f(x)$为定义在上$E$的实函数，如果存在$N\subset E$，满足：$mN=0$，对于任意的$x_{0}\in E/N$函数$f(x)$在$x_{0}$处都可导，则称$f(x)$在$E$上几乎处处可导。</p></blockquote><p>如果将激活函数输入值$x$看做是随机变量，则它落在这些不可导点处的概率是$0$。在计算机实现时，因此有一定的概率会落在不可导点处，但概率非常小。</p><blockquote><p>例如ReLU函数在$x=0$处不可导</p><script type="math/tex; mode=display">f(x) = \begin{cases}    x & x \geq 0 \\    0 & x < 0\end{cases}</script></blockquote><h1 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h1><p><img src="/2018/10/20/Activate-Functions/常用的激活函数.jpg" alt="常用的激活函数"></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Feedforward Neural Network</title>
      <link href="/2018/10/20/Feedforward-Neural-Network/"/>
      <url>/2018/10/20/Feedforward-Neural-Network/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一，既可以用于解决分类问题，也可以用于解决回归问题。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>前馈神经网络也叫作多层感知机，包含输入层，隐含层和输出层三个部分。它的目的是为了实现输入到输出的映射。</p><script type="math/tex; mode=display">y = f(x;W)</script><p>由于各层采用了非线性激活函数，神经网络具有良好的非线性特性，如下图所示。</p><ul><li>激活函数为线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/Linear.png" alt="Linear"></li><li>激活函数为非线性单元<br><img src="/2018/10/20/Feedforward-Neural-Network/nonLinear.png" alt="nonLinear"></li></ul><p>前馈神经网络可用于解决非线性的分类或回归问题，参数通过反向传播算法<code>(Back Propagation)</code>学习。</p><h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><h2 id="神经元与网络结构图"><a href="#神经元与网络结构图" class="headerlink" title="神经元与网络结构图"></a>神经元与网络结构图</h2><p>单个神经元的示意图如下，输入为前一层的输出参数$X^{(l-1)}$</p><script type="math/tex; mode=display">h_{w, b}(x) = \sigma (WX + b)</script><p>$\sigma(·)$表示激活函数。</p><p><img src="/2018/10/20/Feedforward-Neural-Network/单个神经元示意图.png" alt="单个神经元示意图"></p><p>以下为典型的神经网络结构图<br><img src="/2018/10/20/Feedforward-Neural-Network/前馈神经网络结构图.png" alt="前馈神经网络结构图"></p><ul><li>第一层为输入层<code>input layer</code>，一般不设置权值，预处理在输入网络前完成；</li><li>最后一层为输出层<code>output layer</code>；</li><li>其余层称为隐藏层<code>hidden layer</code>，隐藏层用于提取数据特征，隐藏层层数与各层神经元个数为超参数。</li></ul><blockquote><p>神经元权值取值不同，可实现不同的逻辑运算，单个超平面只能进行二元划分，利用逻辑运算可将多个超平面划分的区域拼接起来，如图<br><img src="/2018/10/20/Feedforward-Neural-Network/超平面划分区域的拼接.jpg" alt="超平面划分区域的拼接"></p><p>以下说明逻辑运算的实现方法<br><img src="/2018/10/20/Feedforward-Neural-Network/二元逻辑运算.png" alt="二元逻辑运算"><br>其中</p><script type="math/tex; mode=display">f(z) = \begin{cases}    1 & z \geq 0 \\    0 & otherwise\end{cases}</script><ul><li><p>与运算 $a ∧ b$</p><script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -30</script></li><li><p>或运算 $a ∧ b$</p><script type="math/tex; mode=display">w_1 = 20, w_2 = 20, b = -10</script></li><li><p>非运算 $a = \overline{b}$</p><script type="math/tex; mode=display">w_1 = -20, w_2 = 0, b = 0</script></li><li><p>异或运算 $a \bigoplus b$，可通过组合运算实现</p><script type="math/tex; mode=display">a \bigoplus b = (\overline{a} ∧ b) ∨ (a ∧ \overline{b})</script></li></ul></blockquote><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul><li><p>隐藏层的激活函数，详情可查看<a href="https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">另一篇博文：神经网络的激活函数</a>；</p></li><li><p>输出层的激活函数</p><ul><li><p>回归问题时，采用线性单元即可</p><script type="math/tex; mode=display">  f(x) = x</script></li><li><p>分类问题时，一般有以下几种选择</p><ul><li><p>单类别概率输出<br>  即每个神经元的输出对应该类别的$0-1$分布输出，这就需要将输出值限制在$[0, 1]$内，例如</p><script type="math/tex; mode=display">P(y=1|x )= max\{0, min\{1, z\}\}</script><p>  <img src="/2018/10/20/Feedforward-Neural-Network/clf_linearout.png" alt="线性输出单元"></p><p>  但是可以看到，当$(w^Tx+b)$处于单位区间外时，模型的输出对它的参数的梯度都将为$0$ ，不利于网络的训练，故采用$S$形函数<code>Sigmoid</code>(<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)</p><script type="math/tex; mode=display">  P(y=1|x ) = \frac{1}{1+e^{-(w^Tx+b)}}</script><blockquote><p>$(1)$ <code>Sigmoid</code>函数定义域为$(-\infty, \infty)$，值域为$(0, 1)$，且在整个定义域上单调递增，即为单值函数，故可将线性输出单元的结果映射到$(0, 1)$范围内；<br>$(2)$ 在定义域上处处可导。</p></blockquote></li><li><p>多类别的概率输出<br>  即每个神经元的输出对应判别为该类别的概率，且有</p><script type="math/tex; mode=display">  \sum_{i=1}^C y_i = 1</script><p>  例如</p><script type="math/tex; mode=display">  y_i = \frac{z_i}{\sum_j z_j}</script><p>  但是分式求导异常麻烦，故采用<code>Softmax</code>函数(<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">详情</a>)作为输出结点的激活函数，该函数求导结果比较简洁，且可利用输出计算导数，计算量减少。</p><script type="math/tex; mode=display">  Softmax(x) = \frac              {1}              {\sum_{k=1}^K exp(x_k)}              \left[                  \begin{matrix}                      exp(x_1)\\                      exp(x_2)\\                      ...\\                      exp(x_K)                  \end{matrix}              \right]</script></li></ul></li></ul></li></ul><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><ul><li><p>回归问题<br>  常见的用于回归问题的损失函数为<code>MSE</code>，即</p><script type="math/tex; mode=display">  L(y, \hat{y}) = \frac{1}{2M} \sum_{i=1}^M (\hat{y}^{(i)} - y^{(i)})^2</script></li><li><p>分类问题<br>  一般采用交叉熵作为损失函数，如下</p><script type="math/tex; mode=display">  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M 1\{y^{(i)}_j=k\}   \log (\hat{y}^{(i)}_j)</script><script type="math/tex; mode=display">  1\{y^{(i)}_j=k\} =       \begin{cases}          1 & y^{(i)}_j = k \\          0 & y^{(i)}_j \neq k       \end{cases}　j = 1, ..., N</script><p>  或者</p><script type="math/tex; mode=display">  L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M   y^{(i)T} \log (\hat{y}^{(i)})</script><p>  其中$y^{(i)}, \hat{y}^{(i)}$均表示向量，采用<code>one-hot</code>编码。</p></li></ul><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><p>以上内容网上资料一大堆，进入重点，反向传播时的梯度推导，给出网络结构如下。</p><ul><li>回归与分类在输出层有所区别；</li><li>各层激活函数的输入变量以$z^{(l)}$表示，输出变量均以$x^{(l)}$表示；</li><li>$W^{(l)}$表示从第$l$层到第$(l+1)$层的权值矩阵，则$w^{(l)}_{ij}$表示第$l$层第$j$个神经元到$(l+1)$层第$i$个神经元的连接权值；</li><li>$b^{(l)}$表示第$l$层到第$(l+1)$层的偏置，则$b^{(l)}_i$表示到第$(l+1)$层第$i$个神经元的偏置值；</li><li>各层变量维度推广为输入$d_{i}$，中间层$d_{h}$，输出层$d_{o}$；</li><li>全连接，部分线条已省略，激活函数已省略；</li></ul><p><img src="/2018/10/20/Feedforward-Neural-Network/fnn.jpg" alt="FNN"></p><p>则各层参数矩阵为</p><script type="math/tex; mode=display">W^{(1)} = \left[        \begin{matrix}            w^{(1)}_{11} & ... & w^{(1)}_{1d_i} \\            ... & ... & ... \\            w^{(1)}_{d_h1} & ... & w^{(1)}_{d_hd_i}        \end{matrix}\right]　b^{(1)} = \left[        \begin{matrix}            b^{(1)}_{1} \\            ... \\            b^{(1)}_{d_h}        \end{matrix}\right]</script><script type="math/tex; mode=display">W^{(2)} = \left[        \begin{matrix}            w^{(2)}_{11} & ... & w^{(2)}_{1d_h} \\            ... & ... & ... \\            w^{(2)}_{d_o1} & ... & w^{(2)}_{d_od_h}        \end{matrix}\right]　b^{(2)} = \left[        \begin{matrix}            b^{(2)}_{1} \\            ... \\            b^{(2)}_{d_o}        \end{matrix}\right]</script><p>有</p><script type="math/tex; mode=display">Z^{(2)} = W^{(1)} X^{(1)} + b^{(1)}</script><script type="math/tex; mode=display">X^{(2)} = \sigma_1 (Z^{(2)})</script><script type="math/tex; mode=display">Z^{(3)} = W^{(2)} X^{(2)} + b^{(2)}</script><script type="math/tex; mode=display">X^{(3)} = \sigma_2 (Z^{(3)})</script><script type="math/tex; mode=display">X^{(1)} = X　\hat{Y} = X^{(3)}</script><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>损失函数采用<code>MSE</code>，即</p><script type="math/tex; mode=display">L(Y, \hat{Y}) = \frac{1}{M} \sum_{i=1}^M L(Y^{(i)}, \hat{Y}^{(i)})</script><script type="math/tex; mode=display">L(Y^{(i)}, \hat{Y}^{(i)}) = \frac{1}{2} || \hat{Y}^{(i)} - Y^{(i)} ||_2^2= \frac{1}{2} \sum_{d_2=1}^{d_o}(\hat{y}^{(i)}_{d_2} - y^{(i)}_{d_2})^2</script><p>下面推导单个样本的损失函数的梯度，该批数据的梯度为均值。</p><blockquote><p>省略样本标记<code>$^{(i)}$</code></p></blockquote><ul><li><p>隐含层到输出层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = \frac{∂}{∂w^{(2)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2} \tag{1}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}  \end{cases}</script><p>  且</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)}) \frac{∂z_{d_2}^{(3)}}{∂w^{(2)}_{ij}} \tag{2}</script><script type="math/tex; mode=display">  \frac{∂}{∂w^{(2)}_{ij}} z_{d_2}^{(3)} =       \begin{cases}          x^{(2)}_{d_1} & d_1 = j, d_2 = i \\          0 & otherwise      \end{cases} \tag{3}</script><p>  $(3)$代入$(2)$，再代入$(1)$可得到</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) x^{(2)}_{d_1} | _{d_1=j, d_2=i}  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \tag{*1}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i}  = \frac{∂}{∂b^{(2)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(2)}_i} \hat{y}_{d_2} \tag{4}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}  \end{cases}</script><p>  有</p><script type="math/tex; mode=display">  \frac{∂}{∂b^{(2)}_i} z_{d_2}^{(3)} =       \begin{cases}          1 &  d_2 = i \\          0 & otherwise      \end{cases} \tag{5}</script><p>  所以</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) | _{d_2=i}  = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \tag{*2}</script></li></ul></li><li><p>输入层到隐含层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = \frac{∂}{∂w^{(1)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2} \tag{6}</script><p>  其中</p><script type="math/tex; mode=display">  \begin{cases}      \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\      z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \\      x^{(2)}_{d_1} = \sigma_1 (z_{d_1}^{(2)}) \\      z_{d_1}^{(2)} = \sum_{d_0=1}^{d_i} w^{(1)}_{d_1 d_0} x^{(1)}_{d_0} + b^{(1)}_{d_1}  \end{cases}</script><p>  故</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}  = \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}}       \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \tag{7}</script><p>  其中</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}}   = \sigma_2' (z_{d_2}^{(3)})  \tag{8}</script><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}   = \sum_{d1=1}^{d_h} w^{(2)}_{d_2 d_1} \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} \tag{9}</script><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}}  = \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}}      \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}}  \tag{10}</script><p>  而其中</p><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}} = x^{(2)}_{d_1} = \sigma_1' (z_{d_1}^{(2)}) \tag{11}</script><script type="math/tex; mode=display">  \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}} =   \begin{cases}      x^{(1)}_{d_0} & d_1 = i, d_0 = j\\      0 & otherwise  \end{cases} \tag{12}</script><p>  $(11),(12)$代入$(10)$得到</p><script type="math/tex; mode=display">  \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} =   \sigma_1' (z_{d_1}^{(2)})      x^{(1)}_{d_0} | _{d_1 = i, d_0 = j} \tag{13}</script><p>  $(13)$代回$(9)$，有</p><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}   = \sum_{d1=1}^{d_h}   \left[      w^{(2)}_{d_2 d_1}       \sigma_1' (z_{d_1}^{(2)})      x^{(1)}_{d_0}  \right] | _{d_1 = i, d_0 = j}</script><script type="math/tex; mode=display">  = w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{14}</script><p>  将$(8),(14)$代入$(7)$得到</p><script type="math/tex; mode=display">  \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{15}</script><p>  $(15)$代入$(6)$有</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = \sum_{d_2=1}^{d_o}       (\hat{y}_{d_2} - y_{d_2})            \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})      x^{(1)}_j \tag{*3}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i}  = \frac{∂}{∂b^{(1)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2</script><script type="math/tex; mode=display">  = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2} \tag{16}</script><p>  同理可得</p><script type="math/tex; mode=display">  \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2}  = \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)})  \tag{17}</script><p>  所以</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i} =  \sum_{d_2=1}^{d_o}       (\hat{y}_{d_2} - y_{d_2})            \sigma_2' (z_{d_2}^{(3)})        w^{(2)}_{d_2 i}       \sigma_1' (z_i^{(2)}) \tag{*4}</script></li></ul></li></ul><p>综上所述</p><script type="math/tex; mode=display">\frac{∂L}{∂w^{(2)}_{ij}}= (\hat{y}_{i} - y_{i})     \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j}</script><script type="math/tex; mode=display">\frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{i} - y_{i})     \sigma_2' (z_i^{(3)})</script><script type="math/tex; mode=display">\frac{∂L}{∂w^{(1)}_{ij}}= \sum_{d_2=1}^{d_o}     (\hat{y}_{d_2} - y_{d_2})          \sigma_2' (z_{d_2}^{(3)})     w^{(2)}_{d_2 i}     \sigma_1' (z_i^{(2)})    x^{(1)}_j</script><script type="math/tex; mode=display">\frac{∂L}{∂b^{(1)}_i} = \sum_{d_2=1}^{d_o}     (\hat{y}_{d_2} - y_{d_2})          \sigma_2' (z_{d_2}^{(3)})     w^{(2)}_{d_2 i}     \sigma_1' (z_i^{(2)})</script><p>令</p><script type="math/tex; mode=display">\begin{cases}    \delta^{(2)}_i     = (\hat{y}_{i} - y_{i})         \sigma_2' (z_i^{(3)}) \\    \delta^{(1)}_i     = \sum_{d_2=1}^{d_o}         \delta^{(2)}_{d_2}         w^{(2)}_{d_2 i}         \sigma_1' (z_i^{(2)})\end{cases}</script><p>有</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂L}{∂w^{(2)}_{ij}} = \delta^{(2)}_i x^{(2)}_{j}\\    \frac{∂L}{∂b^{(2)}_i}    = \delta^{(2)}_i\\    \frac{∂L}{∂w^{(1)}_{ij}} = \delta^{(1)}_i x^{(1)}_j\\    \frac{∂L}{∂b^{(1)}_i}    = \delta^{(1)}_i\end{cases}</script><p>至此推导完毕。</p><blockquote><p>当隐藏层采用<code>Sigmoid</code>函数，输出层采用线性单元，可得到</p><script type="math/tex; mode=display">\sigma_1' (z_i^{(2)}) = \sigma_1 (z_i^{(2)})     \left[1 - \sigma_1 (z_i^{(2)}) \right]= x_i^{(2)} (1 - x_i^{(2)})</script><script type="math/tex; mode=display">\sigma_2' (z_i^{(3)}) = z_i^{(3)}</script><p>此时</p><script type="math/tex; mode=display">\begin{cases}    \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) z_i^{(3)} x^{(2)}_{j} \\    \frac{∂L}{∂b^{(2)}_i}    = (\hat{y}_{i} - y_{i}) z_i^{(3)} \\    \frac{∂L}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} x^{(1)}_j \\    \frac{∂L}{∂b^{(1)}_i}    = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)}\end{cases}</script><p>可以看到，计算梯度时使用的数据在上一次前向传播时已计算得，故可减少计算量。</p></blockquote><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>损失函数采用<code>Cross Entropy</code>，即</p><script type="math/tex; mode=display">L(\hat{y}, y) = \frac{1}{M} \sum_{i=1}^M L(\hat{y}^{(i)}, y^{(i)})</script><script type="math/tex; mode=display">L(\hat{y}^{(i)}, y^{(i)}) = - y^{(i)T} \log (\hat{y}^{(i)})</script><p>上式中，$y^{(i)}, \hat{y}^{(i)}$均为列向量，且$y^{(i)}$表示<code>one-hot</code>编码后的标签向量，也可写作</p><script type="math/tex; mode=display">L(\hat{y}^{(i)}, y^{(i)})= - \log \hat{y}^{(i)}_{y^{(i)}}</script><ul><li>由该式可以看出，若输出层激活函数采用<code>Sigmoid</code>作为激活函数，则隐藏层——输出层之间权值矩阵$W^{(2)}$只会更新$w^{(i)}_{y^{(i)} d_1}, d_1 = 1, …, d_h$；</li><li>一般采用<code>SoftMax</code>作为输出层激活函数，<code>Sigmoid</code>下面不作推导。</li></ul><blockquote><p>关于<code>SoftMax</code>的梯度，移步<a href="https://louishsu.xyz/2018/10/18/Softmax-Regression/" target="_blank" rel="noopener">SoftMax Regression</a>中查看详细推导过程，这里直接给出结论。<br>对于</p><script type="math/tex; mode=display">S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]</script><p>其梯度为</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i}_{K×1} =  \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] -  \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right]= \left( \left[ \begin{matrix}  0 \\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i</script><p>省略样本标记<code>$^{(i)}$</code></p></blockquote><ul><li><p>隐含层到输出层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = - \frac{∂}{∂w^{(2)}_{ij}} \log \hat{y}_{y}  = - \frac{1}{\hat{y}_y}       \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}} \tag{18}</script><p>  其中$\hat{y}_{y}$与$z^{(3)}_{d_2}(d_2 = 1, …, d_o) $均有联系，故</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}}  = \sum_{d2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}} \tag{19}</script><p>  而</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}  = \begin{cases}      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\      - \hat{y}_{y} \hat{y}_{d_2} & otherwise  \end{cases}</script><script type="math/tex; mode=display">  \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}}  = \begin{cases}      x^{(2)}_{d_1} & i = d_2, j = d_1 \\      0 & otherwise  \end{cases}</script><blockquote><p>$z^{(3)}_{d_2} = \sum_{d_1=0}^{d_h} w^{(2)}_{d_2d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}$</p></blockquote><p>  代回$(19)$，再带回$(18)$，有</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = - \frac{1}{\hat{y}_{y}}       \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       x^{(2)}_{d_1} | _{d_2=i, d_1=j}</script><script type="math/tex; mode=display">  = \begin{cases}      - \frac{1}{\hat{y}_{y}} \hat{y}_{y} (1 - \hat{y}_i) x^{(2)}_j & i = y \\      - \frac{1}{\hat{y}_{y}} (- \hat{y}_{y} \hat{y}_i) x^{(2)}_j & otherwise  \end{cases}</script><script type="math/tex; mode=display">  = \begin{cases}      (\hat{y}_i - 1) x^{(2)}_j & i = y \\      \hat{y}_i x^{(2)}_j & otherwise  \end{cases}</script><p>  即</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(2)}_{ij}}  = (\hat{y}_i - y_i) x^{(2)}_j \tag{*5}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(2)}_i}  = \hat{y}_i - y_i \tag{*6}</script></li></ul></li><li><p>输入层到隐含层</p><ul><li><p>对权值矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = - \frac{∂}{∂w^{(1)}_{ij}} \log \hat{y}_{y}  = - \frac{1}{\hat{y}_{y}}       \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}} \tag{20}</script><p>  其中</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}}  = \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}} \tag{21}</script><p>  $\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}$部分与回归相同，有</p><script type="math/tex; mode=display">  \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}}  = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><p>  由上面分析可得</p><script type="math/tex; mode=display">  \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}  = \begin{cases}      \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\      - \hat{y}_{y} \hat{y}_{d_2} & otherwise  \end{cases}</script><p>  故代回$(20)$可得到</p><script type="math/tex; mode=display">  \frac{∂L}{∂w^{(1)}_{ij}}  = - \frac{1}{\hat{y}_{y}}      \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}</script><script type="math/tex; mode=display">  = - \frac{1}{\hat{y}_{y}}      \sum_{d_2=1}^{d_o}       \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}}       w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">  = \left[       \sum_{d_2=1, d_2 \neq y}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} +       (\hat{y}_y - 1) w^{(2)}_{y i}   \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j</script><script type="math/tex; mode=display">  = \left[       \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} -       w^{(2)}_{y i}  \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*7}</script></li><li><p>对偏置矩阵的梯度</p><script type="math/tex; mode=display">  \frac{∂L}{∂b^{(1)}_i}  = \left[       \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} -       w^{(2)}_{y i}  \right] \sigma_1' (z_i^{(2)}) \tag{*8}</script></li></ul></li></ul><p>至此推导完毕。</p><blockquote><p>这个推导，仅供参考</p></blockquote><h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>和其他算法一样，前馈神经网络也存在过拟合的问题，解决方法有以下几种</p><ul><li><p>正则化<br>  与线性回归类似，神经网络也可以加入范数惩罚项，以下$C$表示普通的损失函数，$\lambda$为惩罚系数，$n$为样本数目，$w$表示权值参数。</p><ul><li><code>L1</code>正则化<br>  惩罚项为网络所有权值的绝对值之和。<script type="math/tex; mode=display">  C = C_0 + \frac{\lambda}{n} \sum_w |w|</script></li><li><code>L2</code>正则化<br>  又称权值衰减<code>weights decay</code>，惩罚项为网络所有权值的平方和。<script type="math/tex; mode=display">  C = C_0 + \frac{\lambda}{2n} \sum_w w^2</script></li></ul></li><li><p>Dropout<br>  以概率大小为<code>p</code>使部分神经元输出值直接为0，如此可以使反向传播时相关权值系数不做更新，只有被保留下来的权值和偏置值会被更新。<br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_1.png" alt="dropout_1"><br>  <img src="/2018/10/20/Feedforward-Neural-Network/dropout_2.png" alt="dropout_2"></p></li><li><p>增加训练数据大小<br>  可在原数据上加以变换或噪声，图像的扩增方法可查看<a href="https://louishsu.xyz/2018/11/02/%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%89%A9%E5%A2%9E-Augment-%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">图像数据集扩增</a>。</p></li></ul><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-PyTorch-Tutorial/blob/master/NeuralNetwork_ANN_MNIST.py" target="_blank" rel="noopener">@Github: Code of Neural Network</a></p><p>使用<code>PyTorch</code>实现神经网络，以下为模型定义<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnnNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AnnNet, self).__init__()</span><br><span class="line">        self.input_size = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">        self.hidden_size = <span class="number">100</span></span><br><span class="line">        self.output_size = <span class="number">10</span></span><br><span class="line">        self.fc1 = nn.Linear(self.input_size,  self.hidden_size)    <span class="comment"># input   - hidden</span></span><br><span class="line">        self.fc2 = nn.Linear(self.hidden_size, self.output_size )   <span class="comment"># hidden  - output</span></span><br><span class="line">        <span class="comment"># self.activate = nn.Sigmoid()  # 参数更新非常慢，特别是层数多时</span></span><br><span class="line">        self.activate = nn.ReLU()       <span class="comment"># 事实证明ReLU作为激活函数更加合适</span></span><br><span class="line">        self.softmax = nn.Softmax()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        h = self.activate(self.fc1(X))</span><br><span class="line">        y_pred = self.softmax(self.fc2(h))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>分类问题的决策平面</title>
      <link href="/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/"/>
      <url>/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>对于分类问题，计算结果一般为概率值，那么如何根据计算得的概率进行判别分类呢？</p><blockquote><p>这部分理解后，<a href="https://louishsu.xyz/2018/10/18/Logistic-Regression/" target="_blank" rel="noopener">Logistic回归</a>与<a href="https://louishsu.xyz/2018/10/18/Softmax-Regression/" target="_blank" rel="noopener">Softmax回归</a>的模型就很容易推得。</p></blockquote><h1 id="判别函数"><a href="#判别函数" class="headerlink" title="判别函数"></a>判别函数</h1><p>对于一个类别为$K$的分类问题，如果对于所有的$ i,j=1,…,K, j\neq i$，有</p><script type="math/tex; mode=display">g_i(x) > g_j(x)</script><p>则此分类器将这个样本对应的特征向量$x$判别为$w_i$，则此分类器的作用是，计算$K$个判别函数并选取与最大判别值最大对应的类别。</p><blockquote><p>判别函数的形式并不唯一，可以将所有的判别函数乘上相同的正常数或者加上一个相同的常量而不影响其判决结果。更一般的情况下，我们使用单调递增函数$f(·)$进行映射，将每一个$g_i(x)$替换成$f(g_i(x))$，分类结果不变。</p><div style="text-align: right"> ——《模式识别原理与应用课程笔记》</div></blockquote><p>例如<a href="https://louishsu.xyz/2018/10/18/Bayes-Decision/" target="_blank" rel="noopener">最小风险贝叶斯决策</a></p><h1 id="正态分布下的判别函数"><a href="#正态分布下的判别函数" class="headerlink" title="正态分布下的判别函数"></a>正态分布下的判别函数</h1><blockquote><p><a href="https://www.cnblogs.com/bingjianing/p/9117330.html" target="_blank" rel="noopener">多元高斯分布（The Multivariate normal distribution） - bingjianing - 博客园</a></p></blockquote><p>由大数定理可知，在样本足够的情况下，数据服从正态分布。多元正态分布形式如下</p><script type="math/tex; mode=display">f(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))</script><p>其中</p><script type="math/tex; mode=display">x = [x_1, ..., x_n]^T</script><script type="math/tex; mode=display">\mu = [\mu_1, ..., \mu_n]^T</script><script type="math/tex; mode=display">\Sigma_{ij} = cov(x_i, x_j)</script><!-- $$\Sigma =  \left[​    \begin{matrix}​        1 & 2 & 3 \\​        4 & 5 & 6 \\​        7 & 8 & 9​    \end{matrix}\right] \tag{3}$$ --><p>在<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">最小错误率判别</a>时</p><script type="math/tex; mode=display">g_i(x) = P(x|c_i)P(c_i)</script><p>即</p><script type="math/tex; mode=display">g_i(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}}exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i)) ·P(c_i)</script><p>取对数运算，并舍去常数项，展开整理得</p><script type="math/tex; mode=display">g_i(x) = -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x  -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i) \tag{0}</script><blockquote><p><code>注：</code> 协方差矩阵 $ \Sigma^T = \Sigma $</p></blockquote><h2 id="1-Sigma-i-sigma-2-I"><a href="#1-Sigma-i-sigma-2-I" class="headerlink" title="1. $\Sigma_i = \sigma^2 I$"></a>1. $\Sigma_i = \sigma^2 I$</h2><p>$\Sigma_i^{-1} = \frac{1}{\sigma^2} I$代入$(0)$，有</p><script type="math/tex; mode=display">g_i(x) = \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)\tag{1}</script><p>定义</p><script type="math/tex; mode=display">w_i = \frac{1}{\sigma^2}\mu_i^T</script><script type="math/tex; mode=display">w_0 =  - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)</script><p>有一般形式如下，表示取$c_i$的概率</p><script type="math/tex; mode=display">g_i(x) = w_i x + w_0\tag{2}</script><p>设决策平面为</p><script type="math/tex; mode=display">w^T (x−x_0)=0\tag{3}</script><p>决策平面上，取$c_i$和$c_j$的概率相等，即</p><script type="math/tex; mode=display">g_i(x) = g_j(x)</script><p>可得</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} \tag{4}</script><blockquote><p>推导过程如下，将$(1)$代入上式<br>$ \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i) = \frac{1}{\sigma^2}\mu_j^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_j ^T \mu_j) + ln P(c_j) $<br>$ \mu_i^T x - \frac{1}{2} \mu_i ^T \mu_i + ln P(c_i) = \mu_j^T x - \frac{1}{2} \mu_j ^T \mu_j + ln P(c_j) $<br>$ (\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} $</p></blockquote><p>由$(3)$$(4)$，利用待定系数法，可得</p><script type="math/tex; mode=display">w = \mu_i - \mu_j</script><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)}</script><p>特别地，当等先验概率时，即$P(c_i) = P(c_j)$时</p><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j)</script><p>故</p><script type="math/tex; mode=display">x_0 = \frac{1}{2}(\mu_i + \mu_j)</script><p>结论：等先验概率时超平面$ w^T (x−x_0)=0 $平分判别空间</p><blockquote><p>$\mu_i$与$\mu_j$分别表示两个类别的中心，由向量运算，$x_0$为两类中心的连线的中点。</p></blockquote><h2 id="2-Sigma-i-Sigma"><a href="#2-Sigma-i-Sigma" class="headerlink" title="2. $\Sigma_i = \Sigma$"></a>2. $\Sigma_i = \Sigma$</h2><p>代入$(0)$后可得</p><script type="math/tex; mode=display">g_i(x) =  \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) \tag{5}</script><p>定义</p><script type="math/tex; mode=display">w_i = \mu_i^T \Sigma ^{-1}</script><script type="math/tex; mode=display">w_0 = - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i)</script><p>有一般形式如下，表示取$c_i$的概率</p><script type="math/tex; mode=display">g_i(x) = w_i x + w_0\tag{6}</script><p>同样的，设决策平面为</p><script type="math/tex; mode=display">w^T (x−x_0)=0\tag{7}</script><p>决策平面上，取$c_i$和$c_j$的概率相等，即</p><script type="math/tex; mode=display">g_i(x) = g_j(x)</script><p>有</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i - \mu_j)^T \Sigma ^{-1} (\mu_i - \mu_j) - ln \frac{P(c_i)}{P(c_j)}</script><blockquote><p>$ \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x   -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $<br>$ \mu_i^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $<br>$ (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j) - ln \frac{P(c_i)}{P(c_j)} $</p></blockquote><p>特别的，当取等先验概率时</p><script type="math/tex; mode=display">(\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)</script><p>由$(7)$$(8)$，利用待定系数法</p><script type="math/tex; mode=display">w^T = (\mu_i - \mu_j)^T \Sigma^{-1}</script><script type="math/tex; mode=display">w^T x_0 = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)</script><blockquote><p><code>注：</code> 协方差矩阵 $ \Sigma^T = \Sigma $</p></blockquote><script type="math/tex; mode=display">w = \Sigma^{-1}(\mu_i - \mu_j)</script><script type="math/tex; mode=display">x_0 = \frac{1}{2} (\mu_i + \mu_j)</script><p>由于通常$w=Σ^{−1}(μ_i−μ_j)$并非朝着$(μ_i−μ_j)$的方向，因而通常分离两类的超平面也并非与均值的连线垂直正交。但是， 如果先验概率相等，其判定面确实是与均值连线交于中点$x_0$处的。如果先验概率不等，最优边界超平面将远离可能性较大的均值。同前，如果偏移量足够大，判定面可以不落在两个均值向量之间。</p><h2 id="3-Sigma-i-Sigma-i-∀"><a href="#3-Sigma-i-Sigma-i-∀" class="headerlink" title="3. $\Sigma_i = \Sigma_i(∀) $"></a>3. $\Sigma_i = \Sigma_i(∀) $</h2><script type="math/tex; mode=display">g_i(x) =  -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x  -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)</script><p>定义</p><script type="math/tex; mode=display">W_i = -\frac{1}{2} \Sigma_i ^{-1}</script><script type="math/tex; mode=display">w_i = \mu_i^T \Sigma_i ^{-1}</script><script type="math/tex; mode=display">w_0 = -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)</script><p>有</p><script type="math/tex; mode=display">g_i(x) = x^TW_ix + w_ix + w_0</script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Bayes Decision</title>
      <link href="/2018/10/18/Bayes-Decision/"/>
      <url>/2018/10/18/Bayes-Decision/</url>
      
        <content type="html"><![CDATA[<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>基于贝叶斯公式</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><script type="math/tex; mode=display">P(x)=\sum_j p(x|c_j)P(c_j)</script><h1 id="几种常用的贝叶斯决策"><a href="#几种常用的贝叶斯决策" class="headerlink" title="几种常用的贝叶斯决策"></a>几种常用的贝叶斯决策</h1><h2 id="最小错误率贝叶斯决策"><a href="#最小错误率贝叶斯决策" class="headerlink" title="最小错误率贝叶斯决策"></a>最小错误率贝叶斯决策</h2><p>在分类问题中，我们往往希望尽可能减少分类错误，即目标是追求最小错误率。假设有$K$分类问题，由贝叶斯公式</p><script type="math/tex; mode=display">P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}</script><p>上式中$ k=1,…,K  $，各部分定义如下</p><blockquote><p>$P(c_k|x)$——<code>后验概率(posteriori probability)</code><br>$P(c_k)$——<code>先验概率(priori probability)</code>，<br>$p(x|c_k)$——$c_k$关于$x$的<code>似然函数(likelihood)</code>，<br>$p(x)$——<code>证据因子(evidence)</code>，</p></blockquote><p>证据因子由下式计算</p><script type="math/tex; mode=display">p(x)=\sum_{j=0}^K p(x|c_j)P(c_j)</script><p>以上就是从样本中训练的参数，在预测阶段，定义决策规则为</p><blockquote><p>$if$ $P(c_i|x)&gt;P(c_j|x)$, $then$ $ x \in c_i $</p></blockquote><p>由于分母为标量，对于任意输入的样本特征$x$，$P(x)$一定，故决策规则可简化为</p><blockquote><p>$if$ $P(x|c_i)P(c_i)&gt;P(x|c_j)P(c_j)$, $then$ $ x \in c_i $</p></blockquote><p>而对于分类错误的样本，如样本$x$属于分类$c_i$，但错误分类为$c_{err}, err \neq i$，样本的错误分类概率为</p><script type="math/tex; mode=display">P(error|x) = P(c_{err}|x)</script><p>上式被称作<code>误差概率</code>，某类后验概率越大，则相应的误差概率就越小，定义平均误差概率</p><script type="math/tex; mode=display">P_{mean} = \int P(error|x)P(x)dx</script><h2 id="带有拒绝域的最小错误率贝叶斯决策"><a href="#带有拒绝域的最小错误率贝叶斯决策" class="headerlink" title="带有拒绝域的最小错误率贝叶斯决策"></a>带有拒绝域的最小错误率贝叶斯决策</h2><p>一些情况下，某样本对应特征$x$计算结果中，属于各类别的概率没有显著比较大的数值，换句话说都比较小，那么对这次的判别就不太信任，选择拒绝决策结果。<br>将决策平面划分为两个区域</p><script type="math/tex; mode=display">Acquired = \{x|max_j P(c_j|x)\geq 1-t\}</script><script type="math/tex; mode=display">Rejected = \{x|max_j P(c_j|x) < 1-t\}</script><p>其中$t$为阈值，$t$越小时，拒绝域$Rejected$越大，当满足</p><script type="math/tex; mode=display">1-t \leq \frac{1}{K}</script><p>或者 </p><script type="math/tex; mode=display">t \geq \frac{K-1}{K}</script><p>此时拒绝域为</p><script type="math/tex; mode=display">Rejected = \{x|max_j P(c_j|x) < \frac{1}{K}\}</script><p>而当且仅当各分类概率相等时才有 $ max_j P(c_j|x) = \frac{1}{K} $，因此此时拒绝域为空，接受所有决策结果</p><h2 id="最小风险贝叶斯决策"><a href="#最小风险贝叶斯决策" class="headerlink" title="最小风险贝叶斯决策"></a>最小风险贝叶斯决策</h2><p>在决策过程中，不同类型的决策错误所产生的代价是不同的。引入风险函数</p><script type="math/tex; mode=display">\lambda_{i, j} = \lambda (\alpha_i|c_j)</script><p>表示实际类别为$c_j$时，采取错误判断为$c_i$的行为$\alpha_i$所产生的损失。该函数称为损失函数，通常它可以用表格的形式给出，叫做决策表，形如<br><img src="/2018/10/18/Bayes-Decision/decision_table.jpg" alt="决策表"><br>定义条件风险</p><script type="math/tex; mode=display">R(\alpha_i|c_j) = \sum_j \lambda (\alpha_i|c_j) P(c_j|x)</script><p>特别地，取$0-1$损失时，即最小错误率贝叶斯决策</p><script type="math/tex; mode=display">\lambda (\alpha_i|c_j) = \begin{cases}0 & i = j \\1 & i \neq j\end{cases}</script><!-- $$函数名 = \begin{cases}公式1 & 条件1 \\公式2 & 条件2 \\公式3 & 条件3 \end{cases}$$--><p>可能比较抽象，这里举了一个例子</p><div style="align: center"><img src="/2018/10/18/Bayes-Decision/最小风险贝叶斯决策例.png"></div><h1 id="关于判别函数"><a href="#关于判别函数" class="headerlink" title="关于判别函数"></a>关于判别函数</h1><p>可查看<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a></p><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><p><img src="/2018/10/18/Bayes-Decision/李航-例4.1.png" alt="例4.1"></p><p>为帮助理解，先手动计算一遍结果</p><blockquote><p>先验概率(<code>priori probability</code>):<br>$ P(Y = -1) = \frac{6}{15} $<br>$ P(Y = 1) = \frac{9}{15} $<br>似然函数(<code>likelihood</code>)<br>$ P(X^{(1)} = 1|Y=-1) = \frac{3}{6}$<br>$ P(X^{(1)} = 2|Y=-1) = \frac{2}{6}$<br>$ P(X^{(1)} = 3|Y=-1) = \frac{1}{6}$<br>$ P(X^{(2)} = S|Y=-1) = \frac{3}{6}$<br>$ P(X^{(2)} = M|Y=-1) = \frac{2}{6}$<br>$ P(X^{(2)} = L|Y=-1) = \frac{1}{6}$<br>$ P(X^{(1)} = 1|Y=1) = \frac{2}{9}$<br>$ P(X^{(1)} = 2|Y=1) = \frac{3}{9}$<br>$ P(X^{(1)} = 3|Y=1) = \frac{4}{9}$<br>$ P(X^{(2)} = S|Y=1) = \frac{1}{9}$<br>$ P(X^{(2)} = M|Y=1) = \frac{4}{9}$<br>$ P(X^{(2)} = L|Y=1) = \frac{4}{9}$</p></blockquote><p>注意：证据因子(<code>evidence</code>)不能用如下朴素贝叶斯求解</p><script type="math/tex; mode=display">P(X) = P(X^{(1)}) P(X^{(2)})</script><p>而是</p><script type="math/tex; mode=display">P(X) =  P(X^{(1)}|Y=-1)P(Y = -1) + P(X^{(2)}|Y=-1)P(Y = -1)</script><p>一般分子用朴素贝叶斯求解</p><script type="math/tex; mode=display">P(X|Y) = P(X^{(1)}|Y) P(X^{(2)}|Y)</script><p>将其加和作为分母</p><script type="math/tex; mode=display">c_k: P(X)_k = \sum_{k=0}^2 P(X^{(1)}|Y=k) P(X^{(2)}|Y=k)</script><script type="math/tex; mode=display">P(Y_k|X) = \frac{P(X|Y_k)P(Y_k)}{P(X)_k}</script><p>选取最大概率的$ k $类别作为判别类别</p><script type="math/tex; mode=display">k = argmax_k P(Y_k|X)</script><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Statistical%20Learning%20Method%2C%20Li%20Hang/naive_bayes_algorithm_demo.py" target="_blank" rel="noopener">@Github: Code for Naive Bayes Decision</a></p><h3 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    X_encoded = self.featureEncoder.fit_transform(X).toarray()</span><br><span class="line">    y_encoded = OneHotEncoder().fit_transform(y.reshape((<span class="number">-1</span>, <span class="number">1</span>))).toarray()</span><br><span class="line">    self.P_X = np.mean(X_encoded, axis=<span class="number">0</span>)                           <span class="comment"># one-hot编码下，各列的均值即各特征的概率</span></span><br><span class="line">    self.P_Y = np.mean(y_encoded, axis=<span class="number">0</span>)                           <span class="comment"># one-hot编码下，各列的均值即各了别的概率</span></span><br><span class="line">    self.n_labels, self.n_features = y_encoded.shape[<span class="number">1</span>], X_encoded.shape[<span class="number">1</span>]   </span><br><span class="line">    self.P_X_Y = np.zeros(shape=(self.n_labels, self.n_features))   <span class="comment"># 各个类别下，分别统计各特征的概率</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_labels):</span><br><span class="line">        X_encoded_of_yi = X_encoded[y_encoded[:, i]==<span class="number">1</span>]             <span class="comment"># 取出属于i类别的样本</span></span><br><span class="line">        self.P_X_Y[i] = np.mean(X_encoded_of_yi, axis=<span class="number">0</span>)            <span class="comment"># one-hot编码下，各列的均值即各特征的概率</span></span><br></pre></td></tr></table></figure><h3 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    X_encoded = self.featureEncoder.transform(X).toarray()</span><br><span class="line">    n_samples = X_encoded.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred_prob = np.zeros(shape=(n_samples, self.n_labels))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n_labels):</span><br><span class="line">            P_Xi_encoded_Yj = X_encoded[i] * self.P_X_Y[j]          <span class="comment"># 在Yj类别下，选出输入样本Xi对应的条件概率</span></span><br><span class="line">            P_Xi_encoded_Yj[P_Xi_encoded_Yj==<span class="number">0.0</span>] = <span class="number">1.0</span>             <span class="comment"># 将为0值替换为1，便于求解ΠP(Xi|yc)，只要将各元素累乘即可</span></span><br><span class="line">            y_pred_prob[i, j] = self.P_Y[j] * P_Xi_encoded_Yj.prod()</span><br><span class="line">        y_pred_prob[i] /= np.sum(y_pred_prob[i])                    <span class="comment"># 分母一般是将分子加和，不能假定各特征独立并用朴素贝叶斯计算分母</span></span><br><span class="line">    <span class="keyword">return</span> np.argmax(y_pred_prob, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">]</span><br><span class="line">y = [<span class="number">0</span> ,<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">estimator = NaiveBayes()</span><br><span class="line">estimator.fit(X, y)</span><br><span class="line"></span><br><span class="line">X_test = np.array([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y_pred = estimator.predict(X_test)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Softmax Regression</title>
      <link href="/2018/10/18/Softmax-Regression/"/>
      <url>/2018/10/18/Softmax-Regression/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" target="_blank" rel="noopener">Unsupervised Feature Learning and Deep Learning Tutorial</a></p></blockquote><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p><code>Logistic Regression</code>中采用的非线性函数为<code>Sigmoid</code>，将输出值映射到$(0, 1)$之间作为概率输出，处理的是二分类问题，那么对于多分类的问题怎么处理呢？</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><blockquote><p>由<a href="https://louishsu.xyz/2018/10/18/Logistic%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">Logistic回归</a>推广而来</p></blockquote><h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p><code>Softmax</code>在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类$(K&gt;2)$问题，分类器最后的输出单元需要<code>Softmax</code>函数进行数值处理。</p><script type="math/tex; mode=display">S(x) = \frac            {1}            {\sum_{k=1}^K exp(x_k)}            \left[                \begin{matrix}                    exp(x_1)\\                    exp(x_2)\\                    ...\\                    exp(x_K)                \end{matrix}            \right]</script><p>其中$x$为矩阵形式的向量，其维度为$(K×1)$，$K$为类别数目。<code>Softmax</code>的输出向量维度与$x$相同，各元素$x_i$加和为$1$，可用于表示取各个类别的概率。</p><p>注意到，对于函数$e^x$</p><script type="math/tex; mode=display">\lim_{x \rightarrow - \infty} e^x = 0</script><script type="math/tex; mode=display">\lim_{x \rightarrow + \infty} e^x = +\infty</script><blockquote><p>假设所有的$x_i$等于某常数$c$，理论上对所有$x_i$上式结果为$\frac{1}{n}$</p><ul><li>若$c$为很小的负数，$e^c$下溢，结果为$NaN$；</li><li>若$c$量级很大，$e^c$上溢，结果为$NaN$。</li></ul></blockquote><p>在数值计算时并不稳定，但是<code>Softmax</code>所有输入增加同一常数时，输出不变，得稳定版本：</p><script type="math/tex; mode=display">S(x) := S(x - max(x_i))</script><blockquote><script type="math/tex; mode=display">e^{x_{max} - max(x_i)} = 1</script><ul><li>减去最大值导致$e^x$最大为$1$，排除上溢；</li><li>分母中至少有一项为$1$，排除分母下溢导致处以$0$的情况。</li></ul><p>其对数</p><script type="math/tex; mode=display">log S(x)_i = x_i - log ({\sum_{k=1}^K exp(x_k)})</script><ul><li>注意到，第一项表示输入$x_i$总是对代价函数有直接的贡献。这一项不会饱和，所以即使$x_i$对上式的第二项的贡献很小，学习依然可以进行；</li><li>当最大化对数似然时，第一项鼓励$x_i$被推高，而第二项则鼓励所有的$x$被压低；</li><li>第二项$log ({\sum_{k=1}^K exp(x_k)})$可以大致近似为$max(x_k)$，这种近似是基于对任何明显小于$max(x_k)$的$x_k$都是不重要的，<strong>负对数似然代价函数总是强烈地惩罚最活跃的不正确预测</strong></li><li>除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习</li></ul><hr><p>作者：NirHeavenX<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/qsczse943062710/article/details/61912464" target="_blank" rel="noopener">https://blog.csdn.net/qsczse943062710/article/details/61912464</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p></blockquote><h2 id="Softmax解决多分类问题"><a href="#Softmax解决多分类问题" class="headerlink" title="Softmax解决多分类问题"></a>Softmax解决多分类问题</h2><p>对于具有$K$个分类的问题，每个类别训练一组参数$ w_k $</p><script type="math/tex; mode=display">z_k^{(i)} = w_k^Tx^{(i)}</script><p>或写作矩阵形式</p><script type="math/tex; mode=display">z^{(i)} = W^Tx^{(i)}</script><p>其中</p><script type="math/tex; mode=display">x^{(i)} =     \left[        \begin{matrix}            x_0^{(i)}\\            x_1^{(i)}\\            ...\\            x_n^{(i)}        \end{matrix}    \right]_{n×1},x_0^{(i)}=1</script><script type="math/tex; mode=display">W = [w_1, w_2, ..., w_K]_{(n+1)×K}</script><script type="math/tex; mode=display">w_i =     \left[        \begin{matrix}            w_{i0}\\            w_{i1}\\            ...\\            w_{in}        \end{matrix}    \right]_{n×1}</script><p>最终各类别输出概率为</p><script type="math/tex; mode=display">\hat{y}^{(i)} = Softmax(z^{(i)})</script><blockquote><p><strong>产生了一个奇怪的脑洞。。。</strong><br>二分类问题</p><script type="math/tex; mode=display">p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }</script><p>定义二分类线性单元输出的差值为</p><script type="math/tex; mode=display">z = x_1 - x_2</script><p>得到</p><script type="math/tex; mode=display">p(x_1) = \frac{1}{1 + e^{-z}}</script><p>以$x_1 = [x_{11}, x_{12}]^T$为例(二维特征)，取$w_1=1, w_2=2, b=3$</p><script type="math/tex; mode=display">p(x_1) = \frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}}</script><p><img src="/2018/10/18/Softmax-Regression/Sigmoid_2dim.png" alt="特征为2时的决策平面"></p><p>而多分类问题，以$3$分类为例</p><script type="math/tex; mode=display">p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }</script><p>定义线性单元输出的差值为</p><script type="math/tex; mode=display">z_{12} = x_1 - x_2</script><script type="math/tex; mode=display">z_{13} = x_1 - x_3</script><script type="math/tex; mode=display">p(x_1) = \frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }</script><p>做出图像为<br><img src="/2018/10/18/Softmax-Regression/3D_sigmoid_1.png" alt="3D_sigmoid_1"><br><img src="/2018/10/18/Softmax-Regression/3D_sigmoid_2.png" alt="3D_sigmoid_2"></p></blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="由交叉熵理解"><a href="#由交叉熵理解" class="headerlink" title="由交叉熵理解"></a>由交叉熵理解</h2><script type="math/tex; mode=display">CrossEnt = \sum_j p_j log \frac{1}{q_j}</script><p>而对于样本$ (X^{(i)}, y^{(i)}) $，为确定事件，故标签概率各元素的取值$p_j$为$ y^{(i)}_j ∈ \{0,1\}$，$ q_j即预测输出的概率值\hat{y}^{(i)}_j$</p><p>一般取各个样本损失的均值$(\frac{1}{N})$</p><script type="math/tex; mode=display">L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N 1\{y^{(i)}_j=k\} log (\hat{y}^{(i)}_j)</script><script type="math/tex; mode=display">1\{y^{(i)}_j=k\} =     \begin{cases}        1 & y^{(i)}_j = k \\        0 & y^{(i)}_j \neq k     \end{cases}</script><p>可对实际标签$y^{(i)}$采取<code>One-Hot</code>编码，便于计算</p><script type="math/tex; mode=display">y^{(i)} = \left[         \begin{matrix}            0, ..., 1_{y^{(i)}}, ..., 0        \end{matrix}     \right]^T</script><p>则</p><script type="math/tex; mode=display">L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N y^{(i)T}log (\hat{y}^{(i)})</script><blockquote><p>实际上，由熵定义</p><script type="math/tex; mode=display">H(p) = \sum_x p(x) \log \frac{1}{p(x)}</script><p>交叉熵为</p><script type="math/tex; mode=display">H(p, q) = \sum_x p(x) \log \frac{1}{q(x)}</script><p>K-L散度为</p><script type="math/tex; mode=display">D_{KL}(p || q) = \sum_x p(x) \log \frac{p(x)}{q(x)}</script><p>也即</p><script type="math/tex; mode=display">D_{KL}(p || q) = H(p, q) - H(p)</script><p>常常用于衡量两个概率分布$p(x), q(x)$之间的差异。而对于固定的数据集，$H(p)$为常熟，故最小化交叉熵$H(p, q)$实际上为最小化K-L散度$D_{KL}(p || q)$。</p></blockquote><h2 id="由决策平面理解"><a href="#由决策平面理解" class="headerlink" title="由决策平面理解"></a>由决策平面理解</h2><p>从<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>和<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a>可知，对于类别$c_i$，有</p><script type="math/tex; mode=display">P(c_i|x) = \frac{P(x|c_i)}{\sum_{j=0}^KP(x|c_j)}</script><blockquote><p>假设每个类别的样本服从正态分布，先验概率相等，各类别样本特征间协方差相等。证明略.</p></blockquote><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><h2 id="Softmax函数的导数"><a href="#Softmax函数的导数" class="headerlink" title="Softmax函数的导数"></a>Softmax函数的导数</h2><p>对于</p><script type="math/tex; mode=display">S(x) = \frac            {1}            {\sum_{k=1}^K exp(x_k)}            \left[                \begin{matrix}                    exp(x_1)\\                    exp(x_2)\\                    ...\\                    exp(x_K)                \end{matrix}            \right]</script><p>一般输出作为概率值，记</p><script type="math/tex; mode=display">P = S(x)</script><script type="math/tex; mode=display">p_i = S(x)_i</script><p>对向量$x$中某元素求导</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i} = \frac{∂}{∂x_i}                    \left[                        \begin{matrix}                            ...\\                            \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}\\                            ...\\                        \end{matrix}                    \right]</script><blockquote><p>$(1)$ $i=k$<br>$<br>\frac{∂}{∂x_i} \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)}$<br>$ = \frac{exp’(x_i)·\sum_{j=1}^K exp(x_j) - exp(x_i)·(\sum_{j=1}^K exp(x_j))’}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{exp(x_i)·\sum_{j=1}^K exp(x_j) - exp^2(x_i)}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)} -<br>(\frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)})^2<br>$<br>$ = p_i (1 - p_i)<br>$</p><p>$(2)$ $i\neq k$<br>$<br>\frac{∂}{∂x_i} \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}$<br>$ = \frac{exp’(x_k)·\sum_{j=1}^K exp(x_j) - exp(x_k)·(\sum_{j=1}^K exp(x_j))’}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$ = \frac{- exp(x_k)exp(x_i)}<br>{(\sum_{j=1}^K exp(x_j))^2}$<br>$= - p_i p_k$</p><p>综上</p><script type="math/tex; mode=display">\frac{∂S(x)}{∂x_i}_{K×1} =  \left[                          \begin{matrix}                              0\\                              ...\\                              p_i\\                              ...\\                              0                           \end{matrix}                      \right] -                       \left[                          \begin{matrix}                                p_i p_1\\                                ...\\                                p_i^2\\                                ...\\                                p_i p_K                            \end{matrix}                      \right]=     \left(                      \left[                          \begin{matrix}                              0\\                              ...\\                              1\\                              ...\\                              0                           \end{matrix}                      \right] -                       p      \right)p_i</script></blockquote><h2 id="损失函数梯度"><a href="#损失函数梯度" class="headerlink" title="损失函数梯度"></a>损失函数梯度</h2><p>在<code>OneHot</code>编码下，损失函数形式为</p><script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{N} \sum_{i=1}^N L (y^{(i)}, \hat{y}^{(i)})</script><script type="math/tex; mode=display">L (y^{(i)}, \hat{y}^{(i)}) = - y^{(i)T}log \hat{y}^{(i)}</script><script type="math/tex; mode=display">\hat{y}^{(i)} = S(z^{(i)})</script><script type="math/tex; mode=display">z^{(i)} = W^T x^{(i)}</script><p>即只考虑实际分类对应的概率值</p><script type="math/tex; mode=display">L (y^{(i)}, \hat{y}^{(i)}) = - log \hat{y}^{(i)}_{y^{(i)}}</script><blockquote><p>由于 $S(z^{(i)})_{t^{(i)}}$与$z^{(i)}$向量各个元素都有关，由链式求导法则</p><script type="math/tex; mode=display">\frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } (\sum_{k=1}^K  \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}  \frac{∂z^{(i)}_k}{∂w_{pq}})</script><p>$1.$ 考察 $\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}$</p><script type="math/tex; mode=display">  \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} = ​      \begin{cases}​          \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_k) & k=y^{(i)} \\​          - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_k & k \neq y^{(i)} ​      \end{cases}</script><p>$2.$ 考察 $\frac{∂z^{(i)}_k}{∂w_{pq}}$</p><script type="math/tex; mode=display">  \frac{∂z^{(i)}_k}{∂w_{pq}} =       \begin{cases}            \frac{∂z^{(i)}_k}{∂w_{pq}} = x^{(i)}_p & k=q\\            \frac{∂z^{(i)}_k}{∂w_{pq}} = 0 & k \neq q        \end{cases}</script></blockquote><p>综上所述</p><script type="math/tex; mode=display">\frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} }   \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}  \frac{∂z^{(i)}_q}{∂w_{pq}}</script><p>其中</p><script type="math/tex; mode=display">\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}= \begin{cases}        \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_q) & q = y^{(i)}\\        - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_q & q \neq y^{(i)}    \end{cases}</script><script type="math/tex; mode=display">\frac{∂z^{(i)}_q}{∂w_{pq}} = x^{(i)}_p</script><p>故对于单个样本$(X^{(i)}, y^{(i)})$，当样本标签采用$OneHot$编码时</p><script type="math/tex; mode=display">\frac{∂L^{(i)}}{∂w_{pq}}  = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} }   \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q}  x^{(i)}_p= \begin{cases}    (\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\    \hat{y}^{(i)}_qx^{(i)}_p & q \neq y^{(i)}\end{cases}</script><blockquote><p>注： 这里可以约分去掉$\hat{y}^{(i)}_{y^{(i)}}$</p></blockquote><script type="math/tex; mode=display">\frac{∂L^{(i)}}{∂w_{pq}} = ( \hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_p</script><p>更一般的，写成矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">∇_W L = X^T(\hat{Y} - Y)</script><blockquote><p><strong>用线性模型解决分类和回归问题时，形式竟如此统一!</strong></p></blockquote><p>至此为止，梯度推导结束，利用梯度下降法迭代求解参数矩阵$W$即可。</p><script type="math/tex; mode=display">W := W - \alpha ∇_W L</script><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Basic-Machine-Learning-Algorithm/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex3-2-softmax_regression" target="_blank" rel="noopener">@GitHub: Code of Softmax Regression</a></p><h2 id="Softmax-1"><a href="#Softmax-1" class="headerlink" title="Softmax"></a>Softmax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="string">""" 数值计算稳定版本的softmax函数</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; X: shape(batch_size, n_labels)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X_max = np.max(X, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))  <span class="comment"># 每行的最大值</span></span><br><span class="line">    X = X - X_max                        <span class="comment"># 每行减去最大值</span></span><br><span class="line">    X = np.exp(X)</span><br><span class="line">    <span class="keyword">return</span> X / np.sum(X, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossEnt</span><span class="params">(self, y_label_true, y_prob_pred)</span>:</span></span><br><span class="line">    <span class="string">""" 计算交叉熵损失函数</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; y_label_true: 真实标签 shape(batch_size,)</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; y_prob_pred: 预测输出 shape(batch_size, n_labels)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mask = self.encoder.transform(y_label_true.reshape(<span class="number">-1</span>, <span class="number">1</span>)).toarray()  <span class="comment"># shape(batch_size, n_labels)</span></span><br><span class="line">    y_prob_masked = np.sum(mask * y_prob_pred, axis=<span class="number">1</span>)          <span class="comment"># 每行真实标签对应的预测输出值</span></span><br><span class="line">    y_prob_masked[y_prob_masked==<span class="number">0.</span>] = <span class="number">1.</span></span><br><span class="line">    y_loss = np.log(y_prob_masked)</span><br><span class="line">    loss = - np.mean(y_loss)                                    <span class="comment"># 求各样本损失的均值</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h2 id="gradient"><a href="#gradient" class="headerlink" title="gradient"></a>gradient</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span><span class="params">(self, X_train, y_train, y_prob_pred)</span>:</span></span><br><span class="line">    <span class="string">""" 计算梯度 \frac &#123;∂L&#125; &#123;∂W_&#123;pq&#125;&#125;</span></span><br><span class="line"><span class="string">    @param X_train: 训练集特征</span></span><br><span class="line"><span class="string">    @param y_train: 训练集标签</span></span><br><span class="line"><span class="string">    @param y_prob_pred:  训练集预测概率输出</span></span><br><span class="line"><span class="string">    @param y_label_pred: 训练集预测标签输出</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    y_train = self.encoder.transform(y_train)</span><br><span class="line">    dW = X_train.T.dot(y_prob_pred - y_train)</span><br><span class="line">    <span class="keyword">return</span> dW</span><br></pre></td></tr></table></figure><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><p>省略可视化和验证部分的代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, X_valid, y_train, y_valid, min_acc=<span class="number">0.95</span>, max_epoch=<span class="number">20</span>, batch_size=<span class="number">20</span>)</span>:</span></span><br><span class="line">    <span class="string">""" 训练</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 添加首1列，输入到偏置w0</span></span><br><span class="line">    X_train = np.c_[np.ones(shape=(X_train.shape[<span class="number">0</span>],)), X_train]</span><br><span class="line">    X_valid = np.c_[np.ones(shape=(X_valid.shape[<span class="number">0</span>],)), X_valid]</span><br><span class="line">    X_train = self.scaler.fit_transform(X_train)    <span class="comment"># 尺度归一化</span></span><br><span class="line">    X_valid = self.scaler.transform(X_valid)        <span class="comment"># 尺度归一化</span></span><br><span class="line">    self.encoder.fit(y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    self.n_features = X_train.shape[<span class="number">1</span>]</span><br><span class="line">    self.n_labels = self.encoder.transform(y_train).shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    self.W = np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1.0</span>, size=(self.n_features, self.n_labels))</span><br><span class="line">    n_batch = X_train.shape[<span class="number">0</span>] // batch_size</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 可视化相关</span></span><br><span class="line">    plt.ion()</span><br><span class="line">    plt.figure(<span class="string">'loss'</span>); plt.figure(<span class="string">'accuracy'</span>)</span><br><span class="line">    loss_train_epoch = []; loss_valid_epoch = []</span><br><span class="line">    acc_train_epoch = [];  acc_valid_epoch = []</span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> range(max_epoch):</span><br><span class="line">        <span class="keyword">for</span> i_batch <span class="keyword">in</span> range(n_batch):              <span class="comment"># 批处理梯度下降</span></span><br><span class="line">            n1, n2 = i_batch * batch_size, (i_batch + <span class="number">1</span>) * batch_size</span><br><span class="line">            X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2]</span><br><span class="line">            <span class="comment"># 预测</span></span><br><span class="line">            y_prob_train = self.predict(X_train_batch, preprocessed=<span class="keyword">True</span>)</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss_train_batch = self.crossEnt(y_train_batch, y_prob_train)</span><br><span class="line">            <span class="comment"># 计算准确率</span></span><br><span class="line">            y_label_train = np.argmax(y_prob_train, axis=<span class="number">1</span>)</span><br><span class="line">            a = y_train_batch.reshape((<span class="number">-1</span>,))</span><br><span class="line">            acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((<span class="number">-1</span>,))).astype(<span class="string">'float'</span>))</span><br><span class="line">            <span class="comment"># 计算梯度 dW</span></span><br><span class="line">            dW = self.grad(X_train_batch, y_train_batch, y_prob_train)</span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            self.W -= self.lr * dW</span><br></pre></td></tr></table></figure></p><h2 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, preprocessed=False)</span>:</span></span><br><span class="line">    <span class="string">""" 对输入的样本进行预测，输出标签</span></span><br><span class="line"><span class="string">    @param &#123;ndarray&#125; X: shape(batch_size, n_features)</span></span><br><span class="line"><span class="string">    @return &#123;ndarray&#125; y_prob: probability, shape(batch_size, n_labels)</span></span><br><span class="line"><span class="string">            &#123;ndarray&#125; y_label: labels, shape(batch_size,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> preprocessed:    <span class="comment"># 训练过程中调用此函数时，不用加首1列</span></span><br><span class="line">        X = np.c_[np.ones(shape=(X.shape[<span class="number">0</span>],)), X]              <span class="comment"># 添加首1项，输入到偏置w0</span></span><br><span class="line">    X = self.scaler.transform(X)</span><br><span class="line"></span><br><span class="line">    y_prob = softmax(X.dot(self.W))                             <span class="comment"># 预测概率值 shape(batch_size, n_labels)</span></span><br><span class="line">    <span class="keyword">return</span> y_prob</span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>以下蓝线为训练集参数，红线为验证集参数，若稳定训练(如<code>batch_size = 20</code>的结果)，最终准确率在$80\%$左右。</p><blockquote><ul><li>由于<code>随机梯度下降(SGD)</code>遍历次数太多，运行较慢，没有用<code>SGD</code>方法训练，就前几个<code>epoch</code>来看，效果没有<code>batch_size = 20</code>的好；</li><li>添加隐含层形成三层结构的<code>前馈神经网络</code>，可提高准确率；</li><li>还有一点，使用<code>批处理梯度下降(n_batch = 1)</code>训练时，可以看到损失值已经趋于$0$，但准确率却很低，说明已经陷入局部最优解。</li></ul></blockquote><ul><li><p>batch size = 20</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batchsize_20.png" alt="loss_batchsize_20"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batchsize_20.png" alt="accuracy_batchsize_20"></li></ul></li><li><p>batch_size = 200</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batchsize_200.png" alt="loss_batchsize_200"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batchsize_200.png" alt="accuracy_batchsize_200"></li></ul></li><li><p>n_batch = 1</p><ul><li>损失<br><img src="/2018/10/18/Softmax-Regression/loss_batch_1.png" alt="loss_batch_1"></li><li>准确率<br><img src="/2018/10/18/Softmax-Regression/accuracy_batch_1.png" alt="accuracy_batch_1"></li></ul></li></ul><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>推公式要我老命。。。。</p><p><code>Softmax</code>回归可以视作<strong>不含隐含层的<a href="https://louishsu.xyz/2018/10/20/Feedforward-Neural-Network/" target="_blank" rel="noopener">前馈神经网络</a></strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression</title>
      <link href="/2018/10/18/Logistic-Regression/"/>
      <url>/2018/10/18/Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>先给出模型，推导过程稍后给出，逻辑回归包含<code>Sigmoid</code>函数</p><script type="math/tex; mode=display">f(z) = \frac{1}{1+e^{-z}}</script><p>其图像如下<br><img src="/2018/10/18/Logistic-Regression/Sigmoid.png" alt="`Sigmod函数`"></p><p>定义</p><script type="math/tex; mode=display">z = w^Tx</script><p>其中$x=[x_0, x_1, …, x_n]^T, x_0=1$</p><script type="math/tex; mode=display">h_w(x) = g(z) =  \frac{1}{1+e^{-z}}</script><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="由最大似然估计推导"><a href="#由最大似然估计推导" class="headerlink" title="由最大似然估计推导"></a>由最大似然估计推导</h2><p>对于二元分类问题，其取值作为随机变量，服从二项分布 $B(1, p)$，其中$p$即为预测输出概率$\hat{y}$</p><script type="math/tex; mode=display">P(y_i^{(i)}) = (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}</script><p>由极大似然估计</p><script type="math/tex; mode=display">L = \prod_{i=0}^N P(y_i^{(i)}) = \prod_{i=0}^N (\hat{y}_i^{(i)})^{y_i^{(i)}}(1-\hat{y}_i^{(i)})^{1-y_i^{(i)}}</script><p>取对数似然函数</p><script type="math/tex; mode=display">logL = \sum_{i=0}^N [y_i^{(i)} log \hat{y}_i^{(i)} + (1-y_i^{(i)}) log (1-\hat{y}_i^{(i)})]</script><p>优化目标是</p><script type="math/tex; mode=display">w = argmax_w logL</script><p>优化问题一般表述成<code>minimize</code>问题，添加负号，构成<code>Neg Log Likelihood</code>损失</p><script type="math/tex; mode=display">w = argmin_w (-logL)</script><p>一般取均值</p><script type="math/tex; mode=display">L(\hat{y}, y)=- \frac{1}{N} \sum_i [y_i^{(i)} log(\hat{y}_i^{(i)})+(1 - y_i^{(i)})log(1-\hat{y}_i^{(i)})]</script><p>其中$y_i$表示真实值，$\hat{y}_i$表示预测值</p><h2 id="从交叉熵理解"><a href="#从交叉熵理解" class="headerlink" title="从交叉熵理解"></a>从交叉熵理解</h2><p>已知交叉熵<code>cross entropy</code>定义如下</p><script type="math/tex; mode=display">CrossEnt = \sum_i p_i log \frac{1}{q_i}</script><p>而对于样本$ (X_i, y_i) $，为确定事件，故标签概率的取值为$ p_i = y_i ∈ \{0,1\}$，$ q_i即预测输出的概率值\hat{y}_i $，可得到与上面相同的推导结论</p><h2 id="从决策平面和贝叶斯决策理解"><a href="#从决策平面和贝叶斯决策理解" class="headerlink" title="从决策平面和贝叶斯决策理解"></a>从决策平面和贝叶斯决策理解</h2><blockquote><p>相关内容查看<a href="https://louishsu.xyz/2018/10/19/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2/" target="_blank" rel="noopener">分类问题的决策平面</a>和<a href="https://louishsu.xyz/2018/10/18/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/" target="_blank" rel="noopener">贝叶斯决策</a>，逻辑回归考虑的一般是等先验概率问题，故决策函数定义为</p><script type="math/tex; mode=display">if \quad P(c_i|x)>P(c_j|x) \quad then \quad x \in c_i, \quad i, j = 1, 2</script></blockquote><p>从贝叶斯决策可知，对于类别$c_1$，有</p><script type="math/tex; mode=display">P(c_1|x) = \frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}</script><p>设在各个类别下，特征$x$服从正态分布</p><script type="math/tex; mode=display">P(x|c_i) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}}exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i))</script><p>则</p><script type="math/tex; mode=display">P(c_1|x) = \frac{1}{    1 + exp(-z)}</script><script type="math/tex; mode=display">P(c_2|x) = 1 - P(c_1|x) = \frac{exp(-z)}{1+exp(-z)}</script><blockquote><p>$<br>P(c_1|x) = \frac<br>{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}<br>{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}<br>$</p><p>$<br>P(c_1|x) = \frac<br>{1}<br>{1 + \frac{exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}<br>}<br>$</p><p>假定各分类的样本方差相等，$ \Sigma_1 = \Sigma_2 = \sigma^2 I $</p><p>$ P(c_1|x) = \frac {1}{1 + exp(- [ \frac{1}{\sigma^2} (\mu_1-\mu_2)^T x - \frac{1}{2 \sigma^2} (\mu_1^T\mu_1 - \mu_2^T\mu_2) ])}<br>$</p><p>令</p><script type="math/tex; mode=display">w = \frac{1}{\sigma^2} (\mu_1 -\mu_2)</script><script type="math/tex; mode=display">b = - \frac{1}{2\sigma^2}(\mu_1^T \mu_1 - \mu_2^T \mu_2)</script><p>即可得到</p><script type="math/tex; mode=display">P(c_1|x) = \frac{1}{    1 + exp(-z)}</script><p>其中</p><script type="math/tex; mode=display">z = w^T x + b</script></blockquote><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><p>先推导<code>Sigmoid</code>函数的导数</p><script type="math/tex; mode=display">f'(z) = (1 - f(z))f(z)</script><p>值得注意的是，从$f’(z)$的图像可以看到，在$ x=0 $处$f’(z)$取极大值，且</p><script type="math/tex; mode=display">f'(z)_{max} = f'(z)|_{z=0} = 0.25</script><script type="math/tex; mode=display">\lim_{z \rightarrow \infty} f'(z) = 0</script><p>在多层神经网络反向传播更新参数时，由于梯度多次累乘，<code>Sigmoid</code>作为<a href="https://louishsu.xyz/2018/10/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" target="_blank" rel="noopener">激活函数</a>会存在“梯度消失”的问题，使得参数更新非常缓慢。</p><p><img src="/2018/10/18/Logistic-Regression/Sigmoid_gradient.png" alt="`Sigmod导函数`"></p><blockquote><p>$ f’(z) $<br>$ = (\frac{1}{1+e^{-z}})’ $<br>$ = \frac<br>​          {-(1+e^{-z})’}<br>​          {(1+e^{-z})^2} $<br>$ = \frac<br>​          {e^{-z}}<br>​          {(1+e^{-z})^2} $<br>$ = \frac<br>​          {e^{-z}}<br>​          {1+e^{-z}}<br>​    \frac<br>​          {1}<br>​          {1+e^{-z}}$<br>$ = (1 - f(z))f(z)$</p></blockquote><p>利用链式求导法则可得</p><script type="math/tex; mode=display">\begin{aligned}    \frac{∂L}{∂w_j}     = -\frac{∂}{∂w_j} \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})]  \\    = - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\frac{∂\hat{y}^{(i)}}{∂w_j}-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\frac{∂\hat{y}^{(i)}}{∂w_j}] \\    = - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})x_j-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})x_j] \\    = - \frac{1}{N} \sum_i [y^{(i)} (1-\hat{y}^{(i)})x_j-(1-y^{(i)}) y^{(i)} x_j] \\    =  \frac{1}{N} \sum_i (\hat{y}^{(i)} - y^{(i)})x_j     \end{aligned}</script><p>写作矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量)</p><script type="math/tex; mode=display">∇_w L = X^T (\hat{Y} - Y)</script><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>和线性回归一样，采用梯度下降法求解</p><script type="math/tex; mode=display">w := w - \alpha ∇_w L</script><h1 id="处理多分类问题"><a href="#处理多分类问题" class="headerlink" title="处理多分类问题"></a>处理多分类问题</h1><p>假设有$K$个类别，则依次以类别$c_i$为正样本训练模型，一共训练$K$个。测试样本在每个模型上计算，最终将概率最大的作为分类结果。</p><blockquote><p>这样划分数据集，会使训练集正负样本数目严重不对称，特别是类别很多的情况，对结果会产生影响。可推广至<a href="https://louishsu.xyz/2018/10/18/softmax%E5%9B%9E%E5%BD%92/" target="_blank" rel="noopener">softmax回归</a>解决这个问题。</p></blockquote><h1 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h1><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/blob/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex2-logisticregression/LogReg.py" target="_blank" rel="noopener">@Github: Code for Logistic Regression</a></p><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFunctionDerivative</span><span class="params">(self, X, theta, y_true)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算损失函数对参数theta的梯度</span></span><br><span class="line"><span class="string">    对theta[j]的梯度为：(y_pred - y_true)*x[j]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    err = self.predict_prob(X, theta) - y_true</span><br><span class="line">    <span class="keyword">return</span> X.T.dot(err)/y_true.shape[<span class="number">0</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFunction</span><span class="params">(self, y_pred_prob, y_true)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    未使用</span></span><br><span class="line"><span class="string">    计算损失值: Cross-Entropy</span></span><br><span class="line"><span class="string">    y_pred_prob, y_true: NumPy array, shape=(n,)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    tmp = y_true*np.log(y_pred_prob) + (<span class="number">1</span> - y_true)*np.log(<span class="number">1</span> - y_pred_prob)</span><br><span class="line">    <span class="keyword">return</span> np.mean(-tmp)</span><br></pre></td></tr></table></figure><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradDescent</span><span class="params">(self, min_acc, learning_rate=<span class="number">0.01</span>, max_iter=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    acc = <span class="number">0</span>; n_iter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n_iter <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(self.n_batch):</span><br><span class="line">            X_batch = self.X[n*self.batch_size:(n+<span class="number">1</span>)*self.batch_size]</span><br><span class="line">            t_batch = self.t[n*self.batch_size:(n+<span class="number">1</span>)*self.batch_size]</span><br><span class="line">            grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch)</span><br><span class="line">            self.theta -= learning_rate * grad <span class="comment"># 梯度下降</span></span><br><span class="line">            acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t)</span><br><span class="line">            <span class="keyword">if</span> acc &gt; min_acc:</span><br><span class="line">                print(<span class="string">'第%d次迭代, 第%d批数据'</span> % (n_iter, n))</span><br><span class="line">                print(<span class="string">"当前总体样本准确率为: "</span>, acc)</span><br><span class="line">                print(<span class="string">"当前参数值为: "</span>, self.theta)</span><br><span class="line">                <span class="keyword">return</span> self.theta</span><br><span class="line">        <span class="keyword">if</span> n_iter%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'第%d次迭代'</span> % n_iter)</span><br><span class="line">            print(<span class="string">'准确率： '</span>, acc)</span><br><span class="line">    print(<span class="string">"超过迭代次数"</span>)</span><br><span class="line">    print(<span class="string">"当前总体样本准确率为: "</span>, acc)</span><br><span class="line">    print(<span class="string">"当前参数值为: "</span>, self.theta)</span><br><span class="line">    <span class="keyword">return</span> self.theta</span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/2018/10/18/Logistic-Regression/logistic_regression_result.png" alt="实验结果"></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2018/10/18/Linear-Regression/"/>
      <url>/2018/10/18/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>线性回归可以说是机器学习最基础的算法</p><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><script type="math/tex; mode=display">\hat{y}^{(i)} = w^Tx^{(i)}</script><p>其中$x^{(i)}_{(n + 1) \times 1}$为第$i$个$n$维向量，$i = 1, \cdots, m$，$w_{(n + 1) \times 1}$为权值向量</p><script type="math/tex; mode=display">\begin{cases}    x^{(i)}=\begin{bmatrix} x_0^{(i)} & x_1^{(i)} & ... & x_n^{(i)} \end{bmatrix}^T, x_0^{(i)}=1 \\     w = \begin{bmatrix} w_0 & w_1 & \cdots & w_n \end{bmatrix}^T\end{cases}</script><p>$x_0^{(i)}=1$项对应偏置项$b$，即$b=w_0$。</p><blockquote><p>注：若需要获得非线性特征，可构造高次特征如$x^{(i)}_{n+1} = (x^{(i)}_{j})^a, a \neq 0, 1$。</p></blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="定义误差"><a href="#定义误差" class="headerlink" title="定义误差"></a>定义误差</h2><script type="math/tex; mode=display">e^{(i)} = \hat{y}^{(i)} - y^{(i)}</script><p>其中$y^{(i)}$表示真实值</p><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>单个样本的误差定义为</p><script type="math/tex; mode=display">L_{single}(\hat{y}^{(i)}, y^{(i)})=\frac{1}{2}||e^{(i)}||_2^2=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2</script><p>所有样本的误差(<strong>经验误差/风险</strong>)定义为</p><script type="math/tex; mode=display">L_{emp}(y, t)=\frac{1}{2N}\sum_i (\hat{y}^{(i)}-y^{(i)})^2</script><p>也可以定义为误差的和而不是均值，对结果无影响，可视作学习率$α$除去一个常数</p><h1 id="梯度推导"><a href="#梯度推导" class="headerlink" title="梯度推导"></a>梯度推导</h1><script type="math/tex; mode=display">\begin{aligned}    \frac{∂L}{∂w_j} = \frac{∂}{∂w_j}\frac{1}{2N}\sum_i(\hat{y}^{(i)}-y^{(i)})^2 \\    = \frac{1}{2N} \sum_i \frac{∂}{∂w_j} (\hat{y}^{(i)}-y^{(i)})^2 \\    = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) \frac{∂t^{(i)}}{∂w_j} \\    =  \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)}\end{aligned}</script><p>或者使用矩阵推导，记$m$个$n$维样本组成的样本矩阵为$X_{m \times (n+1)} = \begin{bmatrix} x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \end{bmatrix}^T$，$x^{(i)}$为第$i$个样本的特征(行向量)，$Y_{m \times 1}$为$m$个样本groundtruth组成的向量，那么有</p><script type="math/tex; mode=display">L = \frac{1}{2}(Xw-Y)^T(Xw-Y)</script><script type="math/tex; mode=display">∇_w L = X^T(\hat{Y}-Y)</script><blockquote><script type="math/tex; mode=display">\begin{aligned}    ∇_w L = \frac{1}{2} ∇_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY) \\    = \frac{1}{2} (2X^TXw - X^TY - X^TY)     = X^T(Xw-Y)\end{aligned}</script></blockquote><p>在梯度为$\vec{0}$的点，即$∇_w L = \vec{0}$时对应最优解</p><script type="math/tex; mode=display">X^T(Xw-Y) = 0</script><blockquote><p>令$X^T(Xw-Y) = 0$，有</p><script type="math/tex; mode=display">\begin{aligned}     X^TXw = X^TY \\ w^*=(X^TX+\lambda I)^{-1}X^TY\end{aligned}</script></blockquote><p>其中$X^+=(X^TX+\lambda I)^{-1}X^T$，表示矩阵$X_{m×n}$的伪逆</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>采用梯度下降法求解</p><script type="math/tex; mode=display">w := w - \alpha ∇_w L</script><p>其中$w$表示参数向量</p><blockquote><p>进一步思考：为什么使用梯度下降可以求取最优解呢？</p><script type="math/tex; mode=display">∇_w^2 L = ∇_w X^T(Xw-Y) = X^TX</script><p>而对于矩阵 $ X^TX $</p><script type="math/tex; mode=display">u^T(X^TX)u = (Xu)^T(Xu) \geq 0</script><p>即损失函数的<code>Hessian</code>矩阵$∇_w^2 L$为正定矩阵，$L$为凸函数，存在全局最优解</p></blockquote><h1 id="从投影的角度理解线性回归"><a href="#从投影的角度理解线性回归" class="headerlink" title="从投影的角度理解线性回归"></a>从投影的角度理解线性回归</h1><p><img src="/2018/10/18/Linear-Regression/projection_linreg2.png" alt="投影理解"></p><p><img src="/2018/10/18/Linear-Regression/projection_linreg3.png" alt="用投影推导最优解"></p><h1 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h1><p>为克服过拟合问题，可加入正则化项$||w||_2^2$，此时损失函数(<strong>结构误差/风险</strong>)定义为</p><script type="math/tex; mode=display">L(\hat{y}, y)=\frac{1}{2N} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_2^2</script><p>或者</p><script type="math/tex; mode=display">L(\hat{y}, y)=\frac{1}{2N} \sum_i (\hat{y}^{(i)}-y^{(i)})^2 +  \frac{\lambda}{2N}\sum_j w_j^2</script><p>其中$i = 1, …, N_{sample}; j = 1, …, N_{feature},j&gt;0 $</p><p>此时梯度为</p><script type="math/tex; mode=display">\frac{∂L}{∂w_j} = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \frac{\lambda}{N}w_j</script><p>其中$j = 1, …, N_{feature},j&gt;0 $</p><h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><p>目标函数定义为</p><script type="math/tex; mode=display">L(y, t)=\frac{1}{2N}\sum_i w^{(i)} (\hat{y}^{(i)}-y^{(i)})^2</script><p>其中</p><script type="math/tex; mode=display">w^{(i)} = e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}</script><p>$x$表示输入的预测样本，$x^{(i)}$表示训练样本</p><p><div style="align: center"><img src="/2018/10/18/Linear-Regression/w_i_x_i.png"></div><br>离很近的样本，权值接近于1，而对于离很远的样本，此时权值接近于0，这样就是在局部构成线性回归，它依赖的也只是周边的点。</p><p>对于线性回归算法，一旦拟合出适合训练数据的参数$w$，保存这些参数$w$，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。而对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数$w$），没有固定的参数$w$，所以是非参数算法。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p><a href="https://github.com/isLouisHsu/Python-Examples-for-ML/tree/master/Andrew%20Ng%2C%20Stanford%20University/mlclass-ex5-regularizedllinearregression" target="_blank" rel="noopener">@Github: Code for Linear Regression</a></p><h2 id="training-step"><a href="#training-step" class="headerlink" title="training step"></a>training step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, learning_rate=<span class="number">0.01</span>, max_iter=<span class="number">5000</span>, min_loss=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="comment"># --------------- 数据预处理部分 ---------------</span></span><br><span class="line">    <span class="comment"># 加入全1列</span></span><br><span class="line">    X = np.c_[np.ones(shape=(X.shape[<span class="number">0</span>])), X]</span><br><span class="line">    <span class="comment"># 构造高次特征</span></span><br><span class="line">    <span class="keyword">if</span> self.n_ploy &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, self.n_ploy + <span class="number">1</span>):</span><br><span class="line">            X = np.c_[X, X[:, <span class="number">1</span>]**i]</span><br><span class="line">    <span class="comment"># ---------------- 参数迭代部分 ----------------</span></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    self.theta = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, size=(X.shape[<span class="number">1</span>],))</span><br><span class="line">    <span class="comment"># 数据批次</span></span><br><span class="line">    n_batch = X.shape[<span class="number">0</span>] <span class="keyword">if</span> self.n_batch==<span class="number">-1</span> <span class="keyword">else</span> self.n_batch</span><br><span class="line">    batch_size = X.shape[<span class="number">0</span>] // n_batch</span><br><span class="line">    <span class="comment"># 停止条件</span></span><br><span class="line">    n_iter = <span class="number">0</span>; loss = float(<span class="string">'inf'</span>)</span><br><span class="line">    <span class="comment"># 开始迭代</span></span><br><span class="line">    <span class="keyword">for</span> n_iter <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            n1, n2 = n*batch_size, (n+<span class="number">1</span>)*batch_size</span><br><span class="line">            X_batch = X[n1: n2]; y_batch = y[n1: n2]</span><br><span class="line">            </span><br><span class="line">            grad = self.lossFunctionDerivative(X_batch, y_batch)</span><br><span class="line">            self.theta -= learning_rate * grad</span><br><span class="line">            </span><br><span class="line">            loss = self.score(y_batch, self.predict(X_batch))</span><br><span class="line">            <span class="keyword">if</span> loss &lt; min_loss:</span><br><span class="line">                print(<span class="string">'第%d次迭代, 第%d批数据'</span> % (n_iter, n))</span><br><span class="line">                print(<span class="string">"当前总体样本损失为: "</span>, loss)</span><br><span class="line">                <span class="keyword">return</span> self.theta</span><br><span class="line">        <span class="keyword">if</span> n_iter%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'第%d次迭代'</span> % n_iter)</span><br><span class="line">            print(<span class="string">"当前总体样本损失为: "</span>, loss)</span><br><span class="line">    print(<span class="string">"超过迭代次数"</span>)</span><br><span class="line">    print(<span class="string">"当前总体样本损失为: "</span>, loss)</span><br><span class="line">    <span class="keyword">return</span> self.theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFunctionDerivative</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    y_pred = self.predict(X)</span><br><span class="line">    <span class="comment"># theta = self.theta;     # ！注意：theta = self.theta 不仅仅是赋值，类似引用，修改theta会影响self.theta</span></span><br><span class="line">    theta = self.theta.copy()</span><br><span class="line">    theta[<span class="number">0</span>] = <span class="number">0</span>            <span class="comment"># θ0不需要正则化</span></span><br><span class="line">    <span class="keyword">return</span> (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h2 id="predict-step"><a href="#predict-step" class="headerlink" title="predict step"></a>predict step</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, preprocessed=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> preprocessed:</span><br><span class="line">        <span class="comment"># 加入全1列</span></span><br><span class="line">        X = np.c_[np.ones(shape=(X.shape[<span class="number">0</span>])), X]</span><br><span class="line">        <span class="comment"># 构造高次特征</span></span><br><span class="line">        <span class="keyword">if</span> self.n_ploy &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, self.n_ploy + <span class="number">1</span>):</span><br><span class="line">                X = np.c_[X, X[:, <span class="number">1</span>]**i]</span><br><span class="line">    <span class="keyword">return</span> X.dot(self.theta)</span><br></pre></td></tr></table></figure><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><ul><li><p>无正则化<br>  <img src="/2018/10/18/Linear-Regression/result_linreg_noreg.png" alt="无正则化的线性回归结果"></p></li><li><p>正则化<br>  <img src="/2018/10/18/Linear-Regression/result_linreg_reg.png" alt="正则化的线性回归结果"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
