<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Graph Neural Network]]></title>
    <url>%2F2020%2F06%2F13%2FGraph-Neural-Network%2F</url>
    <content type="text"><![CDATA[目录《深入浅出图神经网络》阅读笔记，这本书非常OK，看着没多少字，都是干货，比什么《xxx从入门到精通》类的书好太多。 目录 图卷积神经网络(GCN) 图信号 拉普拉斯算子 图傅里叶变换 GFT与IGFT 总变差(TV) 图信号的频域描述 图滤波器 定义 拉普拉斯矩阵多项式拓展形式 空域角度 频域角度 图卷积的定义 滤波器的参数化 频域图卷积模型 频率响应函数的参数化 拉普拉斯矩阵多项式拓展形式系数的参数化 空域图卷积模型 固定滤波器的参数化 GCN的性质 GCN与CNN联系 端到端学习 低通滤波器 过平滑问题 频域视角 空域视角 解决方法 跳跃连接 重分配权重 GCN变体与框架 GraphSAGE 邻居采样 邻居聚合 算法流程与实现 GAT R-GCN 图分类 基于一次性全局池化的图分类 基于层次化池化的图分类 基于图坍缩的池化机制 DIFFPOOL EigenPooling 基于TopK的池化机制 基于边收缩的池化机制 图表示学习 基于重构损失的GNN 基于对比损失的GNN 邻居上下文 子图上下文 全图上下文 Reference 图卷积神经网络(GCN)图信号给定图$G = (V, E)$，$V$表示图中节点集合，$E$表示边集合。假设有$n$个节点，第$i$个节点的信号强度为$x_i$，这个图的图信号可以表示为向量的形式 x = \left[ x_1, \cdots, x_i, \cdots, x_n \right]^T \tag{1} 注意研究图信号性质时，除了考虑图信号的强度外，还需考虑图的拓扑结构。 拉普拉斯算子拉普拉斯矩阵(Laplacian Matrix)是用于研究图的结构性质的核心对象，定义为 L = D - A \tag{2.1}其一种正则化的形式(symmetric normalized laplacian)为 L_{sym} = D^{-1/2} L D^{1/2} \tag{2.2}其中$A$是图的邻接矩阵，$D$是邻接矩阵对应的度矩阵 d_{ii} = \sum_{j=1}^n a_{ij} \tag{3.1}因此 A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \\ \end{bmatrix}, D = \begin{bmatrix} \sum_j a_{1j} & 0 & \cdots & 0 \\ 0 & \sum_j a_{2j} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sum_j a_{nj} \end{bmatrix} \tag{3.2}拉普拉斯算子被用作描述中心节点与邻居节点之间的差异，对于图信号$x$，有 L x = (D - A) x = \begin{bmatrix} \cdots & x_i' & \cdots \end{bmatrix}^T \tag{4.1}其中 \begin{aligned} x_i' & = (- a_{ii} + \sum_j a_{ij}) x_i - \sum_{j \neq i} a_{ij} x_j \\ & = x_i \sum_j a_{ij} - \sum_j a_{ij} x_j \\ & = \sum_j a_{ij} (x_i - x_j) \\ \end{aligned} \tag{4.2}可以看到经$L$变换实际上是根据节点间信号差异$x_i - x_j$更新$x_i’$。 注意 $x^T L x \geq 0$，因此$L$是半正定矩阵，所有特征值大于等于$0$； 由$(4.1)$，$L \cdot \bm{1} = 0 = 0 \cdot \bm{1}$，所以$L$有最小的特征值$0$，且对应特征向量为$\bm{1}$； $L_{sym}$的特征值存在上限，即$\lambda_{sym} \leq 2$。 图傅里叶变换图傅里叶变换可将图信号由空域(spatial domain)视角转换到频域(frequency domain)视角，便于图信号处理理论体系的建立。 GFT与IGFT假设图$G$的拉普拉斯矩阵为$L \in R^{n \times n}$，注意到$L$是一个实对称矩阵，可以被正交对角化为 L = V \Lambda V^T = \begin{bmatrix} v_1 & \cdots & v_n \end{bmatrix} \begin{bmatrix} \lambda_1 & & \\ & \ddots & \\ & & \lambda_n \end{bmatrix} \begin{bmatrix} v_1^T \\ \vdots \\ v_n^T \end{bmatrix} \tag{5}其中$0 \leq \lambda_1 \leq \cdots \leq \lambda_n$；$V$是正交矩阵，即满足$V V^T = I$，$v_i = \begin{bmatrix} v_{i1} &amp; \cdots &amp; v_{in} \end{bmatrix}^T$，$v$彼此正交，且均为单位向量。 图傅里叶变换(Graph Fourier Transform, GFT)：对于任意一个在图$G$上的信号$x$，其图傅里叶变换为 \tilde{x} = V^T x, \tilde{x} \in R^n \tag{6.1}逆图傅里叶变换(Inverse Graph Fourier Transform, IGFT)定义为 x = V \tilde{x}, x \in R^{n} \tag{6.2}那么从线性变换的角度来看，特征向量$v_k$即傅里叶基(完备的)，$\tilde{x}_k$是傅里叶系数，即图信号在$v_k$上的投影，衡量了图信号与傅里叶基之间的相似度。 总变差(TV)一个重要的二次型$TV(x)$如下，称图信号的总变差(Total Variation)，刻画图信号整体的平滑度 TV(x) = x^T L x = \sum_i \sum_j a_{ij} (x_i - x_j)^2 \tag{7}可以进行如下变换 \begin{aligned} & \begin{cases} TV(x) & = & x^T L x \\ x & = & V \tilde{x} \\ L & = & V \Lambda V^T \end{cases} \\ \Rightarrow TV(x) & = (V \tilde{x})^T (V \Lambda V^T) (V \tilde{x}) \\ & = \tilde{x}^T \Lambda \tilde{x} = \sum_{i=1}^n \tilde{x}_i^2 \cdot \lambda_i \end{aligned} \tag{8}因此，图的总变差与图的特征值$\lambda_i$之间有非常直接的线性对应关系(权重为$\tilde{x}_i^2$) 图信号的频域描述 有一个问题需要思考：一个图中什么样的图信号具有最小的总变差？ 限定图信号$x$为单位向量，经分析$\lambda_1 = 0$，那么当$x$与$v_1$完全重合时，$x$在$v_{i\neq1}$上投影均为$0$，即 \tilde{x}_i = x \cdot v_i = \begin{cases} 1 & i = 1 \\ 0 & i \neq 1 \end{cases} \Rightarrow \min TV(x) = \lambda_1 \tag{9.1}实际上可以证明 TV(x_k) = \lambda_k \tag{9.2}如果要选择一组彼此正交的图信号，使各自总变差依次取得最小值，那么这组图信号即傅里叶基$v_1, \cdots, v_n$ \lambda_k = \min_{x: ||x||=1, x \bot x_{x \neq k}} x^T L x \tag{9.3}特征值排列后，对图信号的平滑度做出了梯度刻画，因此可以将特征值等价成频率。特征值越小即频率越低，对应傅里叶基上的图信号总变差越小，变换得越缓慢，相近节点上信号值趋于一致；反之亦然。相应的，傅里叶系数即图信号在对应频率分量上的幅值，反应其强度，在低频分量上强度越大，则信号平滑度越高。 例：给定图的邻接矩阵如下 A = \begin{bmatrix} 0 & 1 & 1 & 0 & 0 \\ 1 & 0 & 1 & 1 & 0 \\ 1 & 1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 & 1 \\ 0 & 0 & 0 & 1 & 0 \end{bmatrix} \Rightarrow L = \begin{bmatrix} 2 & -1 & -1 & 0 & 0 \\ -1 & 3 & -1 & -1 & 0 \\ -1 & -1 & 3 & -1 & 0 \\ 0 & -1 & -1 & 3 & -1 \\ 0 & 0 & 0 & -1 & 1 \end{bmatrix}$L$的特征分解为123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt;&gt;&gt;&gt; A = np.array([... [0, 1, 1, 0, 0],... [1, 0, 1, 1, 0],... [1, 1, 0, 1, 0],... [0, 1, 1, 0, 1],... [0, 0, 0, 1, 0]... ])&gt;&gt;&gt; D = np.diag(A.sum(axis=1))&gt;&gt;&gt; Darray([[2, 0, 0, 0, 0], [0, 3, 0, 0, 0], [0, 0, 3, 0, 0], [0, 0, 0, 3, 0], [0, 0, 0, 0, 1]])&gt;&gt;&gt; L = D - A&gt;&gt;&gt; np.linalg.eig(L)(array([0. , 0.82991351, 2.68889218, 4.4811943 , 4. ]), array([[ 4.47213595e-01, 4.37531395e-01, -7.03081478e-01, -3.37998097e-01, -1.24491566e-16], [ 4.47213595e-01, 2.55974786e-01, 2.42173667e-01, 4.19319477e-01, 7.07106781e-01], [ 4.47213595e-01, 2.55974786e-01, 2.42173667e-01, 4.19319477e-01, -7.07106781e-01], [ 4.47213595e-01, -1.38018756e-01, 5.36249932e-01, -7.02415001e-01, -8.17563909e-16], [ 4.47213595e-01, -8.11462211e-01, -3.17515788e-01, 2.01774144e-01, 3.46536171e-16]])) 可以看到$v_1 = [0.4472136, 0.4472136, 0.4472136, 0.4472136, 0.4472136]^T$，变化最缓慢，全部相同，而$v_5$变化最剧烈。 我们把图信号所有傅里叶系数合在一起称作其频谱(spectrum)，给定频谱就可以通过逆图傅里叶变换完整得到空域中的图信号；同时，频谱也完整描述了图信号的频域特性。 继续以上面给出的图为例，输入随机图信号，其在频域中表示为12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt;&gt; ...&gt;&gt;&gt;&gt;&gt;&gt; eigval, eigvec = np.linalg.eig(L) # 拉氏矩阵分解得到傅里叶基&gt;&gt;&gt; x = np.random.rand(5); x # 随机生成图信号array([0.24427103, 0.69121671, 0.26806286, 0.62879822, 0.53215482])&gt;&gt;&gt; x.T.dot(L).dot(x) # 总变差0.5227516746889123&gt;&gt;&gt; frequency = eigenval # 特征值等价为频率&gt;&gt;&gt; amplitude = eigvec.T.dot(x); amplitude # 图信号投影到傅里叶基上，计算幅值array([ 1.05743818, -0.16618185, 0.22879526, -0.01462076, 0.29921495])&gt;&gt;&gt; &gt;&gt;&gt; from matplotlib import pyplot as plt&gt;&gt;&gt; plt.figure()&lt;Figure size 640x480 with 0 Axes&gt;&gt;&gt;&gt;&gt;&gt;&gt; # 空域&gt;&gt;&gt; plt.subplot(211); plt.grid()&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001BB66698FD0&gt;&gt;&gt;&gt; plt.bar(np.arange(1, 6), x, width=0.2)&lt;BarContainer object of 5 artists&gt;&gt;&gt;&gt; plt.title("spatial domain")Text(0.5, 1.0, 'spatial domain')&gt;&gt;&gt; plt.xlabel("index")Text(0.5, 0, 'index')&gt;&gt;&gt; plt.ylabel("x_i")Text(0, 0.5, 'x_i')&gt;&gt;&gt;&gt;&gt;&gt; # 频域&gt;&gt;&gt; plt.subplot(212); plt.grid()&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001BB666C3048&gt;&gt;&gt;&gt; plt.bar(frequency, amplitude, width=0.2)&lt;BarContainer object of 5 artists&gt;&gt;&gt;&gt; plt.title("frequency domain")Text(0.5, 1.0, 'frequency domain')&gt;&gt;&gt; plt.xlabel("frequency")Text(0.5, 0, 'frequency')&gt;&gt;&gt; plt.ylabel("amplitude")Text(0, 0.5, 'amplitude')&gt;&gt;&gt;&gt;&gt;&gt; plt.show() 图滤波器定义定义图滤波器(Graph Filter)为对给定图信号的频谱中各个频率分量强度进行增强或衰减的操作，假设输入信号频谱为$\tilde{x}$，经滤波器$H \in R^{n \times n}$调整后，得到输出信号频谱为$\tilde{y}$ y = H x \tag{10.1}那么各频率分量上调整幅值，有 \tilde{y}_k = h(\lambda_k) \times \tilde{x}_k \tag{10.2}也即 \tilde{y} = \begin{bmatrix} h(\lambda_1) & & \\ & \ddots & \\ & & h(\lambda_n) \end{bmatrix} \tilde{x} \tag{10.3}那么对应的输出图信号可以通过傅里叶基重构 y = \sum_{k=1}^n \tilde{y}_k \cdot v_k = V \cdot \tilde{y} \tag{10.4}$(10.3)$带入$(10.4)$后有 y = V \cdot \Lambda_h \cdot \tilde{x} = \underbrace{V \Lambda_h V^T}_H \cdot x \tag{10}可以看到$H$只在对角与边坐标上时才有可能取非零值，即 H_{ij} = 0, if \quad i \neq j \quad or \quad e_{ij} \notin E从算子角度看，$Hx$描述了一种作用在每个节点一阶子图上的变换操作。一般来说，满足以上性质的变换矩阵称为$G$的图位移算子(Graph Shift Operator)，拉普拉斯矩阵与邻接矩阵都是典型的图位移算子。 图滤波器具有如下性质： 线性：$H(x + y) = Hx + Hy$； 顺序无关：$H_1 (H_2 x) = H_2 (H_1 x)$； 可逆：若$h(\lambda) \neq 0$，那么滤波操作可逆。 H = V \Lambda_h V^T \tag{11.1}称$\Lambda_h$是图滤波器$H$的频率响应矩阵，其对角元素$h(\lambda)$是$H$的频率响应函数，不同的频率响应函数可以实现不同的滤波效果，如低通、高通、带通等。 拉普拉斯矩阵多项式拓展形式理论上任意性质的图滤波器都可以被实现，即任意类型函数曲线的频率响应函数。通过泰勒展开用多项式进行逼近，有拉普拉斯矩阵多项式拓展形式的图滤波器如下 H = \sum_{k=0}^K h_k L^k \tag{11.2}其中$K$是图滤波器$H$的阶数，$h_k$是多项式系数。 为什么是上述形式，请参考频域角度。 空域角度对于$y = Hx = (\sum_{k=0}^K h_k L^k) \cdot x$，记 x^{(k)} = L^k \cdot x \tag{12.1}由于$L$是图位移算子，那么$x^{(k-1)}$到$x^{(k)}$的变换只需所有节点的一阶邻居参与计算，所以$x^{(k)}$需要所有节点的$k$阶邻居参与，这种性质称为图滤波器的局部性。那么输出图信号是$(K + 1)$组图信号的线性加权 y = \sum_{k=0}^K h_k x^{(k)} \tag{12.2} 经$k$次拉普拉斯矩阵线性变换，聚合节点$i$的$k$阶邻居得到图信号$x^{(k)}_i$； $K$次局部性不同的聚合线性组合(权重$h_k$)得到本次滤波的输出。 从空域角度来看，滤波操作的性质为 局部性：每个节点的输出信号仅需考虑其$K$阶子图； 通过$K$步迭代式的矩阵向量乘法完成滤波操作。 频域角度由于$L = V \Lambda V^T$，那么 \begin{aligned} H & = \sum_{k=0}^K h_k L^k \\ & = \sum_{k=0}^K h_k (V \Lambda V^T)^k \\ & = \sum_{k=0}^K h_k (V \Lambda V^T) \cdots (V \Lambda V^T) \\ & = V \underbrace{(\sum_{k=0}^K h_k \Lambda^k)}_{\Lambda_h} V^T \end{aligned} \tag{13.1}其中 \Lambda_h = \sum_{k=0}^K h_k \Lambda^k = \begin{bmatrix} \underbrace{\sum_{k=0}^K h_k \lambda_1^k}_{h(\lambda_1)} & & \\ & \ddots & \\ & & \underbrace{\sum_{k=0}^K h_k \lambda_n^k }_{h(\lambda_n)} \end{bmatrix} \tag{13.2}那么$H$的频率响应函数$h(\lambda) = \sum_{k=0}^K h_k \lambda^k$是$\lambda$的$K$次多项式，如果$K$足够大，可以逼近任意一个关于$\lambda$的函数。 在滤波操作时 y = Hx = V \Lambda_h V^T \cdot x \tag{13.3}可以看到变换过程由以下三步组成 通过图傅里叶变换，求取$x$的频谱$\tilde{x} = V^T x$； 用$\Lambda_h$对频率分量的幅值进行调节，得到调整后的频谱$\tilde{y} = \Lambda_h \tilde{x}$； 通过反图傅里叶变换，求取输出图信号$y = V \tilde{y}$。 设多项式系数构成向量$h$，那么$H$的频率响应矩阵为 \Lambda_h = \sum_{k=0}^K h_k \Lambda^k = \text{diag}(\Psi \cdot h) \tag{14.1 }$\text{diag}$表示将列向量转换为对角矩阵。 其中$\Psi = \begin{bmatrix} 1 &amp; \lambda_1 &amp; \cdots &amp; \lambda_1^K \ 1 &amp; \lambda_2 &amp; \cdots &amp; \lambda_2^K \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 1 &amp; \lambda_n &amp; \cdots &amp; \lambda_n^K \end{bmatrix}$为范德蒙矩阵，$h = \begin{bmatrix} h_1 &amp; \cdots &amp; h_n \end{bmatrix}^T$。所以在确定频率响应函数$h(\lambda)$后，可以反解得到多项式系数$h$ h = \Psi^{-1} \text{diag}^{-1} (\Lambda_h) \tag{14.2}$\text{diag}^{-1}$表示将对角矩阵转换为列向量。 从频域角度来看，滤波操作的性质为 能更加清晰地完成对图信号地特定滤波操作； 有显式的公式知道图滤波器的设计； 不必进行矩阵分解，减少计算量。 图卷积的定义给定两组$G$上的图信号$x_1, x_2$，其图卷积运算定义为 x_1 * x_2 = IGFT \left( GFT(x_1) \bigodot GFT(x_2) \right) \tag{15.1}这里定义和离散时间信号处理中卷积定义相同，即时域中的卷积运算等价于频域中的乘法运算，其中 \begin{cases} GFT(x) = V^T x \\ IGFT(x) = V x \end{cases} \tag{15.2}$\bigodot$表示哈达玛积，那么 \begin{aligned} x_1 * x_2 & = V \left( V^T (x_1) \bigodot V^T (x_2) \right) \\ & = V \left( \tilde{x_1} \bigodot V^T (x_2) \right) \\ & = V \left( \text{diag}(\tilde{x_1}) \cdot V^T (x_2) \right) \\ & = \left( V \text{diag}(\tilde{x_1}) V^T \right) \cdot x_2 \end{aligned} \tag{15.3}令 H_{\tilde{x}} = V \text{diag}(\tilde{x_1}) V^T \tag{15.4}显然$H_{\tilde{x}}$是一个图位移算子，其频率响应矩阵对应$x_1$的频谱，那么 x_1 * x_2 = H_{\tilde{x}} \cdot x_2 \tag{15.5}所以两组图信号的图卷积运算能转化为对应形式的图滤波运算，可以说图卷积等价于图滤波，后文提到的图卷积都是等号右边的滤波形式，相对应的卷积信号无需显示地表达出来。 设定义在包含$N$个节点的图$G$上的图信号，为$d$维数据，表示为图信号矩阵如下，其中$d$为图信号的总通道数 X_{N \times d} = \begin{bmatrix} x_{11} & x_{12} & \cdots & x_{1d} \\ x_{21} & x_{22} & \cdots & x_{2d} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N1} & x_{N2} & \cdots & x_{Nd} \end{bmatrix} \tag{16.1}那么$Y = HX$可以视作用图滤波器$H$对信号矩阵每个通道(列)的信号分别进行滤波操作。 滤波器的参数化将图卷积运算推广到图数据的学习中，定义神经网络层，并对滤波器进行参数化。神经网络层内引入图滤波器$H$，一般定义为 Y = \sigma(H X) \tag{17}其中$\sigma(\cdot)$是激活函数。 频域图卷积模型只能从频域出发进行矩阵特征分解从而执行图卷积计算的模型。 频率响应函数的参数化经过前面讨论，图滤波器$H$可以分解为 H = V \Lambda_h V^T \tag{11.1}其中 \Lambda_h = \begin{bmatrix} h(\lambda_1) & & \\ & \ddots & \\ & & h(\lambda_n) \end{bmatrix} \tag{10.3}我们将其参数化，用待估参数$\theta_i$代替$h(\lambda_i)$，得到参数矩阵$\text{diag}(\theta)$ \text{diag}(\theta) = \begin{bmatrix} \theta_1 & & \\ & \ddots & \\ & & \theta_n \end{bmatrix} \tag{18}神经网络层定义为 Y = \sigma(V \text{diag}(\theta) V^T X) = \sigma(\Theta X) \tag{19}其中$\Theta$即需要学习的滤波器 从空域的角度理解，该层引入自适应的图位移算子，通过学习的手段指导算子的学习，完成对输入图信号的针对性变换操作； 从频域的角度理解，该层训练了一个可自适应的图滤波器。其频率响应函数可通过任务与数据间的对应关系来进行学习。 但是这种参数化方法有以下缺点 要学习的参数量与节点数目一致，引入过多的参数，在大规模图数据(上亿节点)的图中极易过拟合； 真实图数据中，数据有效信息通常蕴含在低频段中，因此$n$个维度自由度的图滤波器，是完全没有必要的。 拉普拉斯矩阵多项式拓展形式系数的参数化考虑用拉普拉斯矩阵多项式拓展形式逼近任意频率响应函数 \begin{aligned} H & = \sum_{k=0}^K h_k L^k \\ & = \sum_{k=0}^K h_k (V \Lambda V^T)^k \\ & = \sum_{k=0}^K h_k (V \Lambda V^T) \cdots (V \Lambda V^T) \\ & = V \underbrace{(\sum_{k=0}^K h_k \Lambda^k)}_{\Lambda_h} V^T \end{aligned} \tag{13.1}将各阶次的系数参数化为$\theta$，神经网络层定义为 \begin{aligned} Y & = \sigma \left( \left( \sum_{k=0}^K h_k L^k \right) X \right) \\ & = \sigma \left( V \left( \sum_{k=0}^K \theta_k \Lambda^k \right) V^T X \right) \\ & = \sigma \left( V \text{diag}(\Psi \theta) V^T X \right) \end{aligned} \tag{20}这种方法定义神经网络层，参数量$K$可控制，$K$越大，可拟合的频率响应函数次数越高，可以对应输入图信号矩阵与输出图信号矩阵之间复杂的滤波关系。一般设$k &lt;&lt; N$，大大降低模型过拟合风险。但是这种方法还是需要进行矩阵的分解，计算复杂度高。 空域图卷积模型不需要进行矩阵特征分解，能在空域视角执行矩阵乘法运算的模型。 固定滤波器的参数化为解决计算复杂度的问题，对$(19)$进行极大简化，设$K = 1$，那么 Y = \sigma \left( (\theta_0 + \theta_1 L) X \right) \tag{21.1}令$\theta_0 = \theta_1 = \theta$，那么 Y = \sigma \left( \theta (I + L) X \right) \tag{21.2}实际上，$\theta$此时只是一个比例缩放因子，是不必要引入的。设$\theta = 1$，得到固定的图滤波器$\tilde{L} = I + L$ Y = \sigma(\tilde{L} X) \tag{21.3} 从空域上看，只聚合了$1$阶邻居节点的信息。 为了加强网络学习时的数值稳定性，对$\tilde{L}$进行归一化处理，防止梯度消失或爆炸问题 \begin{aligned} \begin{cases} \tilde{A} = A + I \\ \tilde{D} = \text{diag} (\begin{bmatrix} \cdots & \sum_j \tilde{A}_{ij} & \cdots \end{bmatrix}) = D + I \end{cases} \\ \\ \Rightarrow \tilde{L}_{sym} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} \end{aligned} \tag{21.4} 注意$\tilde{L}_{sym}$的特征值取值范围为$(-1, 1]$。 为了加强网络拟合能力，引入参数化的权重矩阵$W$对输入的图信号矩阵进行仿射变换，可变换数据的维度，即 Y = \sigma(\tilde{L}_{sym} \cdot X W) \tag{22}由于$\tilde{L}_{sym}$是一个图位移算子，$\tilde{L}_{sym} X$的计算等价于对邻居节点的特征向量($X$的行向量，$d$个通道)进行聚合操作 y_{i, :} = \sigma(\sum_{v_j \in \tilde{N}(v_i)} {\tilde{L}_{sym}}_{ij} \cdot W x_{j, :}) \tag{23} 对于$v_i$节点，根据每个邻居节点$v_j$更新自身状态 对节点特征向量进行一次仿射变换$W x_j$； 固定滤波器$\tilde{L}_{sym}$的是拉普拉斯算子，第$i$行的可以聚合节点$v_i$的邻居节点； 聚合后非线性激活作为输出。 实际工程中，可以用稀疏矩阵表示$\tilde{L}_{sym}$，进一步降低计算复杂度。相比较于分解矩阵的$O(n^3)$，可以降低至$O(|E|d)$。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import tensorflow as tffrom tensorflow.python.keras.initializers import Identity, glorot_uniform, Zerosfrom tensorflow.python.keras.layers import Dropout, Input, Layer, Embedding, Reshapefrom tensorflow.python.keras.models import Modelfrom tensorflow.python.keras.regularizers import l2class GraphConvolution(Layer): # ReLU(AXW) def __init__(self, units, activation=tf.nn.relu, dropout_rate=0.5, use_bias=True, l2_reg=0, feature_less=False, seed=1024, **kwargs): super(GraphConvolution, self).__init__(**kwargs) self.units = units self.feature_less = feature_less self.use_bias = use_bias self.l2_reg = l2_reg self.dropout_rate = dropout_rate self.activation = activation self.seed = seed def build(self, input_shapes): if self.feature_less: input_dim = int(input_shapes[0][-1]) else: assert len(input_shapes) == 2 features_shape = input_shapes[0] input_dim = int(features_shape[-1]) self.kernel = self.add_weight(shape=(input_dim, self.units), initializer=glorot_uniform( seed=self.seed), regularizer=l2(self.l2_reg), name='kernel', ) if self.use_bias: self.bias = self.add_weight(shape=(self.units,), initializer=Zeros(), name='bias', ) self.dropout = Dropout(self.dropout_rate, seed=self.seed) self.built = True def call(self, inputs, training=None, **kwargs): features, A = inputs features = self.dropout(features, training=training) output = tf.matmul(tf.sparse_tensor_dense_matmul( A, features), self.kernel) if self.bias: output += self.bias act = self.activation(output) act._uses_learning_phase = features._uses_learning_phase return act def get_config(self): config = &#123;'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate, 'l2_reg': self.l2_reg, 'use_bias': self.use_bias, 'feature_less': self.feature_less, 'seed': self.seed &#125; base_config = super(GraphConvolution, self).get_config() return dict(list(base_config.items()) + list(config.items()))def GCN(adj_dim,feature_dim,n_hidden, num_class, num_layers=2,activation=tf.nn.relu,dropout_rate=0.5, l2_reg=0, feature_less=True, ): Adj = Input(shape=(None,), sparse=True) if feature_less: X_in = Input(shape=(1,), ) emb = Embedding(adj_dim, feature_dim, embeddings_initializer=Identity(1.0), trainable=False) X_emb = emb(X_in) h = Reshape([X_emb.shape[-1]])(X_emb) else: X_in = Input(shape=(feature_dim,), ) h = X_in for i in range(num_layers): if i == num_layers - 1: activation = tf.nn.softmax n_hidden = num_class h = GraphConvolution(n_hidden, activation=activation, dropout_rate=dropout_rate, l2_reg=l2_reg)([h,Adj]) output = h model = Model(inputs=[X_in,Adj], outputs=output) return model GCN的性质GCN与CNN联系 CNN中卷积运算没有显式表达邻接矩阵，可视作处理2D栅格结构的图数据；GCN卷积运算用于处理更普遍的非结构化的图数据，数据之间关系更复杂多样； 两者都是局部连接，减少单层网络的计算复杂度； 两者都共享参数，减少单层网络的参数量，避免过拟合； 节点自身特征的更新与卷积运算强耦合，感受野都随着卷积层的增加而变大，特征也更抽象； 在任务上，CNN有对全局进行的图像分类和对局部进行的语义分割，GNN相应地有图分类和图节点分类。 端到端学习端到端学习实现了一种自动化地从数据中进行高效学习的机制，需要大量针对特定类型数据的学习任务的适配工作。图数据中包含两部分信息 属性信息：描述了图中节点的固有性质； 机构信息：描述了节点间的关联性质。 GCN方法对于属性和结构信息的学习，体现在其核心公式上 \tilde{L}_{sym} X W \tag{24}可以看到计算过程可以分为两部分 特征间的交互：用$X W$对属性信息进行放射变换，以学习属性特征间的交互模式； 节点局部结构信息编码：$\tilde{L}_{sym} (X W)$从空域角度看，是聚合邻居节点的过程，对结构信息进行编码。 GCN模型有以下优点 表示学习和任务学习一起进行端到端优化，节点的特征表示和下游任务间有很好的适应性； 同时进行结构信息和属性信息的学习，两者具有很好的互补关系 对于结构稀疏的图，属性信息的补充可以很好地提高模型对节点表示学习的质量； 结构信息蕴含属性信息中没有的知识，对节点刻画十分重要。 低通滤波器再次回到GCN的核心公式$\tilde{L}_{sym} X W$，其中 \tilde{L}_{sym} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} \tag{25.1}根据 \begin{cases} \tilde{A} = A + I \\ \tilde{D} = D + I \\ L = D - A \end{cases} \Rightarrow \tilde{A} = \tilde{D} - L \tag{25.2}那么$\tilde{A}$带入$\tilde{L}_{sym}$可以得到 \tilde{L}_{sym} = \tilde{D}^{-1/2} (\tilde{D} - L) \tilde{D}^{-1/2} = I - \tilde{D}^{-1/2} L \tilde{D}^{-1/2} \tag{25.3}记 \tilde{L}_s = \tilde{D}^{-1/2} L \tilde{D}^{-1/2} \tag{25.4}可以被正交对角化如下，且特征值满足$\tilde{\lambda}_i \in [0, 2)$ \tilde{L}_s = V \tilde{\Lambda} V^T \tag{25.5}所以 \tilde{L}_{sym} = I - V \tilde{\Lambda} V^T = V (1 - \tilde{\Lambda}) V^T \tag{26.1}对应的频率响应函数如下，是一个线性收缩函数，并且$\tilde{\lambda}_1 \leq \cdots \leq \tilde{\lambda}_n$，所以对高频信号有更大的缩减作用，是一个低通滤波器 p_i(\lambda) = 1 - \tilde{\lambda}_i \in (-1, 1] \tag{26.2}如果将信号矩阵$X$不断左乘$K$次$\tilde{L}_{sym}$，其对应的频率响应函数为 p_i^k (\lambda) = (1 - \tilde{\lambda}_i)^k \in (-1, 1] \tag{26.3}其函数图像如下，可以看到当$K$增大时，低通滤波器的效应更强 事实上，为了突出这种低通滤波的能力、减少模型参数量，也有直接将多层GCN退化为 Y = \sigma(\tilde{L}_{sym}^K \cdot X W) \tag{27}过平滑问题由于每层GCN都是一个低通滤波器，在多层堆叠时，信号经不断平滑会越来越趋同，丧失节点特征的多样性，以下分别从频域和空域两个视角对这个问题进行分析。 频域视角从退化的多层GCN模型出发 \begin{aligned} \lim_{K \rightarrow + \infty} \tilde{L}_{sym}^K &= \lim_{K \rightarrow + \infty} (I - \tilde{L}_s)^K = \lim_{K \rightarrow + \infty} \left( V (I - \tilde{\Lambda}) V^T \right)^K \\ & = \lim_{K \rightarrow + \infty} V \begin{bmatrix} (1 - \tilde{\lambda}_1)^K & & & \\ & (1 - \tilde{\lambda}_2)^K & & \\ & & \ddots & \\ & & & (1 - \tilde{\lambda}_n)^K \\ \end{bmatrix} V^T \end{aligned} \tag{28.1}由于$\tilde{\lambda}_1 = 0, \tilde{\lambda}_i &gt; 0, i = 2, \cdots, n$，所以 \lim_{K \rightarrow + \infty} \tilde{L}_{sym}^K = V \begin{bmatrix} 1 & & & \\ & 0 & & \\ & & \ddots & \\ & & & 0 \\ \end{bmatrix} V^T \tag{28.2}那么对于输入图信号$x$，有 \begin{aligned} (\lim_{K \rightarrow + \infty} \tilde{L}_{sym}^K) \cdot x & = V \begin{bmatrix} 1 & & & \\ & 0 & & \\ & & \ddots & \\ & & & 0 \\ \end{bmatrix} V^T \cdot x \\ & = v_1 V^T \cdot x = v_1 v_1^T \cdot x = \tilde{x}_1 v_1 \end{aligned} \tag{28.3} 考虑 \tilde{L}_{sym} \cdot \tilde{D}^{1/2} \bm{1} = \tilde{D}^{-1/2} L \tilde{D}^{-1/2} \cdot \tilde{D}^{1/2} \bm{1} = \tilde{D}^{-1/2} \underbrace{L \bm{1}}_{\sum_j a_{ij} - d_i = 0} = \tilde{D}^{-1/2} \bm{0} = \bm{0} \tag{28.4}所以$(\tilde{D}^{1/2} \bm{1}, 0)$是$\tilde{L}_{sym}$的特征对，又因为 \tilde{L}_s = \tilde{D}^{-1/2} L \tilde{D}^{-1/2} = \begin{cases} \tilde{D}^{-1/2} V \Lambda V^T \tilde{D}^{-1/2} \\ V \tilde{\Lambda} V^T \end{cases}\tag{28.5}经拉普拉斯算子一节分析，拉普拉斯矩阵$L$存在特征对$(\bm{1}, 0)$，所以 v_1 = \tilde{D}^{1/2} \bm{1} \tag{28.6} 结合$(28.3)与(28.6)$可以得到 (\lim_{K \rightarrow + \infty} \tilde{L}_{sym}^K) \cdot x = \tilde{x}_1 \tilde{D}^{1/2} \cdot \bm{1} \tag{29}也就是说，经过$K \rightarrow + \infty$次平滑后，图信号最终趋于常向量，不具有区分性。 空域视角GCN从空域角度理解，是聚合邻居节点的信息。随着GCN层数增加，节点的聚合半径也在增长，最终会覆盖整个图，这与从那个结点出发开始聚合是无关的。这种情况会大大降低每个节点的局部网络结构的多样性，对节点自身的特征学习十分不利。 解决方法跳跃连接通过跳跃连接来聚合模型的每层节点输出，聚合后的节点特征具有混合性的聚合半径。对于任意一个节点而言，都不会因为聚合半径过大而出现过平滑问题，也不会因为半径过小而未充分学习结构信息。 具体操作是将每层GCN网络层的输出，通过跳跃连接与网络的最终输出相连，可以选择如拼接、平均池化、最大池化等操作，得到的最终输出用于监督学习。 重分配权重回到频域视角调节图滤波器的值，例如 A_{ij}' = \begin{cases} 1 - p & i = j \\ p \times \frac{A_{ij}}{\text{deg}(v_i)} & i \neq j \end{cases} \tag{30}其中$\text{deg}(\cdot)$表示取节点的度，也即 \text{deg}(v_i) = \sum_{v_j \in \mathcal{N}(v_i)} A_{ij} \tag{31}调节$p$的值对节点自身权重进行重分配 $p \rightarrow 0$时，模型趋向于不聚合邻居信息，减缓了模型低通滤波的效应； $p \rightarrow 1$时，模型趋向于不使用自身信息，加速了模型低通滤波的效应。 GCN变体与框架GraphSAGE上面以随机游走算法训练过程中，用到了拉普拉斯矩阵$L_{n \times n}$，$n$为节点个数，也就是说需要用全部节点的信息进行参数的学习，这对极大规模的图数据非常不友好，存在以下问题 子图的节点数呈指数级增长：例如节点度的均值为$\overline{d}$，执行$K$层GCN就涉及$1 + \overline{d} + \overline{d}^2 + \cdots + \overline{d}^K$个节点，导致很高的时间复杂度； 图数据节点度呈幂律分布：一些节点的度非常大(超级节点)，这些节点就放大了指数级增长的问题。 GraphSAGE从空域角度出发，每个节点聚合其$K$阶邻居节点的信息用于更新自身图信号，这就涉及到以下两个方面 邻居采样：将GCN由全图的训练方式改造为以节点为中心的小批量训练方式，可以用于训练大规模图数据； 邻居聚合：对聚合操作进行拓展，提出几种新的方式， 邻居采样GraphSAGE使用非常自然的采样邻居操作，用于控制子图发散的增长率。具体操作是在第$k$层的邻居采样倍率设置为$S_k$，即每个节点采样的一阶邻居不超过$S_k$(均匀采样)，那么任意一个中心节点的表达计算涉及的节点数目在$O(\prod_k S_k)$级别。 例如一个$2$层的模型，设置$S_1 = 3, S_2 = 2$，那么总节点个数为$1 + 3 + 3 \times 2 = 10$。 邻居聚合 邻居图信息的聚合需要满足以下两个条件 自适应聚合节点数，聚合输出维度必须一致，即长度统一的向量； 聚合操作对节点具有排列不变性，即邻居节点的顺序与聚合输出无关。 符合以上性质的操作算子有 逐元素的平均/加和：是GCN中图卷积操作的线性近似，加和聚合的网络层公式如下，其中$W, b$是代估参数y^{sum} = \sigma \left( \sum_{v_j \in \mathcal{N}(v_i)} (W h_j + b) \right)\tag{31} 逐元素的池化：借鉴CNN的池化操作，最大池化聚合的网络层公式如下y^{maxpool} = \max_{v_j \in \mathcal{N}(v_i)} \left( \sigma(Wh_j + b) \right) \tag{32} 注意激活函数的位置是不同的。 算法流程与实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203import numpy as npimport tensorflow as tffrom tensorflow.python.keras.initializers import glorot_uniform, Zerosfrom tensorflow.python.keras.layers import Input, Dense, Dropout, Layer, LSTMfrom tensorflow.python.keras.models import Modelfrom tensorflow.python.keras.regularizers import l2class MeanAggregator(Layer): def __init__(self, units, input_dim, neigh_max, concat=True, dropout_rate=0.0, activation=tf.nn.relu, l2_reg=0, use_bias=False, seed=1024, **kwargs): super(MeanAggregator, self).__init__() self.units = units self.neigh_max = neigh_max self.concat = concat self.dropout_rate = dropout_rate self.l2_reg = l2_reg self.use_bias = use_bias self.activation = activation self.seed = seed self.input_dim = input_dim def build(self, input_shapes): self.neigh_weights = self.add_weight(shape=(self.input_dim, self.units), initializer=glorot_uniform( seed=self.seed), regularizer=l2(self.l2_reg), name="neigh_weights") if self.use_bias: self.bias = self.add_weight(shape=(self.units), initializer=Zeros(), name='bias_weight') self.dropout = Dropout(self.dropout_rate) self.built = True def call(self, inputs, training=None): features, node, neighbours = inputs node_feat = tf.nn.embedding_lookup(features, node) neigh_feat = tf.nn.embedding_lookup(features, neighbours) node_feat = self.dropout(node_feat, training=training) neigh_feat = self.dropout(neigh_feat, training=training) concat_feat = tf.concat([neigh_feat, node_feat], axis=1) concat_mean = tf.reduce_mean(concat_feat, axis=1, keep_dims=False) output = tf.matmul(concat_mean, self.neigh_weights) if self.use_bias: output += self.bias if self.activation: output = self.activation(output) # output = tf.nn.l2_normalize(output,dim=-1) output._uses_learning_phase = True return output def get_config(self): config = &#123;'units': self.units, 'concat': self.concat, 'seed': self.seed &#125; base_config = super(MeanAggregator, self).get_config() return dict(list(base_config.items()) + list(config.items()))class PoolingAggregator(Layer): def __init__(self, units, input_dim, neigh_max, aggregator='meanpooling', concat=True, dropout_rate=0.0, activation=tf.nn.relu, l2_reg=0, use_bias=False, seed=1024, ): super(PoolingAggregator, self).__init__() self.output_dim = units self.input_dim = input_dim self.concat = concat self.pooling = aggregator self.dropout_rate = dropout_rate self.l2_reg = l2_reg self.use_bias = use_bias self.activation = activation self.neigh_max = neigh_max self.seed = seed # if neigh_input_dim is None: def build(self, input_shapes): self.dense_layers = [Dense( self.input_dim, activation='relu', use_bias=True, kernel_regularizer=l2(self.l2_reg))] self.neigh_weights = self.add_weight( shape=(self.input_dim * 2, self.output_dim), initializer=glorot_uniform( seed=self.seed), regularizer=l2(self.l2_reg), name="neigh_weights") if self.use_bias: self.bias = self.add_weight(shape=(self.output_dim,), initializer=Zeros(), name='bias_weight') self.built = True def call(self, inputs, mask=None): features, node, neighbours = inputs node_feat = tf.nn.embedding_lookup(features, node) neigh_feat = tf.nn.embedding_lookup(features, neighbours) dims = tf.shape(neigh_feat) batch_size = dims[0] num_neighbors = dims[1] h_reshaped = tf.reshape( neigh_feat, (batch_size * num_neighbors, self.input_dim)) for l in self.dense_layers: h_reshaped = l(h_reshaped) neigh_feat = tf.reshape( h_reshaped, (batch_size, num_neighbors, int(h_reshaped.shape[-1]))) if self.pooling == "meanpooling": neigh_feat = tf.reduce_mean(neigh_feat, axis=1, keep_dims=False) else: neigh_feat = tf.reduce_max(neigh_feat, axis=1) output = tf.concat( [tf.squeeze(node_feat, axis=1), neigh_feat], axis=-1) output = tf.matmul(output, self.neigh_weights) if self.use_bias: output += self.bias if self.activation: output = self.activation(output) # output = tf.nn.l2_normalize(output, dim=-1) return output def get_config(self): config = &#123;'output_dim': self.output_dim, 'concat': self.concat &#125; base_config = super(PoolingAggregator, self).get_config() return dict(list(base_config.items()) + list(config.items()))def GraphSAGE(feature_dim, neighbor_num, n_hidden, n_classes, use_bias=True, activation=tf.nn.relu, aggregator_type='mean', dropout_rate=0.0, l2_reg=0): features = Input(shape=(feature_dim,)) node_input = Input(shape=(1,), dtype=tf.int32) neighbor_input = [Input(shape=(l,), dtype=tf.int32) for l in neighbor_num] if aggregator_type == 'mean': aggregator = MeanAggregator else: aggregator = PoolingAggregator h = features for i in range(0, len(neighbor_num)): if i &gt; 0: feature_dim = n_hidden if i == len(neighbor_num) - 1: activation = tf.nn.softmax n_hidden = n_classes h = aggregator(units=n_hidden, input_dim=feature_dim, activation=activation, l2_reg=l2_reg, use_bias=use_bias, dropout_rate=dropout_rate, neigh_max=neighbor_num[i], aggregator=aggregator_type)( [h, node_input, neighbor_input[i]]) # output = h input_list = [features, node_input] + neighbor_input model = Model(input_list, outputs=output) return modeldef sample_neighs(G, nodes, sample_num=None, self_loop=False, shuffle=True): # 抽样邻居节点 _sample = np.random.choice neighs = [list(G[int(node)]) for node in nodes] # nodes里每个节点的邻居 if sample_num: if self_loop: sample_num -= 1 samp_neighs = [ list(_sample(neigh, sample_num, replace=False)) if len(neigh) &gt;= sample_num else list( _sample(neigh, sample_num, replace=True)) for neigh in neighs] # 采样邻居 if self_loop: samp_neighs = [ samp_neigh + list([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)] # gcn邻居要加上自己 if shuffle: samp_neighs = [list(np.random.permutation(x)) for x in samp_neighs] else: samp_neighs = neighs return np.asarray(samp_neighs), np.asarray(list(map(len, samp_neighs))) GAT图注意力网络(Graph Attention Networks, GAT)通过注意力机制对邻居节点进行聚合操作，实现对不同邻居权重的自适应分配，提高模型表达能力。 设图中任意节点$v_i$在$l$层对应的特征向量为$h^{(l)}_i \in R^{d^{(l)}}$，$d^{(l)}$表示节点的特征维数。经过以注意力机制为核心的聚合操作后，得到输出$h^{(l + 1)}_i \in R^{d^{(l + 1)}}$。 设中心节点为$v_i$，定义$W^{(l)} \in R^{d^{(l + 1)} \times d^{(l)}}$是该层节点特征变换的权重参数，$a(\cdot)$是计算两个节点相关度的函数，那么邻居节点$v_j$(可以将每个节点都视作自己的邻居)到$v_i$的权重系数为 e^{(l)}_{ij} = a(W^{(l)} h^{(l)}_i, W^{(l)} h^{(l)}_j) \tag{33}原文中，作者采用了以下函数计算权重 e^{(l)}_{ij} = \text{Leaky ReLU}({a^{(l)}}^T [W^{(l)} h^{(l)}_i || W^{(l)} h^{(l)}_j]) \tag{34.1}其中$a^{(l)} \in R^{2d^{(l + 1)}}$，$[\cdot || \cdot]$表示特征拼接操作。用softmax进行归一化，得到权重系数$\alpha^{(l)}_{ij}$ \alpha^{(l)}_{ij} = \text{softmax}_j (e^{(l)}_{ij}) = \exp(e^{(l)}_{ij}) / \sum_{v_k \in \mathcal{N}(v_i)} e^{(l)}_{ik} \tag{34.2}那么网络层输入输出的关系可以表示为 h^{(l + 1)}_i = \sigma\left( \sum_{v_k \in \mathcal{N}(v_i)} \alpha^{(l)}_{ij} \cdot W^{(l)} h^{(l)}_j \right) \tag{35} 为进一步提高注意力层的表达能力，可以加入多头注意力机制(mult-head attention)，具体操作是对$(35)$多次调用$K$组相互独立的注意力机制，然后将结果聚集，例如拼接操作 h^{(l + 1)}_i = ||_{k=1}^K \sigma \left( \sum_{v_k \in \mathcal{N}(v_i)} {\alpha^{(l)}_{ij}}_k \cdot W_k^{(l)} h^{(l)}_j \right) \tag{36.1}或者取均值 h^{(l + 1)}_i = \frac{1}{K} \sum_{k=1}^K \sigma \left( \sum_{v_k \in \mathcal{N}(v_i)} {\alpha^{(l)}_{ij}}_k \cdot W_k^{(l)} h^{(l)}_j \right) \tag{36.2}123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143import tensorflow as tffrom tensorflow.python.keras import backend as Kfrom tensorflow.python.keras.initializers import Zerosfrom tensorflow.python.keras.layers import Layer, Dropout,Inputfrom tensorflow.python.keras.regularizers import l2from tensorflow.python.keras.models import Modelclass GATLayer(Layer): def __init__(self, att_embedding_size=8, head_num=8, dropout_rate=0.5, l2_reg=0, activation=tf.nn.relu, reduction='concat', use_bias=True, seed=1024, **kwargs): if head_num &lt;= 0: raise ValueError('head_num must be a int &gt; 0') self.att_embedding_size = att_embedding_size self.head_num = head_num self.dropout_rate = dropout_rate self.l2_reg = l2_reg self.activation = activation self.act = activation self.reduction = reduction self.use_bias = use_bias self.seed = seed super(GATLayer, self).__init__(**kwargs) def build(self, input_shape): X, A = input_shape embedding_size = int(X[-1]) self.weight = self.add_weight(name='weight', shape=[embedding_size, self.att_embedding_size * self.head_num], dtype=tf.float32, regularizer=l2(self.l2_reg), initializer=tf.keras.initializers.glorot_uniform()) self.att_self_weight = self.add_weight(name='att_self_weight', shape=[1, self.head_num, self.att_embedding_size], dtype=tf.float32, regularizer=l2(self.l2_reg), initializer=tf.keras.initializers.glorot_uniform()) self.att_neighs_weight = self.add_weight(name='att_neighs_weight', shape=[1, self.head_num, self.att_embedding_size], dtype=tf.float32, regularizer=l2(self.l2_reg), initializer=tf.keras.initializers.glorot_uniform()) if self.use_bias: self.bias_weight = self.add_weight(name='bias', shape=[1, self.head_num, self.att_embedding_size], dtype=tf.float32, initializer=Zeros()) self.in_dropout = Dropout(self.dropout_rate) self.feat_dropout = Dropout(self.dropout_rate, ) self.att_dropout = Dropout(self.dropout_rate, ) # Be sure to call this somewhere! super(GATLayer, self).build(input_shape) def call(self, inputs, training=None, **kwargs): X, A = inputs X = self.in_dropout(X) # N * D # A = self.att_dropout(A, training=training) if K.ndim(X) != 2: raise ValueError( "Unexpected inputs dimensions %d, expect to be 2 dimensions" % (K.ndim(X))) features = tf.matmul(X, self.weight, ) # None F'*head_num features = tf.reshape( features, [-1, self.head_num, self.att_embedding_size]) # None head_num F' # attn_for_self = K.dot(features, attention_kernel[0]) # (N x 1), [a_1]^T [Wh_i] # attn_for_neighs = K.dot(features, attention_kernel[1]) # (N x 1), [a_2]^T [Wh_j] # head_num None F D --- &gt; head_num None(F) D # querys = tf.stack(tf.split(querys, self.head_num, axis=1)) # keys = tf.stack(tf.split(keys, self.head_num, axis=1))#[?,1,1433,64] # features = tf.stack(tf.split(features, self.head_num, axis=1)) # head_num None F' attn_for_self = tf.reduce_sum( features * self.att_self_weight, axis=-1, keep_dims=True) # None head_num 1 attn_for_neighs = tf.reduce_sum( features * self.att_neighs_weight, axis=-1, keep_dims=True) dense = tf.transpose(attn_for_self, [1, 0, 2]) + \ tf.transpose(attn_for_neighs, [1, 2, 0]) dense = tf.nn.leaky_relu(dense, alpha=0.2) mask = -10e9 * (1.0 - A) dense += tf.expand_dims(mask, axis=0) # [?,8,8], [1,?,2708] self.normalized_att_scores = tf.nn.softmax( dense, dim=-1, ) # head_num None(F) None(F) features = self.feat_dropout(features, ) self.normalized_att_scores = self.att_dropout( self.normalized_att_scores) result = tf.matmul(self.normalized_att_scores, tf.transpose(features, [1, 0, 2])) # head_num None F D [8,2708,8] [8,2708,3] result = tf.transpose(result, [1, 0, 2]) # None head_num attsize if self.use_bias: result += self.bias_weight # head_num Node embeding_size if self.reduction == "concat": result = tf.concat( tf.split(result, self.head_num, axis=1), axis=-1) result = tf.squeeze(result, axis=1) else: result = tf.reduce_mean(result, axis=1) if self.act: result = self.activation(result) result._uses_learning_phase = True return result def compute_output_shape(self, input_shape): if self.reduction == "concat": return (None, self.att_embedding_size * self.head_num) else: return (None, self.att_embedding_size) def get_config(self, ): config = &#123;'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res, 'seed': self.seed&#125; base_config = super(GATLayer, self).get_config() return dict(list(base_config.items()) + list(config.items()))def GAT(adj_dim,feature_dim,num_class,num_layers=2,n_attn_heads = 8,att_embedding_size=8,dropout_rate=0.0,l2_reg=0.0,use_bias=True): X_in = Input(shape=(feature_dim,)) A_in = Input(shape=(adj_dim,)) h = X_in for _ in range(num_layers-1): h = GATLayer(att_embedding_size=att_embedding_size, head_num=n_attn_heads, dropout_rate=dropout_rate, l2_reg=l2_reg, activation=tf.nn.elu, use_bias=use_bias, )([h, A_in]) h = GATLayer(att_embedding_size=num_class, head_num=1, dropout_rate=dropout_rate, l2_reg=l2_reg, activation=tf.nn.softmax, use_bias=use_bias, reduction='mean')([h, A_in]) model = Model(inputs=[X_in, A_in], outputs=h) return model R-GCNR-GCN用于对节点不同关系进行建模，之前介绍的GCN模型都是考虑同构图，而现实生活中图数据往往是异构的。例如在搜索某人的信息时，异构图中需要考虑与其在多种关系图中有关联的人员，如国籍、年龄、出生地等，而关系间没有很大的关联性。 R-GCN基于GCN的聚合邻居操作，增加了聚合关系的维度，使得节点的聚合操作变成双重聚合过程，如下 h^{(l + 1)}_i = \sigma \left( \sum_{r \in R} \sum_{v_k \in \mathcal{N}_r(v_i)} \frac{1}{ {c_i}_r } W_r^{(l)} h^{(l)}_j + W_o^{(l)} h^{(l)}_i \right) \tag{37}其中 $R$表示所有关系的集合； $\mathcal{N}_r(v_i)$表示与节点$v_i$具有$r$关系的邻居集合； ${c_i}_r$用于归一化，如${c_i}_r = |\mathcal{N}_r(v_i)|$； $W_r^{(l)}$是具有$r$关系的邻居对应的权重系数； $W_o^{(l)}$是节点自身对应的权重系数。 但是以上方式存在以下问题 多关系图中可能包含大量关系，如果为每一种关系设计一组权重，那么单层R-GCN参数量十分庞大； 不同关系的节点数目不同，例如不常见关对应的权重参数非常少，增加了过拟合风险。 R-GCN提出了对$W_r^{(l)}$进行基分解(basic decomposition)，即 W_r^{(l)} = \sum_{b=1}^B { {a_b}_r }^{(l)} V_b^{(l)} \tag{38.1}其中$V_b^{(l)} \in R^{d^{(l + 1)} \times d^{(l)}}$，${ {a_b}_r }^{(l)}$是分解系数，$B$是控制分解个数的超参数(一般取$B &lt;&lt; \min(|R|, d^{(l + 1)} \times d^{(l)})$)。那么经过分解后，参数量是原来的 \frac{(|R| + d^{(l + 1)} \times d^{(l)}) \times B}{|R| \times d^{(l + 1)} \times d^{(l)}} \tag{38.2}图分类图分类问题与节点层面任务不同，给定多张图及其对应标签，需要通过学习得出一个由图到相应标签的图分类模型。需要关注图数据的全局信息(包括图的结构信息和各个节点的属性信息)，重点在于如何通过学习得到一个优秀的全图表示向量。 图分类任务与视觉中图像分类一样，需要对全局的信息进行融合学习。CNN中通常采用的方法是层次化池化(Hierarchical Pooling)。由于图像数据为规则的栅格结构，固定大小和步长的滑窗使最大池化或平均池化等简单操作都能高效地提取高阶信息，但非规则结构的图数据无法直接迁移这类池化操作。 以下主要介绍两个部分：基于一次性全局池化的图分类、基于层次化池化的图分类。 基于一次性全局池化的图分类类似CNN模型中常用的最后一个卷积层的全局池化(Global Pooling)，这种方法对经过$K$轮迭代的所有节点进行一次性聚合操作，得到全图的全局表示 y = R(\{ h^{(k)}_i | \forall v_i \in V \}) \tag{39}$R$可以是Sum、Mean、Max等类型的函数。考虑到经过$K$仑迭代后，各个节点的表达会接近全局表达，所以能较好地提取全局信息。但是这种处理方法本质上是将输入数据看作平整且规则的结构数据，丢失了图数据中丰富的结构信息，适用于结构信息相对单一的小规模图数据； 基于层次化池化的图分类基于图坍缩的池化机制将图划分成不同的子图，然后将子图视作超级节点，从而形成坍缩的图。 给定图$G$，通过某种划分策略得到$K$个子图${G^{(k), k = 1, \cdots, K}}$，子图$G^{(k)}$中的节点个数为$N_k$、节点列表为$\Gamma^{(k)}$，并定义两个关键矩阵： 簇分配矩阵$S \in R^{N \times K}$：当且仅当$v_i \in \Gamma^{(k)}$时，有$S_{ik} = 1$。考察一下簇分配矩阵$S$的含义，有 { A_{coar} }_{K \times K} = S^T A S \tag{40} $A_{coar}$描述了图坍塌后超级节点之间的连接强度，${ A_{coar} }_{ii}$是超级节点自身内部的连接强度。 采样算子$C^{(k)} \in R^{N \times N_k}$，$N_k$是簇$k$中结点的个数：当且仅当$v_i = \Gamma^{(k)}_j$时，有$C_{ij} = 1$。 假定定义在$G$上的$1$维图信号为$x \in R^{N}$，下面两个式子完成了对图信号的下采样(切片)和上采样操作 \begin{aligned} x^{(K)} & = { C^{(k)} }^T x & 取出包含节点对应维度的图信号 \\ \overline{x} & = C^{(k)} x^{(K)} & 在不包含的节点对应维度处补0 \\ \end{aligned} 那么可以通过采样算子获取子图的邻接矩阵 A^{(k)} = { C^{(k)} }^T A C^{(k)} \tag{41} 通过$(40)$和$(41)$可以确定簇内的邻接关系和簇间的邻接关系。那么如果能够确定簇内信号的融合方法，将结果表示为超级节点上的信号，那么迭代重复上述过程就可以获得越来越全局的图信号。 如下图所示 那么有邻接矩阵 A = \begin{bmatrix} 0 & 1 & 1 & 0 & 0 & 0 \\ 1 & 0 & 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 0 & 1 \\ 0 & 0 & 0 & 1 & 1 & 0 \end{bmatrix}按图中分为两个簇，$\Gamma^{(1)} = {v_1, v_2, v_3}, \Gamma^{(2)} = {v_4, v_5, v_6}$，那么簇分配矩阵为 S = \begin{bmatrix} 1 & 0 \\ 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \\ 0 & 1 \end{bmatrix}采样算子有 C^{(1)} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}, C^{(2)} = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}在本例中超级节点间的邻接矩阵为 A_{coar} = S^T A S = \begin{bmatrix}6 & 1 \\ 1 & 6 \end{bmatrix}子图$G^{(1)}$内的邻接矩阵为 A^{(1)} = { C^{(1)} }^T A C^{(1)} = \begin{bmatrix}0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}DIFFPOOLDIFFPOOL是首个将图坍塌过程和GNN结合起来进行图层面任务学习的算法。提出了一个可学习的簇分配矩阵，具体地就是通过GNN对每个节点进行特征学习，然后通过另一个GNN为节点学习出所属各个簇的概率分布 \begin{aligned} 特征学习: & Z^{(l)} = & GNN_{l, embed}(A^{(l)}, H^{(l)}) \\ 簇概率分布学习: & S^{(l)} = & softmax \left( GNN_{l, pool}(A^{(l)}, H^{(l)}) \right) \end{aligned} \tag{42.1}其中 $A^{(l)} \in R^{n^{(l)} \times n^{(l)}}, S^{(l)} \in R^{n^{(l)} \times n^{(l + 1)}}$，$n^{(l)}$表示第$l$层的节点数； 相比较$(40)$的簇分配矩阵，这里的$S$是一个软分配器，值表示节点被分配到任意一个簇的概率，由于概率不为$0$，所以这是一个下层超级节点到上层所有节点之间的全连接结构； $GNN_{l, embed}, GNN_{l, pool}$是两个独立的GNN层，输入相同但参数不同，学习任务也不同； 最后一层的簇分配矩阵，需要将图坍缩成一个超级节点，所以直接将该矩阵固定成全$1$的矩阵。 基于上述两个GNN层的输出，可以对图进行坍缩，定义DIFFPOOL层$\left( (A^{(l)}, Z^{(l)}) \rightarrow (A^{(l+1)}, H^{(l+1)}) \right)$，由以下两部分组成 \begin{aligned} 簇内节点的特征加和处理: & H^{(l+1)} = & { S^{(l)} }^T Z^{(l)} \\ 簇间邻接矩阵计算: & A^{(l+1)} = & { S^{(l)} }^T A^{(l)} S^{(l)} \end{aligned} \tag{42.2}EigenPoolingEigenPooling没有对图分类模型引入任何需要学习的参数，其核心步骤在于作用域的选取以及池化操作。 EigenPooling借用一些图分区的算法来实现图的划分，如谱聚类算法，划分后用式$(40)$得到超级节点间的邻接关系 { A_{coar} }_{K \times K} = S^T A S \tag{40} 用子图上的信号在该子图上的图傅里叶变换来代表结构信息与属性信息的整合输出(详细略)。 基于TopK的池化机制对图中每个节点学习得到一个分数，基于分数的排序丢弃一些低分数的节点。这种方法借鉴了最大池化的思路：将更重要的信息筛选出来，但是图数据难以实现局部滑窗，需要依据分数进行全局筛选。 具体来说，首先设置一个表示池化率的超参数$k \in (0, 1)$，接着学习出一个表示节点重要度的值$z$并对其进行降序排序，然后将全图中$N$个节点下采样至$kN$个，即 i = top-rank(z, kN) \tag{43.1}X' = X_{i,:} \tag{43.2}A' = A_{i, i} \tag{43.3}其中 $i$为索引向量，即Topk中的索引； $X_{i,:}$表示按向量$i$的值对特征矩阵按行切片； $A_{i, i}$表示按向量$i$的值对邻接决战同时进行行切片与列切片。 关于节点重要度的学习，在[1811.01287] Towards Sparse Hierarchical Graph Classifiers一文中，作者为图分类模型设置了全局基向量$p$，将节点特征向量在该基向量的投影视作重要度 z = X \cdot \frac{p}{||p||} \tag{44.1}有以下两个作用 可以以投影大小确定Topk排序； 投影大小起到了梯度门限的作用，投影越大其梯度更新增幅越大。 全部细节如下 z = \frac{Xp}{||p||}, i = top-rank(z, kN) \tag{44.2}X' = (X \odot tanh(z))_{i, :} \tag{44.3}A' = A_{i, i} \tag{44.4}$(44.3)$中点乘$tanh(z)$相当于利用节点的重要度对节点特征做一次收缩变换，进一步强化了对重要度高的节点的梯度学习，这一操作被称为gpool层。这种采取层层丢弃节点的做法，可以提高远距离节点的融合效率，但是会使其缺乏对所有节点进行有效信息融合的手段。因此，作者选择在每个gpool层后跟随读出层，实现对该尺度下图的全局信息的一次性聚合 s = \underbrace{\frac{1}{N} \sum_{i=1}^N x_i'}_{全局平均池化} || \underbrace{\max_{i=1}^N x_i'}_{全局最大池化} \tag{44.5}最终将各层的$s$相加。得到全图的表示 s = \sum_{l=1}^L s^{(l)} \tag{44.6}基于边收缩的池化机制基于边收缩的池化机制EdgePool，通过迭代式地对每条边上的节点进行两两归并形成新节点，同时保留合并前节点的连接关系到新结点上。有个问题如何在某节点的多条边中选择用于收缩的边，EdgePool的解决方法是对每条边设计一个分数，根据该分数进行非重复式的挑选和合并，具体操作如下 计算每条边的原始分数$r$ r_{ij} = w^T [h_i || h_j] + b \tag{45.1} 对原始分数沿邻居节点进行归一化 s_{ij} = softmax_j (r_ij) \tag{45.2} 对所有$s_{ij}$进行排序，依次选择分数最高的且未被选中的两个节点进行收缩操作，并用求和的方式求取合并之后的节点特征 h_{ij} = s \cdot (h_i + h_j), s = \max(s_{ij}, s_{ji}) \tag{45.3} 其中分数$s$用于对节点特征进行收缩处理。 注意：未被选中的两个节点才进行合并，即将节点归并比严格控制在$0.5$。 图表示学习前面使用邻接矩阵$A \in R^{N \times N}$表示图的结构信息，一般来说$A$是一个高维且稀疏的矩阵，直接用$A$去表示图数据，相关的任务学习难以高效。图表示学习的主要目标是将图数据转化成低维稠密的向量化表示，同时确保图数据的某些性质在向量空间中也得到对应。 基于重构损失的GNN类比自编码器的思路，可以将节点间的邻接关系进行重构学习，定义图自编码器(Graph Auto Encoder) Z = GNN(X, A) \tag{46.1}\hat{A} = \sigma(Z Z^T) \tag{46.2}其中 $Z$是所有节点的表示，借助GNN模型同时对图的属性信息和结构信息进行编码学习； $\hat{A}$是重构之后的邻接矩阵，这里使用向量内积表示节点之间对的邻接关系。 图自编码器的重构损失定义为 L_{recon} = || \hat{A} - A ||_2^2 \tag{46.3}由于过平滑问题，GNN可以轻易将相邻节点学习出相似的表达，导致$\hat{A}$能很快趋近于原始邻接矩阵$A$，模型参数难以有效优化。那么同自编码器一样，对损失函数加上一些约束目标。如对输入数据进行一定扰动，迫使模型从加噪数据中提取有用的信息，包括 对原图数据的特征矩阵$X$适当增加随机噪声或随机置$0$； 对原图数据的邻接矩阵$A$删除适当比例的边，或修改边上的权重值。 还有如基于变分自编码器(VAE)的图表示学习方法。 基于对比损失的GNN对比损失是无监督表示学习中一种常见的损失函数。通过设置评分函数$D(\cdot)$，在学习过程中将会提高正样本的评分，降低负样本的评分。 类比词向量，在图数据中上下文代表与节点有对应关系的对象，从小到大依次可以是节点的邻居、节点所处的子图、全图。作为节点与上下文之间存在的固有关系，我们希望评分函数提高节点与上下文对的得分、降低节点与非上下文对的得分，即 L_{v_i} = - \log (D(z_i, c)) + \log (D(z_i, \overline{c})) \tag{47}其中$c$表示上下文的表示向量，$\overline{c}$表示非上下文的表示向量。 邻居上下文将邻居节点作为上下文，那么就是建模节点与邻居节点的共现关系。GraphSAGE中描述了这样一种方法，在随机游走时与中心节点$v_i$一起出现在固定长度窗口内的节点$v_j$视作邻居，通过负采样手段将不符合该关系的节点作为负样本 Z = GNN(X, A) \tag{48.1}L_{v_i} = \log(1 - \sigma(z_i^T z_j)) + E_{v_n \sim p_n(v_i)} \log (\sigma(z_i^T z_{v_n})) \tag{48.2}其中 $p_n(v_i)$是一个关于节点出现概率的负采样分布； 得分函数使用向量内积加$sigmoid$函数的形式，将分数限制在$[0, 1]$内。 但是这种方式强调节点之间的共现关系，更多反映了图中节点间的距离远近，缺乏对节点结构相似性的捕捉。 子图上下文[1905.12265] Pre-training Graph Neural Networks一文中提出这样一种方法，将子图作为节点的上下文进行对比学习。用GNN在节点$v_i$的$K$阶子图上提取表示向量；在$r_1-hop$与$r_2-hop$之间的节点定义为$v_i$的上下文锚点，用GNN提取该范围内每个节点作为上下文锚点时的表示向量，然后聚合上下文锚点的表示向量得到一个总的、固定长度的上下文表示向量。具体地 Z = GNN(X, A) \tag{49.1}Z_{context} = GNN_{context}(X, A) \tag{49.2.1}c_i = R\left( \{ Z_{context}[j], \forall v_j \in C_{v_i} \} \right) \tag{49.2.2}L_{v_i} = \log(1 - \sigma(z_i^T c_i)) + \log(\sigma(z_i^T c_{j | j \neq i})) \tag{49.3}其中$C_{v_i}$表示节点$v_i$的上下文锚点集合，示意图如下 全图上下文Deep Graph Infmax(DGI)，略。 Reference shenweichen/GraphNeuralNetwork]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Efficient Feature Selection]]></title>
    <url>%2F2020%2F06%2F12%2FEfficient-Feature-Selection%2F</url>
    <content type="text"><![CDATA[目录《数据挖掘》中介绍许多数据处理方法，但对特征选择没有深入介绍，本文对这方面的方法进行汇总介绍，并结合sklearn做一些样例。 Data-Mining - LOUIS’ BLOG。 目录 过滤方法(Filter) 方差阈值(Variance Threshold) 单变量选择(Univariate Feature Selection) 相关系数(Correlation Coefficient) 卡方检验(Chi-Square Test) 互信息(Mutual Information) 最大信息系数(Maximal Information Coefficient, MIC) 信息增益(Information Gain) 包裹方法(Wrapper) 递归特征消除(Recursive Feature Elimination, REF) 拉斯维加斯方法(Las Vegas Wrapper, LVW) LV随机算法 LVW特征选择 嵌入方法(Embedded) 惩罚项方法(Regularization) 线性回归的最小二乘估计和贝叶斯估计 LASSO(L1) Ridge(L2) Least Angel Regression(LARS) 基于树的方法(Tree-based) 决策树(Decision Tree) 随机森林(Random Forest, RF) 梯度提升树(GBDT) Reference 过滤方法(Filter)过滤法只用于检验特征向量和目标(响应变量)的相关度，不需要任何的机器学习的算法，不依赖于任何模型，只是应用统计量做筛选：我们根据统计量的大小，设置合适的阈值，将低于阈值的特征剔除。 所以，从某种程度上来说，过滤法更像是一个数学问题，我们只在过滤之后的特征子集上进行建模和训练。 方差阈值(Variance Threshold)一种最简单的方法，移除方差较小的特征，可以用于离散或连续的变量。如下，三列方差分别为$0.13888889, 0.22222222, 0.25$，阈值设置为$0.8 \times (1 - 0.8) = 0.16$，第一列被删除。12345678910&gt;&gt;&gt; from sklearn.feature_selection import VarianceThreshold&gt;&gt;&gt; X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]&gt;&gt;&gt; sel = VarianceThreshold(threshold=(.8 * (1 - .8)))&gt;&gt;&gt; sel.fit_transform(X)array([[0, 1], [1, 0], [0, 0], [1, 1], [1, 0], [1, 1]]) 单变量选择(Univariate Feature Selection)分别计算每个特征的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标 These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile): For regression: f_regression, mutual_info_regression For classification: chi2, f_classif, mutual_info_classif 相关系数(Correlation Coefficient)单变量选择方法，通过计每个算特征与标签的相关系数，来评估特征的重要性，计算方式如下 \rho = \frac{Cov(X, Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{\sum_j (X_j - \overline{X}) (Y_j - \overline{Y})} { \sqrt{\sum_j (X_j - \overline{X})^2} \sqrt{\sum_j (Y_j - \overline{Y})^2} }1234567891011121314151617&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from scipy.stats import pearsonr&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.feature_selection import SelectKBest&gt;&gt;&gt;&gt;&gt;&gt; def corr(X, Y):... _corr = lambda X, Y: np.array(list(... map(lambda x: pearsonr(x, Y), X.T))).T... _tolist = lambda x: list(map(list, x))... return _tolist(_corr(X, Y))...&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; X_new = SelectKBest(corr, k=2).fit_transform(X, y)&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 2) 卡方检验(Chi-Square Test)卡方检验用于统计样本的实际观测值于理论推断值之间的偏离程度，卡方值越大表示越偏离，假设有两个$0-1$变量$X, Y$，由统计信息可以得到观测四格表 $X$ $\overline{X}$ 合计 $Y$ $a$ $b$ $a + b$ $\overline{Y}$ $c$ $d$ $c + d$ 合计 $a + c$ $b + d$ $N$ 建立无关性假设：$X$与$Y$是独立无关的，那么随机抽取一个样本 属于$X$的概率是$p = (a + c) / N$ 属于$\overline{X}$的概率是$1 - p = (b + d) / N$ 那么可得理论值四格表如下 $X$ $\overline{X}$ 合计 $Y$ $\overline{a} = (a + b) \times p$ $\overline{b} = (a + b) \times (1 - p)$ $a + b$ $\overline{Y}$ $\overline{c} = (c + d) \times p$ $\overline{d} = (c + d) \times (1 - p)$ $c + d$ 合计 $N \times p$ $N \times (1 - p)$ $N$ 那么卡方值可以由下式计算 \chi^2 = \sum_{x \in \{a, b, c, d\}} \frac{(x - \overline{x})^2}{x}1234567891011&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.feature_selection import SelectKBest&gt;&gt;&gt; from sklearn.feature_selection import chi2&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; X_new = SelectKBest(chi2, k=2).fit_transform(X, y)&gt;&gt;&gt;&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 2) 互信息(Mutual Information)用于评估互信息(Mutual Information)是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。 设随机变量$X, Y$得联合分布为$p(x, y)$，边缘分布分别为$p(x), p(y)$，那么互信息是联合分布$p(x, y)$与边缘分布$p(x), p(y)$的相对熵，即 I(X; Y) = \sum_x \sum_y p(x, y) \log \frac{p(x, y)}{p(x) p(y)}它有一些缺点 不属于度量方式，也不能进行归一化，不同数据集上结果无法比较； 对于连续变量，需要先进行离散化才能计算。 1234567891011&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.feature_selection import SelectKBest&gt;&gt;&gt; from sklearn.feature_selection import mutual_info_classif&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; X_new = SelectKBest(mutual_info_classif, k=2).fit_transform(X, y)&gt;&gt;&gt;&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 2) 最大信息系数(Maximal Information Coefficient, MIC)在计算互信息时，联合分布$p(x, y)$很难求解。采用蒙特卡洛法估计：将两个随机变量的散点描绘在二维平面上，然后将$X$平均划分为$|X|$个子区间，$Y$平均划分为$|Y|$个子区间，然后就可以得到$p(x, y)$的估计。 MIC可以用下式计算 MIC(X, Y) = \max_{|X||Y| < B} \frac{I(X; Y)}{\log (\min(|X|, |Y|))}其中$B$一般取数据总量$N$的$0.6$或者$0.55$次方。 1234567891011121314151617181920212223&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from minepy import MINE&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.feature_selection import SelectKBest&gt;&gt;&gt;&gt;&gt;&gt; def minemic(x, y):... m = MINE()... m.compute_score(x, y)... return m.mic(), 0.5...&gt;&gt;&gt; def mic(X, Y):... _mic = lambda X, Y: np.array(list(... map(lambda x: minemic(x, Y), X.T))).T... _tolist = lambda x: list(map(list, x))... return _tolist(_mic(X, Y))...&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; X_new = SelectKBest(mic, k=3).fit_transform(X, y)&gt;&gt;&gt;&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 3) 信息增益(Information Gain)信息增益被用于决策树的分裂中，用于选择当前最优的分裂属性，可以描述某特征对数据集不确定性减少的程度。而在特征选择中，可以衡量特征能够描述数据集的多少信息量。 假设分类问题数据集$D$包含$|D|$个样本，标签列为$Y$，供包含$K$类，即$y \in {c_1, \cdots, c_K}$，那么类别熵为 H(Y) = - \sum_k p(Y = c_k) \log p(Y = c_k) = - \sum_k \frac{|c_k|}{|D|} \log \frac{|c_k|}{|D|}假设特征$X$有$A$种取值${x_1, \cdots, x_A}$，那么当$X$取$x_a$时的类别熵为 \begin{aligned} H(Y | X = x_a) = - \sum_k p(Y = c_k | X = x_a) \log p(Y = c_k | X = x_a) = - \sum_k \frac{|c_{ak}|}{|x_a|} \log \frac{|c_{ak}|}{|x_a|} \end{aligned}那么 \begin{aligned} H(Y | X) = \sum_a p(X = x_a) H(Y | X = x_a) = - \sum_a \frac{|x_a|}{|D|} \sum_k \frac{|c_{ak}|}{|x_a|} \log \frac{|c_{ak}|}{|x_a|} \end{aligned}信息增益为 IG(X) = H(Y) - H(Y | X)包裹方法(Wrapper)与过滤法不同的是，包裹法采用的是特征搜索的办法。它的基本思路是，从初始特征集合中不断的选择子集合，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。在搜索过程中，我们会对每个子集做建模和训练。 包裹法很大程度上变成了一个计算机问题：在特征子集的搜索问题(subset search)。我们有多种思路，最容易想到的办法是穷举(Brute-force search)，遍历所有可能的子集，但这样的方法适用于特征数较少的情形，特征一旦增多，就会遇到组合爆炸，在计算上并不可行。($N$个特征，则子集会有$x^N - 1$种可能) 另一个思路是随机化搜索，比如拉斯维加斯算法(Las Vegas algorithm)，但这样的算法在特征数大的时候，计算开销仍然很大，而且有给不出任何解的风险。所以，我们常使用的是贪心算法： 前向搜索(Forward search)在开始时，按照特征数来划分子集，每个子集只有一个特征，对每个子集进行评价。然后在最优的子集上逐步增加特征，使模型性能提升最大，直到增加特征并不能使模型性能提升为止。 后向搜索(Backward search)在开始时，将特征集合分别减去一个特征作为子集，每个子集有N—1个特征，对每个子集进行评价。然后在最优的子集上逐步减少特征，使得模型性能提升最大，直到减少特征并不能使模型性能提升为止。 双向搜索(Bidirectional search)将Forward search 和Backward search结合起来。 递归剔除(Recursive elimination)反复的训练模型，并剔除每次的最优或者最差的特征，将剔除完毕的特征集进入下一轮训练，直到所有的特征被剔除，被剔除的顺序度量了特征的重要程度。 递归特征消除(Recursive Feature Elimination, REF)一种典型的递归剔除方法，思路很简单，在sklearn中具体实现如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788def _fit(self, X, y, step_score=None): # Parameter step_score controls the calculation of self.scores_ # step_score is not exposed to users # and is used when implementing RFECV # self.scores_ will not be calculated when calling _fit through fit tags = self._get_tags() X, y = self._validate_data( X, y, accept_sparse="csc", ensure_min_features=2, force_all_finite=not tags.get('allow_nan', True), multi_output=True ) # Initialization n_features = X.shape[1] if self.n_features_to_select is None: n_features_to_select = n_features // 2 else: n_features_to_select = self.n_features_to_select if 0.0 &lt; self.step &lt; 1.0: step = int(max(1, self.step * n_features)) else: step = int(self.step) if step &lt;= 0: raise ValueError("Step must be &gt;0") support_ = np.ones(n_features, dtype=np.bool) ranking_ = np.ones(n_features, dtype=np.int) if step_score: self.scores_ = [] # Elimination while np.sum(support_) &gt; n_features_to_select: # Remaining features features = np.arange(n_features)[support_] # Rank the remaining features estimator = clone(self.estimator) if self.verbose &gt; 0: print("Fitting estimator with %d features." % np.sum(support_)) estimator.fit(X[:, features], y) # Get coefs if hasattr(estimator, 'coef_'): coefs = estimator.coef_ else: coefs = getattr(estimator, 'feature_importances_', None) if coefs is None: raise RuntimeError('The classifier does not expose ' '"coef_" or "feature_importances_" ' 'attributes') # Get ranks if coefs.ndim &gt; 1: ranks = np.argsort(safe_sqr(coefs).sum(axis=0)) else: ranks = np.argsort(safe_sqr(coefs)) # for sparse case ranks is matrix ranks = np.ravel(ranks) # Eliminate the worse features threshold = min(step, np.sum(support_) - n_features_to_select) # Compute step score on the previous selection iteration # because 'estimator' must use features # that have not been eliminated yet if step_score: self.scores_.append(step_score(estimator, features)) support_[features[ranks][:threshold]] = False ranking_[np.logical_not(support_)] += 1 # Set final attributes features = np.arange(n_features)[support_] self.estimator_ = clone(self.estimator) self.estimator_.fit(X[:, features], y) # Compute step score when only n_features_to_select features left if step_score: self.scores_.append(step_score(self.estimator_, features)) self.n_features_ = support_.sum() self.support_ = support_ self.ranking_ = ranking_ return self 可以看到在每次迭代中会删除一些特征，其中有几个重要的步骤 用模型对剩余的特征进行评估，得到每个特征相应的评分；123456789features = np.arange(n_features)[support_]estimator = clone(self.estimator)estimator.fit(X[:, features], y)# Get coefsif hasattr(estimator, 'coef_'): coefs = estimator.coef_else: coefs = getattr(estimator, 'feature_importances_', None) 根据评分，对特征进行排序12345678# Get ranksif coefs.ndim &gt; 1: ranks = np.argsort(safe_sqr(coefs).sum(axis=0))else: ranks = np.argsort(safe_sqr(coefs))# for sparse case ranks is matrixranks = np.ravel(ranks) 删除本次迭代中评分最低的特征1234# Eliminate the worse featuresthreshold = min(step, np.sum(support_) - n_features_to_select)support_[features[ranks][:threshold]] = Falseranking_[np.logical_not(support_)] += 1 例如将手写字每个像素作为特征维度，对其进行递归特征筛选，ranking越小的特征重要性越大12345678910111213141516171819202122232425&gt;&gt;&gt; from sklearn.svm import SVC&gt;&gt;&gt; from sklearn.datasets import load_digits&gt;&gt;&gt; from sklearn.feature_selection import RFE&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt;&gt;&gt;&gt; # Load the digits dataset... digits = load_digits()&gt;&gt;&gt; X = digits.images.reshape((len(digits.images), -1))&gt;&gt;&gt; y = digits.target&gt;&gt;&gt;&gt;&gt;&gt; # Create the RFE object and rank each pixel... svc = SVC(kernel="linear", C=1)&gt;&gt;&gt; rfe = RFE(estimator=svc, n_features_to_select=1, step=1)&gt;&gt;&gt; rfe.fit(X, y)RFE(estimator=SVC(C=1, kernel='linear'), n_features_to_select=1)&gt;&gt;&gt; ranking = rfe.ranking_.reshape(digits.images[0].shape)&gt;&gt;&gt;&gt;&gt;&gt; # Plot pixel ranking... plt.matshow(ranking, cmap=plt.cm.Blues)&lt;matplotlib.image.AxesImage object at 0x0000019E756CDE48&gt;&gt;&gt;&gt; plt.colorbar()&lt;matplotlib.colorbar.Colorbar object at 0x0000019E755994E0&gt;&gt;&gt;&gt; plt.title("Ranking of pixels with RFE")Text(0.5, 1.05, 'Ranking of pixels with RFE')&gt;&gt;&gt; plt.show() 拉斯维加斯方法(Las Vegas Wrapper, LVW)LVW(Las Vegas Wrapper)是一种典型的包裹式特征选择方法，它在拉斯维加斯方法框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则。 LV随机算法拉斯维加斯算法是一种基于随机策略的算法设计方法，与蒙特卡洛方法对比有以下特点 蒙特卡罗算法：采样越多，越接近最优解(强调每一个iteration都在进步，提高的过程)； 拉斯维加斯算法：采样越多，越有可能找到最优解(强调直接想要最优解)。 假设随即搜索中，找到问题的解的概率是$p$，那么每次搜索错误的概率是$1 - p$。经过$n$次搜索命中正确结果的概率，也就是1减去$n$次都未命中的概率，即 \lim_{n \rightarrow \infty} \left( 1 - (1 - p)^n \right) = 1其算法框架可以表示为1234567void Obstinate(InputType x, OutputType &amp;y;)&#123; // 反复调用拉斯维加斯算法LV(x, y)，直到找到问题的一个解 bool success = false; while (!success) success = LV(x,y);&#125; 例如用LV方法解决$N$皇后问题1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include&lt;iostream&gt;using namespace std;class Queen&#123; friend bool nQueen(int);private: bool Place(int k); //测试皇后K置于x[k]列的合法性 bool Backtrack(int t); //解n后问题的回溯法 bool QueenLV(int stopVegas); //随机放置n个皇后的拉斯维加斯算法 int n, *x, *y; &#125;;bool Queen::Place(int k)&#123; for(int j = 1; j &lt; k; j++)//第k个皇后是否跟前面的皇后冲突 if((abs(k - j) == abs(x[j] - x[k])) || (x[j] == x[k])) return false; return true;&#125;bool Queen::Backtrack(int t)&#123; if(t &gt; n)&#123; //存放皇后放置的位置 for(int i = 1; i &lt;= n; i++) y[i] = x[i]; return true; &#125; else &#123; for(int i = 1; i &lt;= n; i++)&#123; x[t] = i;//t皇后放在第i列 if(Place(t) &amp;&amp; Backtrack(t+1)) return true; &#125; &#125; return false;&#125;bool Queen::QueenLV(int stopVegas)&#123; //随机放置n个皇后的拉斯维加斯算法 int k = 1;//随机数产生器 int count = 1; //1&lt;=stopVagas=&lt;n表示允许随机放置的皇后数 while((k &lt;= stopVegas) &amp;&amp; (count &gt; 0))&#123; count = 0; for(int i = 1; i &lt;= n; i++)&#123; x[k] = i; if(Place(k)) y[count++] = i; &#125; if(count &gt; 0) //如果能放置，则在这么多个能放置第k个皇后的位置中选择一个位置 x[k++] = y[rand() % count]; &#125; return(count&gt;0);//count&gt;0表示放置成功 &#125;bool nQueen(int n)&#123; //与回溯法相结合的接n后问题的拉斯维加斯算法 Queen X; X.n = n; int *p = new int[n+1]; int *q = new int[n+1]; for(int i = 0; i &lt;= n; i++)&#123; p[i] = 0; q[i] = 0; &#125; X.y = p; X.x = q; int stop = 5; if(n &gt; 15) stop = n - 15; bool found = false; while(!X.QueenLV(stop)); //直到能放置 //算法的回溯搜索部分 if(X.Backtrack(stop + 1))&#123; for(int i = 1; i &lt;= n; i++) cout &lt;&lt; p[i] &lt;&lt; " "; found = true; &#125; cout &lt;&lt; endl; delete [] p; delete [] q; return found; &#125;int main()&#123; int n; cout &lt;&lt; "n: "; cin &gt;&gt; n; if(!nQueen(n)) cout &lt;&lt; "无解" &lt;&lt; endl; return 0; &#125; LVW特征选择设数据集是$D$，特征集是$A$，LVW每次从特征集$A$中随机产生一个特征子集$A’$，用交叉验证的方法估计学习器在$A’$上的误差，在以下情况保留$A’$ 该误差小于之前获得的最小误差； 与之前最小误差相当，但包含更少的特征 由于LVW算法每次评价子集$A′$时，都需要重新训练学习器，计算开销很大，设置参数$T$控制停止条件。但当特征数很多(即$|A|$很大)并且$T$设置得很大时，可能算法运行很长时间都不能停止。 \begin{aligned} 输入: & 数据集D; \\ & 特征集A; \\ & 学习算法L; \\ & 停止条件控制参数T; \\ 过程:\\ & \begin{aligned} & E^* = \infty; \\ & A^* = A; \\ & d^* = |A|; \\ & t = 0; \\ & while \quad t < T \quad do \\ & \qquad \begin{aligned} & randomly \quad choose \quad A'; \\ & d' = |A'|; \\ & E' = CrossValidation(L(D^{A'})) \\ & if \quad (E' < E^*) \vee [ (E' \approx E^*) \wedge (d' < d^*) ] \quad then \\ & \qquad \begin{aligned} & t = 0; \\ & E^* = E'; \\ & d^* = d'; \\ & A^* = A'; \end{aligned} \\ & else \\ & \qquad t = t + 1; \\ \end{aligned} \\ & \qquad endif \end{aligned} \\ & end \quad while \\ 输出: & 特征子集A^* \end{aligned}嵌入方法(Embedded)过滤法与学习器没有关系，特征选择只是用统计量做筛选，而包裹法则固定了学习器，特征选择只是在特征空间上进行搜索。而嵌入法最大的突破在于，特征选择会在学习器的训练过程中自动完成。 惩罚项方法(Regularization)线性回归的最小二乘估计和贝叶斯估计给定样本集合${(X^{(i)}, y^{(i)}), i = 1, \cdots, N, X^{(i)} \in R^d, y^{(i)} \in R}$，增广后组成样本矩阵$X_{N \times (d + 1)}$，线性回归模型参数为$w \in R^{d + 1}$，那么有 \hat{Y} = Xw 基于最小二乘估计，定义线性回归模型的优化目标 w^* = \argmax_w \frac{1}{N} || Y - Xw ||_2^2 基于贝叶斯估计，假定参数$w_j$服从分布$p(w_j)$，其优化目标为 \begin{aligned} w^* & = \argmax_w \log \left( \prod_i p(y^{(i)} | X^{(i)}, w) p(w) \right) \\ & = \argmax_w \sum_i \left( \log p(y^{(i)} | X^{(i)}, w) + \log p(w) \right) \end{aligned} 在线性回归模型中，假定$y \sim N(w^T x, \delta)$(本质是高斯模型)，那么有 \log p(y^{(i)} | X^{(i)}, w) \propto (y^{(i)} - w^T x^{(i)})^2 另外，假定参数维度间独立，即 p(w) = \prod_j p(w_j) LASSO(L1)LASSO是指Least Absolute Shrinkage and Selection Operator，是采用$L1$正则化的线性回归方法。添加$L1$正则项，构成LASSO优化目标 w^* = \argmax_w \frac{1}{N} || Y - Xw ||_2^2 + \lambda || w ||_1该问题等价于 \begin{aligned} w^* & = \argmax_w \frac{1}{N} || Y - Xw ||_2^2 \\ s.t. & \qquad || w ||_1 \leq t(常数) \end{aligned} 用拉格朗日乘子法进行求解时，可以发现两个优化问题等价 L(w, \lambda) = \frac{1}{N} || Y - Xw ||_2^2 + \lambda(|| w ||_1 - t) 从贝叶斯估计角度考虑，假定参数$w$服从拉普拉斯分布 \begin{aligned} & p(w_j) & = \frac{1}{2 \alpha} \exp(- \frac{|w_j|}{\alpha}) \\ \Rightarrow \quad & \log p(w) & = - \sum_j \left( \frac{|w_j|}{\alpha} + \log(2 \alpha) \right) \end{aligned}那么可以看到是等价的 \begin{aligned} w^* & = \argmax_w \sum_i \left( \log p(y^{(i)} | X^{(i)}, w) + \log p(w) \right) \\ & = \argmax_w \sum_i \left( (y^{(i)} - w^T x^{(i)})^2 - \sum_j ( \frac{|w_j|}{\alpha} + \log(2 \alpha) ) \right) \end{aligned} 其几何意义如下，入了正则化项，相当于是对参数施加了约束，其中L1正则化将参数限制在一个菱形中，，从图中可以看出，L1正则化施加的约束会使得最优值在菱形顶点处取得，也就是说很多参数取值为$0$，得到稀疏的参数估计结果 123456789101112131415&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.linear_model import Lasso&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; clf = Lasso()&gt;&gt;&gt; clf.fit(X, y)Lasso()&gt;&gt;&gt;&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 1) 又如在SVM中增加$L1$正则项12345678910111213&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.svm import LinearSVC&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)&gt;&gt;&gt; model = SelectFromModel(lsvc, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)&gt;&gt;&gt;&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 3) Ridge(L2)在线性回归优化目标中添加$L2$正则项，得到Ridge优化目标 w^* = \argmax_w \frac{1}{N} || Y - Xw ||_2^2 + \lambda || w ||_2^2该问题等价于 \begin{aligned} w^* & = \argmax_w \frac{1}{N} || Y - Xw ||_2^2 \\ s.t. & \qquad || w ||_2^2 \leq t(常数) \end{aligned} 用拉格朗日乘子法进行求解时，可以发现两个优化问题等价 L(w, \lambda) = \frac{1}{N} || Y - Xw ||_2^2 + \lambda(|| w ||_2^2 - t) 但是在求取梯度时，$L1$范数不可导，这就带来求解上的问题，后面介绍的LARS可以近似计算LASSO。 从贝叶斯估计角度考虑，假定参数$w$服从高斯分布 \begin{aligned} & p(w_j) & = \frac{1}{\sqrt{2 \pi \alpha}} \exp(- \frac{w_j^2}{2 \alpha}) \\ \Rightarrow \quad & \log p(w) & = - \sum_j \left( \frac{w_j^2}{2 \alpha} + \frac{1}{2} \log(2 \pi \alpha) \right) \end{aligned}那么可以看到是等价的 \begin{aligned} w^* & = \argmax_w \sum_i \left( \log p(y^{(i)} | X^{(i)}, w) + \log p(w) \right) \\ & = \argmax_w \sum_i \left( (y^{(i)} - w^T x^{(i)})^2 - \sum_j ( \frac{w_j^2}{2 \alpha} + \frac{1}{2} \log(2 \pi \alpha) ) \right) \end{aligned} 其几何意义如下，入了正则化项，相当于是对参数施加了约束，其中L2正则化将参数限制在一个圆中，会使得权重趋向于零(Weight Decay)，并倾向于在相关特征之间均匀分布权重 由于参数$\lambda$并不是唯一确定的，所以得到的$\hat{w}(\lambda)$是回归参数$w$的一个估计族，$\hat{w}_{j}(\lambda) - \lambda$图称为岭迹图，根据以下规则可以筛选有效特征 当$\lambda = 0$时为最小二乘估计，参数$\hat{w}_{j}$不应趋向无穷； 当不存在奇异时，岭迹应稳定渐进趋向$0$； 通过岭迹图可以剔除变量解决多重共线性问题(个别情形下适用) 可以剔除掉标准化岭回归系数比较稳定且绝对值很小的自变量; 随着$\lambda$的增加，回归系数不稳定，震荡趋于零的自变量也可以剔除; 如果依照上述去掉变量的原则，有若干个回归系数不稳定，究竟去掉几个，去掉哪几个，这幵无一般原则可循，这需根据去掉某个变量后重新进行岭回归分析的效果来确定。 一些岭迹图 通过岭迹图选择参数$\lambda$的原则 各回归系数的岭估计基本稳定(如正负)； 用最小二乘估计时符号不合理的回归系数，其岭估计的符号变得合理； 回归系数没有不合乎实际意义的绝对值； 残差平方和增大不太多。 还有方差扩大因子法，注意不同方法建议的选择可能不一致。 存在的问题 岭参数$\lambda$计算方法太多且差异很大； 用岭迹图进行变量筛选，随意性太大，且只能一定程度消除多重共线性，而不能解决其他问题； 岭回归返回的模型若没有经过特征筛选，包含全部变量。 123456789101112131415&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.linear_model import RidgeClassifier&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; clf = RidgeClassifier()&gt;&gt;&gt; clf.fit(X, y)RidgeClassifier()&gt;&gt;&gt;&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 2) Least Angel Regression(LARS)最小角回归(Least Angel Regression, LARS)是一种求解线性回归的方法。线性回归模型可以表示为 Y_{N \times 1} = X_{N \times (d + 1)} w_{(d + 1) \times 1}根据矩阵乘法，可以将$Y$看作是所有特征$X_j, j = 1, \cdots, d+1$的线性组合，其特征空间内的基向量为${X_1, \cdots, X_{d+1}}$ Y = \sum_{j=1}^{d+1} w_j X_j残差$R$定义为 R = Y - \hat{Y}以$2$个样本为例($X_j$维度与样本数有关，再高无法可视化)，在图中表示为 其中 $e_1, e_2, e_3$表示自然基； $x_1, x_2, (2 \times 1)$为每个维度上的特征； $w_1, w_2$为每个维度上的回归系数； $y, y’$分别表示真实值和回归值； $r$表示残差。 可以看到$R$与$X_j, j = 1, \cdots, d + 1$都是垂直的，那么线性回归的优化的实质，从几何角度解释，是找到残差向量$R$与$X_j, j = 1, \cdots, d + 1$垂直，也即线性无关。 基于以上几何意义的解释，就可以介绍LARS算法了，它的基本思想是：线性回归的求解过程，可以看作是$y$从$O(0, 0)$出发，向$\hat{Y}$逐步接近的过程，那么现在的问题就是如何选择路径。 首先介绍相关系数的几何意义，将$A, B$两个向量去中心化、单位化，即 \begin{cases} \hat{a}_i = (a_i - \overline{A}) / \sigma_A \\ \hat{b}_i = (b_i - \overline{B}) / \sigma_B \end{cases}那么相关系数也即这两个向量的余弦距离 r_{AB} = \frac{\sum_{i=1}^n (a_i - \overline{A})(b_i - \overline{B})}{\sigma_A \sigma_B} = \frac{1}{n} \sum_{i=1}^n \hat{a}_i \hat{b}_i = \hat{A} \cdot \hat{B}= \cosLAR具体算法如下，是一种线性的方法，求解结果与Lasso结果几乎一致(并不完全一致，但近似相同) 初始化：计算输出向量$Y$和属性向量$X_j, j = 1, \cdots. (n + 1)$的相关系数$r_j$，并按相关系数从大到小将属性排序； 第一个特征选择：从原点开始沿着相关系数最大的属性$X_1$游走，得到的向量作为预测输出$\hat{Y}$，残差对应为$Y - \hat{Y}$，那么在这个过程中残差与$X_1$的相关性降低r_1' = (Y - \hat{Y}) \cdot X_1 剩余特征加入：当步骤2中相关系数降低至存在特征$X_2$与残差的相关系数与$r_j’$相等时，即游走到$w_1 X_1$时，将该特征加入，开始沿着$w_1 X_1, X_2 - w_1 X_1$两向量的角平分线$(w_1 X_1 + X_2’)$游走，其中$X_2’ = X_2 - w1 X_1$； 重复步骤2，直至所有加入的特征与残差$Y - \hat{Y}$相关系数小于指定的较小常数$\epsilon$； 此时剩余的特征被丢弃，达到特征筛选的作用。 如下图，当残差为$R_2$时，$R_2$几乎与$X_1, X_2$垂直，相关系数为$0$，停止游走，特征$X_3$被丢弃。 123456789101112131415&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.linear_model import Lars&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; clf = Lars()&gt;&gt;&gt; clf.fit(X, y)Lars()&gt;&gt;&gt;&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 1) 12345678910111213141516&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.linear_model import LassoLars&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; clf = LassoLars()&gt;&gt;&gt; clf.fit(X, y)LassoLars()&gt;&gt;&gt;&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)C:\Apps\Anaconda3\envs\PR\lib\site-packages\sklearn\feature_selection\_base.py:81: UserWarning: No features were selected: either the data is too noisy or the selection test too strict. UserWarning)&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 0) 基于树的方法(Tree-based)在决策树训练过程中，就存在选择最优分裂特征的问题，这可以用于评估特征重要性(不进行详细介绍)。 决策树(Decision Tree)123456789101112131415161718&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; clf = DecisionTreeClassifier()&gt;&gt;&gt; clf.fit(X, y)DecisionTreeClassifier()&gt;&gt;&gt;&gt;&gt;&gt; clf.feature_importances_array([0.01333333, 0.01333333, 0.55072262, 0.42261071])&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)&gt;&gt;&gt;&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 2) 随机森林(Random Forest, RF)12345678910111213141516171819&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; clf = RandomForestClassifier()&gt;&gt;&gt; clf.fit(X, y)RandomForestClassifier()&gt;&gt;&gt;&gt;&gt;&gt; clf.feature_importances_array([0.11294047, 0.01823962, 0.43942899, 0.42939091])&gt;&gt;&gt;&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)&gt;&gt;&gt;&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 2) 梯度提升树(GBDT)123456789101112131415161718&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier&gt;&gt;&gt; from sklearn.feature_selection import SelectFromModel&gt;&gt;&gt;&gt;&gt;&gt; X, y = load_iris(return_X_y=True)&gt;&gt;&gt; clf = GradientBoostingClassifier()&gt;&gt;&gt; clf.fit(X, y)GradientBoostingClassifier()&gt;&gt;&gt;&gt;&gt;&gt; clf.feature_importances_array([0.00649375, 0.01077262, 0.31215818, 0.67057545])&gt;&gt;&gt;&gt;&gt;&gt; model = SelectFromModel(clf, prefit=True)&gt;&gt;&gt; X_new = model.transform(X)&gt;&gt;&gt; X.shape(150, 4)&gt;&gt;&gt; X_new.shape(150, 2) Reference 用遗传算法进行特征选择 - CSDN 特征选择之经典三刀 - 知乎 机器学习中，有哪些特征选择的工程方法？ - 知乎 结合Scikit-learn介绍几种常用的特征选择方法 - cnblogs 1.13. Feature selection - scikit learn FeatureLabs/featuretools - Github Yimeng-Zhang/feature-engineering-and-feature-selection - Github 随机化算法(4) — 拉斯维加斯(Las Vegas)算法 - http://www.wutianqi.com 从Lasso开始说起 - 知乎]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python函数静态变量]]></title>
    <url>%2F2020%2F06%2F08%2FPython%E5%87%BD%E6%95%B0%E9%9D%99%E6%80%81%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[目录 目录 Reference Python函数无法通过static关键词定义静态变量，如C/C++中123456void foo()&#123; static int counter = 0; counter++; printf("counter is %d\n", counter);&#125; 注意到Python的特性，函数也是一种对象12345678910&gt;&gt;&gt; def f(x): ... return...&gt;&gt;&gt; print(f.attr)Traceback (most recent call last):File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'function' object has no attribute 'attr'&gt;&gt;&gt; f.attr = True&gt;&gt;&gt; print(f.attr)True 可以通过装饰器，将变量绑定到函数属性中123456def static_vars(**kwargs): def decorate(func): for k in kwargs: setattr(func, k, kwargs[k]) return func return decorate 在定义函数时，绑定属性1234@static_vars(counter=0)def foo(): foo.counter += 1 print("Counter is %d" % foo.counter) 成功1234&gt;&gt;&gt; foo()Counter is 1&gt;&gt;&gt; foo()Counter is 2 Reference What is the Python equivalent of static variables inside a function?]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Least Recently Used Cache]]></title>
    <url>%2F2020%2F06%2F05%2FLeast-Recently-Used-Cache%2F</url>
    <content type="text"><![CDATA[目录 目录 原理介绍 算法设计 put(key, value) get(key) 注意 代码实现 基于双向链表和字典 基于有序字典OrderedDict Reference 原理介绍最近最少使用缓存(Least Recently Usd Cache, LRU Cache)是一种常用的页面置换算法，其设计原则是：如果一个数据在最近一段时间内没有被访问到，那么将来它被访问的几率也很小，那么当限定空间已满时再次存入数据，应当把最久没有访问到的数据删除以提供空间。 应当支持以下操作 获取数据get(key)(查)：若缓存中存在key，那么获取数据值，否则返回空； 写入数据set(key, value)(添、改)：若key不存在缓存中，则写入键值对，当缓存达到上限，需要在写入新数据前删除最近最少使用的数据用于腾出空间。 例如1234567lru = LRUCache(2)lru.set(2, 1)lru.set(1, 1)lru.get(2) # 输出1lru.set(4, 1) # 键值对(1, 1)被删除lru.get(1) # 输出空lru.get(2) # 输出1 如果用字典实现 在每次读写键key时，查找的时间复杂度是$O(1)$； 需要对key计数，也就说每个键都必须新增计数变量； 在缓存已满时删除访问量最少的键值对，需要$O(n)$的时间复杂度找到该键值对。 考虑用链表实现 在每次读写key时，查找的时间复杂度是$O(n)$； 若查找成功，将对应保存该键值对的节点移动到链表头部，以实现根据访问量排序的功能； 在缓存已满时删除访问量最少的键值对，删除链表结尾的元素： 若采用单向链表，删除最后一个节点时，需要进行遍历，以确定最后一个节点及其前驱节点的地址，时间复杂度是$O(n)$； 采用双向链表，并且保存尾节点地址，那么可以直接获取上述两个节点的地址，删除的时间复杂度是$O(1)$。 经上述分析，以上两种方式实现的LRU Cache均不能实现$O(1)$的查找和删除，那么有什么办法改进呢？考虑用字典辅助双向链表的形式实现，即哈希链表 键值对保存在双向链表中，以实现键值对按访问量排序，快速删除最近最少使用的键值对； 关于链表为什么不只保存值，还需保存键：因为在删除最不常用键值对时，同时需要删除字典中对应的键值对，这就需要在节点中保存键以达到反向查找的目的。 将链表地址映射到字典中，以实现快速查找键值对。 算法设计put(key, value)实现添和改，首先在字典中索引key对应的链表节点 若存在于字典中，那么在该节点中替换value，并将其移动到链表头部； 若不存在，考虑此时双向链表是否已满 若未满，在链表头部创建新节点保存value，并将该节点的地址映射到字典中； 若已满，删除链表的尾节点，同时删除字典中该键(这就需要链表元素同时保存键值对)，然后再在连边头部创建新节点保存value，并将该节点的地址映射到字典中； get(key)实现查，首先在字典中索引key对应的链表节点 若存在于字典中，返回该节点对应的值value，并将其移动到链表头部； 若不存在，返回空即可。 注意在实现过程中，注意以下几点 注意双向链表的头尾指针的维护，以及相应的指针操作； 在对某节点操作(添、查、改)后，需要将节点移动到链表头部； 在删除链表节点时，同时需要删除字典中对应的键值对； 注意capcity为$0, 1$及以上三种情况。 代码实现基于双向链表和字典双向链表主要实现两个功能 append：将节点添加到头部； tohead：将节点移动到链表头部。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class ListNode: def __init__(self, value): self.prev = None self.next = None self.value = valueclass DoublyLinkedList: def __init__(self): self.head = None self.tail = None def append(self, node): """ 将节点添加到头部 """ if self.head is None: # 链表为空 self.head = self.tail = node else: # 链表非空，添加到头部 node.prev = None node.next = self.head self.head.prev = node self.head = node def tohead(self, node): """ 移动节点到头部 """ if node is None: return # 已经是头节点 if node.prev is None: return # 断开节点 node.prev.next = node.next if node.next: node.next.prev = node.prev else: # 当该节点为尾节点，需要更新尾节点 self.tail = node.prev self.tail.next = None # 移动到头部 node.prev = node.next = None self.append(node) def printlist(self): node = self.head while node: print(node.value, end='-&gt;') node = node.next print() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class LRUCache: def __init__(self, capcity): self.capcity = capcity self.list = DoublyLinkedList() self.dict = dict() def __len__(self): return len(self.dict) def put(self, key, value): """ 添、改 """ if self.capcity == 0: return # 在字典中查找key对应的节点 node = self.dict.get(key, None) # 不存在 if node is None: # 满了 if len(self) &gt;= self.capcity: tail = self.list.tail # 删除字典中的键 k = tail.value[0] self.dict.pop(k) # 反向查找键并删除 # 删除尾节点 if tail.prev: tail.prev.next = None self.list.tail = tail.prev else: self.list.head = self.list.tail = None del tail # 新建节点并添加到链表和字典 node = ListNode([key, value]) self.list.append(node) self.dict[key] = node # 存在 else: # 修改值，并移动到头部 node.value[-1] = value self.list.tohead(node) def get(self, key): """ 查 """ # 在字典中查找key对应的节点 node = self.dict.get(key, None) if node is None: return None # 移动到头部 self.list.tohead(node) # 获取对应节点的值 return node.value[-1] 测试如下1234567891011121314151617181920212223if __name__ == "__main__": # lru = LRUCache(0) # lru = LRUCache(1) for c in range(4): lru = LRUCache(c) lru.put(1, 1) lru.list.printlist() lru.put(2, 2) lru.list.printlist() lru.get(1) lru.list.printlist() lru.put(3, 3) lru.list.printlist() lru.get(2) lru.list.printlist() lru.get(3) lru.list.printlist() print('-----------') 链表打印输出如下12345678910111213141516171819202122232425262728-----------[1, 1]-&gt;[2, 2]-&gt;[2, 2]-&gt;[3, 3]-&gt;[3, 3]-&gt;[3, 3]-&gt;-----------[1, 1]-&gt;[2, 2]-&gt;[1, 1]-&gt;[1, 1]-&gt;[2, 2]-&gt;[3, 3]-&gt;[1, 1]-&gt;[3, 3]-&gt;[1, 1]-&gt;[3, 3]-&gt;[1, 1]-&gt;-----------[1, 1]-&gt;[2, 2]-&gt;[1, 1]-&gt;[1, 1]-&gt;[2, 2]-&gt;[3, 3]-&gt;[1, 1]-&gt;[2, 2]-&gt;[2, 2]-&gt;[3, 3]-&gt;[1, 1]-&gt;[3, 3]-&gt;[2, 2]-&gt;[1, 1]-&gt;----------- 基于有序字典OrderedDict实际上Python标准库内OrderedDict即哈希链表，使用一个双链表来动态维护一个字典的key的顺序，那么123456789101112131415161718192021222324from collections import OrderedDictclass LRUCache: def __init__(self, capcity): self._ordered_dict = OrderedDict() self._capcity = capcity def _move_to_end_if_exist(self, key): try: self._ordered_dict.move_to_end(key) except KeyError: pass def get(self, key): self._move_to_end_if_exist(key) return self._ordered_dict.get(key, None) def put(self, key, value): self._ordered_dict[key] = value self._move_to_end_if_exist(key) if len(self._ordered_dict) &gt; self._capcity: self._ordered_dict.popitem(last=False) Reference 134. LRU缓存策略 - lintcode]]></content>
  </entry>
  <entry>
    <title><![CDATA[Model Ensemble: Bagging, Boosting, Stacking and Blending]]></title>
    <url>%2F2020%2F06%2F02%2FModel-Ensemble-Bagging-Boosting-Stacking-and-Blending%2F</url>
    <content type="text"><![CDATA[目录 目录 模型集成 原理 策略 平均法 投票法 学习法 Bagging 自助采样法 随机森林 Boosting AdaBoost 分类 回归：AdaBoost R2 多分类：SAMME 带概率输出的多分类：SAMME.R Blending Stacking 多样性 误差-分歧分解(Error-Ambiguity Decomposition) 多样性度量 多样性增强 Reference 模型集成原理集成学习有以下优点 学习任务假设空间很大时，可能有多个假设在训练集上达到同等性能，此时单学习器可能导致泛化能力不佳； 单个学习器可能陷入局部极小值，组合多个学习器可以降低陷入局部极小的风险； 组合多个学习器可以扩大假设空间，得到更好的近似。 策略对于$M$个基学习器$h_1, \cdots, h_M$，可以通过三种策略集成结果：平均法、投票法、学习法。 平均法通常用于回归任务中 简单平均法：求取学习器输出的均值作为输出 H(x) = \frac{1}{M} \sum_{m=1}^M h_m(x) 加权平均法：学习权重$w_m$，加权组合各学习器的输出 \begin{aligned} H(x) = \frac{1}{M} \sum_{m=1}^M w_m h_m(x) \\ w_m \geq 0, \sum_m w_m = 1 \end{aligned}加权平均会引入较多参数，对于规模较大的集成学习容易出现过拟合，不一定优于简单平均法，适合个体学习器性能相差较大的场景。 投票法通常用于分类任务中 绝大多数投票：若某个标记得票数过半，则预测为该标记。可能存在所有标记都未过半的问题，该方法不常用。 相对多数投票：取得票最多的标记作为预测，即H(x) = \arg \max_{c_j} \sum_{m = 1}^M I(h_m(x) = c_j) 加权投票：类似加权平均法H(x) = \arg \max_{c_j} \sum_{m = 1}^M w_i I(h_m(x) = c_j) 学习法通过另一个学习器(次级学习器或元学习器meta learner)来组合个体学习器(初级学习器)的分类结果。用初级学习器产生次级数据集，用作次级学习器的训练，Stacking是一种典型的学习法集成。 BaggingBagging的基本流程是 经过$M$次自助采样，得到$M$个包含$N$个训练样本的采样集； 基于每个训练集训练得到各个基学习器； 将这$M$个基学习器进行组合(如投票法)，得到集成模型。 Bagging主要关注降低方差，能够平滑强学习器的方差，非剪枝决策树、神经网络等易受样本扰动的学习器。 自助采样法Bagging基于自助采样法(Bootstrap Sampling)。具体操作为，给定包含$N$个样本的数据集，有放回地进行$N$次随机采样，得到包含$N$个样本的采样集。注意到采样集中某些样本可能多次出现，也存在样本未出现。在某次抽样中样本$x_i$被采样到的概率是$1/N$，经过$N$次采样，不出现在采样集中的概率为 (1 - \frac{1}{N})^N那么当样本集数目趋于无穷时，有 \lim_{N \rightarrow \infty} (1 - \frac{1}{N})^N = \frac{1}{e} \approx 0.368因此原训练集中有大约有$62.8\%$的样本出现在了采样集中。那么每个基学习器只用初始训练集中$62.8\%$的样本来训练，剩余$36.8\%$的样本可用作验证集来对泛化性能进行包外估计。 随机森林随机森林(Random Forest)是Bagging的变体。以决策树作为基学习器，在每棵决策树训练过程中，除了自助采样引入随机样本扰动，还在某节点划分属性时，从所有属性($K$个)中选择一个包含$k$个属性的子集，从中选择最优属性用于划分，从而引入随机属性扰动。如果$k=n$，即传统决策树，通常选择$k = \log_2 n$。 随机森林的优点是 可以用于高维数据，无需降维或特征选择； 可以判断特征的重要程度，用于特征选择； 可以得到特征间的相互关系； 不易过拟合； 决策树实现简单，且可以并行训练，训练速度快； 对于不平衡数据集，可以平衡误差； 可以有效应对特征缺失问题。 缺点是： 随着树的数目增加，随机森林可以有效缓解过拟合，模型的方差会显著降低，但是不会纠正偏差，在噪音较大的分类或回归问题上会过拟合； 取值划分较多的属性会对随机森林产生很大影响，所以属性权值是不可信的。 Boosting提升方法(Boosting)的基本思想是：通过调整训练样本的权重，训练多个学习器，并将学习器组合得到更好的能力。其理论基础是强可学习和弱可学习是等价的，即弱学习算法可以通过某些方法提升为强学习算法。 其基本流程是 从初始训练集训练得到一个基学习器； 根据基学习器表现对训练样本权重进行调整，使分类错误的样本得到更多关注； 用调整权重后的数据集训练下一个基学习器； 重复直至得到$M$个基学习器，进行加权组合得到集成学习器。 AdaBoost分类给定训练数据集$D = { (X^{(i)}, y^{(i)}), i = 1, \cdots, N}, X^{(i)} \in \mathcal{R}^n, y^{(i)} \in { -1, +1 }$ 初始化训练数据的权值分布$W_1 = \begin{bmatrix} w_1^{(1)}, \cdots, w_1^{(N)} \end{bmatrix}^T, w_1^{(i)} = 1 / N$； 对于$m = 1, \cdots, M$ 用权值分布$W_m$评估数据集，训练得到弱学习算法的基本分类器$h_m$； 计算分类器$h_m$在训练数据集上的分类误差率如下，即所有误分类的加权和，若$e_m \geq 1 / 2$，算法终止； e_m = \sum_i w_m^{(i)} I(h_m(X^{(i)}) \neq y^{(i)}) \in (0， 1/2) 计算分类器$h_m$的权重系数如下，评估该分类器在集成模型中的重要性，注意到该函数为单调递减函数，说明误差越小的分类器权重越大； \alpha_m = \frac{1}{2} \log \frac{1 - e_m}{e_m} \in (- \infty, 0) 更新数据集的权重分布$W_{m+1} = \begin{bmatrix} w_{m+1}^{(1)}, \cdots, w_{m+1}^{(N)} \end{bmatrix}^T$ z_m^{(i)} = w_m^{(i)} \exp \left( - \alpha_m y^{(i)} h_m(X^{(i)}) \right)w_{m+1}^{(i)} = z_m^{(i)} / \sum_j z_m^{(j)}注意：分类正确时，$w_{m+1}^{(i)} = w_m^{(i)} \exp(-\alpha_m) / \sum_j z_m^{(j)}$，分类错误时，$w_{m+1}^{(i)} = w_m^{(i)} \exp(+ \alpha_m) / \sum_j z_m^{(j)}$，因此错误分类样本权重是正确分类样本权重的$\exp(2 \alpha_m)$倍。 基本分类器线性组合得到集成模型 H(X) = \text{sign} (\sum_{m} \alpha_m h_m(X)) 回归：AdaBoost R2AdaBoost用于回归任务的一种推广。给定训练数据集$D = { (X^{(i)}, y^{(i)}), i = 1, \cdots, N}, X^{(i)} \in \mathcal{R}^n, y^{(i)} \in \mathcal{R}$ 初始化训练数据的权值分布$W_1 = \begin{bmatrix} w_1^{(1)}, \cdots, w_1^{(N)} \end{bmatrix}^T, w_1^{(i)} = 1 / N$； 对于$m = 1, \cdots, M$ 用权值分布$W_m$评估数据集，训练得到弱学习算法的基本回归器$h_m$； 计算回归器$h_m$在训练数据集上的分类误差率如下，即所有回归误差的加权和 e_m^{(i)} = y^{(i)} - h_m(X^{(i)})E_m = \max_i | e_m^{(i)} |e_m = \sum_i w_m^{(i)} (e_m^{(i)} / E_m)^2 计算回归器$h_m$的权重系数如下，评估该回归器在集成模型中的重要性，注意到该函数为单调递减函数，说明误差越小的回归器权重越大； \alpha_m = \frac{e_m}{1 - e_m} 更新数据集的权重分布$W_{m+1} = \begin{bmatrix} w_{m+1}^{(1)}, \cdots, w_{m+1}^{(N)} \end{bmatrix}^T$ z_m^{(i)} = w_m^{(i)} \alpha_m^{1 - (\frac{y^{(i)} - h_m(X^{(i)})}{E_m})^2}w_{m+1}^{(i)} = z_m^{(i)} / \sum_j z_m^{(j)} 基本回归器线性组合得到集成模型 H(X) = \sum_{m} (\ln \frac{1}{\alpha_m}) h_m(X) 多分类：SAMMEAdaBoost在多分类任务中的推广，当$K=2$时退化为AdaBoost。给定训练数据集$D = { (X^{(i)}, y^{(i)}), i = 1, \cdots, N}, X^{(i)} \in \mathcal{R}^n, y^{(i)} \in { c_1, \cdots, c_K }$ 初始化训练数据的权值分布$W_1 = \begin{bmatrix} w_1^{(1)}, \cdots, w_1^{(N)} \end{bmatrix}^T, w_1^{(i)} = 1 / N$； 对于$m = 1, \cdots, M$ 用权值分布$W_m$评估数据集，训练得到弱学习算法的基本分类器$h_m$； 计算分类器$h_m$在训练数据集上的分类误差率如下，即所有误分类的加权和，若$e_m \geq (K - 1) / K$，算法终止； e_m = \sum_i w_m^{(i)} I(h_m(X^{(i)}) \neq y^{(i)}) 计算分类器$h_m$的权重系数如下，评估该分类器在集成模型中的重要性，注意到该函数为单调递减函数，说明误差越小的分类器权重越大； \alpha_m = \frac{1}{2} \log \frac{1 - e_m}{e_m} + \log (K - 1) 更新数据集的权重分布$W_{m+1} = \begin{bmatrix} w_{m+1}^{(1)}, \cdots, w_{m+1}^{(N)} \end{bmatrix}^T$ z_m^{(i)} = w_m^{(i)} \exp \left( - \alpha_m y^{(i)} h_m(X^{(i)}) \right)w_{m+1}^{(i)} = z_m^{(i)} / \sum_j z_m^{(j)} 基本分类器线性组合得到集成模型 H(X) = \arg \max_k (\sum_m \alpha_m I \left( h_m(X) = c_k \right)) 带概率输出的多分类：SAMME.R给定训练数据集$D = { (X^{(i)}, y^{(i)}), i = 1, \cdots, N}, X^{(i)} \in \mathcal{R}^n, y^{(i)} \in { c_1, \cdots, c_K }$ 初始化训练数据的权值分布$W_1 = \begin{bmatrix} w_1^{(1)}, \cdots, w_1^{(N)} \end{bmatrix}^T, w_1^{(i)} = 1 / N$； 对于$m = 1, \cdots, M$ 用权值分布$W_m$评估数据集，训练得到弱学习算法的基本分类器$h_m$； 计算分类器$h_m$在训练数据集上的加权概率估计，刻画其预测$X^{(i)}$的输出为类别$c_k$的概率的加权和 {p_m^{(i)}}_k = w_m^{(i)} p(y^{(i)} = c_k | X^{(i)}) 对于$h_m$和类别$c_k$，定义${l_m}_k(X^{(i)})$如下，刻画$\log {p_m^{(i)}}_k$到所有概率加权均值$\frac{1}{K} \sum_{k’} \log {p_m^{(i)}}_{k’}$的距离 {l_m (X^{(i)})}_k = (K - 1) \times \left( \log {p_m^{(i)}}_k - \frac{1}{K} \sum_{k'} \log {p_m^{(i)}}_{k'} \right) 更新数据集的权重分布$W_{m+1} = \begin{bmatrix} w_{m+1}^{(1)}, \cdots, w_{m+1}^{(N)} \end{bmatrix}^T$ z_m^{(i)} = w_m^{(i)} \exp \left( - \frac{K - 1}{K} \sum_k {\sigma^{(i)}}_k \log {p_m^{(i)}}_k \right) \sigma^{(i)}_k = \begin{cases} 1 & y^{(i)} = c_k \\ - \frac{1}{K - 1} & \text{otherwise} \end{cases}w_{m+1}^{(i)} = z_m^{(i)} / \sum_j z_m^{(j)} 基本分类器线性组合得到集成模型 H(X) = \arg \max_k (\sum_m {l_m (X^{(i)})}_k) BlendingBlending的流程如下 将数据划分为训练集trainset、验证集validset测试集testset； 创建第一层模型，这些模型可以是同质的或异质的，用训练集训练这多个模型； 用上述模型预测验证集和测试集，得到validpred和testpred1； 创建第二层模型。用validpred作为训练集训练这多个模型； 用上述模型对testpred1进行预测，作为整个测试集的结果输出testpred。 StackingStacking涉及交叉验证，流程与Blending类似，具体代码参考log0/vertebral/stacked_generalization.py 数据划分为训练集trainset和测试集testset； 创建第一层模型，这些模型可以是同质的或异质的； 上述每个模型以$K$折交叉验证的方式，分别训练 将训练集划分为$K$个子集，其中$1$个用作验证集(cvset)，其余$K-1$个用作本次交叉验证的训练集训练模型； cvset在训练得到模型上获取输出cvpred，作为第二层模型的训练集； testset在训练得到模型上获取输出testpredk，取testpredk的均值testpred1作为第二层模型的测试集； 创建第二层模型，用cvpred训练单个模型； 用上述模型对testpred1进行预测，作为整个测试集的结果输出testpred。 12345678910111213141516171819202122232425262728293031323334353637383940414243clfs = [ RandomForestClassifier(n_estimators = n_trees, criterion = 'gini'), ExtraTreesClassifier(n_estimators = n_trees * 2, criterion = 'gini'), GradientBoostingClassifier(n_estimators = n_trees),]# Ready for cross validationskf = list(StratifiedKFold(Y_dev, n_folds))# Pre-allocate the datablend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiersblend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers# For each classifier, we train the number of fold times (=len(skf))for j, clf in enumerate(clfs): print 'Training classifier [%s]' % (j) blend_test_j = np.zeros((X_test.shape[0], len(skf))) # Number of testing data x Number of folds , we will take the mean of the predictions later for i, (train_index, cv_index) in enumerate(skf): print 'Fold [%s]' % (i) # This is the training and validation set X_train = X_dev[train_index] Y_train = Y_dev[train_index] X_cv = X_dev[cv_index] Y_cv = Y_dev[cv_index] clf.fit(X_train, Y_train) # This output will be the basis for our blended classifier to train against, # which is also the output of our classifiers blend_train[cv_index, j] = clf.predict(X_cv) blend_test_j[:, i] = clf.predict(X_test) # Take the mean of the predictions of the cross validation set blend_test[:, j] = blend_test_j.mean(1)# Start blending!bclf = LogisticRegression()bclf.fit(blend_train, Y_dev)# Predict nowY_test_predict = bclf.predict(blend_test)score = metrics.accuracy_score(Y_test, Y_test_predict)print 'Accuracy = %s' % (score) 多样性误差-分歧分解(Error-Ambiguity Decomposition)假定有$M$个学习器$h_1, \cdots, h_M$，通过加权平均组合产生集成学习器$H$，即 H(x) = \sum_{m=1}^M w_m h_m(x) 对于单个样本$x$，定义学习器$h_m$的分歧为 a_m(x) = (h_m(x) - H(x))^2集成学习器$H$的分歧为 a_H(x) = \sum_{m=1}^M w_m a_m(x) = \sum_{m=1}^M w_m (h_m(x) - H(x))^2学习器$h_m$的误差为 e_m(x) = (y - h_m(x))^2那么多个学习器的误差加权平均为 e_h(x) = \sum_{m=1}^M w_m e_m(x) = \sum_{m=1}^M w_m (y - h_m(x))^2集成学习器$H$的误差为 e_H(x) = (y - H(x))^2 定义集成模型的泛化误差如下，希望将$e_H(x)$进行分解，用个体学习器的指标$E_m, A_m$表示 E = \int e_H(x) p(x) dx设$p(x)$为样本$x$的概率密度分布，定义个体学习器$h_m$在全体样本上的泛化误差与分歧项，和相应的加权平均为 E_m = \int e_m(x) p(x) dx \Rightarrow \overline{E} = \sum_{m=1}^M w_m E_mA_m = \int a_m(x) p(x) dx \Rightarrow \overline{A} = \sum_{m=1}^M w_m A_m又因为 \begin{aligned} e_h(x) - a_H(x) = \sum_{m=1}^M w_m (y - h_m(x))^2 - \sum_{m=1}^M w_m (h_m(x) - H(x))^2 \\ = \sum_{m=1}^M w_m \left[ (y - h_m(x))^2 - (h_m(x) - H(x))^2 \right] \\ = \sum_{m=1}^M w_m \left[ y^2 + \cancel{h_m^2(x)} - 2yh_m(x) - \cancel{h_m^2(x)} - H^2(x) + 2 h_m(x)H(x) \right] \\ = y^2 \cancel{\sum_{m=1}^M w_m} - 2y \sum_{m=1}^M w_m h_m(x) - H^2(x) \cancel{\sum_{m=1}^M w_m} + 2H(x) \sum_{m=1}^M w_m h_m(x) \\ = y^2 - H^2(x) + 2 \sum_{m=1}^M w_m h_m(x) (H(x) - y) \\ = y^2 - H^2(x) + 2 H(x) (H(x) - y) \\ = y^2 - H^2(x) + 2 H^2(x) - 2 y H(x) \\ = (y - H(x))^2 = e_H(x) \end{aligned}也即 e_H(x) = e_h(x) - a_H(x)那么有 \begin{aligned} \int e_H(x) p(x) dx = \int e_h(x) p(x) dx - \int a_H(x) p(x) dx \\ = \int \sum_{m=1}^M w_m e_m(x) p(x) dx - \int \sum_{m=1}^M w_m a_m(x) p(x) dx \\ = \sum_{m=1}^M w_m \underbrace{\int e_m(x) p(x) dx}_{E_m} - \sum_{m=1}^M w_m \underbrace{\int a_m(x) p(x) dx}_{A_m} \\ \end{aligned}那么集成学习的误差-分歧分解如下 E = \overline{E} - \overline{A}那么集成学习的目标是使$E$最小化，即 降低个体学习器的泛化误差的加权均值，个体准确性要高； 提高个体学习器的加权分歧值，个体间差异应尽量大。 但是有以下两个问题 只针对回归问题，难以直接推广到分类问题中； 难以直接作为优化目标进行优化： 定义在整体样本空间上； $\overline{A}$是需要集成学习器构造后才能进行估计。 多样性度量多样性度量(diversity measure)用于刻画集成模型中个体分类器多样性的程度。给定样本集合${(X^{(i)}, y^{(i)}), i = 1, \cdots, N, y \in {-1, +1}}$，分类器$h_i, h_j$的预测结果表为 $h_i = +1$ $h_i = -1$ $h_j = +1$ a b $h_j = -1$ c d 不合度量(disagreement measure)dis_{ij} = \frac{b + c}{N} 相关系数(correlation coefficient)\rho_{ij} = \frac{ad - bc}{\sqrt{(a + b)(a + c)(c + d)(b + d)}} 若$h_i, h_j$无关，则值为$0$； 若$h_i, h_j$正相关，则值大于$0$； 若$h_i, h_j$负相关，则值小于$0$。 $Q$统计量 Q_{ij} = \frac{ad - bc}{ad + bc} $Q_{ij}$符号与$\rho_{ij}$相同，且$|Q_{ij} \leq \rho_{ij}|$。 $\kappa$统计量 \kappa_{ij} = \frac{p_1 - p_2}{1 - p_2} 其中 $p_1$两个分类器取得一致的概率 p_1 = P(h_i = +1, h_j = +1) + P(h_i = -1, h_j = -1) = \frac{a + d}{N} $p_2$是假设两个分类器预测结果相互独立下，预测达成一致的概率 \begin{aligned} p_2 = P(h_i = +1, h_j = +1) + P(h_i = -1, h_j = -1) \\ = P(h_i = +1)P(h_j = +1) + P(h_i = -1)P(h_j = -1) \\ = \frac{(a + b)(a + c) + (c + d)(b + d)}{N^2} \end{aligned} 若两个分类器完全一致，则$b=c$，此时$\kappa=1$； 若两个分类器偶然达成一致，则$p_1 = p_2$，此时$\kappa=0$； 通常$\kappa$取非负值，仅在$h_i$和$h_j$达成一致的概率甚至低于偶然性的情况下，才取负值。 多样性增强为获得多样性较大的个体学习器，需要在学习过程中引入随机性，常见做法是：对数数据样本、输入属性、输出表示、算法参数等进行扰动 数据样本扰动：从给定初始数据集中产生不同的数据子集(通常基于采样)，利用不同数据子集训练不同的个体学习器； 对决策树、神经网络等受样本影响较大的不稳定基学习器很有效，而对于线性学习器、支持向量机、朴素贝叶斯、$k$近邻等稳定基学习器效果不大。 输入属性扰动：由一组属性描述训练样本，从不同的样本子空间提供观察数据的不同视角； 适合包含大量冗余属性的数据，而属性或冗余属性较少的数据，不宜采用。 输出表示扰动：对输出表示进行操纵以增强多样性。 如对训练样本的类标记稍作变动，如翻转法(flipping output)随机改变一些训练样本的标记。 算法参数扰动：通过随机设置不同的超参数，产生差异较大的个体学习器； 交叉验证方法是从不同超参数的个体学习其中选择最优的，而集成模型是全部使用。 Reference 机器学习—集成学习（Ensemble Learning） - cnblogs 集成学习 - huaxiaozhuan.com ID3、C4.5、CART、RF、boosting、Adaboost、GBDT、xgboost模型 - 知乎]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[grep, sed, awk]]></title>
    <url>%2F2020%2F05%2F05%2Fgrep-sed-awk%2F</url>
    <content type="text"><![CDATA[grep: Globally search a Regular Expression and Print 基本用法 参数说明 sed: Stream Editor 基本用法 参数说明 编辑命令 实例 awk: Alfred Aho, Peter Weinberger, Brian Kernighan 基本用法 参数说明 常用内置变量 语法 运算符 BEGIN/END 分支、循环、数组 分支: if 循环: do while, for 数组 常用字符串函数 grep: Globally search a Regular Expression and Print强大的文本搜索工具，它能使用特定模式匹配（包括正则表达式）查找文本，并默认输出匹配行到STDOUT。 基本用法1$ grep [-abcEFGhHilLnqrsvVwxy][-A&lt;显示列数&gt;][-B&lt;显示列数&gt;][-C&lt;显示列数&gt;][-d&lt;进行动作&gt;][-e&lt;范本样式&gt;][-f&lt;范本文件&gt;][--help][范本样式][文件或目录...] 参数说明1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071$ grep --helpUsage: grep [OPTION]... PATTERN [FILE]...Search for PATTERN in each FILE.Example: grep -i 'hello world' menu.h main.cPattern selection and interpretation: -E, --extended-regexp PATTERN is an extended regular expression -F, --fixed-strings PATTERN is a set of newline-separated strings -G, --basic-regexp PATTERN is a basic regular expression (default) -P, --perl-regexp PATTERN is a Perl regular expression -e, --regexp=PATTERN use PATTERN for matching # -e 将PATTERN作为正则表达式 -f, --file=FILE obtain PATTERN from FILE -i, --ignore-case ignore case distinctions # -i 忽略大小写 -w, --word-regexp force PATTERN to match only whole words -x, --line-regexp force PATTERN to match only whole lines -z, --null-data a data line ends in 0 byte, not newlineMiscellaneous: -s, --no-messages suppress error messages -v, --invert-match select non-matching lines # -v 反向匹配，输出不包含PATTERN的文本行 -V, --version display version information and exit --help display this help text and exitOutput control: -m, --max-count=NUM stop after NUM selected lines -b, --byte-offset print the byte offset with output lines -n, --line-number print line number with output lines # -n 输出匹配的文本行的行标 --line-buffered flush output on every line -H, --with-filename print file name with output lines -h, --no-filename suppress the file name prefix on output --label=LABEL use LABEL as the standard input file name prefix -o, --only-matching show only the part of a line matching PATTERN -q, --quiet, --silent suppress all normal output --binary-files=TYPE assume that binary files are TYPE; TYPE is 'binary', 'text', or 'without-match' -a, --text equivalent to --binary-files=text # -a 将二进制文件内容作为text进行搜索 -I equivalent to --binary-files=without-match -d, --directories=ACTION how to handle directories; ACTION is 'read', 'recurse', or 'skip' -D, --devices=ACTION how to handle devices, FIFOs and sockets; ACTION is 'read' or 'skip' -r, --recursive like --directories=recurse # -r 在目录下递归搜索 -R, --dereference-recursive likewise, but follow all symlinks --include=FILE_PATTERN search only files that match FILE_PATTERN --exclude=FILE_PATTERN skip files and directories matching FILE_PATTERN --exclude-from=FILE skip files matching any file pattern from FILE --exclude-dir=PATTERN directories that match PATTERN will be skipped. -L, --files-without-match print only names of FILEs with no selected lines # -L 输出不包含能匹配PATTERN内容的文件名 -l, --files-with-matches print only names of FILEs with selected lines # -l 输出包含能匹配PATTERN内容的文件名 -c, --count print only a count of selected lines per FILE # -c 输出匹配到的文本行的数目 -T, --initial-tab make tabs line up (if needed) -Z, --null print 0 byte after FILE nameContext control: -B, --before-context=NUM print NUM lines of leading context # -B 显示查找到的某行字符串外，还显示之前&lt;NUM&gt;行 -A, --after-context=NUM print NUM lines of trailing context # -A 显示查找到的某行字符串外，还显示随后&lt;NUM&gt;行 -C, --context=NUM print NUM lines of output context # -C 显示查找到的某行字符串外，还显示之前和随后&lt;NUM&gt;行 -NUM same as --context=NUM --color[=WHEN], --colour[=WHEN] use markers to highlight the matching strings; WHEN is 'always', 'never', or 'auto' -U, --binary do not strip CR characters at EOL (MSDOS/Windows)When FILE is '-', read standard input. With no FILE, read '.' ifrecursive, '-' otherwise. With fewer than two FILEs, assume -h.Exit status is 0 if any line is selected, 1 otherwise;if any error occurs and -q is not given, the exit status is 2.Report bugs to: bug-grep@gnu.orgGNU grep home page: &lt;http://www.gnu.org/software/grep/&gt;General help using GNU software: &lt;http://www.gnu.org/gethelp/&gt; sed: Stream Editor利用脚本来编辑文本文件，主要用来自动编辑一个或多个文件，简化对文件的反复操作、编写转换程序等。它执行的操作为 一次从输入中读取一行数据； 根据提供的编辑器命令匹配数据； 按照命令修改流中的数据； 将新的数据输出到STDOUT，不改变原来的文本文件。 基本用法1$ sed [-e &lt;script&gt;][-f &lt;script文件&gt;][文本文件] &lt;script&gt;为字符串格式的编辑命令，多条命令间以;分隔，或者用bash中的次提示符分隔命令； &lt;script文件&gt;表示记录编辑命令的文件名，为与shell脚本区分，一般用.sed作为文件后缀名 参数说明1234567891011121314151617181920212223242526272829303132333435363738394041$ sed --helpUsage: sed [OPTION]... &#123;script-only-if-no-other-script&#125; [input-file]... -n, --quiet, --silent suppress automatic printing of pattern space -e script, --expression=script # -e 从命令行读取执行命令，单条编辑命令时可省略 add the script to the commands to be executed -f script-file, --file=script-file # -f 从文件中读取执行命令 add the contents of script-file to the commands to be executed --follow-symlinks follow symlinks when processing in place -i[SUFFIX], --in-place[=SUFFIX] # -i 直接修改文本内容 edit files in place (makes backup if SUFFIX supplied) -l N, --line-length=N specify the desired line-wrap length for the `l' command --posix disable all GNU extensions. -E, -r, --regexp-extended use extended regular expressions in the script (for portability use POSIX -E). -s, --separate consider files as separate rather than as a single, continuous long stream. --sandbox operate in sandbox mode. -u, --unbuffered load minimal amounts of data from the input files and flush the output buffers more often -z, --null-data separate lines by NUL characters --help display this help and exit --version output version information and exitIf no -e, --expression, -f, or --file option is given, then the firstnon-option argument is taken as the sed script to interpret. Allremaining arguments are names of input files; if no input files arespecified, then the standard input is read.GNU sed home page: &lt;http://www.gnu.org/software/sed/&gt;.General help using GNU software: &lt;http://www.gnu.org/gethelp/&gt;.E-mail bug reports to: &lt;bug-sed@gnu.org&gt;. 编辑命令1234567891011121314151617181920# `a`: 在指定行后添加行，注意若希望添加多行，行间用`\n`进行分隔，而开头和结尾无需添加`\n`；$ sed -e "FROM[,TO] a [CONTENT]" FILENAME# `i`: 在指定行前添加行$ sed -e "FROM[,TO] i [CONTENT]" FILENAME# `d`: 将指定行删除$ sed -e "FROM[,TO] d" FILENAME# `c`: 取代指定行内容$ sed -e "FROM[,TO] c [CONTENT]" FILENAME# `s`: 部分数据的搜索和取代$ sed -e "FROM[,TO] s/[PATTERN]/[CONTENT]/g" FILENAME# `p`: 打印输出指定行$ sed -n -e "FROM[,TO] p" FILENAME# `q`: 退出，终止命令$ sed -e "[COMMANDS;]q" FILENAME 实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 新建文本`test_sed.txt`$ for (( i=1; i&lt;=5; i++ )) &#123;&gt; echo "line $i" &gt;&gt; test_sed.txt&gt; &#125;$ cat test_sed.txtline 1line 2line 3line 4line 5# ================= 基本操作 ==================# ------------------ 打印行 -------------------# 输出第3~5行，若不添加`-n`会输出全部内容$ sed -n -e "3,5 p" test_sed.txt# ------------------ 添加行 -------------------# 在第3行后添加一行$ sed -e "3 a newline" test_sed.txt# 在3~5每行后添加一行$ sed -e "3,5 a newline" test_sed.txt# ------------------ 插入行 -------------------# 在第3行前添加一行$ sed -e "3 i newline" test_sed.txt# 在第3行后添加两行$ sed -e "3 a newline1\nnewline2" test_sed.txt# ------------------ 删除行 -------------------# 删除第3行$ sed -e "3 d" test_sed.txt# 删除第3~5行$ sed -e "3,5 d" test_sed.txt# 删除第3行到最后行$ sed -e "3,$ d" test_sed.txt# ------------------ 替换行 -------------------# 替换第3行$ sed -e "3 c replace" test_sed.txt# 替换第3~5行$ sed -e "3,5 c replace" test_sed.txt# ------------- 查找替换部分文本 ---------------# 替换第3行中的`li`为`LI`$ sed -e "3 s/li/LI/g" test_sed.txt# ----------------- 多点编辑 ------------------# 删除第3行到末尾行内容，并把`line`替换为`LINE`$ sed -e "3,$ d; s/line/LINE/g" test_sed.txt# 或者$ $ sed -e "3,$ d" -e "s/line/LINE/g" test_sed.txt# ============== 搜索并执行命令 ===============# ---------------- 打印匹配行 -----------------# 输出包含`3`的关键行，若不添加`-n`同时会输出所有行$ sed -n -e "/3/p" test_sed.txt# ---------------- 删除匹配行 -----------------# 删除包含`3`的关键行$ sed -e "/3/d" test_sed# ---------------- 替换匹配行 -----------------# 将包含`3`的关键行中，`line`替换为`this line`$ sed -e "/3/&#123;s/line/this line/&#125;" test_sed.txt# 将包含`3`的关键行中，`line`替换为`this line`，并且只输出该行$ sed -n -e "/3/&#123;s/line/this line/; p; &#125;" test_sed.txt# =============== in-place操作 ===============# 直接修改文本内容，`line`替换为`this line`$ sed -i -e "s/line/LINE/g" test_sed.txt# 注意重定向操作可能出现错误$ sed -e "s/line/LINE/g" test_sed.txt &gt; test_sed.txt # 导致文本为空$ sed -e "s/line/LINE/g" test_sed.txt &gt;&gt; test_sed.txt # 正常追加 awk: Alfred Aho, Peter Weinberger, Brian Kernighan逐行扫描指定文件，寻找匹配特定模式的行，并在这些行上进行想要的操作。若未指定匹配模式，将会对所有行进行操作(即默认全部行)；若未指定处理方法，将会被输出到STDOUT(即默认为print)。 基本用法123awk [选项参数] 'script' var=value file(s)或awk [选项参数] -f scriptfile var=value file(s) 参数说明1234567891011121314151617181920212223242526272829303132333435363738394041$ awk --helpUsage: awk [POSIX or GNU style options] -f progfile [--] file ...Usage: awk [POSIX or GNU style options] [--] 'program' file ...POSIX options: GNU long options: (standard) -f progfile --file=progfile # 从文本读取awk命令 -F fs --field-separator=fs # 字符分隔符，即改行文本以该符号作为分隔，例如$PATH中的`:` -v var=val --assign=var=valShort options: GNU long options: (extensions) -b --characters-as-bytes -c --traditional -C --copyright -d[file] --dump-variables[=file] -D[file] --debug[=file] -e 'program-text' --source='program-text' -E file --exec=file -g --gen-pot -h --help -i includefile --include=includefile -l library --load=library -L[fatal|invalid] --lint[=fatal|invalid] -M --bignum -N --use-lc-numeric -n --non-decimal-data -o[file] --pretty-print[=file] -O --optimize -p[file] --profile[=file] -P --posix -r --re-interval -S --sandbox -t --lint-old -V --versionTo report bugs, see node `Bugs' in `gawk.info', which issection `Reporting Problems and Bugs' in the printed version.gawk is a pattern scanning and processing language.By default it reads standard input and writes standard output.Examples: gawk '&#123; sum += $1 &#125;; END &#123; print sum &#125;' file gawk -F: '&#123; print $1 &#125;' /etc/passwd 常用内置变量 变量名 说明 $0 当前记录 $1 ~ $n 当前记录被FS分隔后，第n个字段 NF 当前记录中字段个数 NR 已经读出的记录数 FS 字段分隔符，默认为空格 RS 记录分隔符，默认为换行符 OFS 输出字段分隔符，默认为空格 ORS 输出记录分隔符，默认为换行符 默认情况下，按换行符分隔记录、按空格分隔字段，即记录为单行文本、字段为文本单词。 语法运算符 运算符 说明 = 赋值 +=, -=, =, %=, ^=, *= 赋值运算 \ \ , &amp;&amp;, ! 逻辑或，逻辑与，逻辑非 ~, !~ 匹配和不匹配正则表达式 &lt;, &lt;=, &gt;=, !=, == 关系运算符；可以作为字符串比较，也可以用作数值比较；两个都为数字才为数值比较；字符串按字典序比较 +, -, *, / 加减乘除，所有用作算术运算符进行操作，操作数自动转为数值，所有非数值都变为0 &amp; 求余 ^, * 求幂 ++, — 前缀或后缀自增、自减 $n 字段引用 空格 字符串连接符 ?: 三目运算符 ln 数组中是否存在某键值 BEGIN/END在BEGIN/END代码块内的命令，只会在开始/结束处理输入文件的文本时执行一次。BEGIN块一般用作初始化FS、打印页眉、初始化全局变量等；END一般用于打印计算结果或输出摘要。 12345# 统计`/etc/passwd`记录数$ awk 'BEGIN&#123;count = 0&#125; &#123;count++&#125; END&#123;print count&#125;' /etc/passwd# 统计`/etc/passwd`字段数$ awk 'BEGIN&#123;count = 0; FS=":"&#125; &#123;count += NF&#125; END&#123;print count&#125;' /etc/passwd 分支、循环、数组分支: if类似C的if语句1234567891011121314151617$ cat test.awkBEGIN &#123; FS = ":"&#125;&#123; if ($1 == "louishsu")&#123; if ($2 == "x")&#123; print "louishsu x" &#125; else &#123; print "louishsu _" &#125; &#125; else if ( $1 == "mysql")&#123; print "mysql" &#125;&#125;$ awk -f test.awk /etc/passwd 循环: do while, for可通过break/continue控制循环 1234567891011121314$ cat test.awkBEGIN &#123; FS = ":"&#125;&#123; print "----------------" count = 0 do &#123; print $count count++ &#125; while (count &lt; 3)&#125;$ awk -f test.awk /etc/passwd 12345678910$ cat test.awkBEGIN &#123; FS = ":"&#125;&#123; print "----------------" for (count = 0; count &lt; 3; count++) &#123; print $count &#125;&#125; 数组awk中的数组都是关联数组，数字索引也会转变为字符串索引 123456789101112$ cat test.awk&#123; cities[1] = "beijing" cities[2] = "shanghai" cities["three"] = "guangzhou" for( c in cities) &#123; print cities[c] &#125; print cities[1] print cities["1"] print cities["three"]&#125; 常用字符串函数 函数 说明 sub(r, s, [t]) 在整个t中，用s代替r；t缺省为$0；返回替换数量 gsub(r, s, [t]) r被作为正则表达式，其余同sub函数 index(s1, s2) 查找并返回s2在s1中的位置(从1开始编号)；若不存在则返回0 match(s, r) 在s中匹配正则表达式r(从1开始编号)；若未找到匹配返回-1 length [(s)] 返回s字符串长度，缺省为$0 substr(s, m, [n]) 返回从m开始，长度为n的子字符串；不指定n截取到字符串末尾 split(s, a, [r]) 根据r指定的拓展正则表达式或FS，将字符串s分割为数组元素a[1], a[2], ..., a[n]；返回n tolower(s), toupper(s) 全部转换为小写/大写字母，大小写映射由当前语言环境的LC_CTYPE范畴定义 sprintf(fmt, ...) 根据fmt格式化字符串并返回]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Shell Programming]]></title>
    <url>%2F2020%2F05%2F04%2FShell-Programming%2F</url>
    <content type="text"><![CDATA[目录 目录 Shell基础 常用指令 父子shell 环境变量 输入/输出重定向 执行时重定向 输入重定向 输出重定向 错误重定向 脚本中重定向 输入/输出 自定义文件描述符 重定向到已有文件描述符 管道 变量 字符串 变量参数 数组参数 参数传递 位置参数 命名参数 用户输入 基本输入: read 文件输入: cat | read 脚本退出: exit 命令替换: ( command ) 运算和测试 数学运算 $( expr expression ) $[ expression ] let expression, $(( expression )) 内建计算器bc 测试命令: test expression, [ expression ] 数值测试: -eq, -ne, -gt, -ge, -lt, -le 字符测试: =, !=, &lt;, &gt;, -n -z 文件测试: -e, -d, -f, … 复合条件测试: !, -o / ||, -a / &amp;&amp; 结构化命令 分支 if-then-elif-else-fi case-in 循环 for-do-done while-do-done until-do-done 循环控制: break, continue 函数 创建和调用函数 参数传递 作用域: local 变量参数 数组参数 返回值: return, echo 文件包含: source 总结 Shell基础常用指令Linux 命令大全 - 菜鸟教程 父子shell在当前shell中打开其他shell时，会创建新的shell程序，称为子shell(chile shell)。123456789101112131415161718192021$ ps --forest PID TTY TIME CMD 6 tty1 00:00:00 bash 66 tty1 00:00:00 \_ ps$ bash # 子shell1$ ps --forest PID TTY TIME CMD 6 tty1 00:00:00 bash 75 tty1 00:00:00 \_ bash 125 tty1 00:00:00 \_ ps$ bash # 子shell1的子shell$ ps --forest PID TTY TIME CMD 6 tty1 00:00:00 bash 75 tty1 00:00:00 \_ bash 126 tty1 00:00:00 \_ bash 174 tty1 00:00:00 \_ ps$ exitexit$ exitexit 通过进程列表调用命令可创建子shell，将多条命令以&#39;;&#39;作为间隔，放置在&#39;()&#39;中执行。进程列表是一种命令分组，另一种命令分组是在&#39;{}&#39;中执行，但不会创建子shell。12345678910111213141516$ pwd; ls; ps -f; echo $BASH_SUBSHELL/home/louishsuDownloads anaconda3 backupUID PID PPID C STIME TTY TIME CMDlouishsu 6 5 0 09:35 tty1 00:00:00 -bashlouishsu 176 6 0 09:48 tty1 00:00:00 ps -f0$ # 进程列表$ (pwd; ls; ps -f; echo $BASH_SUBSHELL)/home/louishsuDownloads anaconda3 backupUID PID PPID C STIME TTY TIME CMDlouishsu 6 5 0 09:35 tty1 00:00:00 -bashlouishsu 177 6 0 09:49 tty1 00:00:00 -bash # 创建了子shelllouishsu 179 177 0 09:49 tty1 00:00:00 ps -f1 在shell脚本中，经常使用子shell进行多进程处理，但是会明显拖慢处理速度，一种高效的使用方法是后台模式1234567891011121314151617181920212223$ # 将命令置入后台模式$ sleep 10 &amp; # 置入后台，终端仍可I/O[1] 191$ ps -fUID PID PPID C STIME TTY TIME CMDlouishsu 6 5 0 09:35 tty1 00:00:00 -bashlouishsu 191 6 0 09:51 tty1 00:00:00 sleep 10louishsu 192 6 0 09:51 tty1 00:00:00 ps -f$ jobs[1]+ Running sleep 10 &amp;$ # 将进程列表置入后台模式$ (sleep 10 ; echo $BASH_SUBSHELL ; sleep 10) &amp;[2] 193[1] Done sleep 10$ ps -fUID PID PPID C STIME TTY TIME CMDlouishsu 6 5 0 09:35 tty1 00:00:00 -bashlouishsu 193 6 0 09:53 tty1 00:00:00 -bash # 创建了子shelllouishsu 194 193 1 09:53 tty1 00:00:00 sleep 10louishsu 195 6 0 09:53 tty1 00:00:00 ps -f$ jobs[2]+ Running ( sleep 10; echo $BASH_SUBSHELL; sleep 10 ) &amp; 环境变量环境变量(environment variable)用于存储有关shell会话和工作环境的信息，分为局部变量和全局变量。局部变量只对创建它们的shell可见；全局变量对shell会话和所生成的子shell都是可见的，用printenv或env输出全局变量1234567891011121314$ env | lessCONDA_SHLVL=1LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:CONDA_EXE=/home/louishsu/anaconda3/bin/condaHOSTTYPE=x86_64LESSCLOSE=/usr/bin/lesspipe %s %s[...]$ printenv # 同上$ printenv HOME # 显示单个变量只能用printenv/home/louishsu$ echo $HOME # 需加上$符/home/louishsu 注意变量的作用域 局部环境变量在各进程内是独立的，即父子进程间变量无关联； 设定全局环境变量的进程所创建的子进程中，全局环境变量可见； 子进程只能暂时修改变量(包括删除)，退出后父进程内变量不改变。 12345678910111213141516171819202122232425$ # 在子shell中该变量不可见$ bash$ echo $var$ # 子shell中定义局部变量，在退出后父shell内也不可见$ var=5$ echo $var5$ exitexit$ # 且父shell变量未改变$ echo $varhello world!$ # 设置为全局变量$ export var # 注意无需`$`$ # 在子shell中该变量可见$ bash$ echo $varhello world!$ # 子shell中修改全局变量，父shell变量未改变$ var=5$ exitexit$ echo $varhello world! 以设置环境变量PATH变量为例，用&#39;$&#39;读取变量值，&#39;:&#39;作为分割符进行拼接12345$ echo $PATH[...]:/home/louishsu/Downloads/kibana-6.6.0-linux-x86_64/bin$ export PATH=$PATH:/home/louishsu/Downloads$ echo $PATH[...]:/home/louishsu/Downloads/kibana-6.6.0-linux-x86_64/bin:/home/louishsu/Downloads 若希望PATH变量持久化，将export命令记录在以下几个文件中(无需全部记录)。以下是shell默认的主启动文件，在每次登录Linux时执行(系统级)，在Ubuntu系统中，该文件内部执行调用文件/etc/bash.bashrc /etc/profile 以下四个文件作用相同，都是用户级的启动文件，一般大多数Linux发行版都只用到一到两个。shell会按照.bash_profile、.bash_login、.profile的顺序，执行第一个找到的文件(其余的被省略)。注意.bashrc是在以上三个文件中被执行的。 $HOME/.bash_profile $HOME/.bash_login $HOME/.profile $HOME/.bashrc 但是如果bash是作为交互式shell启动，只会检查执行$HOME/.bashrc，而/etc/profile和$HOME/.profile等均被忽略。 输入/输出重定向通过输入/输出重定向，可将标准输入/标准输出重定向到另一个位置(如文件)。Linux将每个对象视作文件处理，用文件描述符(file descriptor)来标识文件对象。文件描述符是一个非负整数，每个进程一次最多可以有9个文件描述符。其中比较特殊的是标准输入(STDIN, 0)、标准输出(STDOUT, 1)、标准错误(STDERR, 2)。 执行时重定向输入重定向输入重定向是将文件内容重定向到命令，符号是&#39;&lt;&#39;，例如用wc对文本进行计数12$ wc &lt; .bashrc157 636 5119 # 文本行数、词数、字节数 还有一种是内联输入重定向(inline input redirection)，符号是&#39;&lt;&lt;&#39;，无需使用文件进行重定向，直接从stdin读取数据，必须指定一个文本标记来标记输入的开始和结尾。123456$ wc &lt;&lt; EOF # 标记符，也可定义为其他文本&gt; this is&gt; inline&gt; input redirection&gt; EOF3 5 34 输出重定向将命令输出发送到文件中，符号是&#39;&gt;&#39;，会覆盖已有数据，可以用&#39;&gt;&gt;&#39;进行内容追加而不覆盖 注意，错误信息未被重定向。 12345678910$ echo "hello!" &gt; inputRedirection. txt$ cat inputRedirection. txthello!$ echo "world" &gt; inputRedirection. txt$ cat inputRedirection. txtworld$ echo "hello" &gt;&gt; inputRedirection. txt$ cat inputRedirection. txtworldhello 错误重定向一般错误输出和正常输出都会显示在屏幕上，但如果需要将错误信息重定向，则可通过指定文件描述符。例如重定向错误到文本err.logs，而其余正常输出，可通过2&gt;指定文本文件 123456$ wget 2&gt; err.logs$ cat err.logs # 查看文本内容wget: missing URLUsage: wget [OPTION]... [URL]...Try `wget --help' for more options. 同时将正常输出重定向到文本out.logs1234567$ wget 1&gt; out.logs 2&gt; err.logs $ cat out.logs # 空$ cat err.logswget: missing URLUsage: wget [OPTION]... [URL]...Try `wget --help' for more options. 若想同时重定向输出和错误到文本outerr.logs，通过&amp;&gt;指定123456$ wget &amp;&gt; outerr.logs$ cat outerr.logswget: missing URLUsage: wget [OPTION]... [URL]...Try `wget --help' for more options. 脚本中重定向输入/输出在脚本中向文本描述符desc输人/输出的命令如下，注意空格。12command &gt;&amp;desccommand &lt;&amp;desc 例如向标准错误STDERR输出数据123#!/bin/bashecho "[Error]: to file err.logs" &gt;&amp;2 # STDERRecho "[Warining]: to file out.logs" # default STDOUT 如果执行时不指定错误重定向，将被默认打印到屏幕上(默认错误与输出打印到同一位置，即屏幕上)123$ ./test.sh[Error]: to file err.logs[Warining]: to file out.logs 若指定错误重定向，即可输出到文本1234$ ./test.sh 2&gt; err.logs[Warining]: to file out.logs$ cat err.logs[Error]: to file err.logs 自定义文件描述符可通过exec自定义文件描述符1234exec desc&lt; filename # 从文件创建输入重定向exec desc&gt; filename # 从文件创建输出重定向exec desc&lt;&gt; filename # 从文件创建输入输出重定向exec desc&gt;&amp;- # 重定向到`-`，关闭文件描述符 例如in.logs原始文件内容如下1234$ cat in.logsDo not go gentle into that good night,Old age should burn and rave at close of day;Rage, rage against the dying of the light. 编写脚本，从in.logs创建输入输出重定向，并将文件描述符定义为312345678910#!/bin/bashexec 3&lt;&gt; in.logsecho "Read poem:" # stdoutwhile read line &lt;&amp;3; do # get line from descriptor 3 echo $line # stdoutdoneecho "Write poem:" # stdoutecho "Excellent!" &gt;&amp;3 # write line to descriptor 3 123456$ ./test.shRead poem:Do not go gentle into that good night,Old age should burn and rave at close of day;Rage, rage against the dying of the light.Write poem: 再次查看in.logs文件内容12345$ cat in.logsDo not go gentle into that good night,Old age should burn and rave at close of day;Rage, rage against the dying of the light.Excellent! # 追加内容 又如，将STDIN, STDOUT, STDERR均重定向到各自文件 123456789101112131415#!/bin/bash# 输入重定向exec 0&lt; in.logswhile read line; do echo "$line"done# 输出重定向exec 1&gt; out.logsecho "[Warining]: to file out.logs"# 错误重定向exec 2&gt; err.logsecho "[Error]: to file err.logs" &gt;&amp;2 1234567891011121314$ cat in.logsDo not go gentle into that good night,Old age should burn and rave at close of day;Rage, rage against the dying of the light.$ ./test.shDo not go gentle into that good night,Old age should burn and rave at close of day;Rage, rage against the dying of the light.$ cat out.logs[Warining]: to file out.logs$ cat err.logs[Error]: to file err.logs 重定向到已有文件描述符12exec descNew&gt;&amp;desc # 创建输出重定向exec descNew&lt;&amp;desc # 创建输入重定向 12345#!/bin/bash# 重定向3到STDOUT3exec 3&gt;&amp;1 echo "To STDOUT"echo "To desc 3" &gt;&amp;3 # 输出到文本描述符3 可以看到执行后，输出到3的数据也被显示到STDOUT中123$ ./test.shTo STDOUTTo desc 3 管道管道可将一个命令的输出作为另一个命令的输入，是将第一个命令重定向到第二个命令，称为管道连接(piping)。Linux系统会同时调用多个命令，在内部将他们连接，而不是依次执行(管道通信)。例如，用apt-get搜索openssl安装包，排序sort后通过less查看12345678910111213$ apt search openssl | grep openssl* | sort | less Asynchronous event notification library (openssl) D version of the C headers for openssl Loadable module for openssl implementing GOST algorithms Puppet module for managing openssl configurationaolserver4-nsopenssl/bionic,bionic 3.0beta26-6 amd64bruteforce-salted-openssl/bionic,bionic 1.4.0-1build1 amd64dlang-openssl/bionic,bionic 1.1.5+1.0.1g-1 alljruby-openssl/bionic-updates,bionic-security 0.9.21-2~18.04 alllcmaps-openssl-interface/bionic,bionic 1.6.6-2build1 alllibcrypt-openssl-bignum-perl/bionic,bionic 0.09-1build1 amd64libcrypt-openssl-dsa-perl/bionic,bionic 0.19-1build2 amd64[...] 变量除了环境变量，shell支持在脚本中定义和使用用户变量，临时存储数据。 变量名可以由字母、数字和下划线组成，长度不超过20，首个字符不能以数字开头，区分大小写，不可使用保留关键字； 在赋值时同样地，赋值符两侧不能出现空格； shell脚本会自动决定变量值的数据类型，在脚本结束时所有用户变量被删除； 注意&#39;$&#39;的使用：引用变量值时需要，而引用变量进行赋值等操作时不需要。 1234567891011$ var1=1; var2=2$ echo var1 # var1被视作字符串var1$ echo $var11$ var1=var2 # var1内容更改为字符串var2$ echo $var1var2$ var1=$var2 # var1内容更改为变量var2的值$ echo $var12 变量名外面的花括号界定符，加花括号是为了帮助解释器识别变量的边界，比如 123456789101112$ for name in Jack Tom Bob; do&gt; echo "This is $nameBoy" # nameBoy被视作变量名&gt; doneThis isThis isThis is$ for name in Jack Tom Bob; do&gt; echo "This is $&#123;name&#125;Boy" # name被视作变量名，自动拼接字符串&gt; doneThis is JackBoyThis is TomBoyThis is BobBoy 字符串字符串是shell编程中最常用最有用的数据类型，定义字符串时，可以选择单引号、双引号、无引号，但是有部分限制：单引号内引用变量值无效，且不能使用转义字符。123456789$ name=louishsu$ echo 'This is \"$name\"' # 单引号内引用变量值无效，且不能使用转义字符This is \"$name\"$ echo "This is \"$name\"" # 双引号则反之This is "louishsu"$ echo -e 'This is \"$name\"' # echo开启转义也无效This is \"$name\"$ echo -e "This is \"$name\"" # echo开启转义有效This is "louishsu" 字符串可进行拼接12345$ name=louishsu$ echo "Hello, "$name"!"Hello, louishsu!$ echo "Hello, $name!"Hello, louishsu! 字符串长度、子字符串、查找字符串12345678910111213141516171819$ # 字符串长度$ echo $&#123;#name&#125; 7$ # 尝试使用下标$ echo $&#123;name[0]&#125; louishsu$ echo $&#123;name[1]&#125; # 输出回车$ # 截取子字符串$ echo $&#123;name:0:5&#125; # 从0开始，截取5个字符louis$ echo $&#123;name:5:3&#125; # 从5开始，截取3个字符hsu$ # 查找字符串$ echo `expr index $name su` # 查找s或u3 变量参数以下介绍如何定义变量及删除变量12345678910111213141516171819202122232425262728293031$ # 未创建变量$ echo $var # 输出回车$ # 创建变量var，注意赋值符两侧不能有空格$ var=/home/louishsu$ echo $var/home/louishsu$ # 变量可用作路径等$ ls $varDownloads anaconda3 backup$ # 创建带空格的字符串变量$ var="hello world!"$ echo $varhello world!$ # 删除变量$ unset var # 注意无需`$`$ echo $var # 输出回车$ # 只读变量$ var=1$ echo $var1$ readonly var # 设置为只读$ var=2 # 不可更改-bash: var: readonly variable$ unset var # 不可删除-bash: unset: var: cannot unset: readonly variable 数组参数shell可使用数组1234567891011121314151617181920212223242526272829303132333435$ # 定义数组变量var=(1 2 3 4 5)$ echo $var # 无法全部打印输出1$ # 以下标获取数组元素(0开始)$ # 缺少`&#123;&#125;`界定符$ echo $var[1] 1[1] # 失败$ echo $&#123;var[1]&#125;2 # 成功$ # 打印输出全部元素$ echo $&#123;var[*]&#125;1 2 3 4 5$ # 获取数组长度$ echo $&#123;#var&#125;1 # 失败$ echo $&#123;#var[*]&#125;5 # 成功$ # 删除数组元素后，令人疑惑的地方，需注意$ unset var[1]$ echo $&#123;var[1]&#125; # 输出回车$ echo $&#123;var[*]&#125;1 3 4 5$ echo $&#123;#var[*]&#125;4$ # 删除数组$ unset var$ echo $&#123;var[*]&#125; # 输出回车 参数传递位置参数在执行脚本时，可将命令行参数传递给脚本使用，通过位置参数调用 12345678910111213141516171819202122232425262728293031#!/bin/bash# 打印输出参数# $0: 脚本文件名echo "The filename of script is $0"echo "The basename is $( basename $0 )"# $#: 参数个数# $1, ..., $&#123;10&#125;, ...: 位置参数echo -n "There are $# parameters supplied， which are:"for ((i = 1; i &lt;= $#; i++)); do echo -n $&#123;!i&#125;doneecho ""# 若不加引号，则以下两种输出结果相同# 获取参数列表# $*: 将参数视作字符串整体for param in "$*"; do echo $paramdone# $@: 将参数视作字符串内独立的单词for param in "$@"; do echo $paramdone# 获取最后一个变量# echo "The last parameter is $&#123;$#&#125;" # 错误，&#123;&#125;内不能带$echo "The last parameter is $&#123;!#&#125;"argc=$#echo "The last parameter is $argc" 12345678910$ ./test.sh 1 2 3The filename of script is ./test.shThe basename is test.shThere are 3 parameters supplied， which are:1231 2 3123The last parameter is 3The last parameter is 3 命名参数 通过shift命令处理 调用一次shift命令，$1参数被删除，其余所有参数向左移动，即$2移动到$1，$3移动到$2中，以此类推。例如，某脚本需处理命令行参数-a -b 3 -c -d，其中-b为命名参数，则脚本如下编写 123456789101112131415#!/bin/bashwhile [ -n "$1" ] # 不可缺少引号""do case "$1" in -a) echo "Option -a" ;; -b) echo "Option -b" shift echo "Value of option -b is: $1" ;; -c) echo "Option -c";; *) echo "Invalid parameters";; esac shiftdone 12345$ ./test.sh -a -b 5 -cOption -aOption -bValue of option -b is: 5Option -c 通过getopt命令处理 getopt命令简单使用格式如下 1getopt optstring parameters 例如解析-a -b 3 -c -d，指定optsting为ab:cd，其中:表示该处包含参数值，在输出--后的参数均视作位置参数 12$ getopt ab:cd -a -b 5 -c -d 1 2 3-a -b 5 -c -d -- 1 2 3 配合set命令，将脚本原始的命令行参数解析 1set -- $( getopt -q ab:cd "$@" ) 脚本如下 1234567891011121314151617#!/bin/bashset -- $( getopt ab:cd "$@" )while [ -n "$1" ] # 不可缺少引号""do case "$1" in -a) echo "Option -a" ;; -b) echo "Option -b" shift echo "Value of option -b is: $1" ;; -c) echo "Option -c";; --) break ;; *) echo "Invalid parameter: $1";; esac shiftdone 1234567891011121314151617181920212223242526$ ./test.sh -a -b 5 -c -dOption -aOption -bValue of option -b is: 5Option -cInvalid parameter: -d$ ./test.sh -a -b5 -cdOption -aOption -bValue of option -b is: 5Option -cInvalid parameter: -d$ ./test.sh -ab5 -cdOption -aOption -bValue of option -b is: 5Option -cInvalid parameter: -d$ # 但是如下失败$ ./test.sh -ab5cdOption -aOption -bValue of option -b is: 5cd 用户输入read命令可提供用户输入接口，从标准输入或文件描述符中接受输入，实现脚本可交互。 基本输入: readread可指定多个变量，将输入的每个数据依次分配给各个变量，若变量数目不够则将剩余数据全部放入最后一个变量，如下123456789$ read first last agelouis hsu 25$ echo "$first $last, aged $age"louis hsu, aged 25$ read first last agelouis hsu 25 coolman$ echo "$age"25 coolman 指定-p，可输出命令提示符1234$ read -p "Who are you? " first last ageWho are you? louis hsu 25$ echo "$first $last, aged $age"louis hsu, aged 25 指定-t进行超时处理123$ read -t 5 first last age # 5秒$ echo "$first $last, aged $age" , aged 指定-s，隐藏输入1234$ read -s -p "Enter your passwd: " passwdEnter your passwd: # 输入`______`$ echo $passwd______ 文件输入: cat | read配合cat指令，通过管道，实现文件输入12345678$ cat test.txt | while read line; do&gt; echo $line&gt; donehelloworldlouishu25coolman 或者通过重定向实现。 脚本退出: exitshell中运行的命令都使用退出状态码(exit status)作为运行结果标识符，为0~255的整数，可通过$?查看上个执行命令的退出状态码。按照惯例成功运行命令后的退出状态码为0，常用的如下 状态码 描述 0 命令成功执行 1 一般性未知错误 2 不适合的shell命令 126 命令不可执行 127 未查找到命令 128 无效的退出参数 128+x 与linux信号x相关的严重错误 130 通过ctrl+c终止的命令 255 正常范围之外的退出状态码 shell脚本会以最后一个命令的退出码退出，用户也可通过exit命令指定。注意若退出结果超过255，会返回该值对256的模。 123456789101112131415161718192021222324252627282930$ # 正常退出$ echo "hello world!"; echo $?hello world!0$ # 未查找到命令$ unknown command; echo $?Command 'unknown' not found, but can be installed with:sudo apt install fastlink127$ # 一般性未知错误$ wget; echo $?wget: missing URLUsage: wget [OPTION]... [URL]...Try `wget --help' for more options.1$ # 用户指定退出码$ cat test.sh#!/bin/bashecho "hello world!"exit 777$ bash test.sh ; echo $?hello world!9 # 777 % 256 命令替换: ( command )shell脚本最有用的特性是将命令输出赋值给变量，有两种方法可以实现 反引号字符&#39; ( command )格式，$进行取值 例如，以时间信息创建文件123456$ time=$(date +%y%m%d) # 或 time=`date +%y%m%d`$ echo $time200505$ touch $&#123;time&#125;.txt$ ls200505.txt 运算和测试数学运算$( expr expression )仅支持整数运算。支持逻辑操作符|, &amp;、比较操作符&lt;, &lt;=, &gt;, &gt;=, =, !=、运算操作符+, -, *, /, %(注意乘号符需进行转义\*)。12345678910111213$ var1=4; var2=5$ echo $(expr $var1 + $var2)9$ echo $(expr $var1 - $var2)-1$ echo $(expr $var1 / $var2)0$ echo $(expr $var1 * $var2)expr: syntax error$ echo $(expr $var1 \* $var2)20 此外还支持部分字符串操作 $[ expression ]用[ operation ]格式将数学表达式包围，$进行取值，此时乘号符无需进行转义。支持高级运算，如幂运算**、移位运算&gt;&gt;, &lt;&lt;、位运算&amp;, |, ~、逻辑运算&amp;&amp;, ||, !等123456789101112131415161718192021222324252627$ var1=4; var2=5$ echo $(expr $var1 \* $var2)20$ echo $[ $var1 + $var2 ]9$ echo $[ $var1 - $var2 ]-1$ echo $[ $var1 / $var2 ]0$ echo $[ $var1 * $var2 ]20$ echo $[ $var1 ** $var2 ]1024$ echo $[ $var1 &lt;&lt; $var2 ]128$ echo $[ $var1 &gt;&gt; $var2 ]0$ echo $[ $var1 &amp; $var2 ]4$ echo $[ $var1 | $var2 ]5$ echo $[ $var1 &amp;&amp; $var2 ]1$ echo $[ $var1 || $var2 ]1$ echo $[ ! $var1 ]0 let expression, $(( expression ))let expression等价于(( expression ))，都支持一次性计算多个表达式，以最后一个表达式的值作为整个命令的执行结果。不同之处是，let以空格作为分隔符，(())以,作为分隔符。显然前者没有后者灵活。 同样的，(( expression ))用$进行表达式的取值。12345678$ var1=4; var2=5$ echo let $var1+$var2let 4+5 # 被视作字符串$ let sum=$var1+$var2; echo $sum # sum保存变量9$ echo $(( $var1+$var2 ))9 可快速实现变量自增、自减操作1234567891011$ i=0$ let i+=1; echo $i1$ (( i++ )); echo $i2$ (( i-- )); echo $i1$ (( ++i )); echo $i2$ (( --i )); echo $i1 内建计算器bc内建计算器支持浮点运算，实际上是一种编程语言，bash计算器能识别 数字(整数、浮点数) 变量(简单变量、数组) 注释(#或/* */格式) 表达式 编程语句(如if-then) 函数 浮点运算的精度通过内建变量scale控制，表示保留的小数位数，默认值是0123456789101112131415$ bcbc 1.07.1Copyright 1991-1994, 1997, 1998, 2000, 2004, 2006, 2008, 2012-2017 Free Software Foundation, Inc.This is free software with ABSOLUTELY NO WARRANTY.For details type `warranty'.scale # 显示当前scale0var1=4; var2=5var1 / var20scale=2 # scale指定为2var1 / var2.80quit # 退出 在脚本中使用bc命令有两种方式 单行运算： 通过命令替换和管道实现，格式为 variable=$( echo &quot;options; expression&quot; | bc ) 例如 1234$ var1=4; var2=5$ var3=$( echo "scale=2; $var1 / $var2" | bc )$ echo $var3.80 多行运算： 通过命令替换和内联输入重定向实现，格式为 123456variable=$(bc &lt;&lt; EOFoptionsstatementsexpressionsEOF) 需要注意的是，bc内部变量和shell变量是独立的，变量名可重复使用，例如 123456789101112131415161718192021222324252627282930313233$ var3=$(bc &lt;&lt; EOF&gt; scale=2&gt; $var1 / $var2 # 引用shell变量&gt; EOF&gt; )$ echo $var3.80 # 输出shell变量运算结果$ var3=$(bc &lt;&lt; EOF&gt; scale=2&gt; var1=5; var2=4 # 重新定义变量&gt; var1 / var2&gt; EOF&gt; )$ echo $var31.25 # 输出bc变量运算结果$ echo $var1 # 不会修改shell变量4$ echo $var25$ var3=$(bc &lt;&lt; EOF&gt; scale=2&gt; var1=5; var2=4 # 重新定义变量&gt; $var1 / $var2 # 引用shell变量&gt; EOF&gt; )$ echo $var3.80 # 输出shell变量运算结果$ echo $var1 # 不会修改shell变量4$ echo $var25 测试命令: test expression, [ expression ]测试命令用于检查某个条件是否成立，它可以进行数值、字符和文件三个方面的测试，还可进行复合测试，可通过test命令或[ option ]实现 数值测试: -eq, -ne, -gt, -ge, -lt, -le 参数 说明 -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 123456789101112131415$ var1=4; var2=5$ if test $var1 -le $var2; then&gt; echo "less"&gt; else&gt; echo "greater"&gt; filess$ if [ $var1 -le $var2 ]; then # 注意空格&gt; echo "less"&gt; else&gt; echo "greater"&gt; filess 字符测试: =, !=, &lt;, &gt;, -n -z 参数 说明 = 等于则为真 != 不等于则为真 &lt; 小于则为真 &gt; 大于则为真 -n 长度非0或未定义，则为真 -z 长度为0则为真 注意： 大于号&gt;和小于号&lt;必须转义，否则被视作重定向符，字符串值视作文件名； 大写字母被认为是小于小写字母的。 123456789101112131415$ var1="Test"; var2="test"$ if test $var1 \&lt; $var2; then&gt; echo "less"&gt; else&gt; echo "greater"&gt; filess$ if [ $var1 \&lt; $var2 ]; then&gt; echo "less"&gt; else&gt; echo "greater"&gt; filess 注意，若在比较数值时采用&lt;, &gt;等符号，会将数值视作字符串，同样也存在未转义识别为重定向符的问题1234567891011121314151617181920$ if [ 4 &gt; 5 ]; then&gt; echo "4 is greater than 5"&gt; elif [ 4 = 5 ]; then&gt; echo "4 is equal to 5"&gt; else&gt; echo "4 is less than 5"&gt; fi4 is greater than 5$ if [ 4 -gt 5 ]; then&gt; echo "4 is greater than 5"&gt; elif [ 4 -eq 5 ]; then&gt; echo "4 is equal to 5"&gt; else&gt; echo "4 is less than 5"&gt; fi4 is less than 5$ ls5 # 新建文件5 文件测试: -e, -d, -f, … 参数 说明 -e file 如果文件存在则为真 -d file 如果文件存在且为目录则为真 -f file 如果文件存在且为普通文件则为真 -s file 如果文件存在且至少有一个字符则为真 -c file 如果文件存在且为字符型特殊文件则为真 -b file 如果文件存在且为块特殊文件则为真 -r file 如果文件存在且可读则为真 -w file 如果文件存在且可写则为真 -x file 如果文件存在且可执行则为真 -O file 如果文件存在且属于当前用户所有则为真 -G file 如果文件存在且默认组与当前用户相同则为真 file1 -nt file2 文件1比文件2新则为真 file1 -ot file2 文件1比文件2旧则为真 复合条件测试: !, -o / ||, -a / &amp;&amp; 运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。 -o / \ \ 或运算，有一个表达式为 true 则返回 true，满足就近原则，即运算符前表达式为真则跳过后一表达式 [ condition1 -o condition1 ] 或 [ condition1 ] \ \ [ condition1 ] -a / &amp;&amp; 与运算，两个表达式都为 true 才返回 true。 [ condition1 -a condition1 ]或 [ condition1 ] &amp;&amp; [ condition1 ] 12345678910111213$ if [ $var1 -le $var2 -o $var3 -le $var4 ]; then&gt; echo "condition 1"&gt; else&gt; echo "condition 2"&gt; ficondition 1$ if [ $var1 -le $var2 ] || [ $var3 -le $var4 ]; then&gt; echo "condition 1"&gt; else&gt; echo "condition 2"&gt; ficondition 1 结构化命令分支if-then-elif-else-fi完整的if-then语句如下12345678910if condition/commandthen commands # 多个命令elif condition/commandthen commands[...] # 多个elif分支else commandsfi 注意，if后可接命令或测试语句，当所接命令退出码为0时判定为真，测试语句逻辑为真时判定为真。 1234567891011121314$ if pwd; then&gt; echo "pwd successfully exit"&gt; fi/home/louishsupwd successfully exit$ if [ 4 -gt 5 ]; then&gt; echo "4 is greater than 5"&gt; elif [ 4 -eq 5 ]; then&gt; echo "4 is equal to 5"&gt; else&gt; echo "4 is less than 5"&gt; fi4 is less than 5 支持针对字符串比较的高级特性，如模式匹配，使用[[ expression ]]1234$ if [[ $USER == l* ]]; then # 双等号echo "This is louishsu!"fiThis is louishsu! case-in多选择语句，可以用case匹配一个值与一个模式，如果匹配成功，执行相匹配的命令。取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。完整格式如下1234567891011121314case variable inpattern1) # 以右括号结束 commands ;; # 以;;结束，表示 breakpattern2) commands ;;[...]patternN) commands ;;*) # 无一匹配模式 commands ;; 1234567891011121314$ var=3$ case $var in&gt; 1) echo "1"&gt; ;;&gt; 2) echo "2"&gt; ;;&gt; 3) echo "3"&gt; ;;&gt; 4) echo "4"&gt; ;;&gt; *) echo "others"&gt; esac3 循环for-do-done 迭代 用于迭代列表，in列表是可选的，如果不用它，for循环使用命令行的位置参数。在迭代结束后，variable保存itemN的值且在不修改的情况下一直有效。 1234for variable in item1 item2 ... itemN # 注意无`()`do commandsdone 以输出数字列表为例 123456789101112131415$ for number in 1 2 3; do&gt; echo "The number is $number"&gt; doneThe number is 1The number is 2The number is 3$ nums=(1 2 3)# $ for number in $nums; do # 一种错误做法，只会输出1$ for number in $&#123;nums[*]&#125;; do # 迭代数组&gt; echo "The number is $number"&gt; doneThe number is 1The number is 2The number is 3 迭代字符串与数组有所不同 12345678$ str="I am louishsu"$ for wd in $str; do # 迭代字符串# $ for wd in $&#123;str[*]&#125;; do # 同上，也可迭代字符串&gt; echo $wd&gt; doneIamlouishsu 还可迭代输出命令结果、通配符等，in后可接多个命令或目录 1234567891011121314$ for file in $( ls; pwd ); do&gt; echo "$file"&gt; doneDownloadsanaconda3backup/home/louishsu$ for file in /home/louishsu/*; do&gt; echo $file&gt; done/home/louishsu/Downloads/home/louishsu/anaconda3/home/louishsu/backup C/C++风格 1234for (( variable assignment ; condition ; iteration process ))do commandsdone 注意 变量赋值可带等号； condition中变量不需$； 可同时定义两个变量。 12345for (( i=0, j=0; i&lt;3 &amp;&amp; j&lt;4; i++, j+=2 )); do&gt; echo $i, $j&gt; done0, 01, 2 while-do-done基本格式如下，在condition为假时停止循环1234while conditiondo commandsdone 1234567891011121314$ var=0$ while echo $var &amp;&amp; [ $var -le 3 ]; do &gt; echo "loop"&gt; (( var++ ))&gt; done0loop1loop2loop3loop4 # 注意$var为4时，`echo $var`执行了一次 until-do-done基本格式如下，与while相反，在condition为真时停止循环1234until conditiondo commandsdone 123456$ var=0$ until echo $var &amp;&amp; [ $var -le 3 ]; do&gt; echo "loop"&gt; (( var++ ))&gt; done0 循环控制: break, continue循环控制语句，包括break/continue，作用同C/C++或Python，不做过多介绍12345678910111213#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字:" read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的! 游戏结束" break ;; esacdone 1234567891011121314#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字: " read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的!" continue echo "游戏结束" # 永远不会执行 ;; esacdone 函数创建和调用函数创建函数格式如下，注意函数名唯一，且shell中的函数支持递归调用123function func &#123; commands&#125; 调用函数时，在行中指定函数即可，但是函数定义必须在调用之前12345commands[...]func[...]commands 参数传递作用域: local默认情况下，脚本中定义的任何变量都是全局变量(包括函数体内定义的变量)，可以在函数体中读取全局变量进行操作 12345678910111213#!/bin/bashfunction func &#123; var1=3 # 修改全局变量 var2=4 # 定义全局变量&#125;# 仅定义var1var1=2echo "$var1, $var2"# 函数中定义var2，仍为全局变量funcecho "$var1, $var2" 123$ ./test.sh2,3, 4 在函数体内可定义局部变量，使用local关键字，注意 局部变量在函数体外不可见； 即使声明相同名称的局部变量，shell也会保证两个变量是分离的。 12345678910111213#!/bin/bashfunction func &#123; local var1=3 # 定义局部变量 local var2=4 # 定义局部变量&#125;# 仅定义var1var1=2echo "$var1, $var2"# 函数中定义var2funcecho "$var1, $var2" 123$ ./test.sh2,2, 变量参数类似shell脚本的参数传递，函数同样使用标准的参数环境变量进行参数传递，用$0表示函数名，$1, $2, ...表示参数，用$#获取参数数目，用$*/$@获取全部参数。 由于函数使用特殊参数环境变量进行参数传递，因此无法直接获取脚本在命令行中的参数值，两者不关联。 123456789#!/bin/bashfunction func &#123; echo "These are function parameters: $*" echo "There are $# parameters" echo "The last parameter is: $&#123;!#&#125;"&#125;echo -e "These are script parameters: $*\n"func 5 6 7 123456$ ./test.sh 1 2 3These are script parameters: 1 2 3These are function parameters: 5 6 7There are 3 parametersThe last parameter is: 7 数组参数与函数传递数组，不能简单通过数组名进行；利用命令替换获取返回数组。 1234567891011121314#!/bin/bashfunction func &#123; local array=( $(echo "$@") ) for (( i = 0; i &lt; $&#123;#array[*]&#125;; i++ )) &#123; (( array[$i]++ )) &#125; echo "$&#123;array[*]&#125;"&#125;array=(1 2 3)echo "Input: $&#123;array[*]&#125;"ret=( $( func $(echo "$&#123;array[*]&#125;") ) )echo "Output: $&#123;ret[*]&#125;" 123$ ./test.shInput: 1 2 3Output: 2 3 4 返回值: return, echo 默认退出状态码 若函数未指定返回语句return，则执行结束后标准变量$?内存储函数最后一条命令的退出码状态。 指定返回值 使用return退出函数并返回指定的退出状态码，同样地保存在标准变量$?中，但是用这种方式获取返回值需要注意以下两点 函数退出后立即取返回值，防止被覆盖； 退出码范围是0~255； 若函数中命令执行错误导致提前退出函数，则此时$?中为错误状态码，不可作为函数输出。 12345678#!/bin/bashfunction add &#123; return $[ $1 + $2 ]&#125;var1=4; var2=5add $var1 $var2echo "$var1 + $var2 = $?" 12$ ./test.sh4 + 5 = 9 用命令替换获取函数输出作为返回值 这种方式可以避免与状态码复用，还可以返回如浮点、字符串等类型 12345678#!/bin/bashfunction add &#123; echo "$[ $1 + $2 ]"&#125;var1=4; var2=5sum=$( add $var1 $var2 )echo "$var1 + $var2 = $sum" 注意到，函数中的echo并没有输出到STDOUT中 1234567891011121314151617 $ ./test.sh 4 + 5 = 9 ``` # 文件包含: source用`source`命令在当前shell上下文中执行命令，而不是创建新shell，其快捷别名为**点操作符**(dot operator)例如创建函数脚本`funcs.sh```` bash#!/bin/bashfunction add &#123; echo "$[ $1 + $2 ]"&#125;function sub &#123; echo "$[ $1 - $2 ]"&#125; 在test.sh中调用函数 1234567#!/bin/bash# source funcs.sh. funcs.shvar1=4; var2=5sum=$( add $var1 $var2 )echo "Sum of $var1 and $var2 is $sum." 12$ ./test.shSum of 4 and 5 is 9. 总结 注意区分各类括号的使用 变量取值：${ variable } 命令替换：$( command ) 整数计算：$[ expression ] 多行整数计算：$(( expression1, expression2, ... )) 测试：[ expression ] 高级字符串比较测试：[[ expression ]] 注意数值比较和字符串比较的差异 重定向中符号的使用 注意函数参数的传递]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【算法】KMP]]></title>
    <url>%2F2020%2F04%2F22%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91KMP%2F</url>
    <content type="text"><![CDATA[前言字符串匹配问题是指，给定source字符串和一个target字符串，你应该在source字符串中找出target字符串出现的第一个位置(从$0$开始)。如果不存在，则返回$-1$，例如输入：source = &quot;abcaabaabcaabcacabaa&quot;，target = &quot;abcacab&quot;输出；$11$ 朴素模式匹配暴力匹配是指依次对source中的子字符串进行匹配，以前言中case为例，匹配流程如下12345678910111213141516171819注：&#39;[]&#39;表示已匹配字符串，&#39;&lt;&gt;&#39;表示不匹配字符&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;第1次匹配：从souce第1个字符开始，&#39;a&#39;与&#39;c&#39;不匹配，中断[abca]&lt;a&gt;baabcaabcacabaa[abca]&lt;c&gt;ab-------------------------------------------------------------------------第2次匹配：从souce第2个字符开始，&#39;b&#39;与&#39;a&#39;不匹配，中断a&lt;b&gt;caabaabcaabcacabaa &lt;a&gt;bcacab-------------------------------------------------------------------------第3次匹配：从souce第3个字符开始，&#39;c&#39;与&#39;a&#39;不匹配，中断ab&lt;c&gt;aabaabcaabcacabaa &lt;a&gt;bcacab-------------------------------------------------------------------------...-------------------------------------------------------------------------最后一次匹配：从source第12个字符开始，匹配成功，返回索引11abcaabaabca[abcacab]aa [abcacab] 以上算法的时间复杂度为$O(mn)$(最坏为$O((n - m + 1) \times m)$)，其中$m,n$表示两字符串的长度。 KMP匹配KMP算法是一种改进的字符串匹配算法，由D.E.Knuth，J.H.Morris和V.R.Pratt提出的，简称KMP算法。它解决的问题是，在某次匹配过程中出现不匹配字符时，应如何跳转模板指针以减少重复匹配。在长度为$n$的字符串中寻找长度为$m$的字符串，时间复杂度为$O(m + n)$。 在第一次匹配失败时，已知的信息是前$4$个字符成功匹配，且最后一个匹配字符为a，如下12[abca]&lt;a&gt;baabcaabcacabaa[abca]&lt;c&gt;ab 注意到字符串&quot;abca&quot;存在相同的前缀和后缀&#39;a&#39;，那么只需将target后移$3$位，使前缀处于“已匹配”状态，然后从两字符串相应位置继续匹配。即可减少匹配，即123source: abc&#123;a&#125;abaabcaabcacabaatarget: abc&#123;a&#125;cabmoved: &#123;a&#125;bcacab 前缀、后缀对于的字符串&quot;abcdefgh&quot;，其具有的前缀(prefix)、后缀(suffix)集合分别为 类别 集合 前缀 &quot;a&quot;, &quot;ab&quot;, &quot;abc&quot;, &quot;abcd&quot;, &quot;abcde&quot;, &quot;abcdef&quot;, &quot;abcdefg&quot; 后缀 &quot;bcdefgh&quot;, &quot;cdefgh&quot;, &quot;defgh&quot;, &quot;efgh&quot;, &quot;fgh&quot;, &quot;gh&quot;, &quot;h&quot; 部分匹配值给定字符串target后，可计算其子字符串的部分匹配值，即子字符串前后缀集合交集中，最长的字符串长度。例如&quot;abcacab&quot;的部分匹配值计算如下 子字符串 前缀集合 后缀集合 部分匹配值 &quot;a&quot; 0 &quot;ab&quot; &quot;a&quot; &quot;b&quot; 0 &quot;abc&quot; &quot;a&quot;, &quot;ab&quot; &quot;bc&quot;, &quot;c&quot; 0 &quot;abca&quot; &quot;a&quot;, &quot;ab&quot;, &quot;abc&quot; &quot;bca&quot;, &quot;ca&quot;, &quot;a&quot; 1 &quot;abcac&quot; &quot;a&quot;, &quot;ab&quot;, &quot;abc&quot;, &quot;abca&quot; &quot;bcac&quot;, &quot;cac&quot;, &quot;ac&quot;, &quot;c&quot; 0 &quot;abcaca&quot; &quot;a&quot;, &quot;ab&quot;, &quot;abc&quot;, &quot;abca&quot;, &quot;abcac&quot; &quot;bcaca&quot;, &quot;caca&quot;, &quot;aca&quot;, &quot;ca&quot;, &quot;a&quot; 1 &quot;abcacab&quot; &quot;a&quot;, &quot;ab&quot;, &quot;abc&quot;, &quot;abca&quot;, &quot;abcac&quot;, &quot;abcaca&quot; &quot;bcacab&quot;, &quot;bcacab&quot;, &quot;cacab&quot;, &quot;acab&quot;, &quot;cab&quot;, &quot;ab&quot;, &quot;b&quot; 2 匹配过程经过上述说明后，target后移位数可由下式计算 后移位数 = 已匹配位数(也即\rm{target}指针位置) - 已匹配子字符串的部分匹配值实际编程中，修改target指针位置为已匹配子字符串的部分匹配值即可 指针位置 := 指针位置 - 后移位数 = 已匹配子字符串的部分匹配值匹配流程如下1234567891011121314151617181920212223242526272829303132333435363738394041424344注：用i, j分别作为source, target的下标指针；&#39;|&#39;表示匹配开始的位置i &#x3D; 0, j &#x3D; 0 (初始化)&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;第1次匹配：从souce第1(i+1)个字符开始，&#39;a&#39;与&#39;c&#39;不匹配，中断|[abca]&lt;a&gt;baabcaabcacabaa|[abca]&lt;c&gt;abi &#x3D; 4, j &#x3D; 4已匹配4(j)个字符，子字符串&quot;abca&quot;的部分匹配值为1 &#x3D;&gt; target后移3位，更新 j &#x3D; 1-------------------------------------------------------------------------第2次匹配：从souce第5(i+1)个字符开始，&#39;a&#39;与&#39;b&#39;不匹配，中断abc[a]|&lt;a&gt;baabcaabcacabaa [a]|&lt;b&gt;cacabi &#x3D; 4, j &#x3D; 1已匹配1(j)个字符，子字符串&quot;a&quot;的部分匹配值为0 &#x3D;&gt; target后移1位，更新 j &#x3D; 0-------------------------------------------------------------------------第3次匹配：从souce第5(i+1)个字符开始，&#39;a&#39;与&#39;c&#39;不匹配，中断abca|[ab]&lt;a&gt;abcaabcacabaa |[ab]&lt;c&gt;acabi &#x3D; 6, j &#x3D; 2已匹配2(j)个字符，子字符串&quot;ab&quot;的部分匹配值为0 &#x3D;&gt; target后移2位，更新 j &#x3D; 0-------------------------------------------------------------------------第4次匹配：从souce第7(i+1)个字符开始，&#39;a&#39;与&#39;b&#39;不匹配，中断abcaab|[a]&lt;a&gt;bcaabcacabaa |[a]&lt;b&gt;cacabi &#x3D; 7, j &#x3D; 1已匹配1(j)个字符，子字符串&quot;a&quot;的部分匹配值为0 &#x3D;&gt; target后移1位，更新 j &#x3D; 0-------------------------------------------------------------------------第5次匹配：从souce第8(i+1)个字符开始，&#39;a&#39;与&#39;c&#39;不匹配，中断abcaaba|[abca]&lt;a&gt;bcacabaa |[abca]&lt;c&gt;abi &#x3D; 11, j &#x3D; 4已匹配4(j)个字符，子字符串&quot;abca&quot;的部分匹配值为1 &#x3D;&gt; target后移3位，更新 j &#x3D; 1-------------------------------------------------------------------------第6次匹配：从souce第12(i+1)个字符开始，&#39;a&#39;与&#39;b&#39;不匹配，中断abcaabaabc[a]|&lt;a&gt;bcacabaa [a]|&lt;b&gt;cacabi &#x3D; 11, j &#x3D; 1已匹配1(j)个字符，子字符串&quot;a&quot;的部分匹配值为0 &#x3D;&gt; target后移1位，更新 j &#x3D; 0-------------------------------------------------------------------------第7次匹配：从souce第12(i+1)个字符开始，匹配完成abcaabaabca|[abcacab]aa |[abcacab]i &#x3D; 18, j &#x3D; 7返回 j - i &#x3D; 11 实现12345678910111213141516171819202122232425262728293031323334353637383940414243int kmp(const std::string&amp; source, const std::string&amp; target)&#123; int m = source.size(); int n = target.size(); // ======================= 计算部分匹配值 ======================= int* next = new int[n]; for (int i = 0; i &lt; n; i++) &#123; next[i] = 0; // 截取子字符串 std::string subPattern = target.substr(0, i + 1); int _n = subPattern.size(); for (int _len = 1; _len &lt; i + 1; _len++) &#123; // 串前/后缀长度 std::string prefix = subPattern.substr(0, _len); // 前缀 std::string suffix = subPattern.substr(_n - _len, _len); // 后缀 if (prefix == suffix) &#123; next[i] = _len; // 共有元素的长度 &#125; &#125; &#125; // ========================== 开始匹配 ========================== int i = 0, j = 0; while (i &lt; m &amp;&amp; j &lt; n) &#123; // --- 字符相同，两个指针共同前进 --- if (source[i] == target[j]) &#123; i++; j++; continue; &#125; // ------ 字符不相同需要跳转 ------- if (j == 0) &#123; i++; &#125; else if (j &gt; 0) &#123; j = next[j - 1]; // 后缀不匹配，但前缀可能匹配，移动到相同前缀后一位的位置 &#125; &#125; delete [] next; // source到达末尾，target未到达末尾 if (i == 0 &amp;&amp; j != n) return -1 // 同时到达末尾；source未到达末尾，target到达末尾 return i - j; // 开始索引&#125; Reference 字符串匹配的KMP算法 - 阮一峰的网络日志。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markov Chain Monte Carlo]]></title>
    <url>%2F2020%2F04%2F15%2FMarkov-Chain-Monte-Carlo%2F</url>
    <content type="text"><![CDATA[前言马尔科夫链蒙特卡罗法(Markov Chain Monte Carlo, MCMC)是以马尔可夫链为概率模型的蒙特卡罗法。通过构建马尔可夫链，基于该马尔可夫链进行随机游走，产生样本序列，并使用这些样本进行近似数值计算。 蒙特卡罗法蒙特卡罗法可应用在随机抽样、数学期望估计、定积分计算等方面。 随机抽样蒙特卡罗法需要解决的问题是，假设概率分布定义已知，通过抽样获得概率分布的随机样本，用这些随机样本对概率分布特征进行分析。核心是随机抽样(random sampling)，例如 从样本得到经验分布，从而估计总体分布； 从样本计算样本均值，从而估计总体期望； …… 蒙特卡罗法有： 直接抽样法； 接受-拒绝抽样法； 重要性抽样法； …… 接受-拒绝抽样法基本思想如下：假设$p(x)$不可以直接抽样，寻找建议分布(proposal distribution)$q(x)$，使下式成立 c \cdot q(x) \geq p(x), \quad c > 0 \tag{1.1}按照$q(x)$进行抽样，假设得到结果$x$，设置阈值$t \in [0, 1]$，在下式成立时接受该抽样 \frac{p(x)}{c \cdot q(x)} > t \tag{1.2}数学期望估计假设有随机变量$X \in \mathcal{X}$，概率密度为$p(x)$，定义$Y = f(X)$为在定义域$\mathcal{X}$上的函数，求$Y$的数学期望$EY$。 回想一下学习概率论时这种问题的求解方法，通过代换$Y$的分布函数$F_Y(y)$，利用映射$Y = f(X)$将其转换到$X$的分布函数$F_X(x)$中进行求解，即 \begin{aligned} F_Y(y) = P(Y \leq y) = P\{f(X) \leq y\} \\ \Rightarrow P\{X \leq f^{-1}(y)\} = F_X(f^{-1}(y)) \end{aligned} \tag{2.1}其中 F_X(x) = \int p(x) dx \tag{2.2}$f^{-1}(y)$已知，求取$F_Y(y)$后，有 p(y) = \frac{d}{dy} F_Y(y) \tag{2.3}EY = \int y p(y) dy \tag{2.4}在计算$(2.2)$时，若$p(x)$是非标准的，则难以求取积分，需通过数值方法求解近似。蒙特卡罗法可用于求取数学期望估计(estimation of mathematical expectation)：按照概率分布$p(x)$独立同分布地在定义域$\mathcal{X}$多次抽取，得到样本集${ x^{(1)}, x^{(2)}, \cdots, x^{(n)} }$，由下式计算样本均值作为期望的估计 EY \approx \frac{1}{n} \sum_{i = 1}^{n} f(x^{(i)}) \tag{3}积分计算假设有函数$f(x)$，需求取其在区间$\mathcal{X}$上的定积分 I = \int_{\mathcal{X}} f(x) dx \tag{4.1}可将$f(x)$分解为一个函数$g(x)$与某概率密度$p(x)$的乘积 f(x) = g(x) \cdot p(x) \tag{4.2} 实际上，可指定任意概率密度函数$p(x)$得到$g(x)$，即任意定积分都可以表示为某函数的数学期望的形式 g(x) = \frac{f(x)}{p(x)} 那么用计算期望的方法，有 I = \int_{\mathcal{X}} f(x) dx = \int_{\mathcal{X}} g(x) \cdot p(x) dx = E[g(x)] \approx \rm{mean}(f(x^{(i)})) \tag{4.3}马尔可夫链定义设一随机变量$X$(离散或连续均可)，其取值集合称为状态空间$\mathcal{S}$，经过$T$次采样得到时间序列如下，该序列构成随机过程(stochastic process) X = \{X^{(1)}, X^{(2)}, \cdots, X^{(t)}, \cdots, X^{(T)}\} \tag{5.1}假设在时刻$0$，$X^{(0)}$服从初始状态分布 P(X^{(0)}) = \pi(0) \tag{5.2}马尔可夫性：时刻$t(t \geq 1)$采样$X^{(t)}$只依赖于前一时刻$X^{(t - 1)}$，即 P(X^{(t)} | X^{(0)}, X^{(1)}, \cdots, X^{(t - 1)}) = P(X^{(t)} | X^{(t - 1)}) \tag{5.3}具有马尔可夫性的随机序列称为马尔可夫链(Markov chain)，或马尔可夫过程(Markov process)，条件概率分布$P(X^{(t)} | X^{(t - 1)})$称作马尔可夫链的转移概率分布。拓展为$n$阶马尔可夫链，满足$n$阶马尔可夫性，则有 P(X^{(t)} | X^{(0)}X^{(1)} \cdots X^{(t-2)}X^{(t-1)}) = P(X^{(t)} | X^{(t-n)}X^{(t-n-1)} \cdots X^{(t-2)}X^{(t-1)}) \tag{5.4}若马尔可夫链满足$P(X^{(t)} | X^{(t - 1)})$与时间$t$无关(常数)，称该马尔可夫链是时间齐次的(time homogeneous Markov chain)，即 P(X^{(t)} | X^{(t - 1)}) = P(X^{(t + s)} | X^{(t - 1 + s)}), \quad t = 1, 2, \cdots, \quad s = 1, 2, \cdots \tag{5.5} 关于马尔可夫模型的概念的可参考隐马尔可夫模型。 离散与连续描述离散状态离散状态的马尔可夫链，其概率转移分布可以用矩阵表示。记时刻$(t-1)$处于状态$s_j$转移到$t$时刻处于状态$s_i$的转移概率为 \begin{aligned} p_{ij} = P(X^{(t)} = s_i | X^{(t - 1)} = s_j), \quad i, j = 1, 2, \cdots \\ s.t. \quad p_{ij} \geq 0, \quad \sum_i p_{ij} = 1 \end{aligned} \tag{6.1}记作转移概率矩阵$P$，注意列和为$1$(转移到状态$s_i$概率和为$1$) P_{|\mathcal{S}| \times |\mathcal{S}|} = \begin{bmatrix} p_{11} & p_{12} & \cdots \\ p_{21} & p_{22} & \cdots \\ \vdots & \vdots & \ddots \\ \end{bmatrix} \tag{6.2}在时刻$t$时，有$X^{(t)}$的状态分布$\pi(t)$，其中元素$\pi_i(t)$表示$X^{(t)}$位于状态$s_i$的概率$P(X^{(t)} = s_i)$ \pi(t) = \begin{bmatrix} \pi_1(t) \\ \pi_2(t) \\ \vdots \end{bmatrix} \tag{7.1}特别地，通常初始分布$\pi(0)$只有一个元素为$1$，其余都是$0$，表示马尔可夫链从某一具体状态开始。 \pi(0) = \begin{bmatrix} \vdots \\ 1 \\ \vdots \end{bmatrix} \tag{7.2}那么有递推公式 \pi(t) = P \cdot \pi(t - 1) \tag{8.1}那么有$t$步转移概率矩阵$P^t$，使得下式成立 \pi(t) = P^t \cdot \pi(0) \tag{8.2}若存在某分布$\pi$使下式成立，则称$\pi$为马尔可夫链的平稳分布 \pi = P \cdot \pi \tag{8.3}注意，当离散状态马尔可夫链有无穷个状态时，可能不存在平稳分布。 连续状态连续状态的马尔可夫链，其概率转移分布可以用概率转移核(transition kernel)表示。设$\mathcal{S}$为连续状态空间，对任意$x \in \mathcal{S}, A \in \mathcal{S}$，从$x$转移到$A$的转移概率$P(X^{(t)} = A | X^{(t - 1)} = x)$，称为转移核 P(x, A) = P(X^{(t)} = A | X^{(t - 1)} = x) = \int_A p(x, y) dy \tag{9.1}其中$p(x, y)$为概率密度函数(有时也被称为转移核)，且满足 p(x, y) \geq 0, \quad P(x, \mathcal{S}) = \int_{\mathcal{S}} p(x, y) dy = 1 \tag{9.2}同样的，连续状态也存在平稳分布描述。若$\pi(x)$满足以下条件，称$\pi(x)$为该马尔可夫链的平稳分布 \pi(y) = \int p(x, y) \pi(x) dx, \quad \forall y \in \mathcal{S} \tag{10.1}性质以下均采用离散状态马尔可夫链进行性质描述，可推广至连续状态。 可约/不可约对于任意状态$s_i, s_j \in \mathcal{S}$，时刻$0$从$s_j$出发，时刻$t(t&gt;0)$到达状态$s_i$的概率$P(X^{(t)} = s_j | X^{(0)} = s_i) &gt; 0$，那么称此马尔可夫链是不可约的(irreducible)，否则称可约的(reducible)。 不可约马尔可夫链，概率值恒不为$0$，从任意状态出发，充分长时间后可到达任意状态。 例如以下转移概率矩阵对应的马尔可夫链，存在平稳分布$\pi = \begin{bmatrix}0 &amp; 0 &amp; 1\end{bmatrix}^T$，循环停留在状态$3$，故可约 P = \begin{bmatrix} 0 & 1/2 & 0 \\ 1 & 0 & 0 \\ 0 & 1/2 & 1 \\ \end{bmatrix} \tag{11}周期/非周期时刻$0$从任意状态$s \in \mathcal{S}$出发，返回状态$s$的时刻$t$构成集合${t | P(X^{(t)} = s | X^{(0)} = s) &gt; 0}$，其元素的最大公约数为$1$，则称该马尔可夫链是非周期的(aperiodic)，否则是周期的(periodic)。 非周期马尔可夫链，不存在状态$s$，使从该状态出发回到该状态经历时间呈现一定的周期性。 例如以下转移概率矩阵对应的马尔可夫链，存在平稳分布$\pi = \begin{bmatrix}1/3 &amp; 1/3 &amp; 1/3\end{bmatrix}^T$，从任意状态出发返回该状态的时刻为${3, 6, 9, \cdots}$，呈现周期性 P = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ \end{bmatrix} \tag{12}正常返/非正常返对于任意状态$s_i, s_j \in \mathcal{S}$，时刻$0$从$s_j$出发，在时刻$t$首次转移到状态$s_i$的概率记作$p_{ij}^t$，若对所有状态都满足$\lim_{t \rightarrow \infty} p_{ij}^t &gt; 0$，则称该马尔可夫链是正常返的(recurrent)。 正常返马尔可夫链，从某一状态出发，时间趋于无穷时，到达其他任意状态的概率不为$0$。 根据上述三条性质，可得以下两条定理 不可约、非周期的有限状态马尔可夫链，存在唯一平稳分布； 不可约、非周期、正常返的马尔可夫链，存在唯一平稳分布； 可逆/不可逆若某马尔可夫链$X$存在状态分布$\pi$，对于任意状态$s_i, s_j \in \mathcal{S}$，在任意时刻满足下式，则称此马尔可夫链是可逆的(reversible) P(X^{(t)} = s_i | X^{(t-1)} = s_j) \cdot \pi_j = P(X^{(t-1)} = s_j | X^{(t)} = s_i) \cdot \pi_i, \quad i, j = 1, 2, \cdots \tag{12.1}也即 p_{ji} \cdot \pi_j = p_{ij} \cdot \pi_i \tag{12.2}$(13.2)$也被称作细致平衡方程(detailed balance equation)，并且满足细致平衡方程的状态分布$\pi$就是该马尔可夫链的平稳分布。也说明可逆马尔可夫链一定存在唯一平稳分布(充分条件)。 证明： (P \cdot \pi)_i = \sum_j p_{ij} \cdot \pi_j = \sum_j p_{ji} \cdot \pi_i = \pi_i \Rightarrow P \cdot \pi = \pi \tag{12.3} 以可逆马尔可夫链的平稳分布作为初始分布，进行随机状态转移，那么在过去或未来任意时刻的状态分布都是该平稳分布。 遍历定理设有一不可约、非周期且正常返的马尔可夫链$X$，有唯一平稳分布$\pi$，那么转移概率的极限分布是马尔可夫链的平稳分布 \lim_{t \rightarrow \infty} P(X^{(t)} = s_i | X^{(0)} = s_j) = \pi_i > 0, \quad i, j = 1, 2, \cdots \tag{13.1} 该马尔可夫链在时间趋于无穷时，其状态分布趋向于平稳分布；不可约、非周期且正常返保证时间趋于无穷时，达到任意状态的概率不为$0$。 若$f(X)$是定义在状态空间$\mathcal{S}$上的函数，且$E_{\pi}[f(x)] &lt; \infty$，那么依据大数定理，有 P\{\hat{f}^{(t)} \rightarrow E_{\pi}[f(x)]\} = 1 \tag{13.2} 在该马尔可夫链下，随机变量的函数的样本均值以概率$1$收敛于该函数的数学期望。 其中$\hat{f}^{(t)}$为样本均值，$E_{\pi}[f(x)]$为待估期望，即 \begin{cases} \hat{f}^{(t)} = \frac{1}{t} \sum_{s=1}^t f(x^{(s)}) \\ E_{\pi}[f(x)] = \sum_i f(s_i) \pi_i \end{cases} \tag{13.3} 该马尔可夫链可以用样本均值作为时间均值的估计，而数学期望是空间均值。 可取足够大的整数$m$，认为满足要求的马尔可夫链经过$m$次迭代后，其状态分布即平稳分布，然后进行$n$次采样，计算样本均值作为期望的估计 \hat{E}f = \frac{1}{n} \sum_{i=1}^n f(x^{(i)}) \tag{13.4}马尔可夫链蒙特卡罗法MCMC是蒙特卡罗法的一种，适用于随机变量是多元的、密度函数是非标准的、随机变量各分量不独立等情况。 再次描述待求解问题：假设有多元随机变量$X \in \mathcal{X}$，概率密度为$p(x)$，定义$Y = f(X)$为在定义域$\mathcal{X}$上的函数，求$Y$的数学期望$EY$。 用MCMC解决该问题的方法是：在随机变量$X$的状态空间$\mathcal{S}$上定义满足遍历定理要求(不可约、非周期、正常返的)的马尔可夫链$X$，使其平稳分布为抽样的目标分布。时间趋于无穷时，样本分布趋近平稳分布，根据遍历定理可求解数学期望。 注意：马尔可夫链蒙特卡罗法中得到的样本序列，相邻的样本点是相关的，因此在需要独立样本时，可以在样本序列中再次抽样(如每隔一段时间抽样)，将子样本集作为独立样本集合。 那么现在的关键问题是如何构建具体的马尔可夫链，即确定转移概率矩阵或转移核函数。可通过定义特殊的转移概率矩阵或转移核函数保证遍历定理成立。常用方法有Metropolis-Hastings算法、吉布斯抽样。 Metropolis-Hastings算法原理假设要抽样的概率分布为$p(x)$，MH算法采用转移核为$p(x, x’)$的马尔可夫链 p(x, x') = q(x, x') \cdot \alpha(x, x') \tag{14.1}其中 $q(x, x’)$：称建议分布(proposal distribution)，是另一个马尔可夫链的转移核，且不可约(概率值恒不为$0$)，同时易采样； $\alpha(x, x’)$：称接受分布(acceptance distribution)，定义为 \alpha(x, x') = \min\left\{1, \frac{p(x') \cdot q(x', x)} {p(x) \cdot q(x, x')} \right\} \tag{14.2} 定理：由$(14.1 \sim 14.2)$构建的马尔可夫链是可逆的，且$p(x)$是其平稳分布证明：由$(12.2)$可证得可逆 \begin{aligned} p(x) \cdot p(x, x') & = p(x) \cdot q(x, x') \min\left\{1, \frac{p(x') \cdot q(x', x)} {p(x) \cdot q(x, x')} \right\} \\ & = \min\left\{p(x) \cdot q(x, x'), p(x') \cdot q(x', x) \right\} \\ & = p(x') \cdot q(x', x) \min\left\{\frac{p(x) \cdot q(x, x')} {p(x') \cdot q(x', x)}, 1\right\} \\ & = p(x') \cdot p(x', x) \end{aligned} \Rightarrow 可逆那么由$(10.1)$，$p(x)$为平稳分布 \int p(x) \cdot p(x, x') dx = \int p(x') \cdot p(x', x) dx = p(x') \int p(x', x) dx = p(x') 如果在时刻$(t-1)$处于状态$x$，先按建议分布$q(x, x’)$产生候选$x’$，按接受分布$\alpha(x, x’)$抽样，以概率$\alpha(x, x’)$接受$x’$，否则拒绝，时刻$t$仍停留在$x$。即在区间$(0, 1)$上的均匀分布中抽取随机数$u$，当$u \leq \alpha(x, x’)$时接受。 建议分布的选择及接受分布的确定根据$(14.1 \sim 2)$，在选择建议分布$q(x, x’)$后，$\alpha(x, x’)$也随之确定 p(x, x') = q(x, x') \cdot \alpha(x, x') \tag{14.1} \alpha(x, x') = \min\left\{1, \frac{p(x') \cdot q(x', x)} {p(x) \cdot q(x, x')} \right\} \tag{14.2}已证明以上两式构建的马尔可夫链是可逆的，存在唯一平稳分布$\pi = p(x)$。只需通过选择一个不可约、易采样的分布作为建议分布，有如下两种选择： Metropolis-Hastings选择：假设分布是对称的，即对任意$x, x’$，有 \begin{aligned} q(x, x') = q(x', x) \\ \Rightarrow \alpha(x, x') = \min\left\{1, \frac{p(x')} {p(x)} \right\} \end{aligned} \tag{15.1} 特点是当$x’$与$x$接近时，$q(x, x’)$概率高，即状态转移在附近点的可能性更大。有以下两个特例： $q(x, x’) = q(x’ | x)$，即条件概率分布，定义为均值为$x$，协方差矩阵是常数矩阵的多元正态分布； $q(x, x’) = q(|x, x’|)$，称随机游走Metropolis算法，例如q(x, x') \propto \exp(- \frac{(x' - x)^2}{2}) \tag{15.2} 独立抽样：假设$q(x, x’)$与当前状态$x$无关，即 \begin{aligned} q(x, x') = q(x') \\ \Rightarrow \alpha(x, x') = \min\left\{1, \frac{p(x') / q(x')} {p(x) / q(x)} \right\} \end{aligned} \tag{15.3} 独立抽样实现简单，但收敛速度慢，通常选择接近目标分布$p(x)$的分布作为建议分布。 吉布斯抽样吉布斯抽样(Gibbs sampling)可视作Metropolis-Hastings算法的特殊情况，用于多元变量联合分布的抽样和估计。 满条件分布首先介绍满条件分布(full conditional distribution)：MCMC的目标分布通常是多元联合概率分布$p(x)$，其中$x$为$k$维度随机变量，对于 \begin{aligned} I \subset K = \{1, 2, \cdots, k\} \\ x_I = \{x_i, i \in I\}, \quad x_{-I} = \{x_i, i \notin I\} \end{aligned} \tag{16.1}若条件概率分布$p(x_I | x_{-I})$中$k$个维度全部出现，则称这种条件概率分布为满条件分布，有以下性质 p(x_I | x_{-I}) = \frac{p(x)}{\int p(x) dx_I} \propto p(x) \tag{16.2} p(x_{i \in I} | x_{i \notin I}) \propto p(x_1, x_2, \cdots, x_k) 那么相应地 \frac{ p(x_I' | x_{-I}')}{P(x_I | x_{-I})} = \frac{p(x')}{p(x)} \tag{16.3}原理吉布斯抽样是MH算法的特殊情况，假设多元变量$x$的联合概率密度为$p(x)$，定义建议分布为当前抽样维度$x_j, j = 1, 2, \cdots, k$的满条件分布，即 \begin{aligned} q(x, x') = p(x_J' | x_{-J}) \\ x_J = \{x_j\}, \quad x_{-J} = \{x_1, \cdots, x_{j-1}, x_{j+1}, \cdots, x_k\} \end{aligned} \tag{17.1}相应地，此时接受概率分布为 \begin{aligned} \alpha(x, x') = \min\left\{1, \frac{p(x') \cdot q(x', x)} {p(x) \cdot q(x, x')} \right\} = \\ \min\left\{1, \frac{p(x') \cdot p(x_J | x_{-J}')} {p(x) \cdot p(x_J' | x_{-J})} \right\} = \\ \min\left\{1, \frac{p(x')} {p(x)} \cdot \frac{p(x)}{p(x)'} \right\} = 1 \end{aligned} \tag{17.2}吉布斯抽样具体是这样的，从某个初始样本$x^{(0)}$出发，不断进行迭代，每次迭代得到$x^{(i)}$，从而获得样本序列${x^{(0)}, \cdots, x^{(i)}, \cdots}$。在第$i(i &gt; 0)$次迭代时，对其第$j$个维度按以下满条件概率分布进行随机抽样 \begin{cases} p(x_j | \underbrace{x^{(i-1)}_{j+1}, \cdots, x^{(i-1)}_k}_{x^{(i-1)}_{-J}}) & j = 1 \\ p(x_j | \underbrace{x^{(i)}_1, \cdots, x^{(i)}_{j-1}}, \underbrace{x^{(i-1)}_{j+1}, \cdots, x^{(i-1)}_k}) & j = 2, \cdots, k -1 \\ p(x_j | \underbrace{x^{(i)}_1, \cdots, x^{(i)}_{j-1}}_{x^{(i)}_{-J}}) & j = k \end{cases} \tag{17.3}与MH算法不同之处是，MH算法需要进行接受/拒绝判断，可能两次相邻的采样停留在同一状态。 举例例：用吉布斯抽样从以下二元正态分布中抽取随机样本 x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim \mathcal{N}( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix})解：定义建议分布为维度$x_k, k = 1, 2$的条件概率分布，为一元正态分布 p(x_1 | x_2) = \frac{p(x_1, x_2)}{p(x_2)} = \frac{p(x)}{\int p(x) dx_1}其中 p(x) = \frac{1}{\sqrt{3} \pi} \exp \left( - \frac{2}{3} (x_1^2 - x_1 x_2 + x_2^2) \right) 多维正态分布$x \sim \mathcal{N}(\mu, \Sigma)$ p(x) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right) 那么 \begin{cases} p(x_1 | x_2) \sim \mathcal{N}(0.5 x_2, 0.75) \\ p(x_2 | x_1) \sim \mathcal{N}(0.5 x_1, 0.75) \end{cases}假设初始样本$x^{(0)} = \begin{bmatrix}0 &amp; 0\end{bmatrix}^T$，那么由下程序可获得相应序列12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npdef gaussian(x, mu, sigma): a = np.sqrt(2 * np.pi) * sigma b = (x - mu) / sigma c = (1. / a) * np.exp(-0.5 * (b**2)) return c / sum(c)def egGibbs(m = 2000, n = 100, rho = 0.1, state_space=[-10, 10], init_state=[0, 0]): # 状态空间 S = np.linspace(*state_space, num=1000) # 采样 samples = [] for t in range(m + n): if t == 0: samples += [init_state] continue # 两个维度采样 mu = rho * samples[-1][-1] sigma = np.sqrt(1 - rho ** 2) p = gaussian(S, mu, sigma) x1 = np.random.choice(S, p=p) mu = rho * x1 sigma = np.sqrt(1 - rho ** 2) p = gaussian(S, mu, sigma) x2 = np.random.choice(S, p=p) samples += [[x1, x2]] return np.array(samples)[m:]if __name__ == "__main__": from matplotlib import pyplot as plt samples = egGibbs() plt.figure() plt.scatter(*samples.T, c='r') plt.show() 样本点作图如下]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【算法】位运算的应用]]></title>
    <url>%2F2020%2F04%2F12%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言位运算在编程中有不少的应用，但没必要记太多的“奇淫巧计”。 英文字符转换 字符转小写 12&#39;a&#39; | &#39; &#39; &#x3D; &#39;a&#39;&#39;A&#39; | &#39; &#39; &#x3D; &#39;a&#39; 字符转大写 12&#39;a&#39; &amp; &#39;_&#39; &#x3D; &#39;A&#39;&#39;A&#39; &amp; &#39;_&#39; &#x3D; &#39;A&#39; 大小写互换 12&#39;a&#39; ^ &#39; &#39; &#x3D; &#39;A&#39;&#39;A&#39; ^ &#39; &#39; &#x3D; &#39;a&#39; 单个数字的操作 奇偶性 判断末位即可。 1bool isOdd = x &amp; 0x01; 判断是否为$2$的整数幂 $2$的整数幂，其二进制表示中只有一个1，如 \begin{aligned} 2^0 = 1 = 0b0001 \\ 2^1 = 2 = 0b0010 \\ 2^2 = 4 = 0b0100 \end{aligned} x &amp; (x - 1)可将x最右侧1置0 12345bool isPowOf2(int x)&#123; if (x &lt;= 0) return false; return !(x &amp; (x - 1));&#125; 该数字二进制表示中1的个数 123456789int bCountOne(int x)&#123; int count = 0; while (x)&#123; // x != 0x0000 x = x &amp; (x - 1); count++; &#125; return count;&#125; 求一个符号数的绝对值 数字按位取反后加$1$得到该数的相反数(补码)。 123456789int bAbs(int x)&#123; int mask = x &gt;&gt; (sizeof(int) * 8 - 1); // `&gt;&gt;`为算术位移，此时`mask = 0x0000 = 0`或`mask = 0xFFFF = -1` // return (x ^ mask) - mask; // 分解： x ^= mask; // `mask = 0x0000`时不变，`mask = 0xFFFF`时按位取反 x -= mask; return x;&#125; 两个数字的操作 实现两个数字的交换 123456void bExch(int&amp; a, int&amp; b)&#123; a ^= b; b ^= a; a ^= b;&#125; 实现两个数字的加法 位的加法与“异或”一致、进位与“与”一致； 注意进位位的处理。12345678910int bAdd(int a, int b)&#123; int sum = 0, carry = 0; while (b != 0) &#123; sum = a ^ b; // 先求每位和 carry = (a &amp; b) &lt;&lt; 1; // 每位相加后的进位 a = sum; b = carry; &#125; return sum;&#125; 判断两个数字是否异号 如果用乘法判断a * b &lt; 0，可能会导致溢出； 用异或对符号位进行判断，若异号则符号位异或后为1，两数异或结果小于$0$。1234bool hasSameSign(int a, int b)&#123; return (a ^ b) &lt; 0;&#125; 多个数字的操作 某数组除一个数字只有$1$个外，其余数字都有$2$个，找到这个数字 数组异或，相同的数字被抵消 1234567891011int numAlone(const vector&lt;int&gt;&amp; nums)&#123; if (nums.size() == 0) &#123; throw "Invalid Input! "; &#125; int num = 0; for (int i = 0; i &lt; nums.size(); i++) &#123; num ^= nums[i]; &#125; return num;&#125; Reference 《剑指offer》； 位运算在算法中的应用小结 - CSDN； labuladong的算法小抄； Bit Twiddling Hacks]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【算法】回溯法与分支定界法]]></title>
    <url>%2F2020%2F03%2F23%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%9B%9E%E6%BA%AF%E6%B3%95%E4%B8%8E%E5%88%86%E6%94%AF%E5%AE%9A%E7%95%8C%E6%B3%95%2F</url>
    <content type="text"><![CDATA[回溯法回溯法(backtracking)是对候选集进行系统检查的两种方法，该算法策略有以下几步 定义问题的解空间(solution space)，这个空间至少包含一个问题的(最优的)解； 组织解空间，使解空间便于搜索，典型的组织方法是图和树； 例如迷宫老鼠问题：指定$3 \times 3$的迷宫，部分位置不可通行，求解最短的通行路径。该问题如果用树描述，那么不考虑是否能通行，每个节点处有$4$个抉择，树结构较大，用图描述较为简便，如下 在确定解空间的组织方法后，这个空间可以按深度优先方式进行搜索。 初始化E-结点(expansion node)为起始点，标记搜索过程中经过的节点为活动节点(live node)，那么起始点也是一个活动节点； 在E-节点处，尝试可能的选择节点并移动：1) 若存在这样的节点，将当前E-节点新到达节点成为E-节点，并标记新节点也为活动节点；2)若不存在这样的节点(没有可供选择的节点，或超过约束限制)，将当前E-节点标记为死节点(用来杀死该节点的策略称为界定函数(bounding function))，并将E-节点返回至最近的活动节点(回溯)； 循环步骤2直至遍历节空间，在搜索过程中，实时更新最优解。 以迷宫老鼠为例，初始节点$(1, 1)$标记为活动节点，同时也为E-节点，在该节点处可达节点为$(1,2),(2,1)$，那么标记$(1,2)$为活动节点，E-节点更新为$(1,2)$。当E-节点为$(1,3)$时，该节点无可达节点(即界定函数)，标记为死结点，并更新E-节点为最近的活动节点，即$(1,2)$，此时该节点也被标记为死结点，E-节点更新为$(1,1)$。此时存在可达节点$(2,1)$，将其标记为活动节点，并更新E-节点。以此类推，当E-节点为出口$(3,3)$时，结束算法。注：此时活动节点有$(1,1), (2,1), (3,1), (3,2), (3,3)$。 注意区别回溯法和动态规划，两者有一定程度的相似性。动态规划是自底向上的，回溯法是自顶向下的。 例(旅行商问题, TSP)：给定一个$n$顶点的有权网络(有向或无向)，找出一个包含$n$个顶点且具有最小耗费的环路，例如给定下图的网络： 该问题的约束条件是：1) 环路包含$n$个节点；2) 节点间拓扑结构已确定。优化目标是环路耗费最低，用矩阵描述该图为 \begin{bmatrix} \infty & 30 & 35 & \infty & 20 \\ 30 & \infty & 20 & 10 & \infty \\ 35 & 20 & \infty & 5 & 15 \\ \infty & 10 & 5 & \infty & 25 \\ 20 & \infty & 15 & 25 & \infty \\ \end{bmatrix}考虑分支问题可以用栈来解决，用递归的形式借助函数调用栈，求解指定节点$i$所能到达死结点的路径。考虑到已经过节点不能再次经过，将能到达$i$的路径全部删除(置为无穷大)。但采用函数调用栈可能导致函数调用层级很深而溢出。 用下列函数求解得(80.0, [0, 1, 3, 2, 4])，即路径为$1 \rightarrow 2 \rightarrow 4 \rightarrow 3 \rightarrow 5 \rightarrow 1$，最小耗费为$80$。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def solveTSP(A): # 获取从节点0开始的，所有最深路径 path = getPath(A, 0) # 删除不包含所有节点的路径 path = list(filter(lambda x: len(x) == 5, path)) # 计算每条路径的成本 cost = list(map(lambda x: getCost(A, x), path)) minCost = min(cost) minPath = path[cost.index(minCost)] return minCost, minPathdef getPath(A, i): """ Params: A: &#123;ndarray(n, n)&#125; i: &#123;int&#125; Returns: path: &#123;list[list]&#125; Notes: - 深度优先遍历； - 返回A为图下，以节点i为起始的最深路径，包含i； """ n = A.shape[0] path = [] # 计算i能到达的每个节点的最深路径 for j in range(n): if np.isinf(A[i, j]): continue B = A.copy(); B[:, i] = float('inf') # 把所有能到达i的路径删除，使i不重复经过 path += getPath(B, j) # 在不能到达i节点的情况下，求解j节点到达死结点的路径 # 添加节点i if len(path) == 0: # i不能到达任何节点，那么只返回1条包含i的路径 path = [[i]] else: # i能到达其他死结点，那么每条路径前添加i节点 path = list(map(lambda x: [i] + x, path)) return pathdef getCost(A, path): """ Params: A: &#123;ndarray(n, n)&#125; path: &#123;list&#125; Returns: cost Notes: - 计算环路的耗费； - path路径不包含头节点，即不为环路 """ cost = 0 for i in range(len(path) - 1): cost += A[path[i], path[i+1]] cost += A[path[-1], path[0]] return costif __name__ == "__main__": A = np.array([ [float('inf'), 30, 35, float('inf'), 20], [30, float('inf'), 20, 10, float('inf')], [35, 20, float('inf'), 5, 15], [float('inf'), 10, 5, float('inf'), 25], [20, float('inf'), 15, 25, float('inf')] ]) print(solveTSP(A)) 下面是用栈实现的深度优先搜索，需保存当前图、E-节点和当前已加入的路径 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Stack(): def __init__(self): self._data = [] def __len__(self): return len(self._data) def push(self, x): self._data += [x] def pop(self): return self._data.pop(-1)def f2(A): """ Params: A: &#123;ndarray(n, n)&#125; 原始图 Returns: """ n = A.shape[0] path = [] # 从0号结点开始 stack = Stack() stack.push((A, 0, [0])) # 栈内元素：(当前图，当前节点，当前路径) while(len(stack) &gt; 0): # E-节点出栈 a, i, p = stack.pop() # 已经过所有节点，保存退出 if len(p) == n: path += [p] continue # 删除可达E-节点的路径 b = a.copy(); b[:, i] = float('inf') # E-节点的可达节点入栈，这些节点即活动节点 for j in range(n): if np.isinf(b[i, j]): continue stack.push((b, j, p + [j])) # 取最小耗费的路径 cost = list(map(lambda x: getCost(A, x), path)) index = cost.index(min(cost)) return cost[index], path[index]if __name__ == "__main__": A = np.array([ [float('inf'), 30, 35, float('inf'), 20], [30, float('inf'), 20, 10, float('inf')], [35, 20, float('inf'), 5, 15], [float('inf'), 10, 5, float('inf'), 25], [20, float('inf'), 15, 25, float('inf')] ]) print(solveTSP2(A)) 分支定界法和回溯法类似，分支定界法也经常把解空间组织成树或图的结构然后进行搜搜。回溯法使用先深搜索，而分支定界法采用先广搜索。每个节点只有一次机会成为E-节点，E-节点的可达节点都是生成的新节点，那些不可能到处(最优)可行解的节点被舍弃(死结点)。分支定界法也可采用定界函数提前结束搜索。 有两种常用的方法选择E-节点： 先进先出(FIFO)：借助队列，可参考二叉树的层次遍历。将首个节点加入队列，然后依次出队列作为E-节点，将E-节点可达节点全部加入队列，直至队列为空； 最小耗费或最大收益法：借助小根堆/大根堆，将堆顶节点弹出作为E-节点，并将其可达节点全部加入堆。 那么迷宫老鼠问题中，迷宫的路径搜索节点顺序如下 以下借助队列实现广先搜索解决TSP问题，将回溯法中栈修改为队列即可 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Queue(): def __init__(self): self._data = [] def __len__(self): return len(self._data) def push(self, x): self._data += [x] def pop(self): return self._data.pop(-1)def f2(A): """ Params: A: &#123;ndarray(n, n)&#125; 原始图 Returns: """ n = A.shape[0] path = [] # 从0号结点开始 queue = Queue() queue.push((A, 0, [0])) # 栈内元素：(当前图，当前节点，当前路径) while(len(queue) &gt; 0): # E-节点出栈 a, i, p = queue.pop() # 已经过所有节点，保存退出 if len(p) == n: path += [p] continue # 删除可达E-节点的路径 b = a.copy(); b[:, i] = float('inf') # E-节点的可达节点入栈，这些节点即活动节点 for j in range(n): if np.isinf(b[i, j]): continue queue.push((b, j, p + [j])) # 取最小耗费的路径 cost = list(map(lambda x: getCost(A, x), path)) index = cost.index(min(cost)) return cost[index], path[index] 区别回溯法的求解目标是找出满足约束条件的所有解，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【算法】动态规划]]></title>
    <url>%2F2020%2F03%2F21%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[动态规划和贪婪算法一样，对一个问题的解是一系列抉择的结果。在贪婪算法中已做出的抉择是不可更改的，而动态规划需考察一个最优抉择序列是否包含最优抉择子序列。称无论第一次选择是什么，接下来的选择一定是当前状态下的最优选择为最优原则(principal of optimality)。 用动态规划求解的步骤如下 证实最优原则适用，否则不能使用动态规划； 建立动态规划递归方程(dynamic-programming recurrence equation)； 求解动态规划的递归方程式获得最优解； 沿最优解生成过程进行回溯(traceback)。 例1 (0/1背包问题)：设物品$i, 1 \leq i \leq n$的重量为$w_i$，价值为$p_i$，放入总容量为$c$的背包中，求最优解$x = [x_1, x_2, \cdots, x_n], x_i \in {0, 1}$使总价值$\sum_i p_i x_i$最大。 分析：物品按次序拜访，当需要对第$i$个物品进行抉择时，可以对存在的两种可能(放入$i$/不放入$i$)下，规划前$i-1$个物品的抉择。 设$f(i, y)$表示问题状态为剩余容量为$y$时，确定物品$1, 2, \cdots, i$的的最优规划下的准则，那么对于物品$i$，可以不放入，前$i-1$个物品可以在背包容量为$c$的状态下进行最优选择，且不包含本物品价值，设$f(n-1, c)$；而$i$放入时，前$i-1$个物品只能在背包容量为$c - w_i$的状态下进行最优选择，包含本物品价值，设$f(n-1, c - w_i) + p_i$。根据$f(n-1, c)$与$f(n-1, c - w_i) + p_i$的大小，确定物品$i$的最优抉择，如下 f(i, y) = \begin{cases} \max\{ f(i - 1, y), f(i - 1, y - w_i) + p_i \} & y \geq w_i \\ f(i - 1, y) & 0 \leq y < w_i \end{cases}其中$f(i - 1, y)， f(i - 1, y - w_i) + p_i$分别表示在不放入/放入物品$i$的问题状态下求解的价值，递归的终止条件为$i=1$时 f(1, y) = \begin{cases} p_n & y \geq w_n \\ 0 & 0 \leq y < w_n \end{cases} 如$n=3, w = [10, 3, 2], p = [20, 18, 15], c = 14$时，有 \begin{aligned} f(1, y) = \begin{cases} 15 & y \geq 10 \\ 0 & 0 \leq y < 10 \end{cases} \\ f(2, y) = \begin{cases} \max\{ f(1, y), f(1, y - 3) + 18 \} & y \geq 3 \\ f(1, y) & 0 \leq y < 3 \end{cases} \\ f(3, y) = \begin{cases} \max\{ f(2, y), f(2, y - 2) + 20 \} & y \geq 2 \\ f(2, y) & 0 \leq y < 2 \end{cases} \end{aligned}那么在初始情况下$y=14$时，代入有 f(3, 116) = 38 用表格的形式求解，表格元素table[i][y]表示在背包容量为$y$的情况下，物品子集和${0(无物品), 1, 2, \cdots, i}$的最优装载总价值1234567891011121314151617181920212223242526272829303132333435363738394041424344void solve(int* w, int* p, int n, int c)&#123; // 初始化 (n + 1) × (c + 1) 表格 int** table = new int* [n + 1]; for (int i = 0; i &lt; n + 1; i++) &#123; table[i] = new int[c + 1]; memset(table[i], 0, sizeof(int) * (c + 1)); &#125; // 开始填表 for (int i = 1; i &lt; n + 1; i++) &#123; // 已经判定是否装入背包的物品依次增加，即`1, 2, ..., i-1`已装入 for (int y = 1; y &lt; c + 1; y++) &#123; // 剩余容量依次增加，即剩余容量为`y` if (y &lt; w[i - 1]) &#123; // 容量不够，物品`i`不装入 table[i][y] = table[i - 1][y]; &#125; else &#123; // 容量够 int p1 = table[i - 1][y]; // 不装入物品`i` int p2 = table[i - 1][y - w[i - 1]] + p[i - 1]; // 前`i-1`个物品在容量为`y - w[i]`下的最优装载 + 物品`i`价值 table[i][y] = p1 &gt; p2 ? p1 : p2; // 取大 &#125; &#125; &#125; // 打印结果 for (int i = 0; i &lt; n + 1; i++) &#123; for (int j = 0; j &lt; c + 1; j++) &#123; cout &lt;&lt; table[i][j] &lt;&lt; '\t'; &#125; cout &lt;&lt; endl; &#125; // 释放内存 for (int i = 0; i &lt; n + 1; i++) delete[] table[i]; delete[] table;&#125;int main(int argc, char** argv)&#123; int w[3] = &#123;10, 3, 2&#125;; int p[3] = &#123; 20, 18, 15 &#125;; solve(w, p, 3, 14); system("pause");&#125; 当$n=3, c=14, p=[20, 18, 15], w=[10, 3, 2]$时，最优装载价值为$38$123450 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 20 20 20 20 0 0 0 18 18 18 18 18 18 18 20 20 20 38 38 0 0 15 18 18 33 33 33 33 33 33 33 35 38 38 请按任意键继续. . . 例2 (优惠券问题)：有一张满$100$可减免$20$的优惠券，从价格为${30, 20, 35, 55, 45}$的物件商品中选择合适的子集，使商品价格总和超过$100$但又最小。 原问题有两个约束：1) 商品价格最小；2) 商品价格超过$100$。将原问题修改为：从物品中删除若干件，使删除的物品价格总和不超过$\sum_i p_i - 100 = 85$，此时约束只有上限。$f(i, y)$表示在剩余可删除价格为$y$的状态下，前$0, \cdots, i$的最大价格总和，那么 f(i, y) \begin{cases} \max\{ f(i-1, y), f(i-1, y-p_i) + p_i \} & y \geq p_i \\ f(i-1, y) & y < p_i \end{cases}用表格的形式求解，得最多删除$85$元1234567891011121314151617181920212223242526272829303132333435void solve(int* p, int n, int c)&#123; // 初始化 (n + 1) × (c + 1) 表格 int** table = new int* [n + 1]; for (int i = 0; i &lt; n + 1; i++) &#123; table[i] = new int[c + 1]; memset(table[i], 0, sizeof(int) * (c + 1)); &#125; // 开始填表 for (int i = 1; i &lt; n + 1; i++) &#123; // 已经判定是否删除物品依次增加，即`1, 2, ..., i-1`中的某些组合 for (int y = 1; y &lt; c + 1; y++) &#123; // 剩余价格为`y` if (y &lt; p[i - 1]) &#123; // 剩余价格不够，物品`i`不删除 table[i][y] = table[i - 1][y]; &#125; else &#123; // 价格够 int p1 = table[i - 1][y]; // 不删除物品`i` int p2 = table[i - 1][y - p[i - 1]] + p[i - 1]; // 前`i-1`个物品在剩余价格为`y - p[i - 1]`下最多的删除总价 + 物品`i`价格 table[i][y] = p1 &gt; p2 ? p1 : p2; &#125; &#125; &#125; // 打印结果 for (int i = 0; i &lt; n + 1; i++) &#123; for (int j = 0; j &lt; c + 1; j++) &#123; cout &lt;&lt; table[i][j] &lt;&lt; '\t'; &#125; cout &lt;&lt; endl; &#125; // 释放内存 for (int i = 0; i &lt; n + 1; i++) delete[] table[i]; delete[] table;&#125; 12345670 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 300 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 20 20 20 20 20 20 20 20 20 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 30 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 500 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 20 20 20 20 20 20 20 20 20 30 30 30 30 30 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 50 50 50 50 50 55 55 55 55 55 55 55 55 55 55 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 850 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 20 20 20 20 20 20 20 20 20 30 30 30 30 30 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 50 50 50 50 50 55 55 55 55 55 55 55 55 55 55 65 65 65 65 65 65 65 65 65 65 75 75 75 75 75 75 75 75 75 75 850 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 20 20 20 20 20 20 20 20 20 30 30 30 30 30 35 35 35 35 35 35 35 35 35 35 45 45 45 45 45 50 50 50 50 50 55 55 55 55 55 55 55 55 55 55 65 65 65 65 65 65 65 65 65 65 75 75 75 75 75 80 80 80 80 80 85请按任意键继续. . . 例3 (二叉树中的最大路径和)：给出一棵二叉树，寻找一条路径使其路径和最大，路径可以在任一节点中开始和结束（路径和为两个节点之间所在路径上的节点权值之和。 例如给定${1, 2, 3, 4, 2, 5, 10, #, #, #, #, 7, 18, #, #, 20}$，最大路径和为$20 + 7 + 5 + 18$ 分析： 分治策略，查找子树的最大路径和； 动态规划策略，对于高为$h$的二叉树，以第$i(i=1, \cdots, h-1)$层节点为起始的单边路径和最大路径，一定包含以第$i+1(i=1, \cdots, h-1)$层节点为起始的单边路径和最大路径，且$i=h$时路径仅包含叶子节点； 每条最长路径都肯定会以某个顶点为根，然后两边是以那个节点为跟到叶子节点的最长路径(“八”字形，除根节点外每个节点最多包含一个孩子)，故只需比较每棵树这种情况下路径和； 某节点处的上述路径，是由该节点的值、左右子树的最大路径组成的； 注意负数的处理，负数添加到路径中必定会使路径和减小，故用max(0, x)处理； 将所有内部节点按深度优先进行标号，设节点$i$到所有叶子节点的单边路径中，最大路径和为$f(i)$。考虑到$f(i.child)$可能为负值，将小于$0$的单边路径加入后，该单边路径不可能是最大，所以本节点加上单边路径时，考虑$\max{0, f(i.child)}$ f(i) = \begin{cases} i.val + \max \{ \max\{0, f(i.left)\}, \max\{0, f(i.right)\} \} & i不是叶子节点 \\ i.val & i是叶子节点 \end{cases} \max\{0, f(i.left)\}, \max\{0, f(i.right)\} = \max \{ 0, \max\{ f(i.left), f(i, i.right) \} \} 设节点$i$处“八”字形最大路径和为$g(i)$， g(i) = i.val + \max\{0, f(i.left)\} + \max\{0, f(i.right)\}最优为 G = \max g(i)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Definition of TreeNode: * class TreeNode &#123; * public: * int val; * TreeNode *left, *right; * TreeNode(int val) &#123; * this-&gt;val = val; * this-&gt;left = this-&gt;right = NULL; * &#125; * &#125; */class Solution &#123;public: /** * @param root: The root of binary tree. * @return: An integer * @description: 返回整棵树中最大路径和 */ int maxPathSum(TreeNode * root) &#123; // write your code here if (!root) return INT_MIN; int psRoot = maxBinaryLeafPathSum(root); // 以root为根，两边到叶子节点的最长路径 int psLeft = maxPathSum(root-&gt;left); // 左子节点的以root为根，两边到叶子节点的最长路径 int psRight = maxPathSum(root-&gt;right); // 右子节点的以root为根，两边到叶子节点的最长路径 return max(max(psRoot, psLeft), psRight); &#125; /** * @param root: The root of binary tree. * @return: An integer * @description: 返回以root为根，两边到叶子节点的最大路径和 */ int maxBinaryLeafPathSum(TreeNode * root)&#123; if (!root) return INT_MIN; int lpsL = maxLeafPathSum(root-&gt;left); int lpsR = maxLeafPathSum(root-&gt;right); return root-&gt;val + max(0, lpsL) + max(0, lpsR); &#125; /** * @param root: The root of binary tree. * @return: An integer * @description: 返回节点root跟到叶子节点的最大路径和，包含root */ int maxLeafPathSum(TreeNode * root)&#123; if (!root) return INT_MIN; int lpsL = maxLeafPathSum(root-&gt;left); int lpsR = maxLeafPathSum(root-&gt;right); return root-&gt;val + max(0, max(lpsL, lpsR)); &#125;&#125;; 以上解法的缺陷是，在计算某节点的“八”字形树时，对其子节点进行了遍历，但该节点回溯后，即计算更高层节点时，到该节点仍需要节点遍历，造成许多不必要的计算浪费，采用指针可以改进算法，如下1234567891011121314151617181920212223242526272829303132class Solution &#123;public: /** * @param root: The root of binary tree. * @return: An integer * @description: 返回整棵树中最大路径和 */ int maxPathSum(TreeNode * root) &#123; // write your code here if (!root) return INT_MIN; int ret = INT_MIN; maxLeafPathSum(root, ret); return ret; &#125; /** * @param root: The root of binary tree. * @return: An integer * @description: 返回单边节点root跟到叶子节点的最大路径和，包含root */ int maxLeafPathSum(TreeNode * root, int&amp; ret)&#123; if (!root) return INT_MIN; int lpsL = maxLeafPathSum(root-&gt;left, ret); // `root-&gt;left`为起始的单边路径和最大路径 int lpsR = maxLeafPathSum(root-&gt;right, ret); // `root-&gt;right`为起始的单边路径和最大路径 ret = max(ret, root-&gt;val + max(0, lpsL) + max(0, lpsR)); // `root`为根节点的"八"字形子树 return root-&gt;val + max(max(0, lpsL), max(0, lpsR)); // 该路径满足：`root`为起始的、单边路径和最大的 &#125;&#125;; 例4 (最大子数组)：给定一个整数数组，找到一个具有最大和的子数组，返回其最大和。 输入：[−2,2,−3,4,−1,2,1,−5,3]输出：6解释：符合要求的子数组为[4,−1,2,1]，其最大和为 6。 这一题的关键点是数组连续的判定。 考虑${4, -3, 2, 3}$与${4, -5, 2, 3}$，可以看到，连续子数组只会以大于$0$的数开始，在小于$0$的元素处断开。一旦“在第$i$个数添加时，出现该数与之前数的最大和子数组小于$0$”这一条件触发，有两层含义：1) 在此之前数组连续；2) 之前连续的数组在该值处断开。 证明：${s_1, v, s_2}, \sum s_1 &gt; 0，\sum s_2 &gt; 0$的形式下，只有 \begin{cases}\sum s_1 + v + \sum s_2 \geq \sum s_1 \\ \sum s_1 + v + \sum s_2 \geq \sum s_2\end{cases}时，${s_1, v, s_2}$才有可能是最大连续数组，那么 \begin{cases}v \geq - \sum s_1 \\ v \geq -\sum s_2\end{cases}所以$v &lt; - \sum s_1 \Rightarrow \sum s_1 + v &lt; 0$时，${s_1, v, s_2}$必定不是最大和子数组，连续数组$s_1$和也达到最大，可重新累加新的连续数组的和。 设$f(i)$为前$1, \cdots, i-1$不小于$0$的累加，与第$i$个数的和，那么有 f(i) = \begin{cases} nums[i] & i = 1 \\ nums[i] + \max\{ 0, f(i-1) \} & i > 1 \end{cases}那么就有($i \geq 1$) \begin{cases} f(i) < 0 \Rightarrow f(i+1) = nums[i+1] & nums[i]是上述v \\ f(i) < 0 \Rightarrow f(i+1) = nums[i+1] + f(i) & nums[i]不是上述v，可视作当前连续数组(包括nums[i]不包括nums[i+1])的持续累加**。 \end{cases}123456789101112131415161718192021222324252627class Solution &#123;public: /** * @param nums: A list of integers * @return: An integer indicate the sum of max subarray */ int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; // write your code here int ret = INT_MIN; f(nums, nums.size(), &amp;ret); return ret; &#125; int f(vector&lt;int&gt;&amp; nums, int n, int* ret) &#123; int a = 0, b = 0; if (n == 1) &#123; a = b = nums[0]; &#125; else &#123; a = f(nums, n - 1, ret); b = max(0, a) + nums[n - 1]; // a &lt; 0时，数组不连续，nums[n - 1]作为新的连续数组的起始 &#125; *ret = max(*ret, max(a, b)); // 记录最大连续数组和 return b; &#125;&#125;; 用表格法可以使时间复杂度大大降低123456789101112131415161718192021class Solution &#123;public: /** * @param nums: A list of integers * @return: An integer indicate the sum of max subarray */ int maxSubArray(vector&lt;int&gt; &amp;nums) &#123; int n = nums.size(); if (n &lt;= 0) return 0; // dp[i]: 到元素`i`的最大连续子数组和 int* dp = new int[n]; dp[0] = nums[0]; int maxSum = dp[0]; for (int i = 1; i &lt; n; i++)&#123; dp[i] = max(nums[i], nums[i] + dp[i - 1]); if (dp[i] &gt; maxSum) maxSum = dp[i]; &#125; return maxSum; &#125;&#125;; 另外，还有一题类似的乘积最大子序列1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123;public: /** * @param nums: An array of integers * @return: An integer */ int maxProduct(vector&lt;int&gt; &amp;nums) &#123; // write your code here int n = nums.size(); // maximum &amp; minimum // dp[0]: 到元素`i`的最大连续数组乘积 // dp[1]: 到元素`i`的最小连续数组乘积 int** dp = new int* [2]; for (int i = 0; i &lt; 2; i++)&#123; dp[i] = new int [n]; dp[i][0] = nums[0]; &#125; for (int i = 1; i &lt; n; i++)&#123; if (nums[i] &gt; 0)&#123; dp[0][i] = max(nums[i], nums[i] * dp[0][i - 1]); dp[1][i] = min(nums[i], nums[i] * dp[1][i - 1]); &#125; else &#123; dp[0][i] = max(nums[i], nums[i] * dp[1][i - 1]); dp[1][i] = min(nums[i], nums[i] * dp[0][i - 1]); &#125; &#125; // find maximum int ret = INT_MIN; for (int i = 0; i &lt; n; i++)&#123; if (dp[0][i] &gt; ret) ret = dp[0][i]; &#125; delete [] dp[0]; delete [] dp[1]; delete [] dp; return ret; &#125;&#125;; 例5 (旅行商问题, TSP)：给定一个$n$顶点的有权网络(有向或无向)，找出一个包含$n$个顶点且具有最小耗费的环路，例如给定下图的网络： 分析：节点间的先后次序会影响下一个节点的选择，故本例和背包问题不同，不能将节点作为单独的抉择，关键是如何把有序问题转换为无序问题，转换思路，节点的顺序方案却是无序的，那么定义$f(x, n)$为：在给定$x$节点顺序的方案下，再选择$n$个可达节点，使路径耗费最小，下一个抉择方案的可能性是所有$x[-1]$可达顶点$v$。 f(x, n) = \begin{cases} f(x + v, n - 1) & v可达 \\ f(x, n) & v 不可达 \end{cases}12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import numpy as npdef solveTSP(A): return f(A, A, A.shape[0], [0], 0)def f(A, B, num, ordered, cost): """ Params: A: &#123;ndarray(n, n)&#125; 原始图 B: &#123;ndarray(n, n)&#125; 删除“可到达已到达节点路径”后的图 num: &#123;int&#125; 还需选择的顶点个数 ordered: &#123;list[int]&#125; 当前已选择的顶点顺序 cost: &#123;float&#125; 当前路径的耗费累计 Returns: ordered: &#123;list[int]&#125; 当前顶点顺序下，加入新顶点后的最优路径 costNew: &#123;float&#125; """ # 加入起始点 if num == 1: return ordered + [ordered[0]], cost + A[ordered[-1], ordered[0]] # 遍历ordered[-1]可达节点，选择耗费最低 path = [] listCost = [] for i, c in enumerate(B[ordered[-1]]): if np.isinf(c): continue # 节点可达，将其加入路径 ## 将所有能到达已选顶点的路径删除 C = B.copy(); C[:, ordered + [i]] = float('inf') ## 计算加入后最短 orderedNew, costNew = f(A, C, num - 1, ordered + [i], cost + A[ordered[-1], i]) path += [orderedNew]; listCost += [costNew] # 无可达节点，该路死路 if len(path) == 0: return ordered + [ordered[0]], cost + A[ordered[-1], ordered[0]] minCost = min(listCost) index = listCost.index(minCost) return path[index], listCost[index]if __name__ == "__main__": A = np.array([ [float('inf'), 30, 35, float('inf'), 20], [30, float('inf'), 20, 10, float('inf')], [35, 20, float('inf'), 5, 15], [float('inf'), 10, 5, float('inf'), 25], [20, float('inf'), 15, 25, float('inf')] ]) print(solveTSP(A)) # 输出 ([0, 1, 3, 2, 4, 0], 80.0) 一些感受： 动态规划可以理解为根据当前的选择，调整过去的抉择，获得未来的最优。在问题中，需要确定优化目标，以及问题的约束。在约束下，确定一个抉择序列“抉择$1$ $\rightarrow$ 抉择$2$ $\rightarrow$ … $\rightarrow$ 抉择$n$”，在进行第$i$个抉择时，可以对其$m$个可能性$c_{i1}, c_{i2}, \cdots, c_{im}$进行假设，每次假设中，抉择$1 \sim i$中唯一确定的只有抉择$i$，$1 \sim i-1$需要根据抉择$i$引起的约束变更进行最优规划，而这些规划可通过递归求解。实际上，这些一系列的抉择，可构成一个树型结构。 总结，动态规划在以下情况时适用，从上往下分析问题、从下往上求解，并保存子问题的最优解避免重复计算 该问题可以分为若干子问题，且子问题间还有重叠的更小的子问题； 整体问题的最优解依赖各个子问题的最优解； 子问题仍旧适用1，可递归解决。 以$0/1$背包为例，约束是背包容量，优化目标是总价值。在确定第$n$个物品是否放入时，需要考虑放入后对剩余背包容量$y$的更改，即$y - w_n$。那么假设第$n$个物品放入，规划子问题就变成$1, \cdots, n-1$物品在背包容量为$y - w_n$下的最优规划；假设第$n$个物品不放入，规划子问题就变成$1, \cdots, n-1$物品在背包容量为$y $下的最优规划。仍旧以$n=3, w = [10, 3, 2], p = [20, 18, 15], c = 14$为例，用树形图可表示如下 在最大连续子数组中，条件进行了更改，数字是必须加入数组的，约束是“连续”，优化目标是最大和。需要进行判别，确定第$i$个数字是否会引起之前连续数组的“断裂”，这个影响是确定的，不需要对多种可能性假设，那么只需记录当前连续数组的累加即可。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【算法】分而治之]]></title>
    <url>%2F2020%2F03%2F21%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B%2F</url>
    <content type="text"><![CDATA[分治算法是将一个问题的大实例分成若干个更小的实例，分别解决后将小实例的解组合成原始大实例的解。一般来说，大实例分解前是用重复的步骤进行求解的，分解后的子问题也可通过同一种算法解决，用递归解决。 例(金块问题)：有$n$个金块，从中找出最重与最轻的金块。 如果用顺序查找的方式，先通过$n-1$次比较选出最重的，再通过$n-2$次比较选出最轻的，共进行$2n-3$次比较，时间复杂度为$O(n)$。 通过分治的方式，将金块分为若干份，每份中选取最大和最小的金块，如递归二分，将$n$个金块分为$n/2 + n/2$，$n/2$又可划分为$n/4 + n/4$，以此类推，直至划分集合中剩余$1$或$2$个金块，选出最大和最小，在合并过程中比较每份中的最大和最小即可，最终得到全部金块中的最大和最小。 共划分为$\lceil n/2 \rceil$个小组，第$1$次需进行$\lceil n/2 \rceil$次比较，第$i(i \geq 2)$次在$\lceil n / 2^{i} \rceil$份金子间，进行$\lceil n / 2^{i} \rceil \times 2$次比较(最大、最小比较)，共需进行$\lceil \log_2 n \rceil$层的比较，总比较次数为 \lceil n/2 \rceil + 2 \times \sum_{i=2}^{\lceil \log_2 n \rceil} \lceil n / 2^{i} \rceil以$n=17$为例，共进行$9 + 2 \times (5 + 3 + 2 + 1) = 22$次，而顺序比较时进行了$2 \times 17 - 3 = 31$次比较，减少超过$25\%$的比较次数。 例(递归排序)：利用分治思想设计排序。 将$n$个数进行二路划分，对每个子集合进行排(如插入、冒泡等)，再依次组合归并有序集合得到整个有序集合，可以看作是别的排序算法的递归实现。注意到将两个子集合归并时，两子集和已有序，故改进合并算法为直接归并排序(straight merge sort)，不采用其他排序算法进行合并，减少时间复杂度，相应的会增加空间空间复杂度。 如下图，对${8, 4, 7, 3, 6, 1, 2, 9, 10, 5}$进行排序，归并排序共进行$22$次比较，而使用冒泡排序时，需进行$39$次比较。 12345678910111213141516171819202122232425262728293031323334353637383940void merge(T a[], int begin, int middle, int end)&#123; T* b = new T[end]; // 将有序子数组合并到b int i = begin, j = middle, k = begin; while (k &lt; end) &#123; int idx = 0; if (i == middle) idx = j++; else if (j == end) idx = i++; else &#123; idx = a[i] &lt; a[j] ? i++ : j++; &#125; b[k++] = a[idx]; mvCnt++; &#125; // 拷贝至原数组 for (i = begin; i &lt; end; a[i] = b[i++]); delete [] b;&#125;template&lt;typename T&gt;void mergeSort(T a[], int begin, int end)&#123; if (end - begin &lt; 2) return; int middle = (begin + end) / 2; mergeSort(a, begin, middle); mergeSort(a, middle, end); merge(a, begin, middle, end);&#125;template&lt;typename T&gt;void mergeSort(T a[], int n)&#123; mergeSort(a, 0, n);&#125; 例(快速排序)：一种不稳定但速度很快的排序算法 快速排序是在待排序数组中寻找一个支点(pivot)，将当前待排序数组进行划分，使支点左侧的数据均小于支点，右侧的数据均大于支点，然后再分别对两侧进行迭代的排序进行分治，子数组排序完成后依次回溯使整个数组完成排序，示意图如下 与冒泡等不同，当有序数组输入时为最坏情况。此时支点始终在最左侧或最右侧，时间复杂度为$O(n)$。为改善上述情况，可将用三值取中原则(median-of-three rule)进行支点的选择，在$a[begin], a[(begin + end)/2], a[end]$三个元素中选择大小居中的元素。 1234567891011121314151617181920212223242526272829303132333435363738394041template&lt;typename T&gt;void quickSort(T a[], int begin, int end) // a[begin] ~ a[end - 1]&#123; int cmpCnt = 0; int mvCnt = 0; if (end - begin &lt; 2) return; // 仅包含1个数，无需排序 T pivot = a[begin]; // 当前子数组的头元素视作支点 int left = begin, right = end - 1; // 左右索引 while (true) &#123; while (a[left] &lt;= pivot &amp;&amp; left &lt; end) left++; // 搜索从左至右大于支点的元素a[left] while (a[right] &gt;= pivot &amp;&amp; right &gt; 0) right--; // 搜索从右至左小于支点的元素a[right] cmpCnt++; if (left &lt; right) &#123; // 交换a[left]与a[right] mvCnt++; T temp = a[left]; a[left] = a[right]; a[right] = temp; &#125; else &#123; break; &#125; &#125; // 此时索引为`begin + 1 ~ left - 1`的数都小于支点，索引为`left ~ end - 1`的数都大于支点； // 将支点调整至left - 1处，使支点左侧元素都小于支点，右侧都大于支点 mvCnt++; a[begin] = a[left - 1]; a[left - 1] = pivot; // 划分子数组，递归 quickSort(a, begin, left - 1); // a[left-1]位置已确定 quickSort(a, left, end); std::cout &lt;&lt; "比较" &lt;&lt; cmpCnt &lt;&lt; "次 移动" &lt;&lt; mvCnt &lt;&lt; "次" &lt;&lt; std::endl;&#125;template&lt;typename T&gt;void quickSort(T a[], int n)&#123; quickSort(a, 0, n);&#125; 例：将快速排序应用到寻找数组中升序排序中，次序为$k$的数 在快速排序的某次递归中，支点左侧均小于支点，右侧均大于支点，那么支点的位置在整个数组中已确定。在寻找次序为$k$的数时，不在最坏情况下，不需要等排序结束就可解决。 12345678910111213141516171819202122232425262728293031323334353637template&lt;typename T&gt;int findK(T a[], int begin, int end, int k) // a[begin] ~ a[end - 1]&#123; if (end - begin &lt; 2) return a[begin]; // 仅包含1个数，一定为查找的数 T pivot = a[begin]; // 当前子数组的头元素视作支点 int left = begin, right = end - 1; // 左右索引 while (true) &#123; while (a[left] &lt;= pivot &amp;&amp; left &lt; end) left++; // 搜索从左至右大于支点的元素a[left] while (a[right] &gt;= pivot &amp;&amp; right &gt; 0) right--; // 搜索从右至左小于支点的元素a[right] if (left &lt; right) &#123; // 交换a[left]与a[right] T temp = a[left]; a[left] = a[right]; a[right] = temp; &#125; else &#123; break; &#125; &#125; // 此时索引为`begin + 1 ~ left - 1`的数都小于支点，索引为`left ~ end - 1`的数都大于支点； // 将支点调整至left - 1处，使支点左侧元素都小于支点，右侧都大于支点 a[begin] = a[left - 1]; a[left - 1] = pivot; // 划分子数组，递归 if (k &lt; left) findK(a, begin, left - 1, k); // 在左侧寻找第`k`大的数 else findK(a, left, end, k); // 在右侧寻找第`k-left`大的数&#125;template&lt;typename T&gt;int findK(T a[], int n, int k)&#123; return findK(a, 0, n, k);&#125; 例(快速幂)：设计时间复杂度为$\log(n)$的算法，计算$a^n \% b$。 有以下处理方法 $(a \times b) \% c = (a \% c) \times b \% c$ $a^n = \begin{cases} a^{\lfloor n / 2 \rfloor} \times a^{\lfloor n / 2 \rfloor} \times a^{n \% 2 } &amp; n &gt; 1 \ 1 &amp; n = 0 \ a &amp; n = 1 \end{cases}$ 123456789101112131415161718192021class Solution &#123;public: /** * @param a: A 32bit integer * @param b: A 32bit integer * @param n: A 32bit integer * @return: An integer */ long fastPower(int a, int b, int n) &#123; // write your code here if (n == 0) return 1 % b; long temp = fastPower(a, b, n / 2); temp = (temp * temp) % b; if (n % 2 == 1) temp = (temp * a) % b; return temp; &#125;&#125;;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【算法】贪婪算法]]></title>
    <url>%2F2020%2F03%2F21%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在贪婪算法(greedy method)中，逐步构造一个最优解，每一步都是在贪婪准则(greedy criterion)下做出的最优决策。 例($0/1$背包问题)：有$n$个物品和一个容量为$c$的背包，第$i$个物品重量为$w_i$、价值为$p_i$，求取物品总价值最高的可行的背包装载，即 \begin{aligned} \max \sum_{i=1}^n p_i x_i \\ s.t. \quad \sum_{i=1}^n w_i x_i \leq c \\ x_i \in \{0, 1\} \quad 1 \leq i \leq n \end{aligned}该问题是$NP$-复杂问题，难以求取最优解，有以下几种简单的贪婪策略 价值贪婪准则：从剩余物品中选出可以装入背包的价值最大的物品，但如$n=3, w = [10, 10, 10], p = [20, 15, 15], c = 15$时，该准则选取的装载$x = [1, 0, 0]$不是最优的； 重量贪婪准则：从剩余物品中选出可以装入背包的重量最小的物品，但如$n=3, w = [20, 10, 10], p = [20, 5, 5], c = 25$时，该准则选取的装载$x = [0, 1, 1]$不是最优的； 价值密度贪婪准则：从剩余物品中选出可以装入背包的$p_i/w_i$最大的物品，但如$n=3, w = [20, 15, 15], p = [40, 25, 25], c = 25$时，$p/w = [2, 1.67, 1.67]$，该准则选取的装载$x = [1, 0, 0]$不是最优的。 价值密度贪婪法则不能保证最优解，但这是一个好的启发式算法，在很多时候接近最优解。对其进行改进，使解的结果与最优解之差在最优值的$x\%(x &lt; 100)$之内：选择至多$k$件物品放入背包(构成子集)，首先选择价值最高的可行物品集合，再将剩余子集按$p_i/w_i$的递减顺序装入背包，如$n=4, w = [2, 4, 6, 7], p = [6, 10, 12, 13], c = 11$，选择$k=2$，有以下子集| 子集($k \leq 2$) | 重量 | 价值 | 价值密度 || —- | —- | —- | ——— || 1 | 2 | 6 | 3 || 2 | 4 | 10 | 2.5 || 3 | 6 | 12 | 2 || 4 | 7 | 13 | 1.86 || 1, 2 | 6 | 16 | 2.67 || 1, 3 | 8 | 18 | 2.25 || 1, 4 | 9 | 19 | 2.11 || 2, 3 | 10 | 22 | 2.2 || 2, 4 | 11 | 23 | 2.09 || 3, 4 | 13 | 25 | 1.92 | 根据价值，首先选择装载$3, 4$，但该方案重量为$13$不可行，选择$2, 4$可行，此时背包剩余空间$2$，剩余方案$1, 3, 1+3$按价值密度降序为$1, 1+3, 3$，但重量均超过$c$，故不继续装入背包，该方案最终为$x = [0, 1, 0, 1], w_x = 11, p_x = 23$。 例(最小成本生成树)：给出加权图，求取最小生成树，使得成本(选取路径的成本之和)最低。 $n$个节点的最小生成树有$n-1$条边，有三种不同的贪婪策略，产生两个算法：Kruskal、Prim。 Kruskal 分步选择$n-1$条边，从剩下的边中选择成本最小且不会产生回路的边，加入已选择的边集。 “不会产生回路”可通过检查待加入边两个顶点是否在已选定边组成的图中已连通判定，判断连通可通过检查其中一个顶点通过的路径中是否包含另一个顶点。用该算法对上图进行的生成树选择示意图如下 Prim 分步选择$n-1$条边，从剩下的边中选择成本最小的边，加入已选择的边集，使该边集构成一棵树。 即选择与已选边集的顶点关联的、成本最小的边加入边集。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】图]]></title>
    <url>%2F2020%2F03%2F16%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[定义与概念定义：图(graph)是有限集$V$和$E$的有序对$G(V, E)$，其中$V$中的元素称为顶点(也称节点或点)，$E$的元素称为边(也称弧或线)。每一条边连接两个不同的顶点，可以用元组$(v_i, v_j)$来表示，$v_i, v_j$表示边所连接的两个顶点；当且仅当$(v_i, v_j)$是图的边，称顶点$v_i, v_j$是邻接的(adjacent)、边$(v_i, v_j)$关联(incident)于顶点$v_i, v_j$。 根据定义，一个图不能有重复的边；无向图任意两个节点间，最多有一条边，而有向图任意两个节点间每个方向最多有一条边；一个图不可能包含自连边(self-edge)或环(loop)，即$(v_i, v_i)$。 带方向的边叫有向边(directed edge)，不带方向的边叫无向边(undirected edge)；有向边$(v_i, v_j)$是关联至(incident to)顶点$v_j$而关联于(incident from)顶点$v_i$、顶点$v_i$邻接至(adjacent to)$v_j$，顶点$v_j$邻接于(adjacent from)$v_i$。 如果图的所有边都是无向边，那么该图叫做无向图(undirected graph)，如果所有的边都是有向边，那么该图叫做有向图(directed graph或digraph)。 若每条边被赋予一个表示成本的值，该值被称之为权，则此时图称为加权有向图(weighted digraph)和加权无向图(weighted undirected graph)。 概念： 简单路径是除始点和终点外，其余顶点都不相同的路径。 环路(cycle)是始点和终点相同的简单路径。 一个无向图$G = (V, E)$当且仅当$G$的每一对节点之间都有一条路径时，$G$是连通的(connected)，具有$n$个顶点的连通无向图至少有$n-1$条边。 没有环路的连通无向图是一棵树。 $G$的子图，如果包含$G$的所有顶点，且是一棵树，则称该子图为$G$的生成树(spanning tree)。 生成树可将网络建设成本减至最小，且保证网络的连通。 特性特性：设$G = (V, E)$是一个无向图，与顶点$v_i$相关联的边数称为该顶点的度(degree)$d_i$。则有 $\sum_{i=1}^{|V|} d_i = 2 |E|$ $0 \leq |E| \leq \frac{|V|(|V| - 1)}{2}$ 特性：设$G = (V, E)$是一个有向图中，顶点$v_i$的入度(in-degree)$d_i^{in}$是指关联至该顶点的边数，出度(out-degree)$d_i^{out}$是指关联于该顶点的边数，则有 $\sum_{i=1}^{|V|} d_i^{in} = \sum_{i=1}^{|V|} d_i^{out} = |E|$ $0 \leq |E| \leq |V|(|V| - 1)$ 图的描述无权图邻接矩阵一个$n$个顶点图$G = (V, E)$的邻接矩阵(adjacent matrix)是一个$n \times n$的矩阵$A$。对于无权无向图，元素定义如下 A[i, j] = \begin{cases} 1 & (v_i, v_j) \in E 或 (v_j, v_i) \in E \\ 0 & 其他 \end{cases}对于无权有向图 A[i, j] = \begin{cases} 1 & (v_i, v_j) \in E \\ 0 & 其他 \end{cases}如下图，两个图与其对应矩阵 \begin{aligned} A_1 = \begin{bmatrix} 0 & 1 & 1 & 1 \\ 1 & 0 & 1 & 0 \\ 1 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix}; & A_2 = \begin{bmatrix} 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} \end{aligned}以下改进可减少邻接矩阵存储空间 省略左对角线元素； 存储为稀疏矩阵； 无向图只存储上三角/下三角元素。 邻接链表/数组顶点$v_i$的邻接表(adjacent list)是一个线性表，可以用链表或数组描述，存储所有邻接与顶点$v_i$的顶点。一个图的邻接链表描述中，每个顶点都有一个邻接表。 有权图将无权图的描述进行扩充，可得到加权图的描述。 用成本邻接矩阵(cost-adjacency matrix)$C$描述加权图。$C[i, j]$表示边$(v_i. v_j)$的边的权，如果不存在需指定一个值，一般是很大的值； 链表/数组描述中，节点元素用数对表示，添加成员权weight，可从相应的无权图的邻接链表/数组得到加权图的邻接链表/数组； 上图对应矩阵为 \begin{aligned} A_1 = \begin{bmatrix} 0 & 10 & 20 & 30 \\ 10 & 0 & 40 & 0 \\ 20 & 40 & 0 & 50 \\ 30 & 0 & 50 & 0 \end{bmatrix}; & A_2 = \begin{bmatrix} 0 & 10 & 20 & 30 \\ 0 & 0 & 40 & 0 \\ 0 & 0 & 0 & 50 \\ 0 & 0 & 0 & 0 \end{bmatrix} \end{aligned} 图的遍历图的遍历有两种常用的方法：广度优先搜索(breadth-first search, BFS)和深度优先搜索(depth-first search, DFS)，可用于搜索从某个顶点开始可达到的所有顶点。 广度优先搜索广度优先搜索借助队列实现，算法描述如下 从某个指定顶点$v$出发，初始化队列$Q$，将$v$入队，初始化； 从$Q$出队一个顶点$w$，若$w$存在邻接顶点，将所有邻接顶点入队，并将这些顶点标记为可达到顶点；否则跳过； 循环2直至$Q$为空。 广度优先生成树(breadth-first spanning tree)是按BFS所得到的生成树。 深度优先搜索深度优先搜索用递归的方式实现，算法描述如下 从某个指定顶点$v$出发，若$v$存在邻接顶点，对所有邻接顶点标记，并依次进行深度优先搜索； 直至递归完成。 深度优先生成树(depth-first spanning tree)是按DFS所得到的生成树。 实现用Python语言简单实现上述两种遍历方式，并返回下图中的有向无权图的生成树 该图用矩阵描述，为 A = \begin{bmatrix} 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \end{bmatrix}1234567891011121314151617181920212223242526272829303132333435363738394041424344import numpy as npdef bfs(A, v): """ Params: A: &#123;ndarray(n, n)&#125; 邻接矩阵 v: &#123;int&#125; 起始点标号 Returns: T: &#123;ndarray(n, n)&#125; 最小生成树 V: &#123;list&#125; 所有可达到的顶点 """ Q = [v] # 用列表实现队列 T = np.zeros_like(A) # 最小生成树 V = [] while len(Q) &gt; 0: u = Q[0]; Q.pop(0) # 出队列 for j in range(A.shape[1]): if 0 != A[u, j] and j not in V: T[u, j] = A[u, j]; V += [j] # 标记节点 Q += [j] # 入队列 return T, Vdef dfs(A, v): """ Params: A: &#123;ndarray(n, n)&#125; 邻接矩阵 v: &#123;int&#125; 起始点标号 Returns: T: &#123;ndarray(n, n)&#125; 最小生成树 V: &#123;list&#125; 所有可达到的顶点 """ def _dfs(u, T, V): for j in range(A.shape[1]): if 0 != A[u, j] and j not in V: T[u, j] = A[u, j]; V += [j] _dfs(j, T, V) # 迭代 T = np.zeros_like(A); V = [] _dfs(v, T, V) # 从v开始搜索 return T, V 主函数如下12345678910111213141516171819if __name__ == "__main__": A = [ [0, 1, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0], ] T1, V1 = bfs(np.array(A), 0) T2, V2 = dfs(np.array(A), 0) print(T1, V1, T2, V2) 输出1234567891011121314151617181920212223[[0 1 1 1 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 0 0] [0 0 0 0 0 0 0 1 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0]] [1, 2, 3, 4, 5, 6, 7, 8] [[0 1 1 1 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 0 0] [0 0 0 0 0 0 0 1 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0]] [1, 4, 7, 2, 3, 5, 6, 8] 在本例中，两种方式生成树相同，示意图如下 最短路径问题可用Floyd算法计算每对节点间的最短路径：递推产生一个矩阵序列$A_1, \cdots, A_k, \cdots, A_n$，其中矩阵$A_k$的第$i$行第$j$列元素$A_k(i, j)$表示从顶点$v_i$到顶点$v_j$的路径上所经过的顶点个数不大于$k$的最短路径长度，计算时利用迭代公式 \begin{aligned} A_k(i, j) = \min \{ A_{k-1}(i, j), \quad A_{k-1}(i, t) + A_{k-1}(t, j) \}\\ 其中 k > 1, t = 1, \cdots, n \end{aligned}应有 A_1 = W 即从$v_i$到$v_j$途中，依次经过最多$0, 1, 2, \cdots, k$个中转点，保存这些路径中最小的距离。 下面是改进的Floyd算法，除了可求解每对节点间最短路径，还可将最短路径保存 12345678910111213141516171819202122232425def floyd(W): """ Params: W: &#123;ndarray(N, N)&#125; 邻接矩阵 Notes: A: &#123;ndarray(N, N)&#125; 两两之间最短路 R: &#123;list[list[list(k)](N)](N)&#125; 从节点`v_i`到节点`v_j`的最短路中间节点，保存在`R[i][j]`中 """ A = W.copy(); N = A.shape[0] R = [[[] for i in range(N)] for j in range(N)] for k in range(1, N): # 第k次中间节点寻找 for i in range(N): for j in range(N): # 比较经过中转前后路径长度 d = np.minimum(A[i, j], A[i, :] + A[:, j]) # i -&gt; t -&gt; j, t = 1, ..., N min_d = np.min(d) # 保存最短路径 if min_d &lt; A[i, j]: idx = np.argmin(d) R[i][j] = R[i][idx] + [idx] + R[idx][j] # 保存最短路径的长度 A[i, j] = min_d return A, R]]></content>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】平衡搜索树——分裂树和B树]]></title>
    <url>%2F2020%2F03%2F15%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%B9%B3%E8%A1%A1%E6%90%9C%E7%B4%A2%E6%A0%91%E2%80%94%E2%80%94%E5%88%86%E8%A3%82%E6%A0%91%E5%92%8CB%E6%A0%91%2F</url>
    <content type="text"><![CDATA[分裂树定义与概念在字典的很多实际应用中，令我们更感兴趣的不是一个单独操作所需时间，而是一个操作序列所需时间，此时应用的时间复杂度取决于一个字典操作序列而不是任意一个操作。 伸展树基于以下假设：想要对一个二叉查找树执行一系列的查找操作，为了使整个查找时间更小，根据每次的搜索关键字对树的结构进行自调整，使得被查频率高的那些条目就应当经常处于靠近树根的位置。 定义：分裂树(splay tree)，又叫伸展树，是一种二叉搜索树，对一个单独的字典操作，其时间复杂度是$O(n)$，而对于$f$个查找、$i$个插入、$d$个删除所组成的操作序列，其时间复杂度是$O((f + i + d) \log i)$，与使用AVL树或RB树的渐近时间复杂度相同，编码更容易。 定义：分裂节点(slay node)是在字典操作中所检查的最深层的节点。插入操作时，可能生成新节点，或覆盖已存在节点。此时新生成的节点或被覆盖节点即分裂节点；删除操作时，被删除节点不在树中，不可能为分裂节点，所以被删除节点的父节点成为分裂节点。 分裂操作是通过一个分裂步骤序列，将分裂节点移动到根节点的位置上。分裂步骤可将指定节点向上移动一层或两层，移动一层的可分为$L, R 2$种，移动两层的可分为$LL, LR, RR, RL 4$种。记$q$分别为分裂节点、$p, g$分别为$q$的父节点、祖父节点，当$g$存在时选用移动两层的分裂操作。分裂操作示意图如下，注意分裂步骤与AVL树的相似性和区别，子树$\mathcal{T}_g$的$LL/RR$型分裂操作是指自顶向下的，$LR/RL$型分裂操作是自底向上的。 AVL树在插入时遇到$LL$型不平衡时，旋转矫正前后对比图如下 以下图所示的二叉搜索树为例，展示对节点$2$与节点$3$两次查询后，树的结构变化 具体实现由BinarySearchTree&lt;K, V&gt;公有派生，添加分裂操作slay，重写插入insert、删除erase函数 1234567891011121314template&lt;typename K, typename V&gt;class SplayTree : public BinarySearchTree&lt;K, V&gt;&#123;public: SplayTree() : BinarySearchTree&lt;K, V&gt;() &#123;&#125; ~SplayTree()&#123;&#125; void insert(const K&amp;, const V&amp;); void erase(const K&amp;);protected: void slay(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*);&#125;; 分裂操作实现时有以下注意点 用$3bit$内存对$6$种情况进行编码：最高位为0表示一层的分裂操作，次高位表示$g$与$p$的位置关系，最低位表示$p$与$q$的位置关系； 若$L/R$型分裂时父节点为根节点，或$LL/LR/RR/RL$分裂时祖父节点为根节点，则需修改整棵树的根节点为分裂节点； 在$LR/RL$操作时，为自底向上操作，$p$节点地址为node-&gt;parent，第一次旋转结束后，$q$的父节点已经变为$g$，所以对$g$进行旋转时，该节点地址也为node-&gt;parent。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546template&lt;typename K, typename V&gt;void SplayTree&lt;K, V&gt;::slay(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)&#123; // 分裂操作 while (node-&gt;parent) &#123; int type = node-&gt;parent-&gt;isRoot() &lt;&lt; 2; if (type) &#123; // 无祖父节点，即父节点为根节点 type += node-&gt;isLeft(); this-&gt;m_tnRoot = node; // 修改根节点 &#125; else &#123; // 有祖父节点 type += (node-&gt;parent-&gt;isLeft() &lt;&lt; 1) + node-&gt;isLeft(); if (node-&gt;parent-&gt;parent-&gt;isRoot()) this-&gt;m_tnRoot = node; // 修改根节点 &#125; switch (type) &#123; case 0b0101: // L SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent); break; case 0b0100: // R SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent); break; case 0b0011: // LL SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent-&gt;parent); SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent); break; case 0b0010: // LR SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent); SplayTree&lt;K, V&gt;::leftRotate (node-&gt;parent); break; case 0b0000: // RR SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent-&gt;parent); SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent); break; case 0b0001: // RL SplayTree&lt;K, V&gt;::leftRotate(node-&gt;parent); SplayTree&lt;K, V&gt;::rightRotate(node-&gt;parent); break; default: break; &#125; &#125;&#125; 在搜索二叉树的插入、删除基础上，添加分类操作slay()即可 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556template&lt;typename K, typename V&gt;void SplayTree&lt;K, V&gt;::insert(const K&amp; key, const V&amp; value)&#123; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = this-&gt;find(key, true); Pair&lt;K, V&gt;* p = node-&gt;get(); if (p-&gt;getKey() != key) p-&gt;setKey(key); p-&gt;setVal(value); slay(node);&#125;template&lt;typename K, typename V&gt;void SplayTree&lt;K, V&gt;::erase(const K&amp; key)&#123; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = this-&gt;find(key, false); if (!node) return; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* slayNode = node-&gt;parent; // ------------------ 重新组织二叉树 ------------------ // 查找左子树的最大值，或右子树的最小值 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* replace = nullptr; while (!node-&gt;isLeaf()) &#123; // 直到搜索到叶节点为止 if (node-&gt;left) &#123; replace = this-&gt;max(node-&gt;left); &#125; else if (node-&gt;right) &#123; replace = this-&gt;min(node-&gt;right); &#125; if (replace) &#123; // 找到可替换子节点 Pair&lt;K, V&gt;* np = node-&gt;get(); Pair&lt;K, V&gt;* rp = replace-&gt;get(); np-&gt;setKey(rp-&gt;getKey()); np-&gt;setVal(rp-&gt;getVal()); node = replace; &#125; &#125; if (node-&gt;parent) &#123; // 修改父节点信息 if (node == node-&gt;parent-&gt;left) node-&gt;parent-&gt;left = nullptr; else node-&gt;parent-&gt;right = nullptr; &#125; else &#123; // 无父节点，即整棵树只有一个根节点，则修改根节点为空 this-&gt;m_tnRoot = nullptr; &#125; // ------------------ 释放资源 ------------------ Pair&lt;K, V&gt;* pair = node-&gt;get(); delete pair; delete node; slay(slayNode);&#125; 测试1234567891011121314151617181920212223242526272829int main(int argc, char** argv)&#123; string names[6] = &#123; "甲", "乙", "丙", "丁", "戊", "己" &#125;; int numbers[6] = &#123; 2, 3, 4, 1, 5, 6 &#125;; SplayTree&lt;int, string&gt; tree; // 插入键值对 cout &lt;&lt; endl &lt;&lt; "插入" &lt;&lt; endl; for (int i = 0; i &lt; 6; i++) &#123; tree.insert(numbers[i], names[i]); &#125; tree.print(); // 查询2 tree.insert(6, "乙"); tree.print(); // 查询3 tree.insert(3, "丙"); tree.print(); // 删除2 tree.erase(2); tree.print(); system("pause");&#125; 以下均为层级遍历输出，可以看到对指定关键字进行字典操作后，分裂节点将被调整至根节点 123456插入[6]己 [5]戊 [4]丙 [1]丁 [2]甲 [3]乙[6]乙 [5]戊 [4]丙 [1]丁 [2]甲 [3]乙[3]丙 [2]甲 [6]乙 [1]丁 [4]丙 [5]戊[3]丙 [1]丁 [6]乙 [4]丙 [5]戊请按任意键继续. . . B-树当字典足够小时，AVL树和RB树都能保证良好的时间性能。而对于存储在磁盘上的大型字典(外部字典或文件)，则需要度数更高的搜索树来改善字典操作性能。 定义与概念索引顺序访问方法(ISAM)索引顺序访问方法(indexed sequential access method, ISAM)本质上是一种数组描述方法。在该方法中，可用的磁盘空间被划分为很多块，字典元素以升序存储在块中。 在顺序访问时，块按序输入，每一个块中元素按升序搜索；若要支持随机访问，就要维持一个索引，索引包括每个块的最大关键字，且组成的块索引表足以驻留在内存中，在访问关键字为$k$的元素时，先查找索引表确定块索引，然后把相应的块从磁盘中读取进行内部搜索，这样仅需一次磁盘访问即可完成。 若同时存在多个磁盘，每个磁盘存在一个磁盘索引，保存该磁盘中的最大关键字，全部磁盘索引组成磁盘索引表。在查找元素时，按磁盘索引、块索引、内部搜索的顺序进行元素查找，此时需要两次磁盘访问(读取磁盘索引、磁盘中读取块)。 磁盘的块是磁盘空间中用来输入或输出的最小单位，一般具有和此道一样的长度，且可以在一次搜索和延迟中完成输入或输出，按照一种顺序来组织，使得从一块到另一块的延时最短； 为减少块与块之间的元素拷贝，可在每块中预留一些空间供少量元素的插入，删除时可将闲置空间保留，不必进行元素移动操作。 对于存储在磁盘上的数据，B-树是一种适合于索引方法的数据结构。 $m$叉搜索树定义：$m$叉搜索树(m-way search tree)可以是一棵空树，如果非空，需满足以下特征： 进行扩充(外部节点替换空指针)后的搜索树，每个内部节点最多包含$m$个孩子、$1 \sim m-1$个元素； 包含$p$个元素($k_1, \cdots, k_p$)的节点有$p + 1$个孩子($c_0, c_1, \cdots, c_p$)； 对于包含$p$个元素的节点，元素关键字满足$k_i &lt; k_{i + 1}, 1 \leq i \leq p - 1$； 对于包含$p$个元素的节点，子树的元素关键字满足： 子树$\mathcal{T}_{c_0}$的元素关键字都小于$k_1$； 子树$\mathcal{T}_{c_p}$的元素关键字都大于$k_p$； $\mathcal{T}_{c_i}(0 &lt; i &lt; p)$中的元素关键字大于$k_i$而小于$k_{i+1}$。 下图为省略外部节点的$7$叉搜索树 一棵高度为$h$的$m$叉搜索树(不包含外部节点)，最少有$h$个元素，最多有$m^h-1$个元素。 当每层仅有$1$个节点，且每个节点只有$1$个元素时，这棵树元素个数最少； 第$i(i=1, …, h)$层最多有$m^{h-1}$个节点，而每个节点最多有$m-1$个元素，则总的元素个数为$(m - 1) \times \sum_{i=1}^h m^{j-1} = m^h - 1$。 对$m$叉搜索树进行查找时，类似二叉搜索树。从根节点开始搜索关键字$k$，若根节点中不存在查找关键字，则根据特征4，在对应的子节点中进行搜索，直至找到对应关键字或空节点(外部节点)为止。 例如在上图中查找关键字为$33$的数值对时，从根节点出发，在根结点中不存在$33$，在其$10-18$间的子节点内搜索，而该子节点中也不存在$33$，在$30-40$间的子节点内搜索，还是不存在，于是在$32-36$的子节点中查找，但此时为外部节点，故整棵树中$33$不存在，退出算法。 插入操作时，首先查找指定的关键字$k$，若$k$存在，则将值修改后退出算法；若$k$不存在，并且搜索路径上存在元素个数少于$m$、满足特征4的节点，将其插入，如果找不到这样的节点，则在最后一个节点内，根据元素大小，选择合适的节点位置创建新节点，然后将数值对插入。 例如，插入键为$33$的数值对时，用查找算法搜索确定不存在相同键的元素，每个节点可容纳$6$个元素，此时节点$[10, 18]$元素未满，但$33$比子节点$[20, 30, 40, 50, 60, 70]$中元素小，故不满足特征$4$，子节点$[20, 30, 40, 50, 60, 70]$元素已满，第$3$层$[32, 36]$符合要求，故将数值对插入该节点，成为第$2$个元素，即$[32, 33, 36]$。 删除某个元素时，若该元素相邻子节点空，可直接删除，否则将该元素与相邻的左子节点中的键最大元素，或右子节点中最键小元素替换，再将元素删除。 $m$阶B-树定义：$m$阶B-树(B-Tree of order $m$)是一棵$m$叉搜索树，如果B-树非空，那么相应的扩充树满足以下特征： 根节点至少有$2$个孩子，$1$个元素； 除根节点外，内部节点至少有$d = \lceil m/2 \rceil$个孩子，$d-1$个元素； 外部节点在同一层。 $2$阶B-树，内部节点(包括根节点)都恰好有$2$个孩子，且外部节点在同一层，所以$2$阶B-树是满二叉树； $3$阶B-树，内部节点(除根节点)有$2 \sim 3$个孩子，也称$2-3$树； $4$阶B-树，内部节点(除根节点)有$2 \sim 4$个孩子，也称$2-3-4$树； 疑问：$2$阶B-树特征2：$\lceil m/2 \rceil = 1$，为什么内部节点至少有$2$个孩子？ 下图为一棵$2-3$树，当插入$14,16$到$[10]$节点后，改树又变成了$2-3-4$树 定理：设$\mathcal{T}$是一棵高度为$h$的$m$阶B-树。令$d = \lceil m/2 \rceil$，$n$是$\mathcal{T}$的元素个数，那么 $2d^{h-1} - 1 \leq n \leq m^h - 1$ $\log_m (n + 1) \leq h \leq \log_d(\frac{n + 1}{2}) + 1$ 第$1, 2$层的节点最少个数为$1, 2$，第$3, 4, \cdots, h$层的节点的父节点至少有$d$个孩子，那么这些层的节点最少个数为$2d, 2d^2, \cdots, 2d^{h-2}$，每个节点有$d-1$个元素，故下限为$(1 + 2 + 2d + 2d^2 + \cdots + 2d^{h-2}) \times (d - 1) = 2d^{h-1} - 1$；上限由$m$叉搜索树决定； 由1可推得2。 B-树的搜索与$m$叉搜索树算法相同，插入操作有所区别，首先搜索相同关键字的元素，若不存在，可将元素插入在搜索路径中最后一个内部节点中，但如果该节点已饱和，对节点进行元素插入时需要分裂该节点。 现将带有空指针的新元素$e$插入饱和节点$P$，得到具有$m$个元素和$m+1$个孩子的溢出节点，$(e_i, c_i)$表示元素与该元素的右子节点，那么溢出节点可用序列表示如下 m, (c_0), (e_1, c_1), \cdots, (e_m, c_m)使该节点在元素$e_d, d = \lceil m/2 \rceil$处分裂，将右边的节点保存在新节点$Q$中，再将$(e_d, Q)$插入$P$的父节点，使$e_d$成为$P$父节点的元素，$Q$成为$P$的兄弟节点，完成分裂，此时两节点序列为 \begin{aligned} P: d-1, (c_0), (e_1, c_1), \cdots, (e_{d-1}, c_{d-1}) \\ Q: m-d, (c_d), (e_{d+1}, c_{d+1}), \cdots, (e_m, c_m) \end{aligned}若$e_d$插入父节点导致父节点满，则需要对父节点进行分裂操作，以此类推直到至根节点路径上没有饱和节点。 当插入操作引起$s$个节点分裂时，磁盘访问的次数为$h + 2s + 1$(读取搜索路径上的节点 + 回写分裂出的两个新节点 + 回写新的根节点火插入后没有导致分裂的节点)，最多可达到$3h + 1$。 以下图$7$阶B-树为例，插入$3$时，在最后一个内部节点节点$[2, 4, 6]$处搜索失败，且该节点未饱和，将其插入该节点得到$[2, 3, 4, 6]$ 而插入元素$25$时，节点$[20, 30, 40, 50, 60, 70]$是最后一个内部节点，但已饱和，需要进行分裂操作。将新元素插入后溢出节点序列为 7, (0), (20, 0), (25, 0), (30, 0), (40, 0), (50, 0), (60, 0), (70, 0)此时$d = 4$，在元素$40$处进行分裂，得到两个新节点序列 \begin{aligned} P: 3, (0), (20, 0), (25, 0), (30, 0) \\ Q: 3, (0), (50, 0), (60, 0), (70, 0) \end{aligned}将$(40, Q)$插入父节点，此时父节点为($L, R$表示$10$左边的节点，$R$表示$80$右边的节点) 3, (L), (10, P), (40, Q), (80, R)如下图 删除操作可分为两种情况： 该元素位于非叶子节点 用被删除元素的左相邻子树的最大元素，或右相邻子树的最小元素替换被删除元素，替换元素必在叶节点，此时转化为第2种情况； 该元素位于叶子节点 被删除元素所在叶节点的元素个数大于最少数(性质1)，那么直接删除； 被删除元素在非根节点且该节点元素个数最少，可用其最近邻的左兄弟节点中最大元素或最近邻的右兄弟中的最小元素替换； 最近邻兄弟节点不包含额外元素时，将删除元素的节点($d-2$个元素)、最近邻兄弟节点($d-1$个元素)、父节点中介于两兄弟间的元素合并成一个节点($2d-2$个元素)； 若合并导致父节点元素个数低于最小元素个数，对该父节点重复上述操作，直至根节点(最坏情况下)。 以$7$阶B-树为例，删除元素$99$首先，寻找可以替换的叶子节点中的元素，可选择左近邻节点中的$86$，或右近邻中的$92$。 当选择$86$作为替换元素时，节点$2, (0), (82, 0), (84, 0)$缺少一个元素，此时可从该节点的最近邻左兄弟节点取择元素$70$加入 当选择$96$作为替换元素时，节点$2, (0), (92, 0), (94, 0)$缺少一个元素，且$3, (0), (82, 0), (84, 0), (84, 0)$不包含多余元素，那么将这两个节点与父节点中的$92$合并为$6, (0), (82, 0), (84, 0), (84, 0), (86, 0), (92, 0), (94, 0), (96, 0)$以上两种方式都不会使根节点少于$2$个孩子。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】平衡搜索树——AVL树与RB树]]></title>
    <url>%2F2020%2F03%2F14%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E5%B9%B3%E8%A1%A1%E6%90%9C%E7%B4%A2%E6%A0%91%E2%80%94%E2%80%94AVL%E6%A0%91%E4%B8%8ERB%E6%A0%91%2F</url>
    <content type="text"><![CDATA[概述定义：最坏情况下高度为$O(\log n)$的树称为平衡树(balanced tree)。如果搜索树的高度总是$O(\log n)$，就能保证查找、插入和删除的时间为$O(\log n)$。 在实际应用中，当操作是以关键字进行查找、插入和删除时，散列技术在性能方面超过了平衡搜索树。但以下情况提倡使用平衡搜索树 按照关键字实施字典操作，而且操作时间不能超过指定的范围；2. 按名次实施查找和删除； 不按精确的关键字匹配进行的字典操作(如寻找关键字大于$k$的最小元素)。 本节包含两种平衡二叉树结构——AVL和红-黑树(适合内部存储的应用)，下一节介绍B-树(度数更大，高度更小，适合外部存储的应用，如磁盘上的大型词典)。这些平衡树结构能在最坏情况下用时$O(\log n)$实现字典操作和按名次操作。 搜索二叉树的旋转记对子树$\mathcal{T_N}$进行的左旋操作为$lr(\mathcal{N})$，右旋操作为$rr(\mathcal{N})$ 搜索二叉树进行左旋或右旋后，该树根节点发生变化，仍满足搜索二叉树结构，可用于平衡搜索树的不平衡矫正； 单旋转(single rorating)是对子树的根节点进行一次旋转； 双旋转(double rotating)是对根节点进行旋转前，先对其中一棵子树进行旋转。 下图为对以$\mathcal{A}$为根节点的树$\mathcal{T_A}$进行左旋、右旋的示意图， 巧记：以旋转的树的根节点$\mathcal{N}$与$\mathcal{N_L}, \mathcal{N_R}$作圆，右旋时，该圆下方向右。 BinarySearchTree&lt;K, V&gt;添加静态成员函数如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253template&lt;typename K, typename V&gt;class BinarySearchTree : public LinkedBinaryTree&lt;Pair&lt;K, V&gt;*&gt;&#123; // ... static void leftRotate(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*); static void rightRotate(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*);&#125;;template&lt;typename K, typename V&gt;void BinarySearchTree&lt;K, V&gt;::leftRotate(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A)&#123; // 修改根节点 if (A-&gt;parent) &#123; if (A == A-&gt;parent-&gt;left) A-&gt;parent-&gt;left = A-&gt;left; else A-&gt;parent-&gt;right = A-&gt;left; &#125; A-&gt;left-&gt;parent = A-&gt;parent; // 修改A为Al右孩 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* Alr = A-&gt;left-&gt;right; A-&gt;parent = A-&gt;left; A-&gt;parent-&gt;right = A; // 修改Alr为A左孩 A-&gt;left = Alr; if (Alr) Alr-&gt;parent = A;&#125;template&lt;typename K, typename V&gt;void BinarySearchTree&lt;K, V&gt;::rightRotate(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A)&#123; // 修改根节点 if (A-&gt;parent) &#123; if (A == A-&gt;parent-&gt;left) A-&gt;parent-&gt;left = A-&gt;right; else A-&gt;parent-&gt;right = A-&gt;right; &#125; A-&gt;right-&gt;parent = A-&gt;parent; // 修改A为Ar左孩 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* Arl = A-&gt;right-&gt;left; A-&gt;parent = A-&gt;right; A-&gt;parent-&gt;left = A; // 修改Arl为A右孩 A-&gt;right = Arl; if (Arl) Arl-&gt;parent = A;&#125; AVL树AVL树是通过限制左右子节点高度差进行整棵树的高度限制。 定义及概念定义：AVL树由Adelson-Velskii和Landis在1962年提出。一棵空的二叉树是AVL树；若$\mathcal{T}$是一棵非空二叉树，$\mathcal{T_L}$和$\mathcal{T_R}$是其左子树和右子树，那么满足以下条件时，$\mathcal{T}$是一棵AVL树： $\mathcal{T_L}$和$\mathcal{T_R}$是AVL树； $|h_{\mathcal{L}} - h_{\mathcal{R}}| \leq 1$，其中$|h_{}|$是子树$\mathcal{T_}$的高。 既是二叉搜索树，也是AVL树的树称为AVL搜索树；既是索引二叉搜索树，也是AVL树的树称为索引AVL搜索树。下图为一棵AVL搜索树 AVL树具有以下特征 一棵$n$个元素的AVL树，其高度是$O(\log n)$，最坏情况下，AVL树的高度是搜索树中最小的； 对于每一个$n$，$n \geq 0$，都存在一棵AVL树； 对一棵$n$个元素的AVL搜索树，在$O(height) = O(\log n)$的时间内可以实现查找； 将一个元素插入一棵$n$个元素的AVL搜索树，可以得到$n+1$个元素的AVL树，且插入用时为$O(\log n)$； 将一个元素从一棵$n$个元素的AVL搜索树中删除，可以得到$n-1$个元素的AVL树，且删除用时为$O(\log n)$。 关于AVL树的高度，对一棵高度为$h$的AVL树，令$N_h$是其最少的结点数。在最坏情况下，根的一棵子树高度为$h-1$，另一棵子树高度为$h-2$，那么 N_h = N_{h-1} + N_{h-2} + 1, N_0 = 0 且 N_1 = 1注意由斐波那契数列定义 F_n = F_{n-1} + F_{n-2}, F_0 = 0且F_1 = 1可证得 N_h = F_{h+2} - 1 , h \geq 0由斐波那契定理 F_h \approx \Phi^h / \sqrt{5}, 其中\Phi = (1 + \sqrt{5}) / 2那么 \begin{aligned} N_h \approx \Phi^{h+2} / \sqrt{5} - 1 \\ \Rightarrow h = \log_{\Phi} \left[ \sqrt{5}(N_h + 1) \right] - 2 \\ \approx 1.44 \log_2(n + 2) = O(\log n) \end{aligned} 算法描述AVL树的节点$\mathcal{N}$增加平衡因子 bf(\mathcal{N}) = h(\mathcal{T_L}) - h(\mathcal{T_L})其中$h(\mathcal{})$为树$\mathcal{T_{}}$的高度，由平衡搜索定义可知，平衡因子取值范围为${ 0, \pm 1, \pm 2 }$，$bf = \pm2$时表示以该节点为根节点的子树不平衡。 搜索AVL搜索树的搜索，沿用二叉搜索树的搜索方式即可，搜索时间与高度成正比，为$O(\log n)$。 插入记 $\mathcal{X}$为寻找插入节点位置路径上最后一个具有平衡因子$\pm 1$的节点 $\mathcal{A}$为距新插入节点$\mathcal{N}$最近的、平衡因子为$\pm 2$的祖先节点 以下图为例，插入关键字为$36$的新节点$\mathcal{N}$，导致右子树失去AVL树的平衡结构 插入时有以下特性 只有从根节点到新插入节点的路径上的节点，其平衡因子在插入后会改变； 插入操作只会使平衡因子增减$0$或$1$，平衡因子为$\pm 2$的节点在插入前，平衡因子为$\pm 1$，插入操作后$bf(\mathcal{X})$从$\pm 1$变为$\pm 2$是导致AVL失去平衡的唯一过程； 从$\mathcal{A}$到$\mathcal{N}$的路径上，在插入操作前，所有节点的平衡因子都是$0$。 节点$\mathcal{A}$的不平衡情况 可分为$L$型不平衡($\mathcal{N}$在$\mathcal{A}$左子树中)与$R$型不平衡($\mathcal{N}$在$\mathcal{A}$右子树中)两类； 根据$\mathcal{A}$的孙节点情况，又可分为$LL$(孙节点在$\mathcal{A}$左子树的左子树中), $LR$(孙节点在$\mathcal{A}$左子树的右子树中)，$RL$，$RR$不平衡。 不平衡的矫正 LL型不平衡的矫正，只需将$\mathcal{T_A}$进行左旋；RR型同理； LR型不平衡的矫正，，对$LR$型不平衡所作的双旋转可视作$\mathcal{T_{A_L}}$的右旋，再进行$\mathcal{T_A}$的左旋，RL型同理； 由于插入前子树的高度与插入并矫正后的子树高度相同，故在一次旋转后，整棵树是平和的，不需要继续向根节点搜索不平衡节点； 不平衡类型 LL LR RR RL 矫正 $lr(\mathcal{T_{A}})$ $rr(\mathcal{T_{A_L}}) + lr(\mathcal{T_{A}})$ $rr(\mathcal{T_{A}})$ $lr(\mathcal{T_{A_R}}) + rr(\mathcal{T_{A}})$ 算法描述 给定元素的关键字$\mathcal{K}$，从根结点出发查找，若找到具有相同关键字的节点$\mathcal{N}$，则返回该节点，否则进入2； 从根结点出发查找插入位置，并在该位置创建新节点$\mathcal{N}$，更新$\mathcal{N}$至根节点的路径上的节点的平衡因子，若不存在$bf=\pm 2$的节点，返回$\mathcal{N}$退出算法，否则进入3； 记距离$\mathcal{N}$最近的、且$bf = \pm 2$的祖先节点为$\mathcal{A}$，确定$\mathcal{A}$的不平衡类型，并执行相应旋转； 更新$\mathcal{N}$相关节点的平衡因子，返回$\mathcal{N}$退出算法。 删除记 删除节点$\mathcal{N}$的父节点为$\mathcal{P}$； 从$\mathcal{P}$至根节点的路径上第一个平衡因子变为$\pm 2$的节点记作$\mathcal{A}$。 如下图，删除关键字为$25$的结点后，该树不满足平衡搜索树条件 删除时有以下特性 节点$\mathcal{N}$删除后，从根节点到$\mathcal{P}$路径上的一些节点或全部节点的平衡因子都改变了，故要从$\mathcal{P}$原路返回； 如删除发生在$\mathcal{P_L}$(左子节点)，那么$bf(\mathcal{P})$减$1$，发生在$\mathcal{P_R}$(右子节点)时，$bf(\mathcal{P})$加$1$； $bf(\mathcal{P})$更新后记作$bf_*(\mathcal{P})$，则有 \begin{cases} bf_*(\mathcal{P}) = 0 && \mathcal{T_P}高度减少1，需改变它的祖先节点的平衡因子 \\ bf_*(\mathcal{P}) = \pm 1 && \mathcal{T_P}高度不变，无需改变它的祖先节点的平衡因子 \\ bf_*(\mathcal{P}) = \pm 2 && \mathcal{P}不平衡 \end{cases} 节点$\mathcal{A}$的不平衡情况 可分为$L$型不平衡($\mathcal{N}$在$\mathcal{A}$左子树中，$bf(\mathcal{P}) = -2$)与$R$型不平衡($\mathcal{N}$在$\mathcal{A}$右子树中，$bf(\mathcal{P}) = 2$)两类； 若发生$R$型不平衡，考察$\mathcal{P}$的左子节点$\mathcal{P_L}$，根据$bf(\mathcal{P_L})$又可分为$R0, R1, R-1$型不平衡；$L$型同理； 不平衡的矫正 R0型不平衡的矫正，将$\mathcal{T_A}$左旋，注意平衡后$bf(\mathcal{A_L})=-1$，子树高度仍为$h+2$未改变，故无需改变根节点途径上的平衡因子； R1型不平衡的矫正，同$R0$型不平衡矫正，将$\mathcal{T_A}$进行左旋，区别是平衡后$bf(\mathcal{A_L})=0$，子树高度减少为$h+1$，某些祖先平衡因子改变，可能需要进行旋转保持平衡； R-1型不平衡的矫正，先将$\mathcal{T_{A_L}}$右旋，再将$\mathcal{T_A}$左旋，调整后该树高度为$h+1$，某些祖先平衡因子改变，可能需要进行旋转保持平衡； L1型不平衡与L-1型不平衡矫正分别与R-1型不平衡、R1型不平衡对应，即L1为双旋转。 不平衡类型 R-1 R0 R1 L-1 L0 L1 矫正 $rr(\mathcal{T_{A_L}}) + lr(\mathcal{T_{A}})$ $lr(\mathcal{T_{A}})$ $lr(\mathcal{T_{A}})$ $rr(\mathcal{T_{A}})$ $rr(\mathcal{T_{A}})$ $lr(\mathcal{T_{A_R}}) + rr(\mathcal{T_{A}})$ 算法描述 给定元素的关键字$\mathcal{K}$，从根结点出发查找，若未找到具有相同关键字的节点$\mathcal{N}$，退出算法，否则进入2； 从根结点出发查找删除节点$\mathcal{N}$，其父节点为$\mathcal{P}$，记距离$\mathcal{P}$最近的、且$bf = \pm 2$的祖先节点为$\mathcal{A}$，确定$\mathcal{A}$的不平衡类型，并执行相应旋转； 根据$bf(\mathcal{P})$判断是否需要向根节点进行平衡树结构调整，不需要或调整后，退出算法。 具体实现声明由BinarySearchTree&lt;K, V&gt;派生为AVLTree&lt;K, V&gt;，重载insert与erase函数，find无需重载，相应添加部分功能性函数.声明如下 12345678910111213141516template&lt;typename K, typename V&gt;class AVLTree : public BinarySearchTree&lt;K, V&gt;&#123;public: AVLTree() : BinarySearchTree&lt;K, V&gt;() &#123;&#125; ~AVLTree() &#123;&#125; void insert(const K&amp;, const V&amp;); void erase(const K&amp;);protected: static int balancedFactor(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*); static BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* findImbalancedAncestor( BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*, LinkedStack&lt;bool&gt;**);&#125;; 定义 节点平衡因子 为减少代码改动，节点的平衡因子通过调用函数的方式,动态求解 123456template&lt;typename K, typename V&gt;int AVLTree&lt;K, V&gt;::balancedFactor(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)&#123; return AVLTree&lt;K, V&gt;::heightofNode(node-&gt;left) - \ AVLTree&lt;K, V&gt;::heightofNode(node-&gt;right);&#125; 由节点$\mathcal{N}$向上搜索最近的不平衡因子，并将路径保存在栈内 1234567891011121314151617181920212223template&lt;typename K, typename V&gt;BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* AVLTree&lt;K, V&gt;::\ findImbalancedAncestor(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node, LinkedStack&lt;bool&gt;** routine)&#123; // 向上查找节点A，并记录路径 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A = node; int bf = balancedFactor(A); while (bf &gt; -2 &amp;&amp; bf &lt; 2) &#123; if (!A-&gt;parent) return nullptr; if (routine) &#123; if (A == A-&gt;parent-&gt;left) (*routine)-&gt;push(false); else (*routine)-&gt;push(true); &#125; A = A-&gt;parent; bf = balancedFactor(A); &#125; return A;&#125; 插入 插入时，首先调用find进行位置查找，若不存在该节点则创建；由新建节点向根节点查找最近的不平衡祖先并记录路径；根据路径判断不平衡类型做相应旋转 123456789101112131415161718192021222324252627282930313233343536373839404142434445template&lt;typename K, typename V&gt;void AVLTree&lt;K, V&gt;::insert(const K&amp; key, const V&amp; value)&#123; // 查找插入位置 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = this-&gt;find(key, true); Pair&lt;K, V&gt;* p = node-&gt;get(); if (p-&gt;getKey() != key) p-&gt;setKey(key); p-&gt;setVal(value); // 查找最近的不平衡祖先，并记录路径 LinkedStack&lt;bool&gt;* routine = new LinkedStack&lt;bool&gt;; // 向左为false BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A = findImbalancedAncestor(node, &amp;routine); if (!A) &#123; delete routine; return; &#125; // 判断不平衡类型，并相应旋转 int type = (routine-&gt;pop() &lt;&lt; 1) + routine-&gt;pop(); switch (type) &#123; case 0: // LL this-&gt;leftRotate(A); break; case 1: // LR this-&gt;rightRotate(A-&gt;left); this-&gt;leftRotate(A); break; case 2: // RL this-&gt;leftRotate(A-&gt;right); this-&gt;rightRotate(A); break; case 3: // RR this-&gt;rightRotate(A); break; default: break; &#125; // 若A为根节点，旋转后需改变根节点指针 if (!A-&gt;parent-&gt;parent) this-&gt;m_tnRoot = A-&gt;parent; delete routine;&#125; 删除 删除时，首先进行节点查找，若找到，记录其父节点信息(在搜索树结构重组织时会失去该节点信息，故先进行保存)；重新组织搜索二叉树结构后，维持平衡二叉树， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100template&lt;typename K, typename V&gt;void AVLTree&lt;K, V&gt;::erase(const K&amp; key)&#123; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = this-&gt;find(key, false); if (!node) return; // 记录父节点与其兄弟节点 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* P = node-&gt;parent; // ------------------ 重新组织二叉树 ------------------ // 查找左子树的最大值，或右子树的最小值 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* replace = nullptr; while (!node-&gt;isLeaf()) &#123; // 直到搜索到叶节点为止 if (node-&gt;left) &#123; replace = this-&gt;max(node-&gt;left); &#125; else if (node-&gt;right) &#123; replace = this-&gt;min(node-&gt;right); &#125; if (replace) &#123; // 找到可替换子节点 Pair&lt;K, V&gt;* np = node-&gt;get(); Pair&lt;K, V&gt;* rp = replace-&gt;get(); np-&gt;setKey(rp-&gt;getKey()); np-&gt;setVal(rp-&gt;getVal()); node = replace; &#125; &#125; if (node-&gt;parent) &#123; // 修改父节点信息 if (node == node-&gt;parent-&gt;left) node-&gt;parent-&gt;left = nullptr; else node-&gt;parent-&gt;right = nullptr; &#125; else &#123; // 无父节点，即整棵树只有一个根节点，则修改根节点为空 this-&gt;m_tnRoot = nullptr; &#125; // ------------------ 维持平衡二叉树 ------------------ // 查找最近的不平衡祖先，并记录路径 LinkedStack&lt;bool&gt;* routine = new LinkedStack&lt;bool&gt;; // 向左为false BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* A = findImbalancedAncestor(P, &amp;routine); // 向根节点进行 while (A) &#123; // 判断类型并矫正 bool deleteRight = false; // 若为`true`，表示R类型。否则L类型 if (routine-&gt;empty()) &#123; // P即为A，此时必有一子节点为空 if (!A-&gt;left) // 左节点为空 deleteRight = false; else if (!A-&gt;right) // 右节点为空 deleteRight = true; &#125; else &#123; deleteRight = routine-&gt;pop(); &#125; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* B = \ deleteRight ? A-&gt;left : A-&gt;right; // 不平衡节点的另一侧子树 int type = (deleteRight &lt;&lt; 2) + balancedFactor(B); switch (type) &#123; case -1: // L-1 this-&gt;rightRotate(A); break; case 0: // L0 this-&gt;rightRotate(A); break; case 1: // L1 this-&gt;leftRotate(A-&gt;right); this-&gt;rightRotate(A); break; case 3: // R-1 this-&gt;rightRotate(A-&gt;left); this-&gt;leftRotate(A); break; case 4: // R0 this-&gt;leftRotate(A); break; case 5: // R1 this-&gt;leftRotate(A); break; default: break; &#125; // 修改整棵树的根节点 if (!A-&gt;parent-&gt;parent) this-&gt;m_tnRoot = A-&gt;parent; // 向根节点搜索不平衡节点，更新A for (int i = 0; i &lt; routine-&gt;size(); i++) routine-&gt;pop(); // 清除栈 A = findImbalancedAncestor(A, &amp;routine); &#125; // ------------------ 释放资源 ------------------ delete routine; Pair&lt;K, V&gt;* pair = node-&gt;get(); delete pair; delete node;&#125; 测试插入按顺序插入以下关键字 序号 0 1 2 3 4 5 6 7 关键字 20 15 25 12 10 23 24 19 在插入$10$以前，该树无不平衡情况。在插入$10$之后导致键为$15$的节点$LL$不平衡，需进行左旋调整，如下 此时二叉树按层次遍历输出如下，结果正确1插入10: [20]甲 [12]丁 [25]丙 [10]戊 [15]乙 在插入$24$时，节点$25$形成$LR$不平衡，调整过程如下 层次遍历输出结果正确1插入24: [20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙 全部插入后形成平衡二叉树与搜索二叉树对比如下 删除在上述最终生成的AVL中，删除节点$10$导致节点$12$出现$L-1$不平衡，进行单旋转矫正，层次遍历输出结果正确1[20]甲 [15]乙 [24]庚 [12]丁 [19]辛 [23]己 [25]丙 继续删除$12, 15, 19$导致节点$20$出现$L0$不平衡，进行单旋转矫正，层次遍历输出结果正确1[24]庚 [20]甲 [25]丙 [23]己 重新初始化该树，依次删除$25, 12, 10, 15, 19$导致节点$20$出现$L1$不平衡，进行双旋转矫正，层次遍历输出结果正确1[23]己 [20]甲 [24]庚 主函数与输出12345678910111213141516171819202122232425262728293031323334353637383940414243int main(int argc, char** argv)&#123; string names[8] = &#123; "甲", "乙", "丙", "丁", "戊", "己", "庚", "辛" &#125;; int numbers[8] = &#123; 20, 15, 25, 12, 10, 23, 24, 19 &#125;; AVLTree&lt;int, string&gt; tree; // 插入键值对 cout &lt;&lt; endl &lt;&lt; "插入" &lt;&lt; endl; for (int i = 0; i &lt; 8; i++) &#123; cout &lt;&lt; "插入" &lt;&lt; numbers[i] &lt;&lt; ": "; tree.insert(numbers[i], names[i]); tree.print(); &#125; // 删除键值对 cout &lt;&lt; endl &lt;&lt; "删除" &lt;&lt; endl; tree.erase(10); tree.print(); // 12 L-1不平衡 tree.erase(12); tree.erase(15); tree.erase(19); tree.print(); // L0不平衡 // 插入键值对 cout &lt;&lt; endl &lt;&lt; "重新插入" &lt;&lt; endl; for (int i = 0; i &lt; 8; i++) &#123; tree.insert(numbers[i], names[i]); &#125; tree.print(); tree.erase(25); tree.erase(12); tree.erase(10); tree.erase(15); tree.erase(19); tree.print(); // L1不平衡 system("pause");&#125; 123456789101112131415161718插入插入20: [20]甲插入15: [20]甲 [15]乙插入25: [20]甲 [15]乙 [25]丙插入12: [20]甲 [15]乙 [25]丙 [12]丁插入10: [20]甲 [12]丁 [25]丙 [10]戊 [15]乙插入23: [20]甲 [12]丁 [25]丙 [10]戊 [15]乙 [23]己插入24: [20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙插入19: [20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙 [19]辛删除[20]甲 [15]乙 [24]庚 [12]丁 [19]辛 [23]己 [25]丙[24]庚 [20]甲 [25]丙 [23]己重新插入[20]甲 [12]丁 [24]庚 [10]戊 [15]乙 [23]己 [25]丙 [19]辛[23]己 [20]甲 [24]庚请按任意键继续. . . 红-黑树(RB树)红黑树通过节点颜色的限制，进行整棵树的高度限制。 定义及概念定义：红-黑树(red-black tree)是一棵二叉搜索树，将每个空指针用外部节点来代替，得到扩充二叉树，树中每个节点的颜色是黑色或是红色，如下图，图中外部节点(扩充)为黑色正方形。 红-黑树具有以下性质 根节点、外部节点都是黑色； 根节点$\rightarrow$外部节点路径上，没有连续两个节点是红色； 根节点$\rightarrow$外部节点路径上，黑色节点数目相同。 由2，每个红色节点的子节点必定是黑色节点； 红-黑树的另一种等价，取决于父子节点间指针颜色， 父节点$\rightarrow$黑色孩子指针是黑色的； 父节点$\rightarrow$红色孩子指针是红色的； 内部节点$\rightarrow$外部节点的指针为黑色； 根节点$\rightarrow$外部节点路径上，没有连续两个指针是红色； 根节点$\rightarrow$外部节点路径上，黑色指针数目相同。 定义：某节点的阶(rank)，是从该节点$\rightarrow$外部节点路径上黑色指针的数目(或黑色内部节点个数)，一个外部节点的阶是$0$。 定理1：设根节点$\rightarrow$外部节点的路径长度(length),是为该路径的指针数量(或内部节点个数)，$P, Q$是红-黑树中两条根节点$\rightarrow$外部节点的路径，那么 \rm{length}(P) \leq 2 \times \rm{length}(Q) 根节点$\rightarrow$外部节点路径上，黑色内部节点数目即阶$N_b = r$；且红色节点子节点必定为黑色节点，而黑色节点子节点颜色未定，故红色节点的数目$0 \leq N_r\leq r$，那么 r \leq \rm{length} = N_r + N_b \leq 2 \times r所以对于任意两条根节点$\rightarrow$外部节点的路径$P, Q$，长度都满足 \rm{length}(P) \leq 2 \times \rm{length}(Q) 定理2：令$h$是一棵红-黑树的高度(不包括外部节点)，$n$是内部节点数目，$r$是根节点的阶，那么 $h \leq 2r$； $n \geq 2^r - 1$； $h \leq 2 \times \log_2 (n + 1)$。 $h$即根节点$\rightarrow$外部节点的路径长度 见定理1的证明； 树的第$1 \sim r$层均为内部节点，且数目为$2^r - 1$，那么总内部节点数目$n \geq 2^r - 1$； 联立1，2可得3，略。 算法描述搜索同样的，红-黑树搜索仍可沿用二叉搜索树的搜索方法。 插入首先确定新节点插入时的颜色。若新节点为黑色，插入后必定违反性质3(黑色节点数目约束)；若新节点为红色，可能违反性质2(红色节点不连续约束)，故新节点颜色确定为红。 记红色新节点为$\mathcal{U}$，插入后违反性质2，此时$\mathcal{U}$必存在红色父节点$\mathcal{P}$；由性质1(根节点与外部节点颜色约束)，$\mathcal{U}$必存在黑色祖父节点$\mathcal{G}$。 此时不平衡情况可分为$8$种情况，命名为$XYc$，其中$X, Y$取值${L, R}$表示 $\mathcal{U}$为$\mathcal{G}$的$X$子节点的$Y$子节点；$c$取值${r, b}$，表示$\mathcal{G}$右子节点的颜色。 例如$LLr$表示 $\mathcal{U}$为$\mathcal{G}$左子节点($L$)的左子节点($L$)，且$\mathcal{G}$右子节点$\mathcal{G_R}$为红色($r$)。 根据$\mathcal{G_R}$颜色分为两大类进行树的平衡 $XYr$ 当$\mathcal{G_R}$为红色时，该类不平衡可通过颜色调整来处理。将$\mathcal{P}$与$\mathcal{G_R}$的颜色调整为黑色；若$\mathcal{G}$不是根节点，需调整为红色，并将$\mathcal{G}$作为$\mathcal{U}$向根节点进行判别是否违反性质2。如下图 $XYb$ 当$\mathcal{G_R}$为黑色时，此类不平衡需通过旋转操作进行平衡，操作类似AVL树，除此之外，改变$\mathcal{G}$和$\mathcal{P}$节点的颜色。重新平衡后，该子树从根节点到外部节点的路径上，黑色节点数目不变，且向根节点搜索路径上，不存在连续的红色节点，故不需要继续平衡。 如下图示意，$LLb$不平衡时，将子树$\mathcal{T_G}$左旋，并修改该子树根节点$\mathcal{G}$与不平衡端子树根节点$\mathcal{P}$，$LRb$同理。 删除用常规搜索二叉树的删除算法进行节点$\mathcal{N}$的物理删除，记$\mathcal{N}’$为替代$\mathcal{N}$的节点，当且仅当$\mathcal{N}$为黑色，且$\mathcal{N}’$不是树根节点时，会违反性质3(黑色节点数目约束)；$\mathcal{N}$为红色时，该树维持平衡。 记$\mathcal{N}’$的父节点为$\mathcal{P}$，$\mathcal{N}’$的同胞节点为$\mathcal{B}$，注意$\mathcal{N}’$在替代后，必定为黑色。根据$\mathcal{P, B}$的情况，不平衡情况可分为$4$类，命名为$Xc$，$X$取值为${L, R}$，表示$\mathcal{N}’$是$\mathcal{P}$的左孩或右孩；$c$取值为${r, b}$，表示$\mathcal{B}$的颜色。 $Xb$型 根据$\mathcal{B}$的红色子节点个数，可分为$Xb0, Xb1, Xb2$三种。 $Xb0$ 修改$\mathcal{B}$(黑色)为红色节点$\mathcal{B}’$，$\mathcal{P}$(颜色待定)为黑色节点$\mathcal{P}’$，根节点$\mathcal{R}$至$\mathcal{N}’$的路径上增加了一个黑色节点，解决该路不平衡问题。但是考虑$\mathcal{P}$颜色： 若$\mathcal{P}$为红色节点，而根节点$\mathcal{R}$至$\mathcal{B}$路径上，黑色节点数目不变，保持该路平衡状态； 若$\mathcal{P}$为黑色节点，将使得该路上缺少一个黑色节点，$\mathcal{P}$为根节点时无需改动，否则将$\mathcal{P}$视作新的$\mathcal{N}’$直至根节点为止，继续修改路径上节点的颜色。 $Xb1$与$Xb2$ 又可根据$\mathcal{B}$红色子孩的位置分为$Xb1(i)$和$Xb1(ii)$两类。可通过旋转的方式进行矫正，可以证明旋转后，整棵树保持平衡。 $Xb1(i)$ 将树$\mathcal{T_P}$左旋，并修改$\mathcal{B}’$颜色与$\mathcal{P}$一致，$\mathcal{P}$修改为黑色。 $Xb1(ii)$与$Xb2$ 将树$\mathcal{T_B}$右旋后，将树$\mathcal{T_P}$左旋，并修改$\mathcal{B_R}$颜色与$\mathcal{P}$一致，$\mathcal{P}$修改为黑色，$\mathcal{B_L}$颜色保持不变。 $Xr$型 $Xr$型可根据$\mathcal{B_R}$红色孩子数目，分为$Xr0, Xr1, Xr2$三种类型。与$Xb$型不同，此时$\mathcal{P}$必定为黑色节点。该类型都可通过旋转后保持平衡，无需向根节点搜索不平衡节点， $Xr0$ 将树$\mathcal{T_P}$进行左旋，修改$\mathcal{B}$为黑色，$\mathcal{B_R}$为红色 $Xr1$与$Xr2$ 又可根据$\mathcal{B_R}$红色子孩的位置分为$Xr1(i)$和$Xr1(ii)$两类。 $Xr1(i)$ 将树$\mathcal{T_B}$进行右旋，再将树$\mathcal{T_P}$进行左旋，修改$\mathcal{B_{R_L}}$为黑色 $Xr1(ii)$与$Xr2$ 将树$\mathcal{T_{B_R}}$进行右旋后，再将树$\mathcal{T_B}$进行右旋，再将树$\mathcal{T_P}$进行左旋，修改$\mathcal{B_{R_R}}$为黑色 AVL树与红黑树的对比 AVL树通过子树高度限制进行严格平衡，而红黑树通过节点添加颜色的方式进行平衡，不追求严格的平衡； 在插入与删除操作时，两种维持平衡的方式也不相同，AVL树可通过$1 \sim 2$次旋转使子树获得平衡，RB树通过修改颜色或$1 \sim 3$次旋转来矫正。如下表； AVL树在$L-1, R1, L1, R-1$(删除)时，需要对至根节点路径上的不平衡子树继续矫正；而RB树只有在$XYr$(插入)与$Rb0$(删除)时继续搜索不平衡子树，这两种情况都可以通过修改节点颜色来矫正； 平均情况下，AVL树总的旋转次数比RB树多，RB树统计性能高于AVL。 C++的STL标准库中map采用的时RB树实现。 树 操作 判别 类型 矫正 备注 AVL 插入 查找插入节点N最近的不平衡祖先A(bf=-2/2)；根据N所在A子树的子树位置，分为4种不平衡类型(如N在A左子树的左子树则记作LL) LL A左旋 矫正后子树高度不变，整棵树平衡 RR A右旋 LR AL右旋，A左旋 RL AR左旋，A右旋 删除 查找删除节点N最近的不平衡祖先A(bf=-2/2)；根据N所在A的子树位置，可分为L, R两种；考察A另一棵子树平衡因子，L(R)型又可分为0, 1, -1三种，共2×3=6种类型 L0 A右旋 矫正后子树高度不变，整棵树平衡 R0 A左旋 L-1 A右旋 矫正后子树高度减1，某些祖先节点可能不平衡，需要对在至根节点的路径上的不平衡子树继续矫正 R1 A左旋 L1 AR左旋，A右旋 R-1 AL右旋，A左旋 RB树 插入 插入节点N初始为红，其父节点P为红时不平衡，祖父节点G必为黑色；根据N与G的节点关系(LL, RR, LR, RL)、G的另一孩子颜色(r, b)，可分为4×2=8种类型XYc XYr 修改P与GR为黑色节点，若G不为根节点修改为红否则为黑 G颜色改变，能导致G的父节点不平衡(红-红)，故需要对至根节点的路径上不平衡子树继续矫正 LLb G左旋，修改P为黑，G为红 重新平衡后，该子树从根节点到外部节点的路径上，黑色节点数目不变，且向根节点搜索路径上，不存在连续的红色节点，整棵树平衡 RRb G右旋，修改P为黑，G为红 LRb AL右旋，A左旋，修改U为黑，G为红 RLb AR左旋，A右旋，修改U为黑，G为红 删除 删除节点N(黑色时不平衡)后，由搜索树算法找到替换节点N'(修改为黑)，N'的父节点为P(Xb型时颜色不定，Xr型时必为黑)、同胞节点为B；根据N'与P的位置关系(L, R)、B的颜色(r, b)、B的红色子节点个数(0, 1, 2)、B红色结点数为1时该红色子节点相对于B位置(i&lt;左&gt;, ii&lt;右&gt;)分别矫正；共2×2×(1+2+1)=16种类型 Rb0 修改B为红，P为黑 P为红色时，整棵树平衡；否则向跟节点继续矫正 Rb1(i) P左旋，修改B与P一致，再修改P为黑 矫正后，整棵树平衡 Rb1(ii), Rb2 B右旋，P左旋，修改BR与P一致(BL不变)，再修改P为黑 Rr0 P左旋，修改B为黑，BR为红 矫正后，整棵树平衡 Rr1(i) B右旋，P左旋，修改BRL(红)为黑 Rr1(ii), Rr2 BR右旋，B右旋，P左旋，修改BRR(红，此时为子树跟节点)为黑 Lcn 与之类似，略]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】搜索树]]></title>
    <url>%2F2020%2F03%2F08%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
    <content type="text"><![CDATA[定义与概念考虑用散列表述的字典，字典插入、查找和删除等操作所需要的平均时间为$\Theta(1)$，最坏情况下时间与元素个数呈线性关系。若给字典添加如下操作，那么散列便不再具有良好的平均性能 按关键字的升序输出字典元素； 按升序找到第$k$各元素； 删除第$k$个元素。 定义：二叉搜索树(binary search tree)是一棵二叉树，可能为空；一棵非空的二叉搜索树满足以下特征 每个元素对应一个关键字，且所有关键字都唯一； 根节点的左子树中，元素关键字都小于根节点的关键字； 根节点的右子树中，元素关键字都大于根节点的关键字； 根节点的左、右子树也是二叉搜索树。 由$2, 3$可知，某节点$\mathcal{N}$的键，大于该节点的左子树元素的键，小于该节点的右子树的键； 以节点$\mathcal{N}$为根节点的树的最小值位于最左端的叶子节点，最大值位于最右端的叶子节点； 当节点$\mathcal{N}$的左子树的键最大节点或右子树的键最小节点替换该结点时，该树仍满足二叉搜索树的特性。 若条件$1$弱化，即不同元素可包含相同的关键字，且$2, 3$中小于/大于改为不大于/不小于，这样的树称为有重复值的二叉搜索树(binary search tree with duplicates)。 定义：索引二叉搜索树(indexed binary search tree)源于普通二叉搜索树，是在每个节点中添加$leftSize$域，记录该节点左子树的元素个数。 节点$\mathcal{N}$的$leftSize$与索引$index$比较大小，判断向左或向右搜索； 某节点$\mathcal{N}$的$leftSize$视为该节点在以该节点为根节点的树$\mathcal{T}_{\mathcal{N}}$中的索引，如下图$20$在树中索引为$5$，$15$在左子树中，索引为$1$等，以此类推； 下图中元素按照升序排列时$12, 15, 16, 18, 19, 20, 25, 30$ 抽象数据类型12345678910抽象数据类型 BSTree&#123; 实例: 二叉树，每个节点包含数值对；满足二叉搜索树的特征； 操作： find(k)：返回关键字为k的数对 insert(p)：插入数对p erase(k)：删除关键字为k的数对 ascend()：按关键字升序输出所有数对&#125; 索引二叉搜索树支持二叉搜索树的所有操作，另外它还支持按名字进行的查找和删除操作 1234567891011抽象数据类型 IndexedBSTree&#123; 实例: 二叉树，每个节点包含数值对；满足二叉搜索树的特征； 操作： find(k)：返回关键字为k的数对 get(index)：返回第index个数对 insert(p)：插入数对p erase(k)：删除关键字为k的数对 ascend()：按关键字升序输出所有数对&#125; 具体实现二叉搜索树声明1234567891011121314151617181920212223template&lt;typename K, typename V&gt;class BinarySearchTree : public LinkedBinaryTree&lt;Pair&lt;K, V&gt;*&gt;&#123;public: BinarySearchTree() : LinkedBinaryTree&lt;Pair&lt;K, V&gt;*&gt;() &#123;&#125; ~BinarySearchTree() &#123;&#125; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* find(const K&amp;, bool insert=false); V&amp; get(const K&amp;); void insert(const K&amp;, const V&amp;); void erase(const K&amp;); void print(); // 注意 // - 不能声明为const，否则无法调用非const的printPairNode； // - 类的静态成员无法声明为const。 void ascending();protected: static void printPairNode(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*); static BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* max(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*); static BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* min(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;*);&#125;; 定义 查找 根据搜索二叉树特性，当搜索到达节点$\mathcal{N}$时，若查找值小于当前节点的键，则向其左子树搜索，否则向右子树搜索，直至找到对应键(找到)，或直至叶子节点(未找到)；在插入模式(bool insert=true)下查找时，若不存在则创建新节点并返回。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263template&lt;typename K, typename V&gt;BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* BinarySearchTree&lt;K, V&gt;::find(const K&amp; key, bool insert)&#123; // 若树为空 if (!this-&gt;m_tnRoot) &#123; if (!insert) return nullptr; Pair&lt;K, V&gt;* p = new Pair&lt;K, V&gt;; this-&gt;m_tnRoot = new BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;; this-&gt;m_tnRoot-&gt;set(p); return this-&gt;m_tnRoot; &#125; // 树不为空 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = this-&gt;m_tnRoot; bool toLeft = false; while (node) &#123; // 找到对应键的数值对 if (node-&gt;get()-&gt;getKey() == key) return node; // 当前节点的键比搜查键更大，则向左子树搜索 else if (node-&gt;get()-&gt;getKey() &gt; key) &#123; if (!node-&gt;left) &#123; // 左子树为空 toLeft = true; break; &#125; node = node-&gt;left; &#125; // 当前节点的键比搜查键更小，则向右子树搜索 else &#123; if (!node-&gt;right) &#123; // 右子树为空 toLeft = false; break; &#125; node = node-&gt;right; &#125; &#125; // 未找到 if (!insert) return nullptr; // 创建新节点n Pair&lt;K, V&gt;* p = new Pair&lt;K, V&gt;; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* n = \ new BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;(node); n-&gt;set(p); // 将节点加入搜索树 if (toLeft) node-&gt;left = n; else node-&gt;right = n; return n;&#125;template&lt;typename K, typename V&gt;V&amp; BinarySearchTree&lt;K, V&gt;::get(const K&amp; key)&#123; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = find(key, false); if (!node) return 0; return node-&gt;get()-&gt;getVal();&#125; 插入 调用函数find(插入模式)查找结果，设置搜查到的节点的数值对即可。 123456789template&lt;typename K, typename V&gt;void BinarySearchTree&lt;K, V&gt;::insert(const K&amp; key, const V&amp; value)&#123; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = find(key, true); Pair&lt;K, V&gt;* p = node-&gt;get(); if (p-&gt;getKey() != key) p-&gt;setKey(key); p-&gt;setVal(value);&#125; 删除 删除结点时，需考虑剩余节点的重组织问题，与最大堆不同，二叉搜索树具有良好的结构特性，故删除某节点$\mathcal{N}$时，只需将其左子树元素中键最大的，或右子树中元素键最小的与该节点位置替换即可。考虑到节点间拓扑关系修改复杂，以下仅修改节点$\mathcal{N}$的键值对，且继续搜索直至找到叶子节点作为替换节点并释放节点内存。 例如删除键为$20$的节点$\mathcal{N}_{20}$时，查找左子树中键最大的节点$\mathcal{N}_{19}$，或右子树中键最小的节点$\mathcal{N}_{25}$(由于$\mathcal{N}_{25}$非叶子节点，需查找$\mathcal{N}_{25}$为根节点的树$\mathcal{T}_{\mathcal{N}_{25}}$中最大左子树节点或最小右子树节点进行替换)，代替$\mathcal{N}_{20}$，如下图 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455template&lt;typename K, typename V&gt;void BinarySearchTree&lt;K, V&gt;::erase(const K&amp; key)&#123; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = find(key, false); if (!node) return; // ------------------ 重新组织二叉树 ------------------ // 查找左子树的最大值，或右子树的最小值 BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* replace = nullptr; while (!node-&gt;isLeaf()) &#123; // 直到搜索到叶节点为止 if (node-&gt;left) &#123; replace = max(node-&gt;left); &#125; else if (node-&gt;right) &#123; replace = min(node-&gt;right); &#125; if (replace) &#123; // 找到可替换子节点 Pair&lt;K, V&gt;* np = node-&gt;get(); Pair&lt;K, V&gt;* rp = replace-&gt;get(); np-&gt;setKey(rp-&gt;getKey()); np-&gt;setVal(rp-&gt;getVal()); node = replace; // 替换节点node &#125; &#125; if (node-&gt;parent) &#123; // 修改父节点信息 if (node == node-&gt;parent-&gt;left) node-&gt;parent-&gt;left = nullptr; else node-&gt;parent-&gt;right = nullptr; &#125; else &#123; // 无父节点，即整棵树只有一个根节点，则修改根节点为空 this-&gt;m_tnRoot = nullptr; &#125; // ------------------ 释放资源 ------------------ Pair&lt;K, V&gt;* pair = node-&gt;get(); delete pair; delete node;&#125;template&lt;typename K, typename V&gt;BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* BinarySearchTree&lt;K, V&gt;::max(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)&#123; while (node-&gt;right) node = node-&gt;right; return node;&#125;template&lt;typename K, typename V&gt;BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* BinarySearchTree&lt;K, V&gt;::min(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)&#123; while (node-&gt;left) node = node-&gt;left; return node;&#125; 升序打印 将元素按中序遍历进行打印即可 123456template&lt;typename K, typename V&gt;void BinarySearchTree&lt;K, V&gt;::ascending() &#123; this-&gt;inOrder(&amp;printPairNode, this-&gt;m_tnRoot); std::cout &lt;&lt; std::endl;&#125; print实现将元素按层次遍历的顺序打印 1234567891011121314template&lt;typename K, typename V&gt;void BinarySearchTree&lt;K, V&gt;::print()&#123; this-&gt;levelOrder(&amp;printPairNode, this-&gt;m_tnRoot); std::cout &lt;&lt; std::endl;&#125;template&lt;typename K, typename V&gt;void BinarySearchTree&lt;K, V&gt;::printPairNode(BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node)&#123; if (!node) return; Pair&lt;K, V&gt;* p = node-&gt;get(); std::cout &lt;&lt; '[' &lt;&lt; p-&gt;getKey() &lt;&lt; ']' &lt;&lt; p-&gt;getVal() &lt;&lt; ' ';&#125; 测试123456789101112131415161718192021222324252627282930int main(int argc, char** argv)&#123; string names[8] = &#123; "甲", "乙", "丙", "丁", "戊", "己", "庚", "辛" &#125;; int numbers[8] = &#123; 20, 15, 25, 12, 18, 30, 16, 19 &#125;; BinarySearchTree&lt;int, string&gt; tree; // 插入键值对 cout &lt;&lt; endl &lt;&lt; "插入" &lt;&lt; endl; for (int i = 0; i &lt; 8; i++) &#123; cout &lt;&lt; "插入" &lt;&lt; numbers[i] &lt;&lt; ": "; tree.insert(numbers[i], names[i]); tree.print(); &#125; cout &lt;&lt; "升序排列: "; tree.ascending(); // 删除键值对 cout &lt;&lt; endl &lt;&lt; "删除" &lt;&lt; endl; tree.erase(10); for (int i = 0; i &lt; 8; i++) &#123; cout &lt;&lt; "删除" &lt;&lt; numbers[i] &lt;&lt; ": "; tree.erase(numbers[i]); tree.print(); &#125; system("pause");&#125; 123456789101112131415161718192021插入插入20: [20]甲插入15: [20]甲 [15]乙插入25: [20]甲 [15]乙 [25]丙插入12: [20]甲 [15]乙 [25]丙 [12]丁插入18: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊插入30: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己插入16: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚插入19: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚 [19]辛升序排列: [12]丁 [15]乙 [16]庚 [18]戊 [19]辛 [20]甲 [25]丙 [30]己删除删除20: [19]辛 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚删除15: [19]辛 [12]丁 [25]丙 [18]戊 [30]己 [16]庚删除25: [19]辛 [12]丁 [30]己 [18]戊 [16]庚删除12: [19]辛 [16]庚 [30]己 [18]戊删除18: [19]辛 [16]庚 [30]己删除30: [19]辛 [16]庚删除16: [19]辛删除19:请按任意键继续. . . 删除时节点的重组织示意图如下 索引二叉搜索树声明由BinarySearchTree&lt;K, V&gt;派生而来，添加操作符[]的重载 12345678910template&lt;typename K, typename V&gt;class IndexedBinarySearchTree : public BinarySearchTree&lt;K, V&gt;&#123;public: IndexedBinarySearchTree() : BinarySearchTree&lt;K, V&gt;() &#123;&#125; ~IndexedBinarySearchTree() &#123;&#125; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* operator[] (int index);&#125;; 定义初始化偏置$offset = 0$，在向右搜索时，需更新偏置$offset$使右子树中各节点的$leafSize$仍能用于索引 1234567891011121314151617181920212223template&lt;typename K, typename V&gt;BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* IndexedBinarySearchTree&lt;K, V&gt;::operator[] (int index)&#123; if (index &lt; 0) throw "索引必须大于0！"; int leftSize = -1; int offset = 0; BinaryTreeNode&lt; Pair&lt;K, V&gt;*&gt;* node = this-&gt;m_tnRoot; while (leftSize != index &amp;&amp; node) &#123; leftSize = IndexedBinarySearchTree&lt;K, V&gt;::sizeofNode(node-&gt;left); if (offset + leftSize == index) &#123; IndexedBinarySearchTree&lt;K, V&gt;::printPairNode(node); return node; &#125; else if (offset + leftSize &gt; index) node = node-&gt;left; else &#123; node = node-&gt;right; offset += (leftSize + 1); &#125; &#125;&#125; 测试123456789101112131415161718192021222324252627int main(int argc, char** argv)&#123; string names[8] = &#123; "甲", "乙", "丙", "丁", "戊", "己", "庚", "辛" &#125;; int numbers[8] = &#123; 20, 15, 25, 12, 18, 30, 16, 19 &#125;; IndexedBinarySearchTree&lt;int, string&gt; tree; // 插入键值对 cout &lt;&lt; endl &lt;&lt; "插入" &lt;&lt; endl; for (int i = 0; i &lt; 8; i++) &#123; cout &lt;&lt; "插入" &lt;&lt; numbers[i] &lt;&lt; ": "; tree.insert(numbers[i], names[i]); tree.print(); &#125; cout &lt;&lt; "升序排列: "; tree.ascending(); cout &lt;&lt; endl &lt;&lt; "索引" &lt;&lt; endl; for (int i = 0; i &lt; 8; i++) &#123; tree[i]; &#125; cout &lt;&lt; endl; system("pause");&#125; 以下图为例，讲解索引$index=2$的过程 初始化$offset = 0$； $index = 2$时，首先在节点$\mathcal{N}_{20}$比较$2 + offset &lt; 5$，向左搜索； 在节点$\mathcal{N}_{15}$处$2 + offset&gt; 1$，向右搜索，并更新$offset = offset + \mathcal{N}_{15}.leafSize + 1 = 2$； 节点$\mathcal{N}_{18}$处$2 &lt; 1 + offset$，向左搜索；在节点$\mathcal{N}_{16}$处$2 = 0 + offset$，返回该节点。 123456789101112131415插入插入20: [20]甲插入15: [20]甲 [15]乙插入25: [20]甲 [15]乙 [25]丙插入12: [20]甲 [15]乙 [25]丙 [12]丁插入18: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊插入30: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己插入16: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚插入19: [20]甲 [15]乙 [25]丙 [12]丁 [18]戊 [30]己 [16]庚 [19]辛升序排列: [12]丁 [15]乙 [16]庚 [18]戊 [19]辛 [20]甲 [25]丙 [30]己索引[12]丁 [15]乙 [16]庚 [18]戊 [19]辛 [20]甲 [25]丙 [30]己请按任意键继续. . .]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】竞赛树]]></title>
    <url>%2F2020%2F03%2F06%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%AB%9E%E8%B5%9B%E6%A0%91%2F</url>
    <content type="text"><![CDATA[定义及概念定义：竞赛树(tournament tree)也是完全二叉树，它的基本操作是替换最大(或最小)元素。如果有$n$个元素，这个基本操作的用时为$\Theta(\log n)$。 竞赛树可分为赢者树(winner tree)和输者树(loser tree)，每个内部节点分别记录比赛的赢者和输者。在最小赢者树(min winner tree)中，分数小的选手获胜，分数相等则左孩子获胜，最大赢者树(max winner tree)反之。 定义：有$n$个选手的赢者树是一棵完全二叉树，它有$n$个外部节点和$n-1$个内部节点，每个内部节点记录的是在该节点比赛的赢者。它的优点是，当一名选手分数改变时，修改竞赛树比较容易，由底至顶重新对该选手进行比赛即可，需要修改的比赛场次数介于$1 \sim \lceil \log_2 n \rceil$之间。 具体实现可用数组二叉树表示，存储效率最高。为了便于计算机实现，把赢者树限制为完全二叉树。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768template&lt;typename T&gt;class MaxWinnerTree : public ArrayBinaryTree&lt;T&gt;&#123;public: MaxWinnerTree() : ArrayBinaryTree&lt;T&gt;() &#123;&#125; MaxWinnerTree(T* set, int n); ~MaxWinnerTree() &#123;&#125; void set(const int i, const T&amp; value) &#123; this-&gt;m_TElements[i + offset()] = value; &#125; // 设置选手i的值 T&amp; get(const int i) const &#123; return this-&gt;m_TElements[i + offset()]; &#125; // 获取选手i的值 int index(const T&amp; value) const; // 查询选手索引 void play(const int i = -1); // 在忽略选手i的情况下比赛，即始终被视作输者 void replay(const int i); // 在i的值修改后，对i一系列进行重赛 T&amp; winner() const &#123; return this-&gt;m_TElements[0]; &#125; // 返回胜者protected: int offset() const &#123; return (this-&gt;m_iCount - 1) / 2; &#125;&#125;;template&lt;typename T&gt;MaxWinnerTree&lt;T&gt;::MaxWinnerTree(T* set, int n) &#123; int size = 2 * n - 1; // 包含n-1个内部节点与n个外部节点 this-&gt;m_TElements = new T[size]; std::copy(set, set + n, this-&gt;m_TElements + n - 1); this-&gt;m_iCount = this-&gt;m_iSize = size; play();&#125;template&lt;typename T&gt;void MaxWinnerTree&lt;T&gt;::play(const int i) &#123; // 从最后一个内部节点开始竞赛 for (int index = this-&gt;parent(this-&gt;m_iCount - 1); index &gt;= 0; index--) &#123; int left = this-&gt;left(index); int right = this-&gt;right(index); // 包含忽略的选手 if (left - offset() == i &amp;&amp; i &gt;= 0) this-&gt;m_TElements[index] = this-&gt;m_TElements[right]; else if (right - offset() == i &amp;&amp; i &gt;= 0) this-&gt;m_TElements[index] = this-&gt;m_TElements[left]; // 所有选手参加比赛 else this-&gt;m_TElements[index] = this-&gt;m_TElements[left] &gt; this-&gt;m_TElements[right] ? \ this-&gt;m_TElements[left]: this-&gt;m_TElements[right]; &#125;&#125;template&lt;typename T&gt;void MaxWinnerTree&lt;T&gt;::replay(const int i) &#123; // 对选手i进行重赛 int index = this-&gt;parent(i + offset()); // 寻找父节点 while (true) &#123; index = this-&gt;parent(index); int left = this-&gt;left(index); int right = this-&gt;right(index); this-&gt;m_TElements[index] = this-&gt;m_TElements[left] &gt;= this-&gt;m_TElements[right] ? \ this-&gt;m_TElements[left]: this-&gt;m_TElements[right]; if (index == 0) break; &#125;&#125;template&lt;typename T&gt;int MaxWinnerTree&lt;T&gt;::index(const T&amp; value) const&#123; T* start = this-&gt;m_TElements + offset(); T* end = this-&gt;m_TElements + this-&gt;m_iCount; return (int)(std::find(start, end, value) - start);&#125; 在完全二叉树的假设下，上述实现可处理奇数个或偶数个参赛选手的情况，测试如下123456789101112131415161718192021222324int main(int argc, char** argv)&#123; int symbols[32] = &#123; 3, 5, 6, 7, 20, 8, 2, 9, 12, 15, 30, 17&#125;; MaxWinnerTree&lt;int&gt; tree(symbols, 12); cout &lt;&lt; "size: " &lt;&lt; tree.size() &lt;&lt; endl \ &lt;&lt; "height: " &lt;&lt; tree.height() &lt;&lt; endl &lt;&lt; endl; tree.print(); cout &lt;&lt; "Winner: " &lt;&lt; tree.winner() &lt;&lt; endl &lt;&lt; endl; tree.play(10); // 忽略30重赛 tree.print(); cout &lt;&lt; "Winner: " &lt;&lt; tree.winner() &lt;&lt; endl &lt;&lt; endl; tree.set(10, 2); // 修改 30 为 2 tree.replay(10); // 重赛 tree.print(); system("pause");&#125; 1234567891011121314151617181920212223size: 23height: 53030 720 30 5 720 9 15 30 3 5 6 720 8 2 9 12 15 30 17Winner: 302020 720 17 5 720 9 15 17 3 5 6 720 8 2 9 12 15 30 17Winner: 202020 720 17 5 720 9 15 17 3 5 6 720 8 2 9 12 15 2 17请按任意键继续. . . 应用：箱子装载问题问题描述箱子数量不限，每个箱子的容量为$C_i$，待装箱的物体有$n$个，物品$j$的需要占用$c_j, 0 \leq c_j \leq C_i$。可行装载(feasible packing)是指所有物品均装入箱子且不溢出，最优装载(optimal packing)是指使用箱子最少的可行装载。 近似算法箱子装载问题是NP-hard问题，常用近似算法求解，求取次优解。有以下几种常用的算法 最先适配法(First Fit, FF)：物品与箱子均依次排列，第$j$个物品放入最左边的可装载箱子； 最优适配法(Best Fit, BF)：第$j$个物品放入剩余可用容量$\overline{C}_i$最小但不小于$c_j$的箱子； 最先适配递减法(First Fit Decreasing, FFD)：类似FF，区别是将物品按所需容量递减排列，即$c_j \geq c_{j+1}$； 最优适配递减法(Best Fit Decreasing, BFD)：类似BF，区别是将物品按所需容量递减排列，即$c_j \geq c_{j+1}$。 定理：设$I$为箱子装载问题的任一实例，$b(I)$为最优装载所用的箱子数。那么，FF和BF所用箱子不会超过$\frac{17}{10}b(I) + 2$，FFD和BFD所用箱子不会超过$\frac{11}{9}b(I) + 4$。 用竞赛数实现FF适配法，可由根节点向叶节点搜索，减少时间复杂度。用竞赛数实现BF适配法： 指定箱子数n_bins目与箱子容量capacity，初始化竞赛树所需内存空间与尺寸信息等； 结果保存为result，保存各个物体所选箱子的索引(0, …, n_objs)； 输入各物品的所需容量信息objs进行求解； 优先查找当前使用容量最大的箱子，也即剩余容量最小的箱子，若该箱子能放下物品，则继续下一物品的箱子选择；否则进入2； 将不符合要求的箱子已使用容量置为-1，使其在竞赛数中始终无法获胜，重新比赛查找当前使用容量最大的箱子； 循环2直至找到箱子，继续下一物品的箱子选择。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class BinPackingBF : public MaxWinnerTree&lt;int&gt;&#123;public: BinPackingBF(int n = 8, int capacity = 10); // 箱子数与箱子容量 ~BinPackingBF() &#123; if (result) delete result; &#125; void solve(int* objs, int n); // 求解 void print() const;protected: int n_bins; int capacity; int* result; int n_objs; // 保存每个箱子所在的箱子索引&#125;;BinPackingBF::BinPackingBF(int n, int capacity)&#123; int size = 2 * n - 1; this-&gt;m_TElements = new int[size]; this-&gt;m_iCount = this-&gt;m_iSize = size; this-&gt;n_bins = n; this-&gt;capacity = capacity;&#125;void BinPackingBF::solve(int* objs, int n)&#123; result = new int[n]; n_objs = n; // 保存每次物品放置前的箱子使用情况 int* bins = new int[n_bins]; std::memset(bins, 0, sizeof(int) * n_bins); for (int i = 0; i &lt; n; i++) &#123; if (objs[n] &gt; capacity) throw "物体太大！"; // 将当前箱子使用情况赋值给外部节点 memcpy(this-&gt;m_TElements + this-&gt;offset(), bins, sizeof(int) * n_bins); // 查找当前剩余容量最小的箱子 this-&gt;play(); int used = this-&gt;winner(); // 当前已使用容量最大的箱子容量 int index = this-&gt;index(used); // 当前已使用容量最大的箱子索引 if (capacity - used &gt;= objs[i]) &#123; // 剩余容量大于物品大小 result[i] = index; bins[index] += objs[i]; continue; &#125; // 当前剩余容量最小的箱子无法装入物品i int choosen = -1; while (choosen &lt; 0) &#123; this-&gt;m_TElements[index + offset()] = -1; // 该箱子始终无法获胜 this-&gt;play(); // 重新竞赛 used = this-&gt;winner(); // 当前已使用容量最大的箱子容量 index = this-&gt;index(used); // 当前已使用容量最大的箱子索引 if (capacity - used &gt;= objs[i]) choosen = index; &#125; result[i] = choosen; bins[choosen] += objs[i]; &#125; delete[] bins;&#125;void BinPackingBF::print() const&#123; for (int i = 0; i &lt; n_objs; i++) std::cout &lt;&lt; result[i] &lt;&lt; ' '; std::cout &lt;&lt; std::endl;&#125; 主函数测试如下1234567891011int main(int argc, char** argv)&#123; BinPackingBF puzzle(8, 10); int objs[8] = &#123; 3, 5, 2, 4, 2, 5, 4, 8&#125;; puzzle.solve(objs, 8); puzzle.print(); system("pause");&#125; 120 0 0 1 1 2 1 3请按任意键继续. . . 以上输出表示 箱子索引 0 1 2 3 … 箱子物品所需容量 3, 5, 2 4, 2, 4 5 8 / 将箱子排序后输入，即为BFD适配法：12345678910111213iint main(int argc, char** argv)&#123; BinPackingBF puzzle(8, 10); int objs[8] = &#123; 3, 5, 2, 4, 2, 5, 4, 8 &#125;; sort(objs, objs + 8, [](int x, int y) &#123; return x &gt; y; &#125;); puzzle.solve(objs, 8); puzzle.print(); system("pause");&#125; 120 1 1 2 2 3 0 2请按任意键继续. . . 箱子索引 0 1 2 3 … 箱子物品所需容量 8, 2 5, 5 4, 4, 2 3 /]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】优先级队列]]></title>
    <url>%2F2020%2F03%2F03%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%BC%98%E5%85%88%E7%BA%A7%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[定义及抽象数据类型优先级队列(priority queue)是$0$个或多个元素的集合，每个元素都有一个优先权或值，在最小优先级队列(min priority queue)中，对其进行的元素操作都是当前优先级最小的，最大优先级队列(max priority queue)则相反。优先级队列的元素可以有相同的优先级，对这些元素其顺序任意。优先级队列的操作有 查找一个元素； 插入一个元素； 删除一个元素。 1234567891011抽象数据类型 PriorityQueue&#123;实例：操作： empty(): 队列是否为空 size(): 队列的元素个数 top(): 返回当前优先级最大/最小的元素 push(x, p): 插入元素，根据优先级调整队列顺序 pop(): 删除并返回当前优先级最大/最小的元素&#125; 大根堆/小根堆定义定义：大根树/小根树是指每个节点的值都大于其子节点的值的树，节点的子节点个数可以任意。定义：大根堆/小根堆即是大根树/小根数，也是完全二叉树。 利用二叉树的特性，用节点在数组描述中的位置来表示它在堆中的位置。堆作为完全二叉树，具有$n$个元素时高度为$\lceil\log_2(n+1)\rceil$。因此如果能够在$O(height)$时间内完成插入和删除操作，那么操作的复杂度为$O(\log n)$。 实现数组描述用数组描述大根堆/小根堆更为简便，由于其为完全二叉树，可以将元素存储在连续内存中，并用数组下标作为节点索引。主要实现的有以下几个关键步骤 获取某节点的父节点、子节点； 对于索引为$i$的节点，其父节点为$\lfloor(i - 1) / 2\rfloor$，左右子节点分别为$2 \times i + 1, 2 \times i + 2$。注意索引数组时不能越界。 某节点“上浮”、“下沉”； 上浮操作与其父节点进行比较，若满足交换条件则交换；下沉操作需先选择子节点中更大/更小的元素，若满足交换条件则交换。 堆的初始化、入堆、出堆 初始化：从最后一个非叶子节点(也即最后一个叶节点的父节点)开始，直至根节点依次进行“下沉”操作； 入堆：将节点添加至数组末尾作为最后一个叶节点，进行递归的“上浮”操作直至到达合适位置； 出堆：将根节点(首元素)与最后一个叶节点(末元素)交换，删除此时的末尾元素，对当前根节点进行递归的“下沉”操作直至到达合适位置。 在自底向上的建堆过程中，从$i = heapsize / 2$开始，有$1/2$的节点向下比较了$1$次，$1/4$的节点向下比较了$2$次，……，$1/2^k$的节点向下比较了$k$次，其中$1/2^k \leq 1, k \approx \log n$，那么总的比较次数如下，所以建堆的时间复杂度是$O(n)$。 T = n \times \sum_{k=1}^{\log n} k /2^k \leq 2n在一次数据插入/删除中，对树型结构进行调整，时间复杂度是$O(\log n)$。 以下为Python实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class _Container: _data = list() def __len__(self): return len(self._data)class Heap(_Container): def __init__(self, cmp=lambda x, y: x &lt; y): self._cmp = cmp # -------------------------- def _valid(self, index): return index &lt; len(self) and index &gt;= 0 def _parent(self, index): return (index - 1) // 2 def _left(self, index): return 2 * index + 1 def _right(self, index): return 2 * index + 2 # -------------------------- def _toTop(self, index): parent = self._parent(index) if not self._valid(parent): return None if self._cmp(self._data[parent], self._data[index]): self._data[parent], self._data[index] = \ self._data[index], self._data[parent] return parent return None def _toBottom(self, index): left, right = self._left(index), self._right(index) validL, validR = self._valid(left), self._valid(right) # child child = -1 if validL and validR: child = right if self._cmp( self._data[left], self._data[right]) else left elif (not validL) and validR: child = right elif validL and not validR: child = left if child == -1: return None # swap if self._cmp(self._data[index], self._data[child]): self._data[index], self._data[child] = \ self._data[child], self._data[index] return child return None def _recursively(self, index, adjust): while True: index = adjust(index) if index is None: return # -------------------------- def heapify(self, nums): self._data = nums for i in range(self._parent(len(self) - 1), -1, -1): self._recursively(i, self._toBottom) def push(self, x): index = len(self) self._data.insert(index, x) self._recursively(index, self._toTop) def pop(self): if len(self) == 0: return self._data[0], self._data[-1] = \ self._data[-1], self._data[0] self._data.pop(-1) self._recursively(0, self._toBottom) def top(self): if len(self) == 0: return None return self._data[0] STL库中已经包含堆的实现，有以下两种方法 用vector数组模拟堆，实现原理同上 以建立大根堆为例，指定比较函数为less&lt;int&gt;()，也可自定义 12345678910111213141516171819202122232425262728293031#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;bool _cmp(const int&amp; a, const int&amp; b)&#123; return a &lt; b;&#125;int main()&#123; vector&lt;int&gt; maxheap = &#123; 4, 7, 2, 1, 5, 3, 8, 6 &#125;; make_heap(maxheap.begin(), maxheap.end(), less&lt;int&gt;()); // make_heap(maxheap.begin(), maxheap.end(), _cmp); // 向堆中插入数据 maxheap.push_back(9); // 添加至容器尾部 // 调整尾部数据的位置 push_heap(maxheap.begin(), maxheap.end(), less&lt;int&gt;()); // 从堆中获取最大元素 cout &lt;&lt; maxheap[0] &lt;&lt; endl; // 删除最大元素 // 与尾部数据交换，并维护前`n-1`个数组成的大根堆 pop_heap(maxheap.begin(), maxheap.end(), less&lt;int&gt;()); maxheap.pop_back(); // 删除容器尾部元素 return 0;&#125; 用内置的priority_queue定义大根堆/小根堆 12345678910111213141516171819202122232425262728293031323334353637#include &lt;vector&gt;#include &lt;queue&gt;using namespace std;class _cmp&#123;public: bool operator()(const int&amp; a, const int&amp; b) &#123; return a &lt; b; &#125;&#125;;int main()&#123; // 参数分别表示`_Ty, _Container, _Pr`，以下几种定义等价 //priority_queue&lt;int&gt; maxheap; //priority_queue&lt;int, vector&lt;int&gt;, _cmp&gt; maxheap; priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt; maxheap; vector&lt;int&gt; nums = &#123; 4, 7, 2, 1, 5, 3, 8, 6 &#125;; // 插入元素 for (int num : nums) &#123; maxheap.push(num); &#125; // 是否为空 bool isEmpty = maxheap.empty(); // 获取元素个数 int n = maxheap.size(); // 获取最大元素 cout &lt;&lt; maxheap.top() &lt;&lt; endl; // 删除最大元素 maxheap.pop(); return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】二叉树及其他树]]></title>
    <url>%2F2020%2F02%2F29%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E4%BA%8C%E5%8F%89%E6%A0%91%E5%8F%8A%E5%85%B6%E4%BB%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[树定义：一棵树$t$是一个非空的有限元素的集合，其中一个元素为根(root)，其余元素组成$t$的子树。根据树的形态，有如下术语：孩子(child)、父母(parent)、兄弟(sibling)、孙子(grandchild)、祖父(grandparent)、祖先(ancestor)、后代(descendent)等等。树中没有孩子的元素称作叶子(leaf)。 树的另一常用术语为级(level)。树根为$1$级，其孩子为$2$级，以此类推。一棵树的高度(height)或深度(depth)是树的级个数。一个元素的度(degree of an element)是指其孩子的个数，叶节点的度为$0$。一棵树的度(degree of a tree)是其元素的度的最大值。 二叉树定义及概念定义：二叉树(binary tree)是树的一种，其中一个元素为根，其余元素被划分为两棵二叉树，分别称左子树和右子树。二叉树和树的根本区别是： 二叉树每个元素恰好有两棵子树(其中存在空树)，而树的元素孩子数目不定； 二叉树的子树是有序的； 二叉树可以为空，而树不可以。 特性特性(1)：一棵二叉树有$n(n&gt;0)$个元素，他有$(n-1)$条边。 除根节点外，其余元素有且仅有一个父节点，其间对应一条边。 特性(2)：一棵二叉树中，度为$2$的节点数，为度为$0$的节点(叶子节点)数目减$1$。 例：已知二叉树中有45个叶节点，有25个度为1的节点，则二叉树的总结点数为____。解析：二叉树中包含度为$0, 1, 2$的节点，叶子节点即度为$0$的节点，那么$(45 + 25 + (45 - 1) = 114)$。 特性(3)：一棵二叉树的高度为$h(h \geq 0)$，它最少有$h$个元素，最多有$2^h-1$个元素 二叉树的第$1$级包含$1$个元素； 第$i(i \geq 2)$级最少有$1$个元素，$\Sigma_{i=1}^h 1 = h$； 第$i(i \geq 2)$级最多有$2^{(i-1)}$个元素，则$\Sigma_{i=2}^h 2^{(i-1)} + 1 = 2^h - 1$； 特性(4)：一棵二叉树有$n(n&gt;0)$个元素，它的最大高度为$n$，最小高度为$\log_2(n+1)$。 定义(满二叉树)：高度为$h$的二叉树恰好有$2^h-1$个元素称为满二叉树(full binary tree)。 满二叉树有$2^(h-1)$个叶子节点，那么内部节点有$2^h - 1 - 2^(h-1) = 2^(h-1) - 1$。 定义(完全二叉树)：高度为$h$的满二叉树，其元素从第$1$级到第$h$级，每一级从左至右按顺序从$1$到$2^{h-1}$编号，并且删除$k$个编号为$(2^h-i)$的元素，其中$1 \leq i \leq k &lt; 2^h$，剩余元素组成的树称为完全二叉树(complete binary tree)。 特性： 设完全二叉树的一元素其编号为$i$，$1 \leq i \leq n$，则1) 若$i=1$，该元素为根；否则其父节点编号为$\lfloor n/2 \rfloor$；2) 若$2i &gt; n$，则该元素无左孩子；否则其左孩子的编号为$2i$；3) 若$2i+1 &gt; n$，则该元素无右孩子；否则其右孩子的编号为$2i+1$； 描述数组描述用数组描述二叉树时，将二叉树表示为缺少了部分元素的完全二叉树，利用完全二叉树的特性对孩子进行索引。当根节点意外的每个节点都是其父节点的右孩子时，所需存储空间最大，称为右斜二叉树(right-skewed binary tree)。 不完全二叉树 右斜二叉树 链表描述用链表描述二叉树时，除元素数据存储外，每个元素描述为包含两个子节点leftChild与rightChild的节点，若某孩子节点为空则置为NULL。 不完全二叉树 右斜二叉树 抽象数据描述12345678910111213抽象数据类型 binaryTree&#123;实例 元素集合；左子树、右子树；操作 empty() 若树为空返回true否则false size() 返回二叉树的节点&#x2F;元素个数 preOrder(visit) 前序遍历，visit为二叉树节点操作函数 inOrder(visit) 中序遍历 postOrder(visit) 后序遍历 levelOrder(visit) 层次遍历 ...&#125; 具体实现链表描述子节点的声明与定义首先，定义二叉树节点如下 包含三个BinaryTreeNode*类型的公有成员指针变量，保存结点地址信息：父节点、左子树节点、右子树节点； 私有成员指针变量T*存放节点元素值； 添加成员函数isRoot()与isLeaf()，用于返回节点部分信息。 123456789101112131415161718192021template&lt;typename T&gt;class BinaryTreeNode&#123; // 左右子树的维护，放在二叉树`LinkedBinaryTree`中处理public: BinaryTreeNode(BinaryTreeNode&lt;T&gt;* p = nullptr) : parent(p) &#123; left = right = nullptr; value = new T; &#125; ~BinaryTreeNode() &#123; delete value; &#125; bool isRoot() const &#123; return !parent; &#125; // 是否为根节点 bool isLeaf() const &#123; return (!left) &amp;&amp; (!right); &#125; // 是否为叶子节点 T get() const &#123; return *value; &#125; void set(const T&amp; v) &#123; *value = v; &#125; BinaryTreeNode* parent; // 增加父母节点信息 BinaryTreeNode* left, *right;private: T* value;&#125;; 树的声明以下采用链表描述实现二叉树 私有指针变量m_tnRoot存放根节点的地址； 重载构造函数，进行根节点的初始化或树的创建；析构函数中做节点内存释放的清理工作； 共有成员函数size(),height(),empty(),print()用以获取树的信息； [*]Order以不同的遍历顺序访问和修改节点； 私有静态成员函数[*]Node为单个节点的访问和修改函数，供[*]Order调用； 12345678910111213141516171819202122232425262728293031323334353637template&lt;typename T&gt;class LinkedBinaryTree&#123;public: LinkedBinaryTree(); LinkedBinaryTree(T*, int); // 由给定集合生成二叉树，空叶节点用`#`表示，形如`&#123;3,9,20,#,#,15,7&#125;` LinkedBinaryTree(const LinkedBinaryTree&lt;T&gt;&amp;); ~LinkedBinaryTree(); int size(); // 二叉树结点数 int height(); // 二叉树高度 bool empty(); // 二叉树是否为空 void print(int mode = 3); void swap(); // 交换二叉树的左右子节点 // ------------------ 深度优先搜索 ------------------ void preOrder(void (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node = m_tnRoot); // 前序遍历 void inOrder(void (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node = m_tnRoot); // 中序遍历 void postOrder(void (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node = m_tnRoot); // 后序遍历 // ------------------ 广度优先搜索 ------------------ void levelOrder(void (*visit)(BinaryTreeNode&lt;T&gt; * node), BinaryTreeNode&lt;T&gt; * node = m_tnRoot); // 层次遍历protected: static LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* tree2flattenQueue(BinaryTreeNode&lt;T&gt;** ptr); // 节点操作，注意需要定义为static成员函数，才能取址供遍历函数调用 static BinaryTreeNode&lt;T&gt;* createNode(BinaryTreeNode&lt;T&gt;* p = nullptr); static void deleteNode(BinaryTreeNode&lt;T&gt;* node); BinaryTreeNode&lt;T&gt;* m_tnRoot; // 根节点 static void printNode(BinaryTreeNode&lt;T&gt;* node); static void swapNode(BinaryTreeNode&lt;T&gt;* node); static int sizeofNode(BinaryTreeNode&lt;T&gt;* node); static int heightofNode(BinaryTreeNode&lt;T&gt;* node);&#125;; 树的遍历方法定义：二叉树的遍历(traversal)中，每个元素仅被访问一次。有四种常用的遍历方法1) 前序遍历；2) 中序遍历；3) 后序遍历；4) 层次遍历。 其中，前三种为深度优先探索方法，最后一种为广度优先搜索方法。 前序遍历 前序遍历是指在访问某节点node时，先对该节点进行值的访问或修改，再依次对其左子树、右子树进行同样的嵌套操作。在树的定义与声明成员函数preOrder定义如下 12345678template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::preOrder(void (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node)&#123; if (!node) return; visit(node); preOrder(visit, node-&gt;left); preOrder(visit, node-&gt;right);&#125; 中序遍历 中序遍历是指在访问某节点node时，对其元素的访问或修改放在左子树、右子树的嵌套操作之间。在树的定义与声明成员函数inOrder定义如下 1234567template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::inOrder(void (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node) &#123; if (!node) return; inOrder(visit, node-&gt;left); visit(node); inOrder(visit, node-&gt;right);&#125; 后序遍历 后序遍历是指在访问某节点node时，对其元素的访问或修改放在左子树、右子树的嵌套操作之后。考虑到后续遍历与前序遍历的不同，其左右子节点的访问也倒序访问，在树的定义与声明成员函数postOrder定义如下 1234567template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::postOrder(void (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node) &#123; if (!node) return; postOrder(visit, node-&gt;right); postOrder(visit, node-&gt;left); visit(node);&#125; 层次遍历 层次遍历是一种广度优先搜索方法，需要某层的节点按完全二叉树的顺序依次访问。 将二叉树非空节点按顺序存入队列LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* ordered中，具体过程为 初始化队列queue，将树/子树的根节点存入； 初始化队列ordered，暂时为空； 从queue中弹出一个节点作为父节点，将此节点存入ordered，并将其非空左右子节点存入queue作为后续的父节点，即“出一进二”，不断循环直至queue为空； 返回节点ordered。 值得注意的是，tree2flattenQueue函数需要传入父节点的地址&amp;root。 由队列特性，按先入先出的顺序，依次对ordered的元素进行访问。 123456789101112131415161718192021222324252627282930template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::levelOrder(void (*visit)(BinaryTreeNode&lt;T&gt;* node), BinaryTreeNode&lt;T&gt;* node) &#123; if (!node) return; LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* ordered = tree2flattenQueue(&amp;node); while (ordered-&gt;size() &gt; 0) &#123; BinaryTreeNode&lt;T&gt;** n = ordered-&gt;pop(); visit(*n); &#125; delete ordered;&#125;template&lt;typename T&gt;LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* LinkedBinaryTree&lt;T&gt;::tree2flattenQueue(BinaryTreeNode&lt;T&gt;** ptr) &#123; // - 需要获取`m_tnRoot`的地址，不可定义形参`BinaryTreeNode&lt;T&gt;* node = m_tnRoot`再将`&amp;node`存储，因为函数返回后，形参`node`将被释放 // - 注意这里要定义二级指针，否则在出队列时，会将节点破坏 LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* queue = new LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;; // 中间过程使用的队列 LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* ordered = new LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;; // 已排序的队列 queue-&gt;push(ptr); while (queue-&gt;size() &gt; 0) &#123; BinaryTreeNode&lt;T&gt;** p = queue-&gt;pop(); // 弹出队首元素 ordered-&gt;push(p); // 将该节点存入已排序队列 // 将子节点存入队列 if ((*p)-&gt;left) queue-&gt;push(&amp;((*p)-&gt;left)); if ((*p)-&gt;right) queue-&gt;push(&amp;((*p)-&gt;right)); &#125; delete queue; return ordered;&#125; 树的函数定义 构造函数、复制构造函数、析构函数 1234567891011template&lt;typename T&gt;LinkedBinaryTree&lt;T&gt;::LinkedBinaryTree() &#123; m_tnRoot = nullptr; &#125;template&lt;typename T&gt;LinkedBinaryTree&lt;T&gt;::~LinkedBinaryTree() &#123; postOrder(&amp;deleteNode, m_tnRoot); &#125; 重载了构造函数，可传入数组变量进行树的构造，各元素按完全二叉树的编号顺序进行输入，若节点为空，数组中置为字符&#39;#&#39;。在节点的输入时，参考层级遍历的方法，借助队列实现。 1234567891011121314151617181920212223242526272829303132333435template&lt;typename T&gt;LinkedBinaryTree&lt;T&gt;::LinkedBinaryTree(T* set, int n)&#123; // 初始化根节点 m_tnRoot = createNode(); m_tnRoot-&gt;set(set[0]); // 初始化队列 LinkedQueue&lt;BinaryTreeNode&lt;T&gt;*&gt; queueParent; // 中间过程使用的队列 queueParent.push(m_tnRoot); // 放入根节点 int cnt = 0; while (queueParent.size() &gt; 0) &#123; BinaryTreeNode&lt;T&gt;* parent = queueParent.pop(); // 弹出父节点 // ------------- 左子树 ------------- if ((++cnt) &gt;= n) break; T val = set[cnt]; if (val != (T)'#') &#123; BinaryTreeNode&lt;T&gt;* left = createNode(parent); // 创建左子树 left-&gt;set(val); // 赋值 parent-&gt;left = left; // 子节点链接到父节点 queueParent.push(left); // 置入队列 &#125; // ------------- 右子树 ------------- if ((++cnt) &gt;= n) break; val = set[cnt]; if (val != (T)'#') &#123; BinaryTreeNode&lt;T&gt;* right = createNode(parent); // 创建右子树 right-&gt;set(val); // 赋值 parent-&gt;right = right; // 子节点链接到父节点 queueParent.push(right); // 置入队列 &#125; &#125;&#125; 复制构造函数主要实现树间的深拷贝，即必须保证两棵树没有内存共享的情况下值完全一致，有以下关键点 节点的创建，即开辟新的内存并赋值； 节点间的关系，由parent、left、right三个指针共同维护一棵树的结构，因此只需各个节点的指针信息正确对应所指节点，将父节点为空的节点作为树的根节点，一棵树就能“立起来”。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546template&lt;typename T&gt;LinkedBinaryTree&lt;T&gt;::LinkedBinaryTree(const LinkedBinaryTree&lt;T&gt;&amp; tree)&#123; // 初始化待拷贝数组 LinkedQueue&lt;BinaryTreeNode&lt;T&gt;**&gt;* queuePtrFrom = tree2flattenQueue(const_cast&lt;BinaryTreeNode&lt;T&gt;**&gt;(&amp;tree.m_tnRoot)); // 取消`const`特性 LinkedQueue&lt; BinaryTreeNode&lt;T&gt;*&gt;* queueFrom = new LinkedQueue&lt; BinaryTreeNode&lt;T&gt;*&gt;; while (queuePtrFrom-&gt;size() &gt; 0) &#123; queueFrom-&gt;push(*(queuePtrFrom-&gt;pop())); &#125; delete queuePtrFrom; // 更改类型为列表，便于查找，实际上不修改可也 ChainList&lt;BinaryTreeNode&lt;T&gt;*&gt;* listFrom = queueFrom; ChainList&lt;BinaryTreeNode&lt;T&gt;*&gt;* listTo = new ChainList&lt;BinaryTreeNode&lt;T&gt;*&gt;; // 复制数组的值 for (int i = 0; i &lt; listFrom-&gt;size(); i++) &#123; BinaryTreeNode&lt;T&gt;* nodeFrom = listFrom-&gt;get(i); BinaryTreeNode&lt;T&gt;* nodeTo = new BinaryTreeNode &lt;T&gt;; // 创建节点 nodeTo-&gt;set(nodeFrom-&gt;get()); // 复制值 listTo-&gt;insert(listTo-&gt;size(), nodeTo); &#125; // 链接节点间关系 for (int i = 0; i &lt; listFrom-&gt;size(); i++) &#123; BinaryTreeNode&lt;T&gt;* nodeFrom = listFrom-&gt;get(i); BinaryTreeNode&lt;T&gt;* nodeTo = listTo -&gt;get(i); int index = -1; // 父节点 if (nodeFrom-&gt;parent) &#123; index = listFrom-&gt;indexOf(nodeFrom-&gt;parent); nodeTo-&gt;parent = listTo-&gt;get(index); &#125; else &#123; m_tnRoot = nodeTo; &#125; // 左右子树节点 if (nodeFrom-&gt;left) &#123; index = listFrom-&gt;indexOf(nodeFrom-&gt;left); nodeTo-&gt;left = listTo-&gt;get(index); &#125; if (nodeFrom-&gt;right) &#123; index = listFrom-&gt;indexOf(nodeFrom-&gt;right); nodeTo-&gt;right = listTo-&gt;get(index); &#125; &#125; delete listFrom; delete listTo; // 回收内存&#125; 树的信息 1234567891011121314template&lt;typename T&gt;int LinkedBinaryTree&lt;T&gt;::size() &#123; return sizeofNode(m_tnRoot); &#125;template&lt;typename T&gt;int LinkedBinaryTree&lt;T&gt;::sizeofNode(BinaryTreeNode&lt;T&gt;* node) &#123; if (!node) return 0; // 采用后续遍历 int sizeL = sizeofNode(node-&gt;left); int sizeR = sizeofNode(node-&gt;right); return 1 + sizeL + sizeR;&#125; 1234567891011121314template&lt;typename T&gt;int LinkedBinaryTree&lt;T&gt;::height() &#123; return heightofNode(m_tnRoot); &#125;template&lt;typename T&gt;int LinkedBinaryTree&lt;T&gt;::heightofNode(BinaryTreeNode&lt;T&gt;* node) &#123; if (!node) return 0; // 采用后续遍历 int heightL = heightofNode(node-&gt;left); int heightR = heightofNode(node-&gt;right); return 1 + (heightL &gt; heightR ? heightL : heightR);&#125; 12345678910111213141516171819202122232425262728293031template&lt;typename T&gt;bool LinkedBinaryTree&lt;T&gt;::empty() &#123; return size() == 0; &#125;template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::print(int mode) &#123; switch (mode) &#123; case 0: std::cout &lt;&lt; "preOrder: "; preOrder(&amp;printNode, m_tnRoot); break; case 1: std::cout &lt;&lt; "inOrder: "; inOrder(&amp;printNode, m_tnRoot); break; case 2: std::cout &lt;&lt; "postOrder: "; postOrder(&amp;printNode, m_tnRoot); break; case 3: std::cout &lt;&lt; "levelOrder: "; levelOrder(&amp;printNode, m_tnRoot); break; default: break; &#125; std::cout &lt;&lt; std::endl;&#125; 树的操作 交换每个节点的左右子树 12345template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::swap()&#123; preOrder(&amp;swapNode, m_tnRoot);&#125; 节点操作与访问 12345678910111213141516171819202122232425262728template&lt;typename T&gt;BinaryTreeNode&lt;T&gt;* LinkedBinaryTree&lt;T&gt;::createNode(BinaryTreeNode&lt;T&gt;* p) &#123; return new BinaryTreeNode&lt;T&gt;(p); &#125;template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::deleteNode(BinaryTreeNode&lt;T&gt;* node) &#123; if (!node) return; delete node; &#125;template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::printNode(BinaryTreeNode&lt;T&gt;* node) &#123; if (!node) return; std::cout &lt;&lt; node-&gt;get() &lt;&lt; " "; &#125;template&lt;typename T&gt;void LinkedBinaryTree&lt;T&gt;::swapNode(BinaryTreeNode&lt;T&gt;* node)&#123; if (!node) return; BinaryTreeNode&lt;T&gt;* temp = node-&gt;left; node-&gt;left = node-&gt;right; node-&gt;right = temp;&#125; 数组描述数组描述比较简单，只需将节点按照完全二叉树进行标号，正确计算节点间的索引关系即可，访问元素更为便捷并节省空间。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667template&lt;typename T&gt;class ArrayBinaryTree: public ArrayList&lt;T&gt;&#123;public: ArrayBinaryTree() :ArrayList&lt;T&gt;() &#123;&#125; ArrayBinaryTree(T* set, int n) &#123; // 由给定集合生成二叉树，空叶节点用`#`表示，形如`&#123;3,9,20,#,#,15,7&#125;` this-&gt;m_TElements = new T[n]; std::copy(set, set + n, this-&gt;m_TElements); this-&gt;m_iCount = this-&gt;m_iSize = n; &#125; ArrayBinaryTree(const ArrayBinaryTree&lt;T&gt;&amp; tree) &#123; int n = tree.size(); this-&gt;m_TElements = new T[n]; std::copy(tree.m_TElements, tree.m_TElements + n, this-&gt;m_TElements); this-&gt;m_iCount = this-&gt;m_iSize = n; &#125; ~ArrayBinaryTree() &#123;&#125; int size() &#123; return sizeOfNode(0); &#125; // 二叉树结点数 int height() &#123; return heightofNode(0); &#125; // 二叉树高度 bool empty() &#123; return size() == 0; &#125; // 二叉树是否为空 void print() const &#123; int H = 0; for (int i = 0; i &lt; this-&gt;m_iCount; i++) &#123; int h = std::floor(std::log2(i + 1)); if (h &gt; H) &#123; H = h; std::cout &lt;&lt; std::endl; &#125; std::cout &lt;&lt; this-&gt;m_TElements[i] &lt;&lt; ' '; &#125; std::cout &lt;&lt; std::endl; &#125; // ------------------ 深度优先搜索 ------------------ void preOrder(void (*visit)(int node), int node = 0) &#123; // 前序遍历 if (!checkNode(node)) return; visit(node); preOrder(visit, left(node)); preOrder(visit, right(node)); &#125; void inOrder(void (*visit)(int node), int node = 0) &#123; // 中序遍历 if (!checkNode(node)) return; inOrder(visit, left(node)); visit(node); inOrder(visit, right(node)); &#125; void postOrder(void (*visit)(int node), int node = 0) &#123; // 后序遍历 if (!checkNode(node)) return; postOrder(visit, left(node)); postOrder(visit, right(node)); visit(node); &#125; // ------------------ 广度优先搜索 ------------------ void levelOrder(void (*visit)(int node), int node = 0) &#123; // 层次遍历 if (!checkNode(node)) return; for (int i = node; i &lt; this-&gt;m_iCount; i++) visit(i); &#125;protected: bool checkNode(int node) &#123; if (node &lt; 0 || node &gt;= this-&gt;m_iCount) return false; if (this-&gt;m_TElements[node] == (T)'#') return false; return true; &#125; static int parent(int node) &#123; return (node - 1) / 2; &#125; static int left(int node) &#123; return 2 * (node + 1) - 1; &#125; static int right(int node) &#123; return 2 * (node + 1); &#125; int sizeOfNode(int node) &#123; if (!checkNode(node)) return 0; int sizeL = sizeOfNode(left(node)); int sizeR = sizeOfNode(right(node)); return 1 + sizeL + sizeR; &#125; int heightofNode(int node) &#123; if (!checkNode(node)) return 0; int heightL = heightofNode(left(node)); int heightR = heightofNode(right(node)); return 1 + (heightL &gt; heightR ? heightL : heightR); &#125;&#125;; 应用二叉树的一个应用是计算图的表示，单个节点的值为操作符，左右子树为操作数，例如对于运算式 ((a + b) > (c > e)) | a < b \& (x < y | y > z)其运算图为 由LinkedBinaryTree&lt;T&gt;派生出类Expression，指定其输入输入类型为字符(char) 子类调用父类成员时，需添加this-&gt;指针； 公有继承时，父类公有成员与保护成员权限不变，但子类不继承私有成员； 12345678910111213141516171819202122class Expression : public LinkedBinaryTree&lt;char&gt;&#123;public: Expression(char* symbols, int n) : LinkedBinaryTree&lt;char&gt;(symbols, n) &#123; translate(); &#125; ~Expression() &#123; if (expression) delete expression; &#125; int size() &#123; return LinkedBinaryTree&lt;char&gt;::size(); &#125; int height() &#123; return LinkedBinaryTree&lt;char&gt;::height(); &#125; bool empty() &#123; return LinkedBinaryTree&lt;char&gt;::empty(); &#125; void print(int mode = 3) &#123; LinkedBinaryTree&lt;char&gt;::print(mode); &#125; float fCalculate() const; char* expression;private: void translate(); static void translateNode(BinaryTreeNode&lt;char&gt;* node, char* buff, int n); static float fCalculateNode(BinaryTreeNode&lt;char&gt;* node); static float char2float(char&amp; c) &#123; return c - '0'; &#125;&#125;;&#125;; 可通过打印各节点对比查看前序遍历、中序遍历、后序遍历、层级遍历间的区别1234567891011121314151617int main(int argc, char** argv)&#123; char symbols[32] = &#123; '|', '&gt;', '&amp;', '+', '-', '&lt;', '|', 'a', 'b', 'c', 'e', 'a', 'b', '&lt;', '&gt;', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', 'x', 'y', 'y', 'z' &#125;; Expression expression(symbols, 31); cout &lt;&lt; "size: " &lt;&lt; expression.size() &lt;&lt; endl \ &lt;&lt; "height: " &lt;&lt; expression.height() &lt;&lt; endl; for (int i = 0; i &lt; 4; i++) expression.print(i); system("pause");&#125; 其输出如下，得到分别为前缀(prefix)、中缀(infix)、后缀(postfix)形式的输出1234567size: 19height: 5preOrder: | &gt; + a b - c e &amp; &lt; a b | &lt; x y &gt; y zinOrder: a + b &gt; c - e | a &lt; b &amp; x &lt; y | y &gt; zpostOrder: z y &gt; y x &lt; | b a &lt; &amp; e c - b a + &gt; |levelOrder: | &gt; &amp; + - &lt; | a b c e a b &lt; &gt; x y y z请按任意键继续. . . 可以看到三张顺序输出结果差异较大，如下图所示，中序遍历inOrder是通常的书写形式。 在实际使用该类用于计算时，应考虑运算优先级，有以下几种解决方法 给不同的运算符设置不同的优先级，例如×, ÷比+, -优先级更高； 将操作符的每个操作数用括号括起来，如其成员函数translate()与translateNode()实现如下 123456789101112131415161718192021void Expression::translate()&#123; if (!expression) &#123; expression = new char[256]; expression[0] = '\0'; &#125; translateNode(this-&gt;m_tnRoot, expression, 256);&#125;void Expression::translateNode(BinaryTreeNode&lt;char&gt;* node, char* buff, int n)&#123; // 嵌套中止条件：叶节点 if (node-&gt;isLeaf()) &#123; sprintf_s(buff, n, "%s%c%c%c", buff, '(', node-&gt;get(), ')'); return; &#125; translateNode(node-&gt;left, buff, n); // 左子树 sprintf_s(buff, n, "%c%s%c", '(', buff, node-&gt;get()); translateNode(node-&gt;right, buff, n); // 右子树 sprintf_s(buff, n, "%s%c", buff, ')');&#125; 调用cout &lt;&lt; expression.expression &lt;&lt; endl;后，输出为$ ((((((((((a)+(b))&gt;(c)-(e)))|(a)&lt;(b))\&amp;(x)&lt;(y))|(y)&gt;(z)))))$，外层冗余括号不影响计算结果。 按节点计算时，已根据输入形式自动考虑运算优先次序，实现如下 12345678910111213141516171819202122232425262728293031323334float Expression::fCalculate() const&#123; return fCalculateNode(this-&gt;m_tnRoot); // 计算根节点的值&#125;float Expression::fCalculateNode(BinaryTreeNode&lt;char&gt;* node)&#123; if (!node) return 0; // 若为叶子节点，返回操作数 if (node-&gt;isLeaf()) &#123; char c = node-&gt;get(); return char2float(c); &#125; // 不为叶子节点，获取左右子树的运算结果 float x = fCalculateNode(node-&gt;left); float y = fCalculateNode(node-&gt;right); // 从自身读取操作符，并运算 char op = node-&gt;get(); switch (op) &#123; case '+': return x + y; case '-': return x - y; case '*': return x * y; case '/': return x / y; default: throw "未定义的运算符"; &#125;&#125; 输入$(1+2)/(5\times6-7)$得到输出 (((((1)+(2))/(5)*(6))-(7))) = 0.130435 12345678910111213int main(int argc, char** argv)&#123; char symbols[32] = &#123; '/', '+', '-', '1', '2', '*', '7', '#', '#', '#', '#', '5', '6'&#125;; Expression expression(symbols, 13); cout &lt;&lt; expression.expression &lt;&lt; " = " &lt;&lt; expression.fCalculate() &lt;&lt; endl; system("pause");&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】跳表和散列]]></title>
    <url>%2F2020%2F02%2F24%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E8%B7%B3%E8%A1%A8%E5%92%8C%E6%95%A3%E5%88%97%2F</url>
    <content type="text"><![CDATA[字典定义及抽象数据描述定义：字典(dictionary)是由一些形如$(k, v)$的数对所组成的集合，其中$k$为关键字，$v$是与关键字$k$对应的值。 抽象数据描述：1234567891011抽象数据类型 dictionary&#123; 实例: 关键字各不相同的一组数据对； 操作： empty(); // 判断是否为空 size(); // 返回字典内数对个数 find(k); // 返回关键字为k的数对 insert(k, v); // 插入数据对 erase(k); // 删除关键字为k的数对&#125; 线性表描述将字典保存在线性表$p_0, p_1, \dots$中，其中$p_is$是字典中按关键字递增次序排列的数对。利用二分查找进行数据对的插入、查询与删除操作。其时间复杂度为$O(\log n)$。 在插入数据对时，首先对关键字进行查询，若未找到数对则创建新的数对，插入到对应位置；否则更新已有数对。 若采用数组描述的线性表，其二分查找较容易实现(实现略)。而对于链表描述的线性表，可引入跳表(跳表描述)减少时间复杂度， 跳表描述对于线性表描述(链表描述)的字典，对$n$个数对进行查找，至多需要$n$此关键字比较。若添加中间节点的指针信息，其比较次数可减少到$(n/2 + 1)$。由此引入跳表(skiplist)，在不同的节点位置处添加多级指针信息。 跳表可用对数形式进行级的分配。例如，对于$n$个数，$0$级链表包括所有数对，$1$级链表每$2$个数对取一个，$2$级链表每$4$个数对取一个，以此类推。那么对于一个属于$i$级链表的数对，当且仅当它属于$0~i$级链表，但不属于$i+1$级链表。如下图所示，键为$20$的数对属于$0,1$级链表，不属于$2$级链表。 但是采用对数形式描述跳表，需在插入或删除数对后更新各级链表，其指针不易维护。对于该问题，可采用概率形式解决，在查询位置处(仍需满足有序线性表)插入结点时生成随机数判定级别，使s它属于$i-1$级链表的情况下，又属于$i$级链表的概率服从均匀分布，使$n$个数对在各级中分布是均匀的，即 P(属于i级链表|属于i-1级链表) = p具体实现如下，构造键值对对象123456789101112131415161718//linkeddictionary.htemplate&lt;typename K, typename V&gt;class Pair&#123;public: Pair() &#123; m_key = new K; m_value = new V; &#125;; Pair(const K k, const V v) &#123; m_key = new K; *m_key = k; m_value = new V; *m_value = v; &#125;; ~Pair() &#123; delete m_key; delete m_value; &#125; void setKey(const K k) &#123; *m_key = k; &#125; void setVal(const V v) &#123; *m_value = v; &#125; K getKey() const &#123; return *m_key; &#125; V getVal() const &#123; return *m_value; &#125;private: K* m_key; V* m_value;&#125;; 以下几点说明： 跳表内各级链表为ChainList&lt;T&gt;模板类的实例，将键值对Pair&lt;K, V&gt;*视作各节点的值，即实例为ChainList&lt;Pair&lt;K, V&gt;* &gt;； 指定最大级数，各级链表地址存储在ChainList&lt;Pair&lt;K, V&gt;*&gt;** m_kvp中； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//linkeddictionary.htemplate&lt;typename K, typename V&gt;class SkipList&#123;public: SkipList(int maxLevel=3, float p=0.5); ~SkipList(); bool empty() const &#123; return size() == 0; &#125; int size() const &#123; return m_kvp[0]-&gt;size(); &#125; void insert(const K&amp; k, const V&amp; v); V get (const K&amp; k) const; void erase(const K&amp; k); void print() const;private: ChainNode&lt;Pair&lt;K, V&gt;*&gt;* find(const K&amp; k) const; int level() const &#123; int lv = 0; for (; std::rand() &gt; m_iCutoff; lv++); return (lv &gt; m_iMaxLevel - 1) ? (m_iMaxLevel - 1) : lv; &#125; int m_iCutoff; // 条件概率，截断 int m_iMaxLevel; // 最大链表层数 ChainList&lt;Pair&lt;K, V&gt;*&gt;** m_kvp; // 多层链表数组&#125;;template&lt;typename K, typename V&gt;SkipList&lt;K, V&gt;::SkipList(int maxLevel, float p) : m_iMaxLevel(maxLevel), m_iCutoff(p * RAND_MAX)&#123; m_kvp = new ChainList&lt;Pair&lt;K, V&gt;*&gt; * [maxLevel]; for (int i = 0; i &lt; m_iMaxLevel; i++) m_kvp[i] = new ChainList&lt;Pair&lt;K, V&gt;*&gt;;&#125;template&lt;typename K, typename V&gt;SkipList&lt;K, V&gt;::~SkipList()&#123; for (int i = 0; i &lt; m_iMaxLevel; i++) delete m_kvp[i]; delete[] m_kvp;&#125;template&lt;typename K, typename V&gt;void SkipList&lt;K, V&gt;::print() const&#123; for (int i = 0; i &lt; m_iMaxLevel; i++) &#123; ChainNode&lt;Pair&lt;K, V&gt;*&gt;* pn = m_kvp[i]-&gt;getNode(0); // 当前级链表的首节点 std::cout &lt;&lt; "Level [" &lt;&lt; i &lt;&lt; "]: "; while (pn-&gt;ptr &amp;&amp; pn-&gt;next) &#123; Pair&lt;K, V&gt;* kvp = pn-&gt;getVal(); std::cout &lt;&lt; kvp-&gt;getKey() &lt;&lt; ": " &lt;&lt; kvp-&gt;getVal() &lt;&lt; " -&gt; "; pn = pn-&gt;next; &#125; std::cout &lt;&lt; std::endl; &#125; std::cout &lt;&lt; std::endl;&#125; 在搜索时，自最高级链表向低级搜索，因为若高级链表中存在某键值对，那么低级链表中必定存在，此时可调用ChainList&lt;T&gt;::indexOf(T)直接获取起始搜索位置，实现二分搜索降低时间复杂度 12345678910111213141516171819202122232425262728293031323334template&lt;typename K, typename V&gt;ChainNode&lt;Pair&lt;K, V&gt;*&gt;* SkipList&lt;K, V&gt;::find(const K&amp; k) const&#123; // 若字典内无数对，返回空 if (empty()) return nullptr; ChainNode&lt;Pair&lt;K, V&gt;*&gt;* ret = m_kvp[0]-&gt;getNode(0); // 初始化为0级链表首节点 for (int i = m_iMaxLevel - 1; i &gt;= 0; i--) &#123; // 由高级向低级搜索 if (m_kvp[i]-&gt;empty()) continue; int index = m_kvp[i]-&gt;indexOf(ret-&gt;getVal()); // 查找当前节点 ChainNode&lt;Pair&lt;K, V&gt;*&gt; * pn = (index == -1)? \ m_kvp[i]-&gt;getNode(0): \ m_kvp[i]-&gt;getNode(index); // 当前级链表开始搜索的位置 while ((pn-&gt;getVal()-&gt;getKey() &lt; k) &amp;&amp; pn-&gt;next-&gt;ptr) &#123; pn = pn-&gt;next; ret = pn; &#125; if (!pn-&gt;next-&gt;ptr) continue; // 已为尾节点 if (pn-&gt;next-&gt;getVal()-&gt;getKey() == ret-&gt;getVal()-&gt;getKey()) return pn-&gt;next; // 后一节点是否符合 &#125; return ret;&#125;template&lt;typename K, typename V&gt;V SkipList&lt;K, V&gt;::get(const K&amp; k) const&#123; // 查找 ChainNode&lt;Pair&lt;K, V&gt;*&gt;* found = find(k); if (found) return found-&gt;getVal()-&gt;getVal(); return NULL;&#125; 插入时，需确定插入键值对的级数level()，依次插入链表即可，注意有序插入1234567891011121314151617181920212223242526272829303132333435363738394041424344454647template&lt;typename K, typename V&gt;void SkipList&lt;K, V&gt;::insert(const K&amp; k, const V&amp; v)&#123; // 先进行查找，若找到节点则不添加键，仅修改值 ChainNode&lt;Pair&lt;K, V&gt;*&gt; * found = find(k); // ----- 若字典为空，直接插入键值对 ----- if (!found) &#123; // 创建节点 Pair&lt;K, V&gt;* pn = new Pair&lt;K, V&gt;(k, v); // 确定层级 int lv = level(); // 插入各级链表 for (int i = 0; i &lt;= lv; i++) m_kvp[i]-&gt;insert(0, pn); return; &#125; // ----- 若找到节点则不添加键，仅修改值 ----- if (found-&gt;getVal()-&gt;getKey() == k) &#123; found-&gt;getVal()-&gt;setVal(v); return; &#125; // ----- 若未找到，在标志节点附近有序插入 ----- Pair&lt;K, V&gt;* pn = new Pair&lt;K, V&gt;(k, v); // 确定层级 int lv = level(); // 插入各级链表 for (int i = 0; i &lt;= lv; i++) &#123; // 当前链表空 if (m_kvp[i]-&gt;empty()) &#123; m_kvp[i]-&gt;insert(0, pn); continue; &#125; int index = 0; ChainNode&lt;Pair&lt;K, V&gt;*&gt;* lvn = m_kvp[i]-&gt;getNode(0); // 当前级链表的首节点 while (lvn) &#123; if (!lvn-&gt;ptr) &#123; // 尾节点 m_kvp[i]-&gt;insert(index, pn); break; &#125; else &#123; if (lvn-&gt;ptr &amp;&amp; lvn-&gt;getVal()-&gt;getKey() &gt; k) &#123; // 当前节点的键大于k m_kvp[i]-&gt;insert(index, pn); break; &#125; else &#123; lvn = lvn-&gt;next; index++; &#125; &#125; &#125; &#125;&#125; 删除结点时，从高级链表至低级的顺序释放，因为若从低级链表至高级的顺序释放，某节点在调用ChainList&lt;T&gt;::erase(int)后，内存被释放导致高级链表中ChainList&lt;T&gt;::indexOf(T)难以定位 1234567891011121314151617template&lt;typename K, typename V&gt;void SkipList&lt;K, V&gt;::erase(const K&amp; k)&#123; // 先进行查找 ChainNode&lt;Pair&lt;K, V&gt;*&gt;* found = find(k); // ----- 若未找到 ---- if (!found) return; // ----- 若找到，删除各级链中的该数对----- for (int i = m_iMaxLevel - 1; i &gt;= 0; i--) &#123; ChainList&lt;Pair&lt;K, V&gt;*&gt;* pc = m_kvp[i]; // 当前级链表 int index = pc-&gt;indexOf(found-&gt;getVal()); if (index == -1) continue; pc-&gt;erase(index); &#125;&#125; 主函数测试及输出如下123456789101112131415161718int main(int argc, char** argv)&#123; SkipList&lt;int, int&gt; dict(3, 0.5); cout &lt;&lt; "insert: " &lt;&lt; endl; for (int i = 0; i &lt; 10; i++) &#123; dict.insert(i % 6, i); dict.print(); &#125; cout &lt;&lt; "erase: " &lt;&lt; endl; for (int i = 0; i &lt; 4; i++) &#123; dict.erase(i); dict.print(); &#125; return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657insert:Level [0]: 0: 0 -&gt;Level [1]:Level [2]:Level [0]: 0: 0 -&gt; 1: 1 -&gt;Level [1]: 1: 1 -&gt;Level [2]:Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt;Level [1]: 1: 1 -&gt; 2: 2 -&gt;Level [2]: 2: 2 -&gt;Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt;Level [1]: 1: 1 -&gt; 2: 2 -&gt;Level [2]: 2: 2 -&gt;Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt;Level [1]: 1: 1 -&gt; 2: 2 -&gt; 4: 4 -&gt;Level [2]: 2: 2 -&gt; 4: 4 -&gt;Level [0]: 0: 0 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 1: 1 -&gt; 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [2]: 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [0]: 0: 6 -&gt; 1: 1 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 1: 1 -&gt; 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [2]: 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [0]: 0: 6 -&gt; 1: 7 -&gt; 2: 2 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 1: 7 -&gt; 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [2]: 2: 2 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [0]: 0: 6 -&gt; 1: 7 -&gt; 2: 8 -&gt; 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 1: 7 -&gt; 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [0]: 0: 6 -&gt; 1: 7 -&gt; 2: 8 -&gt; 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 1: 7 -&gt; 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;erase:Level [0]: 1: 7 -&gt; 2: 8 -&gt; 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 1: 7 -&gt; 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [0]: 2: 8 -&gt; 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [2]: 2: 8 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [0]: 3: 3 -&gt; 3: 9 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 4: 4 -&gt; 5: 5 -&gt;Level [2]: 4: 4 -&gt; 5: 5 -&gt;Level [0]: 3: 3 -&gt; 4: 4 -&gt; 5: 5 -&gt;Level [1]: 4: 4 -&gt; 5: 5 -&gt;Level [2]: 4: 4 -&gt; 5: 5 -&gt; 散列描述散列函数和散列表字典的另一种表示方法是散列(hashing)。它用一个散列函数(哈希函数)把字典的数对映射到一个散列表(哈希表)的具体位置。若关键字范围过大，可将若干个不同的关键字映射到散列表的同一位置。散列表的每个位置称作桶(bucket)，桶的数目等于散列表的长度。对于关键字为$k$的数对，$f(k)$为起始桶(home bucket)。 当两个不同关键字所对应的起始桶相同，产生冲突(collision)，但一个桶内可存放多个数位，但若存储桶内没有空间存储新数对，则发生溢出(overflow)。当冲突和溢出同时发生，可采用线性探查法、平法探查法、双重散列法等解决。 可采取均匀散列函数(uniform hash function)，使映射到散列表中各个桶的关键字数量大致相同，此时冲突和溢出发生的平均数最少。在实际应用中性能表现良好的均匀散列函数被称作良好散列函数(good hash function)。 均匀散列函数输入值应依赖关键字的所有位； 要使得除法散列函数成为良好散列函数，$D$应取素数，且不能被小于$20$的整数除。f(k) = k \% D 为了增加哈希表的搜索效率，尽量不让哈希表插满，规定一个负载因子，控制哈希表内插入元素的个数，从而提高效率。负载因子的定义为： \alpha = 填入表中元素(关键字个数) / 可填入总元素(表长)$alpha$越大，表明填入表内元素越多，产生哈希冲突的可能就越大。相反则越小。冲突的机会越大，则查找的成本越高；反之，查找的成本越小，查找时间就越小. 以下为STL模板类hash&lt;T&gt;的专业实现版本123456789101112template&lt;&gt;class hash&lt;string&gt;&#123;public: size_t operator() (const string key) const &#123; unsigned long hashVal = 0; for (int i = 0; i &lt; key.size(); i++) hashVal = 5 * hashVal + key.at(i); return hashVal; &#125;&#125; 线性探查(闭散列法，开放地址法)线性探查(Linear Probing)是指，在某数对插入时。键所对应的桶满时，向哈希表往后找到下一个可用的桶。对于关键字为$k$的数据对，其具体插入步骤如下 搜索起始桶$f(k)$，若为空，则插入并退出，否则进入2； 将散列表当作环表继续搜索下一个可用的桶，直至以下情况发生：a)存有关键字$k$的桶已找到，修改其数值；b)到达一个空桶，插入后退出；c)回到起始桶，冲突和溢出同时发生。 设散列表中有$n$条数据，桶数为$b$，散列函数的除数为$D$且$D=b$ 散列表初始化时间为$O(b)$； 最坏情况下(关键字都对应同一起始桶)插入和查找时间均为$\Theta(n)$； 平均性能而言，散列远优于线性表。一次成功搜索和不成功搜索平均搜索桶数分别记作$U_n$与$S_n$，则有U_n \approxeq \frac{1}{2} \left( 1 + \frac{1}{(1 - \alpha)^2} \right)S_n \approxeq \frac{1}{2} \left( 1 + \frac{1}{(1 - \alpha) } \right) 其中$\alpha = n / b$，称作负载因子，当负载因子比较小是，线性探查使得散列的平均性能比线性表优越许多。 基于线性探查实现的字典如下123456789101112131415161718192021222324252627282930313233template&lt;typename K, typename V&gt;class HashTabel&#123;public: HashTabel(int length = 50, int d = 23); ~HashTabel(); bool empty() const &#123; return size() == 0; &#125; int size() const &#123; int cnt = 0; for (int i = 0; i &lt; m_iLength; i++) if (m_listIsEmpty[i]) cnt++; return cnt; &#125; void insert(const K&amp; k, const V&amp; v); V get(const K&amp; k) const; void erase(const K&amp; k); void print() const;private: size_t hash(const K k) const &#123; return k % m_iD; &#125; Pair&lt;K, V&gt;* find(const K&amp; k, bool insert = true) const; int indexOf(const Pair&lt;K, V&gt;* p) const &#123; for (int i = 0; i &lt; m_iLength; i++) if (m_listTable[i] == p) return i; &#125; int m_iD; Pair&lt;K, V&gt;** m_listTable; bool* m_listIsEmpty; int m_iLength;&#125;; 关键的线性探查步骤如下 12345678910111213141516171819202122template&lt;typename K, typename V&gt;Pair&lt;K, V&gt;* HashTabel&lt;K, V&gt;::find(const K&amp; k, bool insert) const&#123; int hv = hash(k); // 求取hash值 // 从初始桶开始查找 for (int i = 0; i &lt; m_iLength; i++) &#123; int index = (hv + i) % m_iLength; // 回到初始位置，同时发生冲突与溢出，返回空 if (i != 0 &amp;&amp; index == hv) return nullptr; // 在插入操作时，可返回空桶 if (insert &amp;&amp; m_listIsEmpty[index]) return m_listTable[index]; // 查找到相同键的桶 if (m_listTable[index]-&gt;getKey() == k) return m_listTable[index]; &#125; return nullptr;&#125; 构造函数、析构函数、输出函数 12345678910111213141516171819202122232425262728293031323334template&lt;typename K, typename V&gt;HashTabel&lt;K, V&gt;::HashTabel(int length, int d) :m_iLength(length), m_iD(d)&#123; m_listTable = new Pair&lt;K, V&gt;* [m_iLength]; m_listIsEmpty = new bool [m_iLength]; for (int i = 0; i &lt; m_iLength; i++) &#123; m_listTable[i] = new Pair&lt;K, V&gt;; m_listIsEmpty[i] = true; &#125;&#125;template&lt;typename K, typename V&gt;HashTabel&lt;K, V&gt;::~HashTabel()&#123; for (int i = 0; i &lt; m_iLength; i++) delete m_listTable[i]; delete[] m_listTable; delete[] m_listIsEmpty;&#125;template&lt;typename K, typename V&gt;void HashTabel&lt;K, V&gt;::print() const&#123; for (int i = 0; i &lt; m_iLength; i++) &#123; std::cout &lt;&lt; "|[" &lt;&lt; i &lt;&lt; "]"; if (!m_listIsEmpty[i]) std::cout &lt;&lt; m_listTable[i]-&gt;getKey() &lt;&lt; ": " &lt;&lt; \ m_listTable[i]-&gt;getVal() &lt;&lt; "| -&gt;"; else std::cout &lt;&lt; " -&gt; "; &#125; std::cout &lt;&lt; std::endl;&#125; 插入、查询、删除 12345678910111213141516171819202122232425262728293031323334353637template&lt;typename K, typename V&gt;void HashTabel&lt;K, V&gt;::insert(const K&amp; k, const V&amp; v)&#123; Pair&lt;K, V&gt;* found = find(k); if (!found) &#123; std::cout &lt;&lt; "哈希表已满" &lt;&lt; std::endl; return; &#125; // 反查位置 int index = indexOf(found); // 若为空，设置键 if (m_listIsEmpty[index]) found-&gt;setKey(k); // 设置值 found-&gt;setVal(v); m_listIsEmpty[index] = false;&#125;template&lt;typename K, typename V&gt;V HashTabel&lt;K, V&gt;::get(const K&amp; k) const&#123; Pair&lt;K, V&gt;* found = find(k); if (found) return found-&gt;getVal(); return NULL;&#125;template&lt;typename K, typename V&gt;void HashTabel&lt;K, V&gt;::erase(const K&amp; k)&#123; Pair&lt;K, V&gt;* found = find(k, false); if (!found) return; // 反查位置 int index = indexOf(found); // 设置为空 m_listIsEmpty[index] = true;&#125; 主函数测试如下12345678910111213141516171819int main(int argc, char** argv)&#123; HashTabel&lt;int, int&gt; dict(10, 3); cout &lt;&lt; "insert: " &lt;&lt; endl; for (int i = 0; i &lt; 8; i++) &#123; dict.insert(2 * i, i); dict.print(); &#125; cout &lt;&lt; "erase: " &lt;&lt; endl; for (int i = 4; i &lt; 8; i++) &#123; dict.erase(2 * i); dict.print(); &#125; return 0;&#125; 插入过程如下图所示 输出格式为|[index] key: value| -&gt; ...1234567891011121314insert:|[0]0: 0| -&gt;|[1] -&gt; |[2] -&gt; |[3] -&gt; |[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1] -&gt; |[2]2: 1| -&gt;|[3] -&gt; |[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3] -&gt; |[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5]10: 5| -&gt;|[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5]10: 5| -&gt;|[6]12: 6| -&gt;|[7] -&gt; |[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4]8: 4| -&gt;|[5]10: 5| -&gt;|[6]12: 6| -&gt;|[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;erase:|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5]10: 5| -&gt;|[6]12: 6| -&gt;|[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6]12: 6| -&gt;|[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6] -&gt; |[7]14: 7| -&gt;|[8] -&gt; |[9] -&gt;|[0]0: 0| -&gt;|[1]4: 2| -&gt;|[2]2: 1| -&gt;|[3]6: 3| -&gt;|[4] -&gt; |[5] -&gt; |[6] -&gt; |[7] -&gt; |[8] -&gt; |[9] -&gt; 链式散列(开散列法，链地址法)链式散列是指，给散列表的每一个桶配置一个链式线性表，那么每个桶可以容纳无限多个容量，那么便不存在溢出问题。 1234567891011121314151617181920212223242526272829template&lt;typename K, typename V&gt;class HashChains&#123;public: HashChains(int length = 50, int d = 23); ~HashChains(); bool empty() const &#123; return size() == 0; &#125; int size() const &#123; int cnt = 0; for (int i = 0; i &lt; m_iLength; i++) cnt += m_listChains[i]-&gt;size(); return cnt; &#125; void insert(const K&amp; k, const V&amp; v); V get(const K&amp; k) const; void erase(const K&amp; k); void print() const;private: size_t hash(const K k) const &#123; return k % m_iD; &#125; Pair&lt;K, V&gt;* find(const K&amp; k, bool insert = true) const; ChainList&lt;Pair&lt;K, V&gt;*&gt;* findChain(const K&amp; k) const &#123; return m_listChains[hash(k)]; &#125; int m_iD; ChainList&lt;Pair&lt;K, V&gt;*&gt;** m_listChains; // 也可以换成跳表 int m_iLength;&#125;; 构造函数、析构函数、打印输出 123456789101112131415161718192021222324252627282930313233template&lt;typename K, typename V&gt;HashChains&lt;K, V&gt;::HashChains(int length, int d) : m_iLength(length), m_iD(d)&#123; m_listChains = new ChainList&lt;Pair&lt;K, V&gt;*&gt;* [m_iLength]; for (int i = 0; i &lt; m_iLength; i++) m_listChains[i] = new ChainList&lt;Pair&lt;K, V&gt;*&gt;;&#125;template&lt;typename K, typename V&gt;HashChains&lt;K, V&gt;::~HashChains()&#123; for (int i = 0; i &lt; m_iLength; i++) delete m_listChains[i]; delete[] m_listChains;&#125;template&lt;typename K, typename V&gt;void HashChains&lt;K, V&gt;::print() const&#123; for (int i = 0; i &lt; m_iLength; i++) &#123; ChainList&lt;Pair&lt;K, V&gt;*&gt;* chain = m_listChains[i]; ChainNode&lt;Pair&lt;K, V&gt;*&gt;* node = chain-&gt;getNode(0); std::cout &lt;&lt; "[" &lt;&lt; i &lt;&lt; "]"; while (node &amp;&amp; node-&gt;ptr) &#123; Pair&lt;K, V&gt;* p = node-&gt;getVal(); std::cout &lt;&lt; p-&gt;getKey() &lt;&lt; ":" &lt;&lt; p-&gt;getVal() &lt;&lt; " -&gt; "; node = node-&gt;next; &#125; std::cout &lt;&lt; " | "; &#125; std::cout &lt;&lt; std::endl;&#125; 在各桶内查找 1234567891011121314151617181920212223242526template&lt;typename K, typename V&gt;Pair&lt;K, V&gt;* HashChains&lt;K, V&gt;::find(const K&amp; k, bool insert) const&#123; int hv = hash(k); // 求取hash值 // 获取哈希值对应的桶 ChainList&lt;Pair&lt;K, V&gt;*&gt;* chain = m_listChains[hv]; // 在桶内依次查找 ChainNode&lt;Pair&lt;K, V&gt;*&gt;* node = chain-&gt;getNode(0); while (node) &#123; // 已到达链尾 if (!node-&gt;next) &#123; if (!insert) return nullptr; // 插入操作时，创建节点 Pair&lt;K, V&gt;* p = new Pair&lt;K, V&gt;; chain-&gt;insert(chain-&gt;size(), p); return p; &#125; // 找到对应键 if (node-&gt;getVal()-&gt;getKey() == k) return node-&gt;getVal(); node = node-&gt;next; &#125; return nullptr;&#125; 插入、查询、删除操作 123456789101112131415161718192021222324template&lt;typename K, typename V&gt;void HashChains&lt;K, V&gt;::insert(const K&amp; k, const V&amp; v)&#123; Pair&lt;K, V&gt;* p = find(k); if (p-&gt;getKey() != k) p-&gt;setKey(k); p-&gt;setVal(v);&#125;template&lt;typename K, typename V&gt;V HashChains&lt;K, V&gt;::get(const K&amp; k) const&#123; Pair&lt;K, V&gt;* p = find(k, false); if (p) return p-&gt;getVal(); return NULL;&#125;template&lt;typename K, typename V&gt;void HashChains&lt;K, V&gt;::erase(const K&amp; k)&#123; ChainList&lt;Pair&lt;K, V&gt;*&gt;* chain = findChain(k); Pair&lt;K, V&gt;* p = find(k, false); chain-&gt;erase(chain-&gt;indexOf(p)); delete p;&#125; 主函数测试如下12345678910111213141516171819int main(int argc, char** argv)&#123; HashChains&lt;int, int&gt; dict(10, 3); cout &lt;&lt; "insert: " &lt;&lt; endl; for (int i = 0; i &lt; 8; i++) &#123; dict.insert(2 * i, i); dict.print(); &#125; cout &lt;&lt; "erase: " &lt;&lt; endl; for (int i = 4; i &lt; 8; i++) &#123; dict.erase(2 * i); dict.print(); &#125; return 0;&#125; 分析：插入过程如下图 1234567891011121314insert:[0]0:0 -&gt; | [1] | [2] | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]0:0 -&gt; | [1] | [2]2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]0:0 -&gt; | [1]4:2 -&gt; | [2]2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]6:3 -&gt; 0:0 -&gt; | [1]4:2 -&gt; | [2]2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]6:3 -&gt; 0:0 -&gt; | [1]4:2 -&gt; | [2]8:4 -&gt; 2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]6:3 -&gt; 0:0 -&gt; | [1]10:5 -&gt; 4:2 -&gt; | [2]8:4 -&gt; 2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt; | [1]10:5 -&gt; 4:2 -&gt; | [2]8:4 -&gt; 2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt; | [1]10:5 -&gt; 4:2 -&gt; | [2]14:7 -&gt; 8:4 -&gt; 2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |erase:[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt; | [1]10:5 -&gt; 4:2 -&gt; | [2]14:7 -&gt; 2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]12:6 -&gt; 6:3 -&gt; 0:0 -&gt; | [1]4:2 -&gt; | [2]14:7 -&gt; 2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]6:3 -&gt; 0:0 -&gt; | [1]4:2 -&gt; | [2]14:7 -&gt; 2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |[0]6:3 -&gt; 0:0 -&gt; | [1]4:2 -&gt; | [2]2:1 -&gt; | [3] | [4] | [5] | [6] | [7] | [8] | [9] |]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】栈与队列]]></title>
    <url>%2F2020%2F02%2F21%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[栈定义及抽象数据描述定义：栈(stack)是一种特殊的线性表，其插入(aka. 入栈、压栈)和删除(aka. 出栈、弹栈)操作在表的同一端进行，即后进先出(last-in-first-out, LIFO)。两端称作栈顶(top)、栈底(bottom)。 抽象数据描述：1234567891011抽象数据类型 stack&#123; 实例: linearList; 操作： empty(); // 判断是否为空 size(); // 返回栈内元素个数 top(); // 返回栈顶元素 pop(); // 出栈 push(x); // 入栈&#125; 链表描述继承自模板类ChainList编写模板类LinkedStack即可。 注：若构造的类为模板类，那么派生类不可以直接使用继承到的基类数据和方法，需要通过this指针使用。子类内存释放时，将自动调用父类析构函数。 1234567891011121314151617181920212223242526272829// linkedstack.h#pragma once#include &lt;iostream&gt;#include "arraylist.h"template&lt;typename T&gt;class LinkedStack: public ChainList&lt;T&gt;&#123;public: LinkedStack(): ChainList&lt;T&gt;() &#123;&#125; ~LinkedStack()&#123;&#125; T&amp; top() const &#123; checkStack(); return this-&gt;get(0); &#125; T&amp; pop() &#123; checkStack(); T value = this-&gt;get(0); this-&gt;erase(0); return value; &#125; void push(const T&amp; value) &#123; this-&gt;insert(0, value); &#125;private: void checkStack() const &#123; if (this-&gt;empty()) throw "The stack is empty()";&#125;&#125;; 主函数测试123456789101112131415161718int main(int argc, char** argv)&#123; LinkedStack&lt;int&gt; stack; // 入栈 for (int i = 0; i &lt; 5; i++) &#123; stack.push(3 * i); &#125; stack.print(); // 出栈 for (int i = 0; i &lt; 3; i++) &#123; stack.pop(); &#125; stack.print(); return 0;&#125; 1212 9 6 3 03 0 应用例：一辆从前至后各车厢标号混乱的火车，从轨道入口驶入，出口驶出，与轨道垂直方向分布$k$个缓冲轨道(holding track)。要求利用缓冲轨道将车厢进行排序。 实际上，此时缓冲轨道和入口轨道均可视作栈，均用LinkedStack表示。 也可考虑入口至出口为双向链表，另一解决方案。 两个功能性函数：1) 判断缓冲轨道是否为空；2) 寻找栈顶最大的缓冲轨道，返回其索引。1234567891011121314151617181920bool isHoldingTracksEmpty(const LinkedStack&lt;int&gt;* holdingTracks, int k = 3)&#123; for (int i = 0; i &lt; k; i++) &#123; if (holdingTracks[i].top() != -1) return false; &#125; return true;&#125;int findHoldingTracksWithMaxTop(const LinkedStack&lt;int&gt;* holdingTracks, int k = 3)&#123; int index = -1; int max = -1; for (int i = 0; i &lt; k; i++) &#123; int top = holdingTracks[i].top(); if (top &gt; max) &#123; index = i; max = top; &#125; &#125; return index;&#125; 算法说明如下：1) 依次将火车车厢装入缓冲轨道。注意在第$i$节标号为$s_i$的车厢选择缓冲轨道时，选择栈顶车厢标号$t_j$小于$s_i$且$|t_j - s_i|$最小的缓冲轨道$j$，这样的目的是，使缓冲轨道内数字从栈顶至栈底按从大到小的顺序排列，且尽可能连续。若找不到满足要求的轨道，进入2)； 12345678910111213int findHoldingTrack(const int num, const LinkedStack&lt;int&gt;* holdingTracks, int k = 3)&#123; // 寻找 `栈顶 &lt; 当前车厢标号num` 且 `最接近num` 的栈 int index = -1; int em = INT32_MAX; for (int i = 0; i &lt; k; i++) &#123; int e = holdingTracks[i].top() - num; if (e &lt; 0 &amp;&amp; abs(e) &lt; em) &#123; em = abs(e); index = i; &#125; &#125; return index;&#125; 2) 将所有缓冲轨道内的车厢按照从大到小的顺序，装回原车厢，进入3)； 12345678void putBackBins(LinkedStack&lt;int&gt;&amp; bins, LinkedStack&lt;int&gt;* holdingTracks, int k = 3)&#123; // 注意，此时插入车辆时，按从大到小排序 while (!isHoldingTracksEmpty(holdingTracks, k)) &#123; int itop = findHoldingTracksWithMaxTop(holdingTracks, k); bins.push(holdingTracks[itop].pop()); &#125;&#125; 3) 重新按1)对车厢进行排序，如此循环，直至排序完成，进入4)；4) 重复步骤2)一次，完成排序。 程序入口如下：12345678910111213141516171819202122232425262728293031323334353637383940414243LinkedStack&lt;int&gt; reSortBins(const LinkedStack&lt;int&gt;&amp; bins, int k = 3)&#123; int cnt = 0; LinkedStack&lt;int&gt;* holdingTracks = new LinkedStack&lt;int&gt;[k]; LinkedStack&lt;int&gt; bins_c(bins); // 初始化各栈，栈底置`-1` for (int i = 0; i &lt; k; i++) holdingTracks[i].push(-1); // 将链表内数据划入栈内，要求每个栈内从顶至底，标号从大到小 while(!bins_c.empty()) &#123; int sign = bins_c.pop(); int index = -1; while (true) &#123; // 打印当前车厢编号信息 cout &lt;&lt; "Time[" &lt;&lt; cnt++ &lt;&lt; "]: "; bins_c.print(); // 寻找 `栈顶 &lt; 当前车厢标号sign` 且 `最接近sign` 的栈 index = findHoldingTrack(sign, holdingTracks, k); if (index != -1) break; // 若未找到，将当前已拍好序的放回车辆 putBackBins(bins_c, holdingTracks, k); &#125; // 入栈 holdingTracks[index].push(sign); &#125; // 查看缓冲车道内排序 cout &lt;&lt; endl &lt;&lt; "Holding tracks: " &lt;&lt; endl; for (int i = 0; i &lt; k; i++) holdingTracks[i].print(); // 重新装回车辆 putBackBins(bins_c, holdingTracks, k); // 回收工作 delete[] holdingTracks; return bins_c;&#125; 测试主函数如下，输入581742936，排序后应输出1234567891234567891011121314int main(int argc, char** argv)&#123; LinkedStack&lt;int&gt; bins; int origin[9] = &#123; 5, 8, 1, 7, 4, 2, 9, 6, 3 &#125;; for (int i = 0; i &lt; 9; i++) bins.push(origin[9 - i - 1]); cout &lt;&lt; "origin: "; bins.print(); cout &lt;&lt; endl; LinkedStack&lt;int&gt; bins_c = reSortBins(bins, 3); cout &lt;&lt; endl &lt;&lt; "sorted: "; bins_c.print(); return 0;&#125; 输出如下123456789101112131415161718192021222324origin: 5 8 1 7 4 2 9 6 3Time[0]: 8 1 7 4 2 9 6 3Time[1]: 1 7 4 2 9 6 3Time[2]: 7 4 2 9 6 3Time[3]: 4 2 9 6 3Time[4]: 2 9 6 3Time[5]: 9 6 3Time[6]: 1 4 5 7 8 9 6 3Time[7]: 4 5 7 8 9 6 3Time[8]: 5 7 8 9 6 3Time[9]: 7 8 9 6 3Time[10]: 8 9 6 3Time[11]: 9 6 3Time[12]: 6 3Time[13]: 3Time[14]:Holding tracks:9 8 7 5 4 2 -16 1 -13 -1sorted: 1 2 3 4 5 6 7 8 9 队列定义及抽象数据描述定义：队列(queue)是一个线性表，其插入和删除操作分别在表的两端进行，即先进先出(first-in-first-out, FIFO)。插入的一端称队尾(back或rear)，删除元素的一端称为队首(front)。 抽象数据描述：123456789101112抽象数据类型 queue&#123; 实例: linearList; 操作： empty(); // 判断是否为空 size(); // 返回队列内元素个数 front(); // 返回队首元素 back(); // 返回队尾元素 pop(); // 删除队首元素 push(x); // 把元素加入队尾&#125; 链表描述同样的，继承自模板类ChainList编写模板类LinkedQueue即可。 123456789101112131415161718192021222324252627282930313233343536373839404142// linkedqueue.h#pragma once#include &lt;iostream&gt;#include "arraylist.h"template&lt;typename T&gt;class LinkedQueue : public ChainList&lt;T&gt;&#123;public: LinkedQueue() : ChainList&lt;T&gt;() &#123;&#125; ~LinkedQueue() &#123;&#125; T front() const &#123; checkStack(); return this-&gt;get(this-&gt;size() - 1); &#125; T back() const &#123; checkStack(); return this-&gt;get(0); &#125; T pop() &#123; checkStack(); T value = this-&gt;get(this-&gt;size() - 1); this-&gt;erase(this-&gt;size() - 1); return value; &#125; void push(const T&amp; value) &#123; this-&gt;insert(0, value); &#125; void print() &#123; for (int i = 0; i &lt; this-&gt;size(); i++) std::cout &lt;&lt; this-&gt;get(i) &lt;&lt; " "; std::cout &lt;&lt; std::endl; &#125;private: void checkStack() const &#123; if (this-&gt;empty()) throw "The queue is empty()"; &#125;&#125;; 主函数测试程序:123456789101112131415161718192021int main(int argc, char** argv)&#123; LinkedQueue&lt;int&gt; queue; // 入栈 for (int i = 0; i &lt; 5; i++) &#123; queue.push(3 * i); &#125; queue.print(); cout &lt;&lt; "front: " &lt;&lt; queue.front() &lt;&lt; endl &lt;&lt; "back: " &lt;&lt; queue.back() &lt;&lt; endl; // 出栈 for (int i = 0; i &lt; 3; i++) &#123; queue.pop(); &#125; queue.print(); return 0;&#125; 123412 9 6 3 0front: 0back: 1212 9 应用例：一辆从前至后各车厢标号混乱的火车，从轨道入口驶入，出口驶出，与轨道水平方向分布$k$个缓冲轨道(holding track)。要求利用缓冲轨道将车厢进行排序。 此时，出入轨道、缓冲轨道均视作队列。 两个功能性函数：1) 判断缓冲轨道是否为空；2) 寻找队首最小的缓冲轨道，返回其索引。123456789101112131415161718192021bool isHoldingTracksEmpty(const LinkedQueue&lt;int&gt;* holdingTracks, int k = 3)&#123; for (int i = 0; i &lt; k; i++) &#123; if (!holdingTracks[i].empty()) return false; &#125; return true;&#125;int findHoldingTracksWithMinFront(const LinkedQueue&lt;int&gt;* holdingTracks, int k = 3)&#123; int index = -1; int min = 11; for (int i = 0; i &lt; k; i++) &#123; if (holdingTracks[i].empty()) continue; int front = holdingTracks[i].front(); if (front &lt; min) &#123; index = i; min = front; &#125; &#125; return index;&#125; 算法说明如下：1) 依次将火车车厢装入缓冲轨道。注意在第$i$节标号为$s_i$的车厢选择缓冲轨道时，选择队尾标号$t_j$小于$s_i$且$|t_j - s_i|$最小的缓冲轨道$j$，这样的目的是，使缓冲轨道内数字从队首至队尾按从小到大的顺序排列，且尽可能连续。注意，必须留下一条轨道用于存放1号车厢。 12345678910111213int findHoldingTrack(const int num, const LinkedQueue&lt;int&gt;* holdingTracks, int k = 3)&#123; // 寻找 `队尾 &lt; 当前车厢标号num` 且 `最接近num` 的栈 int index = -1; int em = INT32_MAX; for (int i = 0; i &lt; k; i++) &#123; int e = holdingTracks[i].back() - num; if (e &lt; 0 &amp;&amp; abs(e) &lt; em) &#123; em = abs(e); index = i; &#125; &#125; return index;&#125;2) 将车厢放入出轨道，完成排序。 1234567891011void putIntoOutTrack(LinkedQueue&lt;int&gt;&amp; outTrack, LinkedQueue&lt;int&gt;* holdingTracks, int k = 3)&#123; // 缓冲车道的队首标记元素。 for (int i = 0; i &lt; k; i++) holdingTracks[i].pop(); // 注意，此时进入出轨道时时，按从小到大排序 while (!isHoldingTracksEmpty(holdingTracks, k)) &#123; int itop = findHoldingTracksWithMinFront(holdingTracks, k); outTrack.push(holdingTracks[itop].pop()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849LinkedQueue&lt;int&gt; reSortBins(const LinkedQueue&lt;int&gt;&amp; bins, int k = 3)&#123; int cnt = 0; LinkedQueue&lt;int&gt;* holdingTracks = new LinkedQueue&lt;int&gt;[k]; LinkedQueue&lt;int&gt; bins_c(bins); // 初始化各队列，列首置`-1`，其中一条置`11`，**用于存放`1号车`** for (int i = 0; i &lt; k; i++) &#123; if (i == 0) &#123; holdingTracks[i].push(11); continue; &#125; holdingTracks[i].push(-1); &#125; // 依次处理各车厢 while (!bins_c.empty()) &#123; int sign = bins_c.pop(); int index = -1; while (true) &#123; // 打印当前车厢编号信息 cout &lt;&lt; "Time[" &lt;&lt; cnt++ &lt;&lt; "]: "; bins_c.print(); // 寻找 `队尾 &lt; 当前车厢标号sign` 且 `最接近sign` 的栈 index = findHoldingTrack(sign, holdingTracks, k); if (index != -1) break; // 若未找到，存入`0`号轨道 holdingTracks[0].push(sign); sign = bins_c.pop(); &#125; // 入栈 holdingTracks[index].push(sign); &#125; // 查看缓冲车道内排序 cout &lt;&lt; endl &lt;&lt; "Holding tracks: " &lt;&lt; endl; for (int i = 0; i &lt; k; i++) holdingTracks[i].print(); // 排列到出轨道 LinkedQueue&lt;int&gt; bins_o; putIntoOutTrack(bins_o, holdingTracks, k); return bins_o;&#125; 1234567891011121314int main(int argc, char** argv)&#123; LinkedQueue&lt;int&gt; bins; int origin[9] = &#123; 5, 8, 1, 7, 4, 2, 9, 6, 3 &#125;; for (int i = 0; i &lt; 9; i++) bins.push(origin[9 - i - 1]); cout &lt;&lt; "origin: "; bins.print(); cout &lt;&lt; endl; LinkedQueue&lt;int&gt; bins_c = reSortBins(bins, 3); cout &lt;&lt; endl &lt;&lt; "sorted: "; bins_c.print(); return 0;&#125; 123456789101112131415161718origin: 5 8 1 7 4 2 9 6 3Time[0]: 5 8 1 7 4 2 9 6Time[1]: 5 8 1 7 4 2 9Time[2]: 5 8 1 7 4 2Time[3]: 5 8 1 7 4Time[4]: 5 8 1 7Time[5]: 5 8 1Time[6]: 5 8Time[7]: 5Time[8]:Holding tracks:5 1 119 6 3 -18 7 4 2 -1sorted: 9 8 7 6 5 4 3 2 1]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【数据结构】线性表]]></title>
    <url>%2F2020%2F02%2F17%2F%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E3%80%91%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[线性表数据结构定义：线性表(linear list)也称有序表(ordered list)，他的每一个实例都是元素的一个有序集合。对于形如$(e_0, e_1, \dots, e_{n-1})$的实例，$n$为有穷自然数表示线性表的长度或大小，$e_i$为线性表的元素，$i$是元素$e_i$的索引。可以认为$e_0$先于$e_1, \cdots, e_{n-1}$，除了这种先后关系外，线性表不再有其他关系。 线性表应具有以下操作： 创建/删除线性表 判断是否为空 确定长度或大小 按索引查找元素 给定元素确定索引 给定索引插入/删除元素 按顺序输出线性表元素 注意，在删除某元素时，包括查找和删除两步骤。采用数组描述时间复杂度为$O(1) + O(n)$，链表描述时间复杂度为$O(n) + O(1)$，故两者删除操作的时间复杂度相同都为$O(n)$。 描述方式数组和链表的区别整理如下： 数组静态分配内存，链表动态分配内存； 数组在内存中连续，链表不连续； 数组元素在栈区，链表元素在堆区； 数组利用下标定位，时间复杂度为O(1)，链表定位元素时间复杂度O(n)； 数组插入或删除元素的时间复杂度O(n)，链表的时间复杂度O(1)。 数组描述定义：数组描述(array representation)中，利用数组来存储线性表的元素。 映射关系：用于确定一个元素在线性表中的位置，如$location(i)=i, location(i)=arrayLength-i-1$等等； 删除/插入元素：若在指定索引$i$处删除/插入元素$e_{new}$，除记录/删除元素外，需将$e_i, e_{i+1}, \cdots$前移/后移； 创建数组类：a)确定数据类型，b)确定数组长度。可使用模板类解决问题a)，使用动态数组解决问题b)。 具体实现 注意：实现模板类时，成员函数的声明与实现需位于同一文件，即.h中需包含函数的实现。否则将出现连接错误LINK2019错误。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126// arraylist.h#pragma once#include &lt;iostream&gt;#include &lt;algorithm&gt;template&lt;typename T&gt;class ArrayList&#123;public: // 构造、拷贝构造、析构 ArrayList(int initSize = 10); ArrayList(const ArrayList&lt;T&gt;&amp;); ~ArrayList() &#123; delete[] m_TElements;&#125; // 数组信息 bool empty() const &#123; return m_iCount == 0; &#125; int size() const &#123; return m_iCount; &#125; // 元素获取、查询 T&amp; get (const int index) const; int indexOf(const T&amp; element) const; // 数组操作 void insert(const int index, const T&amp; e); void erase (const int index); // 输出 void print() const;private: void checkIndex(int index) const &#123; if (index &lt; 0 || index &gt; m_iCount - 1) throw "无效的索引"; &#125; void expend(); T* m_TElements; int m_iSize; int m_iCount;&#125;;// ************************************************template&lt;typename T&gt;ArrayList&lt;T&gt;::ArrayList(int initSize) : m_iSize(initSize), m_iCount(0)&#123; if (m_iSize &lt;= 0) throw "无效的数组长度"; m_TElements = new T[m_iSize];&#125;template&lt;typename T&gt;ArrayList&lt;T&gt;::ArrayList(const ArrayList&lt;T&gt;&amp; a)&#123; m_iSize = a.m_iSize; m_iCount = a.m_iCount; m_TElements = new T[m_iSize]; std::copy(a.m_TElements, a.m_TElements + m_iSize, m_TElements);&#125;template&lt;typename T&gt;T&amp; ArrayList&lt;T&gt;::get(const int index) const&#123; checkIndex(index); return m_TElements[index];&#125;template&lt;typename T&gt;int ArrayList&lt;T&gt;::indexOf(const T&amp; element) const&#123; int index = (int)(find(m_TElements, m_TElements + m_iSize, element) - m_TElements); // 若未找到，返回-1 return index == m_iSize - 1 ? -1 : index;&#125;template&lt;typename T&gt;void ArrayList&lt;T&gt;::insert(const int index, const T&amp; e)&#123; checkIndex(index); // 若数组已满，则倍增 if (m_iCount &gt;= m_iSize - 1) expend(); // 元素后移 std::copy(m_TElements + index, m_TElements + m_iCount, m_TElements + index + 1); // 保存元素，计数自增 m_TElements[index] = e; m_iCount++;&#125;template&lt;typename T&gt;void ArrayList&lt;T&gt;::erase(const int index)&#123; checkIndex(index); if (index &gt; m_iCount - 1) return; // 前移元素 std::copy(m_TElements + index + 1, m_TElements + m_iCount, m_TElements + index); // 计数自减 m_iCount--;&#125;template&lt;typename T&gt;void ArrayList&lt;T&gt;::print() const&#123; for (int i = 0; i &lt; m_iCount; i++) std::cout &lt;&lt; m_TElements[i] &lt;&lt; " "; std::cout &lt;&lt; std::endl;&#125;template&lt;typename T&gt;void ArrayList&lt;T&gt;::expend()&#123; int size = 2 * m_iSize; // 新辟内存空间，并复制 T* elements = new T[size]; std::copy(m_TElements, m_TElements + m_iSize, elements); // 释放原内存，重新指定 delete [] m_TElements; m_TElements = elements; // 数组信息 m_iSize = size;&#125; 主函数测试如下1234567891011121314151617int main()&#123; int size = 3; ArrayList&lt;int&gt; a1(size); for (int i = 0; i &lt; size; i++) &#123; a1.insert(i, 2 * i); a1.print(); &#125; for (int i = 0; i &lt; size; i++) &#123; a1.erase(0); a1.print(); &#125; return 0;&#125; 输出1234500 20 2 42 44 分析优点：a)很多线性表的操作可以调用C++ 方法实现；b)时间性能良好，方法indexOf，erase，insert最坏时间复杂度与表达大小为线性关系。缺点：空间利用率低，在expend方法中，需开辟两倍新的内存空间并赋值后，才释放原有空间。 解决空间需求的一个方式是，把所有线性表映射到一个足够大的数组array中，再用数组front与last作为数组索引，构造多重表。其中front[i]与last[i]分别指定第i个表a[i]的第一个元素位置与最后一个元素位置。 需注意元素操作时，各表间元素相互干扰。 链式描述定义：在链式描述中，对象实例的每一个元素都用一个单元或节点来描述，每个节点明确包含另一个节点的位置信息，该信息称作链(link)或指针(pointer)。 具体实现链表操作即可实现，注意内存的申请与释放。 首先，构造双向链表的节点12345678910111213141516// arraylist.htemplate&lt;typename T&gt;class ChainNode&#123;public: ChainNode() &#123; ptr = nullptr; prev = nullptr; next = nullptr; &#125; ChainNode(const T val) &#123; ptr = new T; *ptr = val; prev = nullptr; next = nullptr; &#125; ~ChainNode() &#123; if (ptr) delete ptr; &#125; void setVal(T val) &#123; if (!ptr) ptr = new T; *ptr = val;&#125; T&amp; getVal() &#123; if (!ptr) throw "invalid node"; return *ptr; &#125; ChainNode* prev; ChainNode* next; T* ptr;&#125;; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143// arraylist.htemplate&lt;typename T&gt;class ChainList&#123;public: // 构造、析构、拷贝构造 ChainList() &#123; m_cnHead = new ChainNode&lt;T&gt;; m_iCount = 0; &#125; ChainList(const ChainList&lt;T&gt;&amp;); ~ChainList(); // 数组信息 bool empty() const &#123; return !m_cnHead-&gt;ptr; &#125; int size() const &#123; return m_iCount; &#125; // 元素获取、查询 ChainNode&lt;T&gt;* getNode(int index) const; T&amp; get(const int index) const; int indexOf(const T&amp; value) const; // 数组操作 void insert(const int index, const T&amp; value); void erase(const int index); // 输出 void print() const;private: void checkIndex(int index) const &#123; if (index &lt; 0 || index &gt; m_iCount) throw "无效的索引";&#125; ChainNode&lt;T&gt;* m_cnHead; int m_iCount;&#125;;template&lt;typename T&gt;ChainList&lt;T&gt;::ChainList(const ChainList&lt;T&gt;&amp; list)&#123; m_cnHead = new ChainNode&lt;T&gt;; m_iCount = 0; ChainNode&lt;T&gt;* node = list.m_cnHead; for (int i = 0; i &lt; list.m_iCount &amp;&amp; node; i++) &#123; insert(i, node-&gt;getVal()); node = node-&gt;next; &#125;&#125;template&lt;typename T&gt;ChainList&lt;T&gt;::~ChainList()&#123; ChainNode&lt;T&gt;* node = m_cnHead; while (!node-&gt;ptr &amp;&amp; !node) &#123; ChainNode&lt;T&gt;* tmp = node-&gt;next; // 释放节点空间 node-&gt;~ChainNode(); delete node; node = tmp; &#125;&#125;template&lt;typename T&gt;ChainNode&lt;T&gt;* ChainList&lt;T&gt;::getNode(const int index) const&#123; checkIndex(index); // 根据索引，链式寻找节点 ChainNode&lt;T&gt;* node = m_cnHead; for (int i = 0; i &lt; index; i++)node = node-&gt;next; return node;&#125;template&lt;typename T&gt;T&amp; ChainList&lt;T&gt;::get(const int index) const&#123; return getNode(index)-&gt;getVal();&#125;template&lt;typename T&gt;int ChainList&lt;T&gt;::indexOf(const T&amp; value) const&#123; ChainNode&lt;T&gt;* node = m_cnHead; // 搜索 int index = 0; while (node-&gt;ptr &amp;&amp; node-&gt;getVal()!=value) &#123; node = node-&gt;next; index++; &#125; return index == m_iCount ? -1 : index;&#125;template&lt;typename T&gt;void ChainList&lt;T&gt;::insert(const int index, const T&amp; value)&#123; // 创建节点 ChainNode&lt;T&gt;* tmp = new ChainNode&lt;T&gt;(value); // 获取节点 ChainNode&lt;T&gt;* node = getNode(index); // 连接，注意头节点的处理 if (node-&gt;prev) &#123; node-&gt;prev-&gt;next = tmp; tmp-&gt;prev = node-&gt;prev; &#125; tmp-&gt;next = node; node-&gt;prev = tmp; if (index == 0) m_cnHead = tmp; m_iCount++;&#125;template&lt;typename T&gt;void ChainList&lt;T&gt;::erase(const int index)&#123; // 获取节点 ChainNode&lt;T&gt;* node = getNode(index); if (index == 0) &#123; m_cnHead = node-&gt;next; m_cnHead-&gt;prev = nullptr;&#125; // 连接，释放，注意头节点的处理 if(node-&gt;prev) node-&gt;prev-&gt;next = node-&gt;next; node-&gt;~ChainNode(); m_iCount--;&#125;template&lt;typename T&gt;void ChainList&lt;T&gt;::print() const&#123; ChainNode&lt;T&gt;* node = m_cnHead; while (node-&gt;ptr &amp;&amp; node-&gt;next) &#123; std::cout &lt;&lt; node-&gt;getVal() &lt;&lt; " "; node = node-&gt;next; &#125; std::cout &lt;&lt; std::endl;&#125; 主函数测试如下：1234567891011121314151617181920212223242526int main()&#123; // 创建链表 ChainList&lt;int&gt; c1; cout &lt;&lt; "empty(1): " &lt;&lt; c1.empty() &lt;&lt; endl; cout &lt;&lt; "size (1): " &lt;&lt; c1.size() &lt;&lt; endl; // 插入0, 2, 4 for (int i = 0; i &lt; 3; i++) &#123; c1.insert(i, 2 * i); c1.print(); &#125; cout &lt;&lt; "empty(2): " &lt;&lt; c1.empty() &lt;&lt; endl; cout &lt;&lt; "size (2): " &lt;&lt; c1.size() &lt;&lt; endl; cout &lt;&lt; "index of `2`: " &lt;&lt; c1.indexOf(2) &lt;&lt; endl; // 按顺序删除2, 0 for (int i = 0; i &lt; 2; i++) &#123; c1.erase(1 - i); c1.print(); &#125; cout &lt;&lt; "empty(3): " &lt;&lt; c1.empty() &lt;&lt; endl; cout &lt;&lt; "size (3): " &lt;&lt; c1.size() &lt;&lt; endl; return 0;&#125; 输出：123456789101112empty(1): 1size (1): 000 20 2 4empty(2): 0size (2): 3index of `2`: 10 44empty(3): 0size (3): 1 分析 内存空间 与数组描述的线性表不同，链式描述动态分配内存空间，故可较少expend操作时的内存占用。 由于使用指针进行内存维护，对于n个元素的双向线性表，该链表共申请$n(12+s)$字节内存空间(单个元素指针占用$4 \times 3$字节，数据存储占用$s$字节)。空间需求不是决定因素。 运行时间 链式描述的线性表时间复杂度为$\Theta(n)$，数组描述的为$\Theta(1)$。链表的插入位置是随机的，也即逻辑相邻的元素在空间上不一定相邻，这样导致很多高速缓存缺失。 考虑时间复杂度，可使用双向链表(doubly linked list)，用两个数据成员firstNode与lastNode分别指向链表的头部和尾部，若查询下标index大于size/2，则从末端进行查找。 指针的优点 操作灵活，且动态分配内存空间。 应用 箱子排序 对n个存放在链表中的数据进行排序，假设每个箱子为一个链表，其节点数介于$0~n$之间，初始状态每个箱子都是空的。 a) 逐个删除输入链表的首元素，并将删除节点分配至相应箱子的链表首位； b) 从最后一个箱子开始，逐个删除每个箱子的元素，并插入一个初始为空的链表首位。 添加代码 123456789101112131415161718192021222324252627282930313233343536373839404142template&lt;typename T&gt;ChainList&lt;int&gt; ChainList&lt;T&gt;::binsort(int range, T (*pFunc)(T))&#123; // 创建箱子(存放下标，节省内存，但时间多用于查询) ChainList&lt;int&gt;** bins = new ChainList&lt;int&gt;* [range]; for (int i = 0; i &lt; range; i++)&#123; bins[i] = new ChainList&lt;int&gt;; &#125; // 将数据划入箱子 for (ChainNode&lt;T&gt;* node = m_cnHead; node-&gt;ptr; node = node-&gt;next) &#123; T val = node-&gt;getVal(); int index = pFunc? (*pFunc)(val): val; bins[index]-&gt;insert(0, indexOf(val)); &#125; for (int i = 0; i &lt; range; i++) &#123; bins[i]-&gt;print(); &#125; // 重新整合顺序，存储于链表 ChainList&lt;int&gt; order; for (int i = 0; i &lt; range; i++) &#123; for (ChainNode&lt;int&gt;* node = bins[i]-&gt;m_cnHead; node-&gt;ptr; node = node-&gt;next) &#123; order.insert(order.size(), node-&gt;getVal()); &#125; delete bins[i]; &#125; delete bins; return order;&#125;template&lt;typename T&gt;ChainList&lt;T&gt; ChainList&lt;T&gt;::ordered(const ChainList&lt;int&gt; order)&#123; ChainList&lt;T&gt; output; int cnt = 0; for (ChainNode&lt;int&gt;* node = order.m_cnHead; node-&gt;ptr; node = node-&gt;next) &#123; output.insert(cnt++, get(node-&gt;getVal())); &#125; return output;&#125; 主程序测试： 1234567891011121314151617int main()&#123; cout &lt;&lt; "Hello World!" &lt;&lt; endl; ChainList&lt;int&gt; c; for (int i = 0; i &lt; 100; i++) &#123; c.insert(0, 53 * i % 50); &#125; c.print(); int(*pf1)(int) = f1; ChainList&lt;int&gt; order = c.binsort(50, pf1); ChainList&lt;int&gt; ordered = c.ordered(order); ordered.print(); return 0;&#125; 1247 44 41 38 35 32 29 26 23 20 17 14 11 8 5 2 49 46 43 40 37 34 31 28 25 22 19 16 13 10 7 4 1 48 45 42 39 36 33 30 27 24 21 18 15 12 9 6 3 0 47 44 41 38 35 32 29 26 23 20 17 14 11 8 5 2 49 46 43 40 37 34 31 28 25 22 19 16 13 10 7 4 1 48 45 42 39 36 33 30 27 24 21 18 15 12 9 6 3 00 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43 44 44 45 45 46 46 47 47 48 48 49 49 对于具有$m$级大小关系的$n$个数据，该排序算法需初始化$m$个空箱子。在箱子排序基础上进行改进，可得到基数排序。 基数排序 基于箱子排序，基数排序可在$\Theta(n)$时间内，对$0~n^c-1$之间的$n$个整数进行排序。具体算法为：按从低到高的位数顺序，按数据当前位数字大小进行箱子排序，整数范围位于$0~9$之间，故仅需初始化$10$个箱子即可。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【算法】程序性能分析与渐近记号]]></title>
    <url>%2F2020%2F02%2F16%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B8%8E%E6%B8%90%E8%BF%91%E8%AE%B0%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[程序性能程序性能(performance of a program)是指运行这个程序所需要的内存和时间的多少。 空间复杂度定义：空间复杂度(space complexity)是该程序的运行所需内存的大小。 程序需要的空间主要以以下部分组成 指令空间(instruction space) 是指编译后的程序指令所需要的存储空间，取决于以下因素 编译器； 编译选项； 目标计算机。 数据空间(data space)是所有常量和变量值所需要的存储空间，由 常量和简单变量所需要的存储空间； 动态数组和动态类实例等动态对象所需要的空间。 环境栈空间(environment stack space) 保存暂停的函数和方法在恢复运行时所需要的信息： 返回地址； 正在调用的函数的所有局部变量的值以及形式参数的值(仅对递归函数而言)。 程序所需要的空间可分成两部分 固定部分，独立于实例特征，包括上述指令空间、简单变量空间、常量空间等； 可变部分，由动态分配空间(某种程度上依赖实力特征)和递归栈空间(依赖实力特征)构成。 将空间需求的固定部分记作$c$，可变部分记作$S_p$，那么所需空间为$c + S_p(实例特征)$。 时间复杂度定义：时间复杂度(time complexity)是该程序的运行所需要的时间。 操作计数估算程序或函数的时间复杂度，一种方法是选择一种或多种关键操作，如加、乘、比较等，确定每一种操作的执行次数，注意操作计数忽视了其他操作。以选择排序和冒泡排序为例，选择排序与冒泡排序都需要$\sum_{i=1}^{n-1} i = n(n-1)/2$次比较和$3(n-1)$次移动。123456789101112131415161718192021222324252627template&lt;typename T&gt;void selectionSort(T a[], int n) &#123; for (int i = n - 1; i &gt;= 1; i--) &#123; // n - 1次 int index = i; for (int j = i - 1; j &gt;= 0; j--)&#123; // i次比较 if (a[j] &gt; a[index]) index = j; &#125; T temp = a[i]; // 3次移动 a[i] = a[index]; a[index] = temp; &#125;&#125;template&lt;typename T&gt;void bubbleSort(T a[], int n)&#123; for (int i = n - 1; i &gt;= 1; i--) &#123; // n - 1次循环 for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j + 1]) &#123; // i次比较 T temp = a[j]; // 3次移动 a[j] = a[j + 1]; a[j + 1] = temp; &#125; &#125; &#125;&#125; 操作计数不总是由实例特征唯一确定，如冒泡排序中，交换次数不仅依赖于实例特征$n$，还依赖于数组元素的具体值，所以就要估算最好、最坏和平均操作计数。 步数步数(step-count)方法中，将程序/函数的所有操作部分都进行统计。步数也是实例特征的函数，为确定程序的步数，必须先确定所采用的实例特征，如输入个数、输出个数、输入或输出的大小，不仅确定步数计算表达式中的变量，还确定了一步应该包含多少次计算。一步(a step)是独立于所选定实例特征的一个计算单位，例如$100$次乘法可视作一步，但$n$次乘法包含实例特征不能视作一步。 定义：程序步(a program step)可以大概定义为一个语法或语义上的程序片段，该片段执行时间独立于实例特征，例如下面语句中变量均独立于实例特征，可视作一个程序步：1return a + b + b * c + (a + b - c) / (a + b) + 4; 故而相同步数的程序可能差别很大； 步数与时间复杂度是同义词。 用剖析法计算程序步数时，记$s/e$为每条语句每次执行所需要的步数(steps pre execution)，$f$为语句的执行次数或频率，进行列表计算，例如对于冒泡排序 程序语句 s/e 频率 总步数 template\ / / / void bubbleSort(T a[], int n) { / / / for (int i = n - 1; i &gt;= 1; i—) { $1$ $(n - 1) + 1$ $n$ for (int j = 0; j &lt; i; j++) { $i+1$ $n-1$ $n(n-1)/2 + (n-1)$ if (a[j] &gt; a[j + 1]) { T temp = a[j]; a[j] = a[j + 1]; a[j + 1] = temp; } $i$ $n-1$ $n(n-1)/2$ }}} / / / 总计 / / $n^2 + n - 1$ 渐近记号当实例特征$n$很大($n \rightarrow \infty$)时，使用步数可以准确地预计运行时间的增长，以比较两个程序的性能。当$n \rightarrow \infty$时，最高次项在整个多项式中占主导，可省略$c_2 n + c_3$两项。渐近分析主要确定的时复杂函数中的最大项。 例如，对于任何具有形式$c_1 n^2 + c_3 + c_2 n + c_3$,($c_1 &gt; 0$且$c_1, c_2, c_3$为常数)的步数的程序，都有 \lim_{n \rightarrow \infty} \frac{c_2 n + c_3}{c_1 n^2} = 0 定义：令$p(n)$和$q(n)$是两个非负函数，那么当且仅当 \lim_{n \rightarrow \infty} \frac{q(n)}{p(n)} = 0称$p(n)$渐近地大于$q(n)$($p(n)$渐近地优于$q(n)$)，或$q(n)$渐近地小于$p(n)$；当且仅当任何一个都不是渐近大于另一个时，称$p(n)$渐近地等于$q(n)$。 步数分析中常见的项如下标，大小关系从上往下依次减小 项 名称 $1$ 常量 $\log n$ 对数 $n$ 线性 $n \log n$ $n$倍对数 $n^2$ 平方 $n^3$ 立方 $2^n$ 指数 $n!$ 阶乘 定义：渐近记法(asymptotic notation)描述的是大实例特征的时间或空间复杂度，用步数中渐近最大的一项的单位项来描述复杂度。几种常见的渐近记法如下 表示法 读法 含义 例 $f(n) = O(g(n))$ $f(n)$ is big O of $g(n)$ $f(n)$渐近小于或等于$f(n)$，$g(n)$是$f(n)$的上限 $100n^2 - 20 n + 3 = O(n^2) \neq O(n)$ $f(n) = \Omega(g(n))$ $f(n)$ is big omega of $g(n)$ $f(n)$渐近大于或等于$f(n)$，$g(n)$是$f(n)$的下限 $100n^2 - 20 n + 3 = \Omega(n^2) = \Omega(n) \neq \Omega(n^3)$ $f(n) = \Theta(g(n))$ $f(n)$ is big theta of $g(n)$ $f(n)$渐近等于$f(n)$，$g(n)$是$f(n)$的上限 $100n^2 - 20 n + 3 = \Theta(n^2) \neq \Theta(n) \neq \Theta(n^3)$ 注意$f(n) = O(g(n))$与$O(g(n)) = f(n)$含义不同。 仍以冒泡排序为例，总的时间复杂度为$\Theta(n^2)$。 程序语句 s/e 频率 总步数 template\ / / / void bubbleSort(T a[], int n) { / / / for (int i = n - 1; i &gt;= 1; i—) { $1$ $(n - 1) + 1$ $\Theta(n)$ for (int j = 0; j &lt; i; j++) { $i+1$ $n-1$ $\Theta(n^2)$ if (a[j] &gt; a[j + 1]) { T temp = a[j]; a[j] = a[j + 1]; a[j + 1] = temp; } $i$ $n-1$ $\Theta(n^2)$ }}} / / / 总计 / / $\Theta(n^2)$]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elastic Search]]></title>
    <url>%2F2020%2F02%2F13%2FElastic-Search%2F</url>
    <content type="text"><![CDATA[目录 Elasticsearch: 权威指南注意ES6.x 移除string，新增text及keyword，并且mappings中index参数为布尔值 移除missing 目录 基础入门 安装运行 特性 交互语句 数据操作 元数据 检查文档: HEAD 索引文档: PUT或POST 新建文档: PUT /URL/_creat或POST 更新文档: PUT 部分更新: POST /URL/_update 取回文档: GET 取回多个文档: GET /_mget 删除文档: DELETE 批量操作: POST /_bulk 搜索(可用请求体查询替代) 空搜索 多索引和多类型 指定查询搜索 结果分页 映射与分析 映射 简单域类型 查看映射: GET /_index/_mapping 自定义映射 更新映射: PUT /_index {“mappings”: {…} } 测试映射: GET /_index/_analyze 分析 请求体查询: GET /URL/_search {“query”: {…} } 几个重要的查询 match_all match/multi_match term/terms exists range 组合多个查询 must must_not should filter bool constant_score 验证查询: GET /URL/_search/_validate/query?explain 排序与相关性 排序: GET /URL/_search { “sort”: { … } } 按字段值排序 多级排序 多值字段排序 相关性: _score 索引管理 创建: PUT /_index settings number_of_shards/number_of_replicas analysis mappings 删除: DELETE /_index 深入搜索 结构化搜索 精确值查找: term 组合过滤器: bool 精确查找多个值: terms 范围: range 处理空值: exists 全文搜索 匹配查询: match 多词查询: match 组合查询: bool 语句权重: boost 控制分析: GET /URL/_index/_analyze { “field”: …, “text”: … } 多字段查询 多字段查询: multi_match 最佳字段(best_fields): dis_max 多数字段(most_fields): field.subField 跨字段(cross_fields): filed1, …, filedn _all字段: copy_to 近似匹配 短语匹配: match_phrase 结果集重新评分: rescore 相关词: shingles/Ngrams 部分匹配 前缀查询: prefix 前缀短语匹配: match_phrase_prefix 通配符查询: wildcard 正则表达式查询: regexp 控制相关度 聚合 尝试聚合 命令格式: GET /URL/_search { “aggs”: { “NAME”: { AGG_TYPE: { … } } } } 简单例子: terms, avg, max, min 限定聚合范围: query, global 直方图统计: histogram 统计量计算: extended_stats 按时间统计: date_histogram 过滤和聚合 查询过滤: “query”: { …, “filter”, … } 聚合过滤: “aggs”: { …, “filter”, … } 后过滤器: post_filter 多桶排序: order 内置排序: _count, _term, _key 按度量排序:自定义嵌套度量 “深度”度量排序:自定义嵌套更深的度量 近似聚合 统计去重后的数目: cardinality 设置精度: precision_threshold 优化速度: hash 百分位数度量: percentiles elasticsearch-py 基础入门 Elasticsearch: 权威指南 » 基础入门 » 集群内的原理 安装运行 注意JDK8仅支持Elasticsearch 6.x和Kibana 6.x 下载并解压 1234$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.0.tar.gz$ tar -zxvf elasticsearch-6.6.0.tar.gz$ wget https://artifacts.elastic.co/downloads/kibana/kibana-6.6.0-linux-x86_64.tar.gz$ tar -zxvf kibana-6.6.0-linux-x86_64.tar.gz 修改配置文件elasticsearch-6.6.0/config/elasticsearch.yml 主要修改的几条配置如下 123456789# ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:cluster.name: louishsu-application# ----------------------------------- Paths ------------------------------------# Path to directory where to store the data (separate multiple locations by comma):path.data: /home/louishsu/Downloads/elasticsearch-6.6.0/tmp/data# Path to log files:path.logs: /home/louishsu/Downloads/elasticsearch-6.6.0/tmp/logs 补齐文件夹 12$ mkdir tmp$ mkdir tmp/data tmp/logs 添加环境变量：将elasticsearch-6.6.0/bin/和kibana-6.6.0-linux-x86_64/bin/目录添加到环境变量${PATH}中 123456789# &gt;&gt;&gt; elasticsearch &gt;&gt;&gt;export ELASTICSEARCH_HOME=/home/louishsu/Downloads/elasticsearch-6.6.0export PATH=$PATH:$ELASTICSEARCH_HOME/bin# &lt;&lt;&lt; elasticsearch &lt;&lt;&lt;# &gt;&gt;&gt; kibana &gt;&gt;&gt;export KIBANA_HOME=/home/louishsu/Downloads/kibana-6.6.0-linux-x86_64export PATH=$PATH:$KIBANA_HOME/bin# &lt;&lt;&lt; kibana &lt;&lt;&lt; 运行ElasticSearch 12$ elasticsearch... 对http://localhost:9200/?pretty发送请求，可以得到如下输出 123456789101112131415161718$ curl 'http://localhost:9200/?pretty'&#123; "name" : "KFnpkyD", "cluster_name" : "louishsu-application", "cluster_uuid" : "Jc3kJRbKSWeqUvNepFWogQ", "version" : &#123; "number" : "6.6.0", "build_flavor" : "default", "build_type" : "tar", "build_hash" : "a9861f4", "build_date" : "2019-01-24T11:27:09.439740Z", "build_snapshot" : false, "lucene_version" : "7.6.0", "minimum_wire_compatibility_version" : "5.6.0", "minimum_index_compatibility_version" : "5.0.0" &#125;, "tagline" : "You Know, for Search"&#125; 运行Kibana 运行kibana，并通过http://localhost:5601访问 1$ kibana 特性ES是面向文档的，存储整个对象或文档，通过索引每个文档的内容使其可以被检索。同其他NoSQL一样，使用JSON作为文档序列化格式，例如一个user对象可以表示为1234567891011&#123; "email": "john@smith.com", "first_name": "John", "last_name": "Smith", "info": &#123; "bio": "Eco-warrior and defender of the weak", "age": 25, "interests": [ "dolphins", "whales" ] &#125;, "join_date": "2014/05/01"&#125; 一个 Elasticsearch 集群可以包含多个索引(指向一个或者多个物理分片的逻辑命名空间) ，相应的每个索引可以包含多个类型。 这些不同的类型存储着多个文档，每个文档又有多个属性，文档路径表示为/_index/_type/_id。 交互语句Elasticsearch请求和HTTP请求类似，由以下格式组成1curl -X &lt;VERB&gt; '&lt;PROTOCOL&gt;://&lt;HOST&gt;:&lt;PORT&gt;/&lt;PATH&gt;?&lt;QUERY_STRING&gt;' -d '&lt;BODY&gt;' 若希望得到head信息，添加-i，即curl -i -X&lt;... 表示为缩写格式(Kibana, Sense等控制台中使用)12&lt;VERB&gt; /&lt;PATH&gt;&lt;BODY&gt; 在kibana的dev tool中可将缩写格式快速转换为curl。 例如获取集群中文档的数目1234567curl -XGET 'http://localhost:9200/_count?pretty' -d '&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125;' 对应缩写格式为123456GET /_count&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; 其中各部件说明如下 部件 说明 VERB HTTP方法或谓词，GET, HEAD, PUT, POST, DELETE PROTOCOL http或https(ES前有https代理的情况下) HOST ES集群中节点的主机名，本地主机用localhost表示 PORT 主机运行ES服务的端口号，默认9200 PATH API的终端路径，/_index/_type/_id，，及_count, _search等 QUERY_STRING 查询字符串，如?pretty BODY JSON格式的请求体(如果需要) 数据操作元数据文档元数据包括 索引(_index)：因共同的特性被分组到一起的文档集合； 类别(_type)：共享一种相同的（或非常相似）模式的文档集合，定义“子分区”； 标识(_id)：文档的唯一标识，字符串。 检查文档: HEAD仅获得HTTP请求报头1HEAD /website/blog/123 1200 OK 索引文档: PUT或POST索引文档是指将文档存储，并可被索引，其基本命令格式为12345PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;&#123; "field": "value", ...&#125; 新建索引website，类别bolg，文档编号123，用命令PUT 123456PUT /website/blog/123&#123; "title": "My first blog entry", "text": "Just trying this out...", "date": "2014/01/01"&#125; 1234567&#123; "_index": "website", "_type": "blog", "_id": "123", "_version": 1, "created": true&#125; 不指定文档编号，也可自动生成_id，用命令POST 自动生成的 ID 是 URL-safe、 基于 Base64 编码且长度为20个字符的 GUID 字符串。 这些 GUID 字符串由可修改的 FlakeID 模式生成，这种模式允许多个节点并行生成唯一 ID ，且互相之间的冲突概率几乎为零。 123456POST /website/blog/&#123; "title": "My second blog entry", "text": "Still trying this out...", "date": "2014/01/01"&#125; 1234567&#123; "_index": "website", "_type": "blog", "_id": "AVFgSgVHUP18jI2wRx0w", "_version": 1, "created": true&#125; 新建文档: PUT /URL/_creat或POST若希望创建全新文档而不是覆盖原有文档 PUT命令中添加_creat，可指定_id 12345678PUT /website/blog/123/_create# 或者# PUT /website/blog/123?op_type=create&#123; "title": "My first blog entry", "text": "Just trying this out...", "date": "2014/01/01"&#125; 若文档_index/_type/_id已存在，则会返回409错误 12345678910111213141516171819&#123; "error": &#123; "root_cause": [ &#123; "type": "version_conflict_engine_exception", "reason": "[blog][123]: version conflict, document already exists (current version [1])", "index_uuid": "TEPTZxdTRi6aAXE-lRvvFQ", "shard": "0", "index": "website" &#125; ], "type": "version_conflict_engine_exception", "reason": "[blog][123]: version conflict, document already exists (current version [1])", "index_uuid": "TEPTZxdTRi6aAXE-lRvvFQ", "shard": "0", "index": "website" &#125;, "status": 409&#125; POST命令生成唯一_id 123456POST /website/blog/&#123; "title": "My third blog entry", "text": "Still trying this out...", "date": "2014/01/01"&#125; 1234567891011121314&#123; "_index" : "website", "_type" : "blog", "_id" : "G62WC3IBb2a50-3pYPp3", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 2&#125; 更新文档: PUT在ES中文档是不可变的，实际上更新现有文档是通过重建索引进行替换实现的123456PUT /website/blog/123&#123; "title": "My first blog entry", "text": "I am starting to get the hang of this...", "date": "2014/01/02"&#125; 1234567891011121314&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 4, "_primary_term" : 2&#125; 部分更新: POST /URL/_update在POST指令中添加_update选项，可接收文档的一部分内容，并进行更新123456POST /website/blog/3/_update&#123; "doc" : &#123; "date": "2020/01/02" &#125;&#125; 1234567891011121314&#123; "_index" : "website", "_type" : "blog", "_id" : "3", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 2&#125; 取回文档: GET 用GET指令取回文档 1GET /website/blog/123?pretty 12345678910111213141516# HTTP/1.1 200 OK# content-type: application/json; charset=UTF-8# content-length: 264&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "found" : true, "_source" : &#123; "title": "My first blog entry", "text": "Just trying this out...", "date": "2014/01/01" &#125;&#125; 只返回_source字段 1GET /website/blog/123/_source 12345&#123; "title": "My first blog entry", "text": "Just trying this out...", "date": "2014/01/01"&#125; 通过_source指定文档的部分信息，例如只返回title和text，忽略date 1GET /website/blog/123?_source=title,text 12345678910111213&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "_seq_no" : 0, "_primary_term" : 1, "found" : true, "_source" : &#123; "text" : "Just trying this out...", "title" : "My first blog entry" &#125;&#125; 取回多个文档: GET /_mget在GET命令中添加_mget选项，可取回多个文档，需接收docs参数指定文档元数据12345678910111213141516GET /_mget&#123; "docs" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : 3 &#125;, &#123; "_index" : "website", "_type" : "pageviews", "_id" : 1, "_source": "views" &#125; ]&#125; 对于同一索引或同一类型中的文档，可指定_index/_type/1234567GET /website/blog/_mget&#123; "docs" : [ &#123; "_id" : 3 &#125;, &#123; "_type" : "pageviews", "_id" : 1 &#125; ]&#125; 或者用ids参数指定_id列表1234GET /website/blog/_mget&#123; "ids" : ["3", "1"]&#125; 响应结果相同，均为123456789101112131415161718192021222324&#123; "docs" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "3", "_version" : 2, "_seq_no" : 1, "_primary_term" : 2, "found" : true, "_source" : &#123; "title" : "My first blog entry", "text" : "I am starting to get the hang of this...", "date" : "2020/01/02" &#125; &#125;, &#123; "_index" : "website", "_type" : "pageviews", "_id" : "1", "found" : false &#125; ]&#125; 删除文档: DELETE1DELETE /website/blog/123 1234567891011121314&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 2, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 2&#125; 若不存在，则返回如下，注意_version仍旧增加1234567891011121314&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 3, "result" : "not_found", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 2&#125; 批量操作: POST /_bulk在POST命令中添加_bulk选项，进行批量处理，每个子请求都是独立执行，因此某个子请求的失败不会对其他子请求的成功与否造成影响，基本格式为12345POST /_bulk&#123; action: &#123; metadata &#125;&#125;&#123; request body &#125;&#123; action: &#123; metadata &#125;&#125;&#123; request body &#125; action | action | 说明 | | —- | —- | | create | 如果文档不存在，那么就创建它 | | index | 创建一个新文档或者替换一个现有的文档 | | update | 部分更新一个文档 | | delete | 删除一个文档 | metadata指定_index, _type, _id request body 行由文档的 _source 本身组成—​文档包含的字段和值。它是 index 和 create 操作所必需的；它也是 update 操作所必需的，并且应该包含你传递给 update API 的相同请求体： doc 、 upsert 、 script 等等。 删除操作不需要 request body 行。 例如12345678POST /_bulk&#123; "delete": &#123; "_index": "website", "_type": "blog", "_id": "123" &#125;&#125; &#123; "create": &#123; "_index": "website", "_type": "blog", "_id": "123" &#125;&#125;&#123; "title": "My first blog post" &#125;&#123; "index": &#123; "_index": "website", "_type": "blog" &#125;&#125;&#123; "title": "My second blog post" &#125;&#123; "update": &#123; "_index": "website", "_type": "blog", "_id": "123", "_retry_on_conflict" : 3&#125; &#125;&#123; "doc" : &#123;"title" : "My updated blog post"&#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&#123; "took" : 203, "errors" : false, "items" : [ &#123; "delete" : &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 3, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 5, "_primary_term" : 2, "status" : 200 &#125; &#125;, &#123; "create" : &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 4, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 6, "_primary_term" : 2, "status" : 201 &#125; &#125;, &#123; "index" : &#123; "_index" : "website", "_type" : "blog", "_id" : "HK23C3IBb2a50-3pQPpQ", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 2, "status" : 201 &#125; &#125;, &#123; "update" : &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 5, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 7, "_primary_term" : 2, "status" : 200 &#125; &#125; ]&#125; 搜索(可用请求体查询替代)空搜索不指定任何查询条件的搜索1GET /_search 1234567891011121314151617181920212223242526272829303132333435363738&#123; "took" : 27, # 搜索还费时间(毫秒) "timed_out" : false, # 是否超时 "_shards" : &#123; # 主分片信息 "total" : 6, # 参与分片总数 "successful" : 6, # 成功个数 "skipped" : 0, # 跳过个数 "failed" : 0 # 失败个数 &#125;, "hits" : &#123; "total" : 9, # 匹配到的文档总数 "max_score" : 1.0, "hits" : [ # 文档列表 ... &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_score" : 1.0, # 文档与查询的匹配度 "_source" : &#123; "title" : "My updated blog post" &#125; &#125;, &#123; "_index" : "website", "_type" : "blog", "_id" : "G62WC3IBb2a50-3pYPp3", "_score" : 1.0, "_source" : &#123; "title" : "My third blog entry", "text" : "Still trying this out...", "date" : "2014/01/01" &#125; &#125;, ... ] &#125;&#125; 多索引和多类型假如当前存在索引gb, us，类型为user, tweet，希望查找文档，可进行多索引/多类型查询 搜索所有索引和类型 1GET /_search # 在所有索引中搜索所有类型 搜索指定索引 123GET /gb/_search # 在索引`gb`中搜索GET /gb,us/_search # 在索引`gb, us`中搜索GET /g*,u*/_search # 支持字符匹配 搜索指定类型 123GET /_all/user/_search # 在所有索引中搜索`user`类型，注意`_all`只能用于代表全部索引GET /gb/user/_search # 在索引`gb`中搜索`user`类型GET /gb/user,tweet/_search # 在索引`gb`中搜索`user, tweet`类型 指定查询搜索 在tweet类型中查询字段&quot;tweet&quot;包含&quot;elasticsearhc&quot;单词的所有文档 1GET /_all/tweet/_search?q=tweet:elasticsearch 在所有结果中查询字段&quot;name&quot;包含&quot;john&quot;且&quot;tweet&quot;字段中包含&quot;mary&quot;的文档， 1+name:john +tweet:mary 其URL编码如下 1GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary %2B为+，%3A为:，故上句中实际为 +前缀表示必须与查询条件匹配，-表示不一定与查询条件匹配。 在所有结果中查询&quot;name&quot;包含&quot;john&quot;, &quot;mary&quot;，&quot;date&quot;字段值大于&quot;2014-09-10&quot;，&quot;all&quot;字段包含&quot;aggregations&quot;或者&quot;geo&quot; 1+name:(john mary) +date:&gt;2014-09-10 +(aggregations geo) 相应编码为 1?q=%2Bname%3A(john+mary)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo) 结果分页用size和from参数指定分页尺寸 size: 每页显示结果条数，默认为10 from: 跳过的初始结果数目 例如12GET /_search?size=2 # 每页显示2条结果GET /_search?size=2&amp;from=1 # 每页显示2条结果，从第2条开始显示 相应分别为123456789101112131415161718192021222324252627282930&#123; ..., "hits" : &#123; "total" : 5, "max_score" : 1.0, "hits" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_score" : 1.0, "_source" : &#123; "title" : "My updated blog post" &#125; &#125;, &#123; "_index" : "website", "_type" : "blog", "_id" : "G62WC3IBb2a50-3pYPp3", "_score" : 1.0, "_source" : &#123; "title" : "My third blog entry", "text" : "Still trying this out...", "date" : "2014/01/01" &#125; &#125; ] &#125;&#125; 1234567891011121314151617181920212223242526272829303132&#123; ..., "hits" : &#123; "total" : 5, "max_score" : 1.0, "hits" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "G62WC3IBb2a50-3pYPp3", "_score" : 1.0, "_source" : &#123; "title" : "My third blog entry", "text" : "Still trying this out...", "date" : "2014/01/01" &#125; &#125;, &#123; "_index" : "website", "_type" : "blog", "_id" : "l5WYCHIBrXn2VHz0FgRn", "_score" : 1.0, "_source" : &#123; "title" : "My second blog entry", "text" : "Still trying this out...", "date" : "2014/01/01" &#125; &#125; ] &#125;&#125; 映射与分析映射映射(mapping)定义了类型中的域、每个域的数据类型和ES如何处理这些域，也用于配置与类型有关的元数据。 简单域类型ES支持如下简单域类型 字符串: text, keyword 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 查看映射: GET /_index/_mapping查看ES对索引website默认生成的mapping，1GET /website/_mapping 1234567891011121314151617181920212223242526272829303132&#123; "website" : &#123; #_index "mappings" : &#123; # 注意mapping是_index的属性 "blog" : &#123; # _type "properties" : &#123; # _type的各属性定义 "date" : &#123; "type" : "date", "format" : "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis" &#125;, "text" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125; &#125;, "title" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 自定义映射若需要为域自定义映射，允许执行下面操作 全文字符串域和精确字符串域的区别: index 可选以下3种 analyzed: 进行分析后，全文索引该域； not_analyzed: 精确索引，能够被搜索但不进行分析； no: 不对该域进行索引，不会被搜索到。 分析是指进行大小字母匹配、同义词替换等操作。 使用特定语言分析器: analyzer 默认使用standard分析器，其余可选内置分析器有whitespace、simple和english等 优化域以适应部分匹配 指定自定义数据格式 … 更新映射: PUT /_index {“mappings”: {…} }创建索引时，指定类型的映射；用/mapping为新类型增加映射，或为存在类型更新映射，但是不能修存在的类型映射，否则会破坏索引。同一索引内的所有类型(字段不冲突)共享相同的映射。 注意每个域可包含多个值，构成数组，但数组元素必须是相同类型的，如1"tag": ["search", "nosql"] 新建索引/gb并添加映射 12345678910111213141516171819202122PUT /gb &#123; "mappings": &#123; # gb索引内所有类型的映射 "tweet" : &#123; # 自定义映射名 "properties" : &#123; "tweet" : &#123; "type" : "text", "analyzer": "english" # 指定分析器 &#125;, "date" : &#123; "type" : "date" &#125;, "name" : &#123; "type" : "text", &#125;, "user_id" : &#123; "type" : "long" &#125; &#125; &#125; &#125;&#125; 为tweet新增文本域tag，已存在的域不能修改，故不必指定 12345678PUT /gb/_mapping/tweet&#123; "properties" : &#123; "tag" : &#123; "type" : "keyword" # 精确搜索 &#125; &#125;&#125; 更为复杂度，为对象添加内部域 123456789101112131415161718192021222324252627&#123; "gb": &#123; "tweet": &#123; "properties": &#123; "tweet": &#123; "type": "text" &#125;, "user": &#123; # 域user "type": "object", "dynamic": true, # 动态映射，详情查看 # https://www.elastic.co/guide/cn/elasticsearch/guide/current/dynamic-mapping.html#dynamic-mapping "properties": &#123; "id": &#123; "type": "text" &#125;, "gender": &#123; "type": "text" &#125;, "age": &#123; "type": "long" &#125;, "name": &#123; # 子域user.name "type": "object", "properties": &#123; "full": &#123; "type": "text" &#125;, "first":&#123; "type": "text" &#125;, "last": &#123; "type": "text" &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 测试映射: GET /_index/_analyze12345GET /website/_analyze&#123; "field": "title", "text": ["hello", "world"]&#125; 123456789101112131415161718&#123; "tokens" : [ &#123; "token" : "hello", "start_offset" : 0, "end_offset" : 5, "type" : "&lt;ALPHANUM&gt;", "position" : 0 &#125;, &#123; "token" : "world", "start_offset" : 6, "end_offset" : 11, "type" : "&lt;ALPHANUM&gt;", "position" : 101 &#125; ]&#125; 分析 Elasticsearch: 权威指南 » 基础入门 » 映射和分析 » 分析与分析器 请求体查询: GET /URL/_search {“query”: {…} }将查询语句传递给query参数 一个典型的结构如下123456789GET /_search&#123; "query": &#123; QUERY_NAME: &#123; ARGUMENT: VALUE, ..., &#125; &#125;&#125; 查询可分为有评分/无评分的查询，即查询/过滤 查询: 是一个评分的查询，除询问“是否匹配”，还需判断匹配程度 过滤: 简单询问“是否匹配”，结果是yes或no 通常的规则是，使用查询query语句来进行全文搜索或者其它任何需要影响相关性得分的搜索。除此以外的情况都使用过滤filters。 几个重要的查询match_all默认的查询方式，简单匹配所有文档1GET /_search 等价于123456GET /_search&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125; match/multi_match可用的标准查询，对任何字段进行全文搜索或精确查询12345678GET /_search&#123; "query": &#123; # 查询 "match": &#123; # 匹配 "tweet": "About Search" # "tweet"域出现"About Search" &#125; &#125;&#125; 在多个字段上执行相同的match查询，例如在&quot;title&quot;, &quot;text&quot;字段搜索出现&quot;blog&quot;的文档123456789GET /_search&#123; "query": &#123; # 查询 "multi_match": &#123; # 多字段查询 "query": "blog", # 查询内容 "fields": ["title", "text"] # 字段/域 &#125; &#125;&#125; term/terms用于精确匹配，可以是数字、时间、布尔或not_analyzed的字符串。如查询&quot;title&quot;域中包含&quot;third&quot;的文档12345678GET /_search&#123; "query": &#123; # 查询 "term": &#123; # 精确匹配 "title": "third" &#125; &#125;&#125; 允许指定多值的精确匹配。例如查询&quot;title&quot;域中包含&quot;first&quot;, &quot;third&quot;, &quot;second&quot;的文档12345678910GET /_search&#123; "query": &#123; "terms": &#123; "title": [ "first", "second", "third" ] &#125; &#125;&#125; exists查询指定字段中有值的文档，例如查询&quot;title&quot;字段不为空的文档12345678GET /_search&#123; "query": &#123; "exists": &#123; "field": "title" &#125; &#125;&#125; 查询指定字段中无值的文档，例如查询&quot;title&quot;字段为空的文档1234567891011121314GET /my_store/_search&#123; "query": &#123; "bool": &#123; "must_not": [ &#123; "exists": &#123; "field": "title" &#125; &#125; ] &#125; &#125;&#125; range查询落在指定区间内的数字或时间，允许的操作符有 gt: 大于 gte: 大于等于 lt: 小于 lte: 小于等于 如年龄域&quot;age&quot;不小于20且小于30的文档1234567891011GET /_search&#123; "query": &#123; # 查询 "range": &#123; # 范围查询 "age": &#123; # 年龄域 "gte": 20, # 不小于20 "lt" : 30, # 大于30 &#125; &#125; &#125;&#125; 组合多个查询例如要求查询满足下列条件的文档 &quot;title&quot;域包含&quot;how to make millions&quot; 不能匹配&quot;tag&quot;值为&quot;spam&quot;的文档 &quot;tag&quot;值是&quot;starred&quot;的文档最佳 日期&quot;date&quot;在&quot;2014-01-01&quot;之后的文档最佳 注意： 以下查询内需嵌套几个重要的查询 若一个查询内包含多个语句，用[]表示数组，且每个完整的查询包含{}，注意括号使用 must文档必须匹配给定条件，才能被匹配成功。12345"must": &#123; "match": &#123; "title": "how to make millions" &#125;&#125; must_not文档必须不匹配给定条件，才能被匹配成功。12345"must_not": &#123; "match": &#123; "tag": "spam" &#125;&#125; should用于修正每个文档的相关性得分，如果满足任意语句将增加_score，否则无影响。1234567891011121314"should": [ &#123; "match": &#123; "tag": "starred" &#125; &#125;, &#123; "range": &#123; "date": &#123; "gte": "2014-01-01" &#125; &#125; &#125;] filter必须匹配，以过滤模式进行，满足条件的文档被包含。考虑日期条件写入should会影响文档评分，将其更改为filter匹配1234567"filter": &#123; "range": &#123; "date": &#123; "gte": "2014-01-01" &#125; &#125;&#125; bool通过bool查询将多查询组合在一起123456789101112131415161718192021222324252627GET /_search&#123; "bool": &#123; "must": &#123; # "title"域包含"how to make millions" "match": &#123; "title": "how to make millions" &#125; &#125;, "must_not": &#123; # 不能匹配"tag"值为"spam"的文档 "match": &#123; "tag": "spam" &#125; &#125;, "should": &#123; # "tag"值是"starred"或日期"date"在"2014-01-01"之后的文档最佳 "match": &#123; "tag": "starred" &#125; &#125;, "filter": &#123; # 不想让日期影响文档评分，将日期条件写入`filter` "range": &#123; "date": &#123; "gte": "2014-01-01" &#125; &#125; &#125; &#125;&#125; constant_score将一个不变的常量评分应用于所有匹配的文档，经常用于只需执行只包含filter查询而没有其他查询(评分查询)的情况123456789101112GET /_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "term": &#123; "category": "ebooks" &#125; &#125; &#125; &#125;&#125; 验证查询: GET /URL/_search/_validate/query?explain例如一个错误的查询语句如下123456789101112GET /_search&#123; "query": &#123; # 正确查询语句如下 # "match": &#123; # "title": "third" # &#125; "title": &#123; "match": "third" &#125; &#125;&#125; 1234567891011121314151617&#123; "error": &#123; "root_cause": [ &#123; "type": "parsing_exception", "reason": "Unknown key for a VALUE_STRING in [query].", "line": 3, "col": 5 &#125; ], "type": "parsing_exception", "reason": "Unknown key for a VALUE_STRING in [query].", "line": 3, "col": 5 &#125;, "status": 400&#125; 通过_validate/query验证语句是否合法 12345678GET /_search/_validate/query&#123; "query": "title": &#123; "match": "third" &#125; &#125;&#125; 123&#123; "valid" : false&#125; 添加参数explain输出错误原因 12345678GET /_search/_validate/query?explain&#123; "query": "title": &#123; "match": "third" &#125; &#125;&#125; 1234&#123; "valid" : false, "error" : "org.elasticsearch.common.ParsingException: [_na] query malformed, must start with start_object"&#125; 排序与相关性排序: GET /URL/_search { “sort”: { … } }可以将排序语句与搜索语句放入一个GET请求中，即对搜索结果进行排序，例如123456789GET /URL/_search&#123; "query": &#123; ... &#125;, "sort": &#123; ... &#125;&#125; 按字段值排序如根据&quot;date&quot;域排序，将最新的文档放在最前12345678GET /website/_search&#123; "sort": &#123; "date": &#123; "order": "desc" &#125; &#125;&#125; 若不指定顺序，默认升序排序12345678910111213GET /website/_search&#123; "sort": "date"&#125;# 同GET /website/_search&#123; "sort": &#123; "date": &#123; "order": "asc" &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 5, "max_score" : null, # 在排序中，`_score`无意义，故未计算；将track_scores参数设置为true也可计算 "hits" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "3", "_score" : null, # 在排序中，`_score`无意义，故未计算；将track_scores参数设置为true也可计算 "_source" : &#123; "title" : "My first blog entry", "text" : "I am starting to get the hang of this...", "date" : "2020/01/02", "index" : 1 &#125;, "sort" : [ 1577923200000 # 时间戳，自epoch(January 1, 1970 00:00:00 UTC)以来的毫秒数 ] &#125;, ..., &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_score" : null, "_source" : &#123; "title" : "My updated blog post" &#125;, "sort" : [ -9223372036854775808 ] &#125;, ..., ] &#125;&#125; 多级排序结合&quot;date&quot;和&quot;_source&quot;进行排序，匹配结果先以日期降序排序，再按索引升序排序1234567891011GET /website/_search&#123; "sort": [ &#123; "date": "desc" &#125;, &#123; "index": "asc" &#125; ]&#125; 多值字段排序若指定排序字段包含多个值，这些值没有顺序关系，仅仅是被包装，那么可以通过参数mode将多值处理后进行排序，支持min, max, avg, sum。例如统计各班平均分并按降序排序123456789GET /_search&#123; "sort": &#123; "score": &#123; "order": "desc", "mode": "avg" &#125; &#125;&#125; 相关性: _scoreES的文档相似度用TF/IDF定义，包括 词频：检索词在字段中出现的频率，出现频率越高相关性越高； 反文档频率：检索词在索引中出现的频率，出现频率越高相关性越低； 字段长度准则：字段长度越长，相关性越低。 索引管理创建: PUT /_index希望在索引数据前，分析器和映射已经被建立，那么可以手动创建索引，在请求体中传入设置或类型映射 在config/elasticsearch.yml中指定action.auto_create_index: false可以禁止自动创建索引 123456789PUT /index&#123; "settings": &#123; ... any settings ... &#125;, "mappings": &#123; "type_one": &#123; ... any mappings ... &#125;, "type_two": &#123; ... any mappings ... &#125;, ... &#125;&#125; settingsnumber_of_shards/number_of_replicas有连个重要的设置参数 number_of_shards: 每个索引的主分片数目，默认为5； number_of_replicas: 每个主分片的副本数，默认为1。 1234567PUT /index&#123; "settings": &#123; "number_of_shards": 1 "number_of_replicas": 0 &#125;&#125; 可以动态修改副本数1234PUT /index/_settings&#123; "number_of_replicas": 1&#125; analysis设置本索引内的分析器，可以配置为已存在的分析器或自定义，默认情况下为standard分析器，包含以下部分 standard分词器，通过单词边界分割输入的文本； standard词汇单元过滤器，整理分词器触发的词汇单元； lowercase词汇单元过滤器，转换所有词汇单元为小写； stop词汇单元过滤器，删除停用词(默认禁用)。 自定义西语分析器方式如下12345678910111213PUT /spanish_docs&#123; "settings": &#123; # 设置 "analysis": &#123; # 分析器设置 "analyzer": &#123; # 分析器 "es_std": &#123; # 分析器名 "type": "standard",# 使用standard分析器 "stopwords": "_spanish_"# 设置西语停用词过滤器 &#125; &#125; &#125; &#125;&#125; 创建一个自定义分析器见详细文档说明。 mappings详情查看更新映射: PUT /_index {“mappings”: {…} } 删除: DELETE /_index 删除指定索引 1DELETE /index 删除多个索引 12DELETE /index_1,index_2DELETE /index_* 删除全部索引 12DELETE /_allDELETE /* 深入搜索结构化搜索存储以下数据123456789POST /my_store/products/_bulk&#123; "index": &#123; "_id": 1 &#125;&#125;&#123; "price" : 10, "productID" : "XHDK-A-1293-#fJ3", "from": null, "count": 10 &#125;&#123; "index": &#123; "_id": 2 &#125;&#125;&#123; "price" : 20, "productID" : "KDKE-B-9947-#kL5" &#125;&#123; "index": &#123; "_id": 3 &#125;&#125;&#123; "price" : 30, "productID" : "JODL-X-1937-#pV7" &#125;&#123; "index": &#123; "_id": 4 &#125;&#125;&#123; "price" : 30, "productID" : "QQPX-R-3956-#aD8" &#125; 精确值查找: term 精确查找时，会使用过滤器filter； 不希望对查询进行评分计算，使用constant_score查询； 精确查询文本时，该域mappings需设置为not_analyzed； 理论上非评分查询先于评分查询执行，减少评分计算降低计算成本。 删除原有索引，并新建mapping，重新导入数据123456789101112PUT /my_store&#123; "mappings": &#123; "products": &#123; "properties": &#123; "productID": &#123; "type": "keyword" # 注意ES6.x移除`string` &#125; &#125; &#125; &#125;&#125; 精确搜索&quot;productID&quot;为&quot;XHDK-A-1293-#fJ3&quot;的文档12345678910111213GET /my_store/_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "term": &#123; "productID": "XHDK-A-1293-#fJ3" &#125; &#125;, "boost": 1.2 &#125; &#125;&#125; 组合过滤器: bool 查找价格为20或&quot;productID&quot;为&quot;XHDK-A-1293-#fJ3&quot;，但价格必定不为30的文档 12345678910111213141516171819202122232425262728GET /my_store/_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "bool": &#123; # 组合`should`x2 + `must_not` "should": [ &#123; "term": &#123; "price": 30 &#125; &#125;, &#123; "term": &#123; "productID": "XHDK-A-1293-#fJ3" &#125; &#125; ], "must_not": &#123; "term": &#123; "price": 30 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 查找满足&quot;productID&quot;为&quot;KDKE-B-9947-#kL5&quot;，或&quot;productID&quot;为&quot;JODL-X-1937-#pV7&quot;且价格为30的文档 123456789101112131415161718192021222324252627282930313233GET /my_store/_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "bool": &#123; # 组合`should`x2 "should": [ &#123; "term": &#123; "productID": "KDKE-B-9947-#kL5" &#125; &#125;, &#123; "bool": &#123; # `must`中为列表，将被展开，对每个元素求取布尔值，因此用`bool`将其包裹，即`must`x2 "must": [ &#123; "term": &#123; "productID": "JODL-X-1937-#pV7" &#125; &#125;, &#123; "term": &#123; "price": 30 &#125; &#125; ] &#125; &#125; ] &#125; &#125;, "boost": 1.2 &#125; &#125;&#125; 精确查找多个值: terms注意： 文档包含terms给出列表中的词就会被匹配，而不是等值； 若需精确相等，最好的方式是增加并索引另一个字段。 查找价格字段为20或30的文档 123456789101112131415GET /my_store/_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "terms": &#123; "price": [ 20, 30 ] &#125; &#125;, "boost": 1.2 &#125; &#125;&#125; 范围: range可用于限定数值、日期、字符串(字典序)范围进行查询 查找价格大于20小于40的文档 12345678910111213141516GET /my_store/_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "range": &#123; "price": &#123; "gt": 20, "lt": 40 &#125; &#125; &#125;, "boost": 1.2 &#125; &#125;&#125; 处理空值: existsnull, [], [null]是等价的，无法存于倒排索引中 查询&quot;count&quot;域不为空的文档 12345678910111213GET /my_store/_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "exists": &#123; "field": "count" &#125; &#125;, "boost": 1.2 &#125; &#125;&#125; 查询&quot;count&quot;域为空的文档 1234567891011121314151617GET /my_store/_search&#123; "query": &#123; "constant_score": &#123; "filter": &#123; "bool": &#123; # ES6.x移除missing "must_not": &#123; "exists": &#123; "field": "count" &#125; &#125; &#125; &#125;, "boost": 1.2 &#125; &#125;&#125; 全文搜索在全文字段中搜索到最相关的文档，两个最重要的方面是 相关性：评价查询与结果间的相关程度，可以是TF/IDF、地理位置邻近、模糊相似，或其他算法； 分析：将文本转换为有区别的、规范化的token，为了1)创建倒排索引；2)查询倒排索引。 索引一些数据1234567891011121314DELETE /my_index PUT /my_index&#123; "settings": &#123; "number_of_shards": 1 &#125;&#125; POST /my_index/my_type/_bulk&#123; "index": &#123; "_id": 1 &#125;&#125;&#123; "title": "The quick brown fox" &#125;&#123; "index": &#123; "_id": 2 &#125;&#125;&#123; "title": "The quick brown fox jumps over the lazy dog" &#125;&#123; "index": &#123; "_id": 3 &#125;&#125;&#123; "title": "The quick brown fox jumps over the quick dog" &#125;&#123; "index": &#123; "_id": 4 &#125;&#125;&#123; "title": "Brown fox brown dog" &#125; 匹配查询: match 查询&quot;title&quot;中包含&quot;quick&quot;的文档，&quot;title&quot;为&quot;text&quot;类型可被分析(&quot;quick&quot;同&quot;QUICK&quot;等) 12345678GET /my_index/_search&#123; "query": &#123; "match": &#123; "title": "quick" &#125; &#125;&#125; 多词查询: match 查询&quot;title&quot;中包含&quot;BROWN DOG!&quot;的文档 12345678GET /my_index/_search&#123; "query": &#123; "match": &#123; "title": "BROWN DOG!" &#125; &#125;&#125; 查询匹配所有词项的文档 1234567891011GET /my_index/_search&#123; "query": &#123; "match": &#123; "title": &#123; # 域的参数设置 "query": "BROWN DOG!", "operator": "and" &#125; &#125; &#125;&#125; 指定必须匹配的词项数 1234567891011GET /my_index/_search&#123; "query": &#123; "match": &#123; "title": &#123; # 域的参数设置 "query": "BROWN DOG!", "minimum_should_match": "75%" &#125; &#125; &#125;&#125; 组合查询: bool 匹配出现&quot;quick&quot;，但不出现&quot;lazy&quot;，最佳匹配&quot;brown&quot;, &quot;dog&quot;的文档 12345678910111213141516171819202122232425262728GET /my_index/_search&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "title": "quick" &#125; &#125; ], "must_not": [ &#123; "match": &#123; "title": "lazy" &#125; &#125; ], "should": [ &#123; "match": &#123; "title": "brown dog" &#125; &#125; ] &#125; &#125;&#125; 匹配精度控制 123456789101112131415GET /my_index/_search&#123; "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "title": "brown fox dog" &#125; &#125; ], "minimum_should_match": 1 &#125; &#125;&#125; 语句权重: boost希望为&quot;brown&quot;, &quot;quick&quot;提升更高的权重，即出现这些词的文档比未出现这些词的文档更匹配，可以用should语句如下1234567891011121314GET /my_index/_search&#123; "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "title": "brown quick" &#125; &#125; ] &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; ..., "hits" : &#123; "total" : 4, "max_score" : 0.7564473, "hits" : [ &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "2", "_score" : 0.7564473, "_source" : &#123; "title" : "The quick brown fox jumps over the lazy dog" &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "3", "_score" : 0.68324494, "_source" : &#123; "title" : "The quick brown fox jumps over the quick dog" &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "1", "_score" : 0.5753642, "_source" : &#123; "title" : "The quick brown fox" &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "4", "_score" : 0.2810996, "_source" : &#123; "title" : "Brown fox brown dog" &#125; &#125; ] &#125;&#125; 希望提升&quot;quick&quot;的权重多于&quot;brown&quot;12345678910111213141516171819202122232425GET /_search&#123; "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "title": &#123; # 修改域的参数 "query": "quick", "boost": 3 &#125; &#125; &#125;, &#123; "match": &#123; "title": &#123; "query": "brown", "boost": 2 &#125; &#125; &#125; ] &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; ..., "hits" : &#123; "total" : 4, "max_score" : 2.111807, "hits" : [ &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "2", "_score" : 2.111807, "_source" : &#123; "title" : "The quick brown fox jumps over the lazy dog" &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "3", "_score" : 1.7620528, "_source" : &#123; "title" : "The quick brown fox jumps over the quick dog" &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "1", "_score" : 1.4384105, "_source" : &#123; "title" : "The quick brown fox" &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "4", "_score" : 0.5621992, "_source" : &#123; "title" : "Brown fox brown dog" &#125; &#125; ] &#125;&#125; 控制分析: GET /URL/_index/_analyze { “field”: …, “text”: … }12345678GET /my_index/_analyze&#123; "field": "title", "text": [ "The quick brown fox jumps over the lazy dog", "The quick brown fox jumps over the quick dog" ]&#125; 多字段查询多字段查询: multi_match注意：将not_analyzed字段与multi_match中analyzed字段混在一起没有多大用处。在multi_match查询中避免使用not_analyzed字段。 在&quot;title&quot;和&quot;body&quot;字段搜索&quot;Quick brown fox&quot;，最低should匹配30%，可写作123456789101112131415161718192021222324GET /my_index/my_type/_search&#123; "query": &#123; "bool": &#123; # `should`x2 "should": [ &#123; "match": &#123; "body": &#123; "query": "Quick brown fox", "minimum_should_match": "30%" &#125; &#125; &#125;, &#123; "match": &#123; "title": &#123; "query": "Quick brown fox", "minimum_should_match": "30%" &#125; &#125; &#125; ] &#125; &#125;&#125; 可简写为 12345678910GET /my_index/my_type/_search&#123; "query": &#123; "multi_match": &#123; "query": "Quick brown fox", "fields": ["title", "body"], "minimum_should_match": "30%" &#125; &#125;&#125; 用^boost为&quot;body&quot;字段提升权重 12345678910GET /my_index/my_type/_search&#123; "query": &#123; "multi_match": &#123; "query": "Quick brown fox", "fields": ["title", "body^2"], "minimum_should_match": "30%" &#125; &#125;&#125; 若在&quot;book_title&quot;, &quot;chapter_title&quot;等字段中搜索，可将fields参数指定为 123456789GET /my_index/my_type/_search&#123; "query": &#123; "multi_match": &#123; "query": "Quick brown fox", "fields": "*_title" &#125; &#125;&#125; 最佳字段(best_fields): dis_max考虑以下两个文档1234567891011PUT /my_index/my_type/1&#123; "title": "Quick brown rabbits", "body": "Brown rabbits are commonly seen."&#125;PUT /my_index/my_type/2&#123; "title": "Keeping pets healthy", "body": "My quick brown fox eats rabbits on a regular basis."&#125; 这两个文档简单判断有如下特点 文档1两个字段中都包含&quot;brown&quot;，但不包含&quot;fox&quot;； 文档2的&quot;body&quot;字段包含&quot;brown&quot;, &quot;fox&quot;两个词 那么文档2匹配度可能会更高，当用match进行匹配&quot;Brown fox&quot;时，产生如下结果123456789GET /my_index/my_type/_search&#123; "query": &#123; "multi_match": &#123; # 多字段匹配 "query": "Brown fox", "fields": ["title", "body"] &#125; &#125;&#125; 12345678910111213141516171819202122232425..., &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "2", "_score" : 0.5753642, "_source" : &#123; "title" : "Keeping pets healthy", "body" : "My quick brown fox eats rabbits on a regular basis." &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "1", "_score" : 0.5753642, "_source" : &#123; "title" : "Quick brown rabbits", "body" : "Brown rabbits are commonly seen." &#125; &#125;,... 产生的结果是两者匹配度是一致的，这是因为bool计算评分方式如下 执行should语句中多个查询，得到各个查询的评分； 求取各个评分的均值作为文档查询评分。 可以采用分离最大化查询语句dis_max进行查询，将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回123456789101112131415GET /my_index/my_type/_search&#123; "query": &#123; "dis_max": &#123; # 分离最大化查询 "queries": [ # 查询列表 &#123; "multi_match": &#123; "query": "Brown fox", "fields": ["title", "body"] &#125; &#125; ] &#125; &#125;&#125; 123456789101112131415161718192021222324..., &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "2", "_score" : 0.5753642, "_source" : &#123; "title" : "Keeping pets healthy", "body" : "My quick brown fox eats rabbits on a regular basis." &#125; &#125;, &#123; "_index" : "my_index", "_type" : "my_type", "_id" : "1", "_score" : 0.2876821, "_source" : &#123; "title" : "Quick brown rabbits", "body" : "Brown rabbits are commonly seen." &#125; &#125;... 指定tie_breaker参数将其他匹配语句评分也考虑其中 获得最佳匹配语句的评分_score; 将其他匹配语句的评分结果与tie_breaker相乘; 对以上评分求和并规范化。 tie_breaker 可以是 0 到 1 之间的浮点数，其中 0 代表使用 dis_max 最佳匹配语句的普通逻辑， 1 表示所有匹配语句同等重要。最佳的精确值需要根据数据与查询调试得出，但是合理值应该与零接近（处于 0.1 - 0.4 之间），这样就不会颠覆 dis_max 最佳匹配性质的根本。 12345678910111213141516GET /my_index/my_type/_search&#123; "query": &#123; "dis_max": &#123; "queries": [ &#123; "multi_match": &#123; "query": "Brown fox", "fields": ["title", "body"] &#125; &#125; ], "tie_breaker": 0.3 &#125; &#125;&#125; 多数字段(most_fields): field.subField可以在某字段(域)添加子字段，如希望在查找时，用不同的分析器对某字段进行分析，定义_mappings1234567891011121314151617181920PUT /my_index&#123; "settings": &#123; "number_of_shards": 1 &#125;, "mappings": &#123; "my_type": &#123; "properties": &#123; "title": &#123; "type": "string", "analyzer": "english", "fields": &#123; "std": &#123; # 字段`title.std` "type": "string", "analyzer": "standard" &#125; &#125; &#125; &#125; &#125; &#125;&#125; 查询时可以对字段及其子字段进行查询，将type设置为most_fields，希望将所有匹配字段的评分合并起来，所以使用most_fields类型。这让multi_match查询用bool查询将两个字段语句包在里面，而不是使用dis_max查询。12345678910GET /my_index/_search&#123; "query": &#123; "multi_match": &#123; "query": "jumping rabbits", "type": "most_fields", "fields": [ "title", "title.std" ] &#125; &#125;&#125; 跨字段(cross_fields): filed1, …, filedn某些实体需要多个字段唯一标识信息，如地址123456&#123; "street": "5 Poland Street", "city": "London", "country": "United Kingdom", "postcode": "W1V 3DG"&#125; 用multi_match进行查询，并将type设置为most_fields123456789&#123; "query": &#123; "multi_match": &#123; "query": "Poland Street W1V", "type": "most_fields", "fields": [ "street", "city", "country", "postcode" ] &#125; &#125;&#125; _all字段: copy_to在创建索引mappings时，可将部分字段的值“复制”，如下，first_name和last_name的映射并不影响full_name如何被索引，full_name将两个字段的内容复制到本地，然后根据full_name的映射自行索引。 1234567891011121314151617181920PUT /my_index&#123; "mappings": &#123; "person": &#123; "properties": &#123; "first_name": &#123; "type": "string", "copy_to": "full_name" &#125;, "last_name": &#123; "type": "string", "copy_to": "full_name" &#125;, "full_name": &#123; "type": "string" &#125; &#125; &#125; &#125;&#125; 近似匹配短语匹配: match_phrase 找到彼此邻近搜索词的查询，例如匹配短语&quot;quick brown fox&quot; 12345678GET /my_index/my_type/_search&#123; "query": &#123; "match_phrase": &#123; "title": "quick brown fox" &#125; &#125;&#125; 添加slop参数近似短语匹配 词条相隔多远时仍然能将文档视为匹配,相隔多远的意思是为了让查询和文档匹配你需要移动词条多少次，通过设置一个像50或者100这样的高slop值能够排除单词距离太远的文档，但是也给予了那些单词临近的的文档更高的分数 1234567891011GET /my_index/my_type/_search&#123; "query": &#123; "match_phrase": &#123; "title": &#123; "query": "quick brown fox", "slop": 3 &#125; &#125; &#125;&#125; 用邻近度提高相关度 考虑到短语匹配需要全部词条出现在文中中，这要求显然过高，因为在7个词条中若6条匹配，那么相似度就很高了，所以可以将match_phrase放入should语句，以邻近度提升文档的相关度 123456789101112131415161718192021222324GET /my_index/my_type/_search&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "title": "dog" &#125; &#125; ], "should": [ &#123; "match_phrase": &#123; "title": &#123; "query": "quick brown fox", "slop": 3 &#125; &#125; &#125; ] &#125; &#125;&#125; 结果集重新评分: rescore用rescoreAPI对一次简单的match匹配结果顶部文档重新评分并排序，以给同时匹配短语查询的文档一个额外的相关度升级，例如123456789101112131415161718192021222324GET /my_index/my_type/_search&#123; "query": &#123; # match简单查询 "match": &#123; "title": &#123; "query": "quick brown fox", "minimum_should_match": "30%" &#125; &#125; &#125;, "rescore": &#123; # 重新评分 "window_size": 50, # 每一分片进行重新评分的顶部文档数量 "query": &#123; # 定制另一个查询进行重新评分 "rescore_query": &#123; "match_phrase": &#123; "title": &#123; "query": "quick brown fox", "slop": 50 &#125; &#125; &#125; &#125; &#125;&#125; 相关词: shingles/Ngrams用Ngrams进行匹配以提高相关性。 部分匹配创建索引如下123456789101112PUT /my_index&#123; "mappings": &#123; "address": &#123; "properties": &#123; "postcode": &#123; "type": "keyword" &#125; &#125; &#125; &#125;&#125; 索引一些数据1234567891011PUT /my_index/address/_bulk&#123;"index": &#123;"_id": 1&#125;&#125;&#123; "postcode": "W1V 3DG" &#125;&#123;"index": &#123;"_id": 2&#125;&#125;&#123; "postcode": "W2F 8HW" &#125;&#123;"index": &#123;"_id": 3&#125;&#125;&#123; "postcode": "W1F 7HW" &#125;&#123;"index": &#123;"_id": 4&#125;&#125;&#123; "postcode": "WC1N 1LZ" &#125;&#123;"index": &#123;"_id": 5&#125;&#125;&#123; "postcode": "SW5 0BE" &#125; 前缀查询: prefix查找所有以W1开始的邮编12345678910GET /my_index/_search&#123; "query": &#123; "prefix": &#123; "postcode": &#123; "value": "W1" &#125; &#125; &#125;&#125; 前缀短语匹配: match_phrase_prefix可通过前缀短语匹配查询，注意not_analyzed的字段不能被匹配成功。例如某文档存在字段&quot;info&quot;: &quot;somebody&#39;s home&quot;，可以通过如下查找12345678GET &#x2F;my_index&#x2F;address&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;info&quot;: &quot;somebody&#39;s h&quot; &#125; &#125;&#125; 通配符查询: wildcard允许指定匹配的正则式。它使用标准的shell通配符查询： ?匹配任意字符； *匹配0或多个字符。 匹配包含W1F 7HW和W2F 8HW的文档12345678GET /my_index/address/_search&#123; "query": &#123; "wildcard": &#123; "postcode": "W?F*HW" &#125; &#125;&#125; 正则表达式查询: regexp只匹配W区域所有邮编，但是通配符查询: wildcard中查询语句也会匹配WC开头得邮编，那么用正则表达式匹配可以处理这种复杂的情况12345678GET /my_index/address/_search&#123; "query": &#123; "regexp": &#123; "postcode": "W[0-9].+" &#125; &#125;&#125; 控制相关度聚合索引一些数据1234567891011121314151617POST /cars/transactions/_bulk&#123; "index": &#123;&#125;&#125;&#123; "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 30000, "color" : "green", "make" : "ford", "sold" : "2014-05-18" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 15000, "color" : "blue", "make" : "toyota", "sold" : "2014-07-02" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 12000, "color" : "green", "make" : "toyota", "sold" : "2014-08-19" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 80000, "color" : "red", "make" : "bmw", "sold" : "2014-01-01" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 25000, "color" : "blue", "make" : "ford", "sold" : "2014-02-12" &#125; 尝试聚合命令格式: GET /URL/_search { “aggs”: { “NAME”: { AGG_TYPE: { … } } } }聚合语句的典型命令格式如下，一个聚合语句内只包含一个AGG_TYPE，包含其余聚合语句时嵌套1234567891011121314151617GET /URL/_search&#123; "aggs": &#123; "NAME": &#123; "AGG_TYPE": &#123; ... &#125;, "aggs": &#123; "NAME": &#123; "AGG_TYPE": &#123; ... &#125; &#125; &#125; &#125; &#125;&#125; 简单例子: terms, avg, max, min 根据颜色分桶计数 1234567891011121314GET /cars/transactions/_search&#123; "size": 0, # ------------ 新增 ------------ "aggs": &#123; # 聚合语句，根据颜色进行分桶计数 "popular_colors": &#123; "terms": &#123; "field": "color" &#125; &#125; &#125; # ------------ 新增 ------------&#125; 得到错误 123456789101112&#123; "error": &#123; "root_cause": [ &#123; "type": "illegal_argument_exception", "reason": "Fielddata is disabled on text fields by default. Set fielddata=true on [color] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead." &#125; ], ..., &#125;, "status": 400&#125; 原因见ES报错：”illegal_argument_exception” - cnblogs，应修改为 1234567891011GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "popular_colors": &#123; "terms": &#123; "field": "color.keyword" &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324&#123; ..., "aggregations" : &#123; "popular_colors" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ # 分桶信息 &#123; "key" : "red", "doc_count" : 4 # 红色总计4辆 &#125;, &#123; "key" : "blue", "doc_count" : 2 &#125;, &#123; "key" : "green", "doc_count" : 2 &#125; ] &#125; &#125;&#125; 计算每种颜色车的价格均值 12345678910111213141516171819202122GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; # 聚合语句，根据颜色进行分桶计数 "popular_colors": &#123; "terms": &#123; "field": "color.keyword" &#125;, # ------------ 新增 ------------ # 嵌套聚合语句，在颜色分桶内计算均值 "aggs": &#123; "average_price_of_color": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125; # ------------ 新增 ------------ &#125; &#125;&#125; 123456789101112131415161718192021&#123; ..., "aggregations" : &#123; "popular_colors" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "red", "doc_count" : 4, # 红色总计4辆 "average_price_of_color" : &#123; "value" : 32500.0 # 红色分桶内价格均值 &#125; &#125;, ... ] &#125; &#125;&#125; 添加每种颜色车制造厂商信息 12345678910111213141516171819202122232425262728GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; # 聚合语句，根据颜色进行分桶计数 "popular_colors": &#123; "terms": &#123; "field": "color.keyword" &#125;, "aggs": &#123; # 嵌套聚合语句，在颜色分桶内计算均值 "average_price_of_color": &#123; "avg": &#123; "field": "price" &#125; &#125;, # ------------ 新增 ------------ # 嵌套聚合语句，在颜色分桶内统计制造厂商 "the_maker_of_the_car_of_this_color": &#123; "terms": &#123; "field": "make.keyword" &#125; &#125; # ------------ 新增 ------------ &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435&#123; ..., "aggregations" : &#123; "popular_colors" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "red", "doc_count" : 4, # 红色总计4辆 "the_maker_of_the_car_of_this_color" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ # 红色分桶内，按制造厂商分桶 &#123; "key" : "honda", "doc_count" : 3 # honda制造3辆 &#125;, &#123; "key" : "bmw", "doc_count" : 1 # bwm制造3辆 &#125; ] &#125;, "average_price_of_color" : &#123; "value" : 32500.0 # 红色分桶内价格均值 &#125; &#125;, ... ] &#125; &#125;&#125; 添加每种颜色、每种厂商制造的车的价格最高、最低价格信息 123456789101112131415161718192021222324252627282930313233343536373839404142GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; # 聚合语句，根据颜色进行分桶计数 "popular_colors": &#123; "terms": &#123; "field": "color.keyword" &#125;, "aggs": &#123; # 嵌套聚合语句，在颜色分桶内计算均值 "average_price_of_color": &#123; "avg": &#123; "field": "price" &#125; &#125;, # 嵌套聚合语句，在颜色分桶内统计制造厂商 "the_maker_of_the_car_of_this_color": &#123; "terms": &#123; "field": "make.keyword" &#125;, # ------------ 新增 ------------ "aggs": &#123; # 嵌套聚合语句的嵌套聚合语句，在颜色分桶、制造厂商子分桶内计算价格最小值 "minimal_price_of_this_color_of_this_maker": &#123; "min": &#123; "field": "price" &#125; &#125;, # 嵌套聚合语句的嵌套聚合语句，在颜色分桶、制造厂商子分桶内计算价格最大值 "maximal_price_of_this_color_of_this_maker": &#123; "max": &#123; "field": "price" &#125; &#125; &#125; # ------------ 新增 ------------ &#125; &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839&#123; ..., "aggregations" : &#123; "popular_colors" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "red", "doc_count" : 4, # 红色总计4辆 "the_maker_of_the_car_of_this_color" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ # 红色分桶内，按制造厂商分桶 &#123; "key" : "honda", "doc_count" : 3, # honda制造3辆 "maximal_price_of_this_color_of_this_maker" : &#123; "value" : 20000.0 # 红色车、honda制造，价格最大值 &#125;, "minimal_price_of_this_color_of_this_maker" : &#123; "value" : 10000.0 # 红色车、honda制造，价格最小值 &#125; &#125;, ... ] &#125;, "average_price_of_color" : &#123; "value" : 32500.0 # 红色分桶内价格均值 &#125; &#125;, ... ] &#125; &#125;&#125; 限定聚合范围: query, global在上面例子中，未进行query请求，所以默认是对所有文档进行聚合的。aggs可以和query同时使用，也就是说，聚合是基于我们查询匹配的文档集合进行计算的。 计算honda生产的汽车各颜色的价格的所有统计量 12345678910111213141516171819202122232425262728293031GET /cars/transactions/_search&#123; "size": 0, # ----- 查询honda汽车 ----- "query": &#123; "term": &#123; "make": &#123; "value": "honda" &#125; &#125; &#125;, # ----- 查询honda汽车 ----- "aggs": &#123; # ----- 以颜色分桶 ------ "color_of_cars": &#123; "terms": &#123; "field": "color.keyword" &#125;, "aggs": &#123; # -- 该颜色价格统计量 -- "all_stats_of_cars_with_this_color": &#123; "extended_stats": &#123; "field": "price" &#125; &#125; # -- 该颜色价格统计量 -- &#125; &#125; # ----- 以颜色分桶 ------ &#125;&#125; 但有时也想在获得子集的同时，得到所有对象的聚合结果，如下 统计honda汽车均值与所有汽车售价均值的对比，可以用查询获得子集计算普通聚合，再用全局桶获取全部汽车售价均值 123456789101112131415161718192021222324252627282930313233343536GET /cars/transactions/_search&#123; "size": 0, # ----- 查询honda汽车 ----- "query": &#123; "term": &#123; "make": &#123; "value": "honda" &#125; &#125; &#125;, # ----- 查询honda汽车 ----- "aggs": &#123; # ---- honda汽车均值 ---- "average_price_of_honda": &#123; "avg": &#123; "field": "price" &#125; &#125;, # ---- honda汽车均值 ---- # ----- 全部汽车统计 ---- "all_maker": &#123; "global": &#123;&#125;, # 全局桶，无参数 "aggs": &#123; # -- 全部汽车均值 --- "average_price_of_all_maker": &#123; "avg": &#123; "field": "price" &#125; &#125; # -- 全部汽车均值 --- &#125; &#125; # ----- 全部汽车统计 ---- &#125;&#125; 直方图统计: histogram 对价格进行直方图统计 以间隔20000对车价格进行直方图统计 123456789101112GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "histogram_of_price": &#123; "histogram": &#123; # 直方图统计 "field": "price", "interval": 20000 # 间隔20000 &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930&#123; ..., "aggregations" : &#123; "histogram_of_price" : &#123; "buckets" : [ &#123; "key" : 0.0, # 区间下界，代表0 ~ 19,999 "doc_count" : 3 &#125;, &#123; "key" : 20000.0, "doc_count" : 4 &#125;, &#123; "key" : 40000.0, "doc_count" : 0 &#125;, &#123; "key" : 60000.0, "doc_count" : 0 &#125;, &#123; "key" : 80000.0, "doc_count" : 1 &#125; ] &#125; &#125;&#125; 求取每个价格区间车辆总价 123456789101112131415161718192021GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "histogram_of_price": &#123; "histogram": &#123; # 直方图统计 "field": "price", "interval": 20000 # 间隔20000 &#125;, # ------------ 新增 ------------ "aggs": &#123; "sum_of_price_in_this_bin": &#123; "sum": &#123; # 求和 "field": "price" &#125; &#125; &#125; # ------------ 新增 ------------ &#125; &#125;&#125; 12345678910111213141516171819&#123; ..., "aggregations" : &#123; "histogram_of_price" : &#123; "buckets" : [ &#123; "key" : 0.0, "doc_count" : 3, "sum_of_price_in_this_bin" : &#123; "value" : 37000.0 &#125; &#125;, ... ] &#125; &#125;&#125; 统计量计算: extended_stats关于每个厂商车的价格所有统计量1234567891011121314151617181920GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "the_maker_of_cars": &#123; "terms": &#123; "field": "make.keyword" &#125;, # ----------- 新增 ----------- "aggs": &#123; "stats_of_price_of_this_maker": &#123; "extended_stats": &#123; "field": "price" &#125; &#125; &#125; # ----------- 新增 ----------- &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132&#123; ..., "aggregations" : &#123; "the_maker_of_cars" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "honda", "doc_count" : 3, "stats_of_price_of_this_maker" : &#123; "count" : 3, "min" : 10000.0, "max" : 20000.0, "avg" : 16666.666666666668, "sum" : 50000.0, "sum_of_squares" : 9.0E8, "variance" : 2.222222222222221E7, "std_deviation" : 4714.045207910315, "std_deviation_bounds" : &#123; "upper" : 26094.757082487296, "lower" : 7238.5762508460375 &#125; &#125; &#125;, ... ] &#125; &#125;&#125; 按时间统计: date_histogram 根据销售时间，统计每月售出数目和总价 1234567891011121314151617181920GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "sold_every_month": &#123; "date_histogram": &#123; # 按时间求直方图 "field": "sold", "interval": "month", # 按月 "format": "yyyy-MM-dd" # 指定时间格式(可选) &#125;, "aggs": &#123; "totally_sold_price_in_this_month": &#123; "sum": &#123; # 计算每个月售出总价 "field": "price" &#125; &#125; &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829&#123; ..., "aggregations" : &#123; "sold_every_month" : &#123; "buckets" : [ &#123; "key_as_string" : "2014-01-01", "key" : 1388534400000, "doc_count" : 1, "totally_sold_price_in_this_month" : &#123; "value" : 80000.0 &#125; &#125;, ..., &#123; "key_as_string" : "2014-11-01", "key" : 1414800000000, "doc_count" : 2, "total_sold_price" : &#123; "value" : 40000.0 &#125; &#125; ] &#125; &#125;&#125; 不忽略空桶 可以看到上面输出中并不包含2014-01-01 ~ 2014-12-31全年，因为统计为空的桶被忽略了，可指定如下两个参数强制返回空桶，并返回全年信息 123456789101112131415161718192021222324252627GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "sold_every_month": &#123; "date_histogram": &#123; "field": "sold", "interval": "month", "format": "yyyy-MM-dd", # ------- 新增 ------- "min_doc_count": 0, # 强制返回空桶 "extended_bounds": &#123; # 强制返回该日期范围内的桶 "min": "2014-01-01", "max": "2014-12-31" &#125; # ------- 新增 ------- &#125;, "aggs": &#123; "total_sold_price": &#123; "sum": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125;&#125; 按季度展示所有汽车品牌总销售额 123456789101112131415161718192021222324252627282930313233343536373839404142434445GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "sold_per_quarter": &#123; # --- 按季度统计直方图 --- "date_histogram": &#123; "field": "sold", "interval": "quarter", "format": "yyyy-MM-dd", "min_doc_count": 0, "extended_bounds": &#123; "min": "2014-01-01", "max": "2014-12-31" &#125; &#125;, "aggs": &#123; # --- 统计每个季度总销售额 --- "total_sold_in_this_quarter": &#123; "sum": &#123; "field": "price" &#125; &#125;, # --- 统计每个季度总销售额 --- # -------- 按品牌分箱 ------- "sold_of_maker": &#123; "terms": &#123; "field": "make.keyword" &#125;, # ---- 求该品牌总销售额 ---- "aggs": &#123; "sum_of_sold_price_of_this_maker": &#123; "sum": &#123; "field": "price" &#125; &#125; # ---- 求该品牌总销售额 ---- &#125; &#125; # -------- 按品牌分箱 ------- &#125; &#125; # --- 按季度统计直方图 --- &#125;&#125; 过滤和聚合查询过滤: “query”: { …, “filter”, … }在查询过程中设置过滤条件，同时影响搜索和聚合结果 售价在10000美元之上的汽车计算其均值 1234567891011121314151617181920212223GET /cars/transactions/_search&#123; "size": 0, "query": &#123; # 过滤查询 "constant_score": &#123; "filter": &#123; "range": &#123; "price": &#123; "gt": 10000 &#125; &#125; &#125; &#125; &#125;, # ------------------------ "aggs": &#123; "average_price_of_cars": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125;&#125; 聚合过滤: “aggs”: { …, “filter”, … }只想对聚合结果进行过滤时，在聚合语句aggs中设置过滤条件，影响聚合 统计honda汽车2014年下半年售出的车的均值，但是要求查询结果返回全部honda的文档 12345678910111213141516171819202122232425262728293031GET /cars/transactions/_search&#123; "query": &#123; "term": &#123; "make": &#123; "value": "honda" &#125; &#125; &#125;, # --------------------------------------- "aggs": &#123; "cars_sold_in_2014_M6_to_M12": &#123; "filter": &#123; "range": &#123; "sold": &#123; "from": "2014-07-01", "to": "2014-12-31" &#125; &#125; &#125;, # ----------------------------------- "aggs": &#123; "average_price_of_cars_sold_in_2014_M6_to_M12": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125;&#125; 后过滤器: post_filter用后过滤器(post_filter)只过滤搜索结果而不过滤聚合结果，只影响搜索结果 统计honda汽车2014年下半年售出的车均值，但是要求查询结果返回全部车信息 12345678910111213141516171819202122232425262728GET /cars/transactions/_search&#123; "size": 0, "query": &#123; "term": &#123; "make": &#123; "value": "honda" &#125; &#125; &#125;, # --------------------------------------- "post_filter": &#123; "range": &#123; "sold": &#123; "from": "2014-07-01", "to": "2014-12-31" &#125; &#125; &#125;, # --------------------------------------- "aggs": &#123; "average_price_of_cars_sold_in_2014_M6_to_M12": &#123; "avg": &#123; "field": "price" &#125; &#125; &#125;&#125; 多桶排序: order默认情况下，桶会根据doc_count降序排序，可以通过不同的处理方式修改桶的排序方式 内置排序: _count, _term, _key在聚合对象中引入order对象，可根据以下几个值进行排序 _count: 按文档排序，对terms, histogram, date_histogram有效； _term: 按词项的字符串值的字母排序，只对terms有效； _key: 按桶的键值数值排序，理论上与_term类似，只对histogram, date_histogram有效 例如，按车的颜色进行分桶，各桶按文档数升序排序12345678910111213141516GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "colors_of_cars": &#123; "terms": &#123; "field": "color.keyword", # ------------------ "order": &#123; "_term": "asc" &#125; # ------------------ &#125; &#125; &#125;&#125; 按度量排序:自定义嵌套度量可在order中传入自定义度量，该度量在聚合内定义，用aggs.metric进行访问 例如，按车的颜色进行分桶，各桶按价格的均值升序排序12345678910111213141516171819202122232425GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "colors_of_cars": &#123; "terms": &#123; "field": "color.keyword", # ----------------------------- "order": &#123; "stats_of_cars_in_this_color.variance": "asc" &#125; # ----------------------------- &#125;, # ------------------------------- "aggs": &#123; "stats_of_cars_in_this_color": &#123; "extended_stats": &#123; "field": "price" &#125; &#125; &#125; # ------------------------------- &#125; &#125;&#125; “深度”度量排序:自定义嵌套更深的度量在子聚合中定义孙聚合，用孙聚合的指标作为排序度量，用aggs&gt;aggs.metric访问，可定义更深的路径 例如，按价格分桶后，以各价格区间内红色、绿色车的价格方差降序排序12345678910111213141516171819202122232425262728293031323334353637GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "histogram_of_price": &#123; "histogram": &#123; "field": "price", "interval": 5000, # ----------------------------------- "order": &#123; "red_green_cars&gt;price_stats.variance": "desc" &#125; # ----------------------------------- &#125;, # ------------------------------------- "aggs": &#123; "red_green_cars": &#123; "filter": &#123; "terms": &#123; "color": [ "red","green" ] &#125; &#125;, # --------------------------------- "aggs": &#123; "price_stats": &#123; "extended_stats": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 近似聚合统计去重后的数目: cardinalitycardinality聚合可以确定某字段的值去重后的个数，例如：统计所有车颜色的种类个数12345678910111213GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; # --------------------------- "distinct_colors": &#123; "cardinality": &#123; "field": "color.keyword" &#125; &#125; # --------------------------- &#125;&#125; 123456789&#123; ..., "aggregations" : &#123; "distinct_colors" : &#123; "value" : 3 &#125; &#125;&#125; 设置精度: precision_threshold指定参数precision_threshold，定义在何种基数水平下我们希望得到一个近乎精确的结果，例如1234567891011121314GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "distinct_colors": &#123; "cardinality": &#123; "field": "color.keyword", # --------------------------- "precision_threshold": 100 # --------------------------- &#125; &#125; &#125;&#125; 优化速度: hash在mappings中将property.fields.hash设置为murmur3类型，预先计算而不是在统计时计算哈希值12345678910111213141516171819202122232425262728293031323334353637DELETE /cars/PUT /cars/&#123; "mappings": &#123; "transactions": &#123; "properties": &#123; "color": &#123; "type": "string", "fields": &#123; "hash": &#123; "type": "murmur3" &#125; &#125; &#125; &#125; &#125; &#125;&#125;POST /cars/transactions/_bulk&#123; "index": &#123;&#125;&#125;&#123; "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 30000, "color" : "green", "make" : "ford", "sold" : "2014-05-18" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 15000, "color" : "blue", "make" : "toyota", "sold" : "2014-07-02" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 12000, "color" : "green", "make" : "toyota", "sold" : "2014-08-19" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 80000, "color" : "red", "make" : "bmw", "sold" : "2014-01-01" &#125;&#123; "index": &#123;&#125;&#125;&#123; "price" : 25000, "color" : "blue", "make" : "ford", "sold" : "2014-02-12" &#125; 1234567891011GET /cars/transactions/_search&#123; "size" : 0, "aggs" : &#123; "distinct_colors" : &#123; "cardinality" : &#123; "field" : "color.hash" &#125; &#125; &#125;&#125; 百分位数度量: percentiles展现以具体百分比下观察到的数值，如第95各百分位上的数值，是高于95%的数据总和。常用于寻找异常数据，如应用于$3\sigma$准则 例如求各车价格的百分位数1234567891011121314GET /cars/transactions/_search&#123; "size": 0, "aggs": &#123; "percentile": &#123; "percentiles": &#123; "field": "price", "percents": [ 1, 5, 25, 50, 75, 95, 99 ] &#125; &#125; &#125;&#125; elasticsearch-py Docs » Python Elasticsearch Client - Elasticsearch 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# See more at https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch# 定义一些数据INDEX, TYPE = "cars", "transactions"TABLE = [ &#123; "price" : 10000, "color" : "red", "make" : "honda", "sold" : "2014-10-28" &#125;, &#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;, &#123; "price" : 30000, "color" : "green", "make" : "ford", "sold" : "2014-05-18" &#125;, &#123; "price" : 15000, "color" : "blue", "make" : "toyota", "sold" : "2014-07-02" &#125;, &#123; "price" : 12000, "color" : "green", "make" : "toyota", "sold" : "2014-08-19" &#125;, &#123; "price" : 20000, "color" : "red", "make" : "honda", "sold" : "2014-11-05" &#125;, &#123; "price" : 80000, "color" : "red", "make" : "bmw", "sold" : "2014-01-01" &#125;, &#123; "price" : 25000, "color" : "blue", "make" : "ford", "sold" : "2014-02-12" &#125;]# 创建ES对象from elasticsearch6 import Elasticsearches = Elasticsearch(["localhost:9200"])# 单条记录插入es.index(index=INDEX, doc_type=TYPE, id="0", body=TABLE[0])# 指定文档查找es.get(index=INDEX, doc_type=TYPE, id="0")# 指定文档更新body = &#123; "doc":&#123; "price": 15000, "color": "green" &#125;&#125;es.update(index=INDEX, doc_type=TYPE, id="0", body=body)# 单条记录删除es.delete(index=INDEX, doc_type=TYPE, id="0")# 索引批量数据，注意bulk输入格式body = []for i, doc in enumerate(TABLE): option = &#123; "index": &#123; "_index": INDEX, "_type": TYPE, "_id": str(i) &#125; &#125; body += [option, doc]print(body)es.bulk(index=INDEX, doc_type=TYPE, body=body)# 查询，例如：# 查找所有红色车中，车辆价格在15000以上，下半年售出的车最佳body = &#123; "query": &#123; "bool": &#123; "must": [ &#123; "term": &#123; "color": &#123; "value": "red" &#125; &#125; &#125;, &#123; "range": &#123; "price": &#123; "gte": 15000 &#125; &#125; &#125; ], "should": [ &#123; "range": &#123; "sold": &#123; "from": "2014-07-01", "to": "2014-12-31" &#125; &#125; &#125; ] &#125; &#125;&#125;es.search(index=INDEX, doc_type=TYPE, body=body)# 聚合，例如：# 按颜色分桶，计算每种颜色平均价格，统计每种颜色的制造厂商及其各颜色售价的最大、最小值body = &#123; "size": 0, "aggs": &#123; "popular_colors": &#123; "terms": &#123; "field": "color.keyword" &#125;, "aggs": &#123; "average_price_of_color": &#123; "avg": &#123; "field": "price" &#125; &#125;, "the_maker_of_the_car_of_this_color": &#123; "terms": &#123; "field": "make.keyword" &#125;, "aggs": &#123; "minimal_price_of_this_color_of_this_maker": &#123; "min": &#123; "field": "price" &#125; &#125;, "maximal_price_of_this_color_of_this_maker": &#123; "max": &#123; "field": "price" &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;es.search(index=INDEX, doc_type=TYPE, body=body)]]></content>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【算法】排序]]></title>
    <url>%2F2020%2F02%2F13%2F%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[目录 目录 前言 基于分配的排序 计数/箱子排序 桶排序 基数排序 基于交换的排序 冒泡排序 快速排序 数组实现 链表实现 基于选择的排序 简单选择排序 堆排序 基于插入的排序 直接插入排序 希尔排序 基于归并的排序 归并排序 数组实现 链表实现 性能对比 Reference 注：基于交换、选择、插入、归并的排序都是基于交换的排序。 前言排序算法的实际意义，除排序外，还可以拓展应用到其他问题的解决中，如： 利用快速排序对数组进行分区，使数组前半与后半元素满足不同性质(如奇偶性)； 利用快速排序查找第$k$大元素、中位数； 利用归并排序寻找数组中的逆序对； 利用桶排序求取数组中最大间距； …… 基于分配的排序计数/箱子排序箱子排序类似借助哈希表计数的方法，对数组内各元素进行计数存放在“箱子”中，然后从“箱子”收集元素形成有序数组。当待排序数组中数据范围较小时可考虑使用该方法 数据范围较大时，空间复杂度高且利用率可能较低，可借助哈希表。 1234567891011121314151617181920212223template&lt;typename T&gt;void binSort(std::vector&lt;T&gt;&amp; nums)&#123; if (nums.size() &lt; 2) return; // 寻找最小最大元素 T minVal = *std::min_element(nums.begin(), nums.end()); T maxVal = *std::max_element(nums.begin(), nums.end()); // 构筑“箱子”，时间O(n)，空间O(k) std::vector&lt;T&gt; bins(maxVal - minVal + 1); for (int i = 0; i &lt; nums.size(); i++) &#123; bins[nums[i] - minVal]++; &#125; // 收集元素，时间O(k) int k = 0; for (int i = 0; i &lt; bins.size(); i++) &#123; if (bins[i] == 0) continue; for (int j = 0; j &lt; bins[i]; j++) &#123; nums[k++] = i + minVal; &#125; &#125;&#125; 桶排序类似计数/箱子排序，不同之处是，每个桶(“箱子”)内存放一定范围的数，每个桶对应一个数字区间(桶间区间无重复且连续)以降低时间复杂度，并保证桶内数组有序，那么就有两个关键点： 元素值域的划分，即元素到桶的映射规则； 桶内元素的排序，使用其他排序。 桶排序作为排序算法不是特别实用，更为突出的是一种算法设计方法。 123456789101112131415161718192021222324252627template&lt;typename T&gt;void bucketSort(std::vector&lt;T&gt;&amp; nums)&#123; if (nums.size() &lt; 2) return; // 寻找最小最大元素 T minVal = *std::min_element(nums.begin(), nums.end()); T maxVal = *std::max_element(nums.begin(), nums.end()); // 构筑“桶”，间隔为`10` std::vector&lt;std::vector&lt;T&gt; &gt; bins((maxVal - minVal) / 10 + 1); for (int i = 0; i &lt; nums.size(); i++) &#123; // 映射 int index = (nums[i] - minVal) / 10; bins[index].push_back(nums[i]); &#125; // 桶内排序，并取出 int k = 0; for (int i = 0; i &lt; bins.size(); i++) &#123; std::sort(bins[i].begin(), bins[i].end()); while (!bins[i].empty()) &#123; auto it = bins[i].begin(); nums[k++] = *it; bins[i].erase(it); &#125; &#125;&#125; 基数排序基数排序也借助桶进行排序，只保留$10$个桶，存放$0 \sim 9$内的数字。对数组进行排序时，自个位开始向高位每位进行排序。注意基数排序为稳定排序，因此高位进行排序时，可以保证之前低位排序的数字顺序不改变。 12345678910111213141516171819202122232425262728293031323334353637383940template&lt;typename T&gt;int digit(T num, int d)&#123; int base = (int)std::pow(10, d); return num / base % 10;&#125;template&lt;typename T&gt;void radixSort(std::vector&lt;T&gt;&amp; nums)&#123; if (nums.size() &lt; 2) return; // 构筑“桶” std::vector&lt;std::vector&lt;T&gt; &gt; bins(10); int d = 0; // 位标 while (true) &#123; // 根据第`d`位，将数字放入桶 for (int i = 0; i &lt; nums.size(); i++) &#123; int index = digit(nums[i], d); bins[index].push_back(nums[i]); &#125; // 数字排序完毕 if (bins[0].size() == nums.size()) &#123; break; &#125; // 从桶中收集数字 int k = 0; for (int i = 0; i &lt; bins.size(); i++) &#123; // 每个桶有序取出 while (!bins[i].empty()) &#123; auto it = bins[i].begin(); // 首元素 nums[k++] = *it; // 存放到数组 bins[i].erase(it); // 删除 &#125; &#125; d++; &#125;&#125; 基于交换的排序冒泡排序冒泡排序是由一系列冒泡操作组成的，元素比较和移动发生在相邻元素间。一次冒泡结束后，未排序部分中的最大元素被移动到数组最右端，然后对小于该元素的部分，重复冒泡操作直至排序完成。 12345678910111213141516171819202122template&lt;typename T&gt;void bubbleSort(T a[], int n)&#123; if (a == nullptr || n &lt;= 0) return; // 当n-1个数排序后，最后一个数也排序完毕，故进行n-1次循环即可 for (int i = n - 1; i &gt;= 1; i--) &#123; // 一次冒泡操作，将最大数存储至索引为`i`处 bool moved = false; for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j + 1]) &#123; T temp = a[j]; a[j] = a[j + 1]; a[j + 1] = temp; moved = true; &#125; &#125; if (!moved) break; // 本次冒泡中无元素移动，提前结束 &#125;&#125; 快速排序快速排序是一种分治策略的排序算法。对$n$个数进行排序时 选择某个数(如最左端的数)为支点(pivot)； 从左至右搜索大于支点的元素，再从右至左搜索小于支点的元素，将上述两个元素交换； 重复2直至两端元素满足一端全部小于支点，一端全部大于支点； 将支点移动到两端元素中间，并分别对左右两端递归排序。 在最坏情况下，left总是等于begin，此时快速排序用时$\Theta(n^2)$；而最好情况下，left在(begin + end) / 2处，这时复杂度为$\Theta (n \log n)$；平均性能下，时间复杂度也为$\Theta(n \log n)$。 在某次partition完成后，优先处理较短序列可以减少程序的递归深度(系统栈保存的深度)，如果按长的递归优先的话，那么短的递归会一直保存在栈中，直到长的处理完。短的优先的话，长的递归调用没有进行，他是作为一个整体保存在栈中的，所以递归栈中的保留的递归数据少一些。但是栈深度(函数调用深度)与是否先处理短序列无关。 partition有三种实现方式： 利用左右双指针，左指针从pivot之后第1个元素开始，从左至右查找比pivot大的元素，右指针从最后一个元素开始从右至左查找比pivot小的元素，两者进行交换，循环直至指针相遇，然后将pivot放置到合适位置(左指针附近)； 12345678910111213141547 31 83 91 57 18 96 16 p i j 初始化&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;47 31 83 91 57 18 96 16 p i j i &lt; j 交换元素-----------------------47 16 83 91 57 18 96 31 p i j i &lt; j 交换元素-----------------------47 16 18 91 57 83 96 31 p j i i &gt; j 终止循环&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;18 16 47 91 57 83 96 31 p 移动pivot到i-1----------------------- 123456789101112131415161718192021222324252627282930313233template&lt;typename T&gt;int partition(T a[], int begin, int end)&#123; if (a == nullptr || begin &gt;= end) return -1; // 当前子数组的头元素视作支点 T pivot = a[begin]; // 左右索引 int left = begin, right = end - 1; while (true) &#123; // 搜索从左至右大于支点的元素a[left] while (a[left] &lt;= pivot &amp;&amp; left &lt; end) left++; // 搜索从右至左小于支点的元素a[right] while (a[right] &gt;= pivot &amp;&amp; right &gt; begin) right--; // 交换a[left]与a[right] if (left &lt; right) &#123; T temp = a[left]; a[left] = a[right]; a[right] = temp; &#125; else &#123; break; &#125; &#125; // 此时索引为`begin + 1 ~ left - 1`的数都小于支点，索引为`left ~ end - 1`的数都大于支点； // 将支点调整至left - 1处，使支点左侧元素都小于支点，右侧都大于支点 a[begin] = a[left - 1]; a[left - 1] = pivot; return left - 1;&#125; 利用快慢指针，快指针从左到右查找不大于pivot的元素，将慢指针向前移动一步后交换两个元素，循环直至快指针遍历整个数组，再将pivot移动至合适位置(慢指针附近)； 123456789101112131415161718192047 31 83 91 57 18 96 16p&#x2F;i,j &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;47 31 83 91 57 18 96 16p&#x2F;i j 快指针查找找不大于pivot的元素 47 18 83 91 57 31 96 16 p i j 慢指针向前一步，交换-----------------------47 18 83 91 57 31 96 16 p i j 快指针查找找不大于pivot的元47 18 31 91 57 83 96 16 p i j 慢指针向前一步，交换-----------------------47 18 31 91 57 83 96 16 p i j 快指针查找找不大于pivot的元47 18 31 16 57 83 96 91 p i j 慢指针向前一步，交换&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;16 18 31 47 57 83 96 91 p 移动pivot到i 1234567891011121314151617181920212223242526272829template&lt;typename T&gt;int partition2(T a[], int begin, int end)&#123; if (a == nullptr || begin &gt;= end) return -1; // 当前子数组的头元素视作支点 T pivot = a[begin]; // 快慢指针 int slow = begin, fast = begin; while (true) &#123; // 先走一步 fast++; if (fast == end) break; // 快指针不停的走，直到值遇到不大于支点的节点 if (a[fast] &lt;= pivot) &#123; // 慢指针跟上一步 slow++; // 交换 T temp = a[slow]; a[slow] = a[fast]; a[fast] = temp; &#125; &#125; // 支点放置到合适位置 a[begin] = a[slow]; a[slow] = pivot; return slow;&#125; 也利用左右双指针，首先初始化左指针、右指针为数组头尾，注意此时左指针元素即pivot。首先右指针从右至左查找找大于pivot的元素，将其与左指针元素交换，那么此时右指针元素即pivot，然后左指针从pivot开始，从左至右查找小于pivot的元素(包括pivot)，循环直至两指针相遇，判断是左指针超限或右指针超限以确定pivot当前位置索引。 1234567891011121314151617181920212223242526272847 31 83 91 57 18 96 16p&#x2F;i j 初始化&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;47 31 83 91 57 18 96 16p&#x2F;i j 右指针从右至左查找找大于pivot的元素16 31 83 91 57 18 96 47 i p&#x2F;j 与左指针元素交换-----------------------16 31 83 91 57 18 96 47 i p&#x2F;j 左指针从左至右查找小于pivot的元素16 31 47 91 57 18 96 83 p&#x2F;i j 与右指针元素交换-----------------------16 31 47 91 57 18 96 83 p&#x2F;i j 右指针从右至左查找找大于pivot的元素16 31 18 91 57 47 96 83 i p&#x2F;j 与左指针元素交换-----------------------16 31 18 91 57 47 96 83 i p&#x2F;j 左指针从左至右查找小于pivot的元素16 31 18 47 57 91 96 83 p&#x2F;i j 与右指针元素交换-----------------------16 31 18 47 57 91 96 83 j p&#x2F;i 右指针从右至左查找比pivot小的元素，结束循环&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;16 31 18 47 57 91 96 83 p 移动pivot已在合适位置处 1234567891011121314151617181920212223242526272829template&lt;typename T&gt;int partition3(T a[], int begin, int end)&#123; if (a == nullptr || begin &gt;= end) return -1; // 当前子数组的头元素视作支点 T pivot = a[begin]; // 左右指针 bool flag = true; // 指针移动标志 int left = begin, right = end - 1; while (true) &#123; // 指针顺序查找 if (flag) &#123; // 从右到左查找小于pivot的元素 while (right &gt;= 0 &amp;&amp; a[right] &gt;= pivot) right--; if (left &gt;= right) return left; &#125; else &#123; // 从左到右查找大于pivot的元素 while (left &lt; end &amp;&amp; a[left] &lt;= pivot) left++; if (left &gt;= right) return right; &#125; // 交换元素 T temp = a[left]; a[left] = a[right]; a[right] = temp; // 交换方向 flag = !flag; &#125;&#125; 数组实现12345678910111213141516171819template&lt;typename T&gt;void quickSort(T a[], int n)&#123; if (a == nullptr || n &lt;= 0) return; quickSort(a, 0, n);&#125;template&lt;typename T&gt;void quickSort(T a[], int begin, int end) // a[begin] ~ a[end - 1]&#123; if (a == nullptr || begin &gt;= end) return; if (end - begin &lt; 2) return; // 终止条件：仅包含1个数，无需排序 int pivot = partition(a, begin, end); // 任意一种均可 if (pivot == -1) return; // 划分子数组，递归 quickSort(a, begin, pivot); quickSort(a, pivot + 1, end);&#125; 链表实现链表的快排算法如下，大部分同数组快排无异，主要是分区函数。这里用节点地址判断是否到达子链表终点12345678910111213void quicksortList(ListNode* head) &#123; if (head == nullptr) return; quicksortList(head, nullptr);&#125;void quicksortList(ListNode* start, ListNode* end) &#123; if (start == end) return; // 分区 ListNode* pivot = partition(start, end); // 左右两侧递归 quicksortList(start, pivot); quicksortList(pivot-&gt;next, end);&#125; 由于单向链表无法获取父节点信息，故借助快慢指针12345678910111213141516171819202122232425262728293031ListNode* partition(ListNode* start, ListNode* end) &#123; // 支点 int pivot = start-&gt;val; // 借助快慢指针，将数据分区 ListNode* slow = start; // `slow`左侧(包含slow)的数据均小于等于`pivot` ListNode* fast = start; // `fast`的数据待判断 // 调整数据位置 while (fast != end) &#123; // 先走一步 fast = fast-&gt;next; if (fast == nullptr) break; // 快指针不停的走，直到值遇到不大于支点的节点 if (fast-&gt;val &lt;= pivot) &#123; // 赶紧跟上 slow = slow-&gt;next; // 交换 int temp = slow-&gt;val; slow-&gt;val = fast-&gt;val; fast-&gt;val = temp; &#125; &#125; // 支点放置到合适位置 start-&gt;val = slow-&gt;val; slow-&gt;val = pivot; // 慢指针即支点位置 return slow;&#125; 基于选择的排序简单选择排序选择排序是一种简单直观的排序算法。它的基本算法描述是： 从待排序的数据元素中选出最大的元素，存放在数组的末尾； 再从剩余的未排序元素中寻找到最大元素，然后放到已排序的序列的前端； 重复2直至排序完成。 交换次数比冒泡排序少，由于交换所需CPU时间比比较所需的CPU时间多，$n$较小时，选择排序比冒泡排序快。 123456789101112131415161718template&lt;typename T&gt;void selectionSort(T a[], int n) &#123; if (a == nullptr || n &lt;= 0) return; // 当n-1个数排序后，最后一个数也排序完毕，故进行n-1次循环即可 for (int i = n - 1; i &gt;= 1; i--) &#123; // 将索引为i的数与索引为0 ~ i-1的数进行比较，找到最大 int index = i; for (int j = i - 1; j &gt;= 0; j--) &#123; if (a[j] &gt; a[index]) index = j; &#125; // 交换 T temp = a[i]; a[i] = a[index]; a[index] = temp; &#125;&#125; 堆排序堆排序是利用堆设计的排序算法，是一种选择排序，不稳定，最坏，最好，平均时间复杂度均为$O(n\log n)$，堆排序的基本算法描述是： 将输入数组初始化为大根堆(完全二叉树的数组描述)：从最后一个非叶子节点开始调整，若该节点元素小于孩子节点元素，则交换位置并对该孩子节点递归调整； 将大根堆堆顶元素与数组的末尾元素交换，末尾元素加入堆顶递归调整，维护剩余元素组成的堆结构； 重复2直至排序完成。 建堆时间复杂度是$O(n)$，插入/删除一个元素的时间复杂度是$O(\log n)$，所以堆排序时间复杂度是$O(n \log n)$。 关于优先级队列的时间复杂度，可查看【数据结构】优先级队列。 123456789101112131415161718192021222324252627282930313233template&lt;typename T&gt;void heapSort(T a[], int n)&#123; if (a == nullptr || n &lt;= 0) return; // 初始化最大堆(需从最后一个节点开始) for (int i = n - 1; i &gt;= 0; adjust(a, n, i--)); // 当n-1个数排序后，最后一个数也排序完毕，故进行n-1次循环即可 for (int i = n - 1; i &gt;= 1; i--) &#123; // 堆顶元素依次出堆，与末尾元素交换 T temp = a[i]; a[i] = a[0]; a[0] = temp; // 调整堆顶元素位置 adjust(a, i, 0); &#125;&#125;template&lt;typename T&gt;void adjust(T a[], int n, int index) // 调整元素a[index]在最大堆中的位置&#123; if (a == nullptr || n &lt;= 0 || index &lt; 0 || index &gt;= n) return; // 判断元素与左右子节点元素的值大小关系，若不满足最大堆则递归调整位置 for (int c = 1; c &lt;= 2; c++) &#123; int child = 2 * index + c; // 左、右子节点的索引 if (child &gt;= n) return; // 无子节点，无需调整 if (a[index] &lt; a[child]) &#123; // 该节点不满足最大堆，进行“上浮”调整 T temp = a[index]; a[index] = a[child]; a[child] = temp; adjust(a, n, child); // 继续调整直至叶子节点 &#125; &#125;&#125; 借助STL标准库，上述代码可进行简化1234567891011121314#include &lt;vector&gt;#include &lt;algorithm&gt;template&lt;typename T&gt;void heapSort(std::vector&lt;T&gt;&amp; nums)&#123; if (nums.size() &lt; 2) return; for (int i = nums.size() - 1; i &gt;= 0; i--) &#123; // 剩余元素构筑最大堆 std::make_heap(nums.begin(), nums.begin() + i + 1, std::less&lt;T&gt;()); // 将堆顶元素放置到末尾 std::pop_heap(nums.begin(), nums.begin() + i + 1, std::less&lt;T&gt;()); &#125;&#125; 基于插入的排序直接插入排序直接插入排序的算法描述如下 对第$i(i &gt; 0)$个数进行排序时，前$0, \cdots, i - 1$的数字已排序； 在已排序数组中顺序查找(也可改进为二分折半查找)合适的插入位置，将该数字放入，并将该位置后的数字向后挪动； 重复上述操作直至所有数字排序完成。 直接插入排序在小规模数据时效率较高，且对越有序的数组，排序效率越高。 123456789101112131415161718192021222324template&lt;typename T&gt;void insertSort(std::vector&lt;T&gt;&amp; nums)&#123; if (nums.size() &lt; 2) return; for (int i = 0; i &lt; nums.size(); i++) &#123; int num = nums[i]; // 在`0, ..., i - 1`中寻找合适的插入位置 for (int j = 0; j &lt; i; j++) &#123; // `j`处大于`num`，为插入位置 if (nums[j] &gt; num) &#123; // 从后向前挪动 for (int k = i; k &gt; j; k--) &#123; nums[k] = nums[k - 1]; &#125; // 存放`num`在`j`处 nums[j] = num; break; &#125; &#125; &#125;&#125; 希尔排序希尔排序又称递减增量排序，是对直接选择排序的改进，使其对大规模数据或无序数组排序也高效。 注意到直接插入排序在几乎有序的数组中排序效率较高，希尔排序通过分组的策略，先从数据规模较小的分组开始排序，并且保证在某次分组选择排序时，数组经过前几次的分组选择排序已部分有序。 算法描述如下 定义递减的增量排序序列${g_1, g_2, \cdots, g_K}$，其中$g_K = 1$； 在第$k$次排序中，以$g_k$位间隔取出待排序数组中的数，进行分组(共$g_k$组)以减少数据规模，对每个组进行选择排序； 在第$k + 1$次排序时，数组部分有序，减少数据复制次数，提升选择排序的效率； 不断循环直至排序完成。 希尔排序的时间复杂度分析困难，与增量排序序列定义有关，下界是$O(nlogn)$，可以定义为： 希尔增量：${n / 2, n / 4, \cdots, 4, 2, 1}$，时间复杂度最坏为$O(n^2)$； Hibbard增量：${2^k - 1, \cdots, 7, 3, 1}$，时间复杂度(最坏情形)为$O(n^{1.5})$； Sedgewick增量：${9 \times 4^k - 9 \times 2^i + 1或4^i - 3 \times 2^i + 1, \cdots, 41, 19, 5, 1}$，时间复杂度(最坏情形)为$O(n^{1.3})$； … 12345678910111213141516171819202122232425262728293031323334353637383940template&lt;typename T&gt;void shellSort(std::vector&lt;T&gt;&amp; nums)&#123; if (nums.size() &lt; 2) return; // 希尔增量：`&#123;n/2, n/4, ..., 4, 2, 1&#125;` for (int gk = nums.size() / 2; gk &gt; 0; gk /= 2) &#123; // 划分为`gk`个组，每组`n/2-1 ~ n/2`个数 for (int k = 0; k &lt; gk; k++) &#123; // 对分组数据`nums[k], nums[k + gk], nums[k + 2 * gk]... `进行选择排序 shellInsertSort(nums, k, gk); &#125; &#125;&#125;template&lt;typename T&gt;void shellInsertSort(std::vector&lt;T&gt;&amp; nums, int group, int gk)&#123; for (int i = group; i &lt; nums.size(); i += gk) &#123; int num = nums[i]; // 在`group, group + gk, group + 2 * gk, ...`中寻找合适的插入位置 for (int j = group; j &lt; i; j += gk) &#123; // `j`处大于`num`，为插入位置 if (nums[j] &gt; num) &#123; // 从后向前挪动 for (int k = i; k &gt; j; k -= gk) &#123; nums[k] = nums[k - gk]; &#125; // 存放`num`在`j`处 nums[j] = num; break; &#125; &#125; &#125;&#125; 基于归并的排序归并排序归并排序是一种分治策略的排序算法，将$n$个数进行二路划分，对每个子集合进行排(如插入、冒泡等)，再依次组合归并有序集合得到整个有序集合，可以看作是别的排序算法的递归实现。注意到将两个子集合归并时，两子集和已有序，故实际上无需采用其余排序算法。 数组实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465template&lt;typename T&gt;void mergeSort(T a[], int n)&#123; if (a == nullptr || n &lt;= 0) return; mergeSort(a, 0, n);&#125;// 对`a[begin: end]`进行排序template&lt;typename T&gt;void mergeSort(T a[], int begin, int end)&#123; if (a == nullptr || begin &lt;= end) return; if (end - begin &lt; 2) return; // 分治 int middle = (begin + end) / 2; mergeSort(a, begin, middle); mergeSort(a, middle, end); // 合并 merge(a, begin, middle, end);&#125;// 合并`a[begin: middle]`与`a[middle: end]`template&lt;typename T&gt;void merge(T a[], int begin, int middle, int end)&#123; if (a == nullptr || begin &gt; middle || middle &gt; end) return; // 空间复杂度O(n)、时间复杂度O(n) int n = end - middle; T* b = new T[n]; memcpy(b, a + middle, n * sizeof(T)); // 三个指针 T* ptr = a + end - 1; // 目标存放区域的末尾 T* ptr1 = a + middle - 1; // 数组1的末尾 T* ptr2 = b + n - 1; // 数组2的末尾 // 从后往前覆盖a的值 while (ptr &gt;= a + begin) &#123; T* p = nullptr; // 有一个数组已经到达末尾 if (ptr1 &lt; a + begin) &#123; p = ptr2; ptr2--; &#125; else if (ptr2 &lt; b) &#123; p = ptr1; ptr1--; &#125; // 从两个有序数组中选择较大的数 else &#123; if (*ptr1 &gt; * ptr2) &#123; p = ptr1; ptr1--; &#125; else &#123; p = ptr2; ptr2--; &#125; &#125; // 存放数据 *ptr = *p; ptr--; &#125; delete [] b;&#125; 链表实现链表的归并排序如下，比较关键的是将链表划分与合并，总体框架如下1234567891011121314151617181920212223242526ListNode* mergesortList(ListNode* head) &#123; if (head == nullptr) return nullptr; // 划分链表 ListNode* mid = binarySplitList(head); // 递归终止：只有一个或两个节点 if (head == mid) &#123; if (head-&gt;next &amp;&amp; head-&gt;val &gt; head-&gt;next-&gt;val) &#123; // 交换 head-&gt;val ^= head-&gt;next-&gt;val; head-&gt;next-&gt;val ^= head-&gt;val; head-&gt;val ^= head-&gt;next-&gt;val; &#125; return head; &#125; // 分治 head = mergesortList(head); mid = mergesortList(mid ); // 合并 head = binaryMergeList(head, mid); return head;&#125; 链表的划分，可以采用快慢指针的方式将链表对半划分，注意这种方式不能正确处理节点数目不大于$2$的链表 12345678910111213141516171819202122ListNode* binarySplitList(ListNode* head) &#123; // 注意只有一个或两个节点时，`slow == false` ListNode* fast = head; ListNode* slow = head; // 慢指针即中间指针 while (fast-&gt;next &amp;&amp; fast-&gt;next-&gt;next) &#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; &#125; // 寻找前半链表的尾节点 while (head-&gt;next &amp;&amp; head-&gt;next != slow) &#123; head = head-&gt;next; &#125; // 前半链表末尾置空指针 head-&gt;next = nullptr; return slow;&#125; 以下是两个链表合并的过程，时间复杂度$O(m+n)$，空间复杂度$O(1)$1234567891011121314151617181920212223242526272829303132333435363738ListNode* binaryMergeList(ListNode* head1, ListNode* head2) &#123; ListNode* sorted = nullptr; // 记录头节点 ListNode* node = nullptr; // 游走节点 while (true) &#123; // 选择合适的节点 ListNode* choosen = nullptr; if (head1 == nullptr &amp;&amp; head2 == nullptr) &#123; // 两连边均空，排序完成 return sorted; &#125; // 一边链表已到末尾 else if (head1 != nullptr &amp;&amp; head2 == nullptr) &#123; choosen = head1; head1 = head1-&gt;next; &#125; else if (head1 == nullptr &amp;&amp; head2 != nullptr) &#123; choosen = head2; head2 = head2-&gt;next; &#125; // 选择较小的节点 else &#123; if (head1-&gt;val &lt; head2-&gt;val) &#123; choosen = head1; head1 = head1-&gt;next; &#125; else &#123; choosen = head2; head2 = head2-&gt;next; &#125; &#125; // 添加至链表 choosen-&gt;next = nullptr; if (sorted == nullptr) &#123; sorted = node = choosen; &#125; else &#123; node-&gt;next = choosen; node = node-&gt;next; &#125; &#125;&#125; 性能对比 排序算法 平均时间 最好情况 最坏情况 额外空间 稳定度 计数/箱子 $Ο(n+k)$ $Ο(n+k)$ $Ο(n+k)$ $O(k)$ 稳定 桶 $Ο(n+k)$ $Ο(n+k)$ $Ο(n^2)$ $Ο(n+k)$ 稳定 基数 $O(n \times k)$ $O(n \times k)$ $O(n \times k)$ $O(n + k)$ 稳定 冒泡 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 稳定 快速 $O(n \log n)$ $O(n \log n)$ $O(n^2)$ $O(n \log n)$ 不稳定 选择 $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$ 不稳定 堆排序 $O(n \log n)$ $O(n \log n)$ $O(n \log n)$ $O(1)$ 不稳定 插入 $O(n^2)$ $O(n)$ $O(n^2)$ $O(1)$ 稳定 希尔 $O(n \log n)$ $O(n \log^2 n)$ $O(n \log^2 n)$ $O(1)$ 不稳定 归并 $O(n \log n)$ $O(n \log n)$ $O(n \log n)$ $O(n)$ 稳定 上表中$k$表示整数范围内的数; 以上冒泡是指改进之后的算法，可将最好时间复杂度从$O(n^2)$降低至$O(n)$，比较$n-1$次 三种“基础”排序算法冒泡、选择、插入中，直插法在原数组有序时也只比较$n$次，只有选择排序最好情况下也是$O(n^2)$；基于选择的排序、两个优化的排序(快排、希尔)都是不稳定排序； 从$n$个数中选取最$k$个最小的数，最快的是堆排序($O(n \log k)$)；若不要求$k$个数有序，可采用快排partition($O(n)$)； 归并排序可用于外部排序(如用100MB空间对1GB数进行排序)； 就平均性能而言,目前最好的内排序方法是快速排序； 输入若已经是排好序的，插入排序算法最快； 稳定排序，是指在排序算法中，相同值的两个元素，在输入数组中先出现的数在输出数组中也先出现。像冒泡排序，插入排序，基数排序，归并排序等都是稳定排序。原地(原址、就地)排序是指：基本上不需要额外辅助的的空间，允许少量额外的辅助变量进行的排序。就是在原来的排序数组中比较和交换的排序。像选择排序，插入排序，希尔排序，快速排序，堆排序等都会有一项比较且交换操作(swap(i,j))的逻辑在其中，因此他们都是属于原地(原址、就地)排序，而合并排序，计数排序，基数排序等不是原地排序。出于俩概念混乱的目的，在此书之，以免忘记。————————————————版权声明：本文为CSDN博主「liao_hb」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/liao_hb/java/article/details/81335121 Reference 十大经典排序算法 - 菜鸟教程； 稳定排序和不稳定排序 - 龙渊阁；]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[经典机器学习算法推导汇总]]></title>
    <url>%2F2020%2F02%2F10%2F%E7%BB%8F%E5%85%B8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[目录 目录 前言 MLE/MAP 最大似然估计(MLE) 最大后验概率估计(MAP) 线性回归/逻辑斯蒂回归 线性回归 逻辑斯蒂回归(LR) 朴素贝叶斯 PCA/LDA PCA 计算步骤 证明 LDA 计算步骤 证明 EM/GMM EM算法 GMM模型 SVM KKT条件 核技巧 分类问题 线性可分 线性不可分 回归问题 求解优化问题 聚类 距离度量 KMeans Spectral 决策树 ID3 C4.5 CART RF 前言本文只做复习使用，只给出关键算法描述和证明。 MLE/MAP给定$N$个样本对${(X^{(i)}, y^{(i)}), i = 1, \cdots, N}$，其中$y \in {C_k, k = 1, \cdots, K}$，要求估计参数模型$P(X | \theta)$的参数$\theta$，使之最能描述给定数据分布。 最大似然估计(MLE) \begin{aligned} 优化目标：& \hat{\theta} = \arg \max P(D | \theta) \\ 定义：& L(D | \theta) = P(D | \theta) = \prod_i P(X^{(i)} | \theta) \\ 取对数：& \log L(D | \theta) = \sum_i \log P(X^{(i)} | \theta) \\ 求取极值：& \frac{\partial}{\partial \theta} \log L(D | \theta) = 0 \Rightarrow \hat{\theta} \end{aligned}最大后验概率估计(MAP) \begin{aligned} 优化目标：& \hat{\theta} = \arg \max P(\theta | D) \\ 其中：& P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)} \\ & P(\theta)为给定的参数先验概率分布 \\ 定义：& L(\theta | D) = P(D | \theta) P(\theta) = \prod_i P(X^{(i)} | \theta) \cdot P(\theta) \\ 取对数：& \log L(\theta | D) = \sum_i \log P(X^{(i)} | \theta) + \log P(\theta) \\ 求取极值：& \frac{\partial}{\partial \theta} \log L(\theta | D) = 0 \Rightarrow \hat{\theta} \end{aligned} 线性回归/逻辑斯蒂回归给定$N$个样本对${(X^{(i)}, y^{(i)}), i = 1, \cdots, N}$，记样本矩阵$X_{N \times n}$。 线性回归 \begin{aligned} 标签信息：& y \in \mathcal{R}^1， 定义模型：\hat{y}_{1\times 1} = w_{n \times 1}^T x_{n \times 1} + b \\ 增广后：& \hat{y}_{1\times 1} = w_{n \times 1}^T x_{n \times 1} \begin{cases} w_1 = b \\ x_1 = 1 \end{cases} \\ MSE作为损失，则总体损失：& L(\hat{y}, y) = \frac{1}{N} \sum_{i=1}^N \frac{1}{2} (\hat{y}^{(i)} - y^{(i)})^2 \\ 求取梯度：& \frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_{i=1}^N (\hat{y}^{(i)} - y^{(i)}) \frac{\partial \hat{y}^{(i)}}{\partial w_j} = \frac{1}{N} \sum_{i=1}^N (\hat{y}^{(i)} - y^{(i)}) x^{(i)}_j \Rightarrow \\ 梯度下降：& w_j := w_j - \alpha \frac{\partial L}{\partial w_j} \end{aligned}若描述为矩阵 \begin{aligned} \left.\begin{aligned} & 标签信息 Y \in R^{N} \\ 定义模型：& \hat{Y}_{N \times 1} = X_{N \times (n + 1)} w_{(n + 1) \times 1} \\ 总体损失：& L(\hat{Y}, Y) = \frac{1}{N} \cdot \frac{1}{2} || \hat{Y} - Y ||_2^2 = \frac{1}{N} \cdot \frac{1}{2} (\hat{Y} - Y)^T(\hat{Y} - Y) \end{aligned}\right\} \Rightarrow \\ L(\hat{Y}, Y) = \frac{1}{2 N} (w^T X^T X w - 2 Y^T X w + Y^T Y) \\ 求取梯度： \frac{\partial L}{\partial w} = \frac{1}{\cancel{2} N} (\cancel{2} X^T X w - \cancel{2} X^T Y) = 0 \Rightarrow \\ \begin{cases} 梯度下降：& w := w - \alpha \frac{\partial L}{\partial w} \\ 解析解：& \hat{w}^* = \underbrace{(X^T X + \lambda I)^{-1} X^T}_{X^+} Y \end{cases} \end{aligned} 逻辑斯蒂回归(LR) \begin{aligned} 标签信息： y \in \{0, 1\} \\ 定义模型：& \begin{cases} \hat{y} = \sigma(z) \\ z = w^T X + b \end{cases} \\ & 其中 \sigma(z) = \frac{1}{1 + \exp(-z)} \\ 样本X服从0-1分布：& P(X) = (1 - \hat{y})^{1 - y} (\hat{y})^{y} (\hat{y}^{(i)}为直接待估参数) \\ MLE：& L(D | w) = \prod_i P(X^{(i)}) \Rightarrow \log L(D | w) = \sum_i \log P(X^{(i)}) \\ 优化目标：& \hat{w} = \arg \max L(D | w) = \arg \max \log L(D | w) \\ 求取极值：& \begin{aligned} \frac{\partial L}{\partial w_j} & = \frac{\partial}{\partial w_j} \sum_i \log P(X^{(i)}) \\ & = \frac{\partial}{\partial w_j} \sum_i \log (1 - \hat{y}^{(i)})^{1 - y^{(i)}} (\hat{y}^{(i)})^{y^{(i)}} \\ & = \frac{\partial}{\partial w_j} \sum_i (1 - y^{(i)}) \log (1 - \hat{y}^{(i)}) + \frac{\partial}{\partial w_j} \sum_i y^{(i)} \log \hat{y}^{(i)} \\ & = \sum_i (1 - y^{(i)}) \frac{1}{1 - \hat{y}^{(i)}} (- \frac{\partial y^{(i)}}{\partial w_j}) + \sum_i y^{(i)} \frac{1}{\hat{y}^{(i)}} (\frac{\partial y^{(i)}}{\partial w_j}) \end{aligned} \\ 其中：& \frac{\partial y^{(i)}}{\partial w_j} = \sigma'(z^{(i)}) \frac{\partial z^{(i)}}{\partial w_j} = \sigma(z^{(i)}) (1 - \sigma(z^{(i)})) x^{(i)}_j \Rightarrow \\ & \frac{\partial L}{\partial w_j} = \sum_i - (1 - \bcancel{y^{(i)}}) \frac{1}{\cancel{1 - \hat{y}^{(i)}}} \sigma(z^{(i)}) \cancel{(1 - \sigma(z^{(i)}))} x^{(i)}_j + \\ & \sum_i y^{(i)} \frac{1}{\cancel{\hat{y}^{(i)}}} \cancel{\sigma(z^{(i)})} (1 - \bcancel{\sigma(z^{(i)})}) x^{(i)}_j = \sum_i (y^{(i)} - \hat{y}^{(i)}) x^{(i)}_j \Rightarrow \\ 梯度下降：& w_j := w_j - \alpha \frac{\partial L}{\partial w_j} \end{aligned} 朴素贝叶斯给定$N$个样本对${(X^{(i)}, y^{(i)}), i = 1, \cdots, N}$，其中$y \in {C_k, k = 1, \cdots, K}$。 \begin{aligned} 定义模型为条件概率分布：& P(Y | X) \\ 由贝叶斯公式：& P(Y | X) = \frac{P(X | Y) P(Y)}{P(X)} \\ 称：& \begin{cases} 后验概率：& P(Y | X) \\ 似然函数：& P(X | Y) = \prod_{j=1}^n P(X_j | Y) (朴素贝叶斯)\\ 先验概率：& P(Y) \\ 证据因子：& P(X) = \sum_k P(X | Y = C_k) P(Y = C_k) \end{cases} \\ \hat{y} & = \max_k P(X | Y = C_k) P(Y = C_k) \\ & = \max_k \prod_{j=1}^n P(X_j | Y = C_k) P(Y = C_k) \end{aligned}PCA/LDAPCA给定包含$M$个样本的$N$维数据集${X_{N \times 1}^{(i)}, i = 1, \cdots, M}$构成样本矩阵$X_{N \times M} = \begin{bmatrix}X^{(1)} &amp; X^{(2)} &amp; \cdots X^{(M)}\end{bmatrix}$，现希望求取主分量$\beta_k, k = 1, \cdots, K$使得数据投影在各主分量上的散布最大/方差最大。 计算步骤 计算维度间的协方差矩阵$\Sigma_{N \times N} = \frac{1}{M} \tilde{X} \tilde{X}^T$，其中$\tilde{X}^{(i)} = X^{(i)} - \overline{X}, \overline{X} = \frac{1}{M} \sum_{i=1}^{M} X^{(i)}$； 求矩阵$\Sigma$的特征值分解，即$\Sigma \beta_k = \lambda_k \beta_k$； 将特征对$(\lambda_k, \beta_k)$按特征值$\lambda_k$降序排序后，选取前$K$个主分量作为投影轴构成投影矩阵$B_{N \times K}$； 投影：$S_{K \times M} = B_{N \times K}^T X_{N \times M}$；重建；$\hat{X} = B_{N \times K} S_{K \times M}$。 证明 第$1$主成分 优化目标为 \begin{aligned} \beta_1 & = \arg \max ||S_1||_2^2 \\ s.t. & \quad ||\beta_1||_2^2 = 1 \end{aligned} 那么 \begin{aligned} \left. \begin{aligned} \left. \begin{aligned} ||S_1||_2^2 & = S_1^T S_1 \\ S_1 & = X^T \beta_1 \end{aligned} \right\} \Rightarrow ||S_1||_2^2 = \beta_1^T \underbrace{X X^T}_C \beta_1 \\ C = X X^T = W \Lambda W^T \end{aligned} \right\} \Rightarrow \\ \left. \begin{aligned} ||S_1||_2^2 = \beta_1^T W \Lambda \underbrace{W^T \beta_1}_{\alpha_1} = \sum_{i=1}^N \lambda_i \alpha_{1i} \leq \lambda_1 \sum_{i=1}^N \alpha_{1i} \\ \beta_1^T \beta_1 = \alpha_1^T W^T W \alpha = \alpha_1^T \alpha = \sum_{i=1}^N \alpha_{1i} = 1(单位约束) \end{aligned} \right\} \Rightarrow \\ ||S_1||_2^2 \leq \lambda_1 \quad 为使||S_1||_2^2极大化，取 \\ \begin{cases} \alpha_{11} = 1\\ \alpha_{1i} = 0, i = 2, 3, \cdots, N \end{cases} \Rightarrow \beta_1 = W \alpha_1 = w_1 \end{aligned} 第$r(r&gt;1)$主成分 优化目标为 \begin{aligned} \beta_r & = \arg \max ||S_r||_2^2 \\ s.t. & \quad \beta_r^T \beta_i = 0, i = 1, \cdots, r - 1 \\ & ||\beta_r||_2^2 = 1 \end{aligned} 那么 \begin{aligned} \left. \begin{aligned} \left. \begin{aligned} ||S_r||_2^2 = S_r^T S_r \\ S_r = X^T \beta_r \end{aligned} \right\} \Rightarrow ||S_r||_2^2 = \beta_r^T \underbrace{X X^T}_C \beta_r \\ C = X X^T = W \Lambda W^T \end{aligned} \right\} \Rightarrow \\ \left. \begin{aligned} ||S_r||_2^2 = \beta_r^T W \Lambda \underbrace{W^T \beta_r}_{\alpha_r} = \sum_{i=1}^N \lambda_i \alpha_{ri} \\ \beta_r^T \beta_i =(W \alpha_r)^T (w_i) = \alpha_{ri} = 0, i \neq r (正交约束) \\ \beta_r^T \beta_r = \alpha_r^T W^T W \alpha = \alpha_r^T \alpha = \sum_{i=1}^N \alpha_{1i} = 1(单位约束) \end{aligned} \right\} \Rightarrow \\ ||S_r||_2^2 = \lambda_r \alpha_{rr} \quad 为使||S_r||_2^2极大化，取 \\ \begin{cases} \alpha_{rr} = 1 \\ \alpha_{ri} = 0, i = \neq r \end{cases} \Rightarrow \beta_r = W \alpha_r = w_r \end{aligned} LDA给定$N$个样本对${(X^{(i)}, y^{(i)}), i = 1, \cdots, N}$，其中$y \in {C_k, k = 1, \cdots, K}$，记样本矩阵$X_{N \times n}$。现利用类别信息求取投影主轴$u$，使得投影后类内散步小，类间散步大。 定义： \begin{cases} 总样本均值： & \mu = \frac{1}{N} \sum_{i=1}^N X^{(i)} \\ 类别样本均值： & \mu_k = \frac{1}{N_k} \sum_{i=1}^{N_k} X^{(i)}, y^{(i)} = C_k \\ 类内离差阵： & S_{W, n \times n} = \sum_k \frac{N_k}{N} \left[ \frac{1}{N_k} \sum_i (X^{(i)} - \mu_k) (X^{(i)} - \mu_k)^T \right] \\ 类内离差阵： & S_{B, n \times n} = \sum_k \frac{N_k}{N} \left[ (\mu_k - \mu) (\mu_k - \mu)^T \right] \\ \end{cases}计算步骤 计算类内/类间离差阵$S_W/S_B$； 计算矩阵$S_W^{-1}S_B$的特征对$(\lambda_i, u_i)$； 将特征对按特征值降序排序，选取最大的特征值对应特征向量作为投影主轴，构成投影矩阵$U_{n \times m}$； 投影到主轴上，$\hat{X}_{N \times m} = X_{N \times n} U_{n \times m}$。 证明 \begin{aligned} 将样本点X^{(i)}投影到第一主轴u_1上有 \quad \tilde{X}^{(i)} = u_1^T X^{(i)} \quad 在投影空间有 \\ \left.\begin{aligned} \tilde{X}^{(i)} & = u_1^T X^{(i)}, \tilde{\mu} = u_1^T \mu, \tilde{\mu}_k = u_1^T \mu_k \\ \tilde{S_W}_{1 \times 1} & = \sum_k \frac{N_k}{N} \left[ \frac{1}{N_k} \sum_i (\tilde{X}^{(i)} - \tilde{\mu}_k) (\tilde{X}^{(i)} - \tilde{\mu}_k)^T \right] \\ \tilde{S_B}_{1 \times 1} & = \sum_k \frac{N_k}{N} \left[ (\tilde{\mu}_k - \tilde{\mu}) (\tilde{\mu}_k - \tilde{\mu})^T \right] \end{aligned}\right\} \Rightarrow \begin{cases} \tilde{S_W} = u_1^T S_W u_1 \\ \tilde{S_B} = u_1^T S_B u_1 \end{cases} \\ 定义优化目标为：u_1 = \arg \max \frac{\tilde{S_W}}{\tilde{S_B}} = \arg \max \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \\ 求取极值：\frac{\partial}{\partial u_1} \frac{u_1^T S_W u_1}{u_1^T S_B u_1} = \frac{(u_1^T S_B u_1)(2 S_w u_1) - (u_1^T S_w u_1)(2 S_B u_1)}{(u_1^T S_B u_1)^2} = 0 \Rightarrow \\ S_w u_1 = \underbrace{\frac{u_1^T S_W u_1}{u_1^T S_B u_1}}_{\lambda_1} S_B u_1，记\lambda_1 = \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \end{aligned} EM/GMMEM算法给定包含$N$对样本数据${(X^{(i)}, y^{(i)}), i = 1, \cdots, N}$。设分类模型为概率模型$P(X | \theta)$，其中$\theta$待估。该模型包含$K$种隐藏变量状态${w_k, k = 1, \cdots, K}$。那么证明过程总结如下 \begin{aligned} MLE \Rightarrow L(D | \theta) = \prod_i P(X^{(i)} | \theta) \Rightarrow \log L(D | \theta) = \sum_i \log P(X^{(i)} | \theta) \\ \Rightarrow 优化目标：\theta^{(t + 1)} = \arg \max \log L(D | \theta) \\ \\ \left. \begin{aligned} P(X^{(i)} | \theta) = \sum_k P(X^{(i)}, w^{(i)}_k | \theta) (引入隐变量w_k) \\ \frac{P(w^{(i)}_k | \theta^{(t)})}{P(w^{(i)}_k | \theta^{(t)})} = 1 (引入迭代变量\theta^{(t)}) \end{aligned} \right\} \Rightarrow \\ \left. \begin{aligned} \log L(D | \theta) = \sum_i \log \sum_k P(X^{(i)}, w^{(i)}_k | \theta) \frac{P(w^{(i)}_k | \theta^{(t)})}{P(w^{(i)}_k | \theta^{(t)})} \\ \begin{cases} \varphi(\cdot)下凸 \\ \sum_i w_i = 1 \end{cases} \Rightarrow \varphi(\sum_i w_i x_i) \leq \sum_i w_i \varphi(x_i) (Jensen不等式) \end{aligned} \right\} \Rightarrow \\ \log L(D | \theta) = \sum_i \sum_k P(w^{(i)}_k | \theta^{(t)}) \log \frac{P(X^{(i)}, w^{(i)}_k | \theta)}{P(w^{(i)}_k | \theta^{(t)})} \\ = \underbrace{ \sum_i \sum_k P(w^{(i)}_k | \theta^{(t)}) \log P(X^{(i)}, w^{(i)}_k | \theta)}_{E_w\left[ \log P(X^{(i)}, w^{(i)}_k | \theta) \right]} \\ \underbrace{- \sum_i \sum_k P(w^{(i)}_k | \theta^{(t)}) \log P(w^{(i)}_k | \theta^{(t)})}_{H\left[ P(w^{(i)}_k | \theta^{(t)}) \right]} \\ 记 \quad Q(\theta | \theta^{(t)}) = E_w\left[ \log P(X^{(i)}, w^{(i)}_k | \theta) \right] \\ \Rightarrow 优化目标：\theta^{(t + 1)} = \arg \max Q(\theta | \theta^{(t)}) \\ 对Q(\theta | \theta^{(t)})求极值求解\theta^{(t + 1)}。 \end{aligned} GMM模型高斯混合模型，具有如下概率形式 P(X | \mu, \Sigma) = \sum_{k=1}^K \pi_k N(X | \mu_k, \Sigma_k)其中 \begin{cases} \sum_k \pi_k = 1 \\ N(X | \mu_k, \Sigma_k) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp \left[ - \frac{1}{2} (X - \mu_k)^T \Sigma_k^{-1} (X - \mu_k) \right] \end{cases}用EM算法对参数进行估计 \begin{aligned} \left. \begin{aligned} Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \underbrace{P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta)}_{P(x^{(i)}, w_k^{(i)} | \theta)} \\ \begin{cases} P(w_k^{(i)}|\theta^{(t)}) = \frac{\pi_k^{(t)} N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})} {\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})} = \gamma^{(i)(t)}_k \\ P(x^{(i)} | w_k^{(i)}, \theta) = N(x^{(i)}|\mu_k, \Sigma_k) \\ P(w_k^{(i)} | \theta) = \pi_k \end{cases} \end{aligned} \right\} \Rightarrow \\ Q(\theta|\theta^{(t)}) = \sum_i \sum_k \gamma^{(i)(t)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k) \\ 求解Q函数极值 \Rightarrow \begin{cases} \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k} \\ \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k} \\ \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N} \end{cases} \end{aligned} SVMKKT条件 \begin{aligned} \left.\begin{aligned} w = \arg \min f(w) \\ s.t. \quad h_j(w) = 0, j = 1, \cdots, m \\ g_j(w) \leq 0, j = 1, \cdots, p \end{aligned}\right\} \Rightarrow \\ L(w, \lambda, \mu) = f(w) + \sum_j \lambda_j h_j(w) + \sum_j \mu_j \left(g_j(w) + \epsilon^2 \right) \\ \Rightarrow \begin{cases} \frac{\partial}{\partial w} f(w) + \sum_j \lambda_j \frac{\partial}{\partial w} h_j(w) + \sum_j \mu_j \frac{\partial}{\partial w} g_j(w) = 0 \\ h_j(w) = 0, j = 1, \cdots, m \\ \left.\begin{aligned} \mu_j g_j(w) = 0 \\ \mu_j \geq 0 \end{aligned} \right\} j = 1, \cdots, p \end{cases} \end{aligned}核技巧设某函数$\Phi(x)$，可将$x$从$n$维空间映射到$n’$维空间，定义两个向量的核函数为$\kappa(x_i, x_j) = \Phi(x_i)^T \Phi(x_j)$，常用和函数有 \begin{cases} 线性核：& \kappa(x_i, x_j) = x_i^T x_j \\ 多项式核：& \kappa(x_i, x_j) = (\gamma x_i^T x_j + c)^n \\ sigmoid核：& \kappa(x_i, x_j) = \tanh (\gamma x_i^T x_j + c) \\ 拉普拉斯核：& \kappa(x_i, x_j) = \exp (- \gamma \frac{||x_i - x_j||}{\sigma}) \\ 高斯核：& \kappa(x_i, x_j) = \exp (- \gamma \frac{||x_i - x_j||^2}{2 \sigma^2}) \end{cases} 分类问题给定$N$对样本${(X^{(i)}, y^{(i)}), i = 1, \cdots, N}, y \in {-1, 1}$，求取超平面$w^T \Phi(x) + b = 0$使样本点落在该超平面两侧。 线性可分 \begin{aligned} \left.\begin{aligned} 记r_{+/-}为分类平面到支持向量x_{+/-}的距离，则r = r_+ + r_-，且r_{+/-} = \frac{|w^T \Phi(x_{+/-}) + b|}{||w||} = \frac{1}{||w||} \\ 正/负样本分别满足\begin{cases} w^T \Phi(x^{(i)}) + b > 1 & y^{(i)} > 0 \\ w^T \Phi(x^{(i)}) + b < -1 & y^{(i)} < 0 \end{cases} \Rightarrow y^{(i)} [w^T \Phi(x^{(i)}) + b] \geq 1(包括支持向量) \end{aligned}\right\} \Rightarrow \\ \end{aligned} \begin{aligned} 优化目标：& \begin{aligned} w, b & = \arg \max r \\ s.t. & \quad y^{(i)} [w^T \Phi(x^{(i)}) + b] \geq 1 \end{aligned} \\ 即： & \begin{aligned} w, b & = \arg \min \frac{1}{2} ||w||^2 \\ s.t. & \quad y^{(i)} [w^T \Phi(x^{(i)}) + b] \geq 1 \end{aligned} \end{aligned}线性不可分在线性可分支持向量机基础上，对每个样本添加松弛变量$\epsilon^{(i)}$ \begin{aligned} 优化目标：\begin{aligned} w, b & = \arg \min \left[ \frac{1}{2} ||w||^2 + C \sum_i \epsilon^{(i)} \right] \\ s.t. & \quad y^{(i)} [w^T \Phi(x^{(i)}) + b] \geq 1 - \epsilon^{(i)} \\ & \epsilon^{(i)} \geq 0 \end{aligned} \end{aligned}回归问题给定$N$对样本${(X^{(i)}, y^{(i)}), i = 1, \cdots, N}, y \in R$，求回归模型$\hat{y} = w^T \Phi(x) + b$，使得每个样本尽量拟合到该模型上，定义损失为 L^{(i)} = \begin{cases} |y^{(i)} - w^T \Phi(x^{(i)}) - b| - \epsilon & |y^{(i)} - w^T \Phi(x^{(i)}) - b| > \epsilon \\ 0 & otherwise \end{cases} 求解优化问题以线性可分支持向量机为例，讲解参数$w， b$的优化方法 优化目标：\begin{aligned} w, b & = \arg \min \frac{1}{2} ||w||^2 \\ s.t. & \quad y^{(i)} [w^T \Phi(x^{(i)}) + b] \geq 1 \end{aligned} \begin{aligned} 拉格朗日函数：L(w, b, \mu) = \frac{1}{2} ||w||^2 + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} [w^T \Phi(x^{(i)}) + b] \right\} \\ w, b, \mu = \arg \min_{w, b} \max_{\mu} L(w, b, \mu) \Rightarrow w, b, \mu = \arg \max_{\mu} \min_{w, b} L(w, b, \mu)(对偶问题) \\ 求解极值：\begin{cases} \begin{aligned} \frac{\partial}{\partial w_j} L(w, b, \mu) = \frac{1}{2} \frac{\partial}{\partial w_j} ||w||^2 + \sum_i \mu^{(i)} \left\{ - y^{(i)} \frac{\partial}{\partial w_j} w^T \Phi(x^{(i)}) \right\} = \\ w_j - \sum_i \mu^{(i)} y^{(i)} \Phi(x^{(i)})_j \end{aligned} \\ \begin{aligned} \frac{\partial}{\partial b} L(w, b, \mu) = \sum_i \mu^{(i)} \left\{ -y^{(i)} \frac{\partial}{\partial b} b \right\} = \\ - \sum_i \mu^{(i)} y^{(i)} \end{aligned} \end{cases} \\ 由K.K.T条件：\begin{cases} \left.\begin{aligned} \sum_i \mu^{(i)} y^{(i)} \Phi(x^{(i)})_j & = w_j \\ \sum_i \mu^{(i)} y^{(i)} & = 0 \end{aligned}\right\} (极值条件) \\ 1 - y^{(i)} [w^T \Phi(x^{(i)}) + b] \leq 0 (不等式约束) \\ \left.\begin{aligned} \mu^{(i)} \left\{ 1 - y^{(i)} [w^T \Phi(x^{(i)}) + b] \right\} = 0 \\ \mu^{(i)} > 0 \end{aligned} \right\} (优化目标取'='的必要条件) \end{cases} \end{aligned} $拉格朗日函数展开后，将极值条件代入，有$ \begin{aligned} L(w, b, \mu) & = \frac{1}{2} ||w||^2 + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} [w^T \Phi(x^{(i)}) + b] \right\} \\ & = \frac{1}{2} w^T w + \sum_i \mu^{(i)} - \sum_i \mu^{(i)} y^{(i)} w^T \Phi(x^{(i)}) - \sum_i \mu^{(i)} y^{(i)} b \\ & = \frac{1}{2} w^T w + \sum_i \mu^{(i)} - \sum_i \mu^{(i)} y^{(i)} \underbrace{\left( \sum_j w_j \Phi(x^{(i)})_j \right)}_{w^T \Phi(x^{(i)})} - \cancel{\sum_i \mu^{(i)} y^{(i)} b} \\ & \left.\begin{aligned} = \frac{1}{2} w^T w + \sum_i \mu^{(i)} - \sum_j w_j \cdot \underbrace{\sum_i \mu^{(i)} y^{(i)} \Phi(x^{(i)})_j}_{w_i} = - \frac{1}{2} w^T w + \sum_i \mu^{(i)} \\ w^T w = \left( \sum_i \mu^{(i)} y^{(i)} \Phi(x^{(i)}) \right)^T \left( \sum_i \mu^{(i)} y^{(i)} \Phi(x^{(i)}) \right) = \\ \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \Phi(x^{(i)})^T \Phi(x^{(j)}) \end{aligned}\right\} \Rightarrow \\ L(\mu) & = - \frac{1}{2} \underbrace{\sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \Phi(x^{(i)})^T \Phi(x^{(j)})}_{w^T w} + \sum_i \mu^{(i)} \end{aligned}$那么现在的优化问题如下，用SMO进行求解$ \begin{aligned} \mu & = \arg \max_{\mu} L(\mu) \\ s.t. & \quad \mu^{(i)} \geq 0, \quad \sum_i \mu^{(i)} y^{(i)} = 0 \\ \Rightarrow & \mu^* \Rightarrow w^*, b^* \end{aligned} 聚类仅介绍部分概念和算法步骤。给定样本集合${X^{(i)}, i = 1, \cdots, N}$，指定划分类别$K$，要求利用样本分布，将样本划分为$K$个类别。 距离度量定义两个$n$维向量$x, y$，有如下常用距离定义 \begin{aligned} 曼哈顿距离 & d = || x - y ||_1 = \sum_j |x_j - y_j| \\ 欧氏距离 & d = || x - y ||_2 = (\sum_j (x_j - y_j)^2)^{1 / 2} \\ 闵可夫斯基距离 & d = || x - y ||_p = (\sum_j |x_j - y_j|^p)^{1 / p} \\ 余弦距离 & d = || x - y ||_1 = \cos = \frac{x^T y}{||x||\cdot||y||} \\ \end{aligned}KMeans 随机选取$K$个样本点作为初始中心点(初值敏感)； 计算每个样本点到各中心点的距离($N \times K$)； 将每个样本划分到距离最近的中心点指代的类别中； 每个类别重新计算中心点，更新参数； 重复2~4直至收敛。 Spectral 构建相似矩阵$\begin{cases} S_{N \times N} = \begin{bmatrix} d_{ij} \end{bmatrix} \ d_{ij} = ||x^{(i)} - x^{(j)}||_2^2 \end{cases}$； 计算邻接矩阵 \begin{cases} \epsilon近邻法：& w_{ij} = \begin{cases} \epsilon & d_{ij} \leq \epsilon \\ 0 & otherwise \end{cases} \\ K近邻法：& w_{ij} = \begin{cases} \exp(-\frac{d_{ij}}{2 \sigma^2}) & x^{(i)} \in \delta_K(x^{(j)}) \quad AND/OR \quad x^{(j)} \in \delta_K(x^{(i)}) \\ 0 & otherwise \end{cases} \\ & \delta_K(x)表示x的K邻域 \\ 全连接法：& w_{ij} = \exp(-\frac{d_{ij}}{2 \sigma^2}) \end{cases} 求度矩阵$D_{N \times N} = \text{diag}{\sum_j w_{ij}, i = 1, \cdots, N}$，即$W$行和作为对角元素； 求(正则)拉普拉斯矩阵$L = D - W$或$L = D^{-1}(D - W)$或$L = D^{-1/2}(D - W)D^{-1/2}$； 求$L$的特征分解，选取$N’(N’ \leq N)$个最小特征值对应的特征向量组成矩阵$F_{N \times N’}$； 将矩阵$F$每行视作样本$f^{(i)}$，标准化后执行其他简单的聚类如KMeans，得到聚类结果。 决策树给定包含$|D|$个样本的样本集$D = {(X^{(i)}, y^{(i)}), i = 1, \cdots, |D|}$，属于$K$个类别$y \in {C_k, k = 1, \cdots, K}$，设类别$C_k$的样本数目为$|D_{k}|$，设特征$A$有$|A|$个特征${A_a, a = 1, \cdots, |A|}$，每个特征包含样本数目$|D_{a}|$，记特征为$A_a$的样本中属于类别$C_k$的样本数目为$|D_{ak}|$。 ID3用信息增益作为准则选择当前最优划分属性：信息增益越大表示属性越优 \begin{aligned} g(D, A) = H(D) - H(D | A) \\ \left.\begin{aligned} H(D) & = - \sum_k \frac{|D_k|}{|D|} \log \frac{|D_k|}{|D|}(总样本的类别熵) \\ H(D | A) & = \sum_a \frac{|D_a|}{|D|} \underbrace{\left( - \sum_k \frac{|D_{ak}|}{|D_a|} \log \frac{|D_{ak}|}{|D_a|} \right)}_{H(D_a)} (特征A_a的类别熵的加权和) \end{aligned} \right\} \end{aligned}C4.5用信息增益比作为准则选择当前最优划分属性：信息增益比越大表示属性越优 以信息增益比(information gain ratio)作为特征选择的准则，克服ID3会优先选择有较多属性值的特征的缺点； 弥补不能处理特征属性值连续的问题。 \begin{aligned} g_R(D, A) & = \frac{g(D, A)}{H_A(D)} \\ H_A(D) & = - \sum_a \frac{|D_a|}{|D|} \log \frac{|D_a|}{|D|} (特征A的属性熵) \end{aligned}CART用信息增益比作为准则选择当前最优划分属性：信息增益比越大表示属性越优 \begin{aligned} g_G(D, A) = \text{Gini}(D) - \text{Gini}(D|A) \\ \left.\begin{aligned} \text{Gini}(D) & = 1 - \sum_k (\frac{|D_k|}{|D|})^2 (总样本的类别基尼系数) \\ \text{Gini}(D|A) & = \sum_a \frac{|D_a|}{|D|} \underbrace{\left( 1 - \sum_k (\frac{|D_{ak}|}{|D_a|})^2 \right)}_{\text{Gini}(D_a)} (特征A_a的类别基尼系数的加权和) \end{aligned}\right\} \end{aligned}RF随机森林是用Bagging策略，对包含$N$个样本的数据集进行$M$次的有放回的采样，每次随机取$N_m$个样本，得到$M$个样本数目为$N_m$的样本子集，对每个子集建立分类器。 Bootstrap采样：对于一个样本，它在某一次含$m$个样本的训练集的随机采样中，每次被采集到的概率是$1/m$。不被采集到的概率为$1−1/m$。如果$m$次采样都没有被采集中的概率是$(1−1/m)^m$。当$m→\infty$时，$\lim_{m \rightarrow \infty} (1−1/m)^m \approx 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。对于这部分大约$36.8\%$的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。 随机森林在Bagging策略上进行训练： 用Bootstrap策略随机采样$M$次； 一棵树的生成时，仅从所有特征($K$个)中选取$k$个特征； 生成$M$棵树进行投票表决，确定预测结果(分类可取众数、回归可取均值)。]]></content>
      <categories>
        <category>Matchine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL]]></title>
    <url>%2F2020%2F02%2F08%2FMySQL%2F</url>
    <content type="text"><![CDATA[目录 目录 关系型数据库管理系统: RDBMS 特点 术语 安装 测试数据 数据类型 Text类型 Number类型 Date/Time类型 运算符 算术运算符: +, -, *, /, DIV, MOD 比较运算符: =, &lt;&gt;/!=, &lt;, &lt;=, &gt;, &gt;=, BETWEEN, IN, &lt;=&gt;, LIKE, REGEXP, IS NULL 逻辑运算符: NOT, AND, OR, XOR 位运算符: &amp;, |, ^, !, &lt;&lt;, &gt;&gt; 管理 数据库操作 显示: SHOW DATABASES; 创建: CREATE DATABASE database; 删除: DROP DATABASE database; 选择: USE database; 数据表操作 显示: SHOW TABLES; 创建: CREATE TABLE table_name (column_name column_type); 缺失值限定: NOT NULL 编号自增: AUTO_INCREMENT 默认值: DEFAULT 主键: PRIMARY KEY 引擎: ENGINE 临时表: CREATE TEMPORARY TABLE table_name (column_name column_type); 更新: ALTER TABLE table_name; 表 修改表名: RENAME 字段 添加: ADD 删除: DROP 修改名称: CHANGE 修改类型: MODIFY 修改默认值: SET 删除: DROP TABLE table_name; 用户账号 查看: USE mysql; 创建: CREATE USER user_name IDENTIFIED BY ‘passwod’; 删除: DROP USER user_name; 访问权限 查看: SHOW GRANTS FOR user_name; 修改: GRANT, REVOKE 更改密码: SET PASSWORD FOR user_name = PASSWORD(‘new password’); 过滤: WHERE 范围查询: =, &lt;&gt;, !=, &lt;, &lt;=, &gt;, &gt;=, BETWEEN-AND 条件范围: IN 否定关键字: NOT 组合子句: AND, OR 空值检查: IS NULL 通配符过滤: LIKE 匹配0个、1个或多个字符: % 只且必须匹配1个字符: _ 正则表达式 查询: SELECT 排序: ORDER BY 分组: GROUP BY 分组过滤: HAVING 子查询: SELECT 组合查询: UNION 插入: INSERT INTO 更新: UPDATE-SET 删除: DELETE FROM 连接: JOIN-ON 创建连接: WHERE 内连接(等值连接)：JOIN 或 INNER JOIN 左连接：LEFT JOIN 右连接：RIGHT JOIN 函数: FUNCTION 字符串函数 数字函数 日期函数 其他函数 自定义函数 创建: CREATE FUNCTION - RETURNS 查看: SHOW CREATE FUNCTION 删除: DROP FUNCTION function_name; 变量 全局变量: SET @variable_name = variable_value; 局部变量: DECLARE variable_name variable_type DEFAULT variable_value, 调用: SELECT function_name(params_list); 事务 特点 控制语句: BEGIN, END, COMMIT, ROLLBACK, SAVEPOINT 全文搜索 启用: FULLTEXT 搜索: MATCH, AGAINST 查询扩展: WITH QUERY EXPANSION 布尔文本搜索: IN BOOLEAN MODE 存储过程: PROCEDURE 创建: CREATE PROCEDURE procedure_name(…) …; 删除: DROP PROCEDURE procedure_name; 调用: CALL procedure_name(…); 视图: VIEW 创建: CREATE VIEW view_name AS …; 查看: SHOW CREATE VIEW view_name; 删除: DROP VIEW view_name; 游标: CURSOR 创建: DECLARE cursor_name CURSOR FOR …; 打开: OPEN cursor_name; 关闭: CLOSE cursor_name; 触发器: TRIGGER Reference 关系型数据库管理系统: RDBMS关系型数据库是建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据。 特点 数据以表格的形式出现； 每行为各种记录名称； 每列为记录名称所对应的数据域； 许多的行和列组成一张表单 若干的表单组成database。 术语 数据库: 数据库是一些关联表的集合。 表: 表是数据的矩阵。在一个数据库中的表看起来像一个简单的电子表格。 列: 一列(数据元素) 包含了相同类型的数据, 例如邮政编码的数据。 行：一行（=元组，或记录）是一组相关的数据，例如一条用户订阅的数据。 主键：主键是唯一的。一个数据表中只能包含一个主键。你可以使用主键来查询数据。 用于唯一标识一条记录的一列(或一组列)； 在使用多列时，必须保证所有列值的组合时唯一的(此时单列可以不唯一)； 每行都必须有主键值(不允许NULL)； 任意两条记录不具有相同的主键值。 外键：外键用于关联两个表。 复合键：复合键（组合键）将多个列作为一个索引键，一般用于复合索引。 索引：使用索引可快速访问数据库表中的特定信息。索引是对数据库表中一列或多列的值进行排序的一种结构。类似于书籍的目录。 冗余：存储两倍数据，冗余降低了性能，但提高了数据的安全性。 参照完整性: 参照的完整性要求关系中不允许引用不存在的实体。与实体完整性是关系模型必须满足的完整性约束条件，目的是保证数据的一致性。 安装在Ubuntu下安装MySQL命令如下123456789101112$ sudo apt install mysql-server # 安装$ sudo apt-get install libmysqlclient-dev$ sudo service mysql start # 启动$ sudo mysql_secure_installation # 配置密码等$ sudo mysql -u root -p # 登录Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 4Server version: 5.7.30-0ubuntu0.18.04.1 (Ubuntu)Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; 测试数据为结合实际讲解说明，生成一些测试数据如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768-- 创建一个临时内存表set global log_bin_trust_function_creators=1;DROP TABLE IF EXISTS `vote_recordss_memory`;CREATE TABLE `vote_recordss_memory` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `user_id` varchar(20) NOT NULL DEFAULT '', `vote_num` int(10) unsigned NOT NULL DEFAULT '0', `group_id` int(10) unsigned NOT NULL DEFAULT '0', `status` tinyint(2) unsigned NOT NULL DEFAULT '1', `create_time` datetime NOT NULL DEFAULT '1971-01-01 01:01:01', PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`) USING HASH) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;-- 创建一个普通表，用作模拟大数据的测试用例DROP TABLE IF EXISTS `vote_records`;CREATE TABLE `vote_records` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `user_id` varchar(20) NOT NULL DEFAULT '' COMMENT '用户Id', `vote_num` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '投票数', `group_id` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '用户组id 0-未激活用户 1-普通用户 2-vip用户 3-管理员用户', `status` tinyint(2) unsigned NOT NULL DEFAULT '1' COMMENT '状态 1-正常 2-已删除', `create_time` datetime NOT NULL DEFAULT '1971-01-01 01:01:01' COMMENT '创建时间', PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`) USING HASH COMMENT '用户ID哈希索引') ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='投票记录表';-- 为了数据的随机性和真实性，我们需要创建一个可生成长度为n的随机字符串的函数。-- 创建生成长度为n的随机字符串的函数DELIMITER // -- 修改MySQL delimiter：'//'DROP FUNCTION IF EXISTS `rand_strings` //SET NAMES utf8 //CREATE FUNCTION `rand_strings` (n INT) RETURNS VARCHAR(255) CHARSET 'utf8'BEGIN DECLARE char_str varchar(100) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'; DECLARE return_str varchar(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str = concat(return_str, substring(char_str, FLOOR(1 + RAND()*62), 1)); SET i = i+1; END WHILE; RETURN return_str;END //-- 为了操作方便，我们再创建一个插入数据的存储过程-- 创建插入数据的存储过程DROP PROCEDURE IF EXISTS `insert_vote_recordss_memory` //CREATE PROCEDURE `insert_vote_recordss_memory`(IN n INT)BEGIN DECLARE i INT DEFAULT 1; DECLARE vote_num INT DEFAULT 0; DECLARE group_id INT DEFAULT 0; DECLARE status TINYINT DEFAULT 1; WHILE i &lt; n DO SET vote_num = FLOOR(1 + RAND() * 10000); SET group_id = FLOOR(0 + RAND()*3); SET status = FLOOR(1 + RAND()*2); INSERT INTO `vote_recordss_memory` VALUES (NULL, rand_strings(20), vote_num, group_id, status, NOW()); SET i = i + 1; END WHILE;END //DELIMITER ; -- 改回默认的 MySQL delimiter：';'-- 开始执行存储过程，等待生成数据CALL insert_vote_recordss_memory(10000);-- 把数据从内存表插入到普通表中INSERT INTO vote_records SELECT * FROM `vote_recordss_memory`; 可以看到表格创建成功123456789101112mysql&gt; SHOW COLUMNS FROM vote_records;+-------------+---------------------+------+-----+---------------------+----------------+| Field | Type | Null | Key | Default | Extra |+-------------+---------------------+------+-----+---------------------+----------------+| id | int(10) unsigned | NO | PRI | NULL | auto_increment || user_id | varchar(20) | NO | MUL | | || vote_num | int(10) unsigned | NO | | 0 | || group_id | int(10) unsigned | NO | | 0 | || status | tinyint(2) unsigned | NO | | 1 | || create_time | datetime | NO | | 1971-01-01 01:01:01 | |+-------------+---------------------+------+-----+---------------------+----------------+6 rows in set (0.02 sec) 数据类型Text类型 类型 大小(bytes) 用途 CHAR 0-255 定长字符串 VARCHAR 0-65535 变长字符串 TINYBLOB 0-255 不超过 255 个字符的二进制字符串 BLOB 0-65 535 二进制形式的长文本数据 MEDIUMBLOB 0-16 777 215 二进制形式的中等长度文本数据 LONGBLOB 0-4 294 967 295 二进制形式的极大文本数据 TINYTEXT 0-255 短文本字符串 TEXT 0-65 535 长文本数据 MEDIUMTEXT 0-16 777 215 中等长度文本数据 LONGTEXT 0-4 294 967 295 极大文本数据 Number类型 类型 大小(bytes) 范围（有符号） 范围（无符号） 用途 TINYINT 1 (-128，127) (0，255) 小整数值 SMALLINT 2 (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度浮点数值 DOUBLE 8 (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 定点小数值 Date/Time类型 类型 大小(bytes) 范围 格式 用途 YEAR 1 1901/2155 YYYY 年份值 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 ‘-838:59:59’/‘838:59:59’ HH:MM:SS 时间值或持续时间 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038，结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 注：在 INSERT 或 UPDATE 查询中，TIMESTAMP 自动把自身设置为当前的日期和时间。TIMESTAMP 也接受不同的格式，比如 YYYYMMDDHHMMSS、YYMMDDHHMMSS、YYYYMMDD 或 YYMMDD。 运算符算术运算符: +, -, *, /, DIV, MOD 运算符 作用 + 加法 - 减法 * 乘法 / 除法 DIV 取整 % 或 MOD 取余 比较运算符: =, &lt;&gt;/!=, &lt;, &lt;=, &gt;, &gt;=, BETWEEN, IN, &lt;=&gt;, LIKE, REGEXP, IS NULL 符号 描述 备注 = 等于 &lt;&gt;, != 不等于 &gt; 大于 &lt; 小于 &lt;= 小于等于 &gt;= 大于等于 BETWEEN 在两值之间 &gt;=min&amp;&amp;&lt;=max NOT BETWEEN 不在两值之间 IN 在集合中 NOT IN 不在集合中 &lt;=&gt; 严格比较两个NULL值是否相等 两个操作码均为NULL时，其所得值为1；而当一个操作码为NULL时，其所得值为0 LIKE 模糊匹配 REGEXP 或 RLIKE 正则式匹配 IS NULL 为空 IS NOT NULL 不为空 12345678910111213mysql&gt; select 'beijing' REGEXP 'jing';+-------------------------+| 'beijing' REGEXP 'jing' |+-------------------------+| 1 |+-------------------------+mysql&gt; select 'beijing' REGEXP 'xi';+-----------------------+| 'beijing' REGEXP 'xi' |+-----------------------+| 0 |+-----------------------+ 逻辑运算符: NOT, AND, OR, XOR 运算符号 作用 NOT 或 ! 逻辑非 AND 逻辑与 OR 逻辑或 XOR 逻辑异或 位运算符: &amp;, |, ^, !, &lt;&lt;, &gt;&gt; 运算符号 作用 &amp; 按位与 按位或 ^ 按位异或 ! 取反 &lt;&lt; 左移 &gt;&gt; 右移 管理数据库操作显示: SHOW DATABASES;1234567891011mysql&gt; SHOW DATABASES;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys || test |+--------------------+5 rows in set (0.01 sec) 创建: CREATE DATABASE database;删除: DROP DATABASE database;选择: USE database;数据表操作显示: SHOW TABLES;创建: CREATE TABLE table_name (column_name column_type);在创建新表时，指定的表名必须不存在，否则将出错。为防止意外覆盖已有的表，SQL要求首先手工删除该表，然后再重建它，而不是简单地用创建表语句覆盖它。123456789DROP TABLE IF EXISTS table_name;CREATE TABLE table_name( field_name1 field_type1 [NOT NULL [AUTO_INCREMENT [DEFAULT default]]], field_name2 field_type2 [NOT NULL [AUTO_INCREMENT [DEFAULT default]]], ..., field_nameN field_typeN [NOT NULL [AUTO_INCREMENT [DEFAULT default]]], PRIMARY KEY (key1, ..., keyM)) ENGINE = engine_name ...; 缺失值限定: NOT NULL可以通过NOT NULL关键字指定不接受空值的列，即在插入或更新行时，该列必须有值，否则返回错误。 编号自增: AUTO_INCREMENT每次执行一个INSERT操作时，MySQL自动对该列增量，给该列赋予下一个可用的值。这样给每个行分配一个唯一的数值，从而可以用作主键值。每个表只允许一个AUTO_INCREMENT列，而且它必须被索引(如通过使他成为主键)。 可以在INSERT语句中对AUTO_INCREMENT的属性指定一个至今未使用过的值以覆盖当前值，后续的将以该值进行增加；可以通过last_insert_id()获取上一个自动增量值1SELECT last_insert_id(); 默认值: DEFAULT 与大多数DBMS不一样，MySQL不允许使用函数作为默认值，它只支持常量； 用于计算或数据分组的列最好设置默认值。 主键: PRIMARY KEY可以在PRIMARY KEY()中定义主键(一个或多个)，主键可以在创建表时定义，或者在创建表之后定义。 引擎: ENGINEMySQL具有多种引擎，具有不同的功能和特性，为不同的任务选择合适的引擎可以获得良好的功能和灵活性，以下为几个重要的引擎 InnoDB是一个可靠的事务处理引擎，不支持全文本搜索； MyISAM是一个性能极高的引擎，它支持全文本搜索，但不支持事务处理； MEMORY功能等同于MyISAM，数据存储在内存中，速度很快，特别适合于临时表。 临时表: CREATE TEMPORARY TABLE table_name (column_name column_type);在创建时用关键字TEMPORARY临时表，只在当前连接可见，当关闭连接时，Mysql会自动删除表并释放所有空间。 更新: ALTER TABLE table_name;理想状态下，表中存储数据后，该表就不应该被更新，在设计时应花大量时间考虑，以便后期不进行大的改动。 表修改表名: RENAME1ALTER TABLE table_old_name RENAME TO table_new_name 字段添加: ADD1ALTER TABLE table_name ADD field_name field_type; 删除: DROP如果数据表中只剩余一个字段则无法使用DROP来删除字段。1ALTER TABLE table_name DROP field_name; 修改名称: CHANGE1ALTER TABLE table_name CHANGE old_name new_name field_new_type; 修改类型: MODIFY可以指定是否包含值或者是否设置默认值1ALTER TABLE table_name MODIFY field_name field_new_type [NOT NULL [DEFAULT default]]; 修改默认值: SET1ALTER TABLE table_name ALTER field_name SET DEFAULT default; 删除: DROP TABLE table_name;用户账号查看: USE mysql;MYSQL用户账号的信息存储在名为mysql的数据库中12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152mysql&gt; use mysql;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+---------------------------+| Tables_in_mysql |+---------------------------+| columns_priv || db || engine_cost || event || func || general_log || gtid_executed || help_category || help_keyword || help_relation || help_topic || innodb_index_stats || innodb_table_stats || ndb_binlog_index || plugin || proc || procs_priv || proxies_priv || server_cost || servers || slave_master_info || slave_relay_log_info || slave_worker_info || slow_log || tables_priv || time_zone || time_zone_leap_second || time_zone_name || time_zone_transition || time_zone_transition_type || user |+---------------------------+31 rows in set (0.00 sec)mysql&gt; select user from user limit 5;+------------------+| user |+------------------+| debian-sys-maint || mysql.session || mysql.sys || root |+------------------+4 rows in set (0.01 sec) 创建: CREATE USER user_name IDENTIFIED BY ‘passwod’;注意默认的密码要求12345678910111213mysql&gt; SHOW VARIABLES LIKE 'validate_password%';+--------------------------------------+--------+| Variable_name | Value |+--------------------------------------+--------+| validate_password_check_user_name | OFF || validate_password_dictionary_file | || validate_password_length | 8 || validate_password_mixed_case_count | 1 || validate_password_number_count | 1 || validate_password_policy | MEDIUM || validate_password_special_char_count | 1 |+--------------------------------------+--------+7 rows in set (0.00 sec) 删除: DROP USER user_name;访问权限查看: SHOW GRANTS FOR user_name;修改: GRANT, REVOKE更改密码: SET PASSWORD FOR user_name = PASSWORD(‘new password’);过滤: WHERE 类似于程序语言中的 if 条件，根据 MySQL 表中的字段值来读取指定的数据； 可以运用于 SQL 的 SELECT、DELETE 或者 UPDATE 命令； 在同时使用ORDER BY和WHERE子句时，应该让ORDER BY位于WHERE之后，否则将会产生错误； 范围查询: =, &lt;&gt;, !=, &lt;, &lt;=, &gt;, &gt;=, BETWEEN-AND123SELECT *FROM table_nameWHERE field BETWEEN low AND high 操作符 说明 = 等于 &lt;&gt;, != 不等于 &lt; 小于 &lt;= 小于等于 &gt; 大于 &gt;= 大于等于 BETWEEN low AND high 在指定的两个值范围(包含这两个值)内 条件范围: IN指定条件范围，由圆括号和逗号分隔的清单给出123SELECT *FROM table_nameWHERE field IN (value1, value2, ...) 否定关键字: NOT即非运算符，否定之后所有的任何条件123SELECT *FROM table_nameWHERE field NOT IN (value1, value2, ...) 组合子句: AND, OR可以通过逻辑操作符连接或改变WHERE子句中的子句关键字，可以通过AND和OR子句的方式使用。SQL在处理OR操作符前，优先处理AND操作符，使用圆括号明确地分组相应的操作符解决运算符优先级问题。123SELECT *FROM table_nameWHERE condition1 AND (condition2 OR condition3) ... 空值检查: IS NULL在创建表时，表设计人员可以指定其中的列是否可以不包含值。在一个列不包含值时，称其为包含空值NULL。NULL无值(no value)，它与字段包含0、空字符串或仅仅包含空格不同。用IS NULL子句检查具有NULL值的列123SELECT *FROM table_nameWHERE field IS NULL在通过过滤选择出不具有特定值的行时，你可能希望返回具有NULL值的行。但是，不行。因为未知具有特殊的含义，数据库不知道它们是否匹配，所以在匹配过滤或不匹配过滤时不返回它们。因此，在过滤数据时，一定要验证返回数据中确实给出了被过滤列具有NULL的行。 通配符过滤: LIKE在搜索子句中使用通配符，必须使用LIKE操作符，用于指示后跟的搜索模式利用通配符匹配而不是直接相等匹配进行比较，可以用LIKE子句代替等号。 通配符搜索处理花费时间比其他搜索都要长，有以下技巧 不要过度使用通配符，能用其他语句解决尽量用其他语句； 尽量不要把通配符放在搜索模式的开头，这是最慢的； 注意通配符的位置，防止匹配错误。 匹配0个、1个或多个字符: %123SELECT *FROM table_nameWHERE field LIKE pattern -- 如`&lt;head&gt;%`, `%&lt;tail&gt;`, `%&lt;body&gt;%`, `&lt;head&gt;%&lt;tail&gt;`等 NULL无法被%匹配； 记录属性值的首尾空格被视作字符，不会被省略； 根据MySQL的配置方式，可以指定是否区分大小写。 只且必须匹配1个字符: _功能与%类似，但是与%能匹配0个字符不一样，_总是匹配一个字符，不能多也不能少。 123SELECT *FROM table_nameWHERE field LIKE pattern -- 如`T_m`匹配`Tim, Tom, ...` 正则表达式TODO: 查询: SELECT123456SELECT [DISTINCT] field1, field1, ...FROM table_name1, table_name2, ...[WHERE condition][ORDER BY field1 [ASC(default)/DESC, [field2 [ASC(default)/DESC], ...]][GROUP BY field][LIMIT N][ OFFSET M]; 指定SELECT *可返回所有列查询结果； DISTINCT关键字用于去重，应用于所有列而不仅是前置它的列； LIMIT N子句指示返回最多行数； OFFSET M指定SELECT语`句开始查询的数据偏移量。默认情况下偏移量为0。 注意SELECT子句的执行顺序 子句 说明 是否必须使用 SELECT 要返回的列或者表达式 是 FROM 从中检索数据的表 仅在从表里选择数据时使用 WHERE 行级查询 否 GROUP BY 分组说明 仅在按组计算聚集时使用 HAVING 组间过滤（用在GROPUP BY之后） 否 ORDER BY 设置输出排序顺序 否 LIMIT 要检索的函数 否 排序: ORDER BY123SELECT *FROM table_nameORDER BY field DESC 若不指定排序，则返回记录的顺序无实际意义。关系数据库设计理论认为，如果不明确规定排序顺序，则不应该假定检索出的数据的顺序有意义。 可以用非检索的列对记录进行排序，即field无需出现在SELECT中； 按多个列排序时，排序完全按所规定的顺序进行，即先比较field1，在field1值相同的记录中，比较field2继续排序； 默认按升序排序ASC，可指定为降序DESC，该关键字只应用到直接位于其前面的列名。 例：从表中查询所有vip的id，并按创建时间排序123456789101112131415mysql&gt; SELECT user_id -&gt; FROM vote_records -&gt; WHERE group_id = 2 -&gt; ORDER BY create_time; -&gt; LIMIT 5+----------------------+| user_id |+----------------------+| GYEaLaoh2cIRY6tUXVzX || 2dNctAlNOx3xkSgvCnTc || fObjP4F1UkKCHt6VaUTy || Nw5I8pqMrJcBfhusGS90 || Zf8LfLXkD12WrgO7YmHg |+----------------------+5 rows in set (0.01 sec) 分组: GROUP BY123SELECT fieldFROM table_nameGROUP BY field 例：从表中查询所有vip的id，并按用户状态分组1234567891011mysql&gt; SELECT user_id, COUNT(*) AS count_id -&gt; FROM vote_records -&gt; WHERE group_id = 2 -&gt; GROUP BY status;+----------------------+----------+| user_id | count_id |+----------------------+----------+| 2dNctAlNOx3xkSgvCnTc | 1655 || GYEaLaoh2cIRY6tUXVzX | 1667 |+----------------------+----------+2 rows in set (0.01 sec) 分组过滤: HAVING可以设定条件过滤分组，支持所有的WHERE操作符。1234SELECT fieldFROM table_nameGROUP BY fieldHAVING condition 例：从表中查询所有vip的id，并按用户状态分组，筛选出投票数大于2000的用户1234567891011mysql&gt; SELECT user_id, vote_num, COUNT(*) AS count_id -&gt; FROM vote_records -&gt; WHERE group_id = 2 -&gt; GROUP BY status -&gt; HAVING vote_num &gt; 2000;+----------------------+----------+----------+| user_id | COUNT(*) | vote_num |+----------------------+----------+----------+| GYEaLaoh2cIRY6tUXVzX | 1667 | 5341 |+----------------------+----------+----------+1 row in set (0.00 sec) 子查询: SELECT将查询嵌套在其他查询中，例如123456mysql&gt; select cust_id -&gt; from orders -&gt; where order_num in ( -&gt; select order_num -&gt; from orderitems -&gt; where prod_id = 'TNT2'); 组合查询: UNION用于连接两个以上的 SELECT 语句的结果组合到一个结果集合中 必须由两条或两条以上的SELECT语句组成，语句间用UNION关键字分割； 每个SELECT语句必须包含相同的列、表达式或聚集函数，但列的次序不需要一致； 列数据类型必须兼容，能隐式转换类型也可以。 1234567SELECT field1, field2, ... field_nFROM tables[WHERE conditions]UNION [ALL | DISTINCT] -- 默认`DISTINCT`，即去重；`ALL`表示返回所有结果，不进行去重SELECT field1, field2, ... field_nFROM tables[WHERE conditions]; 插入: INSERT INTO对于不指定列名的插入语句，需要对每个列依次提供一个值，虽然这种语法很简单，但并不安全，应该尽量避免使用。123456INSERT INTO table_nameVALUES ( value1, value2, ..., valueN); 在表名后明确指定部分或全部列名，各个值将被插入到对应的列中，次序不影响结果12345678910INSERT INTO table_name ( field1, field2, ..., fieldN)VALUES ( value1, value2, ..., valueN); 还可同时插入多行数据123456789101112131415INSERT INTO table_name ( field1, field2, ..., fieldN)VALUES ( value1, value2, ..., valueN), ( value1, value2, ..., valueN); 插入从其他表中检索得到的数据(复制表)1234567891011INSERT INTO table_name ( field1, field2, ..., fieldN)SELECT field1, field2, ..., fieldNFROM another_table_name; 更新: UPDATE-SET123UPDATE table_name SET field1 = new_value1, field2 = new_value2[WHERE condition] 可以将值设置为NULL以删除它，例如用于将整列数据删除； 若用UPDATE更新多行时，其中一行或多行出现一个错误，那么整个操作被取消，可以使用IGNORE关键忽略错误行继续更新。 123UPDATE IGNORE table_name SET field1 = new_value1, field2 = new_value2[WHERE condition] 删除: DELETE FROM123DELETE FROM table_name [WHERE condition] 在使用UPDATE和DELETE时应遵循如下习惯 必须非常注意WHERE子句，防止数据丢失； 先用SELECT查询需删除的记录，确保没有问题； 保证每个表都有主键，尽可能像WHERE子句那样使用(可以指定各主键、多个值或值得范围)； 使用强制实施引用完整性的数据库，这样MySQL将不允许删除具有与其他表相关联的数据的行。 连接: JOIN-ON创建两个表如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556mysql&gt; create database RUNOOB;mysql&gt; use RUNOOB;Database changedmysql&gt; create table tcount_tbl( -&gt; runoob_author varchar(20), -&gt; runoob_count int(2) -&gt; ); Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into tcount_tbl -&gt; (runoob_author, runoob_count) -&gt; values -&gt; ("runoob", 10), -&gt; ("RUNOOB.COM", 20), -&gt; ("Google", 22); Query OK, 3 rows affected (0.01 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql&gt; create table runoob_tbl( -&gt; runoob_id int(4), -&gt; runoob_title varchar(20), -&gt; runoob_author varchar(20), -&gt; submission_date date); Query OK, 0 rows affected (0.03 sec)mysql&gt; insert into runoob_tbl -&gt; (runoob_id, runoob_title, runoob_author, submission_date) -&gt; values -&gt; (1, "PHP", "runoob", "2017-04-12"), -&gt; (2, "MySQL", "runoob", "2017-04-12"), -&gt; (3, "Java", "RUNOOB.COM", "2015-05-01"), -&gt; (4, "Python", "RUNOOB.COM", "2016-03-06"), -&gt; (5, "C", "FK", "2017-04-05");mysql&gt; SELECT * FROM tcount_tbl;+---------------+--------------+| runoob_author | runoob_count |+---------------+--------------+| runoob | 10 || RUNOOB.COM | 20 || Google | 22 |+---------------+--------------+3 rows in set (0.01 sec) mysql&gt; SELECT * from runoob_tbl;+-----------+---------------+---------------+-----------------+| runoob_id | runoob_title | runoob_author | submission_date |+-----------+---------------+---------------+-----------------+| 1 | PHP | runoob | 2017-04-12 || 2 | MySQL | runoob | 2017-04-12 || 3 | Java | RUNOOB.COM | 2015-05-01 || 4 | Python | RUNOOB.COM | 2016-03-06 || 5 | C | FK | 2017-04-05 |+-----------+---------------+---------------+-----------------+5 rows in set (0.01 sec) 创建连接: WHERE读取runoob_tbl表中所有runoob_author字段在tcount_tbl表对应的runoob_count字段值123456789101112mysql&gt; SELECT a.runoob_id, a.runoob_author, b.runoob_count -- 完全限定列名 -&gt; FROM runoob_tbl AS a, tcount_tbl AS b -&gt; WHERE a.runoob_author = b.runoob_author;+-------------+-----------------+----------------+| a.runoob_id | a.runoob_author | b.runoob_count |+-------------+-----------------+----------------+| 1 | runoob | 10 || 2 | runoob | 10 || 3 | RUNOOB.COM | 20 || 4 | RUNOOB.COM | 20 |+-------------+-----------------+----------------+4 rows in set (0.01 sec) 内连接(等值连接)：JOIN 或 INNER JOIN读取runoob_tbl表中所有runoob_author字段在tcount_tbl表对应的runoob_count字段值123456789101112mysql&gt; SELECT a.runoob_id, a.runoob_author, b.runoob_count -- 完全限定列名 -&gt; FROM runoob_tbl AS a INNER JOIN tcount_tbl AS b -&gt; ON a.runoob_author = b.runoob_author;+-------------+-----------------+----------------+| a.runoob_id | a.runoob_author | b.runoob_count |+-------------+-----------------+----------------+| 1 | runoob | 10 || 2 | runoob | 10 || 3 | RUNOOB.COM | 20 || 4 | RUNOOB.COM | 20 |+-------------+-----------------+----------------+4 rows in set (0.00 sec) 左连接：LEFT JOIN会读取左边数据表的全部数据，即便右边表无对应数据。注意tcount_tbl的runoob_author字段中存在值&quot;FK&quot;。 12345678910111213mysql&gt; SELECT a.runoob_id, a.runoob_author, b.runoob_count -&gt; FROM runoob_tbl AS a LEFT JOIN tcount_tbl AS b -&gt; ON a.runoob_author = b.runoob_author;+-------------+-----------------+----------------+| a.runoob_id | a.runoob_author | b.runoob_count |+-------------+-----------------+----------------+| 1 | runoob | 10 || 2 | runoob | 10 || 3 | RUNOOB.COM | 20 || 4 | RUNOOB.COM | 20 || 5 | FK | NULL |+-------------+-----------------+----------------+5 rows in set (0.01 sec) 右连接：RIGHT JOIN读取右边数据表的全部数据，即便左边边表无对应数据。注意tcount_tbl的runoob_count字段中存在值&quot;22&quot;。12345678910111213mysql&gt; SELECT a.runoob_id, a.runoob_author, b.runoob_count -&gt; FROM runoob_tbl AS a RIGHT JOIN tcount_tbl AS b -&gt; ON a.runoob_author = b.runoob_author;+-------------+-----------------+----------------+| a.runoob_id | a.runoob_author | b.runoob_count |+-------------+-----------------+----------------+| 1 | runoob | 10 || 2 | runoob | 10 || 3 | RUNOOB.COM | 20 || 4 | RUNOOB.COM | 20 || NULL | NULL | 22 |+-------------+-----------------+----------------+5 rows in set (0.01 sec) 函数: FUNCTION字符串函数 函数 描述 ASCII(s) 返回字符串 s 的第一个字符的 ASCII 码。 CHAR_LENGTH(s) 返回字符串 s 的字符数 CHARACTER_LENGTH(s) 返回字符串 s 的字符数 CONCAT(s1,s2…sn) 字符串 s1,s2 等多个字符串合并为一个字符串 CONCAT_WS(x, s1,s2…sn) 同 CONCAT(s1,s2,…) 函数，但是每个字符串之间要加上 x，x 可以是分隔符 FIELD(s,s1,s2…) 返回第一个字符串 s 在字符串列表(s1,s2…)中的位置 FIND_IN_SET(s1,s2) 返回在字符串s2中与s1匹配的字符串的位置 FORMAT(x,n) 函数可以将数字 x 进行格式化 “#,###.##”, 将 x 保留到小数点后 n 位，最后一位四舍五入。 INSERT(s1,x,len,s2) 字符串 s2 替换 s1 的 x 位置开始长度为 len 的字符串 LOCATE(s1,s) 从字符串 s 中获取 s1 的开始位置 LCASE(s) 将字符串 s 的所有字母变成小写字母 LOWER(s) 将字符串 s 的所有字母变成小写字母 UCASE(s) 将字符串转换为大写 UPPER(s) 将字符串转换为大写 LEFT(s,n) 返回字符串 s 的前 n 个字符 RIGHT(s,n) 返回字符串 s 的后 n 个字符 LPAD(s1,len,s2) 在字符串 s1 的开始处填充字符串 s2，使字符串长度达到 len RPAD(s1,len,s2) 在字符串 s1 的结尾处添加字符串 s2，使字符串的长度达到 len TRIM(s) 去掉字符串 s 开始和结尾处的空格 LTRIM(s) 去掉字符串 s 开始处的空格 RTRIM(s) 去掉字符串 s 结尾处的空格 MID(s,n,len) 从字符串 s 的 n 位置截取长度为 len 的子字符串，同 SUBSTRING(s,n,len) REPEAT(s,n) 将字符串 s 重复 n 次 REPLACE(s,s1,s2) 将字符串 s2 替代字符串 s 中的字符串 s1 REVERSE(s) 将字符串s的顺序反过来 SPACE(n) 返回 n 个空格 POSITION(s1 IN s) 从字符串 s 中获取 s1 的开始位置 STRCMP(s1,s2) 比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，如果 s1&gt;s2 返回 1，如果 s1 &lt; s2 返回 -1 SUBSTR(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串 SUBSTRING(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串 SUBSTRING_INDEX(s, delimiter, number) 返回从字符串 s 的第 number 个出现的分隔符 delimiter 之后的子串。如果 number 是正数，返回第 number 个字符左边的字符串；如果 number 是负数，返回第(number 的绝对值(从右边数))个字符右边的字符串。 数字函数 函数名 描述 COUNT(expression) 返回查询的记录总数，expression 参数是一个字段或者 * 号 AVG(expression) 返回一个表达式的平均值，expression 是一个字段 SUM(expression) 返回指定字段的总和 MAX(expression) 返回字段 expression 中的最大值 MIN(expression) 返回字段 expression 中的最小值 GREATEST(expr1, expr2, expr3, …) 返回列表中的最大值 LEAST(expr1, expr2, expr3, …) 返回列表中的最小值 ABS(x) 返回 x 的绝对值 SIGN(x) 返回 x 的符号，x 是负数、0、正数分别返回 -1、0 和 1 ROUND(x) 返回离 x 最近的整数 FLOOR(x) 返回小于或等于 x 的最大整数 CEIL(x) 返回大于或等于 x 的最小整数 CEILING(x) 返回大于或等于 x 的最小整数 TRUNCATE(x,y) 返回数值 x 保留到小数点后 y 位的值（与 ROUND 最大的区别是不会进行四舍五入） n DIV m 整除，n 为被除数，m 为除数 MOD(x,y) 返回 x 除以 y 以后的余数 SQRT(x) 返回x的平方根 EXP(x) 返回 e 的 x 次方 POW(x,y) 返回 x 的 y 次方 POWER(x,y) 返回 x 的 y 次方 LN(x) 返回数字的自然对数，以 e 为底。 LOG(x) 或 LOG(base, x) 返回自然对数(以 e 为底的对数)，如果带有 base 参数，则 base 为指定带底数。 LOG2(x) 返回以 2 为底的对数 LOG10(x) 返回以 10 为底的对数 PI() 返回圆周率(3.141593） DEGREES(x) 将弧度转换为角度 RADIANS(x) 将角度转换为弧度 SIN(x) 求正弦值(参数是弧度) COS(x) 求余弦值(参数是弧度) TAN(x) 求正切值(参数是弧度) COT(x) 求余切值(参数是弧度) ACOS(x) 求 x 的反余弦值(参数是弧度) ASIN(x) 求反正弦值(参数是弧度) ATAN(x) 求反正切值(参数是弧度) ATAN2(n, m) 求反正切值(参数是弧度) RAND() 返回 0 到 1 的随机数 日期函数 函数名 描述 ADDDATE(d,n) 计算起始日期 d 加上 n 天的日期 ADDTIME(t,n) 时间 t 加上 n 秒的时间 CURDATE() 返回当前日期 CURRENT_DATE() 返回当前日期 CURRENT_TIME 返回当前时间 CURRENT_TIMESTAMP() 返回当前日期和时间 CURTIME() 返回当前时间 DATE() 从日期或日期时间表达式中提取日期值 DATEDIFF(d1,d2) 计算日期 d1-&gt;d2 之间相隔的天数 ADDDATE(d，INTERVAL expr type) 计算起始日期 d 加上一个时间段后的日期 DATE_FORMAT(d,f) 按表达式 f的要求显示日期 d DATE_SUB(date,INTERVAL expr type) 函数从日期减去指定的时间间隔。 DAY(d) 返回日期值 d 的日期部分 DAYNAME(d) 返回日期 d 是星期几，如 Monday,Tuesday DAYOFMONTH(d) 计算日期 d 是本月的第几天 DAYOFWEEK(d) 日期 d 今天是星期几，1 星期日，2 星期一，以此类推 DAYOFYEAR(d) 计算日期 d 是本年的第几天 EXTRACT(type FROM d) 从日期 d 中获取指定的值，type 指定返回的值。type可取值为：MICROSECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, YEAR, SECOND_MICROSECOND, MINUTE_MICROSECOND, MINUTE_SECOND, HOUR_MICROSECOND, HOUR_SECOND, HOUR_MINUTE, DAY_MICROSECOND, DAY_SECOND, DAY_MINUTE, DAY_HOUR, YEAR_MONTH FROM_DAYS(n) 计算从 0000 年 1 月 1 日开始 n 天后的日期 HOUR(t) 返回 t 中的小时值 LAST_DAY(d) 返回给给定日期的那一月份的最后一天 LOCALTIME() 返回当前日期和时间 LOCALTIMESTAMP() 返回当前日期和时间 MAKEDATE(year, day-of-year) 基于给定参数年份 year 和所在年中的天数序号 day-of-year 返回一个日期 MAKETIME(hour, minute, second) 组合时间，参数分别为小时、分钟、秒 MICROSECOND(date) 返回日期参数所对应的微秒数 MINUTE(t) 返回 t 中的分钟值 MONTHNAME(d) 返回日期当中的月份名称，如 November MONTH(d) 返回日期d中的月份值，1 到 12 NOW() 返回当前日期和时间 PERIOD_ADD(period, number) 为 年-月 组合日期添加一个时段 PERIOD_DIFF(period1, period2) 返回两个时段之间的月份差值 QUARTER(d) 返回日期d是第几季节，返回 1 到 4 SECOND(t) 返回 t 中的秒钟值 SEC_TO_TIME(s) 将以秒为单位的时间 s 转换为时分秒的格式 STR_TO_DATE(string, format_mask) 将字符串转变为日期 SUBDATE(d,n) 日期 d 减去 n 天后的日期 SUBTIME(t,n) 时间 t 减去 n 秒的时间 SYSDATE() 返回当前日期和时间 TIME(expression) 提取传入表达式的时间部分 TIME_FORMAT(t,f) 按表达式 f 的要求显示时间 t TIME_TO_SEC(t) 将时间 t 转换为秒 TIMEDIFF(time1, time2) 计算时间差值 TIMESTAMP(expression, interval) 单个参数时，函数返回日期或日期时间表达式；有2个参数时，将参数加和 TO_DAYS(d) 计算日期 d 距离 0000 年 1 月 1 日的天数 WEEK(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53 WEEKDAY(d) 日期 d 是星期几，0 表示星期一，1 表示星期二 WEEKOFYEAR(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53 YEAR(d) 返回年份 YEARWEEK(date, mode) 返回年份及第几周（0到53），mode 中 0 表示周天，1表示周一，以此类推 其他函数 函数名 描述 CAST(x AS type) 转换数据类型 BIN(x) 返回 x 的二进制编码 BINARY(s) 将字符串 s 转换为二进制字符串 CONV(x,f1,f2) 返回 f1 进制数变成 f2 进制数 CONVERT(s USING cs) 函数将字符串 s 的字符集变成 cs LAST_INSERT_ID() 返回最近生成的 AUTO_INCREMENT 值 COALESCE(expr1, expr2, …., expr_n) 返回参数中的第一个非空表达式（从左向右） CASE expression WHEN condition1 THEN result1 WHEN condition2 THEN result2 … WHEN conditionN THEN resultN ELSE result END CASE 表示函数开始，END 表示函数结束。如果 condition1 成立，则返回 result1, 如果 condition2 成立，则返回 result2，当全部不成立则返回 result，而当有一个成立之后，后面的就不执行了。 IF(expr,v1,v2) 如果表达式 expr 成立，返回结果 v1；否则，返回结果 v2。 IFNULL(v1,v2) 如果 v1 的值不为 NULL，则返回 v1，否则返回 v2。 ISNULL(expression) 判断表达式是否为 NULL CONNECTION_ID() 返回服务器的连接数 DATABASE() 返回当前数据库名 NULLIF(expr1, expr2) 比较两个字符串，如果字符串 expr1 与 expr2 相等 返回 NULL，否则返回 expr1 CURRENT_USER() 返回当前用户 SESSION_USER() 返回当前用户 SYSTEM_USER() 返回当前用户 USER() 返回当前用户 VERSION() 返回数据库的版本号 自定义函数创建: CREATE FUNCTION - RETURNS123456789DELIMITER $$ -- sql在遇到指定的`delimiter`时，将执行前面的语句，默认为`;`CREATE FUNCTION function_name(param_name1 param_type1, ...) RETURNS return_typeBEGIN body; RETURN return_value;END$$ -- 执行语句DELIMITER ; -- 重置`delimiter` 查看: SHOW CREATE FUNCTION1SHOW CREATE FUNCTION function_name; 删除: DROP FUNCTION function_name;变量全局变量: SET @variable_name = variable_value;例如1234567891011121314-- 计算1 ~ 指定数据之间的和delimiter $$create function my_sum(x int) returns intbegin set @i = 1; set @sum = 0; while @i &lt;= x do set @sum = @sum + @i; set @i = @i + 1; end while; return @sum;end$$delimiter ; 局部变量: DECLARE variable_name variable_type DEFAULT variable_value,例如123456789101112131415161718-- 求1 ~ 指定数之前的和，但5的倍数不加delimiter $$create function my_sum2(x int) returns intbegin declare i int default 1; declare sum int default 0; sumwhile:while i &lt;= x do if i % 5 = 0 then set i = i + 1; iterate sumwhile; end if; set sum = sum + i; set i = i + 1; end while; return sum;end$$delimiter ; 调用: SELECT function_name(params_list);事务特点只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务(transaction)用于处理操作两大、复杂度高的数据，必须满足以下4个条件 原子性：一个事务中的所有操作，要么全部完成，要么全部不完成。若执行过程中发生错误，会被回滚至事务开始的状态； 一致性：事务开始前后数据库的完整性没有被破坏，写入的资料必须完全符合所有的预设规则； 隔离性：允许多个并发事务同时对数据进行读写和修改，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。分为不同级别，包括 未提交(Read uncommitted) 读提交(read committed) 可重复读(repeatable read) 串行化(Serializable) 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 控制语句: BEGIN, END, COMMIT, ROLLBACK, SAVEPOINT 开启事务：BEGIN或START TRANSACTION； 提交事务：COMMIT或COMMIT WORK，使已经对数据库进行的所有修改永久化； 回滚事务：ROLLBACK或ROLLBACK WORK，回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； 保存点：一个事务可以有多个保存点 SAVEPOINT identifier在事务中创建一个保存点； RELEASE SAVEPOINT identifier删除一个保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier把事务回滚到标记点。 SET TRANSACTION用来设置事务的隔离级别，InnoDB 存储引擎提供事务的隔离级别有 READ UNCOMMITTED READ COMMITTED REPEATABLE READ SERIALIZABLE 12345678910111213141516171819202122232425262728293031323334353637383940414243444546mysql&gt; use RUNOOB;Database changedmysql&gt; CREATE TABLE runoob_transaction_test( id int(5)) engine=innodb; -- 创建数据表Query OK, 0 rows affected (0.04 sec) mysql&gt; select * from runoob_transaction_test;Empty set (0.01 sec) mysql&gt; begin; -- 开始事务Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into runoob_transaction_test value(5);Query OK, 1 rows affected (0.01 sec) mysql&gt; insert into runoob_transaction_test value(6);Query OK, 1 rows affected (0.00 sec) mysql&gt; commit; -- 提交事务Query OK, 0 rows affected (0.01 sec) mysql&gt; select * from runoob_transaction_test;+------+| id |+------+| 5 || 6 |+------+2 rows in set (0.01 sec) mysql&gt; begin; -- 开始事务Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into runoob_transaction_test values(7);Query OK, 1 rows affected (0.00 sec) mysql&gt; rollback; -- 回滚Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from runoob_transaction_test; -- 因为回滚所以数据没有插入+------+| id |+------+| 5 || 6 |+------+2 rows in set (0.01 sec) 全文搜索LIKE关键字能用通配操作符匹配文本，但是该机制存在以下限制 性能：通配符和正则表达式匹配尝试匹配表中所有行，这非常耗时； 明确控制：很难用通配符和正则表达式明确空值匹配说明和不匹配说明； 智能化控制：不能提供一种智能化的选择结果的方法。 全本搜索必须索引被搜索的列，且随着数据的改变不断地重新索引，有以下几点说明 在索引全文本匹配时，短词被忽略且从索引中排除(短词即具有指定个字符的词，可更改)； MySQL带有内建停用词，在索引时总是被忽略(可覆盖停用词列表)； MySQL将出现在50%行以上的词作为非用词忽略，该规则不用于布尔匹配模式；因此若表中行数少于3行，则全文搜索不返回结果(因为每个词或者不出现，或者至少出现在50%行中)； 忽略词中的单引号，如don&#39;t被视作dont； 不具有词分隔符的语言(如汉语、日语)不能恰当地返回全文本搜索结果； 只有使用了 MyISAM 数据库引擎的数据库或表才支持全文搜索。 启用: FULLTEXT在创建表时，用关键字指定需要进行全文搜索的字段(一个或多个)。123456789CREATE TABLE table_name( field_name1 field_type1 [NOT NULL [AUTO_INCREMENT [DEFAULT default]]], field_name2 field_type2 [NOT NULL [AUTO_INCREMENT [DEFAULT default]]], ..., field_nameN field_typeN [NOT NULL [AUTO_INCREMENT [DEFAULT default]]], PRIMARY KEY (key1, ..., keyM), FULLTEXT (field_name1, ..., field_nameK)) ENGINE = engine_name ...; 搜索: MATCH, AGAINST用MATCH指定被搜索的列，AGAINST指定要使用的搜索表达式。 传递给MATCH的值必须与FULLTEXT中定义的相同，如果指定多个列必须列出他们，且次序正确； 除非使用BINARY方式，否则全文搜索不区分大小写； 全文本搜索根据行中词的数目、唯一词的数目、整个索引中词的总数、包含该词的行数计算得等级值。 例如123mysql&gt; select note_text -&gt; from productnotes -&gt; where match(note_text) against ("rabbit"); 可以通过以下方式查看等级值123mysql&gt; select note_text, -&gt; match(note_text) against ("rabbit") as rank -&gt; from productnotes; 查询扩展: WITH QUERY EXPANSION用于设法放宽所返回的全文本搜索结果的范围，在进行查询扩展时，MySQL对数据和索引进行两遍扫描来完成搜索 进行一个基本的全文本搜索，找出搜索条件匹配的所有行； MySQL检查这些匹配行并选择所有“有用”的词； 根据原来的条件和所有“有用”的词进行全文本搜索。 例如123mysql&gt; select note_text -&gt; from productnotes -&gt; where match(note_text) against ("rabbit" with query expansion); 布尔文本搜索: IN BOOLEAN MODE布尔方式搜索无需用FULLTEXT索引字段，可以提供如下细节的内容 要匹配的词； 要排斥的词； 排列提示； 表达式分组； 另外一些内容。 布尔操作符 说明 * 词尾通配符 + 包含，词必须存在 - 排除，词不许不出现 &gt; 包含，且增加等级值 &lt; 包含，且减少等级值 ~ 取消一个词的排序值 () 把词组成子表达式，允许这些子表达式作为一个组被包含、排除、排列等 “” 定义一个短语，允许该短语作为整体被包含、排除、排列等 例如查找包含rabbit但排除wolf123mysql&gt; select note_text -&gt; from productnotes -&gt; where match(note_text) against ("+rabbit -wolf" in boolean mode); 存储过程: PROCEDURE存储过程即为以后的使用而保存的一条或多条MySQL语句的集合，可以视作批处理文件。 创建: CREATE PROCEDURE procedure_name(…) …;可以选择带参数，用IN, OUT指定参数的输入输出(变量用@variable_name表示)。12345678910mysel&gt; create procedure productpricing( -&gt; out pl decimal(8, 2), -&gt; out ph decimal(8, 2), -&gt; out pa decimal(8, 2) -&gt; ) -&gt; begin -&gt; select min(prod_price) into pl from products; -&gt; select max(prod_price) into ph from products; -&gt; select avg(prod_price) into pa from products; -&gt; end; 删除: DROP PROCEDURE procedure_name;调用: CALL procedure_name(…);12mysql&gt; call productpricing(@pricelow, @pricehigh, @priceaverage);mysql&gt; select @pricelow, @pricehigh, @priceaverage; 视图: VIEW视图是用于查看存储于别处数据的一种设施，本事不包含数据，返回的数据是从其他表中检索出来的。常被应用于 重用SQL语句； 简化复杂的SQL操作，无需知道基本查询细节； 使用表的组成而不是整个表； 给用户授予表的指定部分的访问权限而不是整个表的访问权限； 视图可返回于底层表的表示和数据格式不同的数据。 创建: CREATE VIEW view_name AS …;123456789mysql&gt; create view productcustomers AS -&gt; select cust_name, cust_contact, prod_id -&gt; from customers, orders, orderitems -&gt; where customers.cust_id = orders.cust_id -&gt; and orderitems.order_num = orders.order_nummysql&gt; select cust_name, cust_contact -&gt; from productcustomers -&gt; where prod_id = 'TNT2' 查看: SHOW CREATE VIEW view_name;删除: DROP VIEW view_name;游标: CURSOR游标是一个存储在MySQL服务器上的数据库查询，不是一条SELECT语句，而是被该语句检索出来的结果集，存储游标后应用程序可以根据需要滚动或浏览其中的数据。 创建: DECLARE cursor_name CURSOR FOR …;打开: OPEN cursor_name;关闭: CLOSE cursor_name;触发器: TRIGGER触发器是MySQL响应DELETE, INSERT. UPDATE语句时自动执行的一组语句。略。 Reference MySQL 教程 - 菜鸟教程 Sample database with test suite 1.0.6 “employees-db-1.0.6” mysql快速生成批量测试数据 - cnblogs]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架darknet【五】——训练解析]]></title>
    <url>%2F2019%2F12%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%94%E3%80%91%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[深度学习框架darknet【一】——简单使用 深度学习框架darknet【二】——目录结构 深度学习框架darknet【三】——调包大法好 深度学习框架darknet【四】——网络配置选项 深度学习框架darknet【五】——训练解析 前言第三节介绍了如何调用函数进行网络的训练，本节扒一扒源码，详细介绍前向、反向、更新部分。查看本节内容要有BP算法的基础，以全连接网络为例详细推导过程查看全连接网络BP算法推导【矩阵形式】。 Dataflow运算过程中最重要的是数据的传递，在整个过程中，向系统申请的内存资源(部分)列表如下 内存地址指针 类型 功能 data.X matrix 输入数据，以二维矩阵存储，每行表示一个样本 data.y matrix 真实标签，以二维矩阵存储，每行表示一个样本 net-&gt;input float* 用于层间输入数据传递的指针：(1)初始化为数据读取地址d.X； (2)在forward时更新为第i层输出作为第i+1层输入； (3)在backward时更新为第i+1层输出用于第i层的参数梯度计算。若为特征图，以CHW格式储存 net-&gt;delta float* 用于层间输出梯度传递的指针，实现链式求导：在backward时更新为第i+1层输出梯度用于第i层的参数梯度计算 net-&gt;truth float* 真实标签数据指针：初始化为d.y(groundtruth)；在损失函数层进行损失计算 net-&gt;workspace float* 运算时可使用的临时工作区，如在卷积运算时需要将特征图展开为矩阵形式 l.output float* 网络层的输出，若为特征图，以CHW格式储存 l.delta float* 网络层输出对输入的梯度 l.weights float* 网络层的权重，若为特征图，以CHW格式储存 l.weight_updates float* 网络层权重的梯度值，由链式法则可知由l.delta与l.weights计算得到 l.biases float* 网络层的偏置 l.bias_updates float* 网络层偏置的梯度值 示意图如下 Entry训练的入口函数为train_netwok，获取一批次数据并进行训练，数据由(float*)d.X、(float*)d.y拷贝至(float*)net-&gt;input、(float*)net-&gt;truth。 123456789101112131415161718192021222324252627float train_network(network *net, data d)&#123; assert(d.X.rows % net-&gt;batch == 0); int batch = net-&gt;batch; int n = d.X.rows / batch; int i; float sum = 0; for(i = 0; i &lt; n; ++i)&#123; get_next_batch(d, batch, i*batch, net-&gt;input, net-&gt;truth); // d.X =&gt; net-&gt;input; d.y =&gt; net-&gt;truth float err = train_network_datum(net); sum += err; &#125; return (float)sum/(n*batch);&#125;float train_network_datum(network *net)&#123; *net-&gt;seen += net-&gt;batch; net-&gt;train = 1; forward_network(net); // 网络前向，计算各层输出 backward_network(net); // 网络反向，计算各层参数梯度 float error = *net-&gt;cost; if(((*net-&gt;seen)/net-&gt;batch)%net-&gt;subdivisions == 0) update_network(net); // 更新各层参数 return error;&#125; Forward网络前向运算会依次调用各层的forward函数，通过指针net-&gt;input实现两层间数据传递，第$i$层将net-&gt;input作为输入进行运算，结果保存在l.output，之后将net-&gt;input更新为l.output完成地址传递，用作第$i+1$层的输入。 那么有疑问，net-&gt;input在创建网络时是分配了空间的，直接替换指针不会造成内存泄露吗？注意到network* netp是原始网络，在前向运算前先定义了局部变量network net，所以运算结束改变的是net.input而不是netp-&gt;input。 在第$1$层时，先读取d.X作为输入，第$i$层接受第$i-1$层输出net-&gt;input，与自身参数l.weights、l.biases运算得到输出，结果保存在l.output。 123456789101112131415161718192021222324void forward_network(network *netp)&#123;#ifdef GPU if(netp-&gt;gpu_index &gt;= 0)&#123; forward_network_gpu(netp); return; &#125;#endif network net = *netp; int i; for(i = 0; i &lt; net.n; ++i)&#123; net.index = i; layer l = net.layers[i]; if(l.delta)&#123; fill_cpu(l.outputs * l.batch, 0, l.delta, 1); &#125; l.forward(l, net); // 各层前向 net.input = l.output; // 指针传递 if(l.truth) &#123; net.truth = l.output; &#125; &#125; calc_network_cost(netp); // 获取总体损失&#125; 以全连接层为例，在线性运算后经激活函数得到最终输出，gemm为BLAS函数，为通用的矩阵乘法接口 l.output_{l.batch \times l.outputs} = net.input_{l.batch \times l.inputs} l.weights_{l.outputs \times l.inputs}^T + l.biases_{l.outputs, broadcast}1234567891011121314151617void forward_connected_layer(layer l, network net)&#123; fill_cpu(l.outputs*l.batch, 0, l.output, 1); int m = l.batch; int k = l.inputs; int n = l.outputs; float *a = net.input; float *b = l.weights; float *c = l.output; gemm(0,1,m,n,k,1,a,k,b,k,1,c,n); if(l.batch_normalize)&#123; forward_batchnorm_layer(l, net); &#125; else &#123; add_bias(l.output, l.biases, l.batch, l.outputs, 1); &#125; activate_array(l.output, l.outputs*l.batch, l.activation);&#125; 损失函数层有所不同，运算示意图如下，除计算该层输出(损失)保存于l.output，最终损失l.cost为该批次样本损失的总和，计算输出对输入的梯度保存于l.delta123456789101112131415161718void forward_cost_layer(cost_layer l, network net)&#123; if (!net.truth) return; if(l.cost_type == MASKED)&#123; int i; for(i = 0; i &lt; l.batch*l.inputs; ++i)&#123; if(net.truth[i] == SECRET_NUM) net.input[i] = SECRET_NUM; &#125; &#125; if(l.cost_type == SMOOTH)&#123; smooth_l1_cpu(l.batch*l.inputs, net.input, net.truth, l.delta, l.output); &#125;else if(l.cost_type == L1)&#123; l1_cpu(l.batch*l.inputs, net.input, net.truth, l.delta, l.output); &#125; else &#123; l2_cpu(l.batch*l.inputs, net.input, net.truth, l.delta, l.output); &#125; l.cost[0] = sum_array(l.output, l.batch*l.inputs); // 各样本损失总和&#125; Backward依次调用各层的backward函数，对于第$i$层的反向运算，需要将第$i-1$层输出prev.output与梯度prev.delta传递到net.input与net.delta。实际上，由于指针的特性，此时net.input即前层输出prev.output，net.delta即前层梯度prev.delta，在第$i$层反向时，需依靠第$i-1$层输出，并对第$i-1$层梯度进行更新。 12345678910111213141516171819202122232425void backward_network(network *netp)&#123;#ifdef GPU if(netp-&gt;gpu_index &gt;= 0)&#123; backward_network_gpu(netp); return; &#125;#endif network net = *netp; int i; network orig = net; for(i = net.n-1; i &gt;= 0; --i)&#123; layer l = net.layers[i]; if(l.stopbackward) break; if(i == 0)&#123; net = orig; &#125;else&#123; layer prev = net.layers[i-1]; net.input = prev.output; net.delta = prev.delta; &#125; net.index = i; l.backward(l, net); &#125;&#125; 损失函数层为反向运算时的第$1$层，运算较简单 net.delta += l.scale \times l.delta1234void backward_cost_layer(const cost_layer l, network net)&#123; axpy_cpu(l.batch*l.inputs, l.scale, l.delta, 1, net.delta, 1);&#125; 示意图如下 如果省略指针传递，那么简化的运算如下 l^{prev}.delta += l.scale \times l.delta 其余各层调用各自的反传函数，先计算输出对输入的梯度l.delta，利用net.input与l.delta计算l.weight_updates、l.bias_updates，之后更新net.delta，示意图如下 如果省略指针传递，那么简化的运算如下 以全连接层为例 \begin{cases} l.weight\_updates_{l.outputs \times l.inputs} = l.delta_{l.batch \times l.outputs}^T net.input_{l.batch \times l.inputs} \\ l.bias\_updates_{l.outputs} += \sum_{i}^{l.batch} (l.delta^{(i)}_{l.outputs}) \\ l^{prev}.delta_{l.batch \times l.inputs} = l.delta_{l.batch \times l.outputs} l.weights_{l.outputs \times l.inputs} \end{cases}12345678910111213141516171819202122232425262728void backward_connected_layer(layer l, network net)&#123; gradient_array(l.output, l.outputs*l.batch, l.activation, l.delta); if(l.batch_normalize)&#123; backward_batchnorm_layer(l, net); &#125; else &#123; backward_bias(l.bias_updates, l.delta, l.batch, l.outputs, 1); &#125; int m = l.outputs; int k = l.batch; int n = l.inputs; float *a = l.delta; float *b = net.input; float *c = l.weight_updates; gemm(1,0,m,n,k,1,a,m,b,n,1,c,n); m = l.batch; k = l.outputs; n = l.inputs; a = l.delta; b = l.weights; c = net.delta; if(c) gemm(0,0,m,n,k,1,a,k,b,n,1,c,n);&#125; Update更新这一步比较简单，调用优化器参数，将当前已计算得到的参数梯度累加到网络层参数上即可 1234567891011121314151617181920212223242526272829void update_network(network *netp)&#123;#ifdef GPU if(netp-&gt;gpu_index &gt;= 0)&#123; update_network_gpu(netp); return; &#125;#endif network net = *netp; int i; update_args a = &#123;0&#125;; a.batch = net.batch*net.subdivisions; a.learning_rate = get_current_rate(netp); a.momentum = net.momentum; a.decay = net.decay; a.adam = net.adam; a.B1 = net.B1; a.B2 = net.B2; a.eps = net.eps; ++*net.t; a.t = *net.t; for(i = 0; i &lt; net.n; ++i)&#123; layer l = net.layers[i]; if(l.update)&#123; l.update(l, a); &#125; &#125;&#125; 以全连接层为例 \begin{cases} l.biases_{l.outputs} += l.bias\_updates_{l.outputs} \times \frac{learning\_rate}{batch} \\ l.weight\_updates_{l.outputs \times l.inputs} -= l.weights_{l.outputs \times l.inputs} \times decay \times batch \\ l.weights_{l.outputs \times l.inputs} += l.weight\_updates_{l.outputs \times l.inputs} \times \frac{learning\_rate}{batch} \\ l.weight\_updates_{l.outputs \times l.inputs} *= momentum \\ l.bias\_updates_{l.outputs} *= momentum \end{cases}123456789101112131415161718void update_connected_layer(layer l, update_args a)&#123; float learning_rate = a.learning_rate*l.learning_rate_scale; float momentum = a.momentum; float decay = a.decay; int batch = a.batch; axpy_cpu(l.outputs, learning_rate/batch, l.bias_updates, 1, l.biases, 1); scal_cpu(l.outputs, momentum, l.bias_updates, 1); if(l.batch_normalize)&#123; axpy_cpu(l.outputs, learning_rate/batch, l.scale_updates, 1, l.scales, 1); scal_cpu(l.outputs, momentum, l.scale_updates, 1); &#125; axpy_cpu(l.inputs*l.outputs, -decay*batch, l.weights, 1, l.weight_updates, 1); axpy_cpu(l.inputs*l.outputs, learning_rate/batch, l.weight_updates, 1, l.weights, 1); scal_cpu(l.inputs*l.outputs, momentum, l.weight_updates, 1);&#125;]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架darknet【四】——网络配置选项]]></title>
    <url>%2F2019%2F12%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E5%9B%9B%E3%80%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[深度学习框架darknet【一】——简单使用 深度学习框架darknet【二】——目录结构 深度学习框架darknet【三】——调包大法好 深度学习框架darknet【四】——网络配置选项 深度学习框架darknet【五】——训练解析 前言一些重要网络层的参数配置可以在network *parse_network_cfg(char *filename)函数中查看，进入对应层的解析函数layer parse_xxx(list *options, size_params params)，部分参数有缺省值。 配置文件书写，以alexnet为例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091[net]batch&#x3D;1subdivisions&#x3D;1height&#x3D;227width&#x3D;227channels&#x3D;3momentum&#x3D;0.9decay&#x3D;0.0005max_crop&#x3D;256learning_rate&#x3D;0.01policy&#x3D;polypower&#x3D;4max_batches&#x3D;800000angle&#x3D;7hue &#x3D; .1saturation&#x3D;.75exposure&#x3D;.75aspect&#x3D;.75[convolutional]filters&#x3D;96size&#x3D;11stride&#x3D;4pad&#x3D;0activation&#x3D;relu[maxpool]size&#x3D;3stride&#x3D;2padding&#x3D;0[convolutional]filters&#x3D;256size&#x3D;5stride&#x3D;1pad&#x3D;1activation&#x3D;relu[maxpool]size&#x3D;3stride&#x3D;2padding&#x3D;0[convolutional]filters&#x3D;384size&#x3D;3stride&#x3D;1pad&#x3D;1activation&#x3D;relu[convolutional]filters&#x3D;384size&#x3D;3stride&#x3D;1pad&#x3D;1activation&#x3D;relu[convolutional]filters&#x3D;256size&#x3D;3stride&#x3D;1pad&#x3D;1activation&#x3D;relu[maxpool]size&#x3D;3stride&#x3D;2padding&#x3D;0[connected]output&#x3D;4096activation&#x3D;relu[dropout]probability&#x3D;.5[connected]output&#x3D;4096activation&#x3D;relu[dropout]probability&#x3D;.5[connected]output&#x3D;1000activation&#x3D;linear[softmax]groups&#x3D;1 net option dtype default function batch int 1 单个批次数据量 max_batches int 0 训练最大迭代次数 learning_rate float .001 学习率 momentum float .9 动量 decay float .0001 权重衰减 subdivisions int 1 对批次进行细分 time_steps int 1 循环神经网络使用，时间步数 notruth int 0 未知 random int 0 classifier.c中使用，是否随机改变图像载入参数 adam int 0 是否使用adam优化器 B1 float .9 adam优化器参数 B2 float .999 adam优化器参数 eps float .0000001 adam优化器参数 height int 0 输入图像高度，必须指定 width int 0 输入图像宽度，必须指定 channels int 0 输入图像通道数，必须指定 inputs int height width channels 输入图像尺寸 max_crop int width * 2 图像裁剪参数 min_crop int width 图像裁剪参数 max_ratio float max_crop / width 图像裁剪参数 min_ratio float max_crop / width 图像裁剪参数 center int 0 未知 clip float 0 未知 angle float 0 图像扩增参数 aspect float 1 图像扩增参数 saturation float 1 图像扩增参数 exposure float 1 图像扩增参数 hue float 0 图像扩增参数 policy char* constant 学习率调整策略，可选random, poly, constant, step, exp, sigmoid, steps burn_in int 0 学习率调整策略参数 power float 4 学习率调整策略参数 step int 1 学习率调整策略参数 scale float 1 学习率调整策略参数 steps int, … / 学习率调整策略参数 scales float,… / 学习率调整策略参数 gamma float 1 学习率调整策略参数 layer option dtype default function clip float net-&gt;clip 图像裁剪 truth int 0 是否以输出作为groundtruth onlyforward int 0 仅前向 stopbackward int 0 该层不进行反向 dontsave int 0 不保存该层参数 dontload int 0 不读取该层参数 numload int 0 卷积层使用，指定卷积核个数 dontloadscales int 0 不读取batchnorm参数 learning_rate float 1 学习率，在网络学习率基础上累乘 smooth float 0 平滑 convolutional option dtype default function filters int 1 卷积核个数 size int 1 卷积核尺寸 stride int 1 卷积步长 pad int 0 卷积是否填充，若填充padding=size/2 padding int 0 卷积填充值，若pad=1则该值无效指定 groups int 1 卷积组个数 activation char* logistic 激活函数类型 batch_normalize int 0 是否使用batchnorm binary int 0 是否二值化参数 xnor int 0 未知 flipped int 0 未知 dot int float 未知 deconvolutional option dtype default function filters int 1 卷积核个数 size int 1 卷积核尺寸 stride int 1 卷积步长 activation char* logistic 激活函数类型 batch_normalize int 0 是否使用batchnorm pad int 0 卷积是否填充，若填充padding=size/2 padding int 0 卷积填充值，若pad=1则该值无效指定 rnn option dtype default function output int 1 输出维数 activation char* logistic 激活函数类型 batch_normalize int 0 是否使用batchnorm shortcut int 0 是否使用shortcut gru option dtype default function output int 1 输出维数 batch_normalize int 0 是否使用batchnorm tanh int 0 是否使用tanh lstm option dtype default function output int 1 输出维数 batch_normalize int 0 是否使用batchnorm crnn option dtype default function output_filters int 1 输出特征通道数 hidden_filters int 1 隐层特征通道数 activation char* logistic 激活函数类型 batch_normalize int 0 是否使用batchnorm shortcut int 0 是否使用shortcut connected option dtype default function output int 1 输出维数 activation char* logistic 激活函数类型 batch_normalize int 0 是否使用batchnorm crop option dtype default function crop_height int 1 裁剪高度 crop_width int 1 裁剪宽度 flip int 0 是否翻转 angle float 0 旋转角度 saturation float 1 饱和度 exposure float 1 曝光 cost option dtype default function type char* sse 损失类型 scale float 1 损失倍率 ratio float 0 未知 noobj float 1 未知 thresh float 0 未知 batchnorm option dtype default function 无 / / / maxpool option dtype default function stride int 1 步长 size int stride 尺寸 padding int size - 1 填充 avgpool option dtype default function 无 / / / upsample option dtype default function stride int 2 步长 scale float 1 未知 shortcut option dtype default function from int / 指定来自某层的连接，可为相对层数如from=-4表示从4层前连接，或绝对层数如from=4表示第4层连接 activation char* linear 激活函数类型 alpha float 1 详情查看shortcut_layer.c/backward_shortcut_layer beta float 1 详情查看shortcut_layer.c/backward_shortcut_layer dropout option dtype default function probability float .5 dropout概率]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架darknet【三】——调包大法好]]></title>
    <url>%2F2019%2F12%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%89%E3%80%91%E2%80%94%E2%80%94%E8%B0%83%E5%8C%85%E5%A4%A7%E6%B3%95%E5%A5%BD%2F</url>
    <content type="text"><![CDATA[深度学习框架darknet【一】——简单使用 深度学习框架darknet【二】——目录结构 深度学习框架darknet【三】——调包大法好 深度学习框架darknet【四】——网络配置选项 深度学习框架darknet【五】——训练解析 前言本节以YOLO为例，介绍如何调用darknet函数来进行网络训练和测试，对框架有一个整体的认识。 Main主函数入口在examples/darknet.c/main，命令行指定参数yolo即可进入yolo的运行函数段12$ ./darknet yolousage: ./darknet yolo [train/test/valid] [cfg] [weights (optional)] 由以下代码可知，可选参数2有 test：测试 train：训练 valid：验证 recall：验证，计算召回率等指标 demo：样机，视频演示，需OpenCV支持 void run_yolo(int argc, char **argv) void run_yolo(int argc, char **argv) { char *prefix = find_char_arg(argc, argv, "-prefix", 0); float thresh = find_float_arg(argc, argv, "-thresh", .2); int cam_index = find_int_arg(argc, argv, "-c", 0); int frame_skip = find_int_arg(argc, argv, "-s", 0); if(argc < 4){ fprintf(stderr, "usage: %s %s [train/test/valid] [cfg] [weights (optional)]\n", argv[0], argv[1]); return; } int avg = find_int_arg(argc, argv, "-avg", 1); char *cfg = argv[3]; char *weights = (argc > 4) ? argv[4] : 0; char *filename = (argc > 5) ? argv[5]: 0; if(0==strcmp(argv[2], "test")) test_yolo(cfg, weights, filename, thresh); else if(0==strcmp(argv[2], "train")) train_yolo(cfg, weights); else if(0==strcmp(argv[2], "valid")) validate_yolo(cfg, weights); else if(0==strcmp(argv[2], "recall")) validate_yolo_recall(cfg, weights); else if(0==strcmp(argv[2], "demo")) demo(cfg, weights, thresh, cam_index, filename, voc_names, 20, frame_skip, prefix, avg, .5, 0,0,0,0); } 本节只介绍基本的训练(train)与测试(test)，其余均为拓展。 Training Stage指定参数2为train，即可进入训练函数段，提示无训练数据/data/voc/train.txt1234567891011$ ./darknet yolo train cfg/yolov3.cfg yolov3.weightsyolov3layer filters size input output 0 conv 32 3 x 3 / 1 608 x 608 x 3 -&gt; 608 x 608 x 32 0.639 BFLOPs 1 conv 64 3 x 3 / 2 608 x 608 x 32 -&gt; 304 x 304 x 64 3.407 BFLOPs... # 略 105 conv 255 1 x 1 / 1 76 x 76 x 256 -&gt; 76 x 76 x 255 0.754 BFLOPs 106 yoloLoading weights from yolov3.weights...Done!Learning Rate: 0.001, Momentum: 0.9, Decay: 0.0005Couldn't open file: /data/voc/train.txt 在训练函数段内，主要做的工作有以下几个部分，分别进行说明 网络构建 数据载入 训练，包括前向、反向、参数更新 void train_yolo(char *cfgfile, char *weightfile) void train_yolo(char *cfgfile, char *weightfile) { char *train_images = "/data/voc/train.txt"; char *backup_directory = "/home/pjreddie/backup/"; srand(time(0)); char *base = basecfg(cfgfile); printf("%s\n", base); float avg_loss = -1; // 网络构建 network *net = load_network(cfgfile, weightfile, 0); printf("Learning Rate: %g, Momentum: %g, Decay: %g\n", net->learning_rate, net->momentum, net->decay); // 数据载入 int imgs = net->batch*net->subdivisions; int i = *net->seen/imgs; data train, buffer; layer l = net->layers[net->n - 1]; int side = l.side; int classes = l.classes; float jitter = l.jitter; list *plist = get_paths(train_images); //int N = plist->size; char **paths = (char **)list_to_array(plist); load_args args = {0}; args.w = net->w; args.h = net->h; args.paths = paths; args.n = imgs; args.m = plist->size; args.classes = classes; args.jitter = jitter; args.num_boxes = side; args.d = &buffer; args.type = REGION_DATA; args.angle = net->angle; args.exposure = net->exposure; args.saturation = net->saturation; args.hue = net->hue; pthread_t load_thread = load_data_in_thread(args); // 开始训练 clock_t time; while(get_current_batch(net) < net->max_batches){ i += 1; time=clock(); pthread_join(load_thread, 0); train = buffer; load_thread = load_data_in_thread(args); printf("Loaded: %lf seconds\n", sec(clock()-time)); time=clock(); float loss = train_network(net, train); if (avg_loss < 0) avg_loss = loss; avg_loss = avg_loss*.9 + loss*.1; printf("%d: %f, %f avg, %f rate, %lf seconds, %d images\n", i, loss, avg_loss, get_current_rate(net), sec(clock()-time), i*imgs); if(i%1000==0 || (i < 1000 && i%100 == 0)){ char buff[256]; sprintf(buff, "%s/%s_%d.weights", backup_directory, base, i); save_weights(net, buff); } free_data(train); } char buff[256]; sprintf(buff, "%s/%s_final.weights", backup_directory, base); save_weights(net, buff); } Load Network通过load_network可快速构建网络，来看一下该函数做了哪些事情1network *net = load_network(cfgfile, weightfile, 0); network *load_network(char *cfg, char *weights, int clear) network *load_network(char *cfg, char *weights, int clear) { network *net = parse_network_cfg(cfg); if(weights && weights[0] != 0){ load_weights(net, weights); } if(clear) (*net->seen) = 0; return net; } 构建框架 network *parse_network_cfg(char *filename) network *parse_network_cfg(char *filename) { list *sections = read_cfg(filename); node *n = sections->front; if(!n) error("Config file has no sections"); network *net = make_network(sections->size - 1); net->gpu_index = gpu_index; size_params params; section *s = (section *)n->val; list *options = s->options; if(!is_network(s)) error("First section must be [net] or [network]"); parse_net_options(options, net); params.h = net->h; params.w = net->w; params.c = net->c; params.inputs = net->inputs; params.batch = net->batch; params.time_steps = net->time_steps; params.net = net; size_t workspace_size = 0; n = n->next; int count = 0; free_section(s); fprintf(stderr, "layer filters size input output\n"); while(n){ params.index = count; fprintf(stderr, "%5d ", count); s = (section *)n->val; options = s->options; layer l = {0}; LAYER_TYPE lt = string_to_layer_type(s->type); if(lt == CONVOLUTIONAL){ l = parse_convolutional(options, params); }else if(lt == DECONVOLUTIONAL){ l = parse_deconvolutional(options, params); }else if(lt == LOCAL){ l = parse_local(options, params); }else if(lt == ACTIVE){ l = parse_activation(options, params); }else if(lt == LOGXENT){ l = parse_logistic(options, params); }else if(lt == L2NORM){ l = parse_l2norm(options, params); }else if(lt == RNN){ l = parse_rnn(options, params); }else if(lt == GRU){ l = parse_gru(options, params); }else if (lt == LSTM) { l = parse_lstm(options, params); }else if(lt == CRNN){ l = parse_crnn(options, params); }else if(lt == CONNECTED){ l = parse_connected(options, params); }else if(lt == CROP){ l = parse_crop(options, params); }else if(lt == COST){ l = parse_cost(options, params); }else if(lt == REGION){ l = parse_region(options, params); }else if(lt == YOLO){ l = parse_yolo(options, params); }else if(lt == ISEG){ l = parse_iseg(options, params); }else if(lt == DETECTION){ l = parse_detection(options, params); }else if(lt == SOFTMAX){ l = parse_softmax(options, params); net->hierarchy = l.softmax_tree; }else if(lt == NORMALIZATION){ l = parse_normalization(options, params); }else if(lt == BATCHNORM){ l = parse_batchnorm(options, params); }else if(lt == MAXPOOL){ l = parse_maxpool(options, params); }else if(lt == REORG){ l = parse_reorg(options, params); }else if(lt == AVGPOOL){ l = parse_avgpool(options, params); }else if(lt == ROUTE){ l = parse_route(options, params, net); }else if(lt == UPSAMPLE){ l = parse_upsample(options, params, net); }else if(lt == SHORTCUT){ l = parse_shortcut(options, params, net); }else if(lt == DROPOUT){ l = parse_dropout(options, params); l.output = net->layers[count-1].output; l.delta = net->layers[count-1].delta; #ifdef GPU l.output_gpu = net->layers[count-1].output_gpu; l.delta_gpu = net->layers[count-1].delta_gpu; #endif }else{ fprintf(stderr, "Type not recognized: %s\n", s->type); } l.clip = net->clip; l.truth = option_find_int_quiet(options, "truth", 0); l.onlyforward = option_find_int_quiet(options, "onlyforward", 0); l.stopbackward = option_find_int_quiet(options, "stopbackward", 0); l.dontsave = option_find_int_quiet(options, "dontsave", 0); l.dontload = option_find_int_quiet(options, "dontload", 0); l.numload = option_find_int_quiet(options, "numload", 0); l.dontloadscales = option_find_int_quiet(options, "dontloadscales", 0); l.learning_rate_scale = option_find_float_quiet(options, "learning_rate", 1); l.smooth = option_find_float_quiet(options, "smooth", 0); option_unused(options); net->layers[count] = l; if (l.workspace_size > workspace_size) workspace_size = l.workspace_size; free_section(s); n = n->next; ++count; if(n){ params.h = l.out_h; params.w = l.out_w; params.c = l.out_c; params.inputs = l.outputs; } } free_list(sections); layer out = get_network_output_layer(net); net->outputs = out.outputs; net->truths = out.outputs; if(net->layers[net->n-1].truths) net->truths = net->layers[net->n-1].truths; net->output = out.output; net->input = calloc(net->inputs*net->batch, sizeof(float)); net->truth = calloc(net->truths*net->batch, sizeof(float)); #ifdef GPU net->output_gpu = out.output_gpu; net->input_gpu = cuda_make_array(net->input, net->inputs*net->batch); net->truth_gpu = cuda_make_array(net->truth, net->truths*net->batch); #endif if(workspace_size){ //printf("%ld\n", workspace_size); #ifdef GPU if(gpu_index >= 0){ net->workspace = cuda_make_array(0, (workspace_size-1)/sizeof(float)+1); }else { net->workspace = calloc(1, workspace_size); } #else net->workspace = calloc(1, workspace_size); #endif } return net; } 首先调用read_cfg读取*.cfg文件，解析为链表结构 list *read_cfg(char *filename) list *read_cfg(char *filename) { FILE *file = fopen(filename, "r"); if(file == 0) file_error(filename); char *line; int nu = 0; list *options = make_list(); section *current = 0; while((line=fgetl(file)) != 0){ ++ nu; strip(line); switch(line[0]){ case '[': current = malloc(sizeof(section)); list_insert(options, current); current->options = make_list(); current->type = line; break; case '\0': case '#': case ';': free(line); break; default: if(!read_option(line, current->options)){ fprintf(stderr, "Config file error line %d, could parse: %s\n", nu, line); free(line); } break; } } fclose(file); return options; } 例如，对于内容为以下的文件 1234567891011121314151617[net]batch&#x3D;1height&#x3D;227width&#x3D;227channels&#x3D;3[convolutional]filters&#x3D;96size&#x3D;11stride&#x3D;4pad&#x3D;0activation&#x3D;relu[maxpool]size&#x3D;3stride&#x3D;2padding&#x3D;0 该函数将其解析为链表list* sections，其元素为section* 1234typedef struct&#123; char *type; // 记录`[xxx]` list *options; // 记录选项&#125;section; sections -&gt; s -&gt; options也为链表，其元素为kvp*，用以存储键值对 12345typedef struct&#123; char *key; char *val; int used;&#125; kvp; 示意图如下 之后按上述输出的参数，构建网络 解析网络参数 12345network *net = make_network(sections-&gt;size - 1);section *s = (section *)n-&gt;val;list *options = s-&gt;options;if(!is_network(s)) error("First section must be [net] or [network]");parse_net_options(options, net); void parse_net_options(list *options, network *net) void parse_net_options(list *options, network *net) { net->batch = option_find_int(options, "batch",1); net->learning_rate = option_find_float(options, "learning_rate", .001); net->momentum = option_find_float(options, "momentum", .9); net->decay = option_find_float(options, "decay", .0001); int subdivs = option_find_int(options, "subdivisions",1); net->time_steps = option_find_int_quiet(options, "time_steps",1); net->notruth = option_find_int_quiet(options, "notruth",0); net->batch /= subdivs; net->batch *= net->time_steps; net->subdivisions = subdivs; net->random = option_find_int_quiet(options, "random", 0); net->adam = option_find_int_quiet(options, "adam", 0); if(net->adam){ net->B1 = option_find_float(options, "B1", .9); net->B2 = option_find_float(options, "B2", .999); net->eps = option_find_float(options, "eps", .0000001); } net->h = option_find_int_quiet(options, "height",0); net->w = option_find_int_quiet(options, "width",0); net->c = option_find_int_quiet(options, "channels",0); net->inputs = option_find_int_quiet(options, "inputs", net->h * net->w * net->c); net->max_crop = option_find_int_quiet(options, "max_crop",net->w*2); net->min_crop = option_find_int_quiet(options, "min_crop",net->w); net->max_ratio = option_find_float_quiet(options, "max_ratio", (float) net->max_crop / net->w); net->min_ratio = option_find_float_quiet(options, "min_ratio", (float) net->min_crop / net->w); net->center = option_find_int_quiet(options, "center",0); net->clip = option_find_float_quiet(options, "clip", 0); net->angle = option_find_float_quiet(options, "angle", 0); net->aspect = option_find_float_quiet(options, "aspect", 1); net->saturation = option_find_float_quiet(options, "saturation", 1); net->exposure = option_find_float_quiet(options, "exposure", 1); net->hue = option_find_float_quiet(options, "hue", 0); if(!net->inputs && !(net->h && net->w && net->c)) error("No input parameters supplied"); char *policy_s = option_find_str(options, "policy", "constant"); net->policy = get_policy(policy_s); net->burn_in = option_find_int_quiet(options, "burn_in", 0); net->power = option_find_float_quiet(options, "power", 4); if(net->policy == STEP){ net->step = option_find_int(options, "step", 1); net->scale = option_find_float(options, "scale", 1); } else if (net->policy == STEPS){ char *l = option_find(options, "steps"); char *p = option_find(options, "scales"); if(!l || !p) error("STEPS policy must have steps and scales in cfg file"); int len = strlen(l); int n = 1; int i; for(i = 0; i < len; ++i){ if (l[i] == ',') ++n; } int *steps = calloc(n, sizeof(int)); float *scales = calloc(n, sizeof(float)); for(i = 0; i < n; ++i){ int step = atoi(l); float scale = atof(p); l = strchr(l, ',')+1; p = strchr(p, ',')+1; steps[i] = step; scales[i] = scale; } net->scales = scales; net->steps = steps; net->num_steps = n; } else if (net->policy == EXP){ net->gamma = option_find_float(options, "gamma", 1); } else if (net->policy == SIG){ net->gamma = option_find_float(options, "gamma", 1); net->step = option_find_int(options, "step", 1); } else if (net->policy == POLY || net->policy == RANDOM){ } net->max_batches = option_find_int(options, "max_batches", 0); } 解析各层参数根据不同类型构建网络，注意不同类型的网络层需定义layer parse_[lt](list *options, size_params params)函数，以申请内存资源等 1234567891011121314151617181920212223242526272829303132333435363738394041424344size_params params;... // 尺寸参数n = n-&gt;next;int count = 0;free_section(s);fprintf(stderr, "layer filters size input output\n");while(n)&#123; // 各层遍历 params.index = count; fprintf(stderr, "%5d ", count); s = (section *)n-&gt;val; options = s-&gt;options; layer l = &#123;0&#125;; LAYER_TYPE lt = string_to_layer_type(s-&gt;type); if(lt == CONVOLUTIONAL)&#123; l = parse_convolutional(options, params); &#125;else if(...)&#123; ... // 判断类型并构建对应网络层 &#125;else&#123; fprintf(stderr, "Type not recognized: %s\n", s-&gt;type); &#125; // 一些其他参数 l.clip = net-&gt;clip; ... l.smooth = option_find_float_quiet(options, "smooth", 0); option_unused(options); net-&gt;layers[count] = l; if (l.workspace_size &gt; workspace_size) workspace_size = l.workspace_size; free_section(s); n = n-&gt;next; ++count; // 更新特征图尺寸，用做下一层网络层的输入尺寸 if(n)&#123; params.h = l.out_h; params.w = l.out_w; params.c = l.out_c; params.inputs = l.outputs; &#125;&#125;free_list(sections); 以卷积层为例 convolutional_layer parse_convolutional(list *options, size_params params) convolutional_layer parse_convolutional(list *options, size_params params) { // 寻找参数，若无指定使用默认值 int n = option_find_int(options, "filters",1); int size = option_find_int(options, "size",1); int stride = option_find_int(options, "stride",1); int pad = option_find_int_quiet(options, "pad",0); int padding = option_find_int_quiet(options, "padding",0); int groups = option_find_int_quiet(options, "groups", 1); if(pad) padding = size/2; char *activation_s = option_find_str(options, "activation", "logistic"); ACTIVATION activation = get_activation(activation_s); // 输入尺寸参数 int batch,h,w,c; h = params.h; w = params.w; c = params.c; batch=params.batch; if(!(h && w && c)) error("Layer before convolutional layer must output image."); int batch_normalize = option_find_int_quiet(options, "batch_normalize", 0); int binary = option_find_int_quiet(options, "binary", 0); int xnor = option_find_int_quiet(options, "xnor", 0); // 创建网络层 convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,groups,size,stride,padding,activation, batch_normalize, binary, xnor, params.net->adam); layer.flipped = option_find_int_quiet(options, "flipped", 0); layer.dot = option_find_float_quiet(options, "dot", 0); return layer; } 内存指针分配网络输出即最后一层out输出，两者共享内存 1234567891011layer out = get_network_output_layer(net);net-&gt;outputs = out.outputs;net-&gt;truths = out.outputs;if(net-&gt;layers[net-&gt;n-1].truths) net-&gt;truths = net-&gt;layers[net-&gt;n-1].truths;net-&gt;output = out.output;net-&gt;input = calloc(net-&gt;inputs*net-&gt;batch, sizeof(float));net-&gt;truth = calloc(net-&gt;truths*net-&gt;batch, sizeof(float));if(workspace_size)&#123; net-&gt;workspace = calloc(1, workspace_size);&#125; 载入权值 load_weights(network *net, char *filename) void load_weights(network *net, char *filename) { load_weights_upto(net, filename, 0, net->n); } void load_weights_upto(network *net, char *filename, int start, int cutoff) { fprintf(stderr, "Loading weights from %s...", filename); fflush(stdout); FILE *fp = fopen(filename, "rb"); if(!fp) file_error(filename); int major; int minor; int revision; fread(&major, sizeof(int), 1, fp); fread(&minor, sizeof(int), 1, fp); fread(&revision, sizeof(int), 1, fp); if ((major*10 + minor) >= 2 && major < 1000 && minor < 1000){ fread(net->seen, sizeof(size_t), 1, fp); } else { int iseen = 0; fread(&iseen, sizeof(int), 1, fp); *net->seen = iseen; } int transpose = (major > 1000) || (minor > 1000); int i; for(i = start; i < net->n && i < cutoff; ++i){ layer l = net->layers[i]; if (l.dontload) continue; if(l.type == CONVOLUTIONAL || l.type == DECONVOLUTIONAL){ load_convolutional_weights(l, fp); } if(l.type == CONNECTED){ load_connected_weights(l, fp, transpose); } if(l.type == BATCHNORM){ load_batchnorm_weights(l, fp); } if(l.type == CRNN){ load_convolutional_weights(*(l.input_layer), fp); load_convolutional_weights(*(l.self_layer), fp); load_convolutional_weights(*(l.output_layer), fp); } if(l.type == RNN){ load_connected_weights(*(l.input_layer), fp, transpose); load_connected_weights(*(l.self_layer), fp, transpose); load_connected_weights(*(l.output_layer), fp, transpose); } if (l.type == LSTM) { load_connected_weights(*(l.wi), fp, transpose); load_connected_weights(*(l.wf), fp, transpose); load_connected_weights(*(l.wo), fp, transpose); load_connected_weights(*(l.wg), fp, transpose); load_connected_weights(*(l.ui), fp, transpose); load_connected_weights(*(l.uf), fp, transpose); load_connected_weights(*(l.uo), fp, transpose); load_connected_weights(*(l.ug), fp, transpose); } if (l.type == GRU) { if(1){ load_connected_weights(*(l.wz), fp, transpose); load_connected_weights(*(l.wr), fp, transpose); load_connected_weights(*(l.wh), fp, transpose); load_connected_weights(*(l.uz), fp, transpose); load_connected_weights(*(l.ur), fp, transpose); load_connected_weights(*(l.uh), fp, transpose); }else{ load_connected_weights(*(l.reset_layer), fp, transpose); load_connected_weights(*(l.update_layer), fp, transpose); load_connected_weights(*(l.state_layer), fp, transpose); } } if(l.type == LOCAL){ int locations = l.out_w*l.out_h; int size = l.size*l.size*l.c*l.n*locations; fread(l.biases, sizeof(float), l.outputs, fp); fread(l.weights, sizeof(float), size, fp); } } fprintf(stderr, "Done!\n"); fclose(fp); } 权值载入比较简单，根据每层的网络参数格式，以二进制格式读取即可。需要注意的是，文件头需读写4个整形int的数据 12345678910111213int major;int minor;int revision;fread(&amp;major, sizeof(int), 1, fp);fread(&amp;minor, sizeof(int), 1, fp);fread(&amp;revision, sizeof(int), 1, fp);if ((major*10 + minor) &gt;= 2 &amp;&amp; major &lt; 1000 &amp;&amp; minor &lt; 1000)&#123; fread(net-&gt;seen, sizeof(size_t), 1, fp);&#125; else &#123; int iseen = 0; fread(&amp;iseen, sizeof(int), 1, fp); *net-&gt;seen = iseen;&#125; 以卷积层为例，需要读取卷积核weights、偏置biases，在无批归一batchnorm的情况下，其读取函数为 12345678910void load_convolutional_weights(layer l, FILE *fp)&#123; if(l.numload) l.n = l.numload; int num = l.c/l.groups*l.n*l.size*l.size; fread(l.biases, sizeof(float), l.n, fp); fread(l.weights, sizeof(float), num, fp); if (l.flipped) &#123; transpose_matrix(l.weights, l.c*l.size*l.size, l.n); &#125;&#125; Load Data训练过程中关于数据载入的代码整理如下，通过多线程的方式进行数据读取。 12345678910111213141516171819202122232425262728293031323334353637383940414243char *train_images = "/data/voc/train.txt";int imgs = net-&gt;batch*net-&gt;subdivisions;int i = *net-&gt;seen/imgs;data train, buffer;layer l = net-&gt;layers[net-&gt;n - 1];// 读取文件列表list *plist = get_paths(train_images);char **paths = (char **)list_to_array(plist);// 数据读取参数load_args args = &#123;0&#125;;args.paths = paths;args.w = net-&gt;w;args.h = net-&gt;h;args.n = imgs;args.m = plist-&gt;size;args.classes = l.classes;args.jitter = l.jitter;args.num_boxes = l.side;args.d = &amp;buffer;args.type = REGION_DATA;args.angle = net-&gt;angle;args.exposure = net-&gt;exposure;args.saturation = net-&gt;saturation;args.hue = net-&gt;hue;pthread_t load_thread = load_data_in_thread(args);clock_t time;while(get_current_batch(net) &lt; net-&gt;max_batches)&#123; i += 1; time = clock(); pthread_join(load_thread, 0); // 主线程阻塞，等待load_thread线程结束 train = buffer; load_thread = load_data_in_thread(args);// 创建新线程 printf("Loaded: %lf seconds\n", sec(clock()-time)); ... // 网络训练 free_data(train);&#125; 以下两个函数用于读取文本文件，每行存储对应的样本路径，char **paths每个内存单元指向字符串首地址。 list *get_paths(char *filename) list *get_paths(char *filename) { char *path; FILE *file = fopen(filename, "r"); if(!file) file_error(filename); list *lines = make_list(); while((path=fgetl(file))){ list_insert(lines, path); } fclose(file); return lines; } void **list_to_array(list *l) void **list_to_array(list *l) { void **a = calloc(l->size, sizeof(void*)); int count = 0; node *n = l->front; while(n){ a[count++] = n->val; n = n->next; } return a; } 重点关注多线程数据载入的部分，先介绍Linux系统下线程控制的函数，以下资料来源于百度百科123456789101112131415函数定义：int pthread_join(pthread_t thread, void **retval);描述：pthread_join()函数，以阻塞的方式等待thread指定的线程结束。当函数返回时，被等待线程的资源被收回。如果线程已经结束，那么该函数会立即返回。并且thread指定的线程必须是joinable的参数： - thread: 线程标识符，即线程ID，标识唯一线程； - retval: 用户定义的指针，用来存储被等待线程的返回值。返回值：0代表成功，若失败则返回错误号函数定义：int pthread_create(pthread_t *tidp, const pthread_attr_t *attr, (void*)(*start_rtn)(void*),void *arg);描述：pthread_create是类Unix操作系统（Unix、Linux、Mac OS X等）的创建线程的函数。它的功能是创建线程（实际上就是确定调用该线程函数的入口点），在线程创建以后，就开始运行相关的线程函数。参数： - tidp: 指向线程标识符的指针； - attr: 设置线程属性； - start_rtn: 线程运行函数的起始地址； - arg: 运行函数的参数。返回值：表示成功，返回0；表示出错，返回-1。 pthread_t load_data_in_thread(load_args args) pthread_t load_data_in_thread(load_args args) { pthread_t thread; struct load_args *ptr = calloc(1, sizeof(struct load_args)); *ptr = args; if(pthread_create(&thread, 0, load_thread, ptr)) error("Thread creation failed"); return thread; } 以上函数用于线程启动，在某批次数据读取结束前，主线程将被阻塞。读取结束后，train = buffer将缓冲区指针传递给train，再重新启动线程。注意在线程函数内，缓冲区buffer各指针将被分配新的内存地址，以防止下一批数据对当前批数据造成影响，因此当前批数据所占内存资源需要通过free_data(train)来释放。 void *load_thread(void *ptr) void *load_thread(void *ptr) { load_args a = *(struct load_args*)ptr; if(a.exposure == 0) a.exposure = 1; if(a.saturation == 0) a.saturation = 1; if(a.aspect == 0) a.aspect = 1; if (a.type == OLD_CLASSIFICATION_DATA){ *a.d = load_data_old(a.paths, a.n, a.m, a.labels, a.classes, a.w, a.h); } else if (a.type == REGRESSION_DATA){ *a.d = load_data_regression(a.paths, a.n, a.m, a.classes, a.min, a.max, a.size, a.angle, a.aspect, a.hue, a.saturation, a.exposure); } else if (a.type == CLASSIFICATION_DATA){ *a.d = load_data_augment(a.paths, a.n, a.m, a.labels, a.classes, a.hierarchy, a.min, a.max, a.size, a.angle, a.aspect, a.hue, a.saturation, a.exposure, a.center); } else if (a.type == SUPER_DATA){ *a.d = load_data_super(a.paths, a.n, a.m, a.w, a.h, a.scale); } else if (a.type == WRITING_DATA){ *a.d = load_data_writing(a.paths, a.n, a.m, a.w, a.h, a.out_w, a.out_h); } else if (a.type == ISEG_DATA){ *a.d = load_data_iseg(a.n, a.paths, a.m, a.w, a.h, a.classes, a.num_boxes, a.scale, a.min, a.max, a.angle, a.aspect, a.hue, a.saturation, a.exposure); } else if (a.type == INSTANCE_DATA){ *a.d = load_data_mask(a.n, a.paths, a.m, a.w, a.h, a.classes, a.num_boxes, a.coords, a.min, a.max, a.angle, a.aspect, a.hue, a.saturation, a.exposure); } else if (a.type == SEGMENTATION_DATA){ *a.d = load_data_seg(a.n, a.paths, a.m, a.w, a.h, a.classes, a.min, a.max, a.angle, a.aspect, a.hue, a.saturation, a.exposure, a.scale); } else if (a.type == REGION_DATA){ *a.d = load_data_region(a.n, a.paths, a.m, a.w, a.h, a.num_boxes, a.classes, a.jitter, a.hue, a.saturation, a.exposure); } else if (a.type == DETECTION_DATA){ *a.d = load_data_detection(a.n, a.paths, a.m, a.w, a.h, a.num_boxes, a.classes, a.jitter, a.hue, a.saturation, a.exposure); } else if (a.type == SWAG_DATA){ *a.d = load_data_swag(a.paths, a.n, a.classes, a.jitter); } else if (a.type == COMPARE_DATA){ *a.d = load_data_compare(a.n, a.paths, a.m, a.classes, a.w, a.h); } else if (a.type == IMAGE_DATA){ *(a.im) = load_image_color(a.path, 0, 0); *(a.resized) = resize_image(*(a.im), a.w, a.h); } else if (a.type == LETTERBOX_DATA){ *(a.im) = load_image_color(a.path, 0, 0); *(a.resized) = letterbox_image(*(a.im), a.w, a.h); } else if (a.type == TAG_DATA){ *a.d = load_data_tag(a.paths, a.n, a.m, a.classes, a.min, a.max, a.size, a.angle, a.aspect, a.hue, a.saturation, a.exposure); } free(ptr); return 0; } 那么线程启动后，执行的函数void *load_thread(void *ptr)做了哪些工作呢？首先初始化数据读取参数，对应不同类型的数据，分别进入对应的数据读取子函数，例如分类、检测、图像等等，读取的数据以指针的形式存放在a.d中，最后释放在线程创建时申请的参数指针ptr。 data load_data_region(int n, char **paths, int m, int w, int h, int size, int classes, float jitter, float hue, float saturation, float exposure) data load_data_region(int n, char **paths, int m, int w, int h, int size, int classes, float jitter, float hue, float saturation, float exposure) { char **random_paths = get_random_paths(paths, n, m); int i; data d = {0}; d.shallow = 0; d.X.rows = n; d.X.vals = calloc(d.X.rows, sizeof(float*)); d.X.cols = h*w*3; int k = size*size*(5+classes); d.y = make_matrix(n, k); for(i = 0; i < n; ++i){ image orig = load_image_color(random_paths[i], 0, 0); int oh = orig.h; int ow = orig.w; int dw = (ow*jitter); int dh = (oh*jitter); int pleft = rand_uniform(-dw, dw); int pright = rand_uniform(-dw, dw); int ptop = rand_uniform(-dh, dh); int pbot = rand_uniform(-dh, dh); int swidth = ow - pleft - pright; int sheight = oh - ptop - pbot; float sx = (float)swidth / ow; float sy = (float)sheight / oh; int flip = rand()%2; image cropped = crop_image(orig, pleft, ptop, swidth, sheight); float dx = ((float)pleft/ow)/sx; float dy = ((float)ptop /oh)/sy; image sized = resize_image(cropped, w, h); if(flip) flip_image(sized); random_distort_image(sized, hue, saturation, exposure); d.X.vals[i] = sized.data; fill_truth_region(random_paths[i], d.y.vals[i], classes, size, flip, dx, dy, 1./sx, 1./sy); free_image(orig); free_image(cropped); } free(random_paths); return d; } 对于YOLO，其载入的数据类型为REGION_DATA，所以进入到load_data_region函数，这里需要关注的为数据存放的格式。可以看到，d.X为matrix类型，其行数rows与列数cols在下面函数中分别指定为n与h*w*3，即尺寸为n × h*w*3，每行表示一副图像数据。 12345678910d.X.rows = n;d.X.cols = h*w*3;d.X.vals = calloc(d.X.rows, sizeof(float*)); // 该指针用于存放内存空间的首地址，即“指针的指针”for(i = 0; i &lt; n; ++i)&#123; image orig = load_image_color(random_paths[i], 0, 0); ... // 数据扩增、缩放等常规操作 d.X.vals[i] = sized.data; fill_truth_region(random_paths[i], d.y.vals[i], classes, size, flip, dx, dy, 1./sx, 1./sy); // 获取groundtrurh，存储在`d.y`中 ... // 内存释放&#125; Forward, Backward and Update读取数据后，训练网络已被包装为函数train_network1float loss = train_network(net, train); 1234567891011121314float train_network(network *net, data d)&#123; assert(d.X.rows % net-&gt;batch == 0); int batch = net-&gt;batch; int n = d.X.rows / batch; int i; float sum = 0; for(i = 0; i &lt; n; ++i)&#123; get_next_batch(d, batch, i*batch, net-&gt;input, net-&gt;truth); // 获取下一批次数据 float err = train_network_datum(net); // 前向、反向、更新 sum += err; &#125; return (float)sum/(n*batch);&#125; 其中获取数据部分，将d.X.vals与d.y.vals内容分别拷贝至net-&gt;input与net-&gt;truth void get_next_batch(data d, int n, int offset, float *X, float *y) void get_next_batch(data d, int n, int offset, float *X, float *y) { int j; for(j = 0; j < n; ++j){ int index = offset + j; memcpy(X+j*d.X.cols, d.X.vals[index], d.X.cols*sizeof(float)); if(y) memcpy(y+j*d.y.cols, d.y.vals[index], d.y.cols*sizeof(float)); } } 训练部分，net-&gt;input作为输入，前向计算得到net-&gt;cost，再反向计算得到各层梯度，之后梯度累加到各层参数上进行参数更新 float train_network_datum(network *net) float train_network_datum(network *net) { *net->seen += net->batch; net->train = 1; forward_network(net); backward_network(net); float error = *net->cost; if(((*net->seen)/net->batch)%net->subdivisions == 0) update_network(net); return error; } 依次调用各层的l.forward，l.backward，l.update forward, backward, update void forward_network(network *netp) { network net = *netp; int i; for(i = 0; i < net.n; ++i){ net.index = i; layer l = net.layers[i]; if(l.delta){ fill_cpu(l.outputs * l.batch, 0, l.delta, 1); } l.forward(l, net); net.input = l.output; if(l.truth) { net.truth = l.output; } } calc_network_cost(netp); } void backward_network(network *netp) { network net = *netp; int i; network orig = net; for(i = net.n-1; i >= 0; --i){ layer l = net.layers[i]; if(l.stopbackward) break; if(i == 0){ net = orig; }else{ layer prev = net.layers[i-1]; net.input = prev.output; net.delta = prev.delta; } net.index = i; l.backward(l, net); } } void update_network(network *netp) { network net = *netp; int i; update_args a = {0}; a.batch = net.batch*net.subdivisions; a.learning_rate = get_current_rate(netp); a.momentum = net.momentum; a.decay = net.decay; a.adam = net.adam; a.B1 = net.B1; a.B2 = net.B2; a.eps = net.eps; ++*net.t; a.t = *net.t; for(i = 0; i < net.n; ++i){ layer l = net.layers[i]; if(l.update){ l.update(l, a); } } } 注意在初始化网络层时，结构体struct layer中已初始化三个函数指针1234567struct layer&#123; ... // 略 void (*forward) (struct layer, struct network); void (*backward) (struct layer, struct network); void (*update) (struct layer, update_args); ... // 略&#125; 以卷积层为例，make_convolutional_layer中123l.forward = forward_convolutional_layer;l.backward = backward_convolutional_layer;l.update = update_convolutional_layer; Test Stage指定参数2为test，可进入测试函数段，提示输入图片路径123456789$ ./darknet yolo test cfg/yolov3.cfg yolov3.weightslayer filters size input output 0 conv 32 3 x 3 / 1 608 x 608 x 3 -&gt; 608 x 608 x 32 0.639 BFLOPs 1 conv 64 3 x 3 / 2 608 x 608 x 32 -&gt; 304 x 304 x 64 3.407 BFLOPs... # 略 105 conv 255 1 x 1 / 1 76 x 76 x 256 -&gt; 76 x 76 x 255 0.754 BFLOPs 106 yoloLoading weights from yolov3.weights...Done!Enter Image Path: 原始测试函数examples/yolo.c/test_yolo并不能实现检测功能(detector.c/test_detector可以使用)，所以将其修改为以下 void test_yolo(char *cfgfile, char *weightfile, char *filename, float thresh) void test_yolo(char *cfgfile, char *weightfile, char *filename, float thresh) { // 获取标签名称、字母表等 list *options = read_data_cfg("cfg/coco.data"); char *name_list = option_find_str(options, "names", "data/names.list"); char **names = get_labels(name_list); image **alphabet = load_alphabet(); // 构建网络 network *net = load_network(cfgfile, weightfile, 0); set_batch_network(net, 1); // 一些参数。。。 srand(2222222); double time; char buff[256]; char *input = buff; float nms=.45; while(1){ if(filename){ strncpy(input, filename, 256); } else { printf("Enter Image Path: "); fflush(stdout); input = fgets(input, 256, stdin); if(!input) return; strtok(input, "\n"); } image im = load_image_color(input,0,0); image sized = letterbox_image(im, net->w, net->h); layer l = net->layers[net->n-1]; float *X = sized.data; time=what_time_is_it_now(); network_predict(net, X); printf("%s: Predicted in %f seconds.\n", input, what_time_is_it_now()-time); int nboxes = 0; detection *dets = get_network_boxes(net, im.w, im.h, thresh, 0, 0, 1, &nboxes); if (nms) do_nms_sort(dets, nboxes, l.classes, nms); draw_detections(im, dets, nboxes, thresh, names, alphabet, l.classes); free_detections(dets, nboxes); save_image(im, "predictions"); #ifdef OPENCV make_window("predictions", 512, 512, 0); show_image(im, "predictions", 0); #endif free_image(im); free_image(sized); if(filename) break; } } 主要的过程如下 读取图像、裁剪并缩放 12image im = load_image_color(input,0,0);image sized = letterbox_image(im, net-&gt;w, net-&gt;h); image letterbox_image(image im, int w, int h) image letterbox_image(image im, int w, int h) { int new_w = im.w; int new_h = im.h; if (((float)w/im.w) < ((float)h/im.h)) { new_w = w; new_h = (im.h * w)/im.w; } else { new_h = h; new_w = (im.w * h)/im.h; } image resized = resize_image(im, new_w, new_h); image boxed = make_image(w, h, im.c); fill_image(boxed, .5); //int i; //for(i = 0; i < boxed.w*boxed.h*boxed.c; ++i) boxed.data[i] = 0; embed_image(resized, boxed, (w-new_w)/2, (h-new_h)/2); free_image(resized); return boxed; } 前向运算 12345layer l = net-&gt;layers[net-&gt;n-1];float *X = sized.data;time = what_time_is_it_now();network_predict(net, X);printf("%s: Predicted in %f seconds.\n", input, what_time_is_it_now()-time); 根据结果生成候选框 12int nboxes = 0; detection *dets = get_network_boxes(net, im.w, im.h, thresh, 0, 0, 1, &amp;nboxes); detection *get_network_boxes(network *net, int w, int h, float thresh, float hier, int *map, int relative, int *num) detection *get_network_boxes(network *net, int w, int h, float thresh, float hier, int *map, int relative, int *num) { detection *dets = make_network_boxes(net, thresh, num); fill_network_boxes(net, w, h, thresh, hier, map, relative, dets); return dets; } YOLO详细算法不过多介绍。 非极大值抑制(NMS) 1if (nms) do_nms_sort(dets, nboxes, l.classes, nms); void do_nms_sort(detection *dets, int total, int classes, float thresh) void do_nms_sort(detection *dets, int total, int classes, float thresh) { int i, j, k; k = total-1; for(i = 0; i]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架darknet【二】——目录结构]]></title>
    <url>%2F2019%2F12%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%BA%8C%E3%80%91%E2%80%94%E2%80%94%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[深度学习框架darknet【一】——简单使用 深度学习框架darknet【二】——目录结构 深度学习框架darknet【三】——调包大法好 深度学习框架darknet【四】——网络配置选项 深度学习框架darknet【五】——训练解析 Tree1234567891011121314151617181920$ tree -L 1.├── cfg/├── data/├── examples/├── include/├── python/├── scripts/├── src/├── LICENSE├── LICENSE.fuck├── LICENSE.gen├── LICENSE.gpl├── LICENSE.meta├── LICENSE.mit├── LICENSE.v1├── Makefile└── README.md7 directories, 9 files cfg/：网络结构123456789101112131415161718darknet/cfg$ tree.├── alexnet.cfg├── ... # 略├── yolo9000.cfg├── yolov1.cfg├── yolov1-tiny.cfg├── yolov2.cfg├── yolov2-tiny.cfg├── yolov2-tiny-voc.cfg├── yolov2-voc.cfg├── yolov3.cfg├── yolov3-openimages.cfg├── yolov3-spp.cfg├── yolov3-tiny.cfg└── yolov3-voc.cfg0 directories, 52 files cfg/内存放的*.cfg文件，用于记录网络结构，类似Caffe的*.prototxt文件，通过调用darknet.h/network *parse_network_cfg(char *filename)，可读取指定的*.cfg文件，快速构建深度神经网络。 include/，src/：底层实现include/内仅包含全局头文件darknet.h，主要用于定义枚举(enum)、结构体(struct)和声明常用函数，几个重要的定义声明整理如下(不全) 名称 类型 作用 ACTIVATION enum 激活函数类型 LAYER_TYPE enum 网络层类型 COST_TYPE enum 损失类型 learning_rate_policy enum 学习率调整策略类型 data_type enum 数据载入类型 image struct 图像结构，数据存放格式为CHW update_args struct 优化器参数 layer struct 面向网络层对象，通用的网络层结构定义，除必要的内存指针(weight, delta, output等)，还包括前向(forward)、反向(backward)、参数更新(update)等函数指针 network struct 面向网络对象，通用的网络结构定义，包括网络层指针(layers)、优化参数(batch, epoch, learning_rate, momentum, decay等)、输入输出内存指针(input, truth, delta, workspace等) augment_args struct 图像扩增属性 box struct 边界框，包含x, y, w, h data struct 用于读取数据，X与y为二维矩阵matrix，每行表示一个样本 load_args struct 数据读取参数 network load_network(char cfg, char *weights, int clear) function 载入网络，内部调用parse_network_cfg与load_weights float train_networks(network **nets, int n, data d, int interval) function 训练网络，内部调用前向、反向、更新等函数 network parse_network_cfg(char filename) function 解析*.cfg文件以快速构建网络 list read_cfg(char filename) function 读取配置文件 void save_weights(network net, char filename) function 保存网络权重 void load_weights(network net, char filename) function 读取网络权重 void save_weights_upto(network net, char filename, int cutoff) function 可指定截断的网络权重保存 void load_weights_upto(network net, char filename, int start, int cutoff) function 可指定截断的网络权重读取 void forward_network(network *net) function 网络前向运算，依次调用各层前向运算函数 void backward_network(network *net) function 网络反向运算求取梯度，依次调用各层反向运算函数 void update_network(network *net) function 更新网络参数，依次调用各层参数更新函数 void forward_network_gpu(network *net) function 网络前向运算，CPU加速 void backward_network_gpu(network *net) function 网络反向运算求取梯度，CPU加速 void update_network_gpu(network *net) function 更新网络参数，CPU加速 void cuda_set_device(int n) function 指定GPU设备 void cuda_free(float *x_gpu) function 释放指针对应的显存 float cuda_make_array(float x, size_t n) function 申请固定字节数的显存 void cuda_pull_array(float x_gpu, float x, size_t n) function 从显存读取数据，存放至内存 void cuda_push_array(float x_gpu, float x, size_t n) function 从内存读取数据，存放至显存 float network_predict(network net, float *input) function 指定输入，前向运算并输出结果 int resize_network(network *net, int w, int h) function 调整网络输入尺寸，相应改变各层尺寸 void free_network(network *net) function 释放网络所占资源，内部释放各层资源 void free_layer(layer) function 释放网络层所占资源 list read_data_cfg(char filename) function 读取数据参数配置文件 load_args get_base_args(network *net) function 读取网络参数以确定数据读取参数 pthread_t load_data(load_args args) function 创建数据读取线程 void free_data(data d) function 释放网络层所占资源 image load_image_color(char *filename, int w, int h) function 读取3通道图 image resize_image(image im, int w, int h) function 图像尺寸调整 image crop_image(image im, int dx, int dy, int w, int h) function 图像裁剪 image rotate_image(image m, float rad) function 图像旋转 void flip_image(image a) function 图像翻转 int show_image(image p, const char *name, int ms) function 图像显示，若OpenCV可用则显示，否则保存图像 void free_image(image m) function 释放图像所占资源 … … … examples/：算法实现调用include/与src/底层函数，实现如yolo、darknet、rnn等算法，如yolo主函数内可进行训练、测试、演示等操作。123456789101112131415161718192021void run_yolo(int argc, char **argv)&#123; char *prefix = find_char_arg(argc, argv, "-prefix", 0); float thresh = find_float_arg(argc, argv, "-thresh", .2); int cam_index = find_int_arg(argc, argv, "-c", 0); int frame_skip = find_int_arg(argc, argv, "-s", 0); if(argc &lt; 4)&#123; fprintf(stderr, "usage: %s %s [train/test/valid] [cfg] [weights (optional)]\n", argv[0], argv[1]); return; &#125; int avg = find_int_arg(argc, argv, "-avg", 1); char *cfg = argv[3]; char *weights = (argc &gt; 4) ? argv[4] : 0; char *filename = (argc &gt; 5) ? argv[5]: 0; if(0==strcmp(argv[2], "test")) test_yolo(cfg, weights, filename, thresh); else if(0==strcmp(argv[2], "train")) train_yolo(cfg, weights); else if(0==strcmp(argv[2], "valid")) validate_yolo(cfg, weights); else if(0==strcmp(argv[2], "recall")) validate_yolo_recall(cfg, weights); else if(0==strcmp(argv[2], "demo")) demo(cfg, weights, thresh, cam_index, filename, voc_names, 20, frame_skip, prefix, avg, .5, 0,0,0,0);&#125; python/：提供接口调用ctypes模块，可在python中调用darknet库，如检测算法调用12345678910111213141516171819def detect(net, meta, image, thresh=.5, hier_thresh=.5, nms=.45): im = load_image(image, 0, 0) # 读取图片 num = c_int(0) pnum = pointer(num) predict_image(net, im) # 前向运算 dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, None, 0, pnum) # 获取边界框 num = pnum[0] if (nms): do_nms_obj(dets, num, meta.classes, nms) # NMS res = [] for j in range(num): for i in range(meta.classes): if dets[j].prob[i] &gt; 0: b = dets[j].bbox res.append((meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h))) res = sorted(res, key=lambda x: -x[1]) free_image(im) free_detections(dets, num) return res scripts/：功能脚本一些自动化脚本，如下载数据、生成样本标签等，略。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架darknet【一】——简单使用]]></title>
    <url>%2F2019%2F12%2F08%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6darknet%E3%80%90%E4%B8%80%E3%80%91%E2%80%94%E2%80%94%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[深度学习框架darknet【一】——简单使用 深度学习框架darknet【二】——目录结构 深度学习框架darknet【三】——调包大法好 深度学习框架darknet【四】——网络配置选项 深度学习框架darknet【五】——训练解析 前言目前可用于深度学习的著名框架有 tensorflow pytorch keras theano caffe … 这些给广大调参狗带来便利，但不利于学生党理解底层实现，例如神经网络的BP算法。 darknet是C语言实现的深度学习开源代码，细细品读，收获颇丰，开源万岁！ 附上官网链接和Github地址 YOLO: Real-Time Object Detection pjreddie/darknet: Convolutional Neural Networks AlexeyAB/darknet: Windows and Linux version of Darknet Yolo v3 &amp; v2 Neural Networks for object detection (Tensor Cores are used) Linux安装与使用首先，从Github下载作者的源码123456$ git clone https://github.com/pjreddie/darknet.gitCloning into 'darknet'...remote: Enumerating objects: 5901, done.remote: Total 5901 (delta 0), reused 0 (delta 0), pack-reused 5901Receiving objects: 100% (5901/5901), 6.16 MiB | 10.00 KiB/s, done.Resolving deltas: 100% (3915/3915), done. 其目录结构如下123456789101112131415161718192021$ cd darknet$ tree -L 1.├── cfg/├── data/├── examples/├── include/├── python/├── scripts/├── src/├── LICENSE├── LICENSE.fuck├── LICENSE.gen├── LICENSE.gpl├── LICENSE.meta├── LICENSE.mit├── LICENSE.v1├── Makefile└── README.md7 directories, 9 files 这其中包括YOLO的算法实现，若需尝试检测算法先进行编译，在Makefile中指定编译选项，由于现在使用的PC显存较小(1G)，暂时不采用CUDA加速(GPU=0, CUDNN=0)；Darknet可不安装图像库，但不支持图像显示，为了便于展示，编译加入本机已安装的OpenCV(OPENCV=1)12345GPU=0CUDNN=0OPENCV=1OPENMP=0DEBUG=0 开始编译，编译完成后，除了新建三个目录obj/, backup/, results/外，还生成可执行文件darknet, 静态连接库libdarknet.a, 动态链接库libdarknet.so12345678$ makemkdir -p obj # 新建目录 obj/mkdir -p backup # 新建目录 backup/mkdir -p results # 新建目录 results/gcc -Iinclude/ -Isrc/ -DOPENCV `pkg-config --cflags opencv` -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -DOPENCV -c ./src/gemm.c -o obj/gemm.o... # 略gcc -Iinclude/ -Isrc/ -DOPENCV `pkg-config --cflags opencv` -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -DOPENCV -c ./examples/darknet.c -o obj/darknet.ogcc -Iinclude/ -Isrc/ -DOPENCV `pkg-config --cflags opencv` -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -DOPENCV obj/captcha.o obj/lsd.o obj/super.o obj/art.o obj/tag.o obj/cifar.o obj/go.o obj/rnn.o obj/segmenter.o obj/regressor.o obj/classifier.o obj/coco.o obj/yolo.o obj/detector.o obj/nightmare.o obj/instance-segmenter.o obj/darknet.o libdarknet.a -o darknet -lm -pthread `pkg-config --libs opencv` -lstdc++ libdarknet.a 尝试运行YOLO3检测算法，下载权重、选择cfg文件，并指定图片进行检测123456789101112131415$ wget https://pjreddie.com/media/files/yolov3.weights... # 略$ ./darknet detect cfg/yolov3.cfg yolov3.weights data/fastfurious.jpeg layer filters size input output 0 conv 32 3 x 3 / 1 608 x 608 x 3 -&gt; 608 x 608 x 32 0.639 BFLOPs 1 conv 64 3 x 3 / 2 608 x 608 x 32 -&gt; 304 x 304 x 64 3.407 BFLOPs 2 conv 32 1 x 1 / 1 304 x 304 x 64 -&gt; 304 x 304 x 32 0.379 BFLOPs... # 略 105 conv 255 1 x 1 / 1 76 x 76 x 256 -&gt; 76 x 76 x 255 0.754 BFLOPs 106 yoloLoading weights from yolov3.weights...Done!data/fastfurious.jpeg: Predicted in 27.746693 seconds.car: 100%person: 100% 放上大光头靓照，检测效果非常好，由于未使用CUDA，所以检测速度较慢(27.7s) Windows安装与使用 新建项目文件夹project，复制darknet_AlexeyAB中文件到该目录下 选择无gpu版本，打开project/build/darknet/darknet_no_gpu.sln 修改版本为release 安装OpenCV3 一键build！如果需要生成.dll动态链接库，则配置项目属性 -&gt; 常规 -&gt; 目标文件扩展名为.dll 从https://pjreddie.com/media/files/yolov3.weights下载yolo_v3权重，放到放在project/build/darknet/x64目录，执行12345678910111213141516171819202122232425262728&gt; .&#x2F;darknet_no_gpu.exe detect cfg&#x2F;yolov3.cfg yolov3.weights data&#x2F;dog.jpglayer filters size input output 0 conv 32 3 x 3 &#x2F; 1 416 x 416 x 3 -&gt; 416 x 416 x 32 0.299 BF 1 conv 64 3 x 3 &#x2F; 2 416 x 416 x 32 -&gt; 208 x 208 x 64 1.595 BF 2 conv 32 1 x 1 &#x2F; 1 208 x 208 x 64 -&gt; 208 x 208 x 32 0.177 BF 3 conv 64 3 x 3 &#x2F; 1 208 x 208 x 32 -&gt; 208 x 208 x 64 1.595 BF 4 Shortcut Layer: 1 5 conv 128 3 x 3 &#x2F; 2 208 x 208 x 64 -&gt; 104 x 104 x 128 1.595 BF 6 conv 64 1 x 1 &#x2F; 1 104 x 104 x 128 -&gt; 104 x 104 x 64 0.177 BF 7 conv 128 3 x 3 &#x2F; 1 104 x 104 x 64 -&gt; 104 x 104 x 128 1.595 BF ... 94 yolo 95 route 91 96 conv 128 1 x 1 &#x2F; 1 26 x 26 x 256 -&gt; 26 x 26 x 128 0.044 BF 97 upsample 2x 26 x 26 x 128 -&gt; 52 x 52 x 128 98 route 97 36 99 conv 128 1 x 1 &#x2F; 1 52 x 52 x 384 -&gt; 52 x 52 x 128 0.266 BF 100 conv 256 3 x 3 &#x2F; 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BF 101 conv 128 1 x 1 &#x2F; 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BF 102 conv 256 3 x 3 &#x2F; 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BF 103 conv 128 1 x 1 &#x2F; 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BF 104 conv 256 3 x 3 &#x2F; 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BF 105 conv 255 1 x 1 &#x2F; 1 52 x 52 x 256 -&gt; 52 x 52 x 255 0.353 BF 106 yoloTotal BFLOPS 65.864Loading weights from yolov3.weights...Done! Reference Windows配置darknet - 楷尘·极客 - CSDN博客]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[全连接网络BP算法推导【矩阵形式】]]></title>
    <url>%2F2019%2F12%2F08%2F%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9CBP%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%E3%80%90%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E3%80%91%2F</url>
    <content type="text"><![CDATA[前言在Feedforward Neural Network/5. 梯度推导中介绍了前馈神经网络的BP算法，但是推导结果为对元素的梯度，不便于编程且影响计算效率。本节以3层前馈全连接神经网络为例，介绍矩阵形式的BP算法推导。 网络定义定义多层全连接网络：【输入(input)】 -&gt; 全连接层 -&gt; sigmoid -&gt; 【隐藏层(hidden)】-&gt; 全连接层 -&gt; 【输出(output)】 -&gt; 损失函数层 -&gt; 损失值(loss)。 示意图如下(已省略激活函数层) 其符号定义为 $\vec{x}^{(l-1)}$: 第$l$层的输入向量，维度为$n_{l-1} \times 1$； $\vec{x}^{(l)}$: 第$l$层的输出向量，维度为$n_l \times 1$； $f^{(l)}(\vec{x})$: 第$l$层输入输出映射，即 \vec{x}^{(l)} = f^{(l)}(\vec{x}_{l-1}) \tag{1} 全连接层: f^{(l)}(\vec{x}) = W^{(l)} \vec{x} + \vec{b}^{(l)} \tag{1.1} 其中$W^{(l)}$, $\vec{b}^{(l)}$分别表示权重与偏置，维度分别为$n_l \times n_{l-1}$, $n_l \times 1$； 激活函数层: f^{(l)}(\vec{x}) = \sigma(\vec{x}) \tag{1.2.1} 当激活函数采用sigmoid函数时，有 f^{(l)}(x_j) = \frac{1}{1 + e^{- x_j}} \tag{1.2.2} 损失函数层： f^{(l)}(\vec{x}) = L(\vec{x}, \vec{t}) \tag{1.3.1} 取MSE损失时，有 L(\vec{x}, \vec{t}) = \frac{1}{2} ||\vec{x} - \vec{t}||_2^2 = \frac{1}{2} (\vec{x} - \vec{t})^T (\vec{x} - \vec{t}) \tag{1.3.2} 其中$\vec{t}$表示groudtruth，$\vec{x}$表示网络的输出。 则有 \begin{aligned} \vec{x}^{(1)} = W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)} \\ \vec{x}^{(2)} = \sigma(\vec{x}^{(1)}) \\ \vec{x}^{(3)} = W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)} \\ x^{(4)} = L(\vec{x}^{(3)}, \vec{t}) \\ \end{aligned} \tag{2} W^{(l)} \vec{x}^{(l-1)} + \vec{b}^{(l)} = \begin{bmatrix} W^{(l)}_{11} & W^{(l)}_{12} & \cdots & W^{(l)}_{1n_{l-1}} \\ \vdots & \vdots & \ddots & \vdots \\ W^{(l)}_{n_l1} & W^{(l)}_{n_l2} & \cdots & W^{(l)}_{n_l n_{l-1}} \\ \end{bmatrix} \begin{bmatrix} x^{(l-1)}_1 \\ \vdots \\ x^{(l-1)}_{n_{l-1}} \\ \end{bmatrix} + \begin{bmatrix} \vec{b}^{(l)}_1 \\ \vdots \\ \vec{b}^{(l)}_{n_l} \\ \end{bmatrix} 梯度推导 标量对矩阵、向量的梯度见附录。 每层输出$\vec{x}^{(l)} = f^{(l)}(\vec{x}^{(l-1)})$对输入$\vec{x}^{(l-1)}$的梯度为$f’^{(l)}(\vec{x}^{(l-1)})$(雅可比矩阵)，记作$p^{(l)}$，即 p^{(l)} = \frac{\partial f^{(l)}(\vec{x}^{(l-1)})}{\partial \vec{x}^{(l-1)T}} \tag{3}则对于已定义的网络，有 全连接层(1)(向量对向量的梯度)： p^{(1)} = \frac{\partial }{\partial \vec{x}^{(0)T}} (W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)}) = \frac{\partial \vec{x}^{(1)}}{\partial \vec{x}^{(0)T}} = W^{(1)} \tag{3.1} 激活函数层(2)(向量对向量的梯度)： p^{(2)} = \frac{\partial }{\partial \vec{x}^{(1)T}} \sigma(\vec{x}^{(1)}) = \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)T}} = \rm{diag}[\sigma(\vec{x}^{(1)}) \odot (1 - \sigma(\vec{x}^{(1)}))] \tag{3.2} 全连接层(3)(向量对向量的梯度)： p^{(3)} = \frac{\partial }{\partial \vec{x}^{(2)T}} (W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)}) = \frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)T}} = W^{(3)} \tag{3.3} 损失函数层(4)(标量对向量的梯度)： p^{(4)} = \frac{\partial }{\partial \vec{x}^{(3)}} [\frac{1}{2} (\vec{x}^{(3)} - \vec{t})^T (\vec{x}^{(3)} - \vec{t})] = \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} = \vec{x}^{(3)} - \vec{t} \tag{3.4} n_1 = n_2; \quad n_4 = 1 待更新参数为$W^{(1)}, \vec{b}^{(1)}, W^{(3)}, \vec{b}^{(3)}$。 损失函数$L$对第$3$层参数$W^{(3)}, \vec{b}^{(3)}$的梯度，有 \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(3)}} = \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \frac{\partial f^{(3)}(\vec{x}^{(2)})}{\partial W^{(3)}} = \underbrace{p^{(4)}}_{n_3 \times 1} \underbrace{\frac{\partial}{\partial W^{(3)}} (W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)})}_{1 \times n_2} = p^{(4)} \vec{x}^{(2)T} \tag{4.1} \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(3)}} = (\frac{\partial f^{(3)}(\vec{x}^{(2)})}{\partial \vec{b}^{(3)T}})^T \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} = \underbrace{\frac{\partial}{\partial \vec{b}^{(3)T}} (W^{(3)} \vec{x}^{(2)} + \vec{b}^{(3)})}_{I_{n_3 \times n_3}} \underbrace{p^{(4)}}_{n_3 \times 1} = p^{(4)} \tag{4.2} 损失函数$L$对第$1$层参数$W^{(1)}, \vec{b}^{(1)}$的梯度，有 \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(1)}} = \frac{\partial x^{(4)}}{\partial \vec{x}^{(1)}} \frac{\partial f^{(1)}(\vec{x}^{(0)})}{\partial W^{(1)}} \tag{5.1} 其中$\frac{\partial x^{(4)}}{\partial \vec{x}^{(1)}}$为标量对向量的梯度，由式$(7.2)$可得 \frac{\partial x^{(4)}}{\partial \vec{x}^{(1)}} = (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(1)}})^T \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} = (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)}} \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)}})^T \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \tag{5.2} 代入式$(5.1)$得 \begin{aligned} \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(1)}} \\ = \left[ (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)}} \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)}})^T \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \right] \frac{\partial f^{(1)}(\vec{x}^{(0)})}{\partial W^{(1)}} \\ = \underbrace{p^{(2)T}}_{n_2 \times n_2} \underbrace{p^{(3)T}}_{n_2 \times n_3} \underbrace{p^{(4)}}_{n_3 \times 1} \underbrace{\frac{\partial}{\partial W^{(1)}} (W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)})}_{1 \times n_0} = p^{(2)T} p^{(3)T} p^{(4)} \vec{x}^{(0)T} \end{aligned} \tag{5.3} \begin{aligned} \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(1)}} = \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \\ = \left[ (\frac{\partial \vec{x}^{(3)}}{\partial \vec{x}^{(2)}} \frac{\partial \vec{x}^{(2)}}{\partial \vec{x}^{(1)}})^T \frac{\partial x^{(4)}}{\partial \vec{x}^{(3)}} \right] \frac{\partial f^{(1)}(\vec{x}^{(0)})}{\partial \vec{b}^{(1)}} \\ = \underbrace{p^{(2)T}}_{n_2 \times n_2} \underbrace{p^{(3)T}}_{n_2 \times n_3} \underbrace{p^{(4)}}_{n_3 \times 1} \underbrace{\frac{\partial}{\partial \vec{b}^{(1)}} (W^{(1)} \vec{x}^{(0)} + \vec{b}^{(1)})}_{I_{n_0 \times n_0}} = p^{(2)T} p^{(3)T} p^{(4)} \end{aligned} \tag{5.4} 如果记 \begin{aligned} \vec{\delta}^{(4)} = p^{(4)} = \vec{x}^{(3)} - \vec{t} \\ \vec{\delta}^{(3)} = p^{(3)T} \vec{\delta}^{(4)} = W^{(3)T} \vec{\delta}^{(4)} \\ \vec{\delta}^{(2)} = p^{(2)T} \vec{\delta}^{(3)} = \rm{diag}[\sigma(\vec{x}^{(1)}) \odot (1 - \vec{x}^{(1)})]^T \vec{\delta}^{(3)} \\ \end{aligned}最终得到梯度表达式 \begin{aligned} \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(3)}} = \vec{\delta}^{(4)} x^{(2)T} \\ \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(3)}} = \vec{\delta}^{(4)} \\ \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial W^{(1)}} = \vec{\delta}^{(2)} \vec{x}^{(0)T} \\ \frac{\partial L(\vec{x}^{(3)}, \vec{t})}{\partial \vec{b}^{(1)}} = \vec{\delta}^{(2)} \end{aligned} \tag{*}附录：矩阵形式的链式求导 向量对向量 假设三个向量满足依赖关系$\vec{x} \rightarrow \vec{y} \rightarrow \vec{z}$，分别为$m, n, p$维向量，则满足 \underbrace{\frac{\partial \vec{z}}{\partial \vec{x}}}_{p \times m} = \underbrace{\frac{\partial \vec{z}}{\partial \vec{y}}}_{p \times n} \underbrace{\frac{\partial \vec{y}}{\partial \vec{x}}}_{n \times m} \tag{7.1} 注意：若其中任意变量为矩阵时上式子不成立，如$\vec{x} \rightarrow Y \rightarrow \vec{z}$ 标量对向量 假设有依赖关系$\vec{\vec{x}} \rightarrow \vec{\vec{y}} \rightarrow z$，分别为$m, n$维向量与标量，则$\frac{\partial z}{\partial \vec{x}}$维度为$m \times 1$，$\frac{\partial z}{\partial \vec{y}}$维度为$n \times 1$，$\frac{\partial \vec{y}}{\partial \vec{x}}$维度为$n \times m$，为保证矩阵维度相容，需增加转置 \underbrace{\frac{\partial z}{\partial \vec{x}}}_{m \times 1} = \underbrace{(\frac{\partial \vec{y}}{\partial \vec{x}})^T}_{m \times n} \underbrace{\frac{\partial z}{\partial \vec{y}}}_{n \times 1} \tag{7.2} 标量对矩阵 矩阵对矩阵求导定义复杂，不给出整体的求导法则，仅对矩阵中某元素进行求导。假设有依赖关系$X \rightarrow Y \rightarrow z$，则满足 \frac{\partial z}{\partial X_{ij}} = \sum_{k, l} \frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}} \tag{7.3} 特别地，对于$z = f(Y), Y = AX + B$，求$\frac{\partial z}{\partial X}$时，由上式应满足 \frac{\partial z}{\partial X_{ij}} = \sum_{k, l} \frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}}其中 \frac{\partial Y_{kl}}{\partial X_{ij}} = \frac{\partial \sum_s A_{ks} X_{sl}}{\partial X_{ij}} = \frac{\partial A_{ki} X_{il}}{\partial X_{ij}} = A_{ki} \delta_{lj}, \quad \delta_{lj} = \begin{cases} 1 & l = j \\ 0 & \rm{otherwise} \end{cases}那么 \frac{\partial z}{\partial X_{ij}} = \sum_{k, l} \frac{\partial z}{\partial Y_{kl}} A_{ki} \delta_{lj} = \sum_{k} \frac{\partial z}{\partial Y_{kj}} A_{ki}也即$A^T$第$i$行与$\frac{\partial z}{\partial Y}$第$j$列内积，即 \frac{\partial z}{\partial X} = A^T \frac{\partial z}{\partial Y} \tag{8.1}类似的，对于$z = f(\vec{y}), \vec{y} = A\vec{x} + \vec{b}$ \frac{\partial z}{\partial \vec{x}} = A^T \frac{\partial z}{\partial \vec{y}} \tag{8.2}对于$z = f(Y), Y = XA + B$ \frac{\partial z}{X} = \frac{\partial z}{\partial Y} A^T \tag{8.3}对于$z = f(\vec{y}), \vec{y} = X\vec{a} + \vec{b}$ \frac{\partial z}{\partial X} = \frac{\partial z}{\partial \vec{y}} \vec{a}^T \tag{8.4}]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>理论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gcc/g++版本切换]]></title>
    <url>%2F2019%2F12%2F04%2Fgcc-g-%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[前言gcc/g++为Linux下C/C++编译器，但版本差异较大，如CUDA编译时不支持gcc-6之后的版本，故需要安装多个版本适应开发任务。 详细Ubuntu 18.04预装gcc/g++ 7.3，若需要安装gcc/g++-5，详细操作步骤如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152# 查看当前所有已安装版本$ ls /usr/bin/gcc*/usr/bin/gcc /usr/bin/gcc-7 /usr/bin/gcc-ar-6 /usr/bin/gcc-nm /usr/bin/gcc-nm-7 /usr/bin/gcc-ranlib-6/usr/bin/gcc-6 /usr/bin/gcc-ar /usr/bin/gcc-ar-7 /usr/bin/gcc-nm-6 /usr/bin/gcc-ranlib /usr/bin/gcc-ranlib-7$ ls /usr/bin/g++*/usr/bin/g++ /usr/bin/g++-6 /usr/bin/g++-7# 安装gcc-5与g++-5$ sudo apt-get install gcc-5Reading package lists... DoneBuilding dependency tree Reading state information... DoneThe following packages were automatically installed and are no longer required: javascript-common libjs-jquery libjs-sphinxdoc libjs-underscore python-apt python-m2crypto python-pkg-resources python-typingUse 'sudo apt autoremove' to remove them.The following additional packages will be installed: cpp-5 gcc-5-base libasan2 libgcc-5-dev libisl15 libmpx0Suggested packages: gcc-5-locales gcc-5-multilib gcc-5-doc libgcc1-dbg libgomp1-dbg libitm1-dbg libatomic1-dbg libasan2-dbg liblsan0-dbg libtsan0-dbg libubsan0-dbg libcilkrts5-dbg libmpx0-dbg libquadmath0-dbgThe following NEW packages will be installed: cpp-5 gcc-5 gcc-5-base libasan2 libgcc-5-dev libisl15 libmpx00 upgraded, 7 newly installed, 0 to remove and 2 not upgraded.Need to get 19.2 MB of archives.After this operation, 61.5 MB of additional disk space will be used.Do you want to continue? [Y/n]Get:1 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 gcc-5-base amd64 5.5.0-12ubuntu1 [17.1 kB]Get:2 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libisl15 amd64 0.18-4 [548 kB]Get:3 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 cpp-5 amd64 5.5.0-12ubuntu1 [7,785 kB] Get:4 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libasan2 amd64 5.5.0-12ubuntu1 [264 kB] Get:5 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libmpx0 amd64 5.5.0-12ubuntu1 [9,888 B] Get:6 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libgcc-5-dev amd64 5.5.0-12ubuntu1 [2,224 kB] Get:7 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 gcc-5 amd64 5.5.0-12ubuntu1 [8,357 kB] Fetched 19.2 MB in 9s (2,082 kB/s) Selecting previously unselected package gcc-5-base:amd64.(Reading database ... 183097 files and directories currently installed.)Preparing to unpack .../0-gcc-5-base_5.5.0-12ubuntu1_amd64.deb ...Unpacking gcc-5-base:amd64 (5.5.0-12ubuntu1) ...Selecting previously unselected package libisl15:amd64.Preparing to unpack .../1-libisl15_0.18-4_amd64.deb ...Unpacking libisl15:amd64 (0.18-4) ...Selecting previously unselected package cpp-5.Preparing to unpack .../2-cpp-5_5.5.0-12ubuntu1_amd64.deb ...Unpacking cpp-5 (5.5.0-12ubuntu1) ...Selecting previously unselected package libasan2:amd64.Preparing to unpack .../3-libasan2_5.5.0-12ubuntu1_amd64.deb ...Unpacking libasan2:amd64 (5.5.0-12ubuntu1) ...Selecting previously unselected package libmpx0:amd64.Preparing to unpack .../4-libmpx0_5.5.0-12ubuntu1_amd64.deb ...Unpacking libmpx0:amd64 (5.5.0-12ubuntu1) ...Selecting previously unselected package libgcc-5-dev:amd64.Preparing to unpack .../5-libgcc-5-dev_5.5.0-12ubuntu1_amd64.deb ...Unpacking libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...Selecting previously unselected package gcc-5.Preparing to unpack .../6-gcc-5_5.5.0-12ubuntu1_amd64.deb ...Unpacking gcc-5 (5.5.0-12ubuntu1) ...Setting up libisl15:amd64 (0.18-4) ...Processing triggers for libc-bin (2.27-3ubuntu1) ...Processing triggers for man-db (2.8.3-2ubuntu0.1) ...Setting up gcc-5-base:amd64 (5.5.0-12ubuntu1) ...Setting up libmpx0:amd64 (5.5.0-12ubuntu1) ...Setting up libasan2:amd64 (5.5.0-12ubuntu1) ...Setting up libgcc-5-dev:amd64 (5.5.0-12ubuntu1) ...Setting up cpp-5 (5.5.0-12ubuntu1) ...Setting up gcc-5 (5.5.0-12ubuntu1) ...Processing triggers for libc-bin (2.27-3ubuntu1) ...$ sudo apt-get install g++-5Reading package lists... DoneBuilding dependency tree Reading state information... DoneThe following packages were automatically installed and are no longer required: javascript-common libjs-jquery libjs-sphinxdoc libjs-underscore python-apt python-m2crypto python-pkg-resources python-typingUse 'sudo apt autoremove' to remove them.The following additional packages will be installed: libstdc++-5-devSuggested packages: g++-5-multilib gcc-5-doc libstdc++6-5-dbg libstdc++-5-docThe following NEW packages will be installed: g++-5 libstdc++-5-dev0 upgraded, 2 newly installed, 0 to remove and 2 not upgraded.Need to get 9,864 kB of archives.After this operation, 38.6 MB of additional disk space will be used.Do you want to continue? [Y/n]Get:1 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 libstdc++-5-dev amd64 5.5.0-12ubuntu1 [1,415 kB]Get:2 http://mirrors.aliyun.com/ubuntu bionic/universe amd64 g++-5 amd64 5.5.0-12ubuntu1 [8,450 kB]Fetched 9,864 kB in 45s (220 kB/s) Selecting previously unselected package libstdc++-5-dev:amd64.(Reading database ... 183333 files and directories currently installed.)Preparing to unpack .../libstdc++-5-dev_5.5.0-12ubuntu1_amd64.deb ...Unpacking libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...Selecting previously unselected package g++-5.Preparing to unpack .../g++-5_5.5.0-12ubuntu1_amd64.deb ...Unpacking g++-5 (5.5.0-12ubuntu1) ...Processing triggers for man-db (2.8.3-2ubuntu0.1) ...Setting up libstdc++-5-dev:amd64 (5.5.0-12ubuntu1) ...Setting up g++-5 (5.5.0-12ubuntu1) ...# 设置版本优先级$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 10$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 10$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-6 10$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 10$ sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 10$ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 10# 选择对应版本的gcc/g++$ sudo update-alternatives --config gccThere are 3 choices for the alternative gcc (providing /usr/bin/gcc). Selection Path Priority Status------------------------------------------------------------ 0 /usr/bin/gcc-6 10 auto mode 1 /usr/bin/gcc-5 10 manual mode* 2 /usr/bin/gcc-6 10 manual mode 3 /usr/bin/gcc-7 10 manual modePress &lt;enter&gt; to keep the current choice[*], or type selection number: 1 # 输入标签update-alternatives: using /usr/bin/gcc-5 to provide /usr/bin/gcc (gcc) in manual mode$ sudo update-alternatives --config g++There are 3 choices for the alternative g++ (providing /usr/bin/g++). Selection Path Priority Status------------------------------------------------------------ 0 /usr/bin/g++-6 10 auto mode 1 /usr/bin/g++-5 10 manual mode* 2 /usr/bin/g++-6 10 manual mode 3 /usr/bin/g++-7 10 manual modePress &lt;enter&gt; to keep the current choice[*], or type selection number: 1 # 输入标签update-alternatives: using /usr/bin/g++-5 to provide /usr/bin/g++ (g++) in manual mode# 查看当前版本号$ gcc -vUsing built-in specs.COLLECT_GCC=gccCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapperTarget: x86_64-linux-gnuConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 5.5.0-12ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnuThread model: posixgcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1)$ g++ -vUsing built-in specs.COLLECT_GCC=g++COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapperTarget: x86_64-linux-gnuConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 5.5.0-12ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnuThread model: posixgcc version 5.5.0 20171010 (Ubuntu 5.5.0-12ubuntu1) Reference linux下gcc、g++不同版本的安装和切换]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[详细！MTCNN训练全过程！]]></title>
    <url>%2F2019%2F11%2F10%2F%E8%AF%A6%E7%BB%86%EF%BC%81MTCNN%E8%AE%AD%E7%BB%83%E5%85%A8%E8%BF%87%E7%A8%8B%EF%BC%81%2F</url>
    <content type="text"><![CDATA[前言记录一下详细的MTCNN训练过程，之前损失函数的问题，训练得到网络效果一般，就拖到了最近。原理在之前写的博客Face Detection: MTCNN中已说明，本文不做详细介绍。详细代码可见本人代码仓库MTCNN_Darknet - Github。 数据集 数据集使用的是WIDER FACE: A Face Detection Benchmark与Deep Convolutional Network Cascade for Facial Point Detection，前者用于生成识别分类任务、定位回归任务的数据，后者用于生成关键点回归任务的数据。 我们所需的文件有WIDER_train.zip，WIDER_valid.zip，wider_face_split.zip，train.zip，解压整理后得到文件目录如下1234567891011data&#x2F;├─WIDER&#x2F;│ ├── wider_face_train_bbx_gt.txt│ ├── wider_face_val_bbx_gt.txt│ ├── WIDER_train&#x2F;│ └── WIDER_val&#x2F;└─Align&#x2F; ├── lfw_5590&#x2F; ├── net_7876&#x2F; ├── testImageList.txt └── trainImageList.txt 运行prepare_data/merge_annotations.py后，合并标注文件得到data/annotations.txt。 PNet网络结构123456layer filters size input output0 conv 10 3 x 3 &#x2F; 1 12 x 12 x 3 -&gt; 10 x 10 x 10 0.000 BFLOPs1 max 2 x 2 &#x2F; 2 10 x 10 x 10 -&gt; 5 x 5 x 102 conv 16 3 x 3 &#x2F; 1 5 x 5 x 10 -&gt; 3 x 3 x 16 0.000 BFLOPs3 conv 32 3 x 3 &#x2F; 1 3 x 3 x 16 -&gt; 1 x 1 x 32 0.000 BFLOPs4 conv 15 1 x 1 &#x2F; 1 1 x 1 x 32 -&gt; 1 x 1 x 15 0.000 BFLOPs 数据准备运行prepare_data/pnet_12x12.py，对原始图像进行随机采样，注意以下几点： WIDER用作分类、定位任务，Align用作关键点定位任务； 滤除尺寸小于40(pixel)的人脸图像； 采样数目设置 对每张图中任意采样$5 \times n_{faces}$份，再在每个人脸附近采样5份$iou &lt; 0.3$的人脸图像作为负样本(0)； 每个人脸附近采样10份$iou &gt; 0.65$的人脸图像作为正样本(1)； 每个人脸附近采样10份$0.4 &lt; iou &lt; 0.65$的人脸图像作为部分样本(-1)； 每个Align样本附近采样50份关键点样本(-2)。 数据集扩增 WIDER：随机左右镜像翻转； Align：随机左右镜像翻转，$\pm 15$度随机旋转； 生成以下文件及文件夹 123456data&#x2F;├── 12x12&#x2F;├── 12x12.txt├── 12x12_train.txt├── 12x12_valid.txt└── 12x12_test.txt 其中12x12/包含随机采样得到的尺寸为$12 \times 12$图像，用于输入网络进行训练；12x12.txt包含所有图像的标注，其格式如下 1[image path] [sample label] [annotations] 其中负样本(0)无annotations，正样本(1)与部分样本(-1)包含定位框左上右下角的相对偏移量(4个)，关键点样本(-2)包含5个关键点相对于图片左上角的相对偏移量(10个)，例如 1234..&#x2F;data&#x2F;12x12&#x2F;25219.jpg 0..&#x2F;data&#x2F;12x12&#x2F;25269.jpg 1 0.07142857142857142 0.03571428571428571 -0.21428571428571427 0.10714285714285714..&#x2F;data&#x2F;12x12&#x2F;25231.jpg -1 -0.030303030303030304 -0.12121212121212122 0.18181818181818182 0.696969696969697..&#x2F;data&#x2F;12x12&#x2F;3926594.jpg -2 0.29134029571831355 0.08374723013072503 0.7777450550823473 0.20460741825864107 0.2530593377292015 0.5109005869899402 0.30317271612810937 0.767843704149384 0.6327263098562724 0.8486940727227192 12x12_*.txt三个文件为对12x12.txt进行一定比例划分得到的标注文件，这里采用的比例为0.6: 0.15: 0.25。 经统计，其样本数量与比例详细信息如下12345678&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Totally 3397282 imagesPos(1): Neg(0): Part(-1): Landmark(-2) &#x3D; 0.3107857987650127: 0.3618628067967275: 0.12916354897827145: 0.1981878454599883--------------------------------------------------Cls | Pos(1) : Neg(0) &#x3D; 0.858849798674096Offset | [Pos(1) + Part(-1)] : n_samples &#x3D; 0.4399493477432842Landmark | Landmark(-2) : n_samples &#x3D; 0.1981878454599883&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 训练数据准备就绪后，可进行网络训练，运行mtcnn_py/main_pnet.py。批次大小设置为512，初始学习率0.01，权重衰减设置为4e-5，采用Adam优化器，学习率调整使用指数衰减策略，每代衰减95%，对于PNet，进行20代训练即可，得到权重文件mtcnn_py/ckptdir/PNet.pkl。 测试 RNet网络结构12345678layer filters size input output0 conv 28 3 x 3 &#x2F; 1 24 x 24 x 3 -&gt; 22 x 22 x 28 0.001 BFLOPs1 max 3 x 3 &#x2F; 2 22 x 22 x 28 -&gt; 11 x 11 x 282 conv 48 3 x 3 &#x2F; 1 11 x 11 x 28 -&gt; 9 x 9 x 48 0.002 BFLOPs3 max 3 x 3 &#x2F; 2 9 x 9 x 48 -&gt; 4 x 4 x 484 conv 64 2 x 2 &#x2F; 1 4 x 4 x 48 -&gt; 3 x 3 x 64 0.000 BFLOPs5 connected 576 -&gt; 1286 connected 128 -&gt; 15 数据准备运行prepare_data/rnet_24x24.py，生成以下文件及文件夹 1234567data&#x2F;├── 24x24&#x2F;├── 24x24.txt├── 24x24_train.txt├── 24x24_valid.txt├── 24x24_test.txt└── rDets.npy RNet训练数据需要用到训练好的PNet生成候选框。首先利用PNet对每张原始图片进行检测，保存检测得到的候选框，文件为data/rDets.npy。 再依据真实标注框对每个候选框进行判别，由于负样本众多，每张图片保留15份负样本，保留全部正样本与部分样本。关键点样本与PNet一致，每张原始图片生成80份关键点样本。 经统计，样本数目如下 12345678&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Totally 1954946 imagesPos(1): Neg(0): Part(-1): Landmark(-2) &#x3D; 0.05522607785585893: 0.10885466913152589: 0.2848656689238475: 0.5510535840887677--------------------------------------------------Cls | Pos(1) : Neg(0) &#x3D; 0.507337703531402Offset | [Pos(1) + Part(-1)] : n_samples &#x3D; 0.34009174677970644Landmark | Landmark(-2) : n_samples &#x3D; 0.5510535840887677&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 训练运行mtcnn_py/main_rnet.py。批次大小设置为512，初始学习率0.01，权重衰减设置为4e-5，采用Adam优化器，学习率调整使用指数衰减策略，每代衰减95%，进行50代训练，得到权重文件mtcnn_py/ckptdir/RNet.pkl。。 测试 ONet网络结构12345678910layer filters size input output0 conv 32 3 x 3 &#x2F; 1 48 x 48 x 3 -&gt; 46 x 46 x 32 0.004 BFLOPs1 max 3 x 3 &#x2F; 2 46 x 46 x 32 -&gt; 23 x 23 x 322 conv 64 3 x 3 &#x2F; 1 23 x 23 x 32 -&gt; 21 x 21 x 64 0.016 BFLOPs3 max 3 x 3 &#x2F; 2 21 x 21 x 64 -&gt; 10 x 10 x 644 conv 64 3 x 3 &#x2F; 1 10 x 10 x 64 -&gt; 8 x 8 x 64 0.005 BFLOPs5 max 2 x 2 &#x2F; 2 8 x 8 x 64 -&gt; 4 x 4 x 646 conv 128 2 x 2 &#x2F; 1 4 x 4 x 64 -&gt; 3 x 3 x 128 0.001 BFLOPs7 connected 1152 -&gt; 2568 connected 256 -&gt; 15 数据准备运行prepare_data/onet_48x48.py，生成以下文件及文件夹 1234567data&#x2F;├── 48x48&#x2F;├── 48x48.txt├── 48x48_train.txt├── 48x48_valid.txt├── 48x48_test.txt└── oDets.npy 数据生成方法与RNet一致，不同之处是用到PNet与RNet生成的候选框，保存检测得到的候选框，文件为data/oDets.npy。 由于负样本众多，每张图片保留10份负样本，保留全部正样本与部分样本。关键点样本与PNet一致，每张原始图片生成80份关键点样本。 经统计，样本数目如下 12345678&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Totally 1106392 imagesPos(1): Neg(0): Part(-1): Landmark(-2) &#x3D; 0.04348278006348564: 0.12449656179726534: 0.10240583807547415: 0.7296148200637749--------------------------------------------------Cls | Pos(1) : Neg(0) &#x3D; 0.34926892305905244Offset | [Pos(1) + Part(-1)] : n_samples &#x3D; 0.14588861813895979Landmark | Landmark(-2) : n_samples &#x3D; 0.7296148200637749&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 训练运行mtcnn_py/main_onet.py，批次大小设置为512，初始学习率0.01，权重衰减设置为4e-5，采用Adam优化器，学习率调整使用指数衰减策略，每代衰减95%，进行30代训练，得到权重文件mtcnn_py/ckptdir/RNet.pkl。 测试 转C语言C语言框架使用的是darknet，我对它进行了一些修改，在isLouisHsu/DarkerNet，安装使用说明在仓库文档说明。 运行mtcnn_py/extract_weights.py生成权重mtcnn_c/weights/*.weights，运行 12345cd mtcnn_c/mkdir build &amp;&amp; cd buildcmake .. &amp;&amp; makecd .../mtcnn -h Reference Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks - arXiv kpzhang93/MTCNN_face_detection_alignment - Github AITTSMD/MTCNN-Tensorflow - Github]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CenterNet - Objects as Points]]></title>
    <url>%2F2019%2F10%2F09%2FCenterNet-Objects-as-Points%2F</url>
    <content type="text"><![CDATA[前言目前大多检测算法将物体视作边界框进行检测，并采用锚框(Anchor)，能很大程度提升检测算法的表现，但是Anchor难以确定参数，且增加大量网络运算量。目前的物体检测算法开始向无Anchor的方向发展，如本文的CenterNet，是把物体所在包围框作为点，并结合热图的形式进行。除物体检测外，此算法还可用于人体姿态识别、3D物体检测等。 个人理解，采用热图的形式，是将Anchor一定程度地连续化了。 论文阅读Introduction In this paper, we provide a much simpler and more efficient alternative. We represent objects by a single point at their bounding box center (see Figure 2). Other properties, such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard keypoint estimation problem [3,39,60]. We simply feed the input image to a fully convolutional network [37, 40] that generates a heatmap. Peaks in this heatmap correspond to object centers. Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised learning [39,60]. Inference is a single network forward-pass, without non-maximal suppression for post-processing. Preliminary若有某三通道图片$I \in \mathcal{R}^{W \times H \times 3}$，经卷积神经网络后得到输出特征图$\left[\hat{Y} || \hat{O} || \hat{S} \right] \in [0, 1]^{\frac{W}{R} \times \frac{H}{R} \times (C + 4)}$。其中$R$为降采样系数，表示特征图上单个位置处的点映射回原图后网格的大小，文中默认为$R=4$；$C$可根据关键点类型指定，如人体姿态估计时指定$C=17$(17个关键点)，物体检测时指定$C=80$(80类物体)。$\hat{Y}_{x, y, c} = 1$表示对应关键点，$\hat{Y}_{x, y, c} = 0$表示为背景。 ground truth设置为热图(heatmap)的形式，采用高斯核 Y_{x, y, c} = \exp\left( - \frac{(x - \tilde{p}_x)^2 + (y - \tilde{p}_y)^2}{2 \sigma_p^2} \right) \tag{1}其中$\tilde{p} = \lfloor \frac{p}{R} \rfloor$，$p$为物体中心点在原图中的坐标；$\sigma_p$称对象尺寸自适应标准差(object size-adaptive standard deviation)，以解决物体尺寸不一致的问题。若两个同类别对象相距过近，其对应类别特征图上重叠部分用逐元素取大的策略(element-wise maximun)。 关键点为热图的回归(keypoint regression)，损失函数参考Focal Loss，添加$(1 - Y_{x, y, z})^{\beta}$项，使其适应回归任务 L_k = - \frac{1}{N} \sum_{x, y, z} \begin{cases} (1 - \hat{Y}_{x, y, z})^{\alpha} \log (\hat{Y}_{x, y, z}) & Y_{x, y, z} = 1 \\ (1 - Y_{x, y, z})^{\beta} (\hat{Y}_{x, y, z})^{\alpha} \log (1 - \hat{Y}_{x, y, z}) & \rm{otherwsie} \\ \end{cases} \tag{2.1}其中$\alpha, \beta$为超参数，文中指定$\alpha=2, \beta=4$；$N$为图片$I$中关键点的个数。 Focal Loss定义为 FL(p_t) = - \alpha_t (1 - p_t)^{\gamma} \log p_t其中$\gamma &gt; 0$ \begin{aligned} p_t = \begin{cases} p & y = 1 \\ 1 - p & \rm{otherwsie} \end{cases} \in [0, 1] \\ \alpha_t = \begin{cases} \alpha & y = 1 \\ 1 - \alpha & \rm{otherwsie} \end{cases} \in [0, 1] \end{aligned} 由于网络的降采样，设置物体坐标偏置(local offset)修正物体中心坐标，即特征图$\hat{O} \in \mathcal{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$，该特征图所有类别共享。采用$L_1$损失，且仅对包含关键点的位置计算损失值。 L_{off} = \frac{1}{N} \sum_p \left| \hat{O}_{\tilde{p}} - \left( \frac{p}{R} - \tilde{p} \right) \right| \tag{2.2}同样地，物体的边界框尺寸回归设置特征图$\hat{S} \in \mathcal{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$，$s_k = (x^{(k)}_2 - x^{(k)}_1, y^{(k)}_2 - y^{(k)}_1)$，也采用$L_1$损失，那么 L_{size} = \frac{1}{N} \sum_k \left| \hat{S}_{p_k} - s_k \right| \tag{2.3}总体损失为以上几项的加权和，原文中$\lambda_{off} = 1, \lambda_{size} = 0.1$。 L = L_k + \lambda_{off} L_{off} + \lambda_{size} L_{size} \tag{2}基于以上，在测试阶段时，先在类别输出特征图$\hat{Y}$上确定位置，即保留100个峰(peak)(某处值在比其8领域值都大)的坐标$(\hat{x}_i, \hat{y}_i), i = 1, \cdots, 100$，各位置处的预测输出为 \begin{aligned} \begin{cases} \hat{O}_{\hat{x}_i, \hat{y}_i} = (\delta_{\hat{x}_i}, \delta_{\hat{y}_i}) \\ \hat{S}_{\hat{x}_i, \hat{y}_i} = (\hat{w}_i, \hat{h}_i) \end{cases} \Rightarrow \\ ( \hat{x}_i + \delta_{\hat{x}_i} - \hat{w}_i / 2, \hat{y}_i + \delta_{\hat{y}_i} - \hat{h}_i / 2, \\ \hat{x}_i + \delta_{\hat{x}_i} + \hat{w}_i / 2, \hat{y}_i + \delta_{\hat{y}_i} + \hat{h}_i / 2 ) \end{aligned} \tag{3}Implementation details原文中分别采用ResNet-18, ResNet101, DLA-34, and Hourglass-104作为网络框架进行实验，效果对比如下 训练细节如下 输入图像分辨率为$512 \times 512$，输出特征图分辨率为$128 \times 128$； 数据集扩增：随机翻转、尺度缩放($0.6$~$1.3$)、随即裁剪、颜色抖动； 使用Adam优化器，批次大小、初始学习率、学习率调整策略与不同网络框架相关。 Experiments Reference Objects as Points - arXiv Focal Loss for Dense Object Detection - arXiv]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《教父》三部曲：充满腐败而不失庄重的资本主义气息]]></title>
    <url>%2F2019%2F10%2F05%2F%E3%80%8A%E6%95%99%E7%88%B6%E3%80%8B%E4%B8%89%E9%83%A8%E6%9B%B2%EF%BC%9A%E5%85%85%E6%BB%A1%E8%85%90%E8%B4%A5%E8%80%8C%E4%B8%8D%E5%A4%B1%E5%BA%84%E9%87%8D%E7%9A%84%E8%B5%84%E6%9C%AC%E4%B8%BB%E4%B9%89%E6%B0%94%E6%81%AF%2F</url>
    <content type="text"><![CDATA[简介《教父》三部曲每一部都很精彩，第二部主角是个活生生的反例，可能稍逊一筹。 第一部《教父》介绍的是柯里昂家族反对毒品交易而引起的大风波，是三部曲中唯一一部被拍成电影的书(电影《教父Ⅰ》完美还原，表现力很强，但缺少细节导致剧情不太连续)。个人认为，维托·唐·柯里昂是三部曲中唯一一位称得上是教父的人，其度量、耐心、冷静与智慧让人折服。教父因拒绝毒品交易被枪击，随后小儿子迈克·柯里昂替父报仇潜逃西西里，长子桑提诺·柯里昂性格鲁莽在之后的大战中被击毙，唐请求各大家族协商和谈停止战争，让迈克返回纽约继承教父衣钵。迈克与父亲一样沉着、冷静、精明、坚强，整本书也像是讲述他褪茧化蝶的过程，最终成为了第二代教父。每一个男人都必须像他一样，让自己的心智不断变得成熟，让自己变得足够强大，可以保护自己、家人和别人。 第二部《教父：西西里人》故事发生在黑手党之乡西西里，整本书的格调非常浪漫，结局悲惨令人唏嘘。图里·吉里安诺劫富济贫、反抗政府、藐视黑手党，是西西里的英雄人物，在政府、黑手党的阴谋之下，各种矛盾激发，最终被他的忠实追随者阿斯帕努·皮肖塔背叛而惨遭杀害。图里失败的原因有很多，他是是理想主义者，信奉个人英雄主义。第二，不具有大局观，被政府与黑手党联手利用以对抗共产党人时，不曾想到长远利益。第三，他也不善于领导下属，他以“属下”而不是“伙伴”的态度来对待和尊重他的团队，最后计划潜逃美国时，没有考虑他的好友阿斯帕努的后路，导致亲密伙伴的叛变。 第三部《教父：最后的教父》，讲述克莱里库齐尼奥家族消灭敌人库奇奥家族后发生的故事，穿插家族与好莱坞两条主线，故事内容较前两部更加丰富、情节更加复杂。全书最精彩的是结尾，唐·克莱里库齐尼奥为完成使家族产业合法化的大局，借用外孙克罗斯·德·莱纳之手除去孙子丹特·克莱里库齐尼奥。这两个人辈分相同，背景与性情却大相径庭：克罗斯·德·莱纳跟随父亲皮皮·德·莱纳长大，他彬彬有礼、勇敢果断；而丹特的父亲是库奇奥家族长子乔治·库奇奥(类似罗密欧与朱丽叶之间的家族纠纷，乔治在结婚之日被皮皮所杀)，继承了库奇奥家族残暴的性情。唐其实不愿将教父之位传与丹特，在丹特为父报仇杀害皮皮后，没有主持所谓的公道，假装不知情，任由克罗斯将丹特杀害，再将克罗斯驱逐。最终克罗斯退出了家族，但个人认为他是天生的黑手党首领，是“最后的教父”。 摘录 有个道理他早就弄清楚了，那就是你必须承受社会强加的侮辱，因为他明白，连最卑微的人，只要时刻擦亮眼睛，就迟早能抓住机会，报复最有权势的人。正是明白这个道理，唐才从不放弃他的谦逊风度，所有朋友都对此敬佩有加。 友谊就是一切。比天赋更重要，比政府更重要。和家人差不多同样重要。 “我会给他一个他无法拒绝的条件。” 知道极地探险家在通往北极的路上要沿途存放口粮，防止日后某天会需要食物吗？那就是我父亲的人情。 黑根的谈判技巧是唐亲自传授的。“永远不要动怒，”唐这么教导他，“决不要威胁，要讲道理。”用意大利语说“讲道理”听上去像“应对”。关键是忽视所有的侮辱和威胁，一边脸挨了打，就把另一边脸也凑上去。 意大利人有个玩笑话，说世界太残酷，所以一个人非得有两个父亲照看他，这就是教父的由来。 在黑根的世界里，柯里昂家族的世界里，女性的美丽肉体和性魅力对世俗事务毫无重要性可言。只要不涉及婚姻和家族的脸面，这就只是私人事务。 动作不慌不忙得吓人，像是全世界的时间都归他们支配。他们不是慌乱地瞎打一通，而是一板一眼，用上躯体的全部力量，慢镜头似的慢慢收拾他。每一拳下去都带着皮开肉绽的声音。 没有一句警告，不装腔作势，不按理出牌，不留任何余地。这种冷酷无情，这种对一切价值的全然蔑视，意味着这个人只认他自己的法律，甚至把自己视为上帝。 绝对不要让家族外的人知道你在想什么。绝对不要让他们知道你手里有什么牌。 可是，克莱门扎有个强烈的感觉，那就是遵守良好的工作习惯很重要，务必做到万无一失。论到生死问题，谁也说不准会发生什么。 将来有可能会出现有人会因为利益而出卖他的情况，要是只有一名同伙，那就是正反双方各执一词。但要是还有第二名同伙作旁证，平衡就会被打破。不，办事必须严格按照程序。 你不能说他走运，彼得，那太低估他了。我们最近太过于低估别人。 “这种事每隔十来年就要发生一次，能释放彼此的仇怨。另外，要是放任他们在小事上随便摆布我们，那他们就会想要夺走我们的一切。必须一冒头就斩断。就像他们当初在慕尼黑就该阻止希特勒，他干了那种事，怎么能随便放过他，放过他就意味着后面的大麻烦都是自找苦吃。” 有些事情非做不可，做了也不值得再次提起，不需要给自己找正当的借口。这种事情正当不起来。反正做就是了，然后忘掉。 当然了，谁都不可能以任何理由挑拨唐，唐的情感只受他自己左右。 尼诺的反应与成功之路背道而驰，无论别人怎么帮他，他都觉得受了羞辱。 一个向警方通风报信的人，一个收钱就可以不寻仇的人，肯定没有过硬的后台。 这段经验催生了他的座右铭：一个人只能有一种命运。 “我会和他讲道理。”维托·柯里昂说，这句话后来成了他的名言，致命攻击前的最后警告。后来他成为唐，每次请对手坐下来和他讲道理，对手就明白这是解决争端而不流血杀人的最后一次机会了。 当时维托·柯里昂还不知道这个笑容的威力。之所以让人毛骨悚然，正是因为毫无威胁的意思，像是听到了只有自己才明白的什么私人玩笑。可是，他只在性命攸关的事情上露出这个笑容，玩笑也并不真的私密，他的双眼毫无笑意，外在性格平时又是那么通情达理和沉默寡言，因此突然摘下面具，露出真实的自我才那么吓人。 伟大的人并非生而伟大，而是越活越伟大，维托·柯里昂就是明证。 他掂量了一下他们的威胁，发现没什么说服力，于是降低了对新伙伴的评价，因为他们太愚蠢，在毫无必要的情况下滥用威胁。 你难道不想好好上学，不想当个律师？律师拎着手提箱能偷的钱，一千个强盗戴着面具拿着枪也比不上。 除了他时常重复的“一个人只有一种命运”理论，唐还喜欢责备桑尼动不动就发脾气的毛病。唐认为威胁是最愚蠢的自我暴露，不假思索就释放怒火是最危险的任性表现。没有谁听唐发出过赤裸裸的威胁，没有谁见过他陷入无法控制的狂怒。那是难以想象的事情。他就这么尽量向桑尼传授自己的准则。他说除了朋友低估你的优点，世上最大的天然优势就是敌人高估你的缺陷。 和历史上所有伟大的统治者和立法者一样，唐·柯里昂看明白了，除非把王国的数量缩减到可控范围之内，否则就不可能缔造秩序与和平。 一个人靠汗水挣面包钱，做什么职业都值得尊敬。 时间能治好创伤。痛苦和恐惧不是死亡，还有挽回的余地， 卡洛觉察到桑尼会杀了他，明白桑尼拥有动物本性，能杀死另一名人类；而他要想杀人，却必须聚集起全部勇气和全部意志力。卡洛从没想到过，这是因为他比桑尼·柯里昂更有人情味——如果“人情味”能用在他们头上的话；他嫉妒桑尼身上那种被镀上传奇色彩的、可怕的凶残。 明白如果她有必要知道的坏事，那么马上就会有人来通知她；如果是坏事但她不知道也无所谓，那么她还是不要知道为妙。 朋友不会轻易求你帮忙，你也不能轻易拒绝。 没有理性，我们和丛林野兽还有什么分别？但是，我们毕竟有理性，能够彼此说理，能够和自己说理。 他没有详细解释，在唐这样的人看来，一个人试图自杀是多么不可思议的事情。 唉，人生又不完全是甜美的音乐，你要是愿意在医院里走一圈，就会看见什么是真正的苦难，就会给肉赘唱一首小情歌了。 面对残暴的绝对权力，苦难中的人民害怕被击垮，学会了决不泄露怒火和恨意，决不空口威胁而让自己受到伤害，因为那就等于提醒对手，会迅速遭到报复。他们学会了社会就是敌人，于是在受到冤屈而寻找救济的时候，转而求助于叛逆的地下王国——黑手党。黑手党通过缄默规则巩固权力。在西西里乡村，陌生人连问怎么去最近的村镇都得不到礼遇和回答。黑手党成员能犯下的最大罪错莫过于告诉警察，他吃了谁的枪子或者被谁以任何方式伤害了。缄默规则成了人们的宗教。一个女人就算丈夫被杀、儿子被杀、女儿被强奸，也不能向警方透露罪犯的姓名。 我经常这么和别人说，‘肉别吃那么多，否则你会死；烟别抽那么多，否则你会死；工作别那么卖力，否则你会死；酒别喝那么凶，否则你会死。’谁也不听我的。知道为什么吗？因为我说的不是‘明天你就会死’。 唐帮助不幸的人，而这些人的不幸又有一部分要归咎于他。不一定出于狡诈或计划，很可能只是因为他在各方面都有利益，也许这就是宇宙的固有性质：善与恶内在联系，宇宙本身就是这样。 她知道全世界只有她能压迈克尔一头，但也知道经常这么做只会毁掉这种能力。 忒西奥看好迈克尔。他感觉到这个年轻人没这么简单，巧妙地隐藏起了某种力量，他戒心很重，生怕把真正的实力暴露在公众视线之下，遵循唐的教诲：让朋友低估你的优点，敌人高估你的缺陷。 你不能对你爱的人说不，至少不能经常说。这就是秘诀。要是非说不可，也得听起来像是肯定。或者想办法让他们自己说。你得耐心，不怕麻烦。 他已在弥留之际。他闻着花园的香味，黄色的光球刺得眼睛生疼，他悄声说：“生活如此美丽。” 要是不赞同某个团体或个人的看法，他要么避而不谈，要么直截了当地说出他的不满。他从不礼节性地随意附和。 很多年轻人在拥抱真正的命运之前都走错过路。时间和运气会改正错误。 既然他不怕死，存心找死，那么诀窍就在于，让他唯独不想死在你的手里。他害怕的事情只有一件——不是死亡，而是他或许会死在你手里。到那个时候，他就完全属于你了。 因为背叛是不能宽恕的罪行。迈克尔可以宽恕他们，但他们永远无法宽恕自己，因此反而更危险。迈克尔真的很喜欢忒西奥，更爱自己的妹妹。可是，如果放过忒西奥和卡洛，那就是对你和孩子、对他的整个家庭、对我和我的家人的失职。他们会对我们所有人、所有人的生命构成危险。 他显然是一个“值得尊敬”的人。 西西里人害怕真相。过去几千年里，暴君和宗教法庭用酷刑逼迫他们说真话，罗马政府用法律要求人们说真话，教堂忏悔处的神父用下地狱的痛苦敦促人们说真话，可是真话是力量的源泉，是控制的杠杆，为什么要拱手交给别人呢？ 天生的恶棍对天生的英雄有着天然的敌意。 穷人总是上当受骗，甚至被那些指引他们通往救赎之路的人欺骗。 我们真正的信仰在于我们相信奇迹会发生。 一个对人类过去两千年历史一无所知的人，是一个在黑暗中生活的人。 唐·克罗切最厉害的地方就是不理会别人对他的侮辱和不恭，但是他会把它牢记在心里。 我们小时候，我们年轻的时候，热爱我们的朋友，慷慨地对待他们，原谅他们的错误，这些都是很自然的。每一天都很新鲜。我们愉快地期待未来，毫无畏惧之心。世界本身并没有那么危险；那是一段欢乐的时光。可是我们长大了，要自食其力了，朋友的情谊就不那么容易维系下去。我们必须随时保持警惕。我们的长辈不再照顾我们，我们也不再对儿时简单的乐趣感到满足。我们开始有了自豪感——我们希望成为了不起的人、有权的人或者有钱的人，或者只是为了使自己免遭不幸。我知道你非常热爱图里·吉里安诺，可是现在你必须问问自己，这样的爱要付出什么代价？经过这么多年，这样的爱是否还存在？是不是只存在于记忆之中？ 皮肖塔很聪明，但只是年轻人的那种聪明——也就是说，他没有充分认识到，最好的人心里也有潜在的恐怖与邪恶。 如果皮肖塔继续对吉里安诺忠心耿耿，他也会被人们所遗忘，因为那样整个传奇故事就将是关于吉里安诺一个人的。但是他犯下了弥天大罪，这样他就会与他热爱的图里永远并列在一起了。 你是不是想成为像吉里安诺一样的英雄？一个传奇人物？一个死人？我喜欢他，因为他是我最好的朋友的儿子，可是我并不羡慕他的名声。你还活着，可是他死了。永远不要忘记这一点。活着不要当英雄，只要活着就行。时过境迁之后，英雄似乎就有点儿愚蠢了。 在熟铁框架上锻造了一行字，是给那些自鸣得意的凭吊者看的：我们曾经像你们一样——你们也会像我们一样。 “我们要做的是助人为乐，”唐说道，“不是舍己为人。” 永远别回头。无论是为了找借口、为自己辩解还是找乐子，永远都不要回头。你现在是什么人，就是什么人，世界眼下是什么样，就是什么样。 自杀——如今这种事儿还属于“政治不正确”吗？ 暴力这种武器太宝贵了，浪费不得，必须用以达到重要目的才行。 每个男人、女人和孩童，在压力之下、悔恨之中，或是艰难的环境面前，都要完全对自己的行为负责。决定一个人的是行为，言语只不过是个屁。 不轻易动怒，不谈论自己。要用行动而不是言语来赢得尊重。尊重家庭的每一分子。赌博是消遣，可不是营生。爱父母和妹妹，但是小心，别爱上老婆以外的女人。老婆就是给你生孩子的女人。有了妻儿，就要牺牲自己的生活去养活他们。 聪明的、理智的男人，绝大多数情况下都不必害怕女人。两种人你必须警惕：第一种是最危险的，那就是遭遇了不幸的小姑娘；第二种就是比你还有野心的女人。 显然他的行为都是让她主动离开的小伎俩，可她这么长时间都不知趣。想想真丢脸。她怎么能这么傻呢？ 事实证明，谨慎地镇压自由意志对人类生存很有必要。 法兰西国王露出屁股，当众大便，以示对教皇的轻蔑。而那个红衣主教则高呼：“噢，这是天使的屁股！”然后冲上去大亲特亲。 迪尔最喜欢用“脆弱”形容他认为愚蠢的人。 皮皮大笑道：“别让任何人告诉你该怎么做，不要让别人插手你的事。他们只能告诉你他们的期望，而且我们只找最有利于我们的方法办事。最简单的就是最好的。如果你必须把事情搞复杂，那就搞到最复杂。” 唐也知道，无论爱有多么深，这种情感都不可靠。爱不一定带来感激和顺从，也不一定能在如此艰难的世界里带来和睦。所以，为了激励真正的爱，必须让人有所畏惧。爱本身一钱不值，必须包括信任和服从才有意义。 他对萝塞·玛丽耶说：“像这样的世界，谁都可以为所欲为，怎么能生活得下去呢？谁都不会受到上帝或者他人的惩罚，谁都不必养活自己？真有这种为所欲为的女人吗？真有男人如此愚蠢和软弱，任何一点欲望、梦想或者快乐都可以使他们屈服吗？那些诚实的丈夫在哪？他们为了食物而工作，想方设法保护孩子们不受命运和这个残酷的世界摆布。谁能真正明白一片奶酪、一杯酒、一个温暖的家，人生就足够了？向往虚无缥缈的幸福的都是什么人？看看他们把生活搞得多么乱七八糟、凭空制造了多少悲剧。”唐拍了拍女儿的头，冲着电视荧幕不屑地摆摆手，说，“让他们都下地狱吧。”最后，他又加了一句金玉良言，“每个人都要为自己的所作所为负责。” 任何形式的伟大都会招致嫉妒。 怪人做怪事，是为了让人们注意不到他们的真面目。他们自卑。 这就是真相。他对人类失望了。他见到了太多的背叛、太多可怜的软弱、太多争名逐利的贪婪。相爱的人之间却都是逢场作戏，夫妻也好、父子也好、母女也好，都是一样。 我没胆子跳出窗户。我想象力丰富，在落地之前就能想到一千种自己落地后摔得七零八落的样子，还有可能会砸到别人；我不敢割腕，见血就晕；我也不敢用刀、枪或者撞车、卧轨。 人可以不断犯错，但绝不能犯要命的错。 阅读是永远不会背叛他的一种快乐。 那些笔触优雅的散文，那么受人欢迎，现在看来都是无病呻吟，夸张做作而且自命不凡。 “你不能认栽，”唐对克罗斯说，“这样的话，大家都把你当傻子看，全世界的人谁也不会尊重你了。” 他是个好孩子，但还年轻，年轻人必须得承担风险。 克罗斯想知道他什么时候最爱她，是当她生气勃勃的时候，当她严肃认真的时候，还是她闷闷不乐的时候。她美丽的面容不断变化，似乎有种魔力，克罗斯自己的情绪也随她一起变化。 他急切地想牺牲自己成全自己爱的女人，很多男人都有过这种感觉，现在轮到他了。 唐说过，绝大多数悲剧都是荒诞的。 下午剩下的时间，克罗斯在片场观看拍摄。有这么一场戏，主角赤手空拳干掉了三个全副武装的敌人。这把他惹毛了。是英雄就不应该让自己陷入这么绝望的局面。这种事只能证明这家伙太蠢，根本不配当英雄。 要知道，世界眼下是什么样，就是什么样；你现在是什么人，就是什么人。]]></content>
      <categories>
        <category>Reading</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PCA and LDA with Kernel]]></title>
    <url>%2F2019%2F09%2F05%2FPCA-and-LDA-with-Kernel%2F</url>
    <content type="text"><![CDATA[前言核函数可将数据进行升维，在更高维度的空间进行数据分析，可以将线性不可分问题转换为线性可分问题，关于核函数，详细可查看Support Vector Machine 2.2核技巧。在PCA与LDA中分别介绍了主成分分析与线性鉴别分析，但这两种均为线性方法，本文利用核技巧将数据映射到高维空间，然后再用两种降维方法进行降维。 先升维再降维！ 原理假设有$M$个$N$维样本组成数据矩阵$X_{N \times M}$ X = \begin{bmatrix} | & | & & | \\ x^{(1)} & x^{(2)} & \cdots & x^{(M)} \\ | & | & & | \end{bmatrix} \tag{1}其中$x^{(i)} = \begin{bmatrix} x^{(i)}_1 &amp; x^{(i)}_2 &amp; \cdots &amp; x^{(i)}_N \end{bmatrix}^T$，属于$C$个类别，第$j$类的样本数为$M_j$。 核函数一般定义为$\kappa(x, y) = \Phi(x)^T \Phi(y) $，注意到，核函数是升维后数据的内积形式，利用隐式映射$\Phi(x)$将样本维度增加。 Kernel PCA同样地，求取升维后数据的协方差矩阵，需要对样本去均值化，即 C = \frac{1}{M} \sum_{i=1}^M \left[\Phi(x^{(i)}) - \Phi(\mu)\right] \left[\Phi(x^{(i)}) - \Phi(\mu)\right]^T \tag{2}其中$\Phi(\mu) = \frac{1}{M} \sum_{i=1}^M \Phi(x^{(i)})$。将其特征分解，记高维空间的第一主轴为$\Phi(u_1)$，有 C \Phi(u_1) = \lambda_1 \Phi(u_1) \tag{3}现在有个问题，由于$\Phi(x)$为隐式映射，实际上$C$是不能直接求解的。将$(2)$代入$(3)$，有 \underbrace{ \frac{1}{M} \sum_{i=1}^M \left[\Phi(x^{(i)}) - \Phi(\mu)\right] \left[\Phi(x^{(i)}) - \Phi(\mu)\right]^T }_C \Phi(u_1) = \lambda_1 \Phi(u_1) \tag{4}其中$\left[\Phi(x^{(i)}) - \Phi(\mu)\right]^T \Phi(u_1)$部分为参数，可记作$a^{(i)}_1$，所以 \frac{1}{M} \sum_{i=1}^M \left[\Phi(x^{(i)}) - \Phi(\mu)\right] a^{(i)}_1 = \lambda_1 \Phi(u_1)记$\Psi(x^{(i)})=\Phi(x^{(i)}) - \Phi(\mu)$，也即 \Phi(u_1) = \frac{1}{\lambda_1 M} \sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)}) \tag{5}所以可以看到，主轴$\Phi(u_1)$是由高维空间样本去均值化后的数据点$\Psi(x^{(i)})$张成的，这个式子很重要，只需求出$a^{(i)}_1$，即可表示出主轴，用于高维数据$\Phi(x^{(i)})$的降维。继续，将式$(1), (5)$代入$C \Phi(u_1) = \lambda_1 \Phi(u_1)$，得到 \begin{aligned} \underbrace{ \frac{1}{M} \sum_{i=1}^M \Psi(x^{(i)}) \Psi(x^{(i)})^T }_C \underbrace{ \cancel{\frac{1}{\lambda_1 M}} \sum_{i=1}^M \Psi(x^{(i)}) a^{(i)}_1 }_{\Phi(u_1)} = \lambda_1 \underbrace{ \cancel{\frac{1}{\lambda_1 M}} \sum_{i=1}^M \Psi(x^{(i)}) a^{(i)}_1 }_{\Phi(u_1)} \\ \frac{1}{M} \sum_{i=1}^M \Psi(x^{(i)}) \sum_{j=1}^M a^{(j)}_1 \Psi(x^{(i)})^T \Psi(x^{(j)}) = \lambda_1 \sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)}) \end{aligned}上式左右同乘$\Psi(x^{(l)})^T$，凑出核函数的形式，得到 \begin{aligned} \frac{1}{M} \Psi(x^{(l)})^T \left[ \sum_{i=1}^M \Psi(x^{(i)}) \sum_{j=1}^M a^{(j)}_1 \Psi(x^{(i)})^T \Psi(x^{(j)}) \right] = \lambda_1 \Psi(x^{(l)})^T \left[ \sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)}) \right] \\ \frac{1}{M} \sum_{i=1}^M \underbrace{\Psi(x^{(l)})^T \Psi(x^{(i)})}_{\kappa(x^{(l)}, x^{(i)}))} \sum_{j=1}^M a^{(j)}_1 \underbrace{\Psi(x^{(i)})^T \Psi(x^{(j)})}_{\kappa(x^{(i)}, x^{(j)}))} = \lambda_1 \sum_{i=1}^M a^{(i)}_1 \underbrace{\Psi(x^{(l)})^T \Psi(x^{(i)})}_{\kappa(x^{(l)}, x^{(i)}))} \\ \frac{1}{M} \sum_{i=1}^M \kappa(x^{(l)}, x^{(i)}) \underbrace{ \sum_{j=1}^M a^{(j)}_1 \kappa(x^{(i)}, x^{(j)}) } = \lambda_1 \underbrace{ \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(l)}, x^{(i)}) } \end{aligned}记$a_1 = \begin{bmatrix} a^{(1)}_1 &amp; a^{(2)}_1 &amp; \cdots &amp; a^{(M)}_1 \end{bmatrix}^T$，核矩阵$K = \begin{bmatrix} \kappa(x^{(i)}, x^{(j)}) \end{bmatrix}, i, j \in {1, 2, \cdots, M}$，则有 \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(j)}, x^{(i)}) = K_{j} a_1 (K的第j行) \tag{6}代入后 \sum_{i=1}^M \kappa(x^{(l)}, x^{(i)}) K_{i} a_1 = \lambda_1 M K_{l} a_1等号左边再使用一次$(6)$，有 K_l \begin{bmatrix} K_1 a_1 \\ K_2 a_1 \\ \cdots \\ K_M a_1 \end{bmatrix} = \lambda_1 M K_{l} a_1其中$K a_1 = \begin{bmatrix} K_1 a_1 &amp; K_2 a_1 &amp; \cdots &amp; K_M a_1 \end{bmatrix}^T$，故$K_l K a_1 = \lambda_1 M K_{l} a_1$，由于$K_l \neq 0$，所以 K a_1 = \lambda_1 M a_1 \tag{7}所以对矩阵$K$进行特征分解即可计算得到$a_1$，同PCA一样，按特征值降序对特征向量排序，选取前$k$个主轴对应的权值向量${a_1, a_2, \cdots, a_k}$，注意$a_k=\Psi(x^{(i)})^T \Phi(u_1)$并不是投影的轴，那么对于输入的样本点$x$进行降维时，应有 y = \Phi(u_1)^T \Phi(x)其中$\Phi(u_1) = \frac{1}{\lambda_1 M} \sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)})$，，仅考虑方向省略$\frac{1}{\lambda_1 M}$，所以 \begin{aligned} y_1 = \left[\sum_{i=1}^M a^{(i)}_1 \Psi(x^{(i)})\right]^T \Phi(x) \\ = \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(i)}, x) \end{aligned} \tag{8}至于数据的重建，应该有$\hat{\Phi}(x) = \sum y_k \Phi(u_k)$，而$\Phi(u_k)$未知，故只能通过迭代优化的形式进行重建，最小化目标可使用MSE，即最小化 J = \min \{|| \Phi(x) - \hat{\Phi}(x) ||^2\} \tag{9}整理一下上面的算法 选择核函数，如高斯核$\kappa(x, y) = - \frac{||x - y||^2}{2 \sigma^2}$，多项式核$\kappa(x, y) = (x^T y + c)^d$； 根据选用的核函数，计算核矩阵$K_{M \times M} = \begin{bmatrix} \kappa(x^{(i)}, x^{(j)}) \end{bmatrix}, i, j \in {1, 2, \cdots, M}$； 对$K$进行特征分解，根据特征值降序对特征向量进行排序，计算得到各个主轴方向上的权重参数${a_1, a_2, \cdots, a_k}$； 对于输入的样本点$x$，根据$y_1 = \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(i)}, x)$进行降维，可以看到，Kernel PCA主轴的计算与训练样本$x^{(i)}$有关，需要进行保存。 Kernel LDA对于映射后数据$\Phi(x)$ ，其类内离差阵$S_W$与类间离差阵$S_B$为 \begin{aligned} S_W = \sum_{j=1}^C \frac{M_j}{M} \left\{ \frac{1}{M_j} \sum_{i=1}^{M_j} \left[\Phi(x^{(i)}) - \Phi(\mu^{(j)})\right] \left[\Phi(x^{(i)}) - \Phi(\mu^{(j)})\right]^T \right\} \\ S_B = \sum_{j=1}^C \frac{M_j}{M} \left\{ \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right] \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]^T \right\} \end{aligned} \tag{10}其中$\Phi(\mu^{(j)}) = \frac{1}{M_j} \sum_{i=1} \Phi(x^{(i)}), \Phi(\mu) = \frac{1}{C} \sum_{j} \Phi(\mu^{(j)})$。现以第一主轴为例，$\tilde{S_W}$与$\tilde{S_B}$分别表示映射后数据$\Phi(x)$ 在第$1$主轴$\Phi(u_1)$上分布的类内离差阵与类间离差阵，即 \begin{aligned} \tilde{S_W} = \sum_{j=1}^C \frac{M_j}{M} \left\{ \frac{1}{M_j} \sum_{i=1}^{M_j} \left[\tilde{\Phi}(x^{(i)}) - \tilde{\Phi}(\mu^{(j)})\right] \left[\tilde{\Phi}(x^{(i)}) - \tilde{\Phi}(\mu^{(j)})\right]^T \right\} \\ \tilde{S_B} = \sum_{j=1}^C \frac{M_j}{M} \left\{ \left[\tilde{\Phi}(\mu^{(j)}) - \tilde{\Phi}(\mu)\right] \left[\tilde{\Phi}(\mu^{(j)}) - \tilde{\Phi}(\mu)\right]^T \right\} \end{aligned} \tag{11}其中$\tilde{\Phi}(\mu^{(j)}) = \frac{1}{M_j} \sum_{i=1} \tilde{\Phi}(x^{(i)}), \tilde{\Phi}(\mu) = \frac{1}{C} \sum_{j} \tilde{\Phi}(\mu^{(j)})$，且由投影可知 \tilde{\Phi} (x^{(i)}) = \Phi(u_1)^T \Phi(x^{(i)});\quad \tilde{\Phi} (\mu^{(j)}) = \Phi(u_1)^T \Phi(\mu^{(j)});\quad \tilde{\Phi} (\mu) = \Phi(u_1)^T \Phi(\mu)将以上代入$\tilde{S_W}$与$\tilde{S_B}$，有 \begin{aligned} \tilde{S_W} = \Phi(u_1)^T S_W \Phi(u_1) \\ \tilde{S_B} = \Phi(u_1)^T S_B \Phi(u_1) \end{aligned} \tag{12}与LDA思路一致，定义优化目标为 J = \min\left\{ \frac{\tilde{S_W}}{\tilde{S_B}} \right\} = \min\left\{ \frac{\Phi(u_1)^T S_W \Phi(u_1)}{\Phi(u_1)^T S_B \Phi(u_1)} \right\} \tag{13}最终同LDA，有$S_B \Phi(u_1) = \lambda_1 S_W \Phi(u_1)$或$S_W^{-1} S_B \Phi(u_1) = \lambda_1 \Phi(u_1)$，但是有同样的问题，由于隐式映射$\Phi(x)$，$S_W, S_B$不能直接求解。由PCA可知，主轴$\Phi(u_1)$一定由高维数据样本点$\Phi(x^{(i)})$张成，即 \Phi(u_1) = \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \tag{14}$(10), (15)$代入$\Phi(u_1)^T S_W \Phi(u_1), \Phi(u_1)^T S_B \Phi(u_1)$，有 \begin{aligned} \Phi(u_1)^T S_W \Phi(u_1) = \underbrace{ \left[ \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \right]^T }_{\Phi(u_1)^T} \underbrace{ \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j} \left\{ \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right] \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right]^T \right\} }_{S_W} \underbrace{ \left[ \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \right] }_{\Phi(u_1)} \\ \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j} \left\{ \underbrace{ \left[ \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)}) \right]^T \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right] } \underbrace{ \left[\Phi(x^{(i)}_j) - \Phi(\mu^{(j)})\right]^T \left[ \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)}) \right] } \right\} \end{aligned}其中 \begin{aligned} \left[ \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)}) \right]^T \left[ \Phi(x^{(i)}_j) - \Phi(\mu^{(j)}) \right] \\ = \sum_{k=1}^M a^{(k)}_1 \kappa(x^{(k)}, x^{(i)}_j) - \left[ \sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)}) \right]^T \left[ \frac{1}{M_j} \sum_{i=1}^{M_j} \Phi(x^{(i)}_j) \right] \\ = \sum_{k=1}^M a^{(k)}_1 \kappa(x^{(k)}, x^{(i)}_j) - \sum_{k=1}^M a^{(k)}_1 \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j) \\ = \sum_{k=1}^M a^{(k)}_1 \left[ \underbrace{ \kappa(x^{(k)}, x^{(i)}_j) }_{K^{(j)}_{k, i}} - \underbrace{ \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j) }_{\mu_{\kappa^{(j)} k}} \right] = a_1^T (K^{(j)}_i - \mu_{\kappa^{(j)}}) \end{aligned} $K^{(k)}_{:, i}$表示第$k$类的第$i$列，为列向量；$K^{(k)}_i$表示第$k$类的第$i$行，为行向量。 所以 \Phi(u_1)^T S_W \Phi(u_1) = \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j} a_1^T (K^{(j)}_i - \mu_{\kappa^{(j)}}) (K^{(j)}_i - \mu_{\kappa^{(j)}})^T a_1 \tag{15}记$M = \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j}(K^{(j)}_i - \mu_{\kappa^{(j)}}) (K^{(j)}_i - \mu_{\kappa^{(j)}})^T$，则 \Phi(u_1)^T S_W \Phi(u_1) = a_1^T M a_1 \tag{16}同理 \begin{aligned} \Phi(u_1)^T S_B \Phi(u_1) = \underbrace{ \left[ \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \right]^T }_{\Phi(u_1)^T} \underbrace{ \sum_{j=1}^C \frac{M_j}{M} \left\{ \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right] \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]^T \right\} }_{S_B} \underbrace{ \left[ \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \right] }_{\Phi(u_1)} \\ \sum_{j=1}^C \frac{M_j}{M} \left\{ \underbrace{ \left[ \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \right]^T \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right] } \underbrace{ \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right]^T \left[ \sum_{i=1}^M a^{(i)}_1 \Phi(x^{(i)}) \right] } \right\} \end{aligned}其中 \begin{aligned} \left[\sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})\right]^T \left[\Phi(\mu^{(j)}) - \Phi(\mu)\right] \\ = \left[\sum_{k=1}^M a^{(k)}_1 \Phi(x^{(k)})\right]^T \left[ \frac{1}{M_j} \sum_{i=1}^{M_j} \Phi(x^{(i)}_j) - \frac{1}{M} \sum_{i=1}^M \Phi(x^{(i)}) \right] \\ = \sum_{k=1}^M a^{(k)}_1 \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j) - \sum_{k=1}^M a^{(k)}_1 \frac{1}{M} \sum_{i=1}^M \kappa(x^{(k)}, x^{(i)}) \\ = \sum_{k=1}^M a^{(k)}_1 \left[ \underbrace{ \frac{1}{M_j} \sum_{i=1}^{M_j} \kappa(x^{(k)}, x^{(i)}_j) }_{\mu_{\kappa^{(j)} k}} - \underbrace{ \frac{1}{M} \sum_{i=1}^M \kappa(x^{(k)}, x^{(i)}) }_{\mu_{\kappa k}} \right] \\ = a_1^T (\mu_{\kappa^{(j)}} - \mu_{\kappa}) \end{aligned}所以 \Phi(u_1)^T S_B \Phi(u_1) = \sum_{j=1}^C \frac{M_j}{M} a_1^T (\mu_{\kappa^{(j)}} - \mu_{\kappa}) (\mu_{\kappa^{(j)}} - \mu_{\kappa})^T a_1记$N = \sum_{j=1}^C \frac{M_j}{M} (\mu_{\kappa^{(j)}} - \mu_{\kappa}) (\mu_{\kappa^{(j)}} - \mu_{\kappa})^T$，则 \Phi(u_1)^T S_B \Phi(u_1) = a_1^T N a_1 \tag{17}所以优化目标改为 J = \min \left\{\frac{a_1^T M a_1} {a_1^T N a_1}\right\} \tag{18}即求解 M^{-1} N a_1 = \lambda_1 a_1 \tag{19}对于新的投影数据，同Kernel PCA，有 y_1 = \sum_{i=1}^M a^{(i)}_1 \kappa(x^{(i)}, x)整理一下上述算法 选择核函数$\kappa(x, y)$； 计算整体的核矩阵$K_{M \times M}$，利用类别标签从中截取每个类别下的核矩阵$K^{(j)}_{M \times M_j}$； 根据下式计算$M$与$N$； M = \frac{1}{M} \sum_{j=1}^C \sum_{i=1}^{M_j}(K^{(j)}_i - \mu_{\kappa^{(j)}}) (K^{(j)}_i - \mu_{\kappa^{(j)}})^TN = \sum_{j=1}^C \frac{M_j}{M} (\mu_{\kappa^{(j)}} - \mu_{\kappa}) (\mu_{\kappa^{(j)}} - \mu_{\kappa})^T 对矩阵$M^{-1}N$进行特征分解，将特征向量按特征值降序排序，选择$k$个主分量对于的权值${a_1, a_2, \cdots, a_k}$； M^{-1} N a_k = \lambda_k a_k 对于需要降维的数据$x$，计算各维度上的坐标，同样的，主轴的计算与训练样本$x^{(i)}$有关，需要进行保存。 y_k = \sum_{i=1}^M a^{(i)}_k \kappa(x^{(i)}, x) 代码详细代码见Basic-Machine-Learning-Algorithm/algorithm/kernelPCA.py与Basic-Machine-Learning-Algorithm/algorithm/kernelFDA.py。 实验构造下图数据，明显为非线性数据，利用线性方法不能提取有效性息用于分类。 利用Kernel PCA进行降维结果如下 利用Kernel FDA进行降维结果如下 相比较于PCA，FDA效果更好。 Reference G. Baudat and F. Anouar. Generalized discriminant analysis using a kernel approach. Neural Computation, 12:2385-2404, 2000. S. Mika, G. Ratsch, J. Weston, B. Scholkopf, and K. Muller. Fisher discriminant analysis with kernels. In IEEE Neural Networks for Signal Processing Workshop, pages 41-48, 1999. Wang Q . Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models[J]. Computer Science, 2012.]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD: Single Shot MultiBox Detector]]></title>
    <url>%2F2019%2F09%2F02%2FSSD-Single-Shot-MultiBox-Detector%2F</url>
    <content type="text"><![CDATA[IntroductionSSD主要做了以下工作 设计了one-stage检测算法，比同样是one-stage的YOLOv1更快更准确，相比较Faster R-CNN等two-stage检测算法，在不降低准确率的情况下速度更快； 其核心是用小卷积核，对特征图进行卷积运算，通过一系列的固定回归框进行预测； 用不同尺度的特征图产生不同尺度的预测，并且明确指定多种纵横比； 可进行端到端训练，即使是低分辨率图像输入，准确率也很高； 在PASCAL VOC、MSCOCO、ILSVRC等数据集上得到state-of-art的结果。 Data Preparation多层特征图一般来说，深层特征图感受野更大，具有更多的语义信息，而浅层特征图保留了更多的细节，因此SSD对两者都加以应用。 对于某层网络输出的特征图，需要确定用特征图上哪个位置的cell负责对应ground truth边界库那个的预测。和YOLO不同，SSD使用杰卡德相似度(jaccard overlap)来确定，也即交并比，$J(A, B) = \frac{|A \bigcap B|}{|A \bigcup B|}$，当该值超过阈值，则该位置负责边界框的回归。 例如对于下图，在$4 \times 4$、$6 \times 6$与$8 \times 8$的特征图上，指定两种anchor：$2.5: 1.5$与$1.5: 2.5$，选择IoU阈值$0.5$，可视化情况如下。可以看到，浅层特征分辨率较高，对细小物体的检测比较有效，而深层特征分辨率低，对大的物体比较敏感。 另外，真实框相对于锚框的计算方法如下，其中$(x_g, y_g, w_g, h_g)$与$(x_a, y_a, w_a, h_a)$分别表示真实框$g$与锚框$a$的中心位置坐标、边长 \begin{aligned} t_x = \frac{x_g - x_a}{w_a} \quad & t_y = \frac{y_g - y_a}{h_a} \quad & t_w = \log (\frac{w_g}{w_a}) \quad & t_h = \log (\frac{h_g}{h_a}) \end{aligned} 关于anchor参数的设置这样带来的问题是，对于各个分辨率下的特征图，每个点处anchor参数是不同的，如上面随意设置的参数，可能实际使用时不适合，SSD是这样解决的。 对于某一层特征图，设置五种纵横比：$a_r \in {\frac{1}{3}, \frac{1}{2}, 1, 2, 3}$，纵横比为$1$的anchor再缩放形成另一尺度的anchor，$s_k’ = \sqrt{s_k s_{k+1}}$，那么该层每个点处包含$6$个anchor。 对于不同层间，特征图的尺度改变，anchor大小也需随之变化。设置$s_{min}=0.2, s_{max}=0.9$，若所用特征图总层数为$m$，则从浅层到深层，各层的anchor尺度为$s_k = s_{min} + \frac{k - 1}{m - 1}(s_{max} - s_{min}), k \in [1, m]$，在第$k$层，anchor尺寸为$w^a_k = h^a_k = \frac{s_k}{\sqrt{a_r}}$。 anchor的中心定于$(\frac{i + 0.5}{|f_k|}, \frac{j + 0.5}{|f_k|})$，$|f_k|$为第$k$层特征图某点对应原图中网格的边长。实际上，锚框参数是人为指定的，是需要被解决的一个问题，应根据实际情况进行修改。 Model SSD接受$300 \times 300$的图像输入，以VGG-16作为backbone网络，从Conv5-3层截断，后面加上一些卷积层作为特征提取器($1 \times 1$与$3 \times 3$卷积层堆叠的形式)。SSD在多个特征图上进行候选框的预测，例如对于$m \times n \times p$的特征图，在该尺度上每个位置指定$k$种尺度的anchor，用$p \times 3 \times 3 \times [k \times (classes + 4)]$的卷积核进行运算，得到$m \times n \times [k \times (classes + 4)]$的特征图。注意，对于$C$个类别的识别，应包含背景，所以$classes = C + 1$。 如上图，共生成$38^2 \times 4 + 19^2 \times 6 + 10^2 \times 6 + 5^2 \times 6 + 3^2 \times 4 + 1^2 \times 4 = 8732$个候选框。 根据置信度，滤除一大部分候选框，再用NMS算法删除冗余框。 Training Step设置参数$1^{ij_k}_l$，表示在特征图$(i, j)$处的第$k$个锚框$a^{ij_k}$，用于预测真实框$g_l$，即 1^{ij_k}_l = \begin{cases} 1 & \text{IoU}(a^{ij_k}, g_l) > \text{threshold} \\ 0 & \text{otherwise} \end{cases}总体损失由定位损失$L_{loc}$与分类损失$L_{conf}$加权组成，定义在特征图位置$(i, j)$处，对于锚框$k$，网络输出为$\hat{y}^{ij_k} = (\hat{t}^{ij_k}_x, \hat{t}^{ij_k}_y, \hat{t}^{ij_k}_w, \hat{t}^{ij_k}_h, \hat{c}^{ij_k}_0, \hat{c}^{ij_k}_1, \hat{c}^{ij_k}_2, \cdots, \hat{c}^{ij_k}_C)$ L_{total} = \frac{1}{N} \left[\alpha \underbrace{\sum_{i, j, k, l} 1^{ij_k}_l \sum_{* \in \{x, y, w, h\}} \text{SmoothL1}(\hat{t}^{ij_k}_*, t^{ij_k}_*)}_{L_{loc}} + \underbrace{\sum_{i, j, k} - \log \hat{c}^{ij_k}_{c}}_{L_{conf}}\right]其中$\hat{c}^{ij_k}_{c} = \frac{\hat{c}^{ij_k}_{c}}{\sum \hat{c}^{ij_k}_{c}}$ 另外，由于物体分布稀疏，分类负样本数目(背景)远大于正样本，所以用困难样本挖掘(hard negative mining)的方法，保留损失值最大的负样本，使分类正负样本的比例保持在$neg: pos = 3: 1$左右。 同样地，SSD训练时也使用了数据集扩增，数据来源于以下三种途径 整个原始图像； 在标注框附近采样，使得杰卡德相似度系数在$0.1, 0.3, 0.5, 0.7, 0.9$； 随机采样截取。 随机产生的patch边长在原始边长的$[0.1, 1]$范围内，纵横比范围为$[0.5, 2]$。此外依照$50\%$的概率进行水平翻转，并添加其余干扰。 用SGD进行优化，批次大小32，初始学习率0.001，动量0.9，权重衰减0.0005。 Results Reference SSD: Single Shot MultiBox Detector - arXiv.org weiliu89/caffe - Github ssd_eccv2016_slide - cs.unc.edu]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO! v1, v2, v3]]></title>
    <url>%2F2019%2F09%2F01%2FYOLO-v1-v2-v3%2F</url>
    <content type="text"><![CDATA[前言YOLO是You Only Look Once的简称，是one-stage检测算法，在2016年刚提出时风靡一时，现在看可能存在一些缺点，后续有许多one-stage检测算法都参考YOLO的思路，如SSD。目前位置，YOLO共发布了3代，论文链接在Reference中。 YOLOv1YOLO的一个关键点是，将目标检测作为回归任务，将分离的边界框回归和分类联合起来，可进行端到端的优化。 We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are. YOLO算法具有以下几个特性 Extremely fast. 将检测作为回归问题后，不需要候选框生成、分类、回归框矫正等几个步骤，可达到45FPS的速度，Fast YOLO甚至能达到155FPS； Reasons globally. YOLO接受整个图像的输入，而不是将感兴趣区域单独提取，这相比于R-CNN算法，对上下文信息的使用更加全面； Highly generalizable. 当在艺术图上进行识别时，也具有较好的识别效果。 数据准备YOLO的思路是，将一张指定尺寸输入的图片分割为$S \times S$个网格，每个网格负责落在它内部的物体的检测，并且每个网格将负责$B$个回归框的检测，每个回归框的参数包括$(x, y, w, h, c)$，$x, y$表示回归框的中心点坐标，$w, h$表示回归框的边长，$c$表示回归框的置信度表示cell内是否包含真实框中心点。此外，每个网格也负责$C$类的物体分类任务，输出$C$维的分类向量。因此输入图片经过深度卷积网络后，得到的特征图尺寸应为$S \times S \times [B \times (4 + 1) + C]$，如在论文中提到，在PSCAL VOC数据集上，指定$S=7, B=2, C=20$，则特征图尺寸为$7 \times 7 \times 30$。 这里有个疑问，是训练样本confidence真实值如何确定，难道是计算cell与真实框的IoU？这样计算得到的值很小。 下图为论文前3个版本中的示意图。 上述说明比较抽象，用代码说明生成的target是个什么玩意。假如我们有图像如下 首先将其裁剪为正方形，并缩放至$448 \times 448$，得到 将图像分成$7 \times 7$个网格，并假设其标注位置为：狗$( 95, 295, 140, 250)$、自行车$(205, 212, 340, 214)$、汽车$(365, 92, 150, 64)$，在图上为 假设在$7 \times 7$特征图上，某cell的位置为$(i, j)$，其中$0 \leq i, j &lt; 7$，应有$i = \lfloor\frac{x}{cellsize}\rfloor, j = \lfloor\frac{y}{cellsize}\rfloor$。若映射回原图后，该cell内包含某真实标记框$(x, y, h, w)$的中心，则置$c=1$，且对应类别的特征图上置$(i, j, class)$处类别信息为1。关于ground truth标记框相对于cell的各个偏移量计算与可视化输出如下 \begin{cases} t_x = \frac{x - x'}{cellsize}, x' = (i + \frac{1}{2}) \times cellsize\\ t_y = \frac{y - y'}{cellsize}, y' = (j + \frac{1}{2}) \times cellsize\\ t_h = \frac{h}{cellsize} \\ t_w = \frac{w}{cellsize} \end{cases}其中$x, y, cellsize$等都除以图像尺寸进行归一化。 类别特征图$(S \times S \times C)$比较好理解，在上图中用颜色表示，展开为张量即可。 网络结构 网络共包含24个卷积层与2个全连接层。卷积的基本结构是先通过$1 \times 1$卷积层减少前层输出特征图的通道数，再紧接用$3 \times 3$卷积层进行运算。其使用darknet训练的配置文件如yolov1.cfg，PyTorch实现可查看yolo.py，文中插代码很不地道。 损失函数损失函数由三部分组成，使用sum-square error作为误差函数，因为易于优化 L = \sum_i (y_i - \hat{y_i})^2定义 1^{\rm{condition}}_{\rm{index}} = \begin{cases} 1 & \text{if meets condition} \\ 0 & \text{otherwise} \end{cases} 坐标 考虑到，小物体定位惩罚应该比大物体的惩罚更大。所以在边长回归时，考虑回归边长的开方值，以平衡大小物体的惩罚。 \begin{aligned} L_{\rm{coord}} = \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right. \\ \left. + (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] \end{aligned} 置信度 图像中具有物体的网格是非常稀疏的，导致正负样本数目不均衡，这些作为负样本的cell会将置信度分数推向$0$，影响网络模型的稳定性,通过系数$\lambda_{\rm{noobj}}$进行平衡 \begin{aligned} L_{\rm{confid}} = \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} (C_i - \hat{C}_i)^2 \\ + \lambda_{\rm{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{noobj}}_{\rm{ij}} (C_i - \hat{C}_i)^2 \end{aligned} 类别 注意到，分类任务仅对包含物体的cell进行惩罚。 L_{\rm{classify}} = \sum_{i=0}^{S^2} 1^{\rm{obj}}_{\rm{i}} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2 如果将定位误差和分类误差相等地加权，可能不理想。所以，通过设置系数$\lambda_{\rm{coord}}$进行平衡，增大边界框回归的惩罚，降低分类的惩罚。 \begin{aligned} \lambda_{\rm{coord}} \underbrace{\sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 + (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right]}_{L_{\rm{coord}}} \\ + \underbrace{\sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{obj}}_{\rm{ij}} (C_i - \hat{C}_i)^2 + \lambda_{\rm{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^B 1^{\rm{noobj}}_{\rm{ij}} (C_i - \hat{C}_i)^2}_{L_{\rm{confid}}} \\ + \underbrace{\sum_{i=0}^{S^2} 1^{\rm{obj}}_{\rm{i}} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2}_{L_{\rm{classify}}} \end{aligned}论文中设置$\lambda_{\rm{coord}}=5, \lambda_{\rm{noobj}}=0.5$。 训练步骤首先，将网络的前20层卷积分离，添加全局均值池化与全连接层，在ImageNet数据集上进行预训练，输入图像的尺寸为$224 \times 224$。大约训练了一周，并取得了top-5达到$88\%$的准确率。 之后，将模型用于检测的训练。Ren等人提出，全连接层可提高网络的performance，所以在预训练的20层卷积后，添加4层卷积层与2层全连接层，权值随机初始化。图像的分辨率升高到$448 \times 448$。 在测试时，每个cell将负责多个框的生成；在训练时，每个cell对每个回归框负责，也就是说，如果某个cell内出现两个重复的真实标记框，将对这两个框分别计算损失并累加。 训练超参数设置。在PASCAL VOC 2007和2012的训练集与验证集上进行训练，注意在测试2012数据集时，也包含2017测试数据。批次大小设置为$64$，动量(momentum)设置为$0.9$，权值衰减设置为$0.0005$。 祖传参数。 学习率调节。如果一开始设置较大的学习率，不稳定的梯度会导致网络发散。因此从第一个epoch开始，缓慢将学习率从$10^{-3}$升高到$10^{-2}$，之后在$10^{-2}$保持$75$代，然后在$10^{-3}$保持$30$代，最后在$10^{-4}$保持$30$代。 测试阶段测试阶段，置信度的计算包含两个部分的乘积，即框内是否含物体$\rm{Pr}(\rm{object})$与候选框与真实框之间的交并比$\rm{IoU}(truth, pred)$，计算方法如下 \rm{Confidence} = \rm{Pr}(Class_i | Object) * \rm{Pr}(Object) * \rm{IoU}^{truth}_{pred} = \rm{Pr}(Class_i) * \rm{IoU}^{truth}_{pred}根据置信度，先删除一些评分较低的候选框，之后用NMS算法删除冗余框。其最终输出可由下图说明 结果对比 缺点和限制 每个单元网格只能预测两个框，且两个框同属于一类，这限制了模型对相邻物体的检测； 另外，对于聚集的小物体，如鸟群等，很难检测到； 由于网络经过已有标注训练，对于新的纵横比很难适应； 网络多次进行下采样，特征较为粗略； 小物体检测比大物体检测错误率高，这是由于错误定位造成的。 YOLOv2YOLOv2即YOLO9000，在上一代YOLO的基础上进行改进。首先，它可以接受任意尺寸图像的输入而不是固定的$448 \times 448$，速度更快。此外，一个比较重要的点是，在训练YOLO9000时，联合使用分类数据，用带标记的图像进行学习精确定位，而分类图像添加到样本集中，用作分类部分的训练。 BetterYOLOv2进行了众多实验，添加别的组件，验证新模型在识别率上的表现，如下表 Batch Normalization 添加批归一化层后，mAP升高$2\%$以上。并且可减少其他形式的正则化惩罚，如移除了dropout层。 High Resolution Classifier 在YOLOv1训练时，使用$224 \times 224$尺寸的图片进行模型预训练，之后用$448 \times 448$尺寸的图片进行检测网络的训练，也就是说，网络需要在学习检测任务的同时，适应新的输入分辨率。所以在YOLOv2训练时，先将预训练网络在$448 \times 448$尺寸的数据机上进行微调，共迭代10代。之后训练检测网络，这提升了$4\%$的mAP。 Convolutional With Anchor Boxes YOLOv2移除了全连接层，并添加了锚框。首先，移除一层最大池化层，使得特征图的分辨率升高，这样输出的特征图尺寸为$13 \times 13$。考虑到特征图的几何意义，最终层输出的特征图每个位置代表一个网格，也就是说，每个网格中心都应对应一个位置，但是$448/13$不为整数，故将图像分辨率调整至$416 \times 416$，网络下采样倍率为$\frac{416}{13}=32$。 在不添加锚框时，一个cell仅能输出$7 \times 7 \times 2 = 98$个候选框。而每个cell设置锚框后，对每个锚框均进行预测，而不是仅考虑cell的空间位置，可生成更多的候选框。 没有锚框的情况下，模型指标为$69.5$mAP与$81\%$的召回率，增加后，mAP降低为$69.2$，召回率上升到$88\%$，这说明模型还有较大空间可以调整。 对于锚框，文中没有提及，以下进行一些说明，现在每个cell的尺寸为$32\times32$，假定对单个cell设置$3$种纵横比的anchor，如$32 \times 32, 24 \times 48, 48 \times 24$，那么对于下图中包含物体的cell，锚框可视化为 对于上述三个锚框，从上至下分别为$32 \times 32, 24 \times 48, 48 \times 24$的锚框对应的特征图显示如下 此外，每个纵横比下可分别设置$3$种尺度。 Dimension Clusters 至此遇到两个问题。第一，锚框的纵横比、尺度等是人为指定的，网络可以适应指定的锚框，但不是最优的，希望能够从数据中得到锚框的参数。所以对标注框的长宽尺寸进行K-Means聚类，注意到，如果使用欧式距离作为聚类指标，框越大它造成的误差也越大，希望的是IOU评分上升，所以定义指标为 d(\rm{box}, \rm{centroid}) = 1 - \rm{IoU}(\rm{box}, \rm{centroid}) 最终确定为$k=5$，在VOC和MSCOCO数据机上可能有所差异，详情查看下图 Direct location prediction 对于偏移量，如果沿用一般的计算方法，则有 \begin{cases} t_x = \frac{x - x_a}{w_a}, x_a = (\lfloor\frac{x}{cellsize}\rfloor + \frac{1}{2}) \times w_a \\ t_y = \frac{y - y_a}{h_a}, y_a = (\lfloor\frac{y}{cellsize}\rfloor + \frac{1}{2}) \times h_a \\ t_w = \frac{w}{w_a} \\ t_h = \frac{h}{h_a} \end{cases} 也即 \begin{cases} x = t_x \times w_a + x_a \\ y = t_y \times h_a + y_a \\ w = t_w \times w_a \\ h = t_h \times h_a \end{cases} 当$t_x=1$时表示相对于锚框向右平移$w_a$的距离，$t_x=-1$时表示相对于锚框向左平移$w_a$的距离，上式中对$t_x$是没有限制作用的。现添加sigmoid函数处理，将值映射到$[0, 1]$范围内，并且直接预测坐标，而不是偏移量的方法，此外，利用指数函数$e^x$进行映射，平衡回归框尺寸对损失值的影响。如下 \begin{cases} b_x = \sigma(t_x) + c_x \\ b_y = \sigma(t_y) + c_y \\ b_w = p_w e^{t_w} \\ b_h = p_h e^{t_h} \end{cases} 其中$(c_x, c_y)$为特征图上相对于左上角的坐标，$\sigma(t_x), \sigma(t_y), t_w, t_h$是网络的输出，$p_*$为锚框的尺寸信息(除以图像边长归一化)。这种计算方法下，各锚框的ground truth应为 \begin{cases} \sigma(t_x) = \frac{x}{cellsize} - \lfloor\frac{x}{cellsize}\rfloor \\ \sigma(t_y) = \frac{y}{cellsize} - \lfloor\frac{y}{cellsize}\rfloor \\ t_w = \log \frac{b_w}{p_w} \\ t_h = \log \frac{b_h}{p_h} \end{cases} Fine-Grained Features 为获得更细粒度的特征，用于小物体的检测，如Faster-RCNN与SSD是在各层特征图上进行候选框的生成。YOLOv2添加了passthrough层，将浅层特征与深层特征堆叠起来。这提升了$1\%$的表现。 Multi-Scale Training 由于YOLOv2中仅包含卷积层与池化层，所以可接受任意尺寸的输入，为使得网络对各尺寸均有良好的鲁棒性，在尺寸为${320, 352, \cdots, 608}$时都进行训练。这样做可以使图像尺寸减小以加快计算速度，减少显存消耗，并且不降低准确率，在$288 \times 288$的尺寸时，可达到$90$FPS。 Faster Darknet-19 大多数检测器用VGG-16作为特征提取网络结构，但是它参数量众多，且运算量很大，即使输入为$224 \times 224$也需要千万级别的FLOPS。新设计的Darknet-19架构，仅需要$5.58$百万级别的FLOPS，达到了72.9% top-1和91.2% top-5的准确率。 Training for classification 首先，在ImageNet上进行1000类的图像分类模型训练，共迭代160代，初始学习率为$0.1$，多项式学习率衰减，权值衰减设置为$4e-5$，动量$0.9$。之后，将图像分辨率提高到$448 \times 448$，学习率0.001，进行10个周期的迭代，来微调网络，使其适应该分辨率。 Training for detection 将预训练模型的最后一层卷积层删除，添加$3 \times 3 \times 1024$卷积层与$1 \times 1 \times [(C + 4 + 1) \times B]$卷积层。注意到，与YOLOv1不同的是，每个框都会输出类别信息。在VOC数据集上，$C = 20, B = 5$。在$3 \times 3 \times 512$层的输出到最终的输出间设置passthrough，以更好地使用浅层特征。 共进行$160$个周期的迭代，初始学习率$0.001$，在第$60, 90$代时衰减$10\%$。权重衰减$5e-4$，动量$0.9$。 Stronger在YOLO9000训练过程中，用到了一种联合分类数据的训练机制。将监测数据与分类数据混合，若训练过程中遇到检测数据，则计算全部的损失，而若遇到分类数据，仅计算分类的损失。但是这样做有一些挑战，检测数据集中对物体的标号仅仅包含dog、boat等，而分类数据标签划分更细，如dog可分为Norfolk terrier、Yorkshire terrier等。分类模型使用softmax作为损失函数，只能进行单标签的分类。一种解决方法是，采用多标签的模型来组合不假设互斥的数据集，这种方法忽略了我们对数据所知的所有结构，例如，MSCOCO的类别都是互斥的。 Hierarchical classification ImageNet的标签来源于WordNet，这是一个语言的数据集合，整理了语义关系。比如说，Norfolk terrier和Yorkshire terrier都是猎犬，都是犬等等。由于语言的复杂性，WordNet的结构是有向图，在训练YOLOv2时，并不需要完整的图结构，因此可通过这种非扁平的数据结构构建分层树形图来简化问题。 对于ImageNet中的视觉名词，许多同义词只有一条路径，所以首先将这些路径添加到树中，然后继续迭代剩下的名词。如果某个概念通往根节点有两条路径，那么其中一条会往树中添加3条边，另一条路径添加1条边，选择尽量短的边添加方案。最终形成层级的结构WordTree，这个树可用于计算条件概率，如 \begin{aligned} Pr(\rm{Norfolk terrier} | \rm{terrier}) \\ Pr(\rm{Yorkshire terrier} | \rm{terrier}) \\ Pr(\rm{Bedlington terrier} | \rm{terrier}) \\ \cdots \end{aligned} 对于某个类别的概率，其通过各条件概率累乘的方法，如 \begin{aligned} Pr(\rm{Norfolk terrier}) = \\ Pr(\rm{Norfolk terrier} | \rm{terrier}) \\ * Pr(\rm{terrier} | \rm{hunting dog}) \\ * \cdots \\ * Pr(\rm{mammal} | \rm{animal}) \\ * Pr(\rm{animal} | \rm{physical object}) \\ * Pr(\rm{physical object}) \end{aligned} 注意$Pr(\rm{physical object})=1$。 Dataset combination with WordTree 借助于WordTree的多层级结构，构建多标签分类模型，如下图，进行多次softmax运算 Joint classification and detection 在MSCOCO与ImageNet数据集上，产生了9000多个类别。此外我们需要对模型进行验证。 We also need to evaluate our method so we add in any classes from the ImageNet detection challenge that were not already included. The corresponding WordTree for this dataset has 9418 classes. ImageNet是一个更大的数据集，所以通过对MSCOCO进行过采样来平衡数据，使ImageNet以4：1的比例增大。使用此数据集进行训练时，仅选用3个锚框以减少内存消耗，当网络遇到带检测标签的数据，进行全部的损失计算，而对于分类数据，仅对分类损失进行反传。值得注意的是，如果标签为dog，仅对该层次进行反传，因为再向下没有细化信息，如这是德国牧羊犬还是金毛犬。 YOLOv3YOLOv3相比较于YOLO9000没有过大的改变： 边界框回归值同YOLO9000； 仍进行多标签的分类； 关于锚框，共3种尺度$(S=3)$，类似特征金字塔的概念，则最终层输出为$N \times N \times [(4 + 1 + C) \times B \times S]$； 将多个浅层的特征层上采样后进行组合，用于候选框的生成； 仍使用K-Means确定锚框的纵横比，最终确定9种纵横比$10 \times 13, 16 \times 30, 33 \times 23, 30 \times 61, 62 \times 45, 59 \times 119, 116 \times 90, 156 \times 198, 373 \times 326$，每个纵横比下3种尺度，共27个锚框。 网络仍使用$3 \times 3$与$1 \times 1$卷积层的堆叠，加入shortcut使深度大大增加，最终形成Darknet-53。 YOLOv3运算速度相当快，准确率也可以 Reference YOLO: Real Time Object Detection You Only Look Once: Unified, Real-Time Object Detection YOLO9000: Better, Faster, Stronger YOLOv3: An Incremental Improvement]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[解读] Recent Advances in Deep Learning for Object Detection]]></title>
    <url>%2F2019%2F08%2F31%2F%E8%A7%A3%E8%AF%BB-Recent-Advances-in-Deep-Learning-for-Object-Detection%2F</url>
    <content type="text"><![CDATA[论文可从Recent Advances in Deep Learning for Object Detection - arXiv.org下载。 Content Abstract Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications &amp; benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning.Keywords: Object Detection, Deep Learning, Deep Convolutional Neural Networks 目标检测：在给定图像中找到目标类对象的精确位置，并分配相应的类别标签。 本文介绍的主要分为以下几个部分 组件组成部分； 学习策略； 应用及数据集、测试。 影响检测的几个细节： 检测器结构； 特征学习； 预选框生成； 采样策略等。 1. Introduction1.1 基本的视觉识别问题 图像分类(image classification)：识别给定图像中对象的语义类别； 物体检测(object detection)： 识别给定图像中对象的语义类别，并用边界框表出物体位置； 语义分割(semantic segementation)：为每个像素分配特定的类别标签； 实例分割(instance segementation)：目标检测和语义分割交集，在语义分割基础上，在同类别物体中分辨出不同实例。 1.2 如何设计好的检测算法好的检测算法需要对语义及空间信息有很好的理解。 A good detection algorithm should have a strong understanding of semantic cues as well as the spatial information about the image. 1.3 早期物体检测算法可分成三个步骤 候选框生成(proposal generation)一种直观方法：滑动窗口(sliding window)，将检测图像以不同比例缩放，并使用不同尺寸的滑动窗口选择感兴趣区域(regions of interest)。 特征提取(feature vector extraction)从ROI区域提取固定长度的向量作为特征，如尺度不变特征变换(SIFT)，Haar，梯度直方图(HOG)，或加速鲁棒特征(SURF)。 区域分类(region classification)例如支持向量机(SVM)，配合bagging，adaboost，级联学习(cascade learning)等方法。 传统方法存在的局限性 在候选框生成时，大量冗余的候选框造成误正(false positive)率高。窗口尺度是人为或启发式指定的，不能很好地匹配物体； 特征提取时，基于低级视觉特征，难以捕获上下文语义信息； 以上三个步骤单独设计优化，无法获得全局最优解。 1.4 深度学习方法模拟生物学分层结构，利用反向传播算法更新参数，但存在局限性： 训练样本过少，造成过拟合； 计算资源的限制； 相比较于SVM等传统方法，缺少理论支持。 其优点是 深度卷积神经网络生成从原始像素到高级语义信息的分层特征表示，其从训练数据中自动学习，并且在复杂上下文中显示出更具辨别力的表达能力； 于传统视觉描述符相比，自动学习特征表示，而不是固定的； 可以以端到端的方式进行优化。 1.5 基于深度学习的对象检测框架 两种系列 两阶段检测器(two-stage detector)：通过候选框生成器，生成数目较少的候选框，并对每个候选框内内容进行特征提取；应用特征对候选区域进行分类。如 R-CNN(CNN + SVM/FNN)； 一阶段检测器(one-stage detector)：无虚级联区域分类步骤，对特征图的每个位置上的对象进行分类预测。如 YOLO 及其变体。通常来说，一阶段检测器速度更快，可用于试试物体检测；二阶段检测器精度更高，在公共数据集等测试中效果更好。 2. Problem Settings目标检测涉及识别(如物体分类)和定位(如坐标回归)任务。给定数据集包含$N$张带标记的图片 D = \{X^{(1)}, X^{(2)}, \cdots, X^{(N)}\}对于第$i$张图片，若其内部包含$M_i$个物体，这些物体属于$C$类，第$j$个物体的标签包含所属类别$c^{(i)}_j$，所在位置$b^{(i)}_j$，则 y^{(i)}_j = (c^{(i)}_j, b^{(i)}_j)则该图片对应标签集为 y^{(i)} = \{(c^{(i)}_1, b^{(i)}_1), (c^{(i)}_2, b^{(i)}_2), \cdots, (c^{(i)}_{M_i}, b^{(i)}_{M_i})\}对于预测输出，为 \hat{y}^{(i)} = \{(\hat{c}^{(i)}_1, \hat{b}^{(i)}_1), (\hat{c}^{(i)}_2, \hat{b}^{(i)}_2), \cdots, (\hat{c}^{(i)}_{M_i}, \hat{b}^{(i)}_{M_i})\}总体损失定义为 L(X, \theta) = \frac{1}{N} \sum_{i=1}^N L(X^{(i)}, y^{(i)}, \hat{y}^{(i)}; \theta) + \frac{\lambda}{2} ||\theta||_2^2定义指标交并比IoU(intersection-over-union) \text{IoU} (b^{(i)}_j, \hat{b}^{(i)}_j) = \frac{\text{Area} (b^{(i)}_j \bigcap \hat{b}^{(i)}_j)}{\text{Area} (b^{(i)}_j \bigcup \hat{b}^{(i)}_j)}则预测正确与否的判决为 \text{Prediction} = \begin{cases} \text{Positive} & c^{(i)}_j = \hat{c}^{(i)}_j \quad \text{and} \quad \text{IoU} (b^{(i)}_j, \hat{b}^{(i)}_j) > \Omega \\ \text{Negative} & \text{otherwise} \end{cases}另外，在$C$类物体的检测问题中，需要计算mAP(mean average precision)进行评估。通常来说，达到20FPS的检测器可用于实时检测场景。 3. Detection Components3.1. Detection Settings: bbox-level and mask-level algorithms bbox-level只需要边界框注释，在评估时，计算预测与实际边界框的交并比IoU进行性能衡量。 mask-level即实例分割，需要通过像素级掩码而不是粗略的边界框来分割对象，对空间信息的处理要求更改。 3.2. Detection Paradigms: two-stage detectors and one-stage detectors 两阶段检测器(two-stage detector)：通过候选框生成器，生成数目较少的候选框，并对每个候选框内内容进行特征提取；应用特征对候选区域进行分类。如 R-CNN(CNN + SVM/FNN)； 一阶段检测器(one-stage detector)：无虚级联区域分类步骤，对特征图的每个位置上的对象进行分类预测。如 YOLO 及其变体。通常来说，一阶段检测器速度更快，可用于试试物体检测；二阶段检测器精度更高，在公共数据集等测试中效果更好。 3.2.1. Two-stage Detectors将检测任务分成两部分： 候选框生成基本上思想是选出高召回率(Recall)的图像区域。 对候选框内图像进行判别基于深度学习的模型，可将框内图像进行分类，此外，该模型输出也用以矫正第一阶段输出的候选框，使其更契合物体位置。 3.2.1.1. R-CNN 组成 候选框生成：通过选择性搜索(Selective Search)生成，可拒绝一些明显是背景的候选框，减少误正率(false positive)； 特征提取：通过裁剪、缩放获得固定尺寸图像，输入到深度卷积网络，得到4096维的向量； 区域分类：分类器选用一对多(one-vs-all)SVM，此外边界框回归器将候选框矫正。 细节 与传统方法相比，深度卷积网络生成分层特征，捕获不同尺度信息； 利用迁移学习方法，使用ImageNet与训练的卷积网络权重，全连接层重新初始化权值； 缺点 不共享权值，造成大量冗余计算； 各部分独立，不能进行端到端的方式优化，难以获得全局最优； 选择性搜索难以适应复杂背景的图片，并且无法使用GPU加速。 3.2.1.2. SPP-Net 细节在处理候选框内的图片时，并不将其进行裁剪缩放，而是定义空间金字塔池化层(Spatial Pyramid Pooling Layer)。例如对于尺寸为$H \times W$的可见光图像，将其划分为$N \times N$个网格，在每个尺寸为$\frac{H}{N} \times \frac{W}{N}$网格内进行池化操作。选取不同的$N$重复采样，将不同划分数目$N$下得到的输出，合并为长度为$N \times N$向量。SPP-layer可以接受不同尺度和纵横比的图像，故避免了信息丢失和几何失真等情况。 缺点 仍旧不能进行端到端的方式优化； 由于该层不能进行反向传播，此层之前的网络参数需要固定。 3.2.1.3. Fast R-CNN 细节计算了整个图像的特征图(同SPP-Net)，在特征图上提取固定长度的特征，这一步使用ROI Pooling Layer实现，是Spatial Pyramid Pooling Layer的特殊情况，仅在一种网格划分数目下采样，实际操作步骤如下：在整个图像计算得到的特征图中，根据实际边界框位置选取尺寸为$h \times w$的感兴趣区域(ROI)，指定网格的大小如$H \times W$，将ROI划分为$\frac{h}{H} \times \frac{w}{W}$个网格，在每个网格内进行池化操作，得到$\frac{h}{H} \times \frac{w}{W}$的向量。分类与边界框回归为两个单独的全连接层网络，而不是采用SVM，分类器输出维数为$C+1$($C$种类别加背景)，回归器输出维数为$4 \times C$(各类别分别对应矫正参数)。值得注意的是，该结构特征提取、区域分类与边界框回归这几个步骤可通过端到端的方式优化， 缺点候选框仍使用传统方法生成，如选择性搜索(Selective Search)或边缘框(Edge Boxes)。 3.2.1.4. Faster R-CNN 细节提出新的候选框生成方法：Region Proposal Network(PRN)，为全卷积神经网络，接受任意大小的图像。在检测时，用大小为$n \times n$的滑动窗口在特征图上滑动，每个位置提取特征，送入分类层(cls)与回归层(reg)，最终结果用于确定候选框。可将RPN插入到Fast R-CNN中从而以端到端的方式进行优化。 缺点尽管Faster R-CNN在提取特征图时共享权值，但对于后续每个ROI内的特征图对应的向量，仍需要单独通过全连接计算。 3.2.1.5. R-FCN 细节提出Positive-Sensitive ROP Pooling(PSROI Polling)，生成名为Position-Sensitive Score Map的特征图，保持了空间信息(to extract spatial-aware region features by encoding each relative position of the target regions)。如对于Stage 1生成的特征图，设其尺寸为$h \times w \times c$，用$k^2(C+1)$个$1 \times 1$的卷积核，即$k^2(C+1) \times 1 \times 1 \times c$进行运算，得到$h \times w \times k^2(C+1)$的Position-Sensitive Score Map。 当取$k=3$时，表示将每个ROI划分为$3 \times 3$，在特征图平面位置为$(x, y)$处，可切片得到向量$f_{3^2 \times (C+1)}$，对于所属类别$C_j$的部分，又有$f^{(j)}_{3^2}$，表示该ROI内，左上、上、右上、左、中、右、左下、下、右下$9$个位置处，所属类别$C_j$的概率。Pooling 操作同Fast R-CNN与Faster R-CNN。 3.2.1.6. FPN, MNC, Mask R-CNN, Mask Scoring R-CNN, …略。 3.2.2. One-stage Detectors该类检测方法没有设置单独的生成候选框的阶段，通常将图像上所有位置视为潜在对象，并尝试将每个感兴趣区域分类。 3.2.2.1. YOLOv1 YOLOv1接受$448 \times 448$大小的$3$通道图像输入，得到尺寸为$7 \times 7 \times ((4 + 1) \times n_{boxes} + n_{classes})$，$n_{boxes}$表示该网格预测输出几个回归框(YOLOv1中设置为2)，每个回归框对应$5$维特征，即$(t_x, t_y, t_w, t_h, t_c)$。可达到45FPS，或者精简网络下155FPS。但存在一些缺点 每个网格处，仅能预测两个物体，对于体型较小的或是聚集的物体，难以识别； 仅通过最后一个特征图来进行预测，没有考虑多尺度和不同横比。 3.2.2.2. SSD SSD将各层特征图均用于预测，分成固定数目的网格，每个网格设置一定尺度和纵横比的anchor。例如对于某层大小为$H \times W \times C$的特征图，经过卷积核$[n_{anchors} \times (n_{classes} + 4)] \times 3 \times 3 \times C$，得到$H \times W \times [n_{anchors} \times (n_{classes} + 4)]$的特征图，此特征图用于预测。 3.2.2.3. RetinaNet 使用Focal Loss解决了不同类别样本数目不均衡问题，此外使用Feature Pyramid Network来检测多尺度物体。 3.2.2.4 CornerNet 该网络为anchor-free类，改变以往寻找anchor框内物体的思路，而预测回归框的关键点。它将物体检测为一对角，在特征图的每个位置上，预测了类热图(Classification Heatmaps)。 3.3. Backbone Architecture采用大规模图像分类的预训练模型已经成为大多物体检测网络的默认策略。但直接使用分类模型是次优的 分类需要更大的感受野，并希望保持空间不变性，故应用多个下采样操作以降低特征的映射分辨率，生成的特征图是低分辨率且空间不变的。但是定位任务需要较高的空间信息； 分类使用单个特征图即可，而检测任务考虑到多尺度问题，需要在多个尺寸的特征图上进行预测。 3.3.1. Basic Architecture of a CNN深度卷积神经网络通常由一系列的卷积层、池化层、非线性激活层和全连接层组成。 卷积层：卷积层输入输出图像可视作通道数为$C_1, C_2$的图像，卷积核参数为$C_2 \times k \times k \times C_1$；每个特征图上像素点对应原图中的大小称为感受野。 池化岑：用于扩大感受野并降低计算成本，在一定程度上增加了对图像旋转等抗干扰性。 非线性激活层：用于添加非线性信息，若无非线性层，网络再深也是线性变化。 全连接层：一般设置一系列卷积操作后，最后输出时，添加全连接层“兜底”。 3.3.2. CNN Backbone for Object Detection VGG16 包括$2 + 2 + 3 + 3 + 3$层卷积层和$3$层全连接层，每组卷积层间为最大池化层。网络层数的增加可增大网络的容量，但是超过20层时，难以使用SGD进行梯度下降反传。 ResNet 引入shortcut connection，一定程度上解决了梯度消失问题，可使网络堆叠得更深，即 x_{l+1} = x_l + f_{l+1}(x_l, \theta) ResNet-v2 ResNet-v2增加Batch Normalization层。尽管shortcut解决了训练问题，但它没有充分利用前层特征图。底层特征在逐元素操作中逐渐丢失，因此提出DenseNet，通过通道组合得方法合并特征，并且层间密集连接。 x_{l+1} = x_l \circ f_{l+1}(x_l, \theta) Dual Path Network Dual Path Network结合两者的特点，某一层的特征图通道可分为密集连接部分和逐元素相加元素，即$C=C^d + C^r$ x_{l+1} = (x^r_l + f^r_{l+1}(x^r_l, \theta^r)) \circ (x^d_l \circ f^d_{l+1}(x^d_l, \theta^d)) ResNeXt 采用分组分离卷积的方法，大大减少了参数量与计算量。 GoogLeNet 除了增加网络深度，还通过增加支路来扩大网络容量。 3.4. Proposal GenerationOne-stage Detectors与Two-stage Detectors都产生了候选框，区别是前者在特征图对应的每个位置生成指定大小的特征图，而后者仅产生前景或背景信息的候选框，得到的结果较为稀疏。 3.4.1. Traditional Computer Vision Methods略。 3.4.2. Anchor-based Methodsanchor通过人为指定的方式 根据不同特征图的感受野设置锚框(SSD)； 为检测细小物体，可增大图像尺寸和减少锚框stride的方式； 基于RPN，分解锚框的维度(DeRPN)； 用K-Means确定(YOLOv2)； 先指定锚框，再通过训练结果矫正锚框(RefineNet)； 3.4.3. Keypoints-based Methods略。暂时没有接触过，理解不深，后续可能补上(TODO:)。 3.4.4. Other Methods略。 3.5. Feature Representation Learning3.5.1. Multi-scale Feature Learning如Fast R-CNN与Faster R-CNN，仅通过单个特征图预测输出，这对应多尺度和多个纵横比的情况下具有极大难度。 底层特征图比高层特征图具有更高的分辨率与更小的感受野，因此更适合于检测小物体。而深层的特征图具有更多的语义信息，对光照、偏移等鲁棒性更好，并具有更大的感受野，更适用于检测大体系物体。 有四种解决多尺度特征学习问题的方法 Image Pyramid通过将图像缩放为不同大小的方式，单独训练适应尺寸的网络，测试时将图片缩放为相应尺寸输入到不同的网络中，计算量较大。但是Singh等人提出，学习单个网络以适应不同尺寸比学习多个适应不同尺寸的网络更困难(SNIP)。 Integrated Features通过组合多个层的特征图，以新构造的特征图来预测输出，常用的方法如跳跃连接(skip connection)。又如ION，通过ROI Pooling剪裁来自不同层的区域特征，将其组合后作为特征。又如HyperNet，用反卷积的方式，通过集成浅层及中间层的特征图，生成新的高分辨率特征用以输出候选框。又如Multi-scale Location-aware Kernel Representation (MLKP)，捕获高阶统计量，生成更多的判别特征表示，更具描述性，为分类和定位提供语义及空间信息。 Prediction Pyramid如SSD，每一层特征图均用于预测输出，每个层设置一定比例的对象。Multi-Scale Deep Convolutional Neural Network(MSCNN)通过反卷积将特征图分辨率增大，用这些特征图预测输出。Reception Field Block Net(RFBNet)设置多分支，每个分支设置不同尺寸的卷积核，从而获得多尺度、不同感受野的特征图，从而用于预测输出。 Feature Pyramid结合Integrated Features与Prediction Pyramid的优点。如Feature Pyramid Network(FPN)自上而下将不同尺度的特征通过逐元素相加或者组合的方式，利用深层特征丰富浅层特征的语义信息。 3.5.2. Region Feature Encoding R-CNN ROI Pooling Layer ROI Warping Layer ROI Align Layer Precise ROI Pooling Layer Position Sensitive ROI Pooling Layer Feature Selective Network CoupleNet Deformable ROI Pooling Layer 3.5.3. Contextual Reasoning由于物体的出现与环境相关，且需要与其他物体进行交互，故上下文信息十分重要。深度卷积网络隐式地从分层特征表示中捕获上下文信息。 全局上下文从整个图像地上下文中学习，利用图像其余部分信息，来对感兴趣区域进行分类。 循环神经网络编码整个图像的四个方向信息(ION)； 学习类别分数，用于作为与检测结果连接的上下文特征(Ouyang et al.); 从整个图像提取嵌入信息，将其与局部特征组合用以改善检测结果(He et al.)； 基于语义分割的方法； 将目标检测和语义分割作为多任务进行优化(He et al. and Dai et al.)； 伪分段语义标注(Zhao et al.)； 通过学习的方式获得语义特征图(Detection with Enriched Semantics)； 局部上下文对周围区域上下文进行编码，并学习对象与周围区域的交互，直接学习不同位置和带有上下文信息的类别是很困难的。 Directly modeling different locations and categories objects relations with the contextual is very challenging. Chen et al. proposed Spatial Memory Network (SMN) [130] which introduced a spatial memory based module. The spatial memory module captured instance-level contexts by assembling object instances back into a pseudo ”image” representations which were later used for object relations reasoning. Liu et al. proposed Structure Inference Net (SIN) [137] which formulated object detection as a graph inference problem by considering scene contextual information and object relationships. In SIN, each object was treated as a graph node and the relationship between different objects were regarded as graph edges. Hu et al. [138] proposed a lightweight framework relation network which formulated the interaction between different objects between their appearance and image locations. The new proposed framework did not need additional annotation and showed improvements in object detection performance. Based on Hu et al., Gu et al. [139] proposed a fully learnable object detector which proposed a general viewpoint that unified existing region feature extraction methods. Their proposed method removed heuristic choices in ROI pooling methods and automatically select the most significant parts, including contexts beyond proposals. Another method to encode contextual information is to implicitly encode region features by adding image features surrounding region proposals and a large number of approaches have been proposed based on this idea [131, 106, 140, 141, 142, 143]. In addition to encode features from region proposals, Gidaris et al. [131] extracted features from a number of different sub-regions of the original object proposals(border regions, central regions, contextual regions etc.) and concatenated these features with the original region features. Similar to their method, [106] extracted local contexts by enlarging the proposal window size and concatenating these features with the original ones. Zeng et al. [142] proposed Gated Bi-Directional CNN (GBDNet) which extracted features from multi-scale subregions.Notably, GBDNet learned a gated function to control the transmission of different region information because not all contextual information is helpful for detection. 3.5.4. Deformable Feature Learning检测器应对图像中物体的非刚性变形具有鲁棒性。传统方式有Deformable Part based Models(DPMs)，使用可变形编码方法由多个组成表示对象。深度学习方法如DeepIDNet，开发Deformable-aware Pooling Layer;Deformable Convolutional Layers自动学习辅助位置，以增强常规采样的信息。 4. Learning Strategy4.1. Training Stage4.1.1. Data Augmentation为解决数据量少的问题，需要进行数据扩增。在物体检测中，常用方法有：水平翻转，旋转、随机裁剪、延申、颜色抖动(亮度，对比度，饱和度和色度)。注意图像变换后，需要对标签也作对应变换。 4.1.2. Imbalance Sampling候选框内图像大多数都是背景，而不是物体，有以下两个问题：1) 类别不平衡(class imbalance)，由于只有一小部分候选框内内容为物体，故负样本占大多数，导致梯度反传时负样本占主导。2) 困难不平衡(difficulty imbalance)，类似第一点，检测器更易分辨背景，难以分辨物体。 一些二阶段检测器，如R-CNN与Fast R-CNN会先拒绝大部分负样本，Fast R-CNN从$2000$个候选框中随机采样，直到在某批次的数据中，正负样本比例达到$1:3$。随机采样能解决类别不平衡问题，但是丢失的负样本可能包含丰富的语义信息。为解决这个问题，刘等人提出困难负样本策略(hard negative sampli Strategy)，主要保留难以判别为负的负样本，具体地说来，就是选取损失值较大的负样本。 为了解决困难不平衡问题，大部分都是通过合理设置损失函数。对于标检测来说，多类别分类器需要分辨$C+1$个类别，即$C$类物体加背景。假定某区域真实标签为$u$，$p$是网络输出的$C+1$个类别的概率分布($p = {p_0, \cdots, p_C}$)，那么损失定义为 L_{cls}(p, u) = - \log p_u现阶段有一种改进的交叉熵损失：Focal Loss，即 L_{FL} = - \alpha (1 - p_u)^{\gamma} \log p_u其中参数$\alpha$与$\gamma$为超参数，该损失函数可根据网络输出的$p_u$计算权值，错误分类样本$p_u$更低，从而使权重更大，可以更多地关注误分类样本。梯度协调机制(gradient harmonizing mechanism - GHM)采用类似的思路，不仅抑制了易分类负样本，并避免了异常负样本的影响。此外，还有如在线困难样本挖掘策略(online hard example mining strategy)，自动选取困难样本用以训练，仅关注样本的困难度而不关注类别信息，即单批正负样本比例没有被考虑，他们认为，对于检测问题，样本困难度比类别不平衡更加重要。 4.1.3. Localization Refinement物体检测算法需要提供一个包含物体的最小矩形框，但精确定位困难，因为预测通常集中在物体更具分辨性特征的部位，而不一定是包含物体的区域。在一定场景下，需要进行高质量预测，可用IoU作为指标。一种解决方法是生成高质量的候选框，以下介绍一些其他方法。在R-CNN中使用L2惩罚作为损失，在Fast R-CNN中使用平滑L1惩罚作为损失，即 L_{reg}(t^c, v) = \sum_{i \in \{x, y, w, h\}} \text{SmoothL1}(t^c_i - v_i)\text{SmoothL1}(x) = \begin{cases} 0.5x^2 & \text{if} |x| < 1 \\ |x| - 0.5 & \end{cases}其中每类均具有回归的偏置，即$t^c = (t^c_x, t^c_y, t^c_w, t^c_h)$，真实边界框位置为$v=(v_x, v_y, v_w, v_h)$。 基于定位校准，一些方法采用辅助模型用于更好地矫正坐标。如Gidaris等人引入一种迭代地边框回归方法，使用R-CNN反复迭代候选框内容，多次矫正候选框；另外，提出LocNet，将每个边界框的分布进行建模。这些方法都需要单独模块，不能进行联合调优。 一些其他的，侧重于设计带有修改的目标函数(modified objective function)的统一网络框架。在多路网络(Multi-Path Network)中，采用一系列的分类器，这些分类器是通过不同的指标，以整体损失进行优化的，每个分类器都适应对应的IoU阈值，所有输出组成最终的预测结果。Fitness NMS学习了一种新式的IoU计算方法，他们认为现在的检测器旨在找到合格的预测，而不是最优的预测，因此高质量、低质量的候选有同等的重要性。Fitne-IoU更看重高度重合的候选框。他们也采用了基于一组IoU上限，来到处边界框的回归损失，以最大化具有对象的预测的IoU(They also derived a bounding box regression loss based on a set of IoU upper bounds to maximum the IoU of predictions with objects)。Grid R-CNN参照CornerNet与DeNet，用角点定位的机制取代边界框的线性回归。 4.1.4. Cascade Learning级联学习是一种粗到细的学习策略，从给定分类器的输出中收集信息，以级联的方式构建更强的分类器，首次被应用于训练人脸检测器。在深度学习算法方面，Cascade Region-proposal-network And FasT-rcnn(CRAFT)学习PRN以及带级联策略的分类器，首先学习一个标准的RPN网络，之后用二分类的Fast R-CNN拒绝一些容易判别的错误样本，剩余样本用来构建包含两个Fast R-CNN网络的级联区域分类器。Yang等人引入层级级联分类器，用于多尺度的物体检测，不同层的特征图设置分类器，浅层的分类器将拒绝易分辨的错误样本，剩余样本将送入更深的网络层。RefineDet和Cascade R-CNN将级联用于回归框的矫正，构建多阶段的边界框回归器，边界框输出将在各个阶段进行矫正，并且这些回归器是通过不同quality的指标训练得到的。Cheng等人通过观察Faster R-CNN的误检图片，注意到即使回归器定位准确，样本分类却存在错误，他们将此归结于特征共享(sharing of features)和联合的多任务优化(joint multi-task optimization)导致的次优特征表示(sub-optimal feature representation)；此外，他们认为Faster R-CNN的较大感受野引入了较多噪声。他们建立了一个基于Faster R-CNN与R-CNN的级联检测系统以互补，即用训练好的Faster R-CNN得到一些初始的预测，这些结果用以训练R-CNN。 4.1.5. Others有一些其他的学习策略 Adversarial Learning 对抗性学习显示了生成模型的重要性，最主要的应用是生成对抗网络(GAN)。生成器对数据分布进行建模，根据给定的噪声输入，得到一副假的图片，送入鉴别器判断该图片是否为真。GAN在许多领域显示其有效性，在目标检测方面也不例外。Li等人提出的Perceptual GAN可用于细小物体检测，通过对抗过程，生成器将学习细小物体的高分辨率特征表征。A-Fast-R-CNN通过生成的对抗样本训练，他们认为困难样本是在分布的长尾上，因此他们映入了两个新式模块，自动生成具有遮挡和变形的特征。 Training from Scratch 现在的检测器大多依赖于ImageNet与训练的分类模型，但是，由于分类和检测任务损失不同、数据分布不同，这可能会对结果造成负面影响。Finetuning可能解决这个问题，但是不能完全消除偏差。另外，将此分类模型用作新的领域可能不合适，如RGB图像到MRI数据。所以有必要从空白开始训练，这样的困难主要是物体检测训练数据过少，会造成过拟合。与图像分类不同，物体检测需要边界框级别的标注，这是非常耗时耗力的(ImageNet有1000个类别，但只有200类包含检测标注)。 Deeply Supervised Object Detectors(DSOD)的设计者认为，密集连接的网络结构用于监督学习可大大减少优化难度。基于DSOD，Shen等人提出了一种门控循环特征金字塔结构，动态调整中间层的监督强度以自适应不同尺度的物体，将空间和语义信息压缩到单个预测特征图，进一步减少了参数量从而加快了收敛速度。此外，门控特征金字塔结构可根据物体大小适应不同强度的监督，这种方法比原始DSOD更有效。但是随后He等人用从头开始训练的检测器在MSCOCO数据机上进行验证，发现vanilla detector可以获得等同于$10K$标注数据训练得到的检测能力，这表明从头开始训练的模型并不依赖特定结构，这与之前的工作相矛盾。 Knowledge Distillation 通过师生教学方案(teacher-student training scheme)，将多个模型集成为一个模型。Li等人提出轻量级检测器，它的训练通过重量级有效的检测器指导进行训练，该检测器速度更快，精准度与后者相近。Cheng等人提出了一种基于RCNN的快速探测器，优化时，R-CNN模型作为教师网络来指导培训过程，与传统的单模型优化策略相比，他们的框架检测精度提高。 4.2. Testing Stage目标检测算法生成了众多预测结果，但是由于大量的冗余性，输出结果不能直接使用。因此有其他策略提升检测准确率，或加速推断过程。 4.2.1. Duplicate Removal Non-maximum suppression(NMS)是物体检测算法的一个组成部分，用于消除重复的假阳(false positive)预测，如上图。对于一阶段检测器来说，同一物体周围的预测边界框，可能具有相同的置信度，导致较高的假阳率；而二阶段检测算法产生较为稀疏的候选框，并且回归器会将这些候选框拉向相近的位置，同样会导致较高的假阳率。 具体说来，对于指定的一类物体，其预测框将按照置信度排序，选择当前置信度最高的候选框，记作$M$，然后计算该框与其余框的IoU，如果IoU值大于某阈值$\Omega_{test}$，该框将被从候选框中删除，即 \text{Score}_B = \begin{cases} \text{Score}_B & \text{IoU}(B, M) < \Omega_{test} \\ 0 & \text{otherwise} \end{cases}但是对于聚集的物体，NMS算法将会把同类物体，靠近的物体框删除，这导致回归框预测确实，因此Navaneeth等人提出Soft-NMS算法，使用指定函数将靠近的框的置信度降低，而不是直接置为0 \text{Score}_B = \begin{cases} \text{Score}_B & \text{IoU}(B, M) < \Omega_{test} \\ \text{F}(\text{IoU}(B, M)) & \text{otherwise} \end{cases} 详细的实现代码可查看该文 此外，Hosong等人设计了一种网络结构，基于置信度和回归框来改进NMS，这是独立于检测器单独进行有监督训练的。他们认为，重复预测的原因是，检测器故意鼓励每个物体进行多次高分检测而不是奖励单个高分。因此他们根据两个动机来进行网络的设计：1)设置损失来惩罚二次检测，使得每个物体准确预测单个精确预测；2)处理附近的检测输出，给检测器提供是否物体已被多次检测的信息。新提出的模型没有舍弃检测结果，而是将NMS用作重估，降低重复检测结果的评分。 4.2.2. Model Acceleration目标检测应用需要较高的实时性，因此需要用指标评估检测器的速度。虽然当前state-of-art算法可以在公开数据集上得到一个较高的检测评价，但是他们的速度限制了他应用在实时场景中。通常来说，二阶段检测器比一阶段检测器速度更慢，因为候选框生成器与分类器单独运算，使得运算量较大。尽管R-FCN提出空间敏感的特征图，通过position-sensitive ROI Pooling来共享计算，但是随着物体种类增多，通道数也随之线性增加。 从检测器的主体结构可以看到，占计算量最多的主要是网络模型部分，因此一个简单的解决方法是用高效率的主体，比如MobileNet，通过分离卷积的方式，大大减少了参数量与计算量。PVANet是一种新的网络结构，他用到了CReLU，较少了非线性计算。 \text{CReLU}(x) = [\text{ReLU}(x), \text{ReLU}(-x)]另一种方法是，将模型进行线下优化，比如模型压缩。最后呢，如NVIDIA开发了一个加速工具TensorRT，在模型部署上进行优化，以加运算速度。 4.2.3. Others其他方法，如将输入图像进行变换，以提高检测精度。图像金字塔是将待检测图像，以一定的比例生成一系列尺寸的图像，在每个图像上进行检测，最终结果合并于每张检测结果。Zhang等人，将图像缩放到指定尺寸，在某一特定尺寸下，仅检测某一指定大小的物体。水平翻转也可用于测试阶段。这些方法都可以增加准确率但是计算量也随之增加。 5. Applications5.1. Face Detection人脸检测是一个经典的计算机视觉问题，通常是人脸验证、对其、识别等算法的第一步。然而，人脸检测问题与通用检测之间存在关键差异：1)人脸检测中，目标的比例范围远大于通用物体；2)脸部对象包括强结构信息，并且仅有一类物体。考虑到这些特效，需要有一些先验来改进检测算法。 5.2. Pedestrian Detection行人检测是一个关键的重要任务。和通用检测不同，1)结构良好，具有几乎固定的纵横比，但也存在于较大范围内；2)行人检测是真实应用场景，有许多挑战：人群拥挤、遮挡、图像模糊等。 5.3. Others其他应用如Logo检测和视频目标检测。 6. Detection Benchmarks 6.1. Generic Detection Benchmarks Pascal VOC2007 Pascal VOC2012 MSCOCO Open Images LVIS ImageNet 6.2. Face Detection Benchmarks WIDER FACE FDDB PASCAL FACE 6.3. Pedestrian Detection Benchmarks CityPersons Caltech ETH INRIA KITTI 7. State-of-the-art for Generic Object Detection 8. Concluding Remarks and Future Directions目标检测还存在许多挑战和未来的发展方向 候选框生成策略 如3.4.节中所述，当前许多检测器是基于anchor的，这些anchor主要是手动设计的，难以匹配多尺度物体，基于IoU的匹配策略也是启发式的。而对于无anchor算法，具有大的改进空间，如计算成本告等。当前无锚框算法是一个热点。 有效的上下文信息编码 上下文信息对物体检测十分重要，但是目前所作的工作对上下文信息的使用比较局限。 基于AutoML的检测方法 给一个特定任务设计合适的网络结构是很重要的，但是也会消耗大量时间和人力。当前一个比较有趣和重要的研究方向是，通过学习的方法，自动设计网络结构。可通过AutoML方法进行网络结构的探索，但是这种算法需要大量的计算资源(more than 100 GPU cards to train a single model)。 目标检测新的数据集 当前MSCOCO是目标检测最常用的数据库，但是它只包含80类物体，在真实世界中，这是远远不够的。最近有一个数据集LVIS旨在收集更多类别物体的图像数据，它包含超过1000个类别的16400张图片，其中还有许多高质量的语义分割标注。此外，它模拟真实世界的场景，其中存在大量类别但是有些类别数据很少。 Low-shot目标检测 有限标记数据训练得到的模型，称作Low-shot。对于边界框级别的图像标注非常耗时耗力，现在有一些通过半监督学习的方法，来减少数据的使用。 检测任务的骨干架构 当前检测器很多都基于分类模型。 其余研究问题 large batch learning和增量学习(incremental learning)等。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Normalization]]></title>
    <url>%2F2019%2F08%2F27%2FBatch-Normalization%2F</url>
    <content type="text"><![CDATA[前言深度学习网络可通过mini-batch随机梯度下降的方式进行优化，然而在优化过程中，存在梯度消失、收敛不稳定的问题。 原理机器学习中，假定训练数据与测试数据独立同分布(independent and identical independent)，当输入数据进行白化操作后，可使模型收敛速度加快，这被称作Covariate Shift。 而在神经网络内部，各层的特征图数据分布不均匀，称为Internal Covariate Shift，有以下几点需求需要进行内部白化 为防止网络发散，网络参数初始化一般服从均值为$0$的正态分布； 激活函数敏感区域一般定义在零点位置，如logistic函数，在零点出该处梯度最大，而其他位置容易出现梯度消失问题； 内部白化后，输出数据近似服从标准正态分布。一般用于卷积层后、激活函数前。 实现Batch Normalization层通常包含参数mean, variance, rolling_mean, rolling_variance，其中mean与variance为训练时根据批次数据计算得到，在训练阶段使用以归一化批次数据；rolling_mean与rolling_variance为该层实际学习的参数，在测试阶段使用以归一化批次数据。 对于某批次输入的数据$X_{\mathcal{B}}$，尺寸为$(N, H, W, C)$，计算该批次数据每个维度上的均值mean与方差variance，尺寸均为$(H, W, C)$，即 \mu = \frac{1}{N} \sum_{i=1}^N X^{(i)}\sigma^2 = \frac{1}{N} \sum_{i=1}^N (X^{(i)} - \mu)^2更新参数rolling_mean与rolling_variance，以动量方式更新，可取$\gamma=0.01$ \tilde{\mu} := \gamma \cdot \mu + (1 - \gamma) \cdot \tilde{\mu}\tilde{\sigma^2} := \gamma \cdot \sigma^2 + (1 - \gamma) \cdot \tilde{\sigma^2}值得注意的是，在训练阶段前向计算时 \hat{X^{(i)}} = \frac{X^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}而测试阶段 \hat{X^{(i)}} = \frac{X^{(i)} - \tilde{\mu}}{\sqrt{\tilde{\sigma^2} + \epsilon}}以下为darknet/src/batchnorm_layer.c前向与反向传播计算的具体实现 1234567891011121314151617181920212223242526272829303132333435363738void forward_batchnorm_layer(layer l, network net)&#123; if(l.type == BATCHNORM) copy_cpu(l.outputs*l.batch, net.input, 1, l.output, 1); copy_cpu(l.outputs*l.batch, l.output, 1, l.x, 1); if(net.train)&#123; mean_cpu(l.output, l.batch, l.out_c, l.out_h*l.out_w, l.mean); variance_cpu(l.output, l.mean, l.batch, l.out_c, l.out_h*l.out_w, l.variance); scal_cpu(l.out_c, .99, l.rolling_mean, 1); axpy_cpu(l.out_c, .01, l.mean, 1, l.rolling_mean, 1); scal_cpu(l.out_c, .99, l.rolling_variance, 1); axpy_cpu(l.out_c, .01, l.variance, 1, l.rolling_variance, 1); normalize_cpu(l.output, l.mean, l.variance, l.batch, l.out_c, l.out_h*l.out_w); copy_cpu(l.outputs*l.batch, l.output, 1, l.x_norm, 1); &#125; else &#123; normalize_cpu(l.output, l.rolling_mean, l.rolling_variance, l.batch, l.out_c, l.out_h*l.out_w); &#125; scale_bias(l.output, l.scales, l.batch, l.out_c, l.out_h*l.out_w); add_bias(l.output, l.biases, l.batch, l.out_c, l.out_h*l.out_w);&#125;void backward_batchnorm_layer(layer l, network net)&#123; if(!net.train)&#123; l.mean = l.rolling_mean; l.variance = l.rolling_variance; &#125; backward_bias(l.bias_updates, l.delta, l.batch, l.out_c, l.out_w*l.out_h); backward_scale_cpu(l.x_norm, l.delta, l.batch, l.out_c, l.out_w*l.out_h, l.scale_updates); scale_bias(l.delta, l.scales, l.batch, l.out_c, l.out_h*l.out_w); mean_delta_cpu(l.delta, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.mean_delta); variance_delta_cpu(l.x, l.delta, l.mean, l.variance, l.batch, l.out_c, l.out_w*l.out_h, l.variance_delta); normalize_delta_cpu(l.x, l.mean, l.variance, l.mean_delta, l.variance_delta, l.batch, l.out_c, l.out_w*l.out_h, l.delta); if(l.type == BATCHNORM) copy_cpu(l.outputs*l.batch, l.delta, 1, net.delta, 1);&#125; Reference Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - arXiv.org How Does Batch Normalization Help Optimization? - arXiv.org darknet/src/batchnorm_layer.c - Github]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Module Layer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eigenface and Fisherface]]></title>
    <url>%2F2019%2F08%2F20%2FEigenface-and-Fisherface%2F</url>
    <content type="text"><![CDATA[前言在众多人脸图像中，能否找到特一组特征脸，用于表征其他人脸呢？在PCA和LDA中分别介绍了两种线性降维方法，本文介绍一种使用以上两种算法的特征提取方法。 原理EigenfaceEigenface由Sirovich与Kirby在1987年提出，认为人脸图像可由一系列特征图加权组合重构而成，即 F = F_m + \sum_i w_i F_i \tag{1}Eigenface可通过PCA生成，在大量的人脸数据图像上进行分析，选取主分量作为特征脸，详细步骤如下 假设有图像尺寸为$(H, W)$的人脸数据，共$N$张。将单张的灰度图片展开成为维数为$H \times W$的向量$x^{(i)}$，构成数据矩阵$X$ X_{(H \times W) \times N} = \begin{bmatrix} x^{(1)} & x^{(2)} & \ldots & x^{(N)} \end{bmatrix} \tag{2.1} 计算各样本向量的均值$\mu$，将各样本去均值化，生成数据矩阵$\overline{X}$ \mu = \frac{1}{N} \sum_{i=1}^{N} x^{(i)}; \quad \overline{x}^{(i)} = x^{(i)} - \mu \tag{2.2} 计算协方差矩阵$C$ C = \frac{1}{N} \overline{X} \cdot \overline{X}^T \tag{2.3} 将协方差矩阵进行特征值分解 C \alpha_i = \lambda_i \alpha_i \tag{2.4} 注意到，在图像尺寸为$(H, W)$的情况下，协方差矩阵的尺寸为$(H \times W, H \times W)$，本文使用数据库内图像为$112 \times 92$，存储为浮点类型(4 byte)，也就是说，该协方差矩阵所占内存 (112 \times 92)^2 \text{pixel} \times 4 \text{byte/pixel} = 441 \rm{kb} \text{(阵亡。。。)}因此，需要转换一下求解问题，查看Eigenface求解。 按特征值降序，重新排列特征对$(\lambda_i, \alpha_i)$，选取前$K$个特征向量作为Eigenface E_{(H \times W) \times K} = \begin{bmatrix} \alpha_1 & \alpha_2 & \ldots & \alpha_K \end{bmatrix} \tag{2.5} 此时将人脸数据$x$投影到各主分量上，可获得相应系数，该系数向量可用于表征该人脸的特征，即 \vec{w} = \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix} \tag{3}其中$w_i = x^T \alpha_i$。 将各主轴恢复原图像尺寸后，其可视化输出如下 FisherFaceFisherface基本思路与Eigenface一致，也是寻找一组特征脸，用于表征人脸特征。可通过LDA生成，LDA考虑类内与类间散布，与PCA不同，为有监督学习。详细步骤如下 同样的，获取数据矩阵$X$ X_{(H \times W) \times N} = \begin{bmatrix} x^{(1)} & x^{(2)} & \ldots & x^{(N)} \end{bmatrix} \tag{4.1} 计算所有数据的均值向量$\mu$与各类别的均值向量$\mu_j$ \mu = \frac{1}{N} \sum_{i=1}^{N} x^{(i)}; \quad \mu_j = \frac{1}{N_j} \sum_{i=1}^{N_j} x^{(i)}, x^{(i)} \in C_j \tag{4.2} 计算类内离散度矩阵$S_W$与类间离散度矩阵$S_B$ \begin{aligned} S_W = \sum_{j=1}^{C} \frac{N_j}{N} \left[ \frac{1}{N_j} \sum_{i=1}^{N_j} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T \right] \\ = \frac{1}{N} \sum_{j=1}^{C} \sum_{i=1}^{N_j} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T; \quad x^{(i)} \in C_j \end{aligned} \tag{4.3}S_B = \sum_{j=1}^{C} \frac{N_j}{N} (\mu_j - \mu) (\mu_j - \mu)^T \tag{4.4} 求解广义特征值问题 S_B \alpha_i = \lambda_i S_W \alpha_i \tag{4.5} 通常该问题可通过分解矩阵$S_W^{-1} S_B$进行求解 S_W^{-1} S_B \alpha_i = \lambda_i \alpha_i但由于函数numpy.linalg.eig(a)问题，在求解$S_W^{-1} S_B$特征对时出现复数。更加令人疑惑的是，作为实对称矩阵$S_W^{-1}$，其特征对用该函数求解时，也会出现复数。实际上，numpy提供了函数numpy.linalg.eigh(a)专门求解实对称矩阵或Hermite矩阵的特征对，故需要对该特征求解进行一定处理，查看广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$。性质1： 实对称矩阵(满足$A^T = A$)的特征值都是实数。性质2： 实对称矩阵(满足$A^T = A$)属于不同特征值的特征向量正交。 按特征值降序，重新排列特征对$(\lambda_i, \alpha_i)$，选取前$K$个特征向量作为FisherFace F_{(H \times W) \times K} = \begin{bmatrix} \alpha_1 & \alpha_2 & \ldots & \alpha_K \end{bmatrix} \tag{4.6} 类似的，将人脸数据$x$投影到各主分量上，可获得相应系数，该系数向量可用于表征该人脸的特征，即 \vec{w} = \begin{bmatrix} w_1 & w_2 & \cdots & w_K \end{bmatrix} \tag{5}其中$w_i = x^T \alpha_i$。 将各主轴恢复原图像尺寸后，其可视化输出如下 实际上，若输入的数据矩阵$X$不做降维处理，计算得到矩阵$S_W$与$S_B$尺寸为$(H \times W) \times (H \times W)$，也是一个令人头疼的计算量问题。而与上面不同，这是无法避免的。所以考虑到这一点，需要对原始数据进行降维，可利用PCA \tilde{X}_{D \times N} = \text{pca}(X_{(H \times W) \times N}) \tag{6.1}对矩阵$\tilde{X}$进行LDA计算后，得到Fisherface序列${\alpha_1, \alpha_2, \ldots}$，其中$\alpha_i$维度为$D \times 1$，若需可视化结果，利用计算得到的PCA模型将其重建即可 A_i = \text{pca}^{-1}(\alpha_i) \tag{6.2}技巧Eigenface求解矩阵$\overline{X}$可经$SVD$分解为 \overline{X} = U \Sigma V^T \tag{7}对于协方差矩阵$C_{(H \times W) \times (H \times W)} = \frac{1}{N} \overline{X} \cdot \overline{X}^T$，对其进行特征值分解如下 \overline{X} \cdot \overline{X}^T u_i = \lambda_i u_i \tag{8}实际上，$(8)$为求解矩阵$\overline{X}$左奇异向量$u_i, i = 1, 2, \ldots, H \times W$的过程，然而其计算量过大，考虑矩阵$\overline{X}$右奇异向量$v_i, i = 1, 2, \ldots, N$。 \overline{X}^T \overline{X} v_i = \lambda_i v_i \tag{9}由$(7)$可得 \begin{aligned} \overline{X} V = U \Sigma & 或 & \overline{X} v_i = \sigma_i u_i \end{aligned}所以 \begin{aligned} u_i = \frac{1}{\sigma_i} \overline{X} v_i & 或 & U = \overline{X} V \Sigma^{-1} \end{aligned} \tag{10}其中$V$与$\Lambda$由式$(9)$已知，奇异值$\sigma_i = \sqrt{\lambda_i}$，故 \begin{aligned} u_i = \frac{1}{\sqrt{\lambda_i}} \overline{X} v_i & 或 & U = \overline{X} V \Lambda^{-\frac{1}{2}} \end{aligned} \tag{11}实际上，由于$\text{rank}(\overline{X}) \leq N$，故后$(H \times W) - N$个特征值均为$0$，对应特征向量无意义，不予求解。 广义特征值问题$S_B \alpha_i = \lambda_i S_W \alpha_i$由于函数numpy.linalg.eig(a)问题，在求解$S_W^{-1} S_B$特征对时出现复数。故作如下处理 $S_W$为实对称矩阵，故可使用函数numpy.linalg.eigh(a)解得其特征对，即 S_W = P \Lambda P^T \tag{12}代入$S_W^{-1} S_B \alpha_i = \lambda_i \alpha_i$并作相应变换 (P \Lambda P^T)^{-1} S_B \alpha_i = \lambda_i \alpha_i\underbrace{P \Lambda^{-\frac{1}{2}} \Lambda^{-\frac{1}{2}} P^T}_{S_W} \cdot S_B \cdot \underbrace{P \Lambda^{-\frac{1}{2}} \Lambda^{\frac{1}{2}} P^T}_I \cdot \alpha_i = \lambda_i \alpha_i\underbrace{\Lambda^{-\frac{1}{2}} P^T \cdot S_B \cdot P \Lambda^{-\frac{1}{2}}}_{A} \underbrace{\Lambda^{\frac{1}{2}} P^T \cdot \alpha_i}_{\beta_i} = \lambda_i \underbrace{\Lambda^{\frac{1}{2}} P^T \cdot \alpha_i}_{\beta_i} \tag{13}其中$A = \Lambda^{-\frac{1}{2}} P^T \cdot S_B \cdot P \Lambda^{-\frac{1}{2}}$也为对称矩阵，可由用函数numpy.linalg.eigh(a)解得其特征对$(\lambda_i, \beta_i)$，则 \beta_i = \Lambda^{\frac{1}{2}} P^T \cdot \alpha_i\alpha_i = P \Lambda^{-\frac{1}{2}} \beta_i \tag{14}实现及实验数据集实验中数据集选用ORL Face，包含40名人员的10份人脸数据，包括拍摄时间、光照、表情(睁/闭眼、笑/不笑)、面部细节(眼镜)的变化。图片保存为.pgm格式，可用OpenCV进行读取。 其数据预览如下 PCA12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class PCA(): """ Principal Components Analysis Attributes: components_: &#123;ndarray(n_components, n_features)&#125; means_: &#123;ndarray(n_components)&#125; """ def __init__(self, n_components): self.n_components = n_components self.components_ = None self.means_ = None def fit(self, X): ''' train the model Params: X: &#123;ndarray(n_samples, n_features)&#125; ''' n_samples, n_features = X.shape self.means_ = np.mean(X, axis=0) X_ = X - self.means_ eigval, eigvec = None, None if n_samples &lt; n_features: eigval, u = np.linalg.eig(X_.dot(X_.T)) eigvec = X_.T.dot(u).dot(np.diag(1 / eigval)) else: covar_ = X_.T.dot(X_) eigval, eigvec = np.linalg.eig(covar_) order = np.argsort(eigval)[::-1] eigval = eigval[order] eigvec = eigvec.T[order].T self.components_ = eigvec[:, :self.n_components].T def transform(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: X_:&#123;ndarray(n_samples, n_components)&#125; Notes: X'_&#123;nxk'&#125; · V_&#123;kxk'&#125;^T = X''_&#123;nxk&#125; """ X_ = X - self.means_ X_ = X_.dot(self.components_.T) return X_ def fit_transform(self, X): self.fit(X) return self.transform(X) def transform_inv(self, X): """ Params: X: &#123;ndarray(n_samples, n_components)&#125; Returns: X_:&#123;ndarray(n_samples, n_features)&#125; """ X_ = X.dot(self.components_) + self.means_ return X_ LDA12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394def eig(A1, A2): """ Params: A1, A2: &#123;ndarray(n, n)&#125; Returns: eigval: &#123;ndarray(n)&#125; eigvec: &#123;ndarray(n, n)&#125; Notes: A1 \alpha = \lambda A2 \alpha """ s, u = np.linalg.eigh(A2) s[s &lt;= 0] = np.finfo(float).eps s_sqrt = np.diag(np.sqrt(s)) s_sqrt_inv = np.linalg.inv(s_sqrt) A = s_sqrt_inv.dot(u.T).dot(A1).dot(u).dot(s_sqrt_inv) eigval, P = np.linalg.eigh(A) eigvec = u.dot(s_sqrt_inv).dot(P) return eigval, eigvecclass LDA(object): """ Attributes: n_components: &#123;int&#125; components_: &#123;ndarray(n_components, n_features)&#125; """ def __init__(self, n_components=-1): self.n_components = n_components self.components_ = None def fit(self, X, y): """ train the model Params: X: &#123;ndarray(n_samples, n_features)&#125; y: &#123;ndarray(n_samples)&#125; """ labels = list(set(list(y))) n_class = len(labels) n_samples, n_feats = X.shape S_W = np.zeros(shape=(n_feats, n_feats)) S_B = np.zeros(shape=(n_feats, n_feats)) mean_ = np.mean(X, axis=0) for i_class in range(n_class): X_ = X[y==labels[i_class]] means_ = np.mean(X_, axis=0) X_ = X_ - means_ means_ = (means_ - mean_).reshape(1, -1) S_W += (X_.T).dot(X_) * (1 / n_samples) S_B += (means_.T).dot(means_) * (X_.shape[0] / n_samples) eigval, eigvec = eig(S_B, S_W) order = np.argsort(eigval)[::-1] eigval = eigval[order] eigvec = eigvec[:, order] self.components_ = eigvec[:, :self.n_components].T def transform(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: X: &#123;ndarray(n_samples, n_components)&#125; """ X_ = X.dot(self.components_.T) return X_ def fit_transform(self, X, y): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: X: &#123;ndarray(n_samples, n_components)&#125; """ self.fit(X, y) X_ = self.transform(X) return X_ def transform_inv(self, X): """ Params: X: &#123;ndarray(n_samples, n_components)&#125; Returns: X: &#123;ndarray(n_samples, n_features)&#125; """ X_ = X.dot(self.components_) return X_ 结果12345678910111213141516def show_features(X, dsize, title="features"): """ Show features Params: X: &#123;ndarray(N, n_features)&#125; dsize: &#123;tuple(H, W)&#125; title: &#123;str&#125; """ SUBPLOT = "19&#123;&#125;" plt.figure(figsize=(18, 2)) plt.title(title) for i in range(9): plt.subplot(int(SUBPLOT.format(i+1))) plt.imshow(X[i].reshape(dsize), cmap="gray") plt.show() Eigenface 12pca = PCA(n_components=n_features); pca.fit(X)show_features(pca.components_, DSIZE, title="Eigenface &#123;&#125;".format(repr(DSIZE))) FisherFace 为减少计算量，将原始数据PCA降维后，再用于LDA计算FisherFace，故维度数目选取会影响实验结果，以下分别选择不同维数时产生的FisherFace序列，显示前9个特征脸。可见主分量数目越少，特征脸越清晰，但包含的细节也越少。 12345678910n_decomposed = n_samples - n_classes - 1pca = PCA(n_components=n_decomposed)X_decomposed = pca.fit_transform(X)lda = LDA(n_components=n_classes - 1)lda.fit(X_decomposed, y)components_ = pca.transform_inv(lda.components_)show_features(components_, DSIZE, title="Fisherface &#123;&#125; &#123;&#125;".format(repr(DSIZE), n_decomposed)) 199 279 359 Reference The Database of Faces 人脸识别之—-FisherFace - 学步园 Face Recognition with Python, by Philipp Wagner]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>特征提取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neighborhood Preserving Embedding]]></title>
    <url>%2F2019%2F08%2F12%2FNeighborhood-Preserving-Embedding%2F</url>
    <content type="text"><![CDATA[前言在Locally Linear Embedding一节中介绍了非线性降维方法LLE，原数据到低维数据没有指定映射方法，故不适用于新数据点。本文介绍的NPE是在其基础上的改进。 原理设有$M$个$N$维数据，构成矩阵$X$ X_{N \times M} = \left[ \begin{matrix} | & | & & | \\ \vec{x}^{(1)} & \vec{x}^{(2)} & \cdots & \vec{x}^{(M)} \\ | & | & & | \end{matrix} \right] \tag{1.1}其中 \vec{x}^{(i)}_{N \times 1} = \left[ \begin{matrix} \vec{x}^{(i)}_1 & \vec{x}^{(i)}_2 & \cdots & \vec{x}^{(i)}_N \end{matrix} \right] ^T \tag{1.2}高维到低维的映射在LLE基础上，将数据的映射方法指定为 \vec{y}^{(i)} = P^T · \vec{x}^{(i)} \tag{*1} \vec{y}^{(i)}_j = \vec{p}_j^T · \vec{x}^{(i)} 其中 P_{N \times D} = \left[ \begin{matrix} | & | & & | \\ \vec{p}_1 & \vec{p}_2 & \cdots & \vec{p}_D \\ | & | & & | \end{matrix} \right] \tag{2.1}\vec{p}_{i_{N \times 1}} = \left[ \begin{matrix} \vec{p}_{i1} & \vec{p}_{i2} & \cdots & \vec{p}_{iN} \end{matrix} \right] ^T \tag{2.2}\vec{y}^{(i)}_{D \times 1} = \left[ \begin{matrix} \vec{y}^{(i)}_1 & \vec{y}^{(i)}_2 & \cdots & \vec{y}^{(i)}_D \end{matrix} \right] ^T \tag{2.3}高维空间的空间结构特征与Locally Linear Embedding一致，通过矩阵$\dot{W}$保存空间结构特征 J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2 \tag{3.1}\text{s.t.} \quad \sum_{j=1}^K w_{ij} = 1 \quad \text{or} \quad \vec{w}_i^T \vec{1} = 1 \tag{3.2}解得 \vec{w}_{i_{K \times 1}} = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}其中 Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)})X^{(i)} = \left[ \begin{matrix} | & | & & | \\ \vec{x}^{(i)} & \vec{x}^{(i)} & \cdots & \vec{x}^{(i)} \\ | & | & & | \end{matrix} \right]N^{(i)} = \left[ \begin{matrix} | & | & & | \\ \vec{x}^{(1)}_N & \vec{x}^{(2)}_N & \cdots & \vec{x}^{(K)}_N \\ | & | & & | \end{matrix} \right]解得矩阵 W_{K \times M} = \left[ \begin{matrix} | & | & & | \\ \vec{w}_1 & \vec{w}_2 & \cdots & \vec{w}_M \\ | & | & & | \end{matrix} \right]低维空间保持同样的空间结构在低维空间中，损失定义为 J(Y) = \sum_{i} || \vec{y}^{(i)} - \sum_j w_{ij} \vec{y}^{(j)} ||_2^2 \tag{4}由于低维空间中近邻情况未知，故将矩阵$W$扩充为$\dot{W}$ \dot{W}_{M \times M} = \left[ \begin{matrix} | & | & & | \\ \dot{\vec{w}}_1 & \dot{\vec{w}}_2 & \cdots & \dot{\vec{w}}_M \\ | & | & & | \end{matrix} \right] \tag{5.1} \dot{w}_{ij} = \begin{cases} w_{ik} & x^{(j)} = N^{(i)}_k \\ 0 & \text{otherwise} \end{cases} \tag{5.2}相应的，$\vec{y}^{(i)}_{D \times 1}$扩充为$\dot{\vec{y}}^{(i)}_{M \times 1}$。 则式$(4)$可变换为 J(\dot{Y}) = \sum_{i} || \dot{\vec{y}}^{(i)} - \sum_j \dot{w}_{ij} \dot{\vec{y}}^{(j)} ||_2^2 \tag{6.1}增加约束条件，与LLE略有不同 \dot{\vec{y}}^{(i)^T} \dot{\vec{y}}^{(i)} = 1 \tag{6.2}写作矩阵形式，即 J(\dot{Y}) = \text{tr} \left[ \dot{Y} (I - \dot{W}) (I - \dot{W})^T \dot{Y}^T \right] \tag{7}\text{s.t.} \dot{\vec{y}}^{(i)^T} \dot{\vec{y}}^{(i)} = 1其中 \dot{Y}_{M \times M} = \dot{P}^T_{(N \times M)^T} X_{N \times M} \tag{8.1}\dot{\vec{y}^{(i)}}_{M \times 1} = \dot{P}^T_{(N \times M)^T} \dot{\vec{x}^{(i)}}_{N \times 1} \tag{8.2}则优化问题转换为 J(\dot{P}) = \text{tr} \left[\dot{P}^T X (I - \dot{W}) (I - \dot{W})^T X^T \dot{P} \right] \tag{9}\text{s.t.} \quad \dot{\vec{x}^{(i)}}^T \dot{P} \dot{P}^T \dot{\vec{x}^{(i)}} = 1记 M = (I - \dot{W}) (I - \dot{W})^T \tag{10}列写拉格朗日函数 L(\dot{P}) = \text{tr} \left[\dot{P}^T X (I - \dot{W}) (I - \dot{W})^T X^T \dot{P} \right] + \lambda (\dot{\vec{x}^{(i)}}^T \dot{P} \dot{P}^T \dot{\vec{x}^{(i)}} - 1) \tag{11}\frac{\nabla L(\dot{P})}{\nabla \dot{P}} = 2 X M X^T \dot{P} + 2 \lambda X X^T \dot{P}\Rightarrow \quad X M X^T \dot{P} = \lambda X X^T \dot{P} \tag{*3} \frac{\nabla b^T X^T X c}{\nabla X} = X(bc^T + cb^T) 同LLE，当低维数据维度为$D$时，按特征值升序排序约化矩阵，即选择最前的$D$个特征向量组成投影矩阵 P = \left[ \begin{matrix} | & | & & | \\ \vec{\alpha}_1 & \vec{\alpha}_2 & \cdots & \vec{\alpha}_D \\ | & | & & | \end{matrix} \right] \tag{*4}实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106def eig(A1, A2): """ Params: A1, A2: &#123;ndarray(n, n)&#125; Returns: eigval: &#123;ndarray(n)&#125; eigvec: &#123;ndarray(n, n)&#125; Notes: A1 \alpha = \lambda A2 \alpha """ s, u = np.linalg.eigh(A2 + np.diag(np.ones(A2.shape[0]))*1e-3) s_sqrt_inv = np.linalg.inv(np.diag(np.sqrt(s))) A = s_sqrt_inv.dot(u.T).dot(A1).dot(u).dot(s_sqrt_inv) eigval, P = np.linalg.eigh(A) eigvec = u.dot(s_sqrt_inv).dot(P) return eigval, eigvec class NeighborhoodPreservingEmbedding(): """ Neighborhood Preserving Embedding Attributes: n_neighbors: &#123;int&#125; n_components: &#123;int&#125; W_: &#123;ndarray&#125; components_: &#123;ndarray(n_samples, n_components)&#125; """ def __init__(self, n_neighbors, n_components=2, k_skip=1): self.n_neighbors = n_neighbors self.n_components = n_components self.k_skip = k_skip self.W_ = None self.components_ = None def fit(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; """ from sklearn.neighbors import KDTree kdtree = KDTree(X, metric='euclidean') n_samples, n_features = X.shape self.W_ = np.zeros((n_samples, n_samples)) for i in range(n_samples): ## 获取近邻样本点 x = X[i] idx = kdtree.query(x.reshape(1, -1), self.n_neighbors + 1, return_distance=False)[0][1: ] ## 求取矩阵 Z = (x - N).dot((x - N).T) N = X[idx] Z = (x - N).dot((x - N).T) ## 求取权重 w_i Z_inv = np.linalg.inv(Z + np.finfo(float).eps * np.eye(self.n_neighbors)) w = np.sum(Z_inv, axis=1) / np.sum(Z_inv) ## 保存至 W for j in range(self.n_neighbors): self.W_[idx[j], i] = w[j] ## 求取矩阵 M = (I - W)(I - W)^T I = np.eye(n_samples) M = (I - self.W_).dot((I - self.W_).T) ## 求解 X M X^T \alpha = \lambda X X^T \alpha A1 = X.T.dot(M).dot(X) A2 = X.T.dot(X) ## 求解拉普拉斯矩阵的特征分解 # eps = np.finfo(float).eps * np.eye(A2.shape[0]) # A = np.linalg.inv(A2 + eps).dot(A1) # eigval, eigvec = np.linalg.eig(A) # 上三句改为 eigval, eigvec = eig(A1, A2) eigvec = eigvec[:, np.argsort(eigval)] eigval = eigval[np.argsort(eigval)] ## 选取 D 维 self.components_ = eigvec[:, self.k_skip: self.n_components + self.k_skip] def transform(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: Y: &#123;ndarray(n_samples, n_components)&#125; """ Y = X.dot(self.components_) return Y def fit_transform(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: Y: &#123;ndarray(n_samples, n_components)&#125; """ self.fit(X) Y = self.transform(X) return Y 实验1234567891011from sklearn import datasetsdigits = datasets.load_digits(n_class=6)X = digits.datay = digits.targetimages = digits.imagesnpe = NeighborhoodPreservingEmbedding(30, 2, k_skip=3)X_npe = npe.fit_transform(X)plot_embedding(X_npe, y, images, title=None, t=2e-3, figsize=(12, 9))]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>降维</tag>
        <tag>manifold</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挪威的森林]]></title>
    <url>%2F2019%2F08%2F10%2F%E6%8C%AA%E5%A8%81%E7%9A%84%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[没有人喜欢孤独，只是不愿失望。 时至今日，我才恍然领悟到直子之所以求我别忘掉她的原因。直子当然知道，知道她在我心目中的记忆迟早要被冲淡。也惟其如此，她才强调说：希望你能记住我，记住我曾这样存在过。 想到这里，我就悲哀得难以自禁。因为，直子连爱都没爱过我的。 死并非生的对立面，而作为生的一部分永存。 或许我的心包有一层硬壳，能破壳而入的东西是极其有限的。所以我才不能对人一往情深。 他也背负着他的十字架匍匐在人生征途中。 那正是同被直子盯视眼睛时所感到的同一性质的悲哀。这种莫可名状的心绪，我既不能将其排遣于外，又不能将其深藏于内。它像掠身而去的阵风一样没有轮廓，没有重量。 这种莫可名状的心绪，我既不能将其排遣于外，又不能将其深藏于内。它像掠身而去的阵风一样没有轮廓，没有重量。我甚至连把它裹在身上都不可能。 “哪里会有人喜欢孤独！不过是不乱交朋友罢了。那样只能落得失望。”我说。 “绅士就是：所做的，不是自己想做之事，而是自己应做之事。” 说不定那时我们是为相遇而相遇的。纵令那时未能相遇，也会在别的地方相遇—倒没什么根据，但我总是有这种感觉。 也许等得过久了。我追求的是十二分完美无缺的东西，所以才这么难。 孤零零一个人，觉得身体就像一点点腐烂似的。渐渐腐烂、融化，最后变成一洼黏糊糊的绿色液体，再被吸进地底下去，剩下来的只是衣服—就是这种感觉，在干等一天的时间里。 每个人无不显得很幸福。至于他们是真的幸福还是仅仅表面看上去如此，就无从得知了。 什么是美好的以及如何获得幸福之类。对我毋宁说是个十分烦琐而错综复杂的命题，从而使我转求其他的标准，诸如公正、正直、普遍性等。” 倘若我在你心中留下什么创伤，那不仅仅是你一个人的，也是我的创伤。 所以如此，是因为什么，而它又意味什么，为什么等等。至于这种分析是将世界简单化还是条理化，我却是不明不白。 普通人啊。生在普通家庭，长在普通家庭，一张普通的脸，普通的成绩，想普通的事情。 人若要在某件事上扯谎，就势必为此编造出一大堆相关的谎言。 世界上，有人喜欢查时刻表一查就整整一天；也有的人把火柴棍拼在一起，准备造一艘一米长的船。所以说，这世上有一两个要理解你的人也没什么不自然的吧？ 世上是有这种人的：尽管有卓越的天赋才华，却承受不住使之系统化的训练，而终归将才华支离破碎地挥霍掉。 现实世界里，很多方面人们都在互相强加，以邻为壑，否则就活不下去。 “那不是努力，只是劳动。”永泽断然说道，“我所说的努力与这截然不同。所谓努力，指的是主动而有目的的活动。” 就在这种气势夺人的暮色当中，我猛然想起了初美，并且这时才领悟她给我带来的心灵震颤究竟是什么东西—它类似一种少年时代的憧憬，一种从来不曾实现而且永远不可能实现的憧憬。这种直欲燃烧般的天真烂漫的憧憬，我在很早以前就已遗忘在什么地方了，甚至在很长时间里我连它曾在我心中存在过都未曾记起。而初美所摇撼的恰恰就是我身上长眠未醒的“我自身的一部分”。 “可爱极了！”“绿子，”她说，“要加上名字。”“可爱极了，绿子。”我补充道。“极了是怎么个程度？”“山崩海枯那样可爱。”绿子扬着脸看着我：“你用词倒还不同凡响。”“给你这么一说，我心里也暖融融的。”我笑道。“来句更棒的。”“最最喜欢你，绿子。”“什么程度？”“像喜欢春天的熊一样。”“春天的熊？”绿子再次扬起脸，“什么春天的熊？”“春天的原野里，你正一个人走着，对面走来一只可爱的小熊，浑身的毛活像天鹅绒，眼睛圆鼓鼓的。它这么对你说道：‘你好，小姐，和我一块打滚玩好么？’接着你就和小熊抱在一起，顺着长满三叶草的山坡咕噜咕噜滚下去，整整玩了一大天。你说棒不棒？”“太棒了。”“我就这么喜欢你。” “喜欢我喜欢到什么程度？”绿子问。“整个世界森林里的老虎全都融化成黄油。” 我则几乎没有抬头，日复一日地打发时光。在我眼里，只有漫无边际的泥沼。往前落下右脚，拔起左脚，再拔起右脚。我判断不出我位于何处，也不具有自己是在朝正确方向前进的信心。我之所以一步步挪动步履，只是因为我必须挪动，而无论去哪里。 在我眼里，春夜里的樱花，宛如从开裂的皮肤中鼓胀出来的烂肉，整个院子都充满烂肉那甜腻而沉闷的腐臭气味。 同情自己是卑劣懦夫干的勾当。 “饼干罐不是装有各种各样的饼干，喜欢的和不大喜欢的不都在里面吗？如果先一个劲儿地挑你喜欢的吃，那么剩下的就全是不大喜欢的。每次遇到麻烦我就总这样想：先把这个应付过去，往下就好过了。人生就是饼干罐。” 纵令听其自然，世事的长河也还是要流往其应流的方向，而即使再竭尽人力，该受伤害的人也无由幸免。所谓人生便是如此。 死并非生的对立面，死潜伏在我们的生之中。 “信终归不过是信。”我说，“即使烧了，该留在心里的自然留下；就算保存在那里，留不下来的照样留不下。” 我给绿子打电话，告诉她：自己无论如何都想和她说话，有满肚子话要说，有满肚子非说不可得话。整个世界上除了她别无他求。相见她想同她说话，两人一切从头开始。绿子在电话的另一头久久默然不语，如同全世界的细雨落在全世界所有的草坪上一般的沉默在持续。这时间里，我一直合着双眼，把额头顶在电话亭玻璃上。良久，绿子用沉静的声音开口道：“你现在在哪里？”我现在在哪里？我拿着听筒扬起脸，飞快读环视电话亭四周。我现在在哪里？我不知道这里是哪里，全然摸不着头脑。这里究竟是哪里？目力所及，无不是不知走去哪里的无数男男女女。我在哪里也不是的场所的中央，不断地呼唤着绿子。]]></content>
      <categories>
        <category>Reading</category>
      </categories>
      <tags>
        <tag>村上春树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Locally Linear Embedding]]></title>
    <url>%2F2019%2F08%2F09%2FLocally-Linear-Embedding%2F</url>
    <content type="text"><![CDATA[前言高维数据可视化需要通过降维的方式，而PCA、LDA等线性降维算法，忽视了数据点间的空间结构特征，本文介绍的局部线性嵌入(LLE)算法为非线性降维方法，基于谱的降维方法，所以很靠谱 :-)。 原理设有$M$个$N$维数据点，构成矩阵$X$，如下 X = \left[ \begin{matrix} \vec{x}^{(1)} & \vec{x}^{(2)} & \cdots & \vec{x}^{(M)} \end{matrix} \right] \tag{1}其中$\vec{x}^{(i)}$表示样本点向量 \vec{x}^{(i)} = \left[ \begin{matrix} x^{(i)}_1 & x^{(i)}_2 & \cdots & x^{(i)}_3 \end{matrix} \right]^T \tag{2}LLE的基本思路： 任一数据点$\vec{x}^{(i)}$可由其的$K$个近邻点$\mathcal{N}_K(\vec{x}^{(i)})$线性表出，可作为数据样本的结构特征，即 \hat{\vec{x}}^{(i)} = \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} \tag{3.1}其中 \vec{w}_i = \left[ \begin{matrix} w_{i1} & w_{i2} & \cdots & w_{ik} \end{matrix} \right]^T若$\vec{x}^{(i)}$映射到低维空间中对应特征点为$\vec{y}^{(i)}$，对相同参数$w_{ij}$，$\vec{x}^{(i)}$也可由其K个近邻点$\mathcal{N}_K(\vec{y}^{(i)})$表出，即 \hat{\vec{y}}^{(i)} = \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} w_{ij} \vec{y}^{(j)} \tag{3.2}数据结构参数求解参数向量$\vec{w}_i$，可利用最小二乘法，即目标函数定义为 J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2 \tag{4.1}对参数向量$\vec{w}_i$归一化，即加入约束项 \sum_{j=1}^K w_{ij} = 1 \quad \text{or} \quad \vec{w}_i^T \vec{1} = 1 \tag{4.2}式$(4.1)$可作如下变换 J(\vec{w}_i) = || \vec{x}^{(i)} - \sum_{\vec{x}^{(j)} \in \mathcal{N}_K(\vec{x}^{(i)})} w_{ij} \vec{x}^{(j)} ||_2^2= || \sum_{j=1}^K w_{ij} \vec{x}^{(i)} - \sum_{j=1}^K w_{ij} \vec{x}^{(j)} ||_2^2= || \sum_{j=1}^K w_{ij} (\vec{x}^{(i)} - \vec{x}^{(j)}) ||_2^2= || (X^{(i)} - N^{(i)}) \vec{w}_i ||_2^2 \tag{5.1}其中 X^{(i)} = \left[ \begin{matrix} \vec{x}^{(i)} & \vec{x}^{(i)} & \cdots & \vec{x}^{(i)} \end{matrix} \right] \tag{5.1.1}N^{(i)} = \left[ \begin{matrix} \vec{x}_N^{(1)} & \vec{x}_N^{(2)} & \cdots & \vec{x}_N^{(K)} \end{matrix} \right] \tag{5.1.2}则 J(\vec{w}_i) = || (X^{(i)} - N^{(i)}) \vec{w}_i ||_2^2 \tag{5.1}= \left[ (X^{(i)} - N^{(i)}) \vec{w}_i \right]^T \left[ (X^{(i)} - N^{(i)}) \vec{w}_i \right]= \vec{w}_i^T (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \vec{w}_i \tag{5.2}记矩阵 Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \tag{5.3}最终优化目标为 J(\vec{w}_i) = \vec{w}_i^T Z^{(i)} \vec{w}_i \tag{*1}\text{s.t.} \quad \vec{w}_i^T \vec{1} = 1利用拉格朗日乘子法，构造拉格朗日函数，有 L(\vec{w}_i) = \vec{w}_i^T Z^{(i)} \vec{w}_i + \lambda (\vec{w}_i^T \vec{1} - 1) \tag{6.1}则 \begin{cases} \frac{\nabla L(\vec{w}_i)}{\nabla \vec{w}_i} = 2 Z^{(i)} \vec{w}_i + \lambda \vec{1} = \vec{0} \\ \frac{\nabla L(\vec{w}_i)}{\nabla \lambda} = \vec{w}_i^T \vec{1} - 1 = 0 \end{cases} \tag{6.2}由式$1$得到 \vec{w}_i = - \frac{\lambda}{2} Z^{(i)-1} \vec{1} \tag{6.3}代入式$2$有 \left( - \frac{\lambda}{2} Z^{(i)-1} \vec{1} \right)^T \vec{1} = 1 \tag{6.4}解得 \lambda = - \frac{2}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{6.5}代回$(6.3)$得到 \vec{w}_i = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2}低维数据的求解由式$(3.2)$，即 \hat{\vec{y}}^{(i)} = \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} \dot{w}_{ij} \vec{y}^{(j)} \tag{3.2}同样的，利用最小二乘法，构建目标函数为 J(Y) = \sum_{i=1}^M || \vec{y}^{(i)} - \sum_{\vec{y}^{(j)} \in \mathcal{N}_K(\vec{y}^{(i)})} \dot{w}_{ij} \vec{y}^{(j)} ||_2^2 \tag{7.1}对低维数据进行标准化，即加入约束 \begin{cases} \sum_{i=1}^M \vec{y}^{(i)} = \vec{0} \\ \frac{1}{M} \sum_{i=1}^M \vec{y}^{(i)} \vec{y}^{(i)T} = I \end{cases} \tag{7.2}矩阵形式为 Y Y^T = M·I \tag{7.3}由于当前数据点具体分布未知，故$\vec{y}^{(i)}$的近邻点$\mathcal{N}_K(\vec{y}^{(i)})$无法得知，故将矩阵$W_{M \times K}$补全为矩阵$\dot{W}_{M \times M}$，即 \dot{w}_{ij} = \begin{cases} w_{ik} & x^{(j)} = N^{(i)}_k \\ 0 & \text{otherwise} \end{cases} \tag{8} 相当于邻接矩阵。 同样的，$\vec{y}^{(i)}$也需在维度上进行补全为$\dot{\vec{y}}^{(i)}$，记 \dot{Y}_{M \times M} = \left[ \begin{matrix} \dot{\vec{y}}^{(1)} & \dot{\vec{y}}^{(2)} & \cdots & \dot{\vec{y}}^{(M)} \end{matrix} \right] \tag{9}则 J(\dot{Y}) = \sum_{i=1}^M || \dot{\vec{y}}^{(i)} - \sum_{\dot{\vec{y}}^{(j)} \in \mathcal{N}_K(\dot{\vec{y}}^{(i)})} \dot{w}_{ij} \dot{\vec{y}}^{(j)} ||_2^2 \tag{7.1}= \sum_{i=1}^M || \dot{Y} \vec{1}_i - \dot{Y} \dot{\vec{w}_i} ||_2^2 = \sum_{i=1}^M || \dot{Y} (\vec{1}_i - \dot{\vec{w}_i}) ||_2^2 \tag{10.1}= \text{tr} \left[ \dot{Y} (I - \dot{W}) (I - \dot{W})^T \dot{Y}^T \right] \tag{10.2} (10.1) \rightarrow (10.2): \mathcal{WTF} 实际上 由于 d_i = \sum_{j=1}^M \dot{w}_{ij} = 1所以由拉普拉斯矩阵定义 L = D - W$I - \dot{W}$即邻接矩阵$\dot{W}$的拉普拉斯矩阵。 其中 \vec{1}_i = \left[ \begin{matrix} 0 & \cdots & 1_i & \cdots & 0 \end{matrix} \right]^TI = \left[\begin{matrix} 1 & & \\ & \cdots & \\ & & 1 \end{matrix}\right]记$A = (I - \dot{W}) (I - \dot{W})^T$, 最终优化目标为 J(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] \tag{*3}\text{s.t.} \quad \dot{Y} \dot{Y}^T = M·I构造拉格朗日函数 L(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] + \lambda (\dot{Y} \dot{Y}^T - M · I) \tag{11} \begin{cases} \frac{\nabla L(\dot{Y})}{\nabla \dot{Y}} = 2 A \dot{Y}^T + 2 \lambda \dot{Y}^T = 0 \\ \frac{\nabla L(\dot{Y})}{\nabla \lambda} = \dot{Y} \dot{Y}^T - M · I = 0\\ \end{cases} \frac{\nabla \text{tr}[F(\vec{x})]}{\nabla \vec{x}} = f(\vec{x})^T 注意$1$式，可变换为 A \dot{Y}^T = \hat{\lambda} \dot{Y}^T \tag{12.1}其中$\hat{\lambda} = - \lambda$，又$\dot{Y} \dot{Y}^T = M·I$，为正交相似变换，所以 \dot{Y}^T = P = \left[ \begin{matrix} \vec{\alpha}_1 & \vec{\alpha}_2 & \cdots & \vec{\alpha}_M \end{matrix} \right] \tag{12.2}由于最小化目标为 J(\dot{Y}) = \text{tr} \left[ \dot{Y} A \dot{Y}^T \right] = \text{tr} (\Lambda) = \sum_{i=1}^M \lambda_i故选择最小的特征值$\lambda_i$及其对应的特征向量$\alpha_i$，要得到$D$维数据集，将$\dot{Y}^T$进行约化，即 Y^T = \left[ \begin{matrix} \vec{\alpha}_{M + 1 - D} & \cdots & \vec{\alpha}_M \end{matrix} \right] \tag{13}由于$\vec{w}_i^T \vec{1} = 1$，即 \dot{W}^T \vec{1} = \vec{1} \tag{14.1}移项整理得 (\dot{W} - I)^T \vec{1} = \vec{0}$\vec{1} \neq \vec{0}$，所以 (\dot{W} - I)^T = 0 \tag{14.2}左边同乘$\dot{W} - I$得到 (\dot{W} - I) (\dot{W} - I)^T \vec{1} = A · \vec{1} = 0 · \vec{1} \tag{14.3}所以 \lambda_M = 0, \quad \alpha_M = \vec{1} \tag{14.4}特征值为$0$表示不能反映数据特征，故低维数据应为 Y^T = \left[ \begin{matrix} \vec{\alpha}_{M - D} & \cdots & \vec{\alpha}_{M-1} \end{matrix} \right] \tag{*4}实现 计算原始样本空间中，每个样本$x^{(i)}$的近邻点$N^{(i)}$，并求取权值$w_{ij}$作为邻接权重，存储为矩阵$\dot{W}_{M \times M}$； Z^{(i)}_{K \times K} = (X^{(i)} - N^{(i)})^T (X^{(i)} - N^{(i)}) \tag{5.3}\vec{w}_i = \frac{Z^{(i)-1} \vec{1}}{\vec{1}^T Z^{(i)-1} \vec{1}} \tag{*2} \dot{w}_{ij} = \begin{cases} w_{ik} & x^{(j)} = N^{(i)}_k \\ 0 & \text{otherwise} \end{cases} \tag{8} 求取矩阵$A$，并将其特征分解 A = (I - \dot{W}) (I - \dot{W})^TA \dot{Y}^T = \hat{\lambda} \dot{Y}^T \tag{12.1} 选择最小的特征值$\lambda_i$及其对应的特征向量$\alpha_i$，约化为$D$维数据，作为低维特征点 Y^T = \left[ \begin{matrix} \vec{\alpha}_{M - D} & \cdots & \vec{\alpha}_{M-1} \end{matrix} \right] \tag{*4} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class LocallyLinearEmbedding(): """ Locally Linear Embedding Attributes: n_neighbors: &#123;int&#125; n_components: &#123;int&#125; W: &#123;ndarray&#125; $$ W = \left[ \begin&#123;matrix&#125; w_1 &amp; w_2 &amp; \cdots &amp; w_&#123;n_samples&#125; \end&#123;matrix&#125; \right] $$ $$ w_i = \left[ \begin&#123;matrix&#125; w_&#123;i1&#125; &amp; w_&#123;i2&#125; &amp; \cdots &amp; w_&#123;i, n_samples&#125; \end&#123;matrix&#125; \right]^T $$ """ def __init__(self, n_neighbors, n_components=2): self.n_neighbors = n_neighbors self.n_components = n_components self.W = None def fit(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: W: &#123;ndarray(n_samples, n_samples)&#125; """ from sklearn.neighbors import KDTree kdtree = KDTree(X, metric='euclidean') n_samples, n_features = X.shape self.W = np.zeros((n_samples, n_samples)) for i in range(n_samples): ## 获取近邻样本点 x = X[i] idx = kdtree.query(x.reshape(1, -1), self.n_neighbors + 1, return_distance=False)[0][1: ] ## 求取矩阵 Z = (x - N).dot((x - N).T) N = X[idx] Z = (x - N).dot((x - N).T) ## 求取权重 w_i Z_inv = np.linalg.inv(Z + np.finfo(float).eps * np.eye(self.n_neighbors)) w = np.sum(Z_inv, axis=1) / np.sum(Z_inv) ## 保存至 W for j in range(self.n_neighbors): self.W[idx[j], i] = w[j] return self.W def transform(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: Y: &#123;ndarray(n_samples, n_components)&#125; """ n_samples, n_features = X.shape ## 求取矩阵 A = (I - W)(I - W)^T I = np.eye(n_samples) A = (I - self.W).dot((I - self.W).T) ## 对 A 进行特征分解，并按特征值升序排序 eigval, eigvec = np.linalg.eig(A) eigvec = eigvec[:, np.argsort(eigval)] ## 选取 D 维 k_skip = 1 Y = eigvec[:, k_skip: self.n_components + k_skip] return Y def fit_transform(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: Y: &#123;ndarray(n_samples, n_components)&#125; """ self.fit(X) Y = self.transform(X) return Y 实验在scikit learn官网有具体实现基于手写数据集的几种基于谱的降维方法对比。 以下为上述代码实现的结果1234567891011from sklearn.datasets import load_digitsdigits = load_digits(n_class=6)X = digits.datay = digits.targetimages = digits.imageslle = LocallyLinearEmbedding(30, 2)X_lle = lle.fit_transform(X)plot_embedding(X_lle, y, images, title=None, t=2e-3, figsize=(12, 9)) Reference (十二)LLE局部线性嵌入降维算法 - 简书 2.2.3. Locally Linear Embedding - scikit learn An Introduction to Locally Linear Embedding]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>降维</tag>
        <tag>manifold</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git工具-submodules]]></title>
    <url>%2F2019%2F08%2F09%2FGit%E5%B7%A5%E5%85%B7-submodules%2F</url>
    <content type="text"><![CDATA[前言当需要在一个项目中使用另一个项目时，可以将后者作为子模块加入前者。 使用新建带子模块的仓库例如在本地新建仓库Repository123$ mkdir Repository &amp;&amp; cd Repository$ git initInitialized empty Git repository in C:/Users/islou/Desktop/Repository/.git/ 若在本仓库中，需要使用该仓库isLouisHsu/Games1234567$ git submodule add https://github.com/isLouisHsu/GamesCloning into 'C:/Users/islou/Desktop/Repository/Games'...remote: Enumerating objects: 30, done.remote: Counting objects: 100% (30/30), done.remote: Compressing objects: 100% (27/27), done.remote: Total 30 (delta 5), reused 0 (delta 0), pack-reused 0Unpacking objects: 100% (30/30), done. 在当前仓库Repository中可以看到生成了文件.gitmodules与子仓库Games，.gitmodules内容如下123[submodule &quot;Games&quot;] path &#x3D; Games url &#x3D; https:&#x2F;&#x2F;github.com&#x2F;isLouisHsu&#x2F;Games 此时，若在当前仓库查询状态，显示更改内容1234567891011121314151617$ git statusOn branch masterNo commits yetChanges to be committed: (use "git rm --cached &lt;file&gt;..." to unstage) new file: .gitmodules new file: Games$$ cd Games$ git statusOn branch masterYour branch is up to date with 'origin/master'.nothing to commit, working tree clean 克隆带子仓库的模块 方法1 克隆这类仓库时，默认包含该子模块目录，但其中没有文件，需要在仓库目录下运行 12$ git submodule init$ git submodule update git submodule init 用来初始化本地配置文件，而 git submodule update 则从该项目中抓取所有数据并检出父项目中列出的合适的提交 方法2 直接使用--recursive参数，自动初始化并更新仓库中的每一个子模块 1$ git clone --recursive https://github.com/louishsu/Repository Reference 7.11 Git工具-子模块 - git]]></content>
      <categories>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python生成Markdown表格]]></title>
    <url>%2F2019%2F08%2F09%2FPython%E7%94%9F%E6%88%90Markdown%E8%A1%A8%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[前言Reference前言Markdown中编辑表格比较繁琐，如编辑下表时，需要按字符输入 姓名\科目 A B C D 小妖 3 4 5 3 小怪 4 5 3 4 小兽 5 3 4 5 12345| 姓名\科目 | A | B | C | D || :------: | - | - | - | - || 小妖 | 3 | 4 | 5 | 3 || 小怪 | 4 | 5 | 3 | 4 || 小兽 | 5 | 3 | 4 | 5 | 可借助字符串操作生成表格。 实现12345678910111213141516171819202122232425262728293031323334def gen_markdown_table_2d(head_name, rows_name, cols_name, data): """ Params: head_name: &#123;str&#125; 表头名， 如"count\比例" rows_name, cols_name: &#123;list[str]&#125; 项目名， 如 1,2,3 data: &#123;ndarray(H, W)&#125; Returns: table: &#123;str&#125; """ ELEMENT = " &#123;&#125; |" H, W = data.shape LINE = "|" + ELEMENT * W lines = [] ## 表头部分 lines += ["| &#123;&#125; | &#123;&#125; |".format(head_name, ' | '.join(cols_name))] ## 分割线 SPLIT = "&#123;&#125;:" line = "| &#123;&#125; |".format(SPLIT.format('-'*len(head_name))) for i in range(W): line = "&#123;&#125; &#123;&#125; |".format(line, SPLIT.format('-'*len(cols_name[i]))) lines += [line] ## 数据部分 for i in range(H): d = list(map(str, list(data[i]))) lines += ["| &#123;&#125; | &#123;&#125; |".format(rows_name[i], ' | '.join(d))] table = '\n'.join(lines) return table 终端中运行1234567891011121314151617&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from temp import gen_markdown_table_2d&gt;&gt;&gt;&gt;&gt;&gt; head_name = "姓名\\科目"&gt;&gt;&gt; rows_name = ["小妖", "小怪", "小兽"]&gt;&gt;&gt; cols_name = ["A", "B", "C", "D"]&gt;&gt;&gt; data = np.arange(4*3).reshape(3, 4)&gt;&gt;&gt;&gt;&gt;&gt; table = gen_markdown_table_2d(head_name, rows_name, cols_name, data)&gt;&gt;&gt; table'| 姓名\\科目 | A | B | C | D |\n| -----: | -: | -: | -: | -: |\n| 小妖 | 0 | 1 | 2 | 3 |\n| 小怪 | 4 | 5 | 6 | 7 |\n| 小兽 | 8 | 9 | 10 | 11 |'&gt;&gt;&gt; print(table)| 姓名\科目 | A | B | C | D || -----: | -: | -: | -: | -: || 小妖 | 0 | 1 | 2 | 3 || 小怪 | 4 | 5 | 6 | 7 || 小兽 | 8 | 9 | 10 | 11 |]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自动化脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ID3, C4.5, CART, RF]]></title>
    <url>%2F2019%2F08%2F02%2FID3-C4-5-CART-RF%2F</url>
    <content type="text"><![CDATA[前言决策树(Decision Tree)是一种基本的分类与回归方法，本文主要讨论分类决策树，它可被认作if-then的集合，也可以认作是定义在特征空间与类空间上的条件概率分布。 决策树决策树模型形式如下，由结点(node)，有向边(directed edge)组成，节点包含两种：内部节点(internal node)和叶节点(leaf node)，内部节点表示一个特征或属性，叶节点包含一个类。 决策树需要满足一个重要性质：互斥且完备。即每个实例都被且仅被一条路径或一条if-then规则覆盖。并且生成的决策树深度不能过大。 决策树学习主要有3个步骤： 特征选择、决策树生成、决策树修剪。 引例现以下某贷款申请样本数据表为例进行解释说明。 ID 年龄 工作 有房 信贷情况 类别 1 青年 否 否 一般 否 2 青年 否 否 好 否 3 青年 是 否 好 是 4 青年 是 是 一般 是 5 青年 否 否 一般 否 6 中年 否 否 一般 否 7 中年 否 否 好 否 8 中年 是 是 好 是 9 中年 否 是 非常好 是 10 中年 否 是 非常好 是 11 老年 否 是 非常好 是 12 老年 否 是 好 是 13 老年 是 否 好 是 14 老年 是 否 非常好 是 15 老年 否 否 一般 否 符号说明设数据集为$D$，$|D|$表示样本总数。设有$K$个类别$C_k(k=1, \cdots, K)$，$|C_k|$为类别$k$的样本数目，那么 |D| = \sum_{k=1}^K |C_k|设特征$A$有$N$个不同的取值${a_1, a_2, \cdots, a_N}$，则根据不同的取值，可将数据集$D$划分为$N$组${D_1, D_2, \cdots, D_N}$，也有 |D| = \sum_{n=1}^N |D_n|记子集$D_n$中属于类别$C_k$的样本集合为$D_{nk}$，即 D_{nk} = D_n \bigcap C_k特征选择在某个内部节点上，需要选择最具有代表性，或区分度最高的特征。可选用的准则有信息增益（条件熵）、基尼系数等。 ID3：以信息增益为准则来选择最优划分属性 定义(信息增益)：特征$A$对训练数据集$D$的信息增益$g(D, A)$，定义为集合$D$的经验熵$H(D)$与特征A给定条件下$D$的经验条件熵$H(D|A)$之差，即 g(D, A) = H(D) - H(D|A) \tag{1.1} 等价于训练数据集中类与特征的互信息(mutual information)，表示得知特征$A$的信息对数据集$D$分类的不确定性减少的程度。 其中 H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} \tag{1.2} 上式即，类别分布的经验熵 H(D|A) = \sum_{n=1}^N \frac{|D_n|}{|D|} H(D_n) = \sum_{n=1}^N \frac{|D_n|}{|D|} \underbrace{ - \sum_{k=1}^K \frac{|D_{nk}|}{|D_n|} \log_2 \frac{|D_{nk}|}{|D_n|} }_{H(D_n)} \tag{1.3} 上式即，以特征$A$不同取值进行子集划分，每个子集的类别分布经验熵，求取加权和 注： 熵 连续 H(X) = - \int_x p(x) \log p(x) dx 离散 H(X) = - \sum_x p(x) \log p(x) 条件熵 \begin{aligned} H(X|Y) = H(X, Y) - H(Y) \\ = (- \sum_{x, y} p(x, y) \log p(x, y)) - (- \sum_y p(y) \log p(y)) \\ = - \sum_{x, y} p(x, y) \log p(x, y) + \sum_x (\sum_y p(x, y)) \log p(y) \\ = - \sum_{x, y} p(x, y) (\log p(x, y) - \log p(y)) \\ = - \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(y)} \\ = - \sum_{x, y} p(x, y) \log p(x | y) \end{aligned} 互信息一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息(mutual information)。 I(X; Y) = H(X) - H(X | Y)也即 \begin{aligned} I(X; Y) = H(X) + H(Y) - H(X, Y) \\ I(X; Y) = I(Y; X) \end{aligned}由定义可得 I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} C4.5：基于信息增益率准则选择最优分割属性的算法 是对ID3的改进： 以信息增益比(information gain ratio)作为特征选择的准则，克服ID3会优先选择有较多属性值的特征的缺点； 属性值较多的特征，可将$D$划分为更多子集，计算得信息增益更大；信息增益比引入特征数据分布的衡量，对于两个相同信息增益的特征，选择特征数据分布更为集中的，划分更有效。 弥补不能处理特征属性值连续的问题 g_R(D, A) = \frac{g(D, A)}{H_A(D)} \tag{2.1}其中$H_A(D)$为特征$A$取值的的分布经验熵 H_A(D) = - \sum_{n=1}^N \frac{|D_n|}{|D|} \log_2 \frac{|D_n|}{|D|} \tag{2.2} CART：以基尼系数为准则选择最优划分属性，可以应用于分类和回归 基尼不纯度可以解释为：一个随机事件变成它的对立事件的概率，可以作为衡量系统混乱程度的标准。 由L.Breiman,J.Friedman,R.Olshen和C.Stone于1984年提出。ID3中根据属性值分割数据，之后该特征不会再起作用，这种快速切割的方式会影响算法的准确率。CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。 定义(基尼系数)：分类问题中，假设有$K$个类别，样本点属于第$k$类的概率为$p_k$，则基尼系数定义为 \text{Gini}(p) = \sum_{k=1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2 \tag{3.1} 分类树 特征选择准则为 g_G(D, A) = \text{Gini}(D) - \text{Gini}(D|A) \tag{3.2} 其中 \text{Gini}(D) = 1 - \sum_{k=1}^K \left( \frac{|C_k|}{|D|} \right)^2 \tag{3.3}\text{Gini}(D|A) = \sum_{n=1}^N \frac{|D_n|}{|D|} \text{Gini}(D_n) = \sum_{n=1}^N \frac{|D_n|}{|D|} \left[ 1 - \sum_{k=1}^K \left( \frac{|D_{nk}|}{|D_n|} \right)^2 \right] \tag{3.4} 回归树 寻找最佳切分点，将输入空间划分为多个区域。 引例求解以求解在引例的第一个内部节点为例，选用信息增益作为分类特征。 类别标签有两种 D.y = \{ 是， 否 \}则 p(y = 是) = \frac{9}{15},\quad p(y = 否) = \frac{6}{15}H(D) = - \frac{9}{15} \log_2 \frac{9}{15} - \frac{6}{15} \log_2 \frac{6}{15} = 0.971当前特征集为 A = \{ 年龄， 有工作， 有自己的房子， 信贷情况 \} 以年龄为特征 年龄含青年、中年、老年，则 p(青年) = \frac{5}{15}, \quad p(中年) = \frac{5}{15}, \quad p(老年) = \frac{5}{15}p(y = 是|青年) = \frac{2}{5}, \quad p(y = 否|青年) = \frac{3}{5}p(y = 是|中年) = \frac{3}{5}, \quad p(y = 否|中年) = \frac{2}{5}p(y = 是|老年) = \frac{4}{5}, \quad p(y = 否|老年) = \frac{1}{5} 则 H(D|青年) = - \frac{2}{5} \log_2 \frac{2}{5} - \frac{3}{5} \log_2 \frac{3}{5} = 0.971H(D|中年) = - \frac{3}{5} \log_2 \frac{3}{5} - \frac{2}{5} \log_2 \frac{2}{5} = 0.971H(D|老年) = - \frac{4}{5} \log_2 \frac{4}{5} - \frac{1}{5} \log_2 \frac{1}{5} = 0.722 则 H(D|年龄) = \sum_{年龄=青年}^{老年} p(年龄) H(D|年龄) = 0.888 所以 g(D, 年龄) = H(D) - H(D|年龄) = 0.083 以工作为特征 g(D, 工作) = H(D) - H(D|工作) = 0.324 以房子为特征 g(D, 房子) = H(D) - H(D|房子) = 0.420 以信贷为特征 g(D, 信贷) = H(D) - H(D|信贷) = 0.363 由于$g(D, 房子)$最大，故当前节点选择房子作为分类特征，然后按此特征可分成2个子集合${D_{有房}，D_{无房}}$，递归即可。 决策树生成决策树生成使用递归方法，将该节点处所属数据子集$D_s$按特征选择准则，选择当期节点最优划分，然后在划分后的子节点处，继续调用该函数进行递归生成。 伪代码如下123456789101112131415161718192021222324252627282930输入：数据集D，特征集A &#x3D; &#123;a_1, a_2, ..., a_N&#125;，阈值eps；输出：节点nodeCREAT-NODE(D, A, eps) if D中所有样本属于类别C_k return Node(C_k) if A为空集 C &#x3D; argmax(|D_k|, k &#x3D; 1, ..., K) return Node(C) # 计算特征选择准则，这里采用互信息g(D, A) for n &#x3D; 1 upto N g(D, A_n) &#x3D; H(D) - H(D|A_n) # 选择最优特征 a_g &#x3D; argmax(g(D, A_n), n &#x3D; 1, ..., N) # 终止条件 if g(D, a_g) &lt; eps C &#x3D; argmax(|D_k|, k &#x3D; 1, ..., K) return Node(C) node &#x3D; Node(a_g) # 递归 for i &#x3D; 1 upto a_g.length node.sub[i] &#x3D; CREAT-NODE(D_i, A - a_g, eps) return node 决策树剪枝如果学习时过多地考虑如何提高对训练数据的正确分类，使决策树划分层次过多(也即叶节点个数过多)，构建出过于复杂的决策树，那么会引起过拟合现象。 设决策树共有$T$个叶节点，且叶节点$t$上有$N_t$个样本点、其中类别$k$的有$N_{tk}$个，$k = 1, \cdots, K$，那么叶节点$t$的类别经验熵为 H_t(T) = - \sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t} \tag{4.1}那么所有叶节点的类别经验熵之和为 C(T) = \sum_{t=1}^{|T|} N_t H_t(T) \tag{4.2}希望$C(T)$极小化，减小类别经验熵，那么会增加划分使叶子节点数目增多，考虑模型复杂度$T$与目标之间的权衡，设置正则化参数$\alpha$，那么决策树的整体结构损失函数为 C_{\alpha} (T) = C(T) + \alpha |T| \tag{4.3}利用损失函数最小原则仅剪枝就是用正则化的极大似然估计进行模型选择。可以通过动态规划递归地使叶子节点向上回缩，求取损失函数最小地子树$T_{\alpha}$。 实现详细查看Github: isLouisHsu/Basic-Machine-Learning-Algorithm 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class Node(): """ Attributes: index: 子树分类标签, 若为叶节点, 则为None childNode: 子树，若为叶节点, 则为分类标签; 否则为字典 """ def __init__(self): self.index = None self.childNode = Noneclass DecisionTree(): ''' @note: - categorical features; - ID3 ''' def __init__(self): self.tree = None def fit(self, X, y): self.tree = self._creatNode(X, y) def _creatNode(self, X, y): node = Node() # 若只含一种类别，则返回叶节点 if len(set(y)) == 1: node.childNode = list(set(y))[0]; return node # entropy: H(D) y_encoded = OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray() p_y = np.mean(y_encoded, axis=0); p_y[p_y==0.0] = 1.0 # 因为 0*np.log(0)结果为nan, 而不是0, 用 1*np.log(1)替代 H_D = - np.sum(p_y * np.log(p_y)) # conditional entropy: H(D|A) H_D_A = np.zeros(shape=(X.shape[1],)) # initialize for i_feature in range(X.shape[1]): X_feature = X[:, i_feature] if len(set(X_feature)) == 1: H_D_A[i_feature] = float('inf'); continue # 若该特征只有一种取值，表示已使用该列作为分类特征 X_feature_encoded = OneHotEncoder().fit_transform(X_feature.reshape((-1, 1))).toarray() p_X = np.mean(X_feature_encoded, axis=0) # 每个取值的概率 for j_feature in range(X_feature_encoded.shape[1]): # 该特征取值有几种，编码后就有几列 y_encoded_feature = y_encoded[X_feature_encoded[:, j_feature]==1] # 该特征某种取值下，其对应的标签值 p_y_X = np.mean(y_encoded_feature, axis=0); p_y_X[p_y_X==0.0] = 1.0 H_D_feature = - np.sum(p_y_X * np.log(p_y_X)) H_D_A[i_feature] += p_X[j_feature] * H_D_feature # 条件熵 # information gain: g(D, A) = H(D) - H(D|A) g_D_A = H_D - H_D_A # 选出最大的作为分类特征 node.index = np.argmax(g_D_A) X_selected = X[:, node.index] # 分类后继续建立树 node.childNode = dict() for val in set(X_selected): valIndex = (X_selected==val) X_val, y_val = X[valIndex], y[valIndex] node.childNode[val] = self._creatNode(X_val, y_val) # 存储在字典中，键为分类值，值为子树 return node def predict(self, X): y_pred = np.zeros(shape=(X.shape[0],)) for i_sample in range(X.shape[0]): currentNode = self.tree # 初始化为父节点 while not currentNode.index==None: # 若为None, 表示为叶子结点 val = X[i_sample, currentNode.index] # 当前样本在分类特征上的值 currentNode = currentNode.childNode[val] # 递归 else: y_pred[i_sample] = currentNode.childNode return y_pred 最终生成分类树如下 随机森林(Random Forest)随机森林是用Bagging策略，对包含$N$个样本的数据集进行$M$次的有放回的采样，每次随机取$N_m$个样本，得到$M$个样本数目为$N_m$的样本子集，对每个子集建立分类器。 Bootstrap采样：对于一个样本，它在某一次含$m$个样本的训练集的随机采样中，每次被采集到的概率是$1/m$。不被采集到的概率为$1−1/m$。如果$m$次采样都没有被采集中的概率是$(1−1/m)^m$。当$m→\infty$时，$\lim_{m \rightarrow \infty} (1−1/m)^m \approx 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。对于这部分大约$36.8\%$的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。 随机森林在Bagging策略上进行训练： 用Bootstrap策略随机采样$M$次； 一棵树的生成时，仅从所有特征($K$个)中选取$k$个特征； 生成$M$棵树进行投票表决，确定预测结果(分类可取众数、回归可取均值)。 那么随机森林有以下超参数： 选取特征数目$k$，一般取$k = \sqrt{K}$； 每棵树的最大深度，一般不超过$8$； 树的数目； …… Reference ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结 统计学习方法，李航，第5章 决策树与随机森林 - cnblogs 常用的模型集成方法介绍：bagging、boosting、stacking - 机器之心 Bagging与随机森林算法原理小结 - cnblogs]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[人间失格]]></title>
    <url>%2F2019%2F07%2F21%2F%E4%BA%BA%E9%97%B4%E5%A4%B1%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[人类是群居的孤独者。 第一手札 尽管如此，他们却能够不死自杀，免于疯狂，纵谈政治，竟不绝望，不屈不挠，继续与生活搏斗。他们不是并不痛苦吗？他们使自己成为彻底的利己主义者，并虔信那一些里所担任，曾几何时怀疑过自己呢？这样一来，不是很轻松惬意吗？ 太平洋战争爆发后，当时的许多作家纷纷创造起了激发人们战斗欲望的小说作品，唯独太宰治不一样，他从不去碰战争题材，相反的，太宰治为了让那些为了度过困难时期而奋斗着的人们于百忙之中能得到片刻慰藉，于是发明了一种新的写作方式——私小说，并创作了令人轻松愉悦捧腹大笑的作品——《御伽草纸》。一直到日本二战战败以后，许多曾经崇拜战争的作家们纷纷“临阵倒戈”。太宰治发现，人们似乎根本没有意识到自己的罪恶，并创作了以二战为背景的战争题材小说《斜阳》：“被记者追捧者，鼓吹民主主义什么的，我不干，所有日本人都参与了战争。” 这是我向人类最后的求爱。尽管我对人类极度恐惧，但似乎始终割不断对人类的缘情，于是借着装傻这一缕细丝，来维系与人类的贯联。表面上我总是笑脸迎人，暗中则是拼了死命，战战兢兢，如履薄冰般才艰难万分做出这样的奉侍。 戴上人性的面具。 我却从人们动怒的面孔中发现了比狮子、鳄鱼、巨龙更可怕的动物本性。平常他们总是隐藏起这种动物本性，可一旦遇到某个时机，他们就会像那些温文尔雅地躺在草地上歇息的牛，蓦然甩动尾巴抽死肚皮上的牛虻一般，暴露出人的这种本性。 对讨厌的事不能说讨厌，而对喜欢的事呢，也是一样。 我对受人尊敬这一状态进行了如下定义：近于完美无缺地蒙骗别人，尔后又被某个全知全能之人识破真相，最终原形毕露，被迫当众出丑，以致于比死亡更难堪更困窘。 那时，我被男女佣人教唆者做出了可悲的丑事。事到如今我认为，对年幼者干出那种事情，无疑是人类所能犯下的罪孽中最丑恶最卑劣的行径。 不公平现象是必然存在的。这点是明摆着的事实。向人诉苦不过是徒劳，与其如此，不如默默承受。 相互欺骗，却又令人惊奇地不受到任何伤害，甚至于就好像没有察觉到彼此在欺骗似的，这种不加掩饰从而显得清冽、豁达的互不信任的例子，在人类生活中比比皆是。 第二手札 在迄今为止的生涯中，我曾经无数次祈望过自己被杀死，却从来也没有动过杀死别人的念头。这是因为我觉得，那样做只会给可怕的对手带来幸福的缘故。 “迷恋”、“被迷恋”这些措辞本身就是粗俗不堪而又戏弄人的说法，给人一种装腔作势的感觉。无论是多么“严肃”的场合，只要让这些词语抛头露面，忧郁的伽蓝就会顷刻间分崩离析，变得索然无味。 对人感到过分恐惧的人，反倒更加迫切地希望用自己地眼睛去看更可怕的妖怪；越是容易对事物感到胆怯的神经质的人，就越是渴望暴风雨降临得更加猛烈。 人啊，明明一点儿也不了解对方，错看对方，却视彼此为独一无二的挚友，一生不解对方的真性情，待一方撒手西去，还要为其哭泣，念诵悼词。 竭力想把觉得美的东西原封不动地描绘为美是幼稚和愚蠢乃至完全谬误的。 胆小鬼连幸福都会害怕，碰到棉花都会受伤，有时还被幸福所伤。 第三手札 人怎么能如此轻易地变得面目全非呢？这令我感到可耻，不，毋宁说是滑稽。 世上所有人的说法，总是显得拐弯抹角，含糊不清，其中有一种试图逃避责任似的微妙性和复杂性。 那以后我也尝试过画各种各样的画，但都远远及不上那记忆中的杰作，以致于我总是被一种失落感所折磨着，恍若整个胸膛都变成了一个空洞。 “可事实上，我是多么畏惧他们啊！我越是畏惧他们，就越是博得他们的喜欢，而越是博得他们的喜欢，我就越是畏惧他们，并不得不里他们远去。” 莫非在别人眼里，我那种畏惧他人、躲避他人、搪塞他人的性格，竟然与遵从俗话所说的那种“明哲保身、得过且过”的处世训条的做法，在表现形式上是相同的吗？ 所谓世间，又是什么呢？是人的复数吗？可哪儿存在着“世间”这个东西的实体呢？迄今为止，我一直以为它是一种苛烈、严酷、而且可怕的东西，并且一直生活在这种想法之中，如今被崛木一说，有一句话差点就脱口而出：“所谓的世人，不就是你吗？”……打那时候起，我开始萌发了一种可以称之为“思想”的念头：所谓世间，不就是个人吗？ 第二天也重复着同一件事情/只需遵从与昨天相同的习性/倘若愿意避免狂喜狂乐/大惊大悲就不会降临/躲开前方的挡路巨石/像蟾蜍一般迂回前进。 我知道有人是爱我的，但我好像缺乏爱人的能力。 所谓世间的真相，就是个人与个人之间的争斗，而且是即时即地的斗争。人需要在那种争斗中当场取胜，人是绝不可能服从他人的。即使是当努力，也会以努力的方式进行卑屈的反击。所以，人除了当场一决胜负外，不可能有别的生存方式。虽然人们提倡大义名分，但努力的目标毕竟是属于个人的，超越了个人之后依旧还是个人，世间的不可思议其实也就是个人的不可思议。 难道纯真无瑕的信赖之心真的是罪恶之源吗？难道纯真无瑕的信赖之心也算是罪过吗？ 我的不幸，恰恰在于我缺乏拒绝的能力。我害怕一旦拒绝别人，便会在彼此心里留下永远无法愈合的裂痕。 但是，被关进这所医院的人全是狂人，而逍遥在外的全都是正常人。我问神灵：难道不反抗也是一种罪过吗？ 我已丧失做人的资格。我已经彻底变成一个废人了。 结语大庭叶藏是善良的人，心思细腻敏感却怯懦敏感，性格孤独，渴望被爱，在世间选择了极端的生活方式，根源最主要还是软弱的灵魂，没有自我 自小充满幻想，对幻想破灭而大觉扫兴； 见到静子母女的背影，黯然离去，不忍打扰； 良子过于信任他人，与书商发生关系，而叶藏反复认定这是因为自己欣赏良子的纯真无暇，这不是她的错； 堀木作为叶藏的“朋友”，对叶藏一直抱有轻蔑的态度，叶藏看破不说破，并从堀木家中看到东京人真实的生活状态； 在药店老板的诱导下，吸毒上瘾； 不忍拒绝他人的请求； “我们所认识的阿叶，又诚实又乖巧，要是不喝酒的话，不，即使是喝酒……也是一个神一样的好孩子呐。” 想要成为一个人，就必须接受自己会犯错，会让人讨厌的事实，不要在乎他人的眼光。]]></content>
      <categories>
        <category>Reading</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SphereFace, CosFace, ArcFace]]></title>
    <url>%2F2019%2F07%2F13%2FSphereFace-CosFace-ArcFace%2F</url>
    <content type="text"><![CDATA[文中合成gif图不清晰，原图可在该页面下载获取 前言深度神经网络可将样本映射到超空间中的嵌入向量，然而若不在损失中增加几何约束，该超空间中的嵌入向量不具有几何意义。现对空间中嵌入向量间的余弦距离研究甚是火热，可用于open-set数据集的识别问题，特别是人脸验证问题。 所谓open-set与close-set，是指训练集中是否包含测试集中的类别，如人脸问题中，训练集不可能包含所有人的人脸数据，则如何识别未出现在训练集中的样本成为一个问题。 目前对嵌入向量的约束方案有如下几种 Centre Loss，惩罚样本对应特征与其对应类别中心之间的欧氏距离，以获得类内紧致性，本文不做介绍； Triplet Loss，在训练样本中寻找三元组，组合数量爆炸，且计算量较大，本文不做介绍； CosFace: Additive Cosine Margin； SphereFace: Multiplicative Angular Margin； ArcFace: Additive Angular Margin。 原理Maltilabel: Softmax + CrossEnt在多分类问题中，网络输出层一般设置为Softmax层 \tilde{y}^{(i)}_j = \frac{e^{f^{(i)}_j}}{\sum_k e^{f^{(i)}_k}} \tag{1}其中$f^{(i)}_j$表示Softmax层输入，一般为前一层网络的线性输出。损失函数一般采用交叉熵Cross Entropy L = \frac{1}{N} \sum_i - \log \tilde{y}^{(i)}_{y^{(i)}} \tag{2}其中，$N$为batchsize大小，$\tilde{y}^{(i)}_j$表示样本$X^{(i)}$预测为第$j$类的概率输出，$y^{(i)}$为其对应真实标签。 也即 L = \frac{1}{N} \sum_i - \log \frac{e^{f^{(i)}_{y^{(i)}}}}{\sum_k e^{f^{(i)}_k}} \tag{*} H(p, q) = \sum_x p(x) \log \frac{1}{q(x)}其中$p(x), q(x)$为概率分布。 Geometric: Modified Softmax Loss由于Softmax不具有几何意义，对其进行修改，在前一层与Softmax层之间添加线性层，即 g^{(i)}_j = W_j^T f^{(i)} + b_j \tag{3}其中 W_j^T f^{(i)} = ||W_j|| ||f^{(i)}|| \cos \theta^{(i)}_j \tag{4}$\theta^{(i)}_j$表示$W_j$与$f^{(i)}$夹角，$f^{(i)}$为上一层输出，其维数为$D$，则参数$W, b$维数应为$C \times D, C \times 1$。 此时类别$c_1$与$c_2$间决策平面为 W_{c_1}^T f^{(i)} + b_{c_1} = W_{c_2}^T f^{(i)} + b_{c_2}令$||W_j|| = 1, b_j = 0$，有 g^{(i)}_j = W_j^T f^{(i)} + b_j = ||f^{(i)}|| \cos \theta^{(i)}_j \tag{5}则判决平面为 ||f^{(i)}|| \cos \theta^{(i)}_{c_1} = ||f^{(i)}|| \cos \theta^{(i)}_{c_2}也即 \cos \theta^{(i)}_{c_1} = \cos \theta^{(i)}_{c_2}\tag{6}则此时向量夹角大小可作为判决平面，赋予角度上的几何意义，将$g^{(i)}$输入Softmax层与CrossEnt损失 \tilde{y}^{(i)}_j = \frac{e^{g^{(i)}_j}}{\sum_k e^{g^{(i)}_k}} = \frac{e^{||f^{(i)}|| \cos \theta^{(i)}_j}}{\sum_k e^{||f^{(i)}|| \cos \theta^{(i)}_k}}L = \frac{1}{N} \sum_i - \log \tilde{y}^{(i)}_{y^{(i)}}写成一个等式，在SphereFace一文中，称下式为Modified Softmax Loss L = \frac{1}{N} \sum_i - \log \frac{e^{||f^{(i)}|| \cos \theta^{(i)}_{y^{(i)}}}}{\sum_k e^{||f^{(i)}|| \cos \theta^{(i)}_k}} \tag{*1}其与Softmax对比如下 CosFace: Additive Cosine Margin在Modified Softmax Loss基础上添加Margin。 希望对于样本$X^{(i)}$，其对应的嵌入向量$f^{(i)}$与其所属类中心向量$W_{y^{(i)}}$间夹角$\theta^{(i)}_{y^{(i)}}$愈小愈好，由余弦函数单调性，可得 \min \theta^{(i)}_{y^{(i)}} \Rightarrow \max \cos \theta^{(i)}_{y^{(i)}}引入Margin参数$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$惩罚更大，也即 \phi^{(i)}_{c_{y^{(i)}}} = \arccos (\cos \theta^{(i)}_{c_{y^{(i)}}} - m) > \theta^{(i)}_{c_{y^{(i)}}}从而 \cos \phi^{(i)}_{c_{y^{(i)}}} = \cos \theta^{(i)}_{c_{y^{(i)}}} - m < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{7}所以 m > 0此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为 \cos \theta^{(i)}_{c_{y^{(i)}}} - m = \cos \theta^{(i)}_{c_j}那么$(*1)$更新为 L = \frac{1}{N} \sum_i - \log \frac{e^{||f^{(i)}|| (\cos \theta^{(i)}_{y^{(i)}} - m)}}{e^{||f^{(i)}|| (\cos \theta^{(i)}_{y^{(i)}} - m)} + \sum_{k \neq y^{(i)}} e^{||f^{(i)}|| \cos \theta^{(i)}_k}}, 其中m > 0 \tag{8}由于决策平面取决于角度，与$||f^{(i)}||$分布无关，故设置为常数$s$，有Additive Cosine Margin Loss L = \frac{1}{N} \sum_i - \log \frac{e^{s (\cos \theta^{(i)}_{y^{(i)}} - m)}}{e^{s (\cos \theta^{(i)}_{y^{(i)}} - m)} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m > 0 \tag{*2}s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}$m$典型值为 m = 0.35 实际上，$s$可控制决策平面的陡峭程度，如在二分类问题中，采用Softmax进行决策时 \tilde{y}^{(i)}_1 = \frac{e^{s f^{(i)}_1}}{e^{s f^{(i)}_1} + e^{s f^{(i)}_2}} = \frac{1}{1 + e^{-s (f^{(i)}_1 - f^{(i)}_2)}}记$z^{(i)}_1 = f^{(i)}_1 - f^{(i)}_2$，有Sigmoid函数 \tilde{y}^{(i)}_1 = \frac{1}{1 + e^{-s z^{(i)}_1}}设置不同的$s$参数，得到图像如下 从另一角度理解，可设$s = \frac{1}{\sigma^2}$，则$s$也可用于描述分布的方差，本文中，这里指角度的分布方差。 SphereFace: Multiplicative Angular Margin与CosFace: Additive Cosine Margin思路一致，在Modified Softmax Loss基础上添加Margin，不同的是对角度增加乘法裕度。 引入乘子$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$惩罚更大 \phi^{(i)}_{c_{y^{(i)}}} = m \theta^{(i)}_{y^{(i)}}\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos m \theta^{(i)}_{y^{(i)}} < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{9}所以$m &gt; 1$，原文中去整数，即$m \geq 2$，同时，沿用CosFace中对$||f^{(i)}||$处理，$(*1)$更新为 L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos m \theta^{(i)}_{y^{(i)}}}}{e^{s \cos m \theta^{(i)}_{y^{(i)}}} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m(\text{integer}) \geq 2 \tag{*3}s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为 m \theta^{(i)}_{c_{y^{(i)}}} = \theta^{(i)}_{c_j}$m$典型值为 m = 2 ArcFace: Additive Angular Margin与CosFace: Additive Cosine Margin思路一致，在Modified Softmax Loss基础上添加Margin，对角度增加加法裕度。 引入$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$惩罚更大 \phi^{(i)}_{c_{y^{(i)}}} = \theta^{(i)}_{y^{(i)}} + m\cos \phi^{(i)}_{c_{y^{(i)}}} = \cos (\theta^{(i)}_{y^{(i)}} + m) < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{9}所以$m &gt; 0$，同时，沿用CosFace中对$||f^{(i)}||$处理，$(*1)$更新为 L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos (\theta^{(i)}_{y^{(i)}} + m)}}{e^{s \cos (\theta^{(i)}_{y^{(i)}} + m)} + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}}, 其中m > 0 \tag{*4}s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}$m$典型值为 m = 0.5此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为 \theta^{(i)}_{c_{y^{(i)}}} + m = \theta^{(i)}_{c_j} 可改进之处CosMulFace?好像还有一个空余的位置可以添加参数，笑:-)。 与CosFace: Additive Cosine Margin思路一致，在Modified Softmax Loss基础上添加Margin，不同的是对余弦值增加乘法裕度。 引入乘子$m$，使$\theta^{(i)}_{y^{(i)}}$更大，更新为$\phi^{(i)}_{c_{y^{(i)}}}$，其余$\theta^{(i)}_j$不变，从而使得对类别$c_{y^{(i)}}$惩罚更大 \phi^{(i)}_{c_{y^{(i)}}} = \arccos (m \cos \theta^{(i)}_{c_{y^{(i)}}}) > \theta^{(i)}_{c_{y^{(i)}}}\cos \phi^{(i)}_{c_{y^{(i)}}} = m \cos \theta^{(i)}_{c_{y^{(i)}}} < \cos \theta^{(i)}_{c_{y^{(i)}}} \tag{10}由于函数cos(x)有 \begin{cases} \cos x \geq 0 & x \in [-\frac{\pi}{2} + 2k\pi, \frac{\pi}{2} + 2k\pi] \\ \cos x \leq 0 & x \in [\frac{\pi}{2} + 2k\pi, \frac{3\pi}{2} + 2k\pi] \end{cases}, 且 -1 \leq \cos x \leq 1所以为使得对于乘子$m$，均有式$(9)$成立，令 \cos \theta^{(i)}_j := \cos \theta^{(i)}_j - 1 \tag{11}所以$m &gt; 1$，同时，沿用CosFace中对$||f^{(i)}||$处理，$(*1)$更新为 L = \frac{1}{N} \sum_i - \log \frac{e^{s m (\cos \theta^{(i)}_{y^{(i)}} - 1)}}{e^{s m (\cos \theta^{(i)}_{y^{(i)}} - 1)} + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}}, 其中 m > 1 \tag{*5}s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||}此时类别$c_{y^{(i)}}$与其他类$c_j$间的判别平面为 m (\cos \theta^{(i)}_{c_{y^{(i)}}} - 1) = \cos \theta^{(i)}_{c_j} - 1 \tag{12} AdaptiveFace: Adaptive Margin?Margin必须为固定常数？ 设置可变参数$m_i, i = 1, \cdots, 4$ 可令 L = \frac{1}{N} \sum_i - \log \frac {e^{s \cdot m^4 \left[\cos (m^1 \theta^{(i)} + m^2) - m^3 - 1\right] }} {e^{s \cdot m^4 \left[\cos (m^1 \theta^{(i)} + m^2) - m^3 - 1\right] } + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}} \tag{*6} \begin{cases} \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\ m_1 > 1 \\ m_2 > 0 \\ m_3 > 0 \\ m_4 > 1 \end{cases}反向传播时，同时更新$m_i, i = 1, \cdots, 4$。 注意点：参数下限，每次参数更新后，需对其限制，即剪裁。 f(x) = \begin{cases} \text{min} & x < \text{min} \\ x & otherwise \\ \text{max} & x > \text{max} \end{cases}船新版本 在Modified Softmax Loss基础上，增加损失项 L = \frac{1}{N} \sum_i - \log \frac{e^{s \cos \theta^{(i)}_{y^{(i)}} }}{e^{s \cos \theta^{(i)}_{y^{(i)}} } + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}} + \lambda || \sum_j W_j ||_2 \tag{*7} s.t. \begin{cases} \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\ || W_j ||_2 = 1 \\ \end{cases}即使得各类别所属向量的矢量和长度趋向$0$，由于最小化损失项$|| \sum_j W_j ||_2$，其最优解为 W_j = \vec{0}或者说$W_j$长度衰减至$0$，所以需要加上限制，在每次参数迭代后，重新归一化该向量，使其为单位向量，即 W_j := \frac{W_j}{||W_j||}CosFace + SphereFace + ArcFace + CosMulFace综合CosFace, SphereFace, ArcFace, CosMulFace, 得到 L = \frac{1}{N} \sum_i - \log \frac {e^{s \cdot m_4 \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 - 1 \right] }} {e^{s \cdot m_4 \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 - 1 \right] } + \sum_{k \neq y^{(i)}} e^{s (\cos \theta^{(i)}_k - 1)}} \tag{**} s.t. \qquad \begin{cases} \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} \\ m_1 \geq 2 \\ m_2 > 0 \\ m_3 > 0 \\ m_4 > 1 \end{cases}通过组合可能获得更好的实验结果。 方法对比ArcFace与CosFace, SphereFace对比有如下特点 数值计算稳定 几何意义不同，决策平面区分度更好 计算方法 初始化各类中心，记作矩阵$W_{C \times D}$，其中$C$表示类别数目， $D$表示特征维数； 迭代过程中，某批输入的样本记作$X_N$，其中$N$表示批次数据数； 计算该批中各样本$X^{(i)}$提取的特征$f^{(i)}$到各类别中心$W_j$的余弦值$\cos \theta^{(i)}_j$，保存为矩阵$Cos_{N \times C}$； 各样本真实标签对应的余弦值加上相应Margin，即 \cos \phi^{(i)}_j = \begin{cases} m_4 (\text{monocos} (m_1 \theta^{(i)}_j + m_2) - m_3 - 1) & j = y^{(i)} \\ \cos \theta^{(i)}_j - 1 & \text{otherwise} \end{cases} \tag{14} monocos在cos函数的单调性问题中说明 将计算得到的矩阵代入Softmax层，计算该样本属于各类别的概率 \tilde{y}^{(i)}_j = \frac{e^{\cos \phi^{(i)}_j}}{\sum_k e^{\cos \phi^{(i)}_k}} \tag{15} 代入Cross Entropy计算损失值 L^{(i)} = - \log \tilde{y}^{(i)}_{y^{(i)}} \tag{16} 则该批次的损失值为 L = \frac{1}{N} \sum_i L^{(i)} \tag{17} 代码实现详情查看isLouisHsu/Toy-Problem-based-on-MNIST - Github。 在实现过程中，有两个注意点： 反余弦函数arccos(x)在$x = \pm 1$处不可导问题； 余弦函数cos(x)的单调性问题； 对于上述两个问题，进行以下处理 arccos函数不可导点问题 由于函数$\arccos(x)$的导函数为 \frac{d}{dx} \arccos(x) = - \frac{1}{\sqrt{1 - x^2}} \tag{18}特征$f^{(i)}$与各类中心$W_j$余弦值范围为 \cos \theta^{(i)}_j \in [-1, 1]则当$\cos \theta^{(i)}_j = \pm 1$时 \lim_{\theta \rightarrow \pm 1} \frac{d}{dx} \arccos(x) = \infty则无法使用BP算法进行参数更新，因此，使用泰勒展开式近似计算$\theta^_j$ \arccos x = \frac{\pi}{2} - x - \frac{1}{2} \cdot \frac{x^3}{3} - \cdots - \frac{(2n-1)!}{(2n)!} \cdot \frac{x^{2n+1}}{2n+1}- \cdots \tag{19} 123456789def taylorArccos(x, n): general_term = lambda x, n: (math.factorial(2 * n - 1) /\ math.factorial(2 * n)) *\ (x**(2 * n + 1) / (2 * n + 1)) y = np.pi / 2 - x for i in range(1, n): y -= general_term(x, i) return y cos函数的单调性问题 由于特征$f^{(i)}$与各类中心$W_j$余弦值范围为 \cos \theta^{(i)}_j \in [-1, 1]利用反三角函数arccos(x)计算其角度后，有 \theta^{(i)}_j \in [0, \pi]仅考虑余弦函数内部部分，添加Margin后，应有 \phi^{(i)}_j = m_1 \theta^{(i)}_j + m_2 \in [m_2, m_2 + m_1 \pi]此时仍然满足 \phi^{(i)}_j > \theta^{(i)}_j然而经过余弦函数后，可能由于其单调性问题，不一定满足下式 \cos \phi^{(i)}_j < \cos \theta^{(i)}_j所以定义如下函数，使其在$[0, \infty]$单调递减 \text{monocos}(\theta) = \cos (\theta - n \pi) - 2n 其中 n = \lfloor{} \frac{\theta}{\pi} \rfloor{} \tag{19}此时必满足 \text{monocos} \phi^{(i)}_j < \text{monocos} \theta^{(i)}_j 由于$\phi^{(i)}_j &gt; \theta^{(i)}_j$，所以满足$[0, \infty]$即可，实际上，该函数在$[-\infty, \infty]$均单调递减。 1234def monocos(x): n = x // np.pi y = np.cos(x - np.pi*n) - 2*n return y 其函数图像如下 网络层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class CosineLayer(nn.Module): """ Attributes: weight: &#123;Parameter(num_classes, feature_size)&#125; """ def __init__(self, num_classes, feature_size): super(CosineLayer, self).__init__() self.weights = Parameter(torch.Tensor(num_classes, feature_size)) nn.init.xavier_uniform_(self.weights) def forward(self, x): """ Params: x: &#123;tensor(N, feature_size)&#125; Notes: \cos \theta^&#123;(i)&#125;_j = \frac&#123;W_j^T f^&#123;(i)&#125;&#125;&#123;||W_j|| ||f^&#123;(i)&#125;||&#125; """ x = F.linear(F.normalize(x), F.normalize(self.weights)) return xclass NetworkMargin(nn.Module): def __init__(self, num_classes, feature_size): super(NetworkMargin, self).__init__() self.pre_layers = nn.Sequential( nn.Conv2d( 1, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d( 64, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d( 64, feature_size, 7), ) self.cosine_layer = CosineLayer(num_classes, feature_size) def get_feature(self, x): x = self.pre_layers(x) x = x.view(x.shape[0], -1) return x def forward(self, x): x = self.get_feature(x) x = self.cosine_layer(x) return x 损失函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class MarginProduct(nn.Module): """ Notes: $$ \text&#123;softmax&#125; = \frac&#123;1&#125;&#123;N&#125; \sum_i -\log \frac&#123;e^&#123;\tilde&#123;y&#125;_&#123;y_i&#125;&#125;&#125;&#123;\sum_i e^&#123;\tilde&#123;y&#125;_i&#125;&#125; $$ $\text&#123;where&#125;$ $$ \tilde&#123;y&#125; = \begin&#123;cases&#125; s(m4 \cos(m_1 \theta_&#123;j, i&#125; + m_2) + m_3) &amp; j = y_i \\ s(m4 \cos( \theta_&#123;j, i&#125;)) &amp; j \neq y_i \end&#123;cases&#125; $$ """ def __init__(self, s=32.0, m1=2.00, m2=0.50, m3=0.35, m4=2.00): super(MarginProduct, self).__init__() self.s = s self.m1 = m1 self.m2 = m2 self.m3 = m3 self.m4 = m4 def forward(self, cosTheta, label): """ Params: cosTheta: &#123;tensor(N, n_classes)&#125; 每个样本(N)，到各类别(n_classes)矢量的余弦值 label: &#123;tensor(N)&#125; Returns: output: &#123;tensor(N, n_classes)&#125; """ one_hot = torch.zeros(cosTheta.size(), device='cuda' if \ torch.cuda.is_available() else 'cpu') one_hot.scatter_(1, label.view(-1, 1).long(), 1) # theta = torch.acos(cosTheta) theta = arccos(cosTheta) cosPhi = self.m4 * (monocos(self.m1*theta + self.m2) - self.m3 - 1) output = torch.where(one_hot &gt; 0, cosPhi, cosTheta - 1) output = self.s * output return outputclass MarginLoss(nn.Module): def __init__(self, s=32.0, m1=2.00, m2=0.50, m3=0.35, m4=2.00): super(MarginLoss, self).__init__() self.margin = MarginProduct(s, m1, m2, m3, m4) self.crossent = nn.CrossEntropyLoss() def forward(self, pred, gt): output = self.margin(pred, gt) loss = self.crossent(output, gt) return loss 实验注意，以下实验网络的参数初始均相同。 1.设置不同参数，对比实验结果 L = \frac{1}{N} \sum_i - \log \frac {e^{s \cdot \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 \right] }} {e^{s \cdot \left[\cos (m_1 \theta^{(i)}_{y^{(i)}} + m_2) - m_3 \right] } + \sum_{k \neq y^{(i)}} e^{s \cos \theta^{(i)}_k}} \tag{**}s.t. \qquad \cos \theta^{(i)}_j = \frac{W_j^T f^{(i)}}{||W_j||||f^{(i)}||} Margin type Modified CosFace SphereFace ArcFace ArcFace ArcFace ArcFace $s$ 8 8 8 16 8 4 1 $m_1(m_1&gt;=2)$ 1 1 2 1 1 1 1 $m_2(m_2&gt;0)$ 0 0 0 0.5 0.5 0.5 0.5 $m_3(m_3&gt;0)$ 0 0.35 0 0 0 0 0 参数$s$对实验结果影响不大； CosFace, SphereFace, ArcFace三种损失作用下，角度的区分度均比Modified Softmax效果好。 嵌入向量维度为2 嵌入向量维度为3 使用imageio.mimread()函数，读取出.gif会改变原图颜色，很迷。 2. 改进的方法实验 Margin type CosMulFace CosMulFace CosMulFace AdaptiveFace $s$ 8 8 8 8 $m_1(m_1&gt;=2)$ 1 1 1 / $m_2(m_2&gt;0)$ 0 0 0 / $m_3(m_3&gt;0)$ 0 0 0 / $m_4(m_4 &gt; 1)$ 2.00 3.00 4.00 / CosMulFace在$m=2, 4$时效果良好。 AdaptiveFace得到自适应参数如下(?) \begin{cases} m_1 = 1 \\ m_2 = 0 \\ m_3 = 0 \\ m_4 = 6.8345 \\ \end{cases} 嵌入向量维度为2 嵌入向量维度为3 3. 传新版本可见其角度分布更加均匀，从而区分度更大。 嵌入向量维度为2 嵌入向量维度为3 上图和#$%@%一样，如下为scatter_lda8原图 数据类别过多可能不够明显，现选择4类，在设置$\lambda=0$与$\lambda=16$时，各类别的三维分布如下两图，区别很明显 Reference ArcFace: Additive Angular Margin Loss for Deep Face Recognition - arxiv.org CosFace: Large Margin Cosine Loss for Deep Face Recognition - arxiv.org SphereFace: Deep Hypersphere Embedding for Face Recognition - arxiv.org deepinsight/insightface - Github wy1iu/sphereface - Github]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python迭代器与生成器]]></title>
    <url>%2F2019%2F07%2F11%2FPython%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言在读取大数据量的文件时，使用迭代器和生成器可减少内存开销。 迭代器(Iterator)迭代是Python访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 内建函数iter()与next()该函数将可迭代对象转换为迭代器，支持的数据容器对象如string、list、dict、tuple等，使用方法如下1234567891011121314151617181920212223242526272829303132333435363738394041424344&gt;&gt;&gt; # 创建可迭代对象&gt;&gt;&gt; a = "iterator"&gt;&gt;&gt; b = [_ for _ in a]&gt;&gt;&gt; c = dict(zip(a, range(len(a))))&gt;&gt;&gt; d = tuple(a)&gt;&gt;&gt; a'iterator'&gt;&gt;&gt; b['i', 't', 'e', 'r', 'a', 't', 'o', 'r']&gt;&gt;&gt; c&#123;'i': 0, 't': 5, 'e': 2, 'r': 7, 'a': 4, 'o': 6&#125;&gt;&gt;&gt; d('i', 't', 'e', 'r', 'a', 't', 'o', 'r')&gt;&gt;&gt;&gt;&gt;&gt; # 调用`iter()`函数将可迭代对象转换为迭代器&gt;&gt;&gt; all = [a, b, c, d]&gt;&gt;&gt; iterAll = list(map(iter, all))&gt;&gt;&gt; all['iterator', ['i', 't', 'e', 'r', 'a', 't', 'o', 'r'], &#123;'i': 0, 't': 5, 'e': 2, 'r': 7, 'a': 4, 'o': 6&#125;, ('i', 't', 'e', 'r', 'a', 't', 'o', 'r')]&gt;&gt;&gt; iterAll[&lt;str_iterator object at 0x00000216C399D6A0&gt;, &lt;list_iterator object at 0x00000216C399D6D8&gt;, &lt;dict_keyiterator object at 0x00000216C37672C8&gt;, &lt;tuple_iterator object at 0x00000216C399D710&gt;]&gt;&gt;&gt;&gt;&gt;&gt; # 反复调用`next()`函数取出迭代器值&gt;&gt;&gt; next(iterAll[0])'i'&gt;&gt;&gt; next(iterAll[0])'t'&gt;&gt;&gt; next(iterAll[0])'e'&gt;&gt;&gt; next(iterAll[0])'r'&gt;&gt;&gt; next(iterAll[0])'a'&gt;&gt;&gt; next(iterAll[0])'t'&gt;&gt;&gt; next(iterAll[0])'o'&gt;&gt;&gt; next(iterAll[0])'r'&gt;&gt;&gt; # 迭代结束&gt;&gt;&gt; next(iterAll[0])Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration for语法糖实际中对可迭代对象进行迭代时，上述这般麻烦。在Python的循环语句for已对其进行包装，内部调用函数iter()和next()，返回可迭代对象元素12345678910111213&gt;&gt;&gt; a'iterator'&gt;&gt;&gt; for i in a:... print(i)...iterator 自定义迭代器(敲黑板划重点！)将自定义类定义为可迭代对象的实现方法是，实现魔术方法__iter__()与__next__()。 例如，要求返回斐波那契数列前20个值。123456789101112131415161718192021class Fibonacci: def __init__(self, m): self.m = m self.n, self.a, self.b = 0, 0, 1 def __iter__(self): return self def __next__(self): if self.n &lt; self.m: t = self.b self.a, self.b = self.b, self.a + self.b self.n += 1 return t else: raise StopIteration 在命令行中执行1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; from iter import Fibonacci&gt;&gt;&gt; &gt;&gt;&gt; g = Fibonacci(20)&gt;&gt;&gt; g&lt;iter.Fibonacci object at 0x000001F359927E10&gt;&gt;&gt;&gt; &gt;&gt;&gt; while True:... try:... next(g)... except StopIteration:... break...11235813213455891442333776109871597258441816765 在Pytorch中，数据集Dataset定义时，重写函数__getitem__()与__len__()，并不是可迭代对象，而Dataloader为可迭代对象，详细源码可查看Github: Pytorch 生成器(Generator)调用一个生成器函数，返回的是一个迭代器对象。在调用生成器运行的过程中，每次遇到yield时函数会暂停并保存当前所有的运行信息，返回yield的值, 并在下一次执行next()方法时从当前位置继续运行。 推导式定义生成器语法类似列表推导式，不同的是将[]改为()123456789&gt;&gt;&gt; g = (i for i in range(20))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x00000161D16F44F8&gt;&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)2 将函数定义为生成器利用函数打印斐波那契数列123456def fibonacci(m): n, a, b = 0, 0, 1 while n &lt; m: print(b) a, b = b, a + b n = n + 1 若需得到生成器，将print()改为yield()即可123456def fibonacci_gen(m): n, a, b = 0, 0, 1 while n &lt; m: yield(b) a, b = b, a + b n = n + 1 在命令行中执行1234567891011121314&gt;&gt;&gt; from iter import fibonacci_gen&gt;&gt;&gt; g = fibonacci_gen(20)&gt;&gt;&gt; g&lt;generator object fibonacci_gen at 0x000001F036DA44F8&gt;&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)2&gt;&gt;&gt; next(g)3&gt;&gt;&gt; next(g)5 Reference Python3 迭代器与生成器 - 菜鸟教程 python 迭代器和生成器详解 - 博客园]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[WSL - Windows Subsystem for Linux]]></title>
    <url>%2F2019%2F07%2F11%2FWSL-Windows-Subsystem-for-Linux%2F</url>
    <content type="text"><![CDATA[前言Windows远程登陆服务器，需要借助xshell等软件。其实安装完子系统后，即可使用ssh登录，并且子系统与原系统隔离较好，不会产生影响。 安装 呼叫小娜，打开“启用或关闭Windows功能”，勾选“适用于Linux的Windows子系统”，并重启； 在巨硬软件商城(Microsoft Store)中下载合适的WSL，选择喜好的WSL进行安装 Ubuntu 16.04 LTS Ubuntu 18.04 LTS OpenSUSE Leap 15 OpenSUSE Leap 42 SUSE Linux Enterprise Server 12 SUSE Linux Enterprise Server 15 Kali Linux Debian GNU/Linux Fedora Remix for WSL Pengwin Pengwin Enterprise Alpine WSL 启动WSL，并添加用户和密码，即可使用 Reference Windows Subsystem for Linux Installation Guide for Windows 10 - Microsoft 如何在Windows 10上开启WSL之旅 关于WSL(Windows上的Linux子系统)的简单介绍及安装 - CNBLOG]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Useful Terminal Control Sequences]]></title>
    <url>%2F2019%2F05%2F28%2FUseful-Terminal-Control-Sequences%2F</url>
    <content type="text"><![CDATA[前言ANSI定义了用于屏幕显示的Escape屏幕控制码，打印输出到终端时，可指定输出颜色、格式等。 基本格式1\033[&lt;background color&gt;;&lt;front color&gt;m string to print \033[0m \033[ xxxx m为一个句段； \033[0m关闭所有属性； 光标控制 ANSI控制码 含义 \033[nA 光标上移n行 \033[nB 光标下移n行 \033[nC 光标右移n行 \033[nD 光标左移n行 \033[y;xH 设置光标位置 \033[2J 清屏 \033[K 清除从光标到行尾的内容 \033[s 保存光标位置 \033[u 恢复光标位置 \033[?25l 隐藏光标 \033[?25h 显示光标 颜色控制 ANSI控制码 含义 \033[m NONE \033[0;32;31m RED \033[1;31m LIGHT RED \033[0;32;32m GREEN \033[1;32m LIGHT GREEN \033[0;32;34m BULE \033[1;34m LIGHT BLUE \033[1;30m GRAY \033[0;36m CYAN \033[1;36m LIGHT CYAN \033[0;35m PURPLE \033[1;35m LIAGHT PURPLE \033[0;33m BROWN \033[1;33m YELLO \033[0;37m LIGHT GRAY \033[1;37m WHITE 背景色与字体颜色符号不同 背景色 字体色 40: 黑 30: 黑 41: 红 31: 红 42: 绿 32: 绿 43: 黄 33: 黄 44: 蓝 34: 蓝 45: 紫 35: 紫 46: 深绿 36: 深绿 47: 白色 37: 白色 格式控制 ANSI控制码 含义 \033[0m 关闭所有属性 \033[1m 设置高亮度 \033[4m 下划线 \033[5m 闪烁 \033[7m 反显 \033[8m 消隐 举例例如用python打印输出123456print("\007") # 发出提示音print("\033[42:31m hello! \033[0m") # 绿底红字` hello! ` print("\033[4m") # 开启下划线print("\033[42:31m hello! \033[0m") # 下划线绿底红字` hello! ` print("\033[0m") # 关闭所有格式print("\033[2J") # 清屏 Reference “\033”(ESC)的用法-ANSI的Esc屏幕控制 - CSDN Useful Terminal Control Sequences - student.cs.uwaterloo.ca]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Support Vector Machine]]></title>
    <url>%2F2019%2F05%2F27%2FSupport-Vector-Machine%2F</url>
    <content type="text"><![CDATA[前言Github: isLouisHsu/Basic-Machine-Learning-Algorithm/algorithm/p97_svm.py 补一补支持向量机的笔记。 支持向量机(SVM)为有监督学习算法，可用于回归、分类甚至聚类(支持向量聚类)，其主要特点为 将样本表示为超空间中的点； 求取支持向量，其余样本点对超平面无影响； 对于线性不可分问题，利用核函数映射到高维空间，使其线性可分； 基本原理以下介绍支持向量机的基本原理，首先对一些概念作补充 概念补充 $n$维空间与超平面 如对于$n$维数据集，其所在空间即为$n$维欧式空间，则在该空间中，余维度为$1$的线性子空间，或称$n-1$维仿射子空间，称为超平面，为$n-1$维，可由下式确定 w^T x + b = 0 \tag{1} 其中$w,x$为$n$维列向量，$x$表示超平面上的点($n$维)，$w$表示超平面的法向量，决定超平面的方向，$b$为实数。 x = (x_1, x_2, \cdots, x_n)^Tw = (w_1, w_2, \cdots, w_n)^T 例如$3$维空间中的$2$维平面方程为 Ax + By + Cz + D = 0 点到超平面的距离 对于样本空间中任一点$x$，到超平面$\mathcal{P}_{w,b}$的距离，可表示为 \mathcal{D} = \frac{|w^T x + b|}{||w||} \tag{2} 其中$||w||$为向量$w$的$2$范数，即 ||w|| = \sqrt{w_1^2 + w_2^2 + \cdots + w_n^2} \tag{3} 证明：假设超平面上一向量点为$x_0$，则向量$x-x_0$在单位法向量$\frac{w}{||w||}$上的投影$d$，即为向量点到超平面的距离 d = |\frac{w^T}{||w||} (x-x_0)|而 w^T x + b = 0 \Rightarrow w^T = -b所以 d = | \frac{- w^T x_0 - b}{||w||} | = \frac{|w^T x_0 + b|}{||w||} 核技巧定义在前言中说到支持向量机使用核函数映射低维空间到高维空间，这是如何做到的？假设有两个$n$维向量$x_i, x_j$，设有映射$\mathit{\Phi}$使其维度扩张到$n’$维，即 x' = \mathit{\Phi}(x)则定义核函数为 \kappa(x_i, x_j) = \mathit{\Phi}(x_i)^T \mathit{\Phi}(x_j) \tag{4}常用核函数 线性(linear)核函数\kappa(x_i, x_j) = x_i^T x_j 多项式(Polynomial)核函数\kappa(x_i, x_j) = (\gamma x_i^T x_j + c)^n 高斯(Gaussian)核函数\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}} 拉普拉斯(Laplace)核函数\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||}{\sigma}} Sigmoid核函数\kappa(x_i, x_j) = \tanh (\gamma x_i^T x_j + c) 等等。 至于如何选取核函数，需要技术人员一定的先验知识，或者使用超参数搜索确定。 实例分析核函数的作用 多项式核函数 指定多项式核函数参数$ \gamma = 1, c = 0, n = 2 $，即 \kappa(x_i, x_j) = (x_i^T x_j)^2 \tag{4.a} 设原始空间为$2$维，即 x_i = (x_{i1}, x_{i2})^T 代入核函数$(4.a)$，有 \kappa(x_i, x_j) = \left[(x_{i1}, x_{i2}) (x_{j1}, x_{j2})^T \right]^2 = (x_{i1}x_{j1} + x_{i2}x_{j2})^2= x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2 把以上$3$项多项式表示成$3$维向量内积，即 \kappa(x_i, x_j) = \left[ \begin{matrix} x_{i1}^2 & \sqrt{2}x_{i1}x_{i2} & x_{i2}^2 \end{matrix} \right] \left[ \begin{matrix} x_{j1}^2 & \sqrt{2}x_{j1}x_{j2} & x_{j2}^2 \end{matrix} \right]^T 由上式与式$(4)$可得，该核函数对应的映射函数为 \mathit{\Phi}(x) = \left[ \begin{matrix} x_1^2 & \sqrt{2}x_1x_2 & x_2^2 \end{matrix} \right]^T \tag{4.b} 这样就把$2$维空间中的点映射到了$3$维空间，作图如下。 123456789101112131415161718192021222324import numpy as npimport matplotlib.pyplot as pltX = np.array([[1,1],[1,2],[1,3],[1,4],[2,1],[2,2],[3,1],[4,1],[5,1],[5,2],[6,1],[6,2],[6,3],[6,4],[3,3],[3,4],[3,5],[4,3],[4,4],[4,5]])Y=np.array([-1] * 14 + [1] * 6)plt.figure(0)plt.scatter(X[:, 0], X[:, 1], c=Y)# 将数据映射到高维后显示# 映射函数为Φ(x)=[x1^2 √2*x1*x2 x2^2]FX = np.zeros((0, 3))for i in range(X.shape[0]): tmp = np.array([X[i, 0]**2, np.sqrt(2)*X[i, 0]*X[i, 1], X[i, 1]**2]).reshape(1, -1) FX = np.r_[FX, tmp]fig = plt.figure(1)figAx = Axes3D(fig)figAx.scatter(FX[:, 0], FX[:, 1], FX[:, 2], c=Y)plt.show() 高斯核函数 \kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}} = e^{c||x_i - x_j||^2} \tag{4.c} 其中包含指数函数，其级数展开式为 e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!} = \sum_{i=0}^{n} \frac{x^i}{i!} + R(n) 因此可将样本点映射到无穷维度。 输入维度为$1$ ||x_i - x_j||^2 = (x_i - x_j)^2 =x_i^2 - 2x_ix_j + x_j^2 则代入核函数$(4.c)$得到 e^{c||x_i - x_j||^2} = e^{c(x_i^2 - 2x_ix_j + x_j^2)} = e^{c(x_i^2 + x_j^2)} e^{-2c x_i x_j} 其中 e^{-2c x_i x_j} = \sum_{n=0}^{\infty} \frac{(-2c)^n x_i^n x_j^n}{n!} = 1 - 2cx_ix_j + \frac{4 c^2 x_i^2 x_j^2}{2} - \frac{8 c^3 x_i^3 x_j^3}{6} + \cdots = \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_i^n}, \cdots \end{matrix} \right] \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_j^n}, \cdots \end{matrix} \right]^T 所以 e^{c||x_i - x_j||^2} = e^{c(x_i^2 + x_j^2)} \sum_{n=0}^{\infty} \frac{(-2c)^n x_i^n x_j^n}{n!} = \left\{ e^{cx_i^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_i^n}, \cdots \end{matrix} \right] \right\} \left\{ e^{cx_j^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x_j^n}, \cdots \end{matrix} \right] \right\}^T 则可得映射函数为 \mathit{\Phi}(x) = e^{cx^2} \left[\begin{matrix} \cdots, \sqrt{\frac{(-2c)^n}{n!}x^n}, \cdots \end{matrix} \right] \tag{4.d} 输入维度为$d$ ||x_i - x_j||^2 = (x_i - x_j)^T (x_i - x_j) = x_i^T x_i - 2 x_i^T x_j + x_j^T x_j 代入核函数$(4.c)$得到 e^{c||x_i - x_j||^2} = e^{c(x_i^T x_i + x_j^T x_j)} e^{-2c x_i^T x_j} 其中 e^{-2c x_i^T x_j} = \sum_{n=0}^{\infty} \frac{(-2c)^n (x_i^T x_j)^n}{n!} 故 e^{c||x_i - x_j||^2} = e^{c(x_i^T x_i + x_j^T x_j)} \sum_{n=0}^{\infty} \frac{(-2c)^n (x_i^T x_j)^n}{n!} 特殊化，对于$d=2$，展开至$3$阶，得到 e^{c||x_i - x_j||^2} = e^{c(x_{i1}^2 + x_{i2}^2 + x_{j1}^2 + x_{j2}^2)} \left[ 1 - 2c (x_{i1}x_{j1} + x_{i2}x_{j2}) + 4c^2 (x_{i1}x_{j1} + x_{i2}x_{j2})^2 -8c^3 (x_{i1}x_{j1} + x_{i2}x_{j2})^3 \right] 其中 \begin{cases} x_{i1}x_{j1} + x_{i2}x_{j2} \\ (x_{i1}x_{j1} + x_{i2}x_{j2})^2 = x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2 \\ (x_{i1}x_{j1} + x_{i2}x_{j2})^3 = x_{i1}^3 x_{j1}^3 + 3x_{i1}x_{j1}x_{i2}^2x_{j2}^2 + 3x_{i1}^2x_{j1}^2x_{i2}x_{j2} + x_{i2}^3 x_{j2}^3 \\ \end{cases} 带入后得 e^{c||x_i - x_j||^2} = e^{c(x_{i1}^2 + x_{i2}^2 + x_{j1}^2 + x_{j2}^2)} [ 1 - 2c (x_{i1}x_{j1} + x_{i2}x_{j2}) + 4c^2 (x_{i1}^2 x_{j1}^2 + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^2 x_{j2}^2) - 8c^3 (x_{i1}^3 x_{j1}^3 + 3x_{i1}x_{j1}x_{i2}^2x_{j2}^2 + 3x_{i1}^2x_{j1}^2x_{i2}x_{j2} + x_{i2}^3 x_{j2}^3) ] 所以映射函数为 \mathit{\Phi}(x) = e^{c(x_1^2 + x_2^2)} \left[ \begin{matrix} 1, \sqrt{-2c} x_1, \sqrt{-2c} x_2, \sqrt{4c^2} x_1^2, \sqrt{8c^2} x_1x_2, \sqrt{4c^2} x_2^2, \sqrt{-8c^3}x_1^3, \sqrt{-24c^3} x_1 x_2^2, \sqrt{-24c^3} x_1^2 x_2, \sqrt{-8c^3} x_1^3 \end{matrix} \right]^T \tag{4.e} 即升维后维数为$10$。 线性支持向量机对于线性可分得情况，目标为求解一个超平面$w^T \mathit{\Phi}(x) + b = 0$使两类点落在超平面两侧。 观察以上两图，图$(c)$分割最佳，应有 超平面在两类点间隔内，可平移距离最大；r = \max (r_+ - r_-) 分割平面$\mathcal{P}$到$\mathcal{P}_+, \mathcal{P}_-$的距离相等，即；r_- = r_+ = \frac{r}{2} 落在支撑超平面$\mathcal{P}_+, \mathcal{P}_-$上的点称为支持向量Support Vector，可记作$x_{+/-}^{sup}$； 设分割超平面$\mathcal{P}$为 g(x) = w^T \mathit{\Phi}(x) + b = 0 \tag{5}判别方程可定义为 \hat{y} = \text{sign} \left[ w^T \mathit{\Phi}(x) + b \right] \tag{6}则平面$\mathcal{P}$上下平移后得到平面$\mathcal{P}_+, \mathcal{P}_-$，即 g_+(x) = w^T \mathit{\Phi}(x) + b - C(常数) = 0g_-(x) = w^T \mathit{\Phi}(x) + b + C(常数) = 0作归一化处理，两边同除以$C(常数)$，则支撑超平面方程为 g_+(x) = w^T \mathit{\Phi}(x) + b - 1 = 0g_-(x) = w^T \mathit{\Phi}(x) + b + 1 = 0对于正负样本$x_{+/-}$，分别满足 \begin{cases} w^T \mathit{\Phi}(x_+) + b > 1 \\ y = 1 \end{cases} \begin{cases} w^T \mathit{\Phi}(x_-) + b < - 1 \\ y = -1 \end{cases}即 y \left[ w^T \mathit{\Phi}(x) + b \right] > 1现希望优化两个平面间的距离，使其达到最大，即优化目标为 w, b = \arg \max rs.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] > 1 \tag{7}那么如何求解支撑超平面间距离$r$呢？，有两种思路 思路一 过超平面$\mathcal{P}$任一点$\mathit{\Phi}(x)$作垂线，分别交$\mathcal{P}_{+/-}$于向量$\mathit{\Phi}(x_{+/-})$，则 \begin{cases} w^T \mathit{\Phi}(x_+) + b = + 1 \\ w^T \mathit{\Phi}(x_-) + b = - 1 \end{cases} 利用距离公式求解点$\mathit{\Phi}(x)$到超平面$\mathcal{P}$的距离 r_+ = r_- = \frac{r}{2} = \frac{| w^T \mathit{\Phi}(x_{+/-}) + b |}{||w||} = \frac{1}{||w||} 所以 r = \frac{2}{||w||} \tag{8.a} 思路二 r_+ = r_- = \frac{r}{2} = \min_i \frac{| w^T \mathit{\Phi}(x^{(i)}) + b |}{||w||}, \quad i = 1, ..., N 由支持向量定义 | w^T \mathit{\Phi}(x_{+/-}^{sup}) + b | = \min | w^T \mathit{\Phi}(x^{(i)}) + b | 且 | w^T \mathit{\Phi}(x_{+/-}^{sup}) + b | = 1 所以 r = \frac{2}{||w||} \tag{8.b} 所以优化目标为 w, b = \arg \max \frac{2}{||w||}s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{9.a}为方便求解，相当于 w, b = \arg \min \frac{1}{2} ||w||^2s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{10.b}回归问题回归模型的目标是让训练集中每个样本点$(x^{(i)}, y^{(i)})$尽量拟合到一个线性模型上 y^{(i)} = w^T \mathit{\Phi}(x^{(i)}) + b对于一般的回归模型，一般使用$MSE$作为损失函数，但是在$y^{(i)} \neq w^T \mathit{\Phi}(x^{(i)}) + b$时就会有损失，所以$SVM$不采用。 定义一个常量$\epsilon &gt; 0$，对于某个样本点$(x^{(i)}, y^{(i)})$，其损失定义如下，即在支撑超平面间隔内的样本点是没有损失的 L^{(i)} = \begin{cases} 0 & | y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) - b | \leq \epsilon \\ | y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) - b | - \epsilon & \text{otherwise} \end{cases} 优化问题的求解与SMO算法带约束的优化问题求解 纯等式约束 一般形式为 \vec{w} = \arg \min f(\vec{w})s.t. \qquad h_j(\vec{w}) = 0, \quad j = 1, 2, \cdots, m \tag{11} 其中$f(\vec{w}), h_j(\vec{w})$均可导 列写拉格朗日函数 L(\vec{w}, \vec{\lambda}) = f(\vec{w}) + \sum_{j=0}^m \lambda_j h_j(\vec{w}) \tag{11.a} 求取极点 \begin{cases} \frac{\partial L}{\partial w_i} = \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^m \lambda_j \frac{\partial h_j(\vec{w})}{\partial w_i} = 0 \\ \frac{\partial L}{\partial \lambda_j} = h_j(\vec{w}) = 0 \end{cases} \tag{11.b} 如 $f(x, y) = x^2 + 3xy + y^2, \quad s.t. \quad x + y = 100 $ 纯不等式约束 一般形式为 \vec{w} = \arg \min f(\vec{w})s.t. \qquad g_j(\vec{w}) \leq 0, \quad j = 1, 2, \cdots, p \tag{12} 对于上不等式约束，引入松弛变量$\epsilon_j^2$，使其转换为等式约束 s.t. \qquad g_j(\vec{w}) + \epsilon_j^2 = 0, \quad j = 1, 2, \cdots, p 注意，这里引入的松弛变量为平方项，如此可避免增加约束$\epsilon_j \geq 0$ 列写拉格朗日函数 L(\vec{w}, \vec{\mu}, \vec{\epsilon^2}) = f(\vec{w}) + \sum_{j=0}^p \mu_j (g_j(\vec{w}) + \epsilon_j^2) \tag{12.a} 求取极值点 \begin{cases} \frac{\partial L}{\partial w_i} = \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\ \frac{\partial L}{\partial \mu_j} = g_j(\vec{w}) + \epsilon_j^2 = 0 \\ \frac{\partial L}{\partial \epsilon_j} = 2 \mu_j \epsilon_j = 0 \\ \mu_j \geq 0 \end{cases} \tag{12.b} 注意等式三 若$\mu_j = 0$，即对应不等式$g_j(\vec{w}) \leq 0$未起到约束作用； 若$\mu_j \neq 0$，则$\epsilon_j = 0$，那么$ g_j(\vec{w}) = 0$；则总结可得 \mu_j g_j(\vec{w}) = 0故$(12.b)$转化为 \begin{cases} \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\ \mu_j g_j(\vec{w}) = 0 \\ \mu_j \geq 0 \end{cases} \tag{12.c} 混合条件约束 一般形式为 \vec{w} = \arg \min f(\vec{w})s.t. \qquad h_j(\vec{w}) = 0, \quad j = 1, 2, \cdots, m\quad \qquad g_k(\vec{w}) \leq 0, \quad k = 1, 2, \cdots, p \tag{13} 经上述内容，可得 L(\vec{w}, \vec{\mu}, \vec{\epsilon^2}) = f(\vec{w}) + \sum_{j=0}^m \lambda_j h_j(\vec{w}) + \sum_{j=0}^p \mu_j (g_j(\vec{w}) + \epsilon_j^2) \tag{13.a} 求取极值点 \begin{cases} \frac{\partial f(\vec{w})}{\partial w_i} + \sum_{j=0}^m \lambda_j \frac{\partial h_j(\vec{w})}{\partial w_i} + \sum_{j=0}^p \mu_j \frac{\partial g_j(\vec{w})}{\partial w_i} = 0 \\ h_j(\vec{w}) = 0 \\ \mu_j g_j(\vec{w}) = 0 \\ \mu_j \geq 0 \end{cases} \tag{13.b} 上式即$K.K.T.$条件。 线性可分情况下求解 对于优化问题$(10.b)$ w, b = \arg \max \frac{2}{||w||}s.t.\qquad y \left[ w^T \mathit{\Phi}(x) + b \right] \geq 1 \tag{10.b}利用拉格朗日乘子法 L(w, b, \mu) = \frac{1}{2} ||w||^2 + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} \tag{14}则带求解问题为 w, b, \mu = \arg \min_{w, b} \max_{\mu} L(w, b, \mu) \tag{15.a}转化为上式的对偶问题 w, b, \mu = \arg \max_{\mu} \min_{w,b} L(w, b, \mu) \tag{15.b} 凸优化 分别对参数求偏导 \frac{\partial}{\partial w_j} L(w, b, \mu) = w_j - \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j)\frac{\partial}{\partial b} L(w, b, \mu) = - \sum_i \mu^{(i)} y^{(i)} $ \frac{\partial}{\partial w_j} \frac{1}{2} ||w||^2 = w_j; \quad \frac{\partial}{\partial w_j} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = \mathit{\Phi}(x^{(i)}_j) $，注意这里还没有用到映射函数$\mathit{\Phi}(x)$计算。 该式为不等式约束，由$K.K.T.$条件，联立得到 \begin{cases} \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) = w_j \\ \sum_i \mu^{(i)} y^{(i)} = 0 \\ \\ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 0 \\ \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0 \\ \mu^{(i)} \geq 0 \end{cases} \tag{16}有 \tilde{L}(\mu) = \frac{1}{2} w^T w + \sum_i \mu^{(i)} \left\{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\}\qquad \qquad \qquad \qquad = \frac{1}{2} w^T w + \sum_i \mu^{(i)} - w^T \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) + b \sum_i \mu^{(i)} y^{(i)} \tag{17.a}将$w = \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}), \sum_i \mu^{(i)} y^{(i)} = 0$代入，消去变量$w, b$，有 \tilde{L}(\mu) = \sum_i \mu^{(i)} - \frac{1}{2} w^T w \tag{17.b}其中 w^T w = \left[ \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \right]^T \left[ \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \right]\qquad = \sum_i \left[ \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T \sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)}) \right]\quad = \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{17.c}代回$(17.b)$得到 \tilde{L}(\mu) = \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{17} 这里出现了核函数：$\kappa(x^{(i)}, x^{(j)}) = \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})$ 那么优化问题现在转化为 \mu = \arg \max_{\mu} \tilde{L}(\mu) = \arg \max_{\mu} \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})s.t.\qquad \mu^{(i)} \geq 0, \quad \sum_i \mu^{(i)} y^{(i)} = 0 \tag{18}其中$i = 1, \cdots, N$，现在只需优化$\mu$即可，该式使用$SMO$或梯度下降法算法求解，再用下式求解参数$w, b$ \begin{cases} w = \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) \\ y^{sup} \left[ w^T \mathit{\Phi}(x^{sup}) + b \right] = 1 \end{cases} \tag{19}可能存在多个支持向量，均满足等式$(19.2)$。 线性不可分情况下求解 如上图，存在部分样本点线性不可分，有两种方法可解决 核函数 软间隔支持向量机 核函数上面介绍了核函数，其作用是将样本特征升维，添加的维度与已存在的特征是线性不相关的，例如我们有样本点 x^{(i)} = \left[\begin{matrix} x^{(i)}_1, \cdots, x^{(i)}_n \end{matrix}\right]^T可以增加多项式维，例如$(x^{(i)}_j)^n, \prod_{k \leq n}^{k \leq K \leq N} x^{(i)}_k$等，或者添加其他形式的非线性函数，但我们知道，所有函数均可在某点$x = x_0$处展开为幂级数，本质上一致 f(x) = \sum_n \frac{f^{(n)}(x - x_0)}{n!} (x - x_0)^{(n)}核函数就是利用级数展开的概念，构造多项式维度，将样本点升维，例如高斯核函数$\kappa(x_i, x_j) = e^{-\gamma \frac{||x_i - x_j||^2}{2\sigma^2}}$将其升高到无穷维。机器学习算法求解各维度的权值系数，特征越重要，系数值占比越大。 软间隔支持向量机本质上仍为线性支持向量机，对于线性不可分的情况，应允许部分样本点不满足条件 y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1可对每个样本引入松弛变量$\epsilon^{(i)}$，即 y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 - \epsilon^{(i)} \tag{20} 仅仅对于落在支撑超平面间的样本点满足$\epsilon^{(i)} &gt; 0$ 形象解释，即对于落在支撑超平面间的样本点$x^{(i)}$，视其为软间隔支持向量(自创)，调整$\epsilon^{(i)}$将支撑超平面进行微量的位移$d_{\epsilon^{(i)}}$，如下图 但是呢，也要对不满足该条件的样本个数进行限制，希望越少越好。那么加入惩罚系数$C$(超参数)，对不满足条件的样本进行惩罚，使$\sum_i \epsilon^{(i)}$越小越好，则优化目标变更为 w, b = \arg \min_{w,b} \left( \frac{1}{2} ||w||^2 + C \sum_i \epsilon^{(i)} \right)s.t. \qquad y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 - \epsilon^{(i)}\qquad \epsilon^{(i)} \geq 0 \tag{21}同样的，构造拉格朗日函数 L(w, b, \epsilon, \mu_1, \mu_2) = \left( \frac{1}{2} ||w||^2 + C \sum_i \epsilon^{(i)} \right) + \sum_i \mu_1^{(i)} \left\{ 1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} + \sum_i \mu_2^{(i)} \left( - \epsilon^{(i)} \right) \tag{22}根据$K.K.T.$条件 \begin{cases} \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) = w_j \\ \sum_i \mu_1^{(i)} y^{(i)} = 0 \\ C - \mu_1^{(i)} - \mu_2^{(i)} = 0 \\ \\ 1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 0 \\ \mu_1^{(i)} \left\{ 1 - \epsilon^{(i)} - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right\} = 0 \\ \mu_1^{(i)} \geq 0 \\ \\ - \epsilon^{(i)} \leq 0 \\ \mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0 \\ \mu_2^{(i)} \geq 0 \end{cases} \tag{23}消除$w, b, \epsilon$ \tilde{L}(\mu_1, \mu_2) = \frac{1}{2} w^T w + C \sum_i \epsilon^{(i)} + \sum_i \mu_1^{(i)} \left( 1 - \epsilon^{(i)} \right) - w^T \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}) - b \sum_i \mu_1^{(i)} y^{(i)} - \sum_i \mu_2^{(i)} \epsilon^{(i)}其中$w = \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}); \quad \sum_i \mu_1^{(i)} y^{(i)} = 0$ \tilde{L}(\mu_1, \mu_2) = - \frac{1}{2} w^T w + C \sum_i \epsilon^{(i)} + \sum_i \mu_1^{(i)} - \sum_i \left( \mu_1^{(i)} + \mu_2^{(i)} \right) \epsilon^{(i)}其中$\mu_1^{(i)} + \mu_2^{(i)} = C$，所以同$(17)$ \tilde{L}(\mu_1) = \sum_i \mu_1^{(i)} - \frac{1}{2} w^T w = \sum_i \mu_1^{(i)} - \frac{1}{2} \sum_i \sum_j \mu_1^{(i)} \mu_1^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{24}那么优化问题现在转化为 \mu_1 = \arg \max_{\mu_1} \tilde{L}(\mu_1) = \arg \max_{\mu_1} \sum_i \mu_1^{(i)} - \frac{1}{2} \sum_i \sum_j \mu_1^{(i)} \mu_1^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})s.t.\qquad \mu_1^{(i)} \geq 0, \quad \mu_2^{(i)} \geq 0\sum_i \mu_1^{(i)} y^{(i)} = 0C - \mu_1^{(i)} - \mu_2^{(i)} = 0 \tag{25}对于上式，有如下分析 \begin{cases} \mu_1^{(i)} \geq 0 \\ \mu_2^{(i)} \geq 0 \\ \mu_2^{(i)} = C - \mu_1^{(i)} \end{cases} \Rightarrow C \geq \mu_1^{(i)} \geq 0 \tag{26.a} $C = \mu_1^{(i)}$时，$\mu_2^{(i)} = 0$，由$\mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0$，可得$\epsilon^{(i)} \geq 0$ $\epsilon^{(i)} = 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1$，该点为支撑向量； $\epsilon^{(i)} &gt; 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] &lt; 1$，该点在支撑超平面间； $C \neq \mu_1^{(i)}$即$C &gt; \mu_1^{(i)} \geq 0$时，$\mu_2^{(i)} \neq 0$，则由$\mu_2^{(i)} \left( - \epsilon^{(i)} \right) = 0$，可得$\epsilon^{(i)} = 0$，那么$\mu_1^{(i)} \left{ 1 - y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \right} = 0$ $\mu_1^{(i)} &gt; 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1$，该点为支撑向量； $\mu_1^{(i)} = 0$时，$y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1$，该点分类正确，在支持超平面上或者两边 总结一下，即 \begin{cases} \mu_1^{(i)} = 0 \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 \\ 0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 \\ \mu_1^{(i)} = C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1 \end{cases} \tag{27}同样利用$SMO$或梯度下降法求解$\mu_1$，然后以下式求解$w, b, \epsilon, \mu_2$ \begin{cases} \mu_2 = C - \mu_1 \\ w_j = \sum_i \mu_1^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)}_j) \\ y^{sup} \left[ w^T \mathit{\Phi}(x^{sup}) + b \right] = 1 \\ y^{sup'} \left[ w^T \mathit{\Phi}(x^{sup'}) + b \right] = 1 - \epsilon^{sup'} \end{cases} \tag{28} 注：$(x^{sup’}, y^{sup’})$表示“软间隔支持向量”。注：(1) 并非所有的样本点都有一个松弛变量与其对应。实际上只有“离群点”才有，所有没离群的点松弛变量都等于0(2) 松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远(3) 惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题(4) 惩罚因子C不是一个变量，整个优化问题在解的时候，C是一个你必须事先指定的值，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个C的值，再解一次优化问题，得到另一个分类器，再看看效果，如此就是一个参数寻优的过程，但这和优化问题本身决不是一回事，优化问题在解的过程中，C一直是定值(5) 尽管加了松弛变量这么一说，但这个优化问题仍然是一个优化问题（汗，这不废话么），解它的过程比起原始的硬间隔问题来说，没有任何更加特殊的地方(C≥$\mu^{(i)}$≥0)(6) 完全可以给每一个离群点都使用不同的C，这时就意味着你对每个样本的重视程度都不一样，有些样本丢了也就丢了，错了也就错了，这些就给一个比较小的C；而有些样本很重要，决不能分类错误（比如中央下达的文件啥的，笑），就给一个很大的C。以上忘记从哪里摘抄的了:-( Sequential Minimal Optimization(SMO)以上我们得到优化目标 \mu = \arg \max_{\mu} \tilde{L}(\mu) = \arg \max_{\mu} \sum_i \mu^{(i)} - \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)})s.t.\qquad \mu^{(i)} \geq 0\quad \sum_i \mu^{(i)} y^{(i)} = 0\quad C \geq \mu^{(i)} \geq 0 \tag{29}基本思想把对整个λ向量的优化转化为对每一对$\mu^{(i)},\mu^{(j)}$的优化，如果我们把其他λ先固定，仅仅优化某一对$\mu^{(i)},\mu^{(j)}$，那么我们可以通过解析式（即通过确定的公式来计算）来优化$\mu^{(i)},\mu^{(j)}$ 。而且此时$K.K.T.$条件很重要，之前说过最优解是一定会满足$K.K.T.$条件的，所以如果我们优化使所有$\mu$都满足了$K.K.T.$条件，那么这样最优解就会找到。 选择优化对的方法寻找两个参数时，应找那些违反$K.K.T.$条件的，具体过程可分为外层循环和内层循环，利用启发式规则寻找待优化参数对。 启发式规则1 在所有样本中选择违反$K.K.T.$条件的一个乘子$\mu^{(i)}$，用启发式规则2选择另一个乘子$\mu^{(j)}$，对这两个乘子进行优化； 接着，从所有非边界样本中，选择违反$K.K.T.$条件的一个乘子作为最外层循环，用启发式规则2选择另一个乘子进行这两个乘子的优化； 最后，若上述非边界样本中没有违反$K.K.T.$条件的样本，则再从整个样本中去找，直到所有样本中没有需要改变的乘子，或满足其他停止条件为止。 启发式规则2 首先在非边界乘子中获得$|E_1−E_2|$最大的样本$\mu^{(j)}$； 如果1中没有找到，则从所有样本中随机确定$\mu^{(j)}$ 满足式$(27)$即满足$K.K.T.$条件 \begin{cases} \mu_1^{(i)} = 0 \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 \\ 0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 \\ \mu_1^{(i)} = C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1 \end{cases} \tag{27}那么以下情况不满足$K.K.T.$条件 \begin{cases} y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \geq 1 时，\mu_1^{(i)} > 0\\ y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1 时，\mu_1^{(i)} = 0 或 \mu_1^{(i)} = 1 \\ y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] \leq 1 时，\mu_1^{(i)} < C \end{cases} \tag{30} 第三部分：SMO算法的个人理解 - cnblogs找第一个参数的具体过程是这样的： 遍历一遍整个数据集，对每个不满足$K.K.T.$条件的参数，选作第一个待修改参数 在上面对整个数据集遍历一遍后，选择那些参数满足$0 &lt; \mu &lt; C$的子集，开始遍历，如果发现一个不满足$K.K.T.$条件的，作为第一个待修改参数，然后找到第二个待修改的参数并修改，修改完后，重新开始遍历这个子集 遍历完子集后，重新开始①②，直到在执行①和②时没有任何修改就结束（为什么要这样遍历，我现在还是不太明白） 找第二个参数的过程是这样的： 启发式找，找能让下式最大的 寻找一个随机位置的满足下式的可以优化的参数进行修改 在整个数据集上寻找一个随机位置的可以优化的参数进行修改 都不行那就找下一个第一个参数 两个变量的优化问题以$\mu^{(1)}, \mu^{(2)}$为例 \min_{\mu^{(1)}, \mu^{(2)}} \left[ \frac{1}{2} \sum_i \sum_j \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - \sum_i \mu^{(i)} \right]= \min_{\mu^{(1)}, \mu^{(2)}} \left[ \frac{1}{2} \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T \sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)}) - \sum_i \mu^{(i)} \right] \tag{31.a}其中 \sum_i \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T = \mu^{(1)} y^{(1)} \mathit{\Phi}(x^{(1)})^T + \mu^{(2)} y^{(2)} \mathit{\Phi}(x^{(2)})^T + \sum_{i=3} \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(i)})^T\sum_j \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)})^T = \mu^{(1)} y^{(1)} \mathit{\Phi}(x^{(1)})^T + \mu^{(2)} y^{(2)} \mathit{\Phi}(x^{(2)})^T + \sum_{j=3} \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(j)})^T\sum_i \mu^{(i)} = \mu^{(1)} + \mu^{(2)} + \sum_{i=3} \mu^{(i)}代回$(31.a)$，多项式展开整理得到 (31.b) = \min_{\mu^{(1)}, \mu^{(2)}} [ \frac{1}{2} \mu^{(1)2} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(1)}) + \frac{1}{2} \mu^{(2)2} \mathit{\Phi}(x^{(2)})^T \mathit{\Phi}(x^{(2)}) + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(2)}) + \mu^{(1)} y^{(1)} \sum_{i=3} \mu^{(i)} y^{(i)} \mathit{\Phi}(x^{(1)})^T \mathit{\Phi}(x^{(i)}) + \mu^{(2)} y^{(2)} \sum_{j=3} \mu^{(j)} y^{(j)} \mathit{\Phi}(x^{(2)})^T \mathit{\Phi}(x^{(j)}) + \frac{1}{2} \sum_{i=3} \sum_{j=3} \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - (\mu^{(1)} + \mu^{(2)}) - \sum_{i=3} \mu^{(i)}] \tag{31.b}令核函数 K_{ij} = \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) \tag{32.a}const = \frac{1}{2} \sum_{i=3} \sum_{j=3} \mu^{(i)} \mu^{(j)} y^{(i)} y^{(j)} \mathit{\Phi}(x^{(i)})^T \mathit{\Phi}(x^{(j)}) - \sum_{i=3} \mu^{(i)} \tag{32.b}定义 f(x^{(i)}) = \sum_m \mu^{(m)} y^{(m)} K_{im} + b \tag{32.c}E^{(i)} = f(x^{(i)}) - y^{(i)}v^{(i)} = \sum_{m=3} \mu^{(m)} y^{(m)} K_{im} = f(x^{(i)}) - \sum_{m=1}^2 \mu^{(m)} y^{(m)} K_{im} - b \tag{32.e}以上，代回$(31.b)$，优化问题转化为 \min_{\mu^{(1)}, \mu^{(2)}} [ \frac{1}{2} \mu^{(1)2} K_{11} + \frac{1}{2} \mu^{(2)2} K_{22} + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} K_{12} + \mu^{(1)} y^{(1)} v^{(1)} + \mu^{(2)} y^{(2)} v^{(2)} - (\mu^{(1)} + \mu^{(2)}) + const]s.t.\qquad \mu^{(i)} \geq 0\mu^{(1)} y^{(1)} + \mu^{(2)} y^{(2)} = - \sum_{i=3} \mu^{(i)} y^{(i)}C \geq \mu^{(i)} \geq 0 \tag{33}记 g(\mu^{(1)}, \mu^{(2)}) = \frac{1}{2} \mu^{(1)2} K_{11} + \frac{1}{2} \mu^{(2)2} K_{22} + \mu^{(1)} \mu^{(2)} y^{(1)} y^{(2)} K_{12} + \mu^{(1)} y^{(1)} v^{(1)} + \mu^{(2)} y^{(2)} v^{(2)} - (\mu^{(1)} + \mu^{(2)}) + const \tag{34}记$\epsilon = - \sum_{i=3} \mu^{(i)} y^{(i)}$，则 \mu^{(1)} y^{(1)} + \mu^{(2)} y^{(2)} = \epsilon消去$\mu^{(1)}$ \mu^{(1)} = \frac{\epsilon - \mu^{(2)} y^{(2)}}{y^{(1)}} \quad 同 \quad \mu^{(1)} = y^{(1)}\epsilon - y^{(1)} y^{(2)} \mu^{(2)}记$\gamma = y^{(1)}\epsilon, \quad s = y^{(1)} y^{(2)} $，则 \mu^{(1)} = \gamma - s \mu^{(2)} \tag{35} s^2 = 1, \quad \gamma s = y^{(2)}\epsilon 代入函数$(34)$，有 \tilde{g}(\mu^{(2)}) = \frac{1}{2} (K_{11} + K_{22} - 2 K_{12}) \mu^{(2)2} + \left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} + (\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{36}转化为单变量$\mu^{(2)}$的二次优化问题。 剪裁边界需考虑$\mu^{(2)}$的取值范围，即编程时$\mu^{(2)}$的剪裁边界，设 L \leq \mu^{(2)}_{new} \leq H \tag{37}综合条件 \mu^{(1)}_{new} y^{(1)} + \mu^{(2)}_{new} y^{(2)} = \epsilon0 \leq \mu^{(i)} \leq C $\mu^{(1)}_{new} y^{(1)} + \mu^{(2)}_{new} y^{(2)} = \epsilon \Rightarrow \mu^{(1)}_{new} + \mu^{(2)}_{new} y^{(1)} y^{(2)} = \epsilon y^{(1)}$ $y^{(1)} \neq y^{(2)}$时，$y^{(1)} y^{(2)} = -1$ \mu^{(1)}_{old} - \mu^{(2)}_{old} = \epsilon_{\neq} (常数) \quad 则 \quad \mu^{(2)}_{old} = \mu^{(1)}_{old} - \epsilon_{\neq} \tag{38.a} 考虑 0 \leq \mu^{(i)} \leq C 则 \begin{cases} 0 \leq \mu^{(1)}_{old} \leq C \\ 0 \leq \mu^{(2)}_{old} \leq C \\ \end{cases} \Rightarrow 0 - \epsilon_{\neq} \leq \mu^{(2)}_{old} = \mu^{(1)}_{old} - \epsilon_{\neq} \leq C - \epsilon_{\neq} 此时上下界为 \begin{cases} L = \max \{ 0, - \epsilon_{\neq} \} \\ H = \min \{ C, C - \epsilon_{\neq} \} \\ \end{cases} \tag{38.b} $(38.a)$代入$(38.b)$，得到迭代式 \begin{cases} L = \max \{ 0, - \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\ H = \min \{ C, C - \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\ \end{cases} \tag{38} $y^{(1)} = y^{(2)}$时，$y^{(1)} y^{(2)} = 1$ \mu^{(1)}_{old} + \mu^{(2)}_{old} = \epsilon_{=} (常数) \quad 则 \quad \mu^{(2)}_{old} = - \mu^{(1)}_{old} + \epsilon_{=} \tag{39.a} 考虑 0 \leq \mu^{(i)} \leq C 则 \begin{cases} 0 \leq \mu^{(1)}_{old} \leq C \\ 0 \leq \mu^{(2)}_{old} \leq C \\ \end{cases} \Rightarrow - C + \epsilon_{=} \leq - \mu^{(1)}_{old} + \epsilon_{=} \leq 0 + \epsilon_{=} 此时上下界为 \begin{cases} L = \max \{ 0, - C + \epsilon_{=} \} \\ H = \min \{ C, 0 + \epsilon_{=} \} \\ \end{cases} \tag{39.b} $(39.a)$代入$(39.b)$，得到迭代式 \begin{cases} L = \max \{ 0, \mu^{(1)}_{old} + \mu^{(2)}_{old} - C \} \\ H = \min \{ C, \mu^{(1)}_{old} + \mu^{(2)}_{old} \} \\ \end{cases} \tag{39} 单变量的二次优化 \tilde{g}(\mu^{(2)}) = \frac{1}{2} (K_{11} + K_{22} - 2 K_{12}) \mu^{(2)2} + \left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} + (\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{36} 二次项系数$K_{11} + K_{22} - 2 K_{12} &gt; 0$时 求极值点 \frac{\partial \tilde{g}(\mu^{(2)})}{\partial \mu^{(2)}} = 0 \Rightarrow \tilde{\mu}^{(2)} \tag{40.a} 若$L \leq \tilde{\mu}^{(2)} \leq H$，则最小值点即为$\mu^{(2)} = \tilde{\mu}^{(2)}$； 否则在边界处取得最小值。 二次项系数$K_{11} + K_{22} - 2 K_{12} = 0$时 \tilde{g}(\mu^{(2)}) = \left[ - \gamma s (K_{11} - K_{12}) - y^{(2)} (v^{(1)} - v^{(2)}) + s - 1 \right] \mu^{(2)} + (\frac{1}{2} K_{11} \gamma^2 + \epsilon v^{(1)} - \gamma) + const \tag{40.b} $\tilde{g}(\mu^{(2)})$为一次函数，在边界处取得最小值。 二次项系数$K_{11} + K_{22} - 2 K_{12} &lt; 0$时 $\tilde{g}(\mu^{(2)})$为开口向下的二次函数，在边界处取得最小值。 综上所述，$\mu^{(2)}$更新的解析解为 \mu^{(2)}_{new, clip} = \begin{cases} H & \mu^{(2)}_{new} > H \\ \mu^{(2)}_{new} & L \leq \mu^{(2)}_{new} \leq H \\ L & \mu^{(2)}_{new} < L \\ \end{cases} \tag{40}又因为 \mu^{(1)}_{old} = \gamma - s \mu^{(2)}_{old} \tag{41.a}\mu^{(1)}_{new} = \gamma - s \mu^{(2)}_{new, clip} \tag{41.b}两式相减，得到$\mu^{(1)}$更新的增量形式 \mu^{(1)}_{new} = \mu^{(1)}_{old} + y^{(1)} y^{(2)} \left( \mu^{(2)}_{old} - \mu^{(2)}_{new, clip} \right) \tag{41}更新$\mu^{(1)}, \mu^{(2)}$后，重新计算$b$，因为$b$影响到$E^{(i)}$的计算，由$(27)$ 0 < \mu_1^{(i)} < C \iff y^{(i)} \left[ w^T \mathit{\Phi}(x^{(i)}) + b \right] = 1此时对应支撑向量$x^{sup}$，上右式两边同乘$y^{(i)}$，得到 w^T \mathit{\Phi}(x^{(i)}) + b = y^{(i)}\Rightarrow b = y^{(i)} - w^T \mathit{\Phi}(x^{(i)}) = y^{(i)} - \sum_m \mu^{(m)} y^{(m)} K_{im} 疑问：$w^T \mathit{\Phi}(x^{(i)}) = \sum_m \mu^{(m)} y^{(m)} K_{im}?$ 所以 b^{(1)}_{new} = y^{(1)} - \sum_{m=3} \mu^{(m)} y^{(m)} K_{1m} - \mu^{(1)}_{new} y^{(1)} K_{11} - \mu^{(2)}_{new} y^{(2)} K_{12} \tag{42.a}其中 \sum_{m=3} \mu^{(m)} y^{(m)} K_{1m} = f(x^{(1)}) - \sum_{m=1}^2 \mu^{(m)}_{old} y^{(m)} K_{1m} - b_{old} \tag{42.b} v^{(i)} = \sum_{m=3} \mu^{(m)} y^{(m)} K_{im} = f(x^{(i)}) - \sum_{m=1}^2 \mu^{(m)} y^{(m)} K_{im} - b \tag{32.e} 所以 b^{(1)}_{new} = y^{(1)} - f(x^{(1)}) - \mu^{(1)}_{old} y^{(1)} K_{11} + \mu^{(2)}_{old} y^{(2)} K_{12} + b_{old} - \mu^{(1)}_{new} y^{(1)} K_{11} - \mu^{(2)}_{new} y^{(2)} K_{12}= - E^{(1)} + (\mu^{(1)}_{old} - \mu^{(1)}_{new}) y^{(1)} K_{11} + (\mu^{(2)}_{old} - \mu^{(2)}_{new}) y^{(2)} K_{12} + b_{old} \tag{42}同理 b^{(2)}_{new} = - E^{(2)} + (\mu^{(1)}_{old} - \mu^{(1)}_{new}) y^{(1)} K_{21} + (\mu^{(2)}_{old} - \mu^{(2)}_{new}) y^{(2)} K_{22} + b_{old} \tag{43}当$b^{(1)}$和$b^{(2)}$均有效时 b_{new} = b^{(1)}_{new} = b^{(2)}_{new} \tag{44.a}当两个乘子都在边界上时，则$b$阈值与$K.K.T.$条件一致时，不满足时取中点 b = \begin{cases} b^{(1)} & 0 < \mu^{(1)}_{new} < C \\ b^{(2)} & 0 < \mu^{(2)}_{new} < C \\ \frac{1}{2} (b^{(1)} + b^{(2)}) & \text{otherwise} \end{cases} \tag{44.b}梳理 计算误差 E^{(i)} = f(x^{(i)}) - y^{(i)} = \sum_i \mu^{(m)} y^{(m)} K_{im} + b -y^{(i)} 计算上下界 \begin{cases} L = \max \{ 0, - \mu^{(i)}_{old} + \mu^{(j)}_{old} \};\quad H = \min \{ C, C - \mu^{(i)}_{old} + \mu^{(j)}_{old} \} & y^{(i)} \neq y^{(j)} \\ L = \max \{ 0, \mu^{(i)}_{old} + \mu^{(j)}_{old} - C \};\quad H = \min \{ C, \mu^{(i)}_{old} + \mu^{(j)}_{old} \} & y^{(i)} = y^{(j)} \end{cases} 计算$\eta$ \eta = K_{ii} + K_{jj} - 2K_{ij} 更新$\mu^{(j)}$ \mu^{(j)}_{new} = \mu^{(j)}_{old} + \frac{y^{(j)}(E^{(i)} - E^{(j)})}{\eta} 修剪$\mu^{(j)}$ \mu^{(j)}_{new, clip} = \begin{cases} H & \mu^{(j)}_{new} > H \\ \mu^{(j)}_{new} & L \leq \mu^{(j)}_{new} \leq H \\ L & \mu^{(j)}_{new} < L \\ \end{cases} 更新$\mu^{(i)}$ \mu^{(i)}_{new} = \mu^{(i)}_{old} + y^{(i)} y^{(j)} \left( \mu^{(j)}_{old} - \mu^{(j)}_{new, clip} \right) 更新$b^{(i)}, b^{(j)}$ b^{(i)}_{new} = - E^{(i)} + (\mu^{(i)}_{old} - \mu^{(i)}_{new}) y^{(i)} K_{ii} + (\mu^{(j)}_{old} - \mu^{(j)}_{new}) y^{(j)} K_{ij} + b_{old}b^{(j)}_{new} = - E^{(j)} + (\mu^{(i)}_{old} - \mu^{(i)}_{new}) y^{(i)} K_{ji} + (\mu^{(j)}_{old} - \mu^{(j)}_{new}) y^{(j)} K_{jj} + b_{old} 修剪$b$ b = \begin{cases} b^{(i)} & 0 < \mu^{(i)}_{new} < C \\ b^{(j)} & 0 < \mu^{(j)}_{new} < C \\ \frac{b^{(i)} + b^{(j)}}{2} & \text{otherwise} \end{cases} Reference 支持向量机 - 维基百科，自由的百科全书 1.4. Support Vector Machine - scikit learn 详解支持向量机 - 机器之心 深入理解拉格朗日乘子法（Lagrange Multiplier) 和KKT条件 - CSDN 浅谈最优化问题的KKT条件 - 知乎 第三部分：SMO算法的个人理解]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mAP]]></title>
    <url>%2F2019%2F05%2F26%2FmAP%2F</url>
    <content type="text"><![CDATA[前言本文介绍一种目标检测任务的评价指标mean Average Precison(mAP)，有以下几个重点 如何绘制Precision-Recall曲线； 如何用插值方法计算Average Precison； 理解目标检测中mAP的计算； Precision, Recall, Average Precision计算方法以上定义查看Metrics/分类(classification)评估指标，计算方式如下 精确率(Precision)即所有真实正样本中，被预测为正样本的比例 P = \frac{TP}{TP + FN}123456789101112131415def precision_score(gt, pred): """ Params: gt: &#123;ndarray(N)&#125; `0` or `1` pred: &#123;ndarray(N)&#125; `0` or `1` Returns： p: &#123;float&#125; """ index = pred == 1 _gt = gt[index] tp = _gt[_gt == 1].shape[0] pp = _gt.shape[0] p = tp / pp if pp != 0 else 0 return p 召回率(Recall)即所有预测的正样本中，真正为正样本的比例 R = \frac{TP}{TP + NP}1234567891011121314def recall_score(gt, pred): """ Params: gt: &#123;ndarray(N)&#125; `0` or `1` pred: &#123;ndarray(N)&#125; `0` or `1` Returns： r: &#123;float&#125; """ index = gt == 1 _pred = pred[index] tp = _pred[_pred == 1].shape[0] gp = _pred.shape[0] r = tp / gp if gp != 0 else 0 return r 平均精确度(Average Precision)依次设定不同的阈值，根据预测评分，得到不同的预测标签结果，那么就可以计算得到不同的$P$和$R$，做出$P-R$曲线，其与坐标轴面积即平均精确度。 Average Precision 该图来自目标检测评价标准-AP mAP，侵删。 那么AP计算公式为 AP = \int_0^1 P(r) dr \tag{1}1234567891011121314151617def average_precision(p, r): """ Params: p: &#123;ndarray(n)&#125; Precision r: &#123;ndarray(n)&#125; Recall Returns: AP:&#123;float&#125; Notes: AP = \int_0^1 P(r) dr """ n = len(p) AP = 0 for i in range(1, n): deltaR = r[i] - r[i-1] P_inter = (p[i] + p[i-1]) / 2 AP += P_inter * deltaR # 梯形面积 return AP Approximated Average Precision 注意图中折线上每个点代表一个样本，则将$(1)$离散化，累积每个样本点带来的面积变化 AP_{approx} = \sum_{k=1}^N P(k) \Delta r(k) \tag{2}其中$N$为样本总数，$k$为样本数，且 \Delta r(k) = r(k) - r(k-1)1234567891011121314151617def average_precision_approximated(p, r): """ Params: p: &#123;ndarray(n)&#125; Precision r: &#123;ndarray(n)&#125; Recall Returns: AP:&#123;float&#125; Notes: AP_&#123;approx&#125; = \sum_&#123;k=1&#125;^N P(k) \Delta r(k) """ n = len(p) AP = 0 for i in range(1, n): deltaR = r[i] - r[i-1] P_inter = p[i] # 每个点处的Precision AP += P_inter * deltaR # 矩形面积 return AP Interpolated average precision 换一种插值方法计算$P(k)$ P_{inter}(k) = \max_{\hat{k} \geq k} P(\hat{k})AP_{inter} = \sum_{k=1}^{N} P_{inter}(k) \Delta r(k) \tag{3}其中$\hat{k}$为第$k$个样本点后的样本索引，也即，$P_{inter}(k)$为第$k$个样本点后，最大的Precision值。 因为正样本影响Recall阈值，故式$(3)$也可写作 P_{inter}(k) = \max_{\hat{k} \geq k} P(\hat{k})AP_{inter} = \sum_{k=1}^{K} P_{inter}(k) \Delta r(k) \tag{4}其中$K$表示正样本的个数。 123456789101112131415161718def average_precision_interpolated(p, r): """ Params: p: &#123;ndarray(n)&#125; Precision r: &#123;ndarray(n)&#125; Recall Returns: AP:&#123;float&#125; Notes: P_&#123;inter&#125;(k) = \max_&#123;\hat&#123;k&#125; \geq k&#125; P(\hat&#123;k&#125;) AP_&#123;inter&#125; = \sum_&#123;k=1&#125;^&#123;N&#125; P_&#123;inter&#125;(k) \Delta r(k) """ n = len(p) AP = 0 for i in range(1, n): deltaR = r[i] - r[i-1] P_inter = max(p[i:]) # 每个点后最大的Precision AP += P_inter * deltaR # 矩形面积 return AP 11 points Interpolated Average Precision 固定选取${0, 0.1, \cdots, 1.0}$ 11个Recall阈值，选取$P_{inter}(k)$为每个阈值点后最大Precision值，进行计算 P_{inter}(k) = \max_{r(\hat{k}) \geq R(k)} P(\hat{k}), R \in \{0.0, 0.1, \cdots, 1.0\}AP_{inter} = \sum_{k=1}^{K} P_{inter}(k) \Delta r(k) \tag{4}123456789101112131415161718192021def average_precision_11_points(p, r): """ Params: p: &#123;ndarray(n)&#125; Precision r: &#123;ndarray(n)&#125; Recall Returns: AP:&#123;float&#125; Notes: P_&#123;inter&#125;(k) = \max_&#123;r(\hat&#123;k&#125;) \geq R(k)&#125; P(\hat&#123;k&#125;), R \in \&#123;0.0, 0.1, \cdots, 1.0\&#125; AP_&#123;inter&#125; = \sum_&#123;k=1&#125;^&#123;K&#125; P_&#123;inter&#125;(k) \Delta r(k) """ AP = 0 deltaR = 0.1 p = np.array(p); r = np.array(r) for i in range(11): if i == 10: continue R = i*deltaR P_inter = np.max(p[r&gt;R])# 每个阈值点后最大的Precision AP += P_inter * deltaR # 矩形面积 return AP 举例以一个二分类任务为例，假设我们得到样本真实标签与预测评分如下 Index Ground Truth Score 0 0 0.4 1 0 0.7 2 0 0.6 3 0 0.45 4 0 0.2 5 1 0.2 6 1 0.3 7 1 0.6 8 1 0.9 9 1 0.8 可依次设定阈值 thresh = 0.2, 0.2, 0.3, 0.4, 0.45, 0.6, 0.6, 0.7, 0.8, 0.9特别注意以下几个正样本Score对应阈值点，得表格如下 Index Ground Truth Score thresh=0.2 thresh=0.3 thresh=0.6 thresh=0.8 thresh=0.9 thresh=1.0 0 0 0.4 1 1 0 0 0 0 1 0 0.7 1 1 1 0 0 0 2 0 0.6 1 1 1 0 0 0 3 0 0.45 1 1 0 0 0 0 4 0 0.2 1 0 0 0 0 0 5 1 0.2 1 0 0 0 0 0 6 1 0.3 1 1 0 0 0 0 7 1 0.6 1 1 1 0 0 0 8 1 0.9 1 1 1 1 1 0 9 1 0.8 1 1 1 1 0 0 recall / / 1 0.8 0.6 0.4 0.2 0.0 precision / / 5/10 4/8 3/5 2/2 1/1 1.0 $P-R$曲线如下 123456789101112131415161718192021## 准备数据gt = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype='int')pred = np.array([0.4, 0.7, 0.6, 0.45, 0.2, 0.2, 0.3, 0.6, 0.9, 0.8])## 作P-R曲线，每个点均需计算thresh = list(np.sort(pred))[::-1]p = []; r = []for t in thresh: label = score2label(pred, t) n = pred[pred==t].shape[0] for i in range(n): p += [precision_score(gt, label)] r += [recall_score(gt, label)]plt.figure("P-R")plt.xlabel("recall"); plt.ylabel("precision")plt.ylim([0, 1.2])plt.grid()plt.plot(r, p)plt.scatter(r, p)plt.show() mAP对每一类计算AP，求其均值 mAP = \frac{1}{C} \sum_{i=0}^{C} AP_iPASCAL VOC原文devkit_doc/3.4.1如下 3.4.1 Average Precision (AP) The computation of the average precision (AP) measure was changed in 2010 to improve precision and ability to measure differences between methods with low AP. It is computed as follows: Compute a version of the measured precision/recall curve with precision monotonically decreasing, by setting the precision for recall r to the maximum precision obtained for any recall $r′ \geq r$. Compute the AP as the area under this curve by numerical integration. No approximation is involved since the curve is piecewise constant. Note that prior to 2010 the AP is computed by sampling the monotonically decreasing curve at a fixed set of uniformly-spaced recall values $0, 0.1, 0.2, \cdots, 1.0$. By contrast, VOC2010–2012 effectively samples the curve at all unique recall values. 记真实回归框为$B_{gt}$，预测回归框为$B_p$，则当预测框与真实框IoU大于阈值$0.5$时，认定物体被检测到，即 IoU_{gt, p} = \frac{area(B_p \bigcap B_{gt})}{area(B_p \bigcup B_{gt})} \geq 0.5其计算方法更加粗暴，计算每个Recall阈值点处最大Precision的均值 P_{inter}(r) = \max_{\hat{r}: \hat{r} \geq r} P(\hat{r})AP_{voc} = \frac{1}{11} \sum_{r \in \{0.0, 0.1, \cdots, 1.0\}} P_{inter}(r) \tag{5}123456789101112131415161718192021def average_precision_voc(p, r): """ Params: p: &#123;ndarray(n)&#125; Precision r: &#123;ndarray(n)&#125; Recall Returns: AP:&#123;float&#125; Notes: P_&#123;inter&#125;(r) = \max_&#123;\hat&#123;r&#125;: \hat&#123;r&#125; \geq r&#125; P(\hat&#123;r&#125;) AP_&#123;voc&#125; = \frac&#123;1&#125;&#123;11&#125; \sum_&#123;r \in \&#123;0.0, 0.1, \cdots, 1.0\&#125;&#125; P_&#123;inter&#125;(r) """ AP = 0 deltaR = 0.1 p = np.array(p); r = np.array(r) for i in range(11): R = i*deltaR if i != 10: P_inter = np.max(p[r&gt;R])# 每个阈值点后最大的Precision AP += P_inter # 矩形面积 AP = AP / 11 return AP COCOCOCO评估指标较多，共有12项，详细查看Detection Evaluation，以下仅介绍其AP计算方法，原文如下。 Average Precision (AP): primary challenge metric($AP$) IoU=.50:.05:.95 PASCAL VOC metric($AP^{IoU}=.50$) IoU=.50 strict metric($AP^{IoU}=.75$) IoU=.75 与VOC类似，但选取更多阈值的IoU，从中标记预测正确的样本进行计算。当$IoU = 0.5$时，即VOC计算方法。 Reference The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit 目标检测评价标准-AP mAP mean average precision（MAP）在计算机视觉中是如何计算和应用的？]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NMS & soft-NMS]]></title>
    <url>%2F2019%2F05%2F26%2FNMS-softer-NMS%2F</url>
    <content type="text"><![CDATA[前言在目标检测中，若多个回归框(Bounding Box)重叠内容较多，则可以保留其中几个，删除其余冗余的框，非极大值抑制(Non-Maximum Suppression, NMS)算法可实现该功能。 那么用什么指标评价两个回归框重叠过多呢，以下介绍图像的交并比IoU(Intersection over Union)。 IoU原理IoU的思路非常简单，即计算两个回归框交集部分面积与并集面积的比值，如下图，其计算公式为 IoU_{a, b} = \frac{Area_{inter}}{Area_{union}}其中 Area_{union} = Area_{a} + Area_{b} - Area_{inter}Area_{inter} = w_{inter}*h_{inter}w_{inter} = x^a_2 - x^b_1h_{inter} = y^a_2 - y^b_1 代码算法实现中，比较关键的一点是计算交集部分的坐标点，记两个回归框$a, b$左上角坐标为$(x^_1, y^_1)$，右下角为$(x^_2, y^_2)$ 则应有 x^{inter}_1 = \max \{x^a_1, x^b_1\}, y^{inter}_1 = \max \{y^a_1, y^b_1\}x^{inter}_2 = \min \{x^a_2, x^b_2\}, y^{inter}_2 = \min \{y^a_2, y^b_2\} 使用C/C++实现如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950typedef struct &#123; float x1, y1, x2, y2;&#125;bbox;float bbox_overlap(float ax1, float ax2, float bx1, float bx2)&#123; float left = _max(ax1, bx1); float right = _min(ax2, bx2); float gap = right - left; return gap;&#125;float bbox_intersection(bbox a, bbox b)&#123; float w = bbox_overlap(a.x1, a.x2, b.x1, b.x2); float h = bbox_overlap(a.y1, a.y2, b.y1, b.y2); if(w &lt; 0 || h &lt; 0) return 0; float area = w*h; return area;&#125;float bbox_area(bbox a)&#123; float w = a.x2 - a.x1; float h = a.y2 - a.y1; return w*h;&#125;float bbox_union(bbox a, bbox b, int mode)&#123; float u = 0; float area_a = bbox_area(a); float area_b = bbox_area(b); if (mode == 0)&#123; u = area_a + area_b - bbox_intersection(a, b); &#125; else if (mode == 1)&#123; u = _min(area_a, area_b); &#125; return u;&#125;float bbox_iou(bbox a, bbox b, int mode)&#123; float i = bbox_intersection(a, b); float u = bbox_union(a, b, mode); return i/u;&#125; NMS以下图为例，其中三个候选框$a,b,c$，其评分依次为0.8, 0.7, 0.9，设定IoU阈值 thresh=0.4计算步骤如下 将其排序，如降序排序， 得到结果$c,a,b$； 保存当前评分最高的回归框，即$c$； 计算$c$与剩余框，即$a,b$的IoU，即IoU_{c,a} = \frac{1×8}{10×9 + 9×11 - 1×8} = 0.044IoU_{c,b} = \frac{4×6}{10×9 + 10×11 - 4×6} = 0.136 无大于阈值的框，故无框被删除； 保存当前评分最高的回归框，即$a$； 计算$a$与剩余框即$b$的IoU，即IoU_{a,b} = \frac{7×9}{9×11 + 10×11 - 7×9} = 0.432 大于阈值，故删除$b$； 最终保留框$a,c$； 代码 Python 123456789101112131415161718192021222324252627282930313233343536373839404142434445def _nms(dets, thresh, mode="Union"): """ Params: dets: &#123;ndarray(n_boxes, 5)&#125; x1, y1, x2, y2 score thresh: &#123;float&#125; retain overlap &lt;= thresh mode: &#123;str&#125; 'Union' or 'Minimum' Returns: idx: &#123;list[int]&#125; indexes to keep Notes: greedily select boxes with high confidence idx boxes overlap &lt;= thresh rule out overlap &gt; thresh if thresh==1.0, keep all """ x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) order = scores.argsort()[::-1] idx = [] while order.size &gt; 0: i = order[0] idx.append(i) xx1 = np.maximum(x1[i], x1[order[1:]]) yy1 = np.maximum(y1[i], y1[order[1:]]) xx2 = np.minimum(x2[i], x2[order[1:]]) yy2 = np.minimum(y2[i], y2[order[1:]]) w = np.maximum(0.0, xx2 - xx1 + 1) h = np.maximum(0.0, yy2 - yy1 + 1) inter = w * h if mode == "Union": ovr = inter / (areas[i] + areas[order[1:]] - inter) elif mode == "Minimum": ovr = inter / np.minimum(areas[i], areas[order[1:]]) inds = np.where(ovr &lt;= thresh)[0] order = order[inds + 1] return idx C/C++ 12345678910111213141516171819202122232425262728293031323334353637383940typedef struct detect&#123; float score; /* 该框评分 */ bbox bx; /* 回归方框 */ bbox offset; /* 偏置 */ landmark mk; /* 位置 */&#125; detect;// decending order, bubblevoid bsort(detect** dets, int n)&#123; for (int i = 0; i &lt; n - 1; i++ )&#123; for (int j = 0; j &lt; n - 1 - i; j++ )&#123; float a = (*dets)[j].score; float b = (*dets)[j+1].score; if (a &lt; b)&#123; detect tmp = (*dets)[j]; (*dets)[j] = (*dets)[j+1]; (*dets)[j+1] = tmp; &#125; &#125; &#125;&#125;void _nms(detect* dets, int n, float thresh, int mode)&#123; bsort(&amp;dets, n); for (int i = 0; i &lt; n; i++) &#123; if (dets[i].score == 0) continue; // 表示该框已被删除 bbox a = dets[i].bx; for (int j = i + 1; j &lt; n; j++) &#123; bbox b = dets[j].bx; if (bbox_iou(a, b, mode) &gt; thresh) dets[j].score = 0; // 删除该框 &#125; &#125;&#125; Soft-NMS原理对于两个相邻较近的同类物体，如下图，NMS可能将左框删除，这是由于在删除同类别框时，只考虑了IoU重叠大小，并没有将两个框的评分引入计算，当两个框相邻很近但评分都较高时，不能简单地删除。 改进后的soft-NMS伪代码如下 也可统一写作 s_i \leftarrow s_i f(iou(\mathcal{M}, b_i))其中NMS算法引入hard threshold，即 f(iou(\mathcal{M}, b_i)) = \begin{cases} 1 & iou(\mathcal{M}, b_i) < N_t \\ 0 & iou(\mathcal{M}, b_i) \geq N_t \end{cases} 应考虑以下因素 相邻检测的分数应该降低到它们具有增加假阳性率(false positive rate)的可能性较小的程度，同时在排序的检测列表中高于明显的假阳性。 完全去除具有低NMS阈值的相邻检测将是次优的并且当在高重叠阈值处执行评估时将增加未命中率。 当使用高NMS阈值时，在一系列重叠阈值上测量的平均精度将下降。 基于以上分析，改进$f(iou(\mathcal{M}, b_i))$为 f(iou(\mathcal{M}, b_i)) = \begin{cases} 1 & iou(\mathcal{M}, b_i) < N_t \\ 1 - iou(\mathcal{M}, b_i) & iou(\mathcal{M}, b_i) \geq N_t \end{cases}其函数图像如下，在超过阈值时，为线性函数，且重叠越多，其抑制效果越大或称惩罚越多 但该函数不连续，可能导致NMS结果的突然改变，故修改为非线性连续函数如下 f(iou(\mathcal{M}, b_i)) = \exp (-\frac{iou(\mathcal{M}, b_i)^2}{\sigma}), \forall b_i \notin \mathcal{D} 注意到$x = \sqrt{\frac{\sigma}{2}}$为函数$f(x)$的拐点 代码对上面代码作如下修改 123456789101112131415161718192021222324252627282930float _f(float x, float sigma, int soft)&#123; float y = 0; if (soft == 0)&#123; y = x &lt; sigma? 1: 0; &#125; else &#123; y = exp(- pow(x, 2) / sigma); &#125; return y;&#125;void _nms(detect* dets, int n, float sigma, int mode, int soft)&#123; bsort(&amp;dets, n); // do nms for (int i = 0; i &lt; n; i++) &#123; if (dets[i].score == 0) continue; bbox a = dets[i].bx; for (int j = i + 1; j &lt; n; j++) &#123; bbox b = dets[j].bx; // if (bbox_iou(a, b, mode) &gt; thresh) // dets[j].score = 0; // 删除该框 dets[j].score *= _f(bbox_iou(a, b, mode), sigma, soft); &#125; &#125;&#125; Reference Efficient Non-Maximum Suppression 非极大值抑制(Non-Maximum Suppression) Non-Maximum Suppression for Object Detection in Python Improving Object Detection With One Line of Code Soft NMS算法笔记]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github清除commit记录]]></title>
    <url>%2F2019%2F05%2F22%2FGithub%E6%B8%85%E9%99%A4commit%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言当commit记录过多时，仓库会过大难以下载，本文介绍删除commit记录的方法。 Reference 新建无任何文件的分支 1git checkout --orphan new 添加文件并提交 12git add -Agit commit -am "recommit" 删除主分支 1git branch -D master 重命名当前分支 1git branch -m master 强制更新远程仓库 1git push -f origin master]]></content>
      <categories>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Install nVidia drivers on Ubuntu]]></title>
    <url>%2F2019%2F05%2F08%2FInstall-nVidia-drivers-on-Ubuntu%2F</url>
    <content type="text"><![CDATA[PPA安装 禁用nouveau驱动先将Ubuntu系统集成的显卡驱动程序nouveau从linux内核卸载 查看当前驱动状态123456789$ lsmod | grep nouveaunouveau 1851392 1mxm_wmi 16384 1 nouveaui2c_algo_bit 16384 2 i915,nouveauttm 110592 1 nouveaudrm_kms_helper 172032 2 i915,nouveaudrm 458752 8 drm_kms_helper,i915,ttm,nouveauwmi 24576 3 wmi_bmof,mxm_wmi,nouveauvideo 45056 3 thinkpad_acpi,i915,nouveau 添加黑名单1234567891011121314$ ll /etc/modprobe.d/blacklist.conf-rw-r--r-- 1 root root 1667 11月 13 05:54 /etc/modprobe.d/blacklist.conf$ sudo chmod 666 /etc/modprobe.d/blacklist.conf[sudo] password for louishsu: $ sudo vim /etc/modprobe.d/blacklist.conf $ sudo chmod 644 /etc/modprobe.d/blacklist.conf$ sudo update-initramfs -uupdate-initramfs: Generating /boot/initrd.img-4.18.0-17-generic...I: Set the RESUME variable to override this. 重启后查看驱动状态，无输出表示禁用成功1$ lsmod | grep nouveau 安装驱动 这里使用PPA方式安装，首先添加源12$ sudo add-apt-repository ppa:graphics-drivers/ppa$ sudo apt-get update 查看合适的驱动版本，如下recommended1$ ubuntu-drivers devices 按ctrl+alt+F1进入tty模式，关闭图形桌面显示管理器LightDM1$ service lightdm stop 安装驱动12$ sudo apt-get install nvidia-418$ sudo reboot 查看安装情况1234567891011121314151617181920$ sudo nvidia-smiWed May 8 20:22:55 2019 +------------------------------------------------------+ | NVIDIA-SMI 340.107 Driver Version: 340.107 | |-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GT 730M Off | 0000:04:00.0 N/A | N/A || N/A 46C P0 N/A / N/A | 185MiB / 1023MiB | N/A Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Compute processes: GPU Memory || GPU PID Process name Usage ||=============================================================================|| 0 Not Supported |+-----------------------------------------------------------------------------+$ sudo nvidia-settings CUDA查看Ubuntu编译安装Tensorflow Reference ubuntu16.04下NVIDIA GTX965M显卡驱动PPA安装]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Face Detection: MTCNN]]></title>
    <url>%2F2019%2F05%2F05%2FFace-Detection-MTCNN%2F</url>
    <content type="text"><![CDATA[前言MTCNN即Multi-task Cascaded Convolutional Networks，利用深度学习方法进行人脸识别、检测与关键点定位，可谓结合机器视觉领域三大任务为一体。 该算法中，人脸检测与识别视作分类任务，即判别框内图像是否包含人脸；定位视作回归任务，共需确定7个坐标点，依次为：回归框左上、右下坐标，左眼、右眼、鼻尖、左嘴角、右嘴角坐标。 在设计损失函数时，分类任务采用交叉熵(Cross Entropy)，回归任务采用均方误差(MSE)，并且三个任务的损失，可给定不同的系数进行网络训练，使各网络侧重点不同，即PNet与RNet侧重于人脸识别与回归框定位，ONet侧重于关键点定位。 算法对比 网络结构共设计3个卷积网络，每个网络均可输出识别概率(1)、回归框坐标(2×2)、关键点坐标(5×2)，三个网络级联以获得良好的预测结果。各网络结构图如下 分类任务输出层可采用softmax，即视作多分类任务；或者采用sigmoid，视作二分类任务。 1. P-Net： Proposal Network检测任务比较重要的一步是产生数目足够多的候选框，PNet设计全卷积网络，可接受任意大小的图片输入，利用输出的特征图生成候选框，具体生成算法在检测算法中说明。该网络在训练时接受$12×12×3$的图像输入，各层参数如下。 123456layer filters size input output0 conv 10 3 x 3 &#x2F; 1 12 x 12 x 3 -&gt; 10 x 10 x 10 0.000 BFLOPs1 max 2 x 2 &#x2F; 2 10 x 10 x 10 -&gt; 5 x 5 x 102 conv 16 3 x 3 &#x2F; 1 5 x 5 x 10 -&gt; 3 x 3 x 16 0.000 BFLOPs3 conv 32 3 x 3 &#x2F; 1 3 x 3 x 16 -&gt; 1 x 1 x 32 0.000 BFLOPs4 conv 15 1 x 1 &#x2F; 1 1 x 1 x 32 -&gt; 1 x 1 x 15 0.000 BFLOPs 2. R-Net： Refine NetworkPNet产生候选框后，将这些候选框内的图像数据分割并缩放到统一大小，输入到RNet改善识别结果。该网络最后增加全连接层，仅接受$24×24×3$的图像输入，各层参数如下。 12345678layer filters size input output0 conv 28 3 x 3 &#x2F; 1 24 x 24 x 3 -&gt; 22 x 22 x 28 0.001 BFLOPs1 max 3 x 3 &#x2F; 2 22 x 22 x 28 -&gt; 11 x 11 x 282 conv 48 3 x 3 &#x2F; 1 11 x 11 x 28 -&gt; 9 x 9 x 48 0.002 BFLOPs3 max 3 x 3 &#x2F; 2 9 x 9 x 48 -&gt; 4 x 4 x 484 conv 64 2 x 2 &#x2F; 1 4 x 4 x 48 -&gt; 3 x 3 x 64 0.000 BFLOPs5 connected 576 -&gt; 1286 connected 128 -&gt; 15 3. O-Net： Output Network该网络功能与RNet相同，不同的是分辨率更高，网络层次更深，且训练过程中，损失的设置更偏重于关键点的回归。接受$48×48×3$的图像输入，各层参数如下。 12345678910layer filters size input output0 conv 32 3 x 3 &#x2F; 1 48 x 48 x 3 -&gt; 46 x 46 x 32 0.004 BFLOPs1 max 3 x 3 &#x2F; 2 46 x 46 x 32 -&gt; 23 x 23 x 322 conv 64 3 x 3 &#x2F; 1 23 x 23 x 32 -&gt; 21 x 21 x 64 0.016 BFLOPs3 max 3 x 3 &#x2F; 2 21 x 21 x 64 -&gt; 10 x 10 x 644 conv 64 3 x 3 &#x2F; 1 10 x 10 x 64 -&gt; 8 x 8 x 64 0.005 BFLOPs5 max 2 x 2 &#x2F; 2 8 x 8 x 64 -&gt; 4 x 4 x 646 conv 128 2 x 2 &#x2F; 1 4 x 4 x 64 -&gt; 3 x 3 x 128 0.001 BFLOPs7 connected 1152 -&gt; 2568 connected 256 -&gt; 15 训练数据1. 数据集 人脸检测 WIDER FACE是目前最常用的训练集，也是目前最大的公开训练集，人工标注的风格比较友好，适合训练。总共32203图像，393703标注人脸，目前难度最大，各种难点比较全面：尺度，姿态，遮挡，表情，化妆，光照等。 WIDER FACE有以下特点： 图像分辨率普遍偏高，所有图像的宽都缩放到1024，最小标注人脸10*10，都是彩色图像； 每张图像的人脸数据偏多，平均12.2人脸/图，密集小人脸非常多； 分训练集train/验证集val/测试集test，分别占40%/10%/50%，而且测试集的标注结果(ground truth)没有公开，需要提交结果给官方比较，更加公平公正，而且测试集非常大，结果可靠性极高； 根据EdgeBox的检测率情况划分为三个难度等级：Easy, Medium, Hard。 关键点定位 CNN FACE POINT，包含5个关键点位置。 Training set: Download It contains 5,590 LFW images and 7,876 other images downloaded from the web. The training set and validation set are defined in trainImageList.txt and testImageList.txt, respectively. Each line of these text files starts with the image name, followed by the boundary positions of the face bounding box retured by our face detector, then followed by the positions of the five facial points.Testing set: Download It contains the 1,521 BioID images, 781 LFPW training images, and 249 LFPW test images used in our testing, together with the text files recording the boundary positions of the face bounding box retured by our face detector for each dataset. A few images that our face detector failed are not listed in the text files. LFPW images are renamed for the convenience of processing. 2. 训练数据生成采用了Hard Example Mining，后一网络的训练数据由前一网络结果生成，即先用训练好的前一网络进行数据评估，在评分较低、难以检测的数据中继续采样。在生成数据时，使用数据增广。 七个关键点坐标转换为偏移量(offset)，使其不受图像缩放影响，且数值较小便于网络收敛，计算方法如下 Bounding Box offset_{x_1'} = \frac{x_1' - x_1}{size} offset_{x_2'} = \frac{x_2' - x_2}{size} $y_n$同$x_n$; $(x_1, y_1)$表示左上角点位置，$(x_2, y_2)$表示右下角点位置; $(x_1’, y_1’)$表示ground true矩形方框位置; $size$表示方形回归框边长，即$size = x_2 - x_1 = y_2 - y_1$; Landmark 均以正方形回归框左上方点坐标作为基准 offset_{x_n''} = \frac{x_n'' - x_1}{size} offset_{y_n''} = \frac{y_n'' - y_1}{size} $(x_n’’, y_n’’)$表示五个关键点坐标; $(x_1’, y_1’)$表示回归框左上角点位置; $size$表示方形回归框边长，即$size = x_2 - x_1 = y_2 - y_1$; 根据groudtruth随机偏移和旋转，切割人脸数据，根据iou评分，共记作3种标签的样本，其标签分别为 Positive(1): $iou &gt; 0.65$，其数据格式样例如下 1xxx&#x2F;positive&#x2F;404031.jpg 1.0 0.18 -0.09 0.15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Negative(0): $iou &lt; 0.3$，其数据格式样例如下 1xxx&#x2F;negative&#x2F;92073.jpg 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Part(-1): $0.4 \leq iou \leq 0.65$，其数据格式样例如下 1xxx&#x2F;part&#x2F;749569.jpg -1.0 -0.04 0.07 -0.15 0.36 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 此外，关键点数据标签记作Landmark(-2)，其数据格式样例如下 1xxx&#x2F;train_PNet_landmark_aug&#x2F;4.jpg -2.0 0.0 0.0 0.0 0.0 0.24367088607594936 0.17405063291139242 0.7563291139240507 0.2310126582278481 0.48417721518987344 0.6170886075949367 0.2310126582278481 0.8069620253164557 0.6677215189873418 0.8575949367088608 损失函数1. Classification loss人脸识别为分类任务，采用二分类交叉熵损失函数，即 L^{(i)}_{cls} = - y^{(i)} \log (p^{(i)}) - (1 - y^{(i)}) \log (1 - p^{(i)})注意，在计算分类损失时，将Negative(0), Part(-1), Landmark(-2)标签均视作0。 2. Regression loss回归任务，采用最小方差准则，即 L^{(i)}_{bbox} = \frac{1}{4} \sum_{j=1}^4 (pred^{(i)}_j - gt^{(i)}_j)^2 L^{(i)}_{landmark} = \frac{1}{10} \sum_{j=1}^{10} (pred^{(i)}_j - gt^{(i)}_j)^2注意，在计算回归框损失时，仅对Positive(1), Part(-1)样本进行，计算关键点损失时，仅对Landmark(-2)样本进行。 3. OHEMOHEM即Online Hard Example Mining，在线硬样本数据挖掘，即对于当前批次数据，计算损失时，选取总损失值最大的k个样本，计算其均值作为该批次的损失值。 4. Total Loss三个网络赋予各损失的系数不同， 使其偏重于其中某个任务 L_{total} = \frac{1}{topk} \sum_{i=0}^{topk} coef_{cls} L^{(i)}_{cls} + coef_{bbox} L^{(i)}_{bbox} + coef_{landmark} L^{(i)}_{landmark}其中$topk$即OHEM计算得到的样本数。 PyTorch实现 class MtcnnLoss(nn.Module): def __init__(self, cls, bbox, landmark, ohem=0.7): super(MtcnnLoss, self).__init__() self.cls = cls self.bbox = bbox self.landmark = landmark self.ohem = ohem self.bce = nn.BCEWithLogitsLoss(reduction='none') self.mse = nn.MSELoss(reduction='none') def forward(self, pred, gt): """ Params: pred: {tensor(N, n) or tensor(N, n, 1, 1)} gt: {tensor(N, n)} Notes: y_true """ N = pred.shape[0] pred = pred.view(N, -1) ## origin label gt_labels = gt[:, 0] ## pos -> 1, neg -> 0, others -> 0 pred_cls = pred[:, 0] gt_cls = gt_labels.clone(); gt_cls[gt_labels!=1.0] = 0.0 loss_cls = self.bce(pred_cls, gt_cls) # ohem n_keep = int(self.ohem * loss_cls.shape[0]) loss_cls = torch.mean(torch.topk(loss_cls, n_keep)[0]) ## label=1 or label=-1 then do regression idx = (gt_labels==1)^(gt_labels==-1) pred_bbox = pred[idx, 1: 5] gt_bbox = gt[idx, 1: 5] loss_bbox = self.mse(pred_bbox, gt_bbox) loss_bbox = torch.mean(loss_bbox, dim=1) # ohem n_keep = int(self.ohem * loss_bbox.shape[0]) loss_bbox = torch.mean(torch.topk(loss_bbox, n_keep)[0]) ## keep label =-2 then do landmark detection idx = gt_labels==-2 pred_landmark = pred[idx, 5:] gt_landmark = gt[idx, 5:] loss_landmark = self.mse(pred_landmark, gt_landmark) loss_landmark = torch.mean(loss_landmark, dim=1) # ohem n_keep = int(self.ohem * loss_landmark.shape[0]) loss_landmark = torch.mean(torch.topk(loss_landmark, n_keep)[0]) ## total loss loss_total = self.cls*loss_cls + self.bbox*loss_bbox + self.landmark*loss_landmark return loss_total, loss_cls, loss_bbox, loss_landmark loss_coef = { 'PNet': [1.0, 0.5, 0.5], 'RNet': [1.0, 0.5, 0.5], 'ONet': [1.0, 0.5, 1.0], } 检测算法检测算法以功能区分，主要分成两个部分：候选框生成与候选框筛除。 1. 候选框生成 该步骤使用的网络为PNet。指定超参数minface，对于任意尺寸输入的图像H × W，先将其缩放 H_c = H × \frac{12}{minface} W_c = W × \frac{12}{minface}指定超参数factor，更新缩小尺度，将图片缩小，在每个尺度上进行计算，即 H_c := H_c × factor W_c := W_c × factor对于某一尺度下的图片得到的运算特征图， Feat_{h×w×15} = PNet(input)提取其分类层输出特征图$Feat_{cls}$，设定阈值thresh，对于大于阈值的点，按下式生成候选框 x_1 = stride × i × \frac{1}{scale} y_1 = stride × j × \frac{1}{scale} x_2 = x_1 + \frac{cellsize}{scale} y_2 = y_1 + \frac{cellsize}{scale}其中cellsize为超参数，一般指定为cellsize=12 2. 候选框筛除 改步使用网络PNet与ONet，依次对上一层网络进行refine，计算方法一致。 获取上一层网络输出回归框，截取图片中相应位置的图像数据，并缩放到对应尺寸； 前向计算，得到特征输出； 设定阈值，只保留分类评估大于阈值的结果； 对剩余结果进行NMS，输出结果； 3. NMS 例如，有中三个候选框a, b, c，其评分依次为0.8, 0.7, 0.9，设定NMS阈值 thresh=0.4 先将其排序，以降序排序为c, a, b； 保存当前评分最高的回归框，即c； 计算c与a, b的IoU，计算方法如下 IoU = \frac{Intersection}{Union} 其中 Intersection = w_{inter} × h_{inter} w_{inter} = \min (x^a_2, x^b_2) - \max (x^a_1, x^b_1) h_{inter} = \min (y^a_2, y^b_2) - \max (y^a_1, y^b_1) 而 Union = Area_a + Area_b - Intersection 则c与a, b的IoU为 IoU_{a,c} = \frac{1×8}{10×9+9×11-1×8}=0.044 IoU_{a,b} = \frac{4×6}{10×9+10×11-4×6}=0.136 均小于阈值，故无框被删除。 保存当前评分最高的回归框，即a； 计算a与b的IoU IoU_{a,b} = \frac{7×9}{9×11+10×11-7×9} = 0.432 大于阈值，删除候选框b 最终保留a，c。 代码详情查看isLouisHsu/MTCNN_Darknet/torch_mtcnn/detector.py - Github。 附：利用关键点进行图像对齐 变换矩阵$M$的求解 例如现有$n$个关键点 xy = \left[\begin{matrix} x_1 & y_1 \\ x_2 & y_2 \\ ... & ... \\ x_n & y_n \\ \end{matrix}\right] 希望对齐后的坐标点为 \hat{xy} = \left[\begin{matrix} \hat{x_1} & \hat{y_1} \\ \hat{x_2} & \hat{y_2} \\ ... & ... \\ \hat{x_n} & \hat{y_n} \\ \end{matrix}\right] 构造矩阵 X_{2n\times4} = \left[\begin{matrix} \vec{x} & \vec{y} & \vec{1} & \vec{0} \\ \vec{y} & -\vec{x} & \vec{0} & \vec{1} \end{matrix}\right] b_{2n} = \left[\begin{matrix} \hat{x_1} & \hat{x_2} & \cdots & \hat{x_n} & \hat{y_1} & \hat{y_2} & \cdots & \hat{y_n} \end{matrix}\right]^T 其中 \vec{x} = \left[\begin{matrix} x_1 & x_2 & \cdots & x_n \end{matrix}\right]^T \vec{y} = \left[\begin{matrix} y_1 & y_2 & \cdots & y_n \end{matrix}\right]^T \vec{1} = \left[\begin{matrix} 1 & 1 & \cdots & 1 \end{matrix}\right]^T \vec{0} = \left[\begin{matrix} 0 & 0 & \cdots & 0 \end{matrix}\right]^T 求解下式解向量$r_{4\times1}$ X \cdot r = b 注意增广矩阵的秩 \text{rank}(X) < rank([X | b]) 上式无解，可使用伪逆求解 r = (X^T X + \lambda I)^{-1} X^T b 构造矩阵 R = \left[\begin{matrix} r_1 & -r_2 & 0 \\ r_2 & r_1 & 0 \\ r_3 & -r_4 & 1 \\ \end{matrix}\right] 则变换矩阵$M$可由下式求解 \left[\begin{matrix} M^T & \begin{matrix} 0 \\ 0 \\ 1 \end{matrix} \end{matrix}\right] = R^{-1} 即$M$为$R^{-1}$的前两列。 坐标变换 M = \left[\begin{matrix} m_{11} & m_{12} & m_{13} \\ m_{21} & m_{22} & m_{23} \end{matrix}\right] 对于坐标$(x, y)$，其变换后的坐标$(\hat{x}, \hat{y})$为 \left[\begin{matrix} \hat{x} \\ \hat{y} \\ \end{matrix}\right] = M \left[\begin{matrix} x \\ y \\ 1 \end{matrix}\right] 几个关键的函数，C/C++实现如下，详情可查看isLouisHsu/MobileFaceNet_Darknet/src/cp2form.c123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/* * @param * uv: [u, v]， Nx2 * xy: [x, y]， Nx2 * @return * @notes * - Xr = Y ===&gt; r = (X^T X + \lambda I)^&#123;-1&#125; X^T Y */CvMat* _findNonreflectiveSimilarity(const CvMat* uv, const CvMat* xy)&#123; CvMat* X = _stitch(xy); // 2N x 4 CvMat* XT = cvCreateMat(X-&gt;cols, X-&gt;rows, CV_32F); cvTranspose(X, XT); // 4 x 2N CvMat* XTX = cvCreateMat(XT-&gt;rows, X-&gt;cols, CV_32F); cvMatMul(XT, X, XTX); // 4 x 4 for (int i = 0; i &lt; XTX-&gt;rows; i++) XTX-&gt;data.fl[i*XTX-&gt;rows + i] += 1e-15; CvMat* XTXi = cvCreateMat(XTX-&gt;rows, XTX-&gt;cols, CV_32F); cvInvert(XTX, XTXi, CV_LU); // 4 x 4 // ----------------------------------------------------------------------- CvMat* uvT = cvCreateMat(uv-&gt;cols, uv-&gt;rows, CV_32F); cvTranspose(uv, uvT); // 2 x N CvMat header; CvMat* YT = cvReshape(uvT, &amp;header, 0, 1); // 1 x 2N TODO CvMat* Y = cvCreateMat(YT-&gt;cols, YT-&gt;rows, CV_32F); cvTranspose(YT, Y); // 2N x 1 CvMat* XTXiXT = cvCreateMat(XTXi-&gt;rows, XT-&gt;cols, CV_32F); CvMat* r = cvCreateMat(XTXiXT-&gt;rows, Y-&gt;cols, CV_32F); cvMatMul(XTXi, XT, XTXiXT); cvMatMul(XTXiXT, Y, r); // 4 x 1 // ----------------------------------------------------------------------- cvReleaseMat(&amp;X); cvReleaseMat(&amp;XT); cvReleaseMat(&amp;XTX); cvReleaseMat(&amp;XTXi); cvReleaseMat(&amp;XTXiXT); cvReleaseMat(&amp;uvT); cvReleaseMat(&amp;Y); // ======================================================================= CvMat* R = cvCreateMat(3, 3, CV_32F); R-&gt;data.fl[0 * 3 + 0] = r-&gt;data.fl[0]; R-&gt;data.fl[0 * 3 + 1] = -r-&gt;data.fl[1]; R-&gt;data.fl[0 * 3 + 2] = 0.; R-&gt;data.fl[1 * 3 + 0] = r-&gt;data.fl[1]; R-&gt;data.fl[1 * 3 + 1] = r-&gt;data.fl[0]; R-&gt;data.fl[1 * 3 + 2] = 0.; R-&gt;data.fl[2 * 3 + 0] = r-&gt;data.fl[2]; R-&gt;data.fl[2 * 3 + 1] = r-&gt;data.fl[3]; R-&gt;data.fl[2 * 3 + 2] = 1.; CvMat* Ri = cvCreateMat(R-&gt;cols, R-&gt;rows, CV_32F); cvInvert(R, Ri, CV_LU); CvMat* MT = cvCreateMat(3, 2, CV_32F); cvGetSubRect(Ri, MT, cvRect(0, 0, 2, 3)); CvMat* M = cvCreateMat(MT-&gt;cols, MT-&gt;rows, CV_32F); cvTranspose(MT, M); // ----------------------------------------------------------------------- cvReleaseMat(&amp;r); cvReleaseMat(&amp;R); cvReleaseMat(&amp;Ri); cvReleaseMat(&amp;MT); return M;&#125;/* * @param * uv: [u, v]， Nx2 * xy: [x, y]， Nx2 * @return * @notes */CvMat* _findReflectiveSimilarity(const CvMat* uv, const CvMat* xy)&#123; CvMat* xyR = cvCloneMat(xy); for (int r = 0; r &lt; xyR-&gt;rows; r++) xyR-&gt;data.fl[r*xyR-&gt;cols] *= -1; CvMat* M1 = _findNonreflectiveSimilarity(uv, xy); CvMat* M2 = _findNonreflectiveSimilarity(uv, xyR); cvReleaseMat(&amp;xyR); for (int r = 0; r &lt; M2-&gt;rows; r++) M2-&gt;data.fl[r*M2-&gt;cols] *= -1; CvMat* xy1 = _tformfwd(M1, uv); CvMat* xy2 = _tformfwd(M2, uv); cvSub(xy1, xy, xy1, NULL); cvSub(xy2, xy, xy2, NULL); float norm1 = _matrixNorm2(xy1); float norm2 = _matrixNorm2(xy2); cvReleaseMat(&amp;xy1); cvReleaseMat(&amp;xy2); if (norm1 &lt; norm2)&#123; cvReleaseMat(&amp;M2); return M1; &#125; else &#123; cvReleaseMat(&amp;M1); return M2; &#125;&#125; Reference Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks kpzhang93/MTCNN_face_detection_alignment AITTSMD/MTCNN-Tensorflow]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[无所畏]]></title>
    <url>%2F2019%2F04%2F30%2F%E6%97%A0%E6%89%80%E7%95%8F%2F</url>
    <content type="text"><![CDATA[前言世间没有佛，但是有带着佛性的人。 精选壹 成功十要素一命二运三风水四积阴德五读书六名七相八敬神九交贵人十养生 曾几何时，我们除了未来一无所有，我们充满好奇，我们有使不完的力气，我们不怕失去，我们眼里有光，我们为建设祖国而读书，我们下身肿胀，我们激素吱吱作响，我们热爱姑娘，我们万物生长。 如何避免成为油腻中年男 不要成为胖子，曾经玉树临风，现在风狂树残； 不要停止学习，吹牛能让我们有瞬间快感，但不能改变我们对一些事物所知甚少的事实； 不要待着不动，说走就走，去散步，去旅行，也好； 不要当众谈性，关于眼神(盯着女生看)的告诫，也适用于权、钱等其他领域； 不要追忆从前，积攒唠叨从前的力气，再创业，再创造，再恋爱，我们还能攻城略地，杀伐战取； 不要教育晚辈，不愤不启； 不要给别人添麻烦； 不要停止购物，完全没了欲望，失去对美好事物的贪心，生命也就没有乐趣； 不要脏兮兮，少年时代的脏是不羁，中年时代的脏是真脏； 不要鄙视和年龄无关的人类习惯，所有的世道变坏，都是从鄙视文艺开始的； 因为苦逼而牛逼，因为逗逼而二逼，因为装逼而傻逼。愿我们原理油腻和猥琐，敬爱女生，过好余生，让世界更美好。 更可怕的是成为了油腻青年 装懂，多学习，多研究，对真正热爱之事，真正投入精力，向那些可以就防晒美白详细说出八种不同方法的女性好好学习； 着急，“夫水之己也不厚，其负大舟也无力”； 逐利，只有对钱的热忱却没有理想，即使站在了风口，也不会成为那只“会飞的猪”； 不要迷恋肉身； 迷恋手机，比起摸不到心爱的姑娘的手，摸不到自己的手机似乎要严重百倍； 不靠谱，将来总有一天，你会明白，困境、死境都是自己曾经立起又自己放倒的目标； 不敢真，对爱的人不敢说“爱”，对不爽的事不敢说“不”，不敢承认自己的处境，不敢承认失败然后从头再来。时过境迁，回过头来，要拿真心对世界的时候，大抵已经找不到心在哪儿了； 假佛系，假装自己无欲无求，其实只是懒得追求，你就不厌倦自己吗？ 审美差，如今担心会跟审美不好的人撞了女朋友的脸(整容)，哈哈哈哈； 不要“脸”，所谓“相由心生”，脸上的油光，就是心里的油渣； “极品”男人如何极致装逼 写信，比如冯唐、比如曾巩； 跑步； 喝茶； 古物，从骨子里明白拥有只是暂时，“欣于所遇，暂得于己，快然自足”； 言语，极致地吹牛逼也是极致装逼的一种，立言也是立德、立功、立言三不朽的一种； 读书； 情怀，极致装逼如下，“为天地立心，为生民立命，为往圣继绝学，为万世开太平”； 喝酒，和好玩儿的人喝，喝完能背出很多唐诗和楚辞； 养生，高逼格的养生是乐生，是在乐生的基础上长生。我老爸抽烟，从十二岁开始抽，现在八十三岁了，他的口头禅是：“天亮了，又赚了。” 修佛，高逼格的修佛是在日常的劳作里、阳光里、花花草草里、众生皆苦里、生命终极无意义里，试图体会到蹦蹦跳跳的快活； 其实，如果志存高远，“三观”正，逼格正，装逼装久了，就是身、心、灵的一部分了。装逼装极致了，就得大成就了。装逼的过程就是学习的过程，就是感受活着的过程，就是实现理想的过程。 那些爱我们或者爱过我们的女生，在她们的一生中要花很多时间陪护我们这些装腔犯，安静地、积极地、有创造力地陪我们装逼好多年。……。如果爱不在了，那就不用管上面说的一切了，让他找别的姑娘配他装逼陪他飞吧。 天天临深履薄，这辈子好惨，而且睡眠毁了、人毁了，也就什么都没了。我不想这样一辈子，我不想总梦见那些提心吊胆的事儿，我还想梦见我以前那些美丽的女朋友以及那些被梨花照过的时光，我提笔在笔记本的扉页上，郑重地写下了我的九字真言：“不着急，不害怕，不要脸。” 对时间的态度：不着急。有时候，关切是不问；有时候，不做比做什么都强； 对结果的态度：不害怕； 对他评的态度：不要脸。“是非审之于己，毁誉听之于人，得失安之于数”； 补充一点变成四点吧：不着急，不害怕，不要脸，不抱怨。 “这是最好的时代，这是最坏的时代；这是智慧的时代，这是愚蠢的时代；这是信仰的时期，这是怀疑的时期；这是光明的季节，这是黑暗的季节；这是希望之春，这是失望之冬；人们面前有着各样事物，人们面前一无所有；人们正在直登天堂，人们正在直下地狱。” 女生把自己整修得越来越像孪生姐妹，男生把自己禅修得越来越无聊。菜越来越没有菜味儿，肉越来越没有肉味儿，街上早就没有野花可以摘了，街上早就没有板砖可以拍了。 面对我们阻止不了的时代变化，多使用肉体，多去狂喜与伤心，多去创造，活出更多人样儿。 人类改变不了人性中的恶，创造完成后保护，保护不住后破坏，破坏后再创造，永陷轮回。 亲爱的，给我写首情诗好吗？越虐心越好。 降维攻击定义：你有道德我没道德，你死，我活；你我都是人你还要做人，我自降为禽兽，你死，我活。 贰 爱情如何对抗时间女人还是要自强不容易生病的身体够用的收入养心的爱好强大到浑蛋的小宇宙 男人同~ 再过一些年，或许宇宙这盆火也会最终熄灭，世界彻底安静下来，时间也瘫倒在空间里，仿佛一只死狗瘫倒在地板上。 爱情大概始于一些及其美妙的刹那。……。在刹那间，希望时间停滞，甚至无疾而终，在刹那间，就此死去。……。幸或者不幸的是，人想死的时候很难死掉，梦幻泡影、闪电烟花之后，生活继续。爱情如何对抗那些璀璨一刹那之外的漫长时间？ 你看他起高楼，你看他楼塌了。起高楼时，这个男的不一定能守得住底线；楼塌时，这个男的不一定能跑得了。 自己穿暖，才是真暖；自己真暖，才有资格相互温暖。 梦里三月桃花，二人一马。 身体极累的时候，心极伤的时候，身外有酒，白、黄、红，心里有姑娘，小鸟、小兽、小妖。白、黄、红流进身体，小鸟、小兽、小妖踏着云彩从心里溜达出来。身体更累，心更伤。风住了，风又起了。沿着伤口，就着酒，往下，再往下，潜水一样，掘井一样，运气好的时候，会看到世界里从来没有的景象。 男性在修炼成功之前(绝大多数在死前都没成功)，似乎总是有种不知进退而成为二逼的风险，过分执着到死拧，过分淡定到麻木，过分较真儿到迂腐，过分邋遢到鼻毛过唇。 只有克服了对于牛逼的过分追求，才能真正避免成为一个傻逼，特别是，随着年纪的增长，避免成为一个老傻逼。 叁 想起一生中后悔的事儿只花时间给三类人：好看的人，好玩的人，又好看又好玩的人。 同样吃一串葡萄，有人先从最好的一颗吃起，好处时每次都吃到可得的最好的一颗；有人先从最差的一颗吃起，好处是每次都能吃到比之前更好的一颗。 前半生认识的朋友来看我，是因为想看我而来看我，而不是因为我在某大机构任职或者刚得了一个世界第一、宇宙无敌的文艺大奖。 一个人在二十岁之前呆过十年的地方，就是他真正的故乡。之后无论他活多久，去过多少地方，故乡都在骨头和血液里，挥之不去。 其实，人一起生活过一段时间，就没了生死的界限，除非彼此的爱意已经被彻底忘记。 我回到您面前，您总会给我一杯热茶，然后也不说话，手指一下，茶在那儿。您走了之后我才明白，一杯热茶之前，要有被子、茶、热水，要问很久、很多次：我儿子什么时候回来啊？ 人皆草木不用成材。 万事都如甘蔗，哪有两头甜？ 因为手写有人味儿。……。手写信，给心里真正放不下的人，贴张邮票，去邮局寄了。 连续七天，口袋里，书包里，我天天带着这只鸟(玉)，手没事儿的时候就摸着它，睡觉的时候也攥着。……。到了第八个晚上，一模那只鸟不见了。我的酒一下醒了，我把行李拆了，没有；我把全身衣服拆了，没有；我把房间拆了，没有；我沿着进房间的路，原路返回到下出租车的那块砖，没有。……。我度过了一个非常清醒、哲学而又精疲力竭的夜晚，和初恋分手的第一晚也比这一晚好过很多。……。我醒来的时候，觉得比睡着之前还要累。我洗把脸，阳光从窗帘缝隙间洒下来，那只玉鸟就安静地待在酒店书桌地一个角落，栖息在酒店的便笺上 —— 应该是我脱裤子之前无意识地把它放到了最安全的地方。……。如果我把那只玉鸟抓过来摔碎，我就成佛了。实际发生的是，在一刹那，我找了根结实的绳儿，穿过玉鸟翅膀上面古老的打眼儿，把玉鸟牢牢地栓在我裤子的皮带扣上。 人生喜悦，失而复得； 太过珍惜，却弄丢了，是人性的矛盾； 愿意被找到的东西，一直在那；不愿意被找到的东西，丢了就丢了把； 为外物而悲喜，这是人性的桎梏。 肆 天用云作字在此刻，天用云作字。在未来某处，在未来某刻，天也用我作字，用我的手蘸着墨作字。 你耐心再看看，再看看，再看看。 如果你有一个期望，长年挥之不去，而且需要别人来满足，这个期望就是妄念。 自责是负能量最大的一种情绪。 任性是被低估的美德。 “事情过去好久了，话也没啥可说的了，但是有时想起你，还是真他妈的难过啊。” 饮酒到微醺，脸红脖子粗，脚下多了一截弹簧，整个人一蹦一跳的，似乎手不抓牢栏杆，身体就随着灵魂飞离地面。 饮酒的一个好处是，用肉的迷失换取灵的觉悟。放不下的、看不开的，几口下肚，眼清目明，仿佛都与自己无关了，不自觉地，脸上堆满了笑。 看看就得了，不要临。字写得漂亮的人太多了，万一你写得漂亮了，再写丑就太难了，你就不是你了，老天给你手上的那一丁丁点独特的东西就没了。 佛界易入，魔界难入。佛界和魔界都入入，人更知道什么是佛、什么是魔，人更容易平衡一点儿，在世上能走的更远点儿。]]></content>
      <categories>
        <category>Reading</category>
      </categories>
      <tags>
        <tag>冯唐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[REPRODUCTION]A Recipe for Training Neural Networks]]></title>
    <url>%2F2019%2F04%2F29%2FREPRODUCTION-A-Recipe-for-Training-Neural-Networks%2F</url>
    <content type="text"><![CDATA[转载自Andrej Karpathy blog The recipeIn light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration. 1. Become one with the dataThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels? data contained duplicate examples corrupted images &amp; labels data imbalances and biases local features v.s. global context quantity &amp; form of variation preprocess out some variation spatial position v.s. average pool detail v.s. downsample labels are noisy? In addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to look at your network (mis)predictions and understand where they might be coming from. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off. Once you get a qualitative sense it is also a good idea to write some simple code to search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis. The outliers especially almost always uncover some bugs in data quality or preprocessing. 2. Set up the end-to-end training/evaluation skeleton + get dumb baselinesNow that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We’ll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way. Tips &amp; tricks for this stage: fix random seed. Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome. This removes a factor of variation and will help keep you sane. simplify. Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage. Data augmentation is a regularization strategy that we may incorporate later, but for now it is just another opportunity to introduce some dumb bug. add significant digits to your eval. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane. verify loss @ init. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure -log(1/n_classes) on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc. init well. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias. human baseline. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth. input-indepent baseline. Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all? overfit one batch. Overfit a single batch of only a few examples (e.g. as little as two). To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). I also like to visualize in the same plot both the label and the prediction and ensure that they end up aligning perfectly once we reach the minimum loss. If they do not, there is a bug somewhere and we cannot continue to the next stage. verify decreasing training loss. At this stage you will hopefully be underfitting on your dataset because you’re working with a toy model. Try to increase its capacity just a bit. Did your training loss go down as it should? visualize just before the net. The unambiguously correct place to visualize your data is immediately before your y_hat = model(x) (or sess.run in tf). That is - you want to visualize exactly what goes into your network, decoding that raw tensor of data and labels into visualizations. This is the only “source of truth”. I can’t count the number of times this has saved me and revealed problems in data preprocessing and augmentation. visualize prediction dynamics. I like to visualize model predictions on a fixed test batch during the course of training. The “dynamics” of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network “struggle” to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter. use backprop to chart dependencies. Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I’ve come across a few times is that people get this wrong (e.g. they use view instead of transpose/permute somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss for some example i to be 1.0, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the i-th example. More generally, gradients give you information about what depends on what in your network, which can be useful for debugging. generalize a special case. This is a bit more of a general coding tip but I’ve often seen people create bugs when they bite off more than they can chew, writing a relatively general functionality from scratch. I like to write a very specific function to what I’m doing right now, get that to work, and then generalize it later making sure that I get the same result. Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time. 固定随机种子，消除随机带来的误差 简单出发，先不使用数据集扩增 测试集不要画曲线，不然会疯的 评估起始损失值，-log(1/n_classes)，各类初始概率应大致相等；思路一致, $p\approx$ 初始化最后一层权重很重要；一般都是采用随机初始化方法？？ 评估人的准确性并与模型比较 设置一个独立于输入的baseline，观察网络是否提取了想要的特征 反复训练同一个批次的数据，使网络过拟合，查看损失最低能到多少；确定网络结构没有问题 试着增加模型capacity，观察训练集损失是否下降，以确定合适的网络容量；确定模型参数量 输入网络前，可视化数据，查看数据是否正确；确定输入数据没有问题 可视化一些相同数据的输出，观察输出波动；观察网络收敛情况 用反向传播debug网络；高级技能？技能点还不够 全循环慢慢改成矢量化代码；一些复杂的计算可参考 3. OverfitAt this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model. The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration. A few tips &amp; tricks for this stage: picking the model. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: Don’t be a hero. I’ve seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don’t be a hero and just copy paste a ResNet-50 for your first run. You’re allowed to do something more custom later and beat this. adam is safe. In the early stages of setting baselines I like to use Adam with a learning rate of 3e-4. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.) complexify only one at a time. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you’d expect. Don’t throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc. do not trust learning rate decay defaults. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you’re not training ImageNet then you almost certainly do not want this. If you’re not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end. 不要逞强，搭建各种奇奇怪怪的模型。先从相近任务效果良好的网络结构出发，慢慢改进再击败它； Adam对参数敏感性低，SGD往往效果更好； 慢慢提高输入数据的复杂性，如输入数据的特征数、图像尺寸； 根据自己的学习任务，调整学习率衰减参数； 4. RegularizeIdeally, we are now at a place where we have a large model that is fitting at least the training set. Now it is time to regularize it and gain some validation accuracy by giving up some of the training accuracy. Some tips &amp; tricks: get more data. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models. data augment. The next best thing to real data is half-fake data - try out more aggressive data augmentation. creative augmentation. If half-fake data doesn’t do it, fake data may also do something. People are finding creative ways of expanding datasets; For example, domain randomization, use of simulation, clever hybrids such as inserting (potentially simulated) data into scenes, or even GANs. pretrain. It rarely ever hurts to use a pretrained network if you can, even if you have enough data. stick with supervised learning. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio). smaller input dimensionality. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don’t matter much try to input a smaller image. smaller model size. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process. decrease the batch size. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale &amp; offset “wiggles” your batch around more. drop. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout does not seem to play nice with batch normalization. weight decay. Increase the weight decay penalty. early stopping. Stop training based on your measured validation loss to catch your model just as it’s about to overfit. try a larger model. I mention this last and only after early stopping but I’ve found a few times in the past that larger models will of course overfit much more eventually, but their “early stopped” performance can often be much better than that of smaller models. Finally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network’s first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems. 略微升高一点训练数据的损失，换取验证集损失的下降。 扩大数据集；拼的就是算力和数据量 数据集扩增；来了 使用一些生成的虚假数据； 预训练模型，即使有足够的数据量； 监督性学习比非监督好； 减少数据特征的冗余性，如删减特征、降低图像分辨率；数据冗余容易过拟合 减少模型参数容量；过大可能过拟合 增大批数据量；梯度下降方向更准确 Dropout；BatchNorm效果更好？ 权重衰减；正则惩罚 Early stopping；提前中断训练防止过拟合 尝试更大的模型，有时候更大容易过拟合，但是提前中断训练效果会更好； 第一层网络的权值是否有可解释性；WTF? 5. TuneYou should now be “in the loop” with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step: random over grid search. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is best to use random search instead. Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter a matters but changing b has no effect then you’d rather sample a more throughly than at a few fixed points multiple times. hyper-parameter optimization. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding. 随机搜索超参数，对重要参数调整更多； 招募一个实习生帮助自己调参hhhhhh； 6. Squeeze out the juiceOnce you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system: ensembles. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can’t afford the computation at test time look into distilling your ensemble into a network using dark knowledge. leave it training. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”). 集成方法； 坚信模型能收敛并能取得良好的效果，不要手贱中断他。 ConclusionOnce you make it here you’ll have all the ingredients for success: You have a deep understanding of the technology, the dataset and the problem, you’ve set up the entire training/evaluation infrastructure and achieved high confidence in its accuracy, and you’ve explored increasingly more complex models, gaining performance improvements in ways you’ve predicted each step of the way. You’re now ready to read a lot of papers, try a large number of experiments, and get your SOTA results. Good luck!]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在宇宙间不易被风吹散]]></title>
    <url>%2F2019%2F04%2F28%2F%E5%9C%A8%E5%AE%87%E5%AE%99%E9%97%B4%E4%B8%8D%E6%98%93%E8%A2%AB%E9%A3%8E%E5%90%B9%E6%95%A3%2F</url>
    <content type="text"><![CDATA[前言喜欢冯唐的文集，用他自己文章里写的句子描述，“毫不掩饰的小说”，露骨但是真实，文字如锦如绣，文字间有墨香、有美人、有Kindle、有古玉和瓷器。 你迷恋什么，什么就是你的障碍。有个笃定的核，在宇宙间不易被风吹散。 精选 人是需要有点精神的，有点通灵的精神，否则很容易出溜成行尸走肉，任由人性中暗黑的一面驱使自己禽兽一样的肉身，在世间做一些腐朽不堪的事情。 一杆进洞，四下无人，人生悲惨莫过于此。 一日茶，一夜酒，一部毫不掩饰的小说，一次没有目的的见面，一群不谈正经事的朋友，用美好的器物消磨必定留不住的时间。 做一个人性的矿工，挖一挖，再挖一挖，看看下面的下面还有什么。 我觉得眼睛看到的一切似乎想要告诉我世界是什么但是又不明说到底是什么。 在纸书里，在啤酒里，在阳光里，在暖气里，宅着，屌着，无所事事，随梦所之。 我的、我的、我的、我的，一瞬间的我执爆棚，真好。 宇宙间大多数现象超越人类的知识范围，不可解释的例子比比皆是，比如人骨骼为啥是206块骨头，比如我爱你你为什么不爱我。 人生苦短，不如不管，继续任性。 时间在床边和鬓边一路小跑，有些事物在不知不觉中浅吟低唱，明生暗长。 人又不是黄金，怎么能让所有人都喜欢？任何事做到顶尖，都是政治，都会被人妒忌；即使是黄金，也会被某些人说成是臭狗屎。 既然死了的人都没睡醒过，活着时候睡觉就是很吃亏的一件事。 白白的，小小的，紧紧的，香香的，佛说第一次触摸最接近佛。——《初恋》 有时候，人会因为一两个微不足道的美好，安安渴望一个巨大的负面，比如因为像有机会用一下图案撩骚的Zippo打火机而渴望抽烟，比如因为一把好乳或者一头长发而舍不得一个三观凌乱的悍妇，比如因为一个火炉而期待北京一个漫长而寒冷的冬天。 脱离长期背在身上的人的羁绊，让身体里的禽兽和仙人在山林里和酒里渐渐增加比例，裸奔、裸泳，在池塘里带着猴子捞月亮，在山顶问神仙：人到底是个什么东西？ 想起后半生最不靠谱的事儿，结论是：最靠谱的还是买个酒庄。 天大理比不过“我喜欢”。 涉及终极的事儿，听天，听命，让自己和身体尽人力，其他不必去想，多想无益，徒增烦恼。 全球化了，各国的建筑师都到处串了，各种时装杂志都到处发行了，各地的楼宇和姑娘越来越像，像到面目模糊，天下一城。 最后一个能想到的原因，是随身佩带之后，无时无刻不提醒自己一些必须珍惜的事物和必须坚守的品质。君子无辜，玉不去身，时刻提醒自己，不要吃喝嫖赌抽、坑蒙拐骗偷。 科技的快速进步让很多人变得过时，也让很多器物变得多余。 我会老到有一天，不需要手表告诉我，时间是如何自己消失，也不需要靠名牌手表告诉周围人类我的品味、格调、富裕程度和牛逼等级。我会根据四季里的光线的变化，大致推断现在是几点了，根据肠胃的叫声决定是否该去街口的小馆儿了。 男人要有些士的精神，有所不为，有所必为，活着不是唯一的追求和最终的底线。 女人一头长发，骑匹大马，很迷人，非常迷人，而且，她是来救你的，就无比迷人。无论她要带你去哪，你都不要拒绝，先上马，然后闭嘴，什么都不要问。 买件立领风衣，浓个眉大个眼，一直走，不要往两边看，还能再混几十年。 家庭太复杂，涉及太多硬件和软件、生理和心理、现在和未来，一篇文章不容易讲透。 上天下地，背山面海，每天看看不一样的云，想想昨晚的梦，和自己聊一会天，日子容易丰盛起来。 朋友们就散住在附近几个街区，不用提前约，菜香升起时，几个电话就能聚起几个人，酒量不同，酒品相近，术业不同，三观接近。菜一般，就多喝点酒；酒不好，就再多喝点，很快就能高兴起来。 一生中，除了做自己喜欢的事儿，剩下最重要的就是和相看两不厌的人待在一起。 不和这个世界争，也不和别人争，更不要和自己争。争的结果可能是一时牛逼，也可能是心脑血管意外，后者造成的持续影响大很多。 尽管没去过南极，但是也见过了风雨，俗事已经懒得分析，不如一起一边慢跑，一边咒骂彼此生活中奇葩一样摇曳的傻逼。 世界这么多凶狠，他人心里那么多地狱，内心没有一点混蛋，如何走的下去？ 其实我们跟鱼、植物甚至草履虫有很多相近的地方，人或如草木，人可以甚至应该偶尔禽兽。 如果没有真的存在，所谓的善只能是伪善，所谓的美也只能是妄美。 因为人是要死的，所以要常常叨念冯唐说的九字箴言：不着急，不害怕，不要脸。 钱超过一定数目就不是用来个人消费的了，个人能温饱就好。多处的个人欲望需要靠修行来消灭，而不能靠多花钱来满足。 有帽子是一种相，没帽子也是一种相。内心不必太执着于无帽子的相，也不必太执着于有帽子的相。有帽子，无帽子，都需要亲尝，皆为玩耍。 既然戴帽子是相，投射到不同人的心识里就是不同的相，何必强求赞美？何必强调一致？何必消除噪音？ “宇戴王冠，必承其重。不要低头，王冠会掉。不要哭泣，有人会笑。”这个态度也太励志、太谋权，放松，戴戴耍耍，不留神，王冠掉了，掉就掉了，掉了就索性长发飘飘。 能做到实事求是地自恋其实是自信和自尊。任何领域做到最好之后，人只能相信自己的判断，只能自恋。……。从这个意义上讲，自恋不应该是被诟病的对象，不能实事求是的傻逼才应该是被诟病的对象。……。实事求是地修炼，实事求是地恋他和自恋，让别人闹心去吧。 矮子更爱居高临下，傻子更容易认为自己充满道理。 非让矮子明白自己是矮子，非让傻子明白自己是傻子，也是很耗神费时的事儿。对付世间闹心的事儿，只需要搞清楚两件事，一件是“关我屁事”，另一件是“关你屁事”。 小孩在天地间疯跑，不知道名利为何物，学习基本常识，食蔬饮水，应付无聊的课程，傻愣愣地杀无聊的时间，骂所有看不上的人“傻逼”。本身近佛，不需要佛。 有多少似乎过不起的事儿过不去一年？有多少看上去的大事最后真是大事？名片上印不下的名头，敌不过左图且书、右琴与壶，抵不过不得不褪去时一颗好心脏、一个好女生。 这样简单下去，再简单下去，脑子没弯儿了，手脚有劲儿了，山顶慢慢低于脚面了，拉萨就在眼前了。你我竟然像山、云、湖水和星空一样，一直在老去，一直在变化，一直没问题。再简单下去，在这样下去，你我都是佛了。 我常年劳碌，尽管热爱妇女，但没时间，无法让任何妇女满意。情伤之后，“得不到”，“留不住”，“无可奈何，奈何奈何”，唯一疗伤的方式就是拿伤口当笔头，写几行诗，血干了，诗出了，心里放下了。 如果去一座荒岛，没电，没电视，没电脑，一片蛮荒。我想了想，如果只能带一个活物。我就带一个和我能聊很多天的女人；如果只能带一本书，我就带一本《唐诗三百首》。]]></content>
      <categories>
        <category>Reading</category>
      </categories>
      <tags>
        <tag>冯唐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LDA]]></title>
    <url>%2F2019%2F04%2F22%2FLDA%2F</url>
    <content type="text"><![CDATA[前言在PCA中，介绍了无监督降维方法，主成分分析(Principal Components Analysis)。从投影后数据方差最大的角度出发，选取主轴。下面介绍一种有监督的降维方法，线性判别分析(Linear Discriminant Analysis)。 原理多分类设有$n$维样本集$D={x^{(1)}, …, x^{(i)}, …, x^{(m)}}$，所属类别数目为$C$，现求其第一投影轴$u_1$，即 \tilde{x}^{(i)}_1 = u_1^Tx^{(i)}现希望投影后，类内距离越小越好，类间距离越大越好，定义衡量指标 类内离差阵 S_W = \sum_{j=1}^C \frac{m_j}{m} \left[ \frac{1}{m_j} \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \right] 即 S_W = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \tag{1} 类间离差阵 S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T \tag{2} 则投影到第一主轴$u_1$后数据的类内离差阵和类间离差阵为 \tilde{S_W} = \sum_{j=1}^C \frac{m_j}{m} \left[ \frac{1}{m_j} \sum_{i=1}^{m_j} (\tilde{x}^{(i)}_1 - \tilde{\mu}^{(j)}_1) (\tilde{x}^{(i)}_1 - \tilde{\mu}^{(j)}_1)^T \right] \tilde{S_B} = \sum_{j=1}^C \frac{m_j}{m} (\tilde{\mu}^{(j)}_1 - \tilde{\mu}_1) (\tilde{\mu}^{(j)}_1 - \tilde{\mu}_1)^T 此时$\tilde{S_W}, \tilde{S_B}$均为标量。 其中 \tilde{x}^{(i)}_1 = u_1^T x^{(i)} \tilde{\mu}^{(j)}_1 = u_1^T \mu^{(j)} \tilde{\mu}_1 = u_1^T \mu带入后得到 \tilde{S_W} = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (u_1^T x^{(i)} - u_1^T \mu^{(j)}) (u_1^T x^{(i)} - u_1^T \mu^{(j)})^T = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} u_1^T(x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T u_1 = u_1^T S_W u_1同理 \tilde{S_B} = u_1^T S_B u_1定义优化目标 J = \min \left\{ \frac{\tilde{S_W}}{\tilde{S_B}} \right\} = \min \left\{ \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \right\} \tag{3}取 L(u_1) = \frac{u_1^T S_W u_1}{u_1^T S_B u_1} \tag{4}则其极值为 \frac{∂L}{∂u_1} = \frac{2(u_1^T S_B u_1) S_W u_1 - 2(u_1^T S_W u_1) S_B u_1}{(u_1^T S_B u_1)^2} = 0得到 (u_1^T S_B u_1) S_W u_1 = (u_1^T S_W u_1) S_B u_1令$\lambda_1 = \frac{u_1^T S_B u_1}{u_1^T S_W u_1}$，有 S_B u_1 = \lambda_1 S_W u_1 \tag{5}当$m$较大时，$S_W$一般非奇异，故 S_W^{-1} S_B u_1 = \lambda_1 u_1 \tag{*1}即${\lambda_1, u_1}$为矩阵$S_W^{-1} S_B$的特征对。 二分类特别地，对于二分类问题，有 S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T = \frac{m_1}{m} (\mu^{(1)} - \mu) (\mu^{(1)} - \mu)^T + \frac{m_2}{m} (\mu^{(2)} - \mu) (\mu^{(2)} - \mu)^T = \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - 2 \mu \left( \frac{m_1}{m} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)T} \right) + \mu \mu^T其中 \frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)} = \mu \tag{6}所以$(6)$代入$S_B$化简得 S_B = \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - \mu \mu^T = \frac{m_1}{m} \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \mu^{(2)} \mu^{(2)T} - (\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)}) (\frac{m_1}{m} \mu^{(1)} + \frac{m_2}{m} \mu^{(2)})^T = \frac{m_1}{m} \left(1-\frac{m_1}{m}\right) \mu^{(1)} \mu^{(1)T} + \frac{m_2}{m} \left(1-\frac{m_2}{m}\right) \mu^{(2)} \mu^{(2)T} - \frac{m_1}{m} \frac{m_2}{m} \mu^{(2)} \mu^{(1)T} - \frac{m_1}{m} \frac{m_2}{m} \mu^{(1)} \mu^{(2)T} = \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T上式代入$S_W^{-1} S_B u_1$，得 左边 = S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T u_1其中$\left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)^T u_1$为常数，记作$\alpha$，所以$\alpha S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) = \lambda_1 u_1$，也即 u_1 = \frac{\alpha}{\lambda_1} S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right)其中常数$\frac{\alpha}{\lambda_1}$不影响投影结果，如取$1$，则得到 u_1 = S_W^{-1} \left( \frac{m_1}{m} \mu^{(1)} - \frac{m_2}{m} \mu^{(2)} \right) \tag{*2}同理，可求得第二、三主成分轴 计算步骤给定数据集矩阵 X = \left[ \begin{matrix} ... \\ x^{(i)T} \\ ... \end{matrix} \right]其中$x^{(i)} = \left[ x^{(i)}_1, …, x^{(i)}_j, … x^{(i)}_n \right]^T$ 计算类内离差阵$S_W$与类间离差阵$S_B$； S_W = \frac{1}{m} \sum_{j=1}^C \sum_{i=1}^{m_j} (x^{(i)} - \mu^{(j)}) (x^{(i)} - \mu^{(j)})^T \tag{1} S_B = \sum_{j=1}^C \frac{m_j}{m} (\mu^{(j)} - \mu) (\mu^{(j)} - \mu)^T \tag{2} 计算矩阵$S_W^{-1}S_B$的特征对$(\lambda_i, u_i)$； S_W^{-1}S_B u_i = \lambda_i u_i \tag{*1} 将特征对按特征值降序排序，选取最大的特征值对应的特征向量作为投影主轴； 将数据投影到主轴上； 应用LDA可用于分类，以二分类为例，在求取主轴$u_1$后，将各类中心投影到主轴上，即 \begin{cases} \tilde{\mu}^{(1)}_1 = u_1^T \mu^{(1)} \\ \tilde{\mu}^{(2)}_1 = u_1^T \mu^{(2)} \end{cases}选取阈值，如 \tilde{x}_1 = \frac{\tilde{\mu}^{(1)}_1 + \tilde{\mu}^{(2)}_1}{2}则预测时，判决方程为 \begin{cases} u_1^T x^{(i)} < \tilde{x}_1 \Rightarrow x^{(i)} \in \omega_1 \\ u_1^T x^{(i)} > \tilde{x}_1 \Rightarrow x^{(i)} \in \omega_2 \end{cases}代码详情查看Github 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117def eig(A1, A2): """ Params: A1, A2: &#123;ndarray(n, n)&#125; Returns: eigval: &#123;ndarray(n)&#125; eigvec: &#123;ndarray(n, n)&#125; Notes: A1 \alpha = \lambda A2 \alpha """ s, u = np.linalg.eigh(A2 + np.diag(np.ones(A2.shape[0]))*1e-3) s_sqrt_inv = np.linalg.inv(np.diag(np.sqrt(s))) A = s_sqrt_inv.dot(u.T).dot(A1).dot(u).dot(s_sqrt_inv) eigval, P = np.linalg.eigh(A) eigvec = u.dot(s_sqrt_inv).dot(P) return eigval, eigvec class LDA(object): """ Attributes: n_components: &#123;int&#125; means_: &#123;ndarray(n_classes, n_features)&#125; components_: &#123;ndarray(n_components, n_features)&#125; """ def __init__(self, n_components): self.n_components = n_components self.means_ = None self.components_ = None def fit(self, X, y): """ train the model Params: X: &#123;ndarray(n_samples, n_features)&#125; y: &#123;ndarray(n_samples)&#125; """ labels = list(set(list(y))) n_classes = len(labels) n_samples, n_features = X.shape self.means_ = np.zeros((n_classes, n_features)) S_W = np.zeros(shape=(n_features, n_features)) S_B = np.zeros(shape=(n_features, n_features)) mean_ = np.mean(X, axis=0) for i_class in range(n_classes): X_ = X[y==labels[i_class]] means_ = np.mean(X_, axis=0) self.means_[i_class] = means_ X_ = X_ - means_ means_ = (means_ - mean_).reshape(1, -1) S_W += (X_.T).dot(X_) * (1 / n_samples) S_B += (means_.T).dot(means_) * (X_.shape[0] / n_samples) eigval, eigvec = eig(S_B, S_W) order = np.argsort(eigval)[::-1] eigval = eigval[order] eigvec = eigvec[:, order] self.components_ = eigvec[:, :self.n_components].T self.components_ /= np.linalg.norm(self.components_, axis=1).reshape(1, -1) def transform(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: X: &#123;ndarray(n_samples, n_components)&#125; """ X_ = X.dot(self.components_.T) return X_ def fit_transform(self, X, y): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: X: &#123;ndarray(n_samples, n_components)&#125; """ self.fit(X, y) X_ = self.transform(X) return X_ def transform_inv(self, X): """ Params: X: &#123;ndarray(n_samples, n_components)&#125; Returns: X: &#123;ndarray(n_samples, n_features)&#125; """ X_ = X.dot(self.components_) return X_ def predict(self, X): """ Params: X: &#123;ndarray(n_samples, n_features)&#125; Returns: y: &#123;ndarray(n_samples)&#125; """ n_samples, n_features = X.shape y = np.zeros(n_samples, dtype=np.int) X_ = self.transform(X) means_ = self.transform(self.means_) for i in range(n_samples): y[i] = np.argmin(np.linalg.norm(means_ - X_[i], axis=1)) return y 与PCA投影结果对比如下 PCA LDA 上图不明显，下图生成了两簇高斯分布的样本点，作出主轴显示，红色为LDA第一主轴方向，蓝色为PCA第一主轴方向 Reference 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[N-dim Array PCA]]></title>
    <url>%2F2019%2F04%2F18%2Fn-dim-Array-PCA%2F</url>
    <content type="text"><![CDATA[前言在PCA中介绍了1维数据的主成分分析，那么对于多维数据，如何进行处理呢？有一种做法是，将单份的样本数据展开为1维向量，再进行PCA，例如著名的Eigenface，如图所示 这有一个缺点是，忽略了多维数据的空间信息，且计算时，若展开后维度过大，协方差矩阵的求逆过程非常耗时间，以下介绍多维主成分分析。 原理对于单维$n$维度数据$X = \left[\begin{matrix} x^{(1)} \ x^{(2)} \ … \ x^{(N)} \end{matrix}\right]$，我们有 X' = XU其中，$x^{(i)}$为$n$维行向量，组成样本矩阵$X_{N \times n}$，$U_{n \times n_1}$为投影矩阵，$X’_{N \times n_1}$为降维后的样本矩阵。 对于多(n)维样本张量数据$X_{N \times n_{d_1} \times n_{d_2} \times … \times n_{d_n}}$，指定各维度降维顺序，在$d_i$维度上，我们将张量在该维度上展开为$1$维向量 X_{N_{d_i} \times n_{d_i}}其中 N_{d_i} = \prod_{j=0,j≠i}^n n_{d_j}然后在$d_i$维度上进行降维，即 X'_{N_{d_i} \times n'_{d_i}} = X_{N_{d_i} \times n_{d_i}} U^{d_i}其中$U^{d_i}$表示$d_i$维度的投影矩阵，其大小为$n_{d_i} \times n’_{d_i}$，然后，将该张量其余维度恢复，得到 X^{d_i}_{N \times n_{d_1} \times .. \times n'_{d_i} \times ... \times n_{d_n}}如此循环，在各维度进行降维，得到张量 X^{d_1, ..., d_n}_{N \times n'_{d_1} \times .. \times n'_{d_i} \times ... \times n'_{d_n}}注意，不同的降维顺序得到的参数会存在差异。 以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$，其降维过程表示如图 在$H$维度上 在$W$维度上 在$C$维度上 代码实现同样的，以3维张量$X(H, W, C)$为例，指定降维顺序为$(d_1, d_2, d_3)$ 在$H$维度上 将该张量转置，得到$X^{T_{d_1d_3}}(C, W, H)$ 将其展开为$X_{flatten}(N_H \times H)$，其中$N_H=C \times W$ 降维得到$X’_{flatten}(N_H \times H’)$ 重新将另外两维恢复，得到$X^{T_{d_1d_3}’}(C, W, H’)$ 将张量转置，得到$X^{d_1}(H’, W, C)$ 在$W$维度上 将$X^{d_1}(H’, W, C)$转置，得到$X^{T_{d_2d_3}}(H’, C, W)$ 将其展开为$X_{flatten}(N_W \times W)$，其中$N_W=H’ \times C$ 降维得到$X’_{flatten}(N_W \times W’)$ 重新将另外两维恢复，得到$X^{T_{d_2d_3}’}(H’, C, W’)$ 将张量转置，得到$X^{d_1d_2}(H’, W’, C)$ 在$C$维度上 将$X^{d_1d_2}(H’, W’, C)$展开为$X_{flatten}(N_C \times C)$，其中$N_C=H’ \times W’$ 降维得到$X’_{flatten}(N_C \times C’)$ 重新将另外两维恢复，得到$X^{d_1d_2d_3}(H’, W’, C’)$ 详细查看isLouisHsu/Basic-Machine-Learning-Algorithm/algorithm/tensor_pca.py- GitHub，其核心代码如下 12345678910111213141516171819202122232425262728def transform(self, X): """ Params: X: &#123;ndarray(n_samples, d0, d1, d2, ..., dn-1)&#125; n-dim array Returns: X: &#123;ndarray(n_samples, d0, d1, d2, ..., dn-1)&#125; n-dim array """ assert self.n_dims == len(X.shape) - 1, 'please check input dimension! ' idx = [i for i in range(len(X.shape))] # index of dimensions for i_dim in range(self.n_dims): ## transpose tensor idx[-1], idx[i_dim + 1] = idx[i_dim + 1], idx[-1] X = X.transpose(idx) shape = list(X.shape) # 1-dim pca X = X.reshape((-1, shape[-1])) X = self.decomposers[i_dim].transform(X) ## transpose tensor X = X.reshape(shape[:-1]+[X.shape[-1]]) X = X.transpose(idx) idx[-1], idx[i_dim + 1] = idx[i_dim + 1], idx[-1] return X 实验显示将2维数据降维，指定通道维度数目从$3$降至$1$，得到实验结果如下 原始数据 降维后数据 重建数据 Reference 特征脸 - 维基百科，自由的百科全书]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python字符串格式化]]></title>
    <url>%2F2019%2F02%2F19%2FPython%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言Python操作字符串行云流水，当然也支持格式化字符串。 通过格式符1print(&quot;我叫%s, 今年%d岁&quot; % (&#39;Louis Hsu&#39;, 18)) 或者使用字典进行值传递1print(&quot;我叫%(name), 今年%(age)岁&quot; % &#123;&#39;name&#39;: &#39;Louis Hsu&#39;, &#39;age&#39;: 18&#125;) typecode| 格式符 | 含义 || ——— | ———- || %s | 字符串 (采用str()的显示) || %r | 字符串 (采用repr()的显示) || %c | 单个字符 || %b | 二进制整数 || %d | 十进制整数 || %i | 十进制整数 || %o | 八进制整数 || %x | 十六进制整数 || %e | 指数 (基底写为e) || %E | 指数 (基底写为E) || %f | 浮点数 || %F | 浮点数，与上相同 || %g | 指数(e)或浮点数 (根据显示长度) || %G | 指数(E)或浮点数 (根据显示长度) || %% | 字符”%” | 高阶12345% [flags] [width].[precision] typecode- flags: &#39;+&#39;(右对齐), &#39;-&#39;(左对齐), &#39; &#39;(左侧填充一个空格，与负数对齐), &#39;0&#39;(用0填充)- width: 显示宽度- precision: 小数精度位数，可使用&#39;*&#39;进行动态代入- typecode: 格式符 例如12345678910print(&#39;pi is %+2.2f&#39; % (3.1415926)) -&gt; pi is +3.14print(&#39;pi is %-2.2f&#39; % (3.1415926)) -&gt; pi is 3.14print(&#39;pi is % 2.2f&#39; % (3.1415926)) -&gt; pi is 3.14print(&#39;pi is %02.2f&#39; % (3.1415926)) -&gt; pi is 3.14# 同print(&#39;pi is %+*.*f&#39; % (2, 2, 3.1415926))print(&#39;pi is %-*.*f&#39; % (2, 2, 3.1415926))print(&#39;pi is % *.*f&#39; % (2, 2, 3.1415926))print(&#39;pi is %0*.*f&#39; % (2, 2, 3.1415926)) 通过format位置传递 使用位置参数 123456789&gt;&gt;&gt; li &#x3D; [&#39;hoho&#39;,18]&gt;&gt;&gt; &#39;my name is &#123;&#125; ,age &#123;&#125;&#39;.format(&#39;hoho&#39;,18)&#39;my name is hoho ,age 18&#39;&gt;&gt;&gt; &#39;my name is &#123;1&#125; ,age &#123;0&#125;&#39;.format(10,&#39;hoho&#39;)&#39;my name is hoho ,age 10&#39;&gt;&gt;&gt; &#39;my name is &#123;1&#125; ,age &#123;0&#125; &#123;1&#125;&#39;.format(10,&#39;hoho&#39;)&#39;my name is hoho ,age 10 hoho&#39;&gt;&gt;&gt; &#39;my name is &#123;&#125; ,age &#123;&#125;&#39;.format(*li)&#39;my name is hoho ,age 18&#39; 使用关键字参数 12345&gt;&gt;&gt; hash &#x3D; &#123;&#39;name&#39;:&#39;hoho&#39;,&#39;age&#39;:18&#125;&gt;&gt;&gt; &#39;my name is &#123;name&#125;,age is &#123;age&#125;&#39;.format(name&#x3D;&#39;hoho&#39;,age&#x3D;19)&#39;my name is hoho,age is 19&#39;&gt;&gt;&gt; &#39;my name is &#123;name&#125;,age is &#123;age&#125;&#39;.format(**hash)&#39;my name is hoho,age is 18&#39; 格式限定基本格式如下12345&#123;[:pad][align][sign][typecode]&#125;- :pad : 填充字符，空白位用该字符填充- align: &#39;^&#39;, &#39;&lt;&#39;, &#39;&gt;&#39; 分别表示 &#39;居中&#39;, &#39;左对齐&#39;, &#39;右对齐&#39;(默认)，后面加宽度- sign : &#39;+&#39;, &#39;-&#39; , &#39; &#39; 分别表示 &#39;正&#39;, &#39;负&#39;, &#39;正数前加空格&#39;- typecode: &#39;b&#39;, &#39;d&#39;, &#39;o&#39;, &#39;x&#39;, &#39;f&#39;, &#39;,&#39;, &#39;%&#39;, &#39;e&#39; 分别表示 &#39;二进制&#39;, &#39;十进制&#39;, &#39;八进制&#39;, &#39;十六进制&#39;, &#39;浮点数&#39;, &#39;逗号分隔&#39;, &#39;百分比格式&#39;, &#39;指数记法&#39; Reference python之字符串格式化(format) - benric - 博客园 https://www.cnblogs.com/benric/p/4965224.htmlPython format 格式化函数 | 菜鸟教程 http://www.runoob.com/python/att-string-format.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python命令行参数解析]]></title>
    <url>%2F2019%2F02%2F18%2FPython%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言C/C++的参数传递我们知道C/C++主函数形式如下12345int main(int argc,char * argv[],char * envp[])&#123; &#x2F;&#x2F; do something return 0;&#125;其中各参数含义如下 argc：argument count，表示参数数量 argv：argument value，表示参数值 最后一个元素存放了一个NULL的指针 envp：系统环境变量 最后一个元素存放了一个NULL的指针 例如12345678910111213141516171819#include&lt;stdio.h&gt;int main(int argc,char * argv[],char * envp[])&#123; printf(&quot;argc is %d \n&quot;, argc); int i; for (i&#x3D;0; i&lt;argc; i++) &#123; printf(&quot;arcv[%d] is %s\n&quot;, i, argv[i]); &#125; for (i&#x3D;0; envp[i]!&#x3D;NULL; i++) &#123; printf(&quot;envp[%d] is %s\n&quot;, i, envp[i]); &#125; return 0;&#125; 测试平台为Windows10，执行编译和运行操作，结果如下:12345678910111213&gt; gcc main.c -o main.exe&gt; .&#x2F;main.exe param1 param2 param3 param4argc is 5arcv[0] is C:\OneDrive\▒ĵ▒\Louis&#39; Blog\source\_drafts\Python▒▒▒▒▒в▒▒▒▒▒▒▒\test.exearcv[1] is param1arcv[2] is param2arcv[3] is param3arcv[4] is param4envp[0] is ACLOCAL_PATH&#x3D;C:\MyApplications\Git\mingw64\share\aclocal;C:\MyApplica tions\Git\usr\share\aclocalenvp[1] is ALLUSERSPROFILE&#x3D;C:\ProgramData...(省略)envp[71] is WINDIR&#x3D;C:\WINDOWSenvp[72] is _&#x3D;.&#x2F;main.exe Python的参数传递可以使用sys模块得到命令行参数，主函数文件main.py如下1234import sysif __name__ &#x3D;&#x3D; &#39;__main__&#39;: print(sys.argv) 执行12&gt; python main.py param1 param2 pram3[&#39;main.py&#39;, &#39;param1&#39;, &#39;param2&#39;, &#39;pram3&#39;] getopt1234567891011121314151617181920212223242526272829303132333435363738394041import sysimport getoptimport argparseif __name__ &#x3D;&#x3D; &#39;__main__&#39;: argv &#x3D; sys.argv if len(argv) &#x3D;&#x3D; 1: print( &quot;&quot;&quot; Usage: python main.py [option] -h or --help: 显示帮助信息 -v or --version: 显示版本 -i or --input: 指定输入文件路径 -o or --output: 指定输出文件路径 &quot;&quot;&quot; ) try: opts, args &#x3D; getopt.getopt(args&#x3D;argv[1:], shortopts&#x3D;&#39;hvi:o:&#39;, longopts&#x3D;[&#39;help&#39;, &#39;version&#39;, &#39;input&#x3D;&#39;, &#39;output&#x3D;&#39;] ) except getopt.GetoptError: print(&quot;argv error,please input&quot;) sys.exit(1) for cmd, arg in opts: if cmd in [&#39;-h&#39;, &#39;--help&#39;]: print(&quot;help info&quot;) sys.exit(0) elif cmd in [&#39;-v&#39;, &#39;--version&#39;]: print(&quot;main 1.0&quot;) sys.exit(0) if cmd in [&#39;-i&#39;, &#39;--input&#39;]: input &#x3D; arg if cmd in [&#39;-o&#39;, &#39;--output&#39;]: output &#x3D; arg 说明 args=sys.argv[1:] 传入的参数，除去sys.argv[0]，即主函数文件路径 shortopts=&#39;hvi:o:&#39; 字符串，支持形如-h的选项 若无需指定参数，形如c； 若必须指定参数，则需为c:； longopts=[&#39;help&#39;, &#39;version&#39;, &#39;input=&#39;, &#39;output=&#39;] 字符串列表，可选参数，是否支持形如--help的选项 若无需指定参数，形如cmd； 若必须指定参数，则需为cmd=； 执行12345678910111213141516171819&gt; python main.py Usage: python main.py [option] -h or --help: 显示帮助信息 -v or --version: 显示版本 -i or --input: 指定输入文件路径 -o or --output: 指定输出文件路径&gt; python main.py -hhelp info&gt; python main.py -vmain 1.0&gt; python main.py -iargv error,please input&gt; python main.py -i a.txt -o b.txt argsparse12345678910111213141516171819202122232425262728import argparsedef show_args(args): if args.opencv: print(&quot;opencv is used &quot;) else: print(&quot;opencv is not used &quot;) print(args.steps) print(args.file) print(args.data) if __name__ &#x3D;&#x3D; &#39;__main__&#39;: parser &#x3D; argparse.ArgumentParser(description&#x3D;&quot;learn to use &#96;argparse&#96;&quot;) # 标志位 parser.add_argument(&#39;--opencv&#39;, &#39;-cv&#39;, action&#x3D;&#39;store_true&#39;, help&#x3D;&#39;use opencv if set &#39;) # 必需参数 parser.add_argument(&#39;--steps&#39;, &#39;-s&#39;, required&#x3D;True, type&#x3D;int, help&#x3D;&#39;number of steps&#39;) # 默认参数 parser.add_argument(&#39;--file&#39;, &#39;-f&#39;, default&#x3D;&#39;a.txt&#39;) # 候选参数 parser.add_argument(&#39;--data&#39;, &#39;-d&#39;, choices&#x3D;[&#39;data1&#39;, &#39;data2&#39;]) args &#x3D; parser.parse_args() show_args(args) 说明 帮助信息 参数help，用于显示在-h帮助信息中 标志位参数 参数action=&#39;store_true&#39;，即保存该参数为True 必需参数 置位required，即运行该程序必须带上该参数，否则报错 默认参数 参数default填写默认参数 候选参数 参数choices填写候选参数列表 运行1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 显示帮助信息&gt; python main.py -husage: main.py [-h] [--opencv] --steps STEPS [--file FILE] [--data &#123;data1,data2&#125;]learn to use &#96;argparse&#96;optional arguments: -h, --help show this help message and exit --opencv, -cv use opencv if set --steps STEPS, -s STEPS number of steps --file FILE, -f FILE --data &#123;data1,data2&#125;, -d &#123;data1,data2&#125;# 测试必须参数&gt; python main.pyusage: main.py [-h] [--opencv] --steps STEPS [--file FILE] [--data &#123;data1,data2&#125;]main.py: error: the following arguments are required: --steps&#x2F;-s&gt; python main.py -s 100opencv is not used100a.txtNone# 测试标志位参数&gt; python main.py -s 100 -cvopencv is used100a.txtNone# 测试默认参数&gt; python main.py -s 100 -f b.txtopencv is not used100b.txtNone# 测试可选参数&gt; python main.py -s 100 -d data1opencv is not used100a.txtdata1&gt; python main.py -s 100 -d data0usage: main.py [-h] [--opencv] --steps STEPS [--file FILE] [--data &#123;data1,data2&#125;]main.py: error: argument --data&#x2F;-d: invalid choice: &#39;data0&#39; (choose from &#39;data1&#39;, &#39;data2&#39;) Reference Python命令行参数解析：getopt和argparse - 死胖子的博客 - CSDN博客 https://blog.csdn.net/lanzheng_1113/article/details/77574446Python模块之命令行参数解析 - 每天进步一点点！！！ - 博客园 https://www.cnblogs.com/madsnotes/articles/5687079.htmlPython解析命令行读取参数 — argparse模块 - Arkenstone - 博客园 https://www.cnblogs.com/arkenstone/p/6250782.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python生成词云图]]></title>
    <url>%2F2019%2F02%2F17%2FPython%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[前言一个没什么用的小技能 模块 wordcloud · PyPI https://pypi.org/project/wordcloud/ 安装该模块1&gt; pip install wordcloud 主要用到的为12345678wordcloud.WordCloud(font_path=None, width=400, height=200, margin=2, ranks_only=None, prefer_horizontal=.9, mask=None, scale=1, color_func=None, max_words=200, min_font_size=4, stopwords=None, random_state=None, background_color='black', max_font_size=None, font_step=1, mode="RGB", relative_scaling='auto', regexp=None, collocations=True, colormap=None, normalize_plurals=True, contour_width=0, contour_color='black', repeat=False) 使用例程123456789101112131415161718192021222324import osimport cv2import numpy as npfrom wordcloud import WordCloudfont = 'C:/Windows/Fonts/SIMYOU.TTF' # 幼圆string = 'LouisHsu 单键 小叔叔 想静静 95后 傲娇 skrrrrrrr 大猫座 佛了 要秃 嘤嘤嘤 真香'mask = cv2.imread('./mask.jpg', cv2.IMREAD_GRAYSCALE)thresh, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)wc = WordCloud( font_path=font, background_color='white', color_func=lambda *args, **kwargs: (0,0,0), mask=mask, max_words=500, min_font_size=4, max_font_size=None, contour_width=1, repeat=True # 允许词重复 )wc.generate_from_text(string)wc.to_file('./wc.jpg') #保存图片 输入原图为 生成图像 Reference python WordCloud 简单实例 - 博客 - CSDN博客 https://blog.csdn.net/cy776719526/article/details/80171790]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Makefile简单教程]]></title>
    <url>%2F2019%2F01%2F05%2Fmakefile%E7%AE%80%E5%8D%95%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言准备源文件新建目录demo/，其结构如下123456789demo├─bin # 二进制文件，即可执行文件├─include # 头文件&#96;.h&#96;│ hello.h│├─obj # 目标文件&#96;.o&#96;└─src # 源文件&#96;.c&#96; hello.c test.c 编辑hello.h12345678#ifndef __HELLO_H#define __HELLO_H#include &lt;stdio.h&gt;void __hello();#endif 编辑hello.c123456#include "hello.h"void __hello()&#123; printf("Hello world!\n");&#125; 编辑test.h1234567#include "hello.h"int main()&#123; __hello(); return 0;&#125; 命令行编译1234567$ lshello.c hello.h makefile README.md test.c$ gcc test.c hello.c -o test$ lshello.c hello.h makefile README.md test test.c$ ./testHello world! 但是这样编译会每次都重新编译整个工程，时间比较长，所以可以先生成.o文件，当test.c代码改动后，重新生成test.o即可，hello.o不用重新编译12345678910$ lshello.c hello.h makefile README.md test.c$ gcc test.c hello.c -c$ lshello.c hello.h hello.o makefile README.md test.c test.o$ gcc test.o hello.o -o test$ lshello.c hello.h hello.o makefile README.md test test.c test.o$ ./testHello world! Makefile格式12&lt;target&gt;: &lt;dependencies&gt; &lt;command&gt; # [TAB]&lt;command&gt; 例如12test: test.c gcc test.c -o test 编辑Makefile123456789101112131415161718192021# directories &amp; target nameDIR_INC = ./include DIR_SRC = ./srcDIR_OBJ = ./objDIR_BIN = ./binTARGET = test# compile macro CC = gccCFLAGS = -g -Wall -I$&#123;DIR_INC&#125;all: $&#123;TARGET&#125;$&#123;TARGET&#125;: $&#123;DIR_OBJ&#125;/hello.o $&#123;DIR_OBJ&#125;/test.o $(CC) $(CFLAGS) $(DIR_OBJ)/hello.o $(DIR_OBJ)/test.o -o $&#123;TARGET&#125;$&#123;DIR_OBJ&#125;/hello.o: $(DIR_SRC)/hello.c $(CC) $(CFLAGS) $(DIR_SRC)/hello.c -o $&#123;DIR_OBJ&#125;/hello.o$&#123;DIR_OBJ&#125;/test.o: $(DIR_SRC)/test.c $(CC) $(CFLAGS) $(DIR_SRC)/test.c -o $&#123;DIR_OBJ&#125;/test.o 或通用性格式12345678910111213141516171819202122232425262728# directories &amp; target nameDIR_INC = ./include DIR_SRC = ./srcDIR_OBJ = ./objDIR_BIN = ./binTARGET = test# compile macro CC = gccCFLAGS = -g -Wall -I$&#123;DIR_INC&#125; # `-g`表示调试选项，`-Wall`表示编译后显示所有警告# load source filesSRC = $(wildcard $&#123;DIR_SRC&#125;/*.c) # 匹配目录中所有的`.c`文件# build targetOBJ = $(patsubst %.c, $&#123;DIR_OBJ&#125;/%.o, $&#123;notdir $&#123;SRC&#125;&#125;) # 由`SRC`字符串内容，指定生成`.o`文件的名称与目录BIN = $&#123;DIR_BIN&#125;/$&#123;TARGET&#125; # 指定可执行文件名称与目录# build$&#123;BIN&#125;: $&#123;OBJ&#125; $(CC) $(OBJ) -o $@ # 即 `$ gcc ./obj/*.o -o ./bin/test`$&#123;DIR_OBJ&#125;/%.o: $&#123;DIR_SRC&#125;/%.c $(CC) $(CFLAGS) -c $&lt; -o $@ # 即 `$ gcc ./src/*.c -g -Wall -I./include -c ./obj/*.o`# clean.PHONY: clean # 伪目标clean: find $&#123;DIR_OBJ&#125; -name *.o -exec rm -rf &#123;&#125; \; 执行123456789101112131415$ makegcc -g -Wall -I./include -c src/hello.c -o obj/hello.ogcc -g -Wall -I./include -c src/test.c -o obj/test.ogcc ./obj/hello.o ./obj/test.o -o bin/test$ ls obj/hello.o test.o$ ls bin/test$ ./bin/test Hello world!$ make cleanfind ./obj -name *.o -exec rm -rf &#123;&#125; \;$ ls obj/$ ls bin/test 解释 符号 $@, $^, $&lt;，$? $@: 表示目标文件 $^: 表示所有的依赖文件 $&lt;: 表示第一个依赖文件 $?: 表示比目标还要新的依赖文件列表 wildcard，notdir，patsubst wildcard : 扩展通配符 SOURCES = $(wildcard *.c): 产生一个所有以 ’.c’ 结尾的文件的列表，然后存入变量 SOURCES 里。 notdir : 去除路径，可以在使用wildcard函数后，再配合使用notdir函数只得到文件名（不含路径）。 patsubst : 替换通配符，需要３个参数，第一个是个需要匹配的式样，第二个表示用什么来替换他，第三个是个需要被处理的由空格分隔的字列。 OBJS = $(patsubst %.c,%.o,$(SOURCES)) - 将处理所有在 SOURCES 字列中的字（一列文件名），如果他的 结尾是 `.c` ，就用 `.o` 把 `.c`取代 - 这里的 % 符号将匹配一个或多个字符，而他每次所匹配的字串叫做一个‘柄’(stem) - 在第二个参数里， %被解读成用第一参数所匹配的那个柄。 -I，-L，-l -I: 将指定目录作为第一个寻找头文件的目录 -L: 将指定目录作为第一个寻找库文件的目录 -l: 在库文件路径中寻找.so动态库文件（如果gcc编译选项中加入了-static表示寻找.a静态库文件） .PHONY后面的target表示的也是一个伪造的target, 而不是真实存在的文件target，注意Makefile的target默认是文件。 关于三个函数的使用123456789101112DIR_INC &#x3D; .&#x2F;includeDIR_SRC &#x3D; .&#x2F;srcDIR_OBJ &#x3D; .&#x2F;objSRC &#x3D; $(wildcard $&#123;DIR_SRC&#125;&#x2F;*.c) # 指定编译当前目录下所有&#96;.c&#96;文件，全路径&#96;.&#x2F;src&#x2F;*.c&#96;DIR &#x3D; $(notdir $&#123;SRC&#125;) # 去除路径名，只留下文件名&#96;*.c&#96;OBJ &#x3D; $(patsubst %.c, $&#123;DIR_OBJ&#125;&#x2F;%.o, $&#123;DIR&#125;) # 将&#96;DIR&#96;中匹配到的&#96;%.c&#96;，替换为&#96;$&#123;DIR_OBJ&#125;&#x2F;%.o&#96;ALL: @echo $(SRC) @echo $(DIR) @echo $(OBJ) 执行1234$ make./src/hello.c ./src/test.chello.c test.c./obj/hello.o ./obj/test.o 注：若./src目录下还有子目录./src/inc，则1SRC = $(wildcard $&#123;DIR_SRC&#125;/*.c) $(wildcard $&#123;DIR_SRC&#125;/inc/*.c) Reference Makefile 使用总结 - wang_yb - 博客园 https://www.cnblogs.com/wang_yb/p/3990952.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CMake编译库文件]]></title>
    <url>%2F2019%2F01%2F05%2FCmake%E7%BC%96%E8%AF%91%E5%BA%93%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言库文件即源代码的二进制文件，我们通常把一些公用函数制作成函数库，供其它程序使用。函数库分为静态库和动态库两种。静态库在程序编译时会被连接到目标代码中，程序运行时将不再需要该静态库；动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此在程序运行时还需要动态库存在。 编译以DarkNet为例，我们将其源代码编译成.a静态库文件。 下载源码 YOLO: Real-Time Object Detection https://pjreddie.com/darknet/yolo/ 1$ git clone https://github.com/pjreddie/darknet 整理文件 我们将include/与src/目录复制到新建文件夹darknet/。目录结构如下 123456789101112darknet├── include│ └── darknet.h└── src ├── activation_kernels.cu ├── activation_layer.c ├── activation_layer.h ├── ... ├── utils.c ├── utils.h ├── yolo_layer.c └── yolo_layer.h 在darknet/目录下编写CmakeLists.txt文件，内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445CMAKE_MINIMUM_REQUIRED(VERSION 2.8) # cmake需要的最小版本号PROJECT(darknet) # 项目名MESSAGE(STATUS "----------------------------------------------------------------------")MESSAGE(STATUS "project name: " $&#123;PROJECT_NAME&#125; # cmake默认参数)MESSAGE(STATUS "source directory: " $&#123;PROJECT_SOURCE_DIR&#125; # cmake默认参数)MESSAGE(STATUS "binary directory: " $&#123;PROJECT_BINARY_DIR&#125; # cmake默认参数)MESSAGE(STATUS "----------------------------------------------------------------------")# ----------------------------------------------------------------------------------INCLUDE_DIRECTORIES( # 头文件目录 $&#123;PROJECT_SOURCE_DIR&#125;/include $&#123;PROJECT_SOURCE_DIR&#125;/src) AUX_SOURCE_DIRECTORY( # 源文件 $&#123;PROJECT_SOURCE_DIR&#125;/src lib_srcfile) SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_BINARY_DIR&#125;/lib) # 设置保存`.a`的目录# ----------------------------------------------------------------------------------ADD_LIBRARY( # 生成库文件，可选择`.a`或`.so` $&#123;PROJECT_NAME&#125; STATIC # `.a` # SHARED # `.so` $&#123;lib_srcfile&#125;) # ----------------------------------------------------------------------------------SET_TARGET_PROPERTIES( $&#123;PROJECT_NAME&#125; PROPERTIES LINKER_LANGUAGE C) 执行命令 我们在darknet/目录打开终端 123456789101112131415161718192021$ mkdir build$ cd build/build$ cmake ..-- The C compiler identification is GNU 7.4.0-- The CXX compiler identification is GNU 7.4.0 ...-- Configuring done-- Generating done-- Build files have been written to: /home/louishsu/Work/Codes/MTCNN_Darknet/darknet/build$ makeScanning dependencies of target darknet[ 2%] Building C object CMakeFiles/darknet.dir/src/activation_layer.c.o ...[ 97%] Building C object CMakeFiles/darknet.dir/src/yolo_layer.c.o[100%] Linking C shared library lib/libdarknet.so[100%] Built target darknet 在darknet/build/lib目录下即可找到libdarknet.a库文件，build目录结构如下123build└── lib └── libdarknet.so (*) 调用库函数为测试该库函数是否编译成功，编写测试代码，新建目录/test/，其文件结构为12345678test├── include│ └── test.h├── src│ └── test.c├── build│ └── test└── CMakeLists.txt 头文件/include/test.h内容为123456#ifndef TEST_H#define TEST_H#include "darknet.h"#endif 源文件/src/test.c内容如下123456789#include "test.h"int main()&#123; printf("Hello! Darknet!\n"); matrix M = make_matrix(4, 5); printf("The size of matrix M is %ld bytes\n", sizeof(M)); return 0;&#125; 编译文件CMakeLists.txt内容如下1234567891011121314151617181920212223242526CMAKE_MINIMUM_REQUIRED(VERSION 2.8) # cmake需要的最小版本号PROJECT(Test) # 项目名ADD_DEFINITIONS(-DOPENCV=1)# ----------------------------------------------------------------------------------SET(DARKNET ../darknet)INCLUDE_DIRECTORIES( # 头文件目录 $&#123;DARKNET&#125;/include $&#123;DARKNET&#125;/src) LINK_DIRECTORIES( # 库文件目录 $&#123;DARKNET&#125;/build/lib) # ----------------------------------------------------------------------------------INCLUDE_DIRECTORIES(./include) # 当前项目头文件目录AUX_SOURCE_DIRECTORY(./src SRC_FILES) # 当前项目源文件目录# ----------------------------------------------------------------------------------ADD_EXECUTABLE($&#123;PROJECT_NAME&#125; $&#123;SRC_FILES&#125;) # 添加可执行文件TARGET_LINK_LIBRARIES( # 引用库 $&#123;PROJECT_NAME&#125; darknet # darknet m # 数学函数库) 执行指令12345678910111213141516$ mkdir build$ cd build$ cmake ..-- The C compiler identification is GNU 5.4.0-- The CXX compiler identification is GNU 5.4.0 ...-- Configuring done-- Generating done-- Build files have been written to: /home/louishsu/Work/Codes/makefile/test/build$ makeScanning dependencies of target Test[ 50%] Building C object CMakeFiles/Test.dir/src/test.c.o[100%] Linking C executable Test[100%] Built target Test 运行可执行文件123$ ./TestHello! Darknet!The size of matrix M is 16 bytes 查看结构体matrix定义1234typedef struct matrix&#123; int rows, cols; float **vals;&#125; matrix;int占32bit，float*占64bit，故1(32bit * 2 + 64bit) &#x2F; 8 &#x3D; 16byte运行成功。 Reference 静态库和动态库的优缺点 - 默默淡然 - 博客园 https://www.cnblogs.com/liangxiaofeng/p/3228145.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu编译安装Tensorflow]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Tensorflow%2F</url>
    <content type="text"><![CDATA[非常重要如果中途出现错误，xxxx文件找不到，不要怀疑！就是大天朝的网络问题！推荐科学上网！ 安装CUDA与CUDNN首先查看显卡是否支持CUDA加速，输入1$ nvidia-smi 在Ubuntu16.04 LTS下，推荐安装CUDA9.0和CUDNN 7。 CUDA CUDA Toolkit 9.0 Downloads | NVIDIA Developer https://developer.nvidia.com/cuda-90-download-archive 下载.run版本，安装方法如下 12$ sudo chmod +x cuda_9.0.176_384.81_linux.run $ sudo sh .&#x2F;cuda_9.0.176_384.81_linux.run 服务条款很长。。。。 CUDNN NVIDIA cuDNN | NVIDIA Developer https://developer.nvidia.com/cudnn 1234$ tar -xzvf cudnn-9.0-linux-x64-v7.4.1.5.tgz$ sudo cp cuda&#x2F;include&#x2F;cudnn.h &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include$ sudo cp cuda&#x2F;lib64&#x2F;libcudnn* &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64$ sudo chmod a+r &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;include&#x2F;cudnn.h &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64&#x2F;libcudnn* 安装后进行验证 1234$ cp -r &#x2F;usr&#x2F;src&#x2F;cudnn_samples_v7&#x2F; $HOME$ cd $HOME&#x2F;cudnn_samples_v7&#x2F;mnistCUDNN$ make clean &amp;&amp; make$ .&#x2F;mnistCUDNN 编译Tensorflow(CPU version)由于训练代码使用Python实现，故C++版本的Tensorflow不使用GPU，仅实现预测代码即可。 bazel Installing Bazel on Ubuntu - Bazel https://docs.bazel.build/versions/master/install-ubuntu.html一定要用源码安装！！！ download the Bazel binary installer named bazel-&lt;version&gt;-installer-linux-x86_64.sh from the Bazel releases page on GitHub. 123456$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python$ chmod +x bazel-&lt;version&gt;-installer-linux-x86_64.sh$ .&#x2F;bazel-&lt;version&gt;-installer-linux-x86_64.sh --user$ sudo nano ~&#x2F;.bashrc # export PATH&#x3D;&quot;$PATH:$HOME&#x2F;bin&quot;$ source ~&#x2F;.bashrc $ bazel version 编译CPU版本的CPU查看java版本1234$ java -versionopenjdk version &quot;1.8.0_191&quot;OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12)OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) 安装依赖软件包环境1234$ sudo apt install python3-dev$ pip3 install six$ pip3 install numpy$ pip3 instal wheel 下载Tensorflow源码1$ git clone https:&#x2F;&#x2F;github.com&#x2F;tensorflow&#x2F;tensorflow 编译与安装12$ cd tensorflow$ .&#x2F;configure 配置选项如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command "bazel shutdown".INFO: Invocation ID: ce26fc12-2926-4ca7-8775-febc553c8ab5You have bazel 0.20.0 installed.Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3Found possible Python library paths: /usr/local/lib/python3.5/dist-packages /usr/lib/python3/dist-packagesPlease input the desired Python library path to use. Default is [/usr/local/lib/python3.5/dist-packages]Do you wish to build TensorFlow with XLA JIT support? [Y/n]: nNo XLA JIT support will be enabled for TensorFlow.Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: nNo OpenCL SYCL support will be enabled for TensorFlow.Do you wish to build TensorFlow with ROCm support? [y/N]: nNo ROCm support will be enabled for TensorFlow.Do you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Do you wish to download a fresh release of clang? (Experimental) [y/N]: nClang will not be downloaded.Do you wish to build TensorFlow with MPI support? [y/N]: nNo MPI support will be enabled for TensorFlow.Please specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -march=native -Wno-sign-compare]: Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Preconfigured Bazel build configs. You can use any of the below by adding "--config=&lt;&gt;" to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=monolithic # Config for mostly static monolithic build. --config=gdr # Build with GDR support. --config=verbs # Build with libverbs support. --config=ngraph # Build with Intel nGraph support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.Preconfigured Bazel build configs to DISABLE default on features: --config=noaws # Disable AWS S3 filesystem support. --config=nogcp # Disable GCP support. --config=nohdfs # Disable HDFS support. --config=noignite # Disable Apacha Ignite support. --config=nokafka # Disable Apache Kafka support. --config=nonccl # Disable NVIDIA NCCL support.Configuration finished 使用bazel编译1$ bazel build --config&#x3D;opt &#x2F;&#x2F;tensorflow:libtensorflow_cc.so 出现错误 TF failing to build on Bazel CI · Issue #19464 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/19464Failure to build TF 1.12 from source - multiple definitions in grpc · Issue #23402 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/issues/23402#issuecomment-436932197Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583Explicitly import tools/bazel.rc by meteorcloudy · Pull Request #23583 · tensorflow/tensorflow https://github.com/tensorflow/tensorflow/pull/23583/commits/03e63a291bc95dacaa821585f39a360b43465cb5 解决方法 方法1 方法2 将tools/bazel.rc中内容粘到.tf_configure.bazelrc中，每次重新配置后需要重新粘贴一次。 源码安装protobuf3.6.0 https://github.com/protocolbuffers/protobuf 1234.&#x2F;autogen.sh.&#x2F;configuremakemake install 下载其他文件 12$ .&#x2F;tensorflow&#x2F;contrib&#x2F;makefile&#x2F;download_dependencies.shmkdir &#x2F;tmp&#x2F;eigen 值得注意，download_dependencies.sh中下载依赖包时，需要用到curl，但是默认方式安装 1$ sudo apt install curl 现在是2018/12/19/02:48，被这个问题折腾了3个小时。 时不支持`https`协议，故需要安装`OpenSSL`，并源码安装，详细资料见[curl提示不支持https协议解决方法 - 标配的小号 - 博客园](https://www.cnblogs.com/biaopei/p/8669810.html) - 执行`./autogen.sh`时，发生错误`autoreconf: not found`，则安装 12$ sudo apt install autoconf aotomake libtool$ sudo apt install libffi-dev 源码安装Eigen 12345cd tensorflow/contrib/makefile/Downloads/eigenmkdir buildcd buildcmakemake install 调用C++版本的Tensorflow创建文件目录如下1234|-- tf_test |-- build |-- main.cpp |-- CMakeLists.txt main.cpp文件内容如下1234567891011121314151617181920212223#include "tensorflow/cc/client/client_session.h"#include "tensorflow/cc/ops/standard_ops.h"#include "tensorflow/core/framework/tensor.h"int main() &#123; using namespace tensorflow; using namespace tensorflow::ops; Scope root = Scope::NewRootScope(); // Matrix A = [3 2; -1 0] auto A = Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125;&#125;); // Vector b = [3 5] auto b = Const(root, &#123; &#123;3.f, 5.f&#125;&#125;); // v = Ab^T auto v = MatMul(root.WithOpName("v"), A, b, MatMul::TransposeB(true)); std::vector&lt;Tensor&gt; outputs; ClientSession session(root); // Run and fetch v TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs)); // Expect outputs[0] == [19; -3] LOG(INFO) &lt;&lt; outputs[0].matrix&lt;float&gt;(); return 0;&#125; CMakeLists.txt内容如下12345678910111213141516171819202122232425262728293031cmake_minimum_required (VERSION 2.8.8)project (tf_example)set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -g -std&#x3D;c++11 -W&quot;)set(EIGEN_DIR &#x2F;usr&#x2F;local&#x2F;include&#x2F;eigen3)set(PROTOBUF_DIR &#x2F;usr&#x2F;local&#x2F;include&#x2F;google&#x2F;protobuf)set(TENSORFLOW_DIR &#x2F;home&#x2F;louishsu&#x2F;install&#x2F;tensorflow-1.12.0)include_directories( $&#123;EIGEN_DIR&#125; $&#123;PROTOBUF_DIR&#125; $&#123;TENSORFLOW_DIR&#125; $&#123;TENSORFLOW_DIR&#125;&#x2F;bazel-genfiles $&#123;TENSORFLOW_DIR&#125;&#x2F;tensorflow&#x2F;contrib&#x2F;makefile&#x2F;downloads&#x2F;absl)link_directories( &#x2F;usr&#x2F;local&#x2F;lib)add_executable( tf_test main.cpp)target_link_libraries( tf_test tensorflow_cc tensorflow_framework) 123$ mkdir build &amp;&amp; cd build$ cmake .. &amp;&amp; make$ .&#x2F;tf_test install tensorflow-gpu for python可使用pip指令安装，推荐下载安装包， tensorflow · PyPI https://pypi.org/project/tensorflow/ 12$ cd ~&#x2F;Downloads$ pip3 --default-timeout&#x3D;1000 install tensorflow_gpu-1.12.0-cp35-cp35m-manylinux1_x86_64.whl --user 安装后进行验证123456789101112131415161718192021222324252627$ python3Python 3.5.2 (default, Nov 12 2018, 13:43:14) [GCC 5.4.0 20160609] on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; sess = tf.Session()2018-12-12 11:58:17.817417: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA2018-12-12 11:58:17.953931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero2018-12-12 11:58:17.954686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: name: GeForce GT 730M major: 3 minor: 5 memoryClockRate(GHz): 0.758pciBusID: 0000:04:00.0totalMemory: 983.44MiB freeMemory: 177.19MiB2018-12-12 11:58:17.954728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 02018-12-12 11:58:18.276013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:2018-12-12 11:58:18.276057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 2018-12-12 11:58:18.276069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 2018-12-12 11:58:18.276223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory) -&gt; physical GPU (device: 0, name: GeForce GT 730M, pci bus id: 0000:04:00.0, compute capability: 3.5)&gt;&gt;&gt; a = tf.Variable([233])&gt;&gt;&gt; init = tf.initialize_all_variables()WARNING:tensorflow:From /home/louishsu/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.Instructions for updating:Use `tf.global_variables_initializer` instead.&gt;&gt;&gt; sess.run(init)&gt;&gt;&gt; sess.run(a)array([233], dtype=int32)&gt;&gt;&gt; sess.close() 注意，如果异常中断程序，显存不会被释放，需要自行kill1$ nvidia-smi 获得PID序号，使用指令结束进程1$ kill -9 pid Reference TensorFlow C++动态库编译 - 简书 https://www.jianshu.com/p/d46596558640Tensorflow C++ 从训练到部署(1)：环境搭建 | 技术刘 http://www.liuxiao.org/2018/08/ubuntu-tensorflow-c-%E4%BB%8E%E8%AE%AD%E7%BB%83%E5%88%B0%E9%A2%84%E6%B5%8B1%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu编译安装OpenCV]]></title>
    <url>%2F2019%2F01%2F04%2FUbuntu%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV%2F</url>
    <content type="text"><![CDATA[下载源码 OpenCV library https://opencv.org/ 编译安装依赖软件包12$ sudo apt install cmake$ sudo apt-get install build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev 编译12345$ unzip opencv-3.4.4.zip$ cd opencv-3.4.4$ mkdir build &amp;&amp; cd build$ cmake ..$ make -j4 安装123$ sudo make install$ sudo nano &#x2F;etc&#x2F;ld.so.conf.d&#x2F;opencv.conf # add &#96;&#x2F;usr&#x2F;local&#x2F;lib&#96;$ sudo ldconfig 验证OpenCV自带验证程序，在opencv-3.4.4/samples/cpp/example_cmake中可以找到 1234$ cd opencv-3.4.4&#x2F;samples&#x2F;cpp&#x2F;example_cmake$ cmake .$ make$ .&#x2F;opencv_example 如果没问题，可以看到你的大脸了~ Reference Ubuntu16.04安装openCV3.4.4 - 辣屁小心的学习笔记 - CSDN博客 https://blog.csdn.net/weixin_39992397/article/details/84345197]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python读写配置文件]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AF%BB%E5%86%99%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[在深度学习中，有许多运行参数需要指定，有几种方法可以解决 定义.py文件存储变量 定义命名元组collections.namedtuple() 创建.config，.ini等配置文件 Python 读取写入配置文件很方便，使用内置模块configparser即可 读出首先创建文件test.config或test.ini，写入如下内容123456789[db]db_port &#x3D; 3306db_user &#x3D; rootdb_host &#x3D; 127.0.0.1db_pass &#x3D; test[concurrent]processor &#x3D; 20thread &#x3D; 10 读取操作如下1234567891011121314151617181920212223242526272829&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; configfile &#x3D; &quot;.&#x2F;test.config&quot;&gt;&gt;&gt; inifile &#x3D; &quot;.&#x2F;test.ini&quot;&gt;&gt;&gt; &gt;&gt;&gt; cf &#x3D; configparser.ConfigParser()&gt;&gt;&gt; cf.read(configfile) # 读取文件内容&gt;&gt;&gt; &gt;&gt;&gt; sections &#x3D; cf.sections() # 所有的section，以列表的形式返回&gt;&gt;&gt; sections[&#39;db&#39;, &#39;concurrent&#39;]&gt;&gt;&gt; &gt;&gt;&gt; options &#x3D; cf.options(&#39;db&#39;) # 该section的所有option&gt;&gt;&gt; options[&#39;db_port&#39;, &#39;db_user&#39;, &#39;db_host&#39;, &#39;db_pass&#39;]&gt;&gt;&gt; &gt;&gt;&gt; items &#x3D; cf.items(&#39;db&#39;) # 该section的所有键值对&gt;&gt;&gt; items[(&#39;db_port&#39;, &#39;3306&#39;), (&#39;db_user&#39;, &#39;root&#39;), (&#39;db_host&#39;, &#39;127.0.0.1&#39;), (&#39;db_pass&#39;, &#39;test&#39;)]&gt;&gt;&gt; &gt;&gt;&gt; db_user &#x3D; cf.get(&#39;db&#39;, &#39;db_user&#39;) # section中option的值，返回为string类型&gt;&gt;&gt; db_user&#39;root&#39;&gt;&gt;&gt; &gt;&gt;&gt; db_port &#x3D; cf.getint(&#39;db&#39;, &#39;db_port&#39;) # 得到section中option的值，返回为int类型&gt;&gt;&gt; # 类似的还有getboolean()与getfloat()&gt;&gt;&gt; db_port3306 写入12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; import os&gt;&gt;&gt; import configparser&gt;&gt;&gt; &gt;&gt;&gt; cf &#x3D; configparser.ConfigParser()&gt;&gt;&gt; cf.add_section(&#39;test1&#39;) # 新增section&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) # 新增option：错误示范Traceback (most recent call last): File &quot;&lt;pyshell#7&gt;&quot;, line 1, in &lt;module&gt; cf.set(&quot;test&quot;, &quot;count&quot;, 1) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1192, in set self._validate_value_types(option&#x3D;option, value&#x3D;value) File &quot;C:\MyApplications\Python3\lib\configparser.py&quot;, line 1177, in _validate_value_types raise TypeError(&quot;option values must be strings&quot;)TypeError: option values must be strings&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test&quot;, &quot;count&quot;, &#39;1&#39;) # 新增option&gt;&gt;&gt; &gt;&gt;&gt; cf.set(&quot;test1&quot;, &quot;opt1&quot;, &#39;ok&#39;) # 新增option&gt;&gt;&gt; cf.remove_option(&quot;test1&quot;, &quot;opt1&quot;) # 删除optionTrue&gt;&gt;&gt; &gt;&gt;&gt; cf.add_section(&#39;test2&#39;) # 新增section&gt;&gt;&gt; cf.remove_section(&#39;test2&#39;) # 删除sectionTrue&gt;&gt;&gt; &gt;&gt;&gt; with open(&quot;.&#x2F;test_wr.config&quot;, &#39;w+&#39;) as f: cf.write(f) # 写入文件test_wr.config &gt;&gt;&gt; 现在目录已创建文件test_wr.config，打开可以看到12[test1]count &#x3D; 1]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python更新安装的包]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E6%9B%B4%E6%96%B0%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%2F</url>
    <content type="text"><![CDATA[pip不提供升级全部已安装模块的方法，以下指令可查看更新信息1$ pip list --outdate 得到输出信息如下123456789101112131415161718192021222324Package Version Latest Type----------------- --------- ---------- -----absl-py 0.3.0 0.6.1 sdistautopep8 1.3.5 1.4.2 sdistbleach 2.1.4 3.0.2 wheelcertifi 2018.8.24 2018.10.15 wheeldask 0.20.0 0.20.1 wheelgrpcio 1.14.1 1.16.0 wheelipykernel 5.0.0 5.1.0 wheelipython 7.0.1 7.1.1 wheeljedi 0.12.1 0.13.1 wheeljupyter-console 5.2.0 6.0.0 wheelMarkdown 2.6.11 3.0.1 wheelMarkupSafe 1.0 1.1.0 wheelmatplotlib 2.2.2 3.0.2 wheelmistune 0.8.3 0.8.4 wheelnumpy 1.14.5 1.15.4 wheelopencv-python 3.4.2.17 3.4.3.18 wheelPillow 5.2.0 5.3.0 wheelprometheus-client 0.3.1 0.4.2 sdistpyparsing 2.2.0 2.3.0 wheelpython-dateutil 2.7.3 2.7.5 wheelpytz 2018.5 2018.7 wheelurllib3 1.23 1.24.1 wheel 以下提供一键升级的方法，可能比较久hhhh123456from tqdm import tqdmfrom subprocess import callfrom pip._internal.utils.misc import get_installed_distributions for dist in tqdm(get_installed_distributions()): call("pip install --upgrade &#123;&#125;".format(dist.project_name), shell=True) 另外，从已有的安装列表，安装所需要的包，可使用以下指令 在已安装的机器中，生成列表 1pip freeze &gt; xxx.list 在未安装的机器中，使用列表安装 1pip install -r xxx.list]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python记录日志]]></title>
    <url>%2F2019%2F01%2F04%2FPython%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[前言日志可以用来记录应用程序的状态、错误和信息消息，也经常作为调试程序的工具。Python提供了一个标准的日志接口，就是logging模块。日志级别有DEBUG、INFO、WARNING、ERROR、CRITICAL五种。 logging — Logging facility for Python — Python 3.7.1 documentation 使用方法logger对象1234&gt;&gt;&gt; import logging&gt;&gt;&gt; logger = logging.getLogger(__name__)&gt;&gt;&gt; logger&lt;Logger __main__ (WARNING)&gt; 日志级别可输出五种不同的日志级别，分别为有DEBUG、INFO、WARNING、ERROR、CRITICAL12345678&gt;&gt;&gt; logger.debug('test log')&gt;&gt;&gt; logger.info('test log')&gt;&gt;&gt; logger.warning('test log')test log&gt;&gt;&gt; logger.error('test log')test log&gt;&gt;&gt; logger.critical('test log')test log 可以看到只有WARNING及以上级别日志被输出，这是由于默认的日志级别是WARNING ，所以低于此级别的日志不会记录。 基础配置1logging.basicConfig(**kwarg) **kwarg中部分参数如下 format 12345678910%(levelname)：日志级别的名字格式%(levelno)s：日志级别的数字表示%(name)s：日志名字%(funcName)s：函数名字%(asctime)：日志时间，可以使用datefmt去定义时间格式，如上图。%(pathname)：脚本的绝对路径%(filename)：脚本的名字%(module)：模块的名字%(thread)：thread id%(threadName)：线程的名字 datefmt 1&#39;%Y-%m-%d %H:%M:%S&#39; level 默认为ERROR 12345logging.DEBUGlogging.INFOlogging.WARNINGlogging.ERRORlogging.CRITICAL 例如12345678910111213141516&gt;&gt;&gt; # 未输出debug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug('test log')&gt;&gt;&gt; &gt;&gt;&gt; # 修改配置&gt;&gt;&gt; log_format = '%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'&gt;&gt;&gt; log_datefmt = '%Y-%m-%d %H:%M:%S'&gt;&gt;&gt; log_level = logging.DEBUG&gt;&gt;&gt; logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level)&gt;&gt;&gt; &gt;&gt;&gt; # 输出debug&gt;&gt;&gt; logger = logging.getLogger()&gt;&gt;&gt; logger.debug('test log')&lt;pyshell#8&gt; [2018-11-13 11:59:52] [DEBUG] test log 输出到日志文件保存代码为文件log_test.py1234567891011121314151617181920import logginglog_format = '%(filename)s [%(asctime)s] [%(levelname)s] %(message)s'log_datefmt = '%Y-%m-%d %H:%M:%S'log_level = logging.DEBUGlog_filename = './test.log'log_filemode = 'a' # 也可以为'w', 'w+'等logging.basicConfig(format=log_format, datefmt=log_datefmt, level=log_level, filename=log_filename, filemode=log_filemode)logger = logging.getLogger(__name__)logger.debug('test log')logger.info('test log')logger.warning('test log')logger.error('test log')logger.critical('test log') 运行完毕，打开log_test.log文件可以看到12345log_test.py [2018-11-13 12:11:04] [DEBUG] test loglog_test.py [2018-11-13 12:11:04] [INFO] test loglog_test.py [2018-11-13 12:11:04] [WARNING] test loglog_test.py [2018-11-13 12:11:04] [ERROR] test loglog_test.py [2018-11-13 12:11:04] [CRITICAL] test log]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github博客搭建]]></title>
    <url>%2F2019%2F01%2F04%2FGithub-Hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[前言那么问题来了，现有的博客还是现有的这篇文章呢？ 软件安装安装node.js, git, hexo 博客搭建初始化推荐使用git命令窗口，执行如下指令12345678910111213141516171819202122232425262728293031$ mkdir Blog$ cd Blog$ hexo initINFO Cloning hexo-starter to ~\Desktop\BlogCloning into 'C:\Users\LouisHsu\Desktop\Blog'...remote: Enumerating objects: 68, done.remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68Unpacking objects: 100% (68/68), done.Submodule 'themes/landscape' (https://github.com/hexojs/hexo-theme-landscape.git) registered for path 'themes/landscape'Cloning into 'C:/Users/LouisHsu/Desktop/Blog/themes/landscape'...remote: Enumerating objects: 1, done.remote: Counting objects: 100% (1/1), done.remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866Receiving objects: 100% (867/867), 2.55 MiB | 494.00 KiB/s, done.Resolving deltas: 100% (459/459), done.Submodule path 'themes/landscape': checked out '73a23c51f8487cfcd7c6deec96ccc7543960d350'Install dependenciesnpm WARN deprecated titlecase@1.1.2: no longer maintainednpm WARN deprecated postinstall-build@5.0.3: postinstall-build's behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.&gt; nunjucks@3.1.6 postinstall C:\Users\LouisHsu\Desktop\Blog\node_modules\nunjucks&gt; node postinstall-build.js srcnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)added 422 packages from 501 contributors and audited 4700 packages in 59.195sfound 0 vulnerabilitiesINFO Start blogging with Hexo! 生成目录结构如下123456\-- scaffolds\-- source \-- _posts\-- themes|-- _config.yml|-- package.json 继续123456$ npm installnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)audited 4700 packages in 5.99sfound 0 vulnerabilities 现在该目录执行指令，开启hexo服务器123$ hexo sINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 生成目录和标签1234$ hexo n page about$ hexo n page archives$ hexo n page categories$ hexo n page tags 修改/source/tags/index.md，其他同理1234567891011121301| ---02| title: tags03| date: 2019-01-04 17:34:1504| ----&gt;01| ---02| title: tags03| date: 2019-01-04 17:34:1504| type: &quot;tags&quot;05| comments: false06| --- 关联Github在Github新建一个仓库，命名为username.github.io，例如isLouisHsu.github.io，新建时勾选Initialize this repository with a README，因为这个仓库必须不能为空。 打开博客目录下的_config.yml配置文件，定位到最后的deploy选项，修改如下1234deploy: type: git repository: git@github.com:isLouisHsu&#x2F;isLouisHsu.github.io.git branch: master 安装插件1$ npm install hexo-deployer-git --save 现在就可以将该目录内容推送到Github新建的仓库中了1$ hexo d 使用个人域名 在source目录下新建文件CNAME，输入解析后的个人域名 在Github主页修改域名 备份博客 没。没什么用我。我不备份了可以新建一个仓库专门保存文件试试 现在博客的源文件仅保存在PC上， 我们对它们进行备份，并将仓库作为博客文件夹 在仓库新建分支hexo，设置为默认分支 将仓库克隆至本地 1$ git clone https:&#x2F;&#x2F;github.com&#x2F;isLouisHsu&#x2F;isLouisHsu.github.io.git 克隆文件 将之前的Hexo文件夹中的 123456scffolds&#x2F;source&#x2F;themes&#x2F;.gitignore_config.ymlpackage.json 复制到克隆下来的仓库文件夹isLouisHsu.github.io 安装包 123$ npm install$ npm install hexo --save$ npm install hexo-deployer-git --save 备份博客使用以下指令 123$ git add .$ git commit -m &quot;backup&quot;$ git push origin hexo 部署博客指令 1$ hexo g -d 单键提交 编写脚本commit.bat，双击即可 1234git add .git commit -m &#39;backup&#39;git push origin hexohexo g -d 使用方法 目录结构 public 生成的网站文件，发布的站点文件。 source 资源文件夹，用于存放内容。 tag 标签文件夹。 archive 归档文件夹。 category分类文件夹。 downloads/code include code文件夹。 :lang i18n_dir 国际化文件夹。 _config.yml 配置文件 指令 123456789101112131415161718192021222324252627$ hexo helpUsage: hexo &lt;command&gt;Commands: clean Remove generated files and cache. config Get or set configurations. deploy Deploy your website. generate Generate static files. help Get help on a command. init Create a new Hexo folder. list List the information of the site migrate Migrate your site from other system to Hexo. new Create a new post. publish Moves a draft post from _drafts to _posts folder. render Render files with renderer plugins. server Start the server. version Display version information.Global Options: --config Specify config file instead of using _config.yml --cwd Specify the CWD --debug Display all verbose messages in the terminal --draft Display draft posts --safe Disable all plugins and scripts --silent Hide output on consoleFor more help, you can use &#39;hexo help [command]&#39; for the detailed information or you can check the docs: http:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F; 拓展功能支持插入图片1$ npm install hexo-asset-image --save 修改文件_config.yml1post_asset_folder: true在执行$ hexo n [layout] &lt;title&gt;时会生成同名文件夹，把图片放在这个文件夹内，在.md文件中插入图片1![image_name](&#x2F;title&#x2F;image_name.png) 搜索功能12$ npm install hexo-generator-searchdb --save$ npm install hexo-generator-search --save 站点配置文件_config.yml中添加12345search: path: search.xml field: post format: html limit: 10000修改主题配置文件/themes/xxx/_config.yml12local_search: enable: true 带过滤功能的首页插件在首页只显示指定分类下面的文章列表。12$ npm install hexo-generator-index2 --save$ npm uninstall hexo-generator-index --save修改_config.yml1234567index_generator: per_page: 10 order_by: -date include: - category Web # 只包含Web分类下的文章 exclude: - tag Hexo # 不包含标签为Hexo的文章 数学公式支持hexo默认的渲染引擎是marked，但是marked不支持mathjax。kramed是在marked的基础上进行修改。1234$ npm uninstall hexo-math --save # 停止使用 hexo-math$ npm install hexo-renderer-mathjax --save # 安装hexo-renderer-mathjax包：$ npm uninstall hexo-renderer-marked --save # 卸载原来的渲染引擎$ npm install hexo-renderer-kramed --save # 安装新的渲染引擎修改/node_modules/kramed/lib/rules/inline.js12345678911| escape: &#x2F;^\\([\\&#96;*&#123;&#125;\[\]()#$+\-.!_&gt;])&#x2F;,...20| em: &#x2F;^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,-&gt;11| escape: &#x2F;^\\([&#96;*\[\]()#$+\-.!_&gt;])&#x2F;,...20| em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,修改/node_modules/hexo-renderer-kramed/lib/renderer.js123456789101112131464| &#x2F;&#x2F; Change inline math rule65| function formatText(text) &#123;66| &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$67| return text.replace(&#x2F;&#96;\$(.*?)\$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);68| &#125;-&gt;64| &#x2F;&#x2F; Change inline math rule65| function formatText(text) &#123;66| &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$67| &#x2F;&#x2F; return text.replace(&#x2F;&#96;\$(.*?)\$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);68| return text;69| &#125;在主题中开启mathjax开关，例如next主题中1234# MathJax Supportmathjax: enable: true per_page: true在文章中12345678---title: title.mddate: 2019-01-04 12:47:37categories:tags:mathjax: truetop:--- 测试 A = \left[\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix}\right]背景图片更换在主题配置文件夹中，如next主题，打开文件hexo-theme-next/source/css/_custom/custom.styl，修改为123456789101112131415161718192021&#x2F;&#x2F; Custom styles.&#x2F;&#x2F; 添加背景图片body &#123; background: url(&#x2F;images&#x2F;background.jpg); background-size: cover; background-repeat: no-repeat; background-attachment: fixed; background-position: 50% 50%;&#125;&#x2F;&#x2F; 修改主体透明度.main-inner &#123; background: #fff; opacity: 0.95;&#125;&#x2F;&#x2F; 修改菜单栏透明度.header-inner &#123; opacity: 0.95;&#125; 背景音乐首先生成外链 添加到合适位置，如Links一栏后 鼠标特效 hustcc/canvas-nest.js 点击文本特效新建hexo-theme-next/source/js/click_show_text.js 123456789101112131415161718192021222324252627282930313233343536var a_idx = 0;jQuery(document).ready(function($) &#123; $("body").click(function(e) &#123; var a = new Array ("for", "while", "catch", "except", "if", "range", "class", "min", "max", "sort", "map", "filter", "lambda", "switch", "case", "iter", "next", "enum", "struct", "void", "int", "float", "double", "char", "signed", "unsigned"); var $i = $("&lt;span/&gt;").text(a[a_idx]); a_idx = (a_idx + 3) % a.length; var x = e.pageX, y = e.pageY; $i.css(&#123; "z-index": 5, "top": y - 20, "left": x, "position": "absolute", "font-weight": "bold", "color": "#333333" &#125;); $("body").append($i); $i.animate(&#123; "top": y - 180, "opacity": 0 &#125;, 3000, function() &#123; $i.remove(); &#125;); &#125;); setTimeout('delay()', 2000);&#125;);function delay() &#123; $(".buryit").removeAttr("onclick");&#125; 在文件hexo-theme-next/layout/_layout.swig中添加12345678910&lt;html&gt;&lt;head&gt; ...&lt;&#x2F;head&gt;&lt;body&gt; ... ... &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;click_show_text.js&quot;&gt;&lt;&#x2F;script&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; 看板娘xiazeyu/live2d-widget-models，预览效果见作者博客。 12npm install --save hexo-helper-live2dnpm install live2d-widget-model-hijiki 站点配置文件添加1234567891011live2d: enable: true scriptFrom: local model: use: live2d-widget-model-hijiki #模型选择 display: position: right #模型位置 width: 150 #模型宽度 height: 300 #模型高度 mobile: show: false #是否在手机端显示 人体时钟新建hexo-theme-next/source/js/honehone_clock_tr.js 1234567891011121314151617181920212223242526272829/****************************************************************************** 初期設定******************************************************************************/var swfUrl = "http://chabudai.sakura.ne.jp/blogparts/honehoneclock/honehone_clock_tr.swf";var swfTitle = "honehoneclock";// 実行LoadBlogParts();/****************************************************************************** 入力 なし 出力 document.writeによるHTML出力******************************************************************************/function LoadBlogParts()&#123; var sUrl = swfUrl; var sHtml = ""; sHtml += '&lt;object classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://fpdownload.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=8,0,0,0" width="160" height="70" id="' + swfTitle + '" align="middle"&gt;'; sHtml += '&lt;param name="allowScriptAccess" value="always" /&gt;'; sHtml += '&lt;param name="movie" value="' + sUrl + '" /&gt;'; sHtml += '&lt;param name="quality" value="high" /&gt;'; sHtml += '&lt;param name="bgcolor" value="#ffffff" /&gt;'; sHtml += '&lt;param name="wmode" value="transparent" /&gt;'; sHtml += '&lt;embed wmode="transparent" src="' + sUrl + '" quality="high" bgcolor="#ffffff" width="160" height="70" name="' + swfTitle + '" align="middle" allowScriptAccess="always" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer" /&gt;'; sHtml += '&lt;/object&gt;'; document.write(sHtml);&#125; 1&lt;script charset&#x3D;&quot;Shift_JIS&quot; src&#x3D;&quot;&#x2F;js&#x2F;honehone_clock_tr.js&quot;&gt;&lt;&#x2F;script&gt; 代码雨新建hexo-theme-next/source/js/digital_rain.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657window.onload = function()&#123; //获取画布对象 var canvas = document.getElementById("canvas"); //获取画布的上下文 var context =canvas.getContext("2d"); var s = window.screen; var W = canvas.width = s.width; var H = canvas.height; //获取浏览器屏幕的宽度和高度 //var W = window.innerWidth; //var H = window.innerHeight; //设置canvas的宽度和高度 canvas.width = W; canvas.height = H; //每个文字的字体大小 var fontSize = 12; //计算列 var colunms = Math.floor(W /fontSize); //记录每列文字的y轴坐标 var drops = []; //给每一个文字初始化一个起始点的位置 for(var i=0;i&lt;colunms;i++)&#123; drops.push(0); &#125; //运动的文字 var str ="WELCOME TO WWW.ITRHX.COM"; //4:fillText(str,x,y);原理就是去更改y的坐标位置 //绘画的函数 function draw()&#123; context.fillStyle = "rgba(238,238,238,.08)";//遮盖层 context.fillRect(0,0,W,H); //给字体设置样式 context.font = "600 "+fontSize+"px Georgia"; //给字体添加颜色 context.fillStyle = ["#33B5E5", "#0099CC", "#AA66CC", "#9933CC", "#99CC00", "#669900", "#FFBB33", "#FF8800", "#FF4444", "#CC0000"][parseInt(Math.random() * 10)];//randColor();可以rgb,hsl, 标准色，十六进制颜色 //写入画布中 for(var i=0;i&lt;colunms;i++)&#123; var index = Math.floor(Math.random() * str.length); var x = i*fontSize; var y = drops[i] *fontSize; context.fillText(str[index],x,y); //如果要改变时间，肯定就是改变每次他的起点 if(y &gt;= canvas.height &amp;&amp; Math.random() &gt; 0.99)&#123; drops[i] = 0; &#125; drops[i]++; &#125; &#125;; function randColor()&#123;//随机颜色 var r = Math.floor(Math.random() * 256); var g = Math.floor(Math.random() * 256); var b = Math.floor(Math.random() * 256); return "rgb("+r+","+g+","+b+")"; &#125; draw(); setInterval(draw,35);&#125;; hexo-theme-next/source/css/main.styl添加12345678910canvas &#123; position: fixed; right: 0px; bottom: 0px; min-width: 100%; min-height: 100%; height: auto; width: auto; z-index: -1;&#125; hexo-theme-next/layout/_layout.swig添加12&lt;canvas id&#x3D;&quot;canvas&quot; width&#x3D;&quot;1440&quot; height&#x3D;&quot;900&quot; &gt;&lt;&#x2F;canvas&gt;&lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;DigitalRain.js&quot;&gt;&lt;&#x2F;script&gt; 留言板用来比力作为后台系统。 打开主题配置文件hexo-theme-next/_config.yml，修改123# Support for LiveRe comments system.# You can get your uid from https://livere.com/insight/myCode (General web site)livere_uid: your uid 在hexo-theme-next/layout/_scripts/third-party/comments/ 目录中添加livere.swig12345678910111213141516171819&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id and not theme.gentie_productKey %&#125; &#123;% if theme.livere_uid %&#125; &lt;script type&#x3D;&quot;text&#x2F;javascript&quot;&gt; (function(d, s) &#123; var j, e &#x3D; d.getElementsByTagName(s)[0]; if (typeof LivereTower &#x3D;&#x3D;&#x3D; &#39;function&#39;) &#123; return; &#125; j &#x3D; d.createElement(s); j.src &#x3D; &#39;https:&#x2F;&#x2F;cdn-city.livere.com&#x2F;js&#x2F;embed.dist.js&#39;; j.async &#x3D; true; e.parentNode.insertBefore(j, e); &#125;)(document, &#39;script&#39;); &lt;&#x2F;script&gt; &#123;% endif %&#125;&#123;% endif %&#125; 在hexo-theme-next/layout/_scripts/third-party/comments.swig1&#123;% include &#39;.&#x2F;comments&#x2F;livere.swig&#39; %&#125; 评论无法保留？？？换成Gitment。 安装模块1npm i --save gitment 在New OAuth App为博客应用一个密钥 定位到主题配置文件，填写`enable，github_user，github_repo，client_id，client_secret123456789101112131415# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/gitment: enable: false mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide 'Powered by ...' on footer, and more language: # Force language, or auto switch by theme github_user: # MUST HAVE, Your Github Username github_repo: # MUST HAVE, The name of the repo you use to store Gitment comments client_id: # MUST HAVE, Github client id for the Gitment client_secret: # EITHER this or proxy_gateway, Github access secret token for the Gitment proxy_gateway: # Address of api proxy, See: https://github.com/aimingoo/intersect redirect_protocol: # Protocol of redirect_uri with force_redirect_protocol when mint enabled 如果遇到登陆不上的问题，转到gh-oauth.imsun.net页面，点高级-&gt;继续访问就可以了。 服务器问题不能解决，换成Gitalk。 定位到路径 themes/next/layout/_third-party/comments下面，创建一个叫做 gitalk.swig的文件，写入如下内容 1234567891011121314151617&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel&#x3D;&quot;stylesheet&quot; href&#x3D;&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;gitalk&#x2F;dist&#x2F;gitalk.css&quot;&gt; &lt;script src&#x3D;&quot;https:&#x2F;&#x2F;unpkg.com&#x2F;gitalk&#x2F;dist&#x2F;gitalk.min.js&quot;&gt;&lt;&#x2F;script&gt; &lt;script src&#x3D;&quot;https:&#x2F;&#x2F;cdn.bootcss.com&#x2F;blueimp-md5&#x2F;2.10.0&#x2F;js&#x2F;md5.min.js&quot;&gt;&lt;&#x2F;script&gt; &lt;script type&#x3D;&quot;text&#x2F;javascript&quot;&gt; var gitalk &#x3D; new Gitalk(&#123; clientID: &#39;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&#39;, clientSecret: &#39;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&#39;, repo: &#39;&#123;&#123; theme.gitalk.repo &#125;&#125;&#39;, owner: &#39;&#123;&#123; theme.gitalk.githubID &#125;&#125;&#39;, admin: [&#39;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&#39;], id: md5(window.location.pathname), distractionFreeMode: &#39;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&#39; &#125;) gitalk.render(&#39;gitalk-container&#39;) &lt;&#x2F;script&gt;&#123;% endif %&#125; 在 上面的同级目录下的 index.swig 里面加入： 1&#123;% include &#39;gitalk.swig&#39; %&#125; 在使能化之前，我们还需要修改或者说是美化一下gitalk的默认样式，如果你不进行这一步也没有影响，可能结果会丑一点。定位到： themes/next/source/css/_common/components/third-party. 然后你需要创建一个 gitalk.styl 文件。 这个文件里面写入：1234.gt-header a, .gt-comments a, .gt-popup a border-bottom: none;.gt-container .gt-popup .gt-action.is--active:before top: 0.7em; 然后同样的，在 third-party.styl里面导入一下：1@import "gitalk"; 在 layout/_partials/comments.swig 里面加入1234&#123;% elseif theme.gitalk.enable %&#125; &lt;div id&#x3D;&quot;gitalk-container&quot;&gt; &lt;&#x2F;div&gt;&#123;% endif %&#125; 在主题配置文件_config.yml12345678gitalk: enable: true githubID: # MUST HAVE, Your Github Username repo: # MUST HAVE, The name of the repo you use to store Gitment comments ClientID: # MUST HAVE, Github client id for the Gitment ClientSecret: # EITHER this or proxy_gateway, Github access secret token for the Gitment adminUser: isLouisHsu distractionFreeMode: true Reference 基于hexo+github搭建一个独立博客 - 牧云云 - 博客园 https://www.cnblogs.com/MuYunyun/p/5927491.htmlhexo+github pages轻松搭博客(1) | ex2tron’s Blog http://ex2tron.wang/hexo-blog-with-github-pages-1/hexo下LaTeX无法显示的解决方案 - crazy_scott的博客 - CSDN博客 https://blog.csdn.net/crazy_scott/article/details/79293576在Hexo中渲染MathJax数学公式 - 简书 https://www.jianshu.com/p/7ab21c7f0674怎么去备份你的Hexo博客 - 简书 https://www.jianshu.com/p/baab04284923Hexo中添加本地图片 - 蜕变C - 博客园 https://www.cnblogs.com/codehome/p/8428738.html?utm_source=debugrun&amp;utm_medium=referralhexo 搜索功能 - 阿甘的博客 - CSDN博客 https://blog.csdn.net/ganzhilin520/article/details/79047983为 Hexo 博客主题 NexT 添加 LiveRe 评论支持 https://blog.smoker.cc/web/add-comments-livere-for-hexo-theme-next.html终于！！！记录如何在hexo next主题下配置gitalk评论系统 https://jinfagang.github.io/2018/10/07/%E7%BB%88%E4%BA%8E%EF%BC%81%EF%BC%81%EF%BC%81%E8%AE%B0%E5%BD%95%E5%A6%82%E4%BD%95%E5%9C%A8hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E9%85%8D%E7%BD%AEgitalk%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Metrics]]></title>
    <url>%2F2018%2F11%2F21%2FMetrics%2F</url>
    <content type="text"><![CDATA[回归(regression)评估指标解释方差(Explained Variance) EV(\hat{y}, y) = 1 - \frac{Var(y-\hat{y})}{Var(y)}解释方差越接近$1$表示回归效果越好。 平均绝对误差(Mean Absolute Error - MAE) MAE(\hat{y}, y) = E(||\hat{y} - y||_1) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} |\hat{y}^{(i)} - y^{(i)}|$MAE$越小表示回归效果越好。 平均平方误差(Mean Squared Error - MSE)在线性回归一节，使用的损失函数即$MSE$ MSE(\hat{y}, y) = E(||\hat{y} - y||_2^2) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2其中$y$与$\hat{y}$均为$1$维向量，$MSE$越小表示回归效果越好。 其含义比较直观，即偏差的平方和。也可以从最小化方差的角度解释，定义误差向量 e = \hat{y} - y我们假定其期望为$0$，即 E(e) = 0 或 \overline{e} = 0那么误差的方差为 Var(e) = E[(e - \overline{e})^T (e - \overline{e})] = E(||e||_2^2)也即$MSE$。 均方根误差(Root Mean Squared Error - RMSE) RMSE(\hat{y}, y) = \sqrt{\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} (\hat{y}^{(i)} - y^{(i)})^2}实质与$MSE$是一样的。只不过用于数据更好的描述，使计算得损失的值较小。$RMSE$越小表示回归效果越好。 均方对数误差(Mean Squard Logarithmic Error - MSLE) MSLE(\hat{y}, y) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} \left[\log (1+y^{(i)}) - \log (1+\hat{y}^{(i)})\right]^2通常用于输出指数增长的模型，如，人口统计，商品的平均销售量，以及一段时间内的平均销售量等。注意，由对数性质，这一指标对过小的预测的惩罚大于预测过大的预测的惩罚。 中值绝对误差(Median Absolute Error - MedAE) MedAE(\hat{y}, y) = median(|y - \hat{y}|)R决定系数(R2)又称拟合优度，提供了一个衡量未来样本有多好的预测模型。最佳可能的分数是$1.0$，它可以是负的(因为模型可以任意恶化)。一个常数模型总是预测$y$的期望值，而不考虑输入特性，则得到$R^2$分数为$0.0$。 R^2(\hat{y}, y) = 1 - \frac{\sum_{i=1}^{n_{samples}} (y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^{n_{samples}} (y^{(i)} - \overline{y})^2}其中 \overline{y} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} y^{(i)}分类(classification)评估指标先作如下定义 准确率(Accuracy) Accuracy(y, \hat{y}) = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} 1(y^{(i)}=\hat{y}^{(i)})也即 Accuracy = \frac{TN+TP}{TN+TP+FN+FP}精度只是简单地计算出比例，但是没有对不同类别进行区分。因为不同类别错误代价可能不同。例如：判断这个病人是不是病危，如果不是病危错误判断为病危，那只是损失一点医务人员的时间和精力，如果是把病危的人判断为非病危状态，那损失的就是一条人命。他们之间存在重要性差异，这时候就不能用精度。对于样本不均衡的情况，也不是用精度来衡量。例如：有A类1000个，B类5个，如果我把这1005个样本都预测成A类，正确率=1000/1005=99.5%。 精确率(Precision)与召回率(Recall) 精确率(Precision) 即预测正样本中，实际为正样本的百分比，度量了分类器不会将真正的负样本错误地分为正样本的能力。 Precision = \frac{TP}{TP+FP} 召回率(Recall) 又称查全率，即实际正样本中，被预测为正样本的百分比，度量了分类器找到所有正样本的能力。 Recall = \frac{TP}{TP + FN} F度量 F1 score - Wikipedia $F_1$ 为精确率(Precision)与召回率(Recall)的调和均值(harmonic mean)。 \frac{1}{F_1} = \frac{1}{2} (\frac{1}{Precision} + \frac{1}{Recall}) 也即 F_1 = 2 · \frac{Precision·Recall}{Precision + Recall} $F_{\beta}$ 在$F_1$度量的基础上增加权值$\beta$，$\beta$越大，$Recall$的权重越大，否则$Precision$的权重越大。 \frac{1}{F_{\beta}} = \frac{1}{1+\beta^2} \frac{1}{Precision} + \frac{\beta^2}{1+\beta^2}\frac{1}{Recall} 也即 F_{\beta} = (1+\beta^2)·\frac{Precision·Recall}{(\beta^2·Precision) + Recall} 混淆矩阵Confusion matrix，也被称作错误矩阵(Error matrix)，是一个特别的表。无监督学习中，通常称作匹配矩阵(Matching matrix)。每一列表达了分类器对样本的类别预测，每一行表达了样本所属的真实类别。 例如我们有$27$个待分类样本，将其划分为Cat，Dog，Rabbit，讲实际标签与预测标签数目统计后填入混淆矩阵。 例如实际上有$8$个样本为Cat，而该分类器将其中$3$个划分为Dog，将$2$个为Dog的样本划分为Cat。我们可以根据上述混淆矩阵得出结论，该分类器对Dog和Cat分类能力较弱，而对Rabbit分类能力较强。而且正确预测的样本数目都在对角线上，很容易直观地检查表中的预测错误。 以下为scikit-learn中混淆矩阵的API1234567891011121314151617181920&gt;&gt;&gt; from sklearn.metrics import confusion_matrix&gt;&gt;&gt; &gt;&gt;&gt; y_true &#x3D; [2, 0, 2, 2, 0, 1]&gt;&gt;&gt; y_pred &#x3D; [0, 0, 2, 2, 0, 2]&gt;&gt;&gt; confusion_matrix(y_true, y_pred)array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])&gt;&gt;&gt; &gt;&gt;&gt; y_true &#x3D; [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]&gt;&gt;&gt; y_pred &#x3D; [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]&gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels&#x3D;[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])&gt;&gt;&gt; &gt;&gt;&gt; # In the binary case, we can extract true positives, etc as follows:&gt;&gt;&gt; tn, fp, fn, tp &#x3D; confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()&gt;&gt;&gt; (tn, fp, fn, tp)(0, 2, 1, 1) ROC曲线Receiver Operating Characteristic，是根据一系列不同的二分类方式(分界值或决定阈)，以召回率(真正率TPR、灵敏度)为纵坐标，fall-out(假正率FPR、$1$-特异度)为横坐标绘制的曲线。 true positive rate - TPR 所有阳性样本中有多少正确的阳性结果。 TPR = \frac{TP}{P} = \frac{TP}{TP + FN} false positive rate - FPR 所有阴性样本中有多少不正确的阳性结果。 FPR = \frac{FP}{N} = \frac{FP}{FP + TN} ROC space 分别以FPR与TPR作为横纵轴(又称灵敏度-$1$特异度曲线sensitivity vs (1 − specificity) plot)； 每次预测结果或混淆矩阵的实例代表了ROC空间中的一个点； 例如上图中$A, B, C, C’$是以下表数据计算得到的点。 在ROC空间中最左上方的点$(0, 1)$称作完美分类器(perfect classification)； 随机分类器的结果分布在ROC space对角线$(0, 0)-(1, 1)$上，当实验次数足够多，其分区趋向$(0.5, 0.5)$; 对角线以上的点代表好的分类结果(比随机的好)；线下的点代表坏的结果(比随机的差)； 注意，持续不良分类器的输出可以简单地反转以获得一个好的分类器，反转后的分类器与原分类器在平面上关于对角线对称，例如点$C’$。 ROC曲线的绘制若训练集样本中，正样本与负样本以正态分布的形式分布在样本平面上，如下图，左峰为负样本，右峰为正样本，存在部分重叠(不然就不用搞这么多分类算法了)。 若假设正样本概率密度为$f_1(x)$，负样本的概率密度为$f_0(x)$，给定阈值$T$，则右 TPR(T) = \int_T^{\infty} f_1(x) dx FPR(T) = \int_T^{\infty} f_0(x) dx选取不同的阈值划分分类器输出，就能得到ROC曲线。 在基于有限样本作ROC图时，可以看到曲线每次都是一个“爬坡”，遇到正例往上爬一格$(1/m+)$，错了往右爬一格$(1/m-)$，显然往上爬对于算法性能来说是最好的。 Area Under the Curve - AUCROC曲线下的面积AUC物理意义为，任取一对正负样本，正样本的预测值大于负样本的预测值的概率。 A = \int_{-\infty}^{\infty} TPR(T) dFPR(T) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} I(T'> T) f_1(T') f_0(T) dT' dT = P(X_1 > X_0)同样的，在有限个样本下，其面积用累加的方法计算(梯形面积) AUC = \sum_{i=1}^{m-1} \frac{1}{2} (y_{i+1} + y_i)(x_{i+1} - x_i) $AUC = 1$，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。 $0.5 &lt; AUC &lt; 1$，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 $AUC = 0.5$，跟随机猜测一样（例：丢铜板），模型没有预测价值。 $AUC &lt; 0.5$，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。 sklearn以下为scikit-learn中混淆矩阵的ROC曲线API。12345678910111213141516&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn import metrics&gt;&gt;&gt; &gt;&gt;&gt; y &#x3D; np.array([1, 1, 2, 2])&gt;&gt;&gt; scores &#x3D; np.array([0.1, 0.4, 0.35, 0.8])&gt;&gt;&gt; &gt;&gt;&gt; fpr, tpr, thresholds &#x3D; metrics.roc_curve(y, scores, pos_label&#x3D;2)&gt;&gt;&gt; fprarray([ 0. , 0.5, 0.5, 1. ])&gt;&gt;&gt; tprarray([ 0.5, 0.5, 1. , 1. ])&gt;&gt;&gt; thresholdsarray([ 0.8 , 0.4 , 0.35, 0.1 ])&gt;&gt;&gt; &gt;&gt;&gt; metrics.auc(fpr, tpr)0.75 聚类(clustering)评估指标 AI（005） - 笔记 - 聚类性能评估（Clustering Evaluation） - DarkRabbit的专栏 - CSDN博客 Wikipedia, the free encyclopedia 说明聚类性能比较好，就是聚类结果簇内相似度(intra-cluster similarity)高，而簇间相似度(inter-cluster similarity)低，即同一簇的样本尽可能的相似，不同簇的样本尽可能不同。 聚类性能的评估（度量）分为两大类： 外部评估(external evaluation)：将结果与某个参考模型(reference model)进行比较； 内部评估(internal evaluation)：直接考虑聚类结果而不利用任何参考模型。 将$n_{samples}$个样本${x^{(1)}, …, x^{(n_{samples})}}$用待评估聚类算法划分为$K$个类${X_1, …, X_K}$，假定参考模型将其划分为$L$类${Y_1, …, Y_L}$，将样本两辆匹配 \begin{cases} a = |SS| & SS = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)}, x^{(j)} \in Y_l\} \\ b = |SD| & SD = \{(x^{(i)}, x^{(j)}) | x^{(i)}, x^{(j)} \in X_k; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \\ c = |DS| & DS = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)}, x^{(j)} \in Y_l\} \\ d = |DD| & DD = \{(x^{(i)}, x^{(j)}) | x^{(i)} \in X_{k1}, x^{(j)} \in X_{k2}; x^{(i)} \in Y_{l1}, x^{(j)} \in Y_{l2}\} \end{cases}其中$k = 1, …, K; l = 1, …, L$ a + b + c + d = \left( \begin{matrix} n \\ 2 \end{matrix} \right) = \frac{n(n-1)}{2} $SS$包含两种划分中均属于同一类的样本对； $SD$包含用待评估聚类算法划分中属于同一类，而在参考模型中属于不同类的样本对； $DS$包含用待评估聚类算法划分中属于不同类，而在参考模型中属于同一类的样本对； $DD$包含两种划分中均不属于同一类的样本对。 常用外部评估(external evaluation)Rand Index(RI) Rand index - Wikipedia RI = \frac{a+d}{a + b + c + d} = \frac{a+d}{\left(\begin{matrix} n \\ 2 \end{matrix}\right)}显然，结果值在$[0,1]$之间，且值越大越好。 当为$0$时，两个聚类无重叠； 当为$1$时，两个聚类完全重叠。 Adjust Rand Index(ARI)让$RI$有了修正机会(corrected-for-chance)，在取值上从$[0,1]$变成$[-1, 1]$ 对于$X$与$Y$的重叠可以用一个列联表(contingency table)表示，记作$[n_{ij}]$，$n_{ij} = |X_i \bigcap Y_j|$ 则定义$ARI$如下 互信息与调整互信息(Adjusted Mutual Information - AMI) 关于互信息可查看熵一节说明。 $X_i$类别的概率定义为 P(k) = \frac{|X_k|}{N}则划分结果的熵定义为 H(X) = - \sum_k P(k) \log P(k)类似的 P'(l) = \frac{|Y_l|}{N} H(Y) = - \sum_j P'(l) \log P'(l)另外 P(k, l) = \frac{|X_k, Y_l|}{N}那么两种划分的互信息定义为 MI(X, Y) = \sum_{k, l} P(k, l) \log \frac{P(k, l)}{P(k) P'(l)}和$ARI$一样，我们对它进行调整。 E[MI(X, Y)] = \sum_k \sum_l \sum_{n_{kl} = \max\{1, a_k + b_l - N\}}^{\min \{a_k, b_l\}} \frac{n_{kl}}{N} \log \left( \frac{N·n_{kl}}{a_k b_l} \right) × \frac {a_k!b_l!(N-a_k)!(N-b_l)!} {N!n_{kl}!(a_k-n_{kl})!(b_l-n_{kl})!(N-a_k-b_l+n_{kl})!}最终$AMI$表达式为 AMI(X, Y) = \frac{MI(X, Y) - E[MI(X, Y)]}{\max \{H(X), H(Y)\} - E[MI(X, Y)]}同质性(Homogeneity)与完整性(Completeness)这两个类似分类种的的准确率(accuracy)与召回率(recall)。 同质性(Homogeneity) 即一个簇仅包含一个类别的样本 H = 1 - \frac{H(X|Y)}{H(X)} 其中$H(X|Y)$为条件熵 H(X|Y) = \sum_k \sum_l P(X_k, Y_l) \log \frac{P(Y_l)}{P(X_k, Y_l)} = \sum_k \sum_l \frac{n_{kl}}{N} \log \frac{n_{kl}}{N} 完整性(Completeness) 同类别样本被归类到相同簇中 C = 1 - \frac{H(Y|X)}{H(Y)} $V-measure$ Homogeneity和Completeness的调和平均 V = \frac{1}{\frac{1}{2} \left(\frac{1}{H} + \frac{1}{C}\right)} = \frac{2HC}{H + C} Fowlkes-Mallows index(FMI)成对精度和召回率的几何均值 Fowlkes–Mallows index - Wikipedia 定义 $TP$ as the number of points that are present in the same cluster in both $A_1$ and $A_2$. $FP$ as the number of points that are present in the same cluster in $A_1$ but not in $A_2$. $FN$ as the number of points that are present in the same cluster in $A_2$ but not in $A_1$. $TN$ as the number of points that are in different clusters in both $A_1$ and $A_2$. 则 TP + FP + TN + FN = \frac{n(n-1)}{2}定义 FMI = \sqrt{\frac{TP}{TP + FP} · \frac{TP}{TP + FN}}杰卡德系数(Jaccard Coefficient - JC) Jaccard index - Wikipedia 给定两个具有$n$个元素的集合$A, B$，定义 $M_{11}$ represents the total number of attributes where $A$ and $B$ both have a value of $1$. $M_{01}$ represents the total number of attributes where the attribute of $A$ is $0$ and the attribute of $B$ is $1$. $M_{10}$ represents the total number of attributes where the attribute of $A$ is $1$ and the attribute of $B$ is $0$. $M_{00}$ represents the total number of attributes where $A$ and $B$ both have a value of $0$. 则有 M_{11} + M_{01} + M_{10} + M_{00} = n Jaccard相似度系数 J = \frac{M_{11}}{M_{01} + M_{10} + M_{11}} 也即$J=\frac{A \cap B}{A \cup B}$ Jaccard距离 D_J = 1 - J 常用内部评估(internal evaluation)轮廓系数(Silhouette coefficient)又称侧影法，适用于实际类别信息未知的情况，对其中一个样本点$x^{(i)}$，记 $a(i)$：到本簇其他样本点的距离的平均值 $b(i)$：该点到其他各个簇的样本点的平均距离的最小值 定义轮廓系数 S(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}或者 S(i) = \begin{cases} 1 - \frac{a(i)}{b(i)} & a(i) < b(i) \\ 0 & a(i) = b(i) \\ \frac{b(i)}{a(i)} - 1 & a(i) > b(i) \end{cases}其含义如下 当$a(i) \ll b(i)$时，无限接近于$1$，则意味着聚类合适； 当$a(i) \gg b(i)$时，无限接近于$-1$，则意味着把样本i聚类到相邻簇中更合适； 当$a(i)\approxeq b(i)$时，无限接近于$0$，则意味着样本在两个簇交集处。 一般再对各个点的轮廓系数求均值 \overline{S} = \frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} S(i) 当$\overline{S} &gt; 0.5$，表示聚类合适； 当$\overline{S} &lt; 0.2$，表示表明数据不存在聚类特征 Calinski-Harabaz(CH)也适用于实际类别信息未知的情况，以$K$分类为例 类内散度$W$ W(K) = \sum_k \sum_{C(j)=k} ||x_j - \overline{x_k}||^2 类间散度$B$ B(K) = \sum_k a_k ||\overline{x_k} - \overline{x}||^2 $CH$ CH(K) = \frac{B(K)(N-K)}{W(K)(K-1)} Davies-Bouldin Index(DBI)定义 $c_k$：簇$C_k$的中心点 $\sigma_k$：簇$C_k$中所有元素到$c_k$的距离的均值 $d(c_i, c_j)$：簇中心$c_i$与$c_j$之间的距离 则 DBI = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)$DBI$越小越好 Dunn index(DI)定义 $d(i,j)$：两类簇的距离，定义方法多样，例如两类簇中心的距离； $d’(k)$：簇$C_k$的类内距离，同样的，可定义多种，例如簇$C_k$中任意两点距离的最大值。 则 DI = \frac{\min_{1 \leq i < j \leq K} d(i, j)}{\max_{1 \leq k \leq K} d'(k)}sklearn中的评价指标 3.3. Model evaluation: quantifying the quality of predictions — scikit-learn 0.19.0 documentation - ApacheCN 1234567891011121314151617181920212223242526&gt;&gt;&gt; from sklearn import metrics&gt;&gt;&gt; dir(metrics)[&#39;SCORERS&#39;, &#39;__all__&#39;, &#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__path__&#39;, &#39;__spec__&#39;, &#39;accuracy_score&#39;, &#39;adjusted_mutual_info_score&#39;, &#39;adjusted_rand_score&#39;, &#39;auc&#39;, &#39;average_precision_score&#39;, &#39;balanced_accuracy_score&#39;, &#39;base&#39;, &#39;brier_score_loss&#39;, &#39;calinski_harabaz_score&#39;, &#39;check_scoring&#39;, &#39;classification&#39;, &#39;classification_report&#39;, &#39;cluster&#39;, &#39;cohen_kappa_score&#39;, &#39;completeness_score&#39;, &#39;confusion_matrix&#39;, &#39;consensus_score&#39;, &#39;coverage_error&#39;, &#39;davies_bouldin_score&#39;, &#39;euclidean_distances&#39;, &#39;explained_variance_score&#39;, &#39;f1_score&#39;, &#39;fbeta_score&#39;, &#39;fowlkes_mallows_score&#39;, &#39;get_scorer&#39;, &#39;hamming_loss&#39;, &#39;hinge_loss&#39;, &#39;homogeneity_completeness_v_measure&#39;, &#39;homogeneity_score&#39;, &#39;jaccard_similarity_score&#39;, &#39;label_ranking_average_precision_score&#39;, &#39;label_ranking_loss&#39;, &#39;log_loss&#39;, &#39;make_scorer&#39;, &#39;matthews_corrcoef&#39;, &#39;mean_absolute_error&#39;, &#39;mean_squared_error&#39;, &#39;mean_squared_log_error&#39;, &#39;median_absolute_error&#39;, &#39;mutual_info_score&#39;, &#39;normalized_mutual_info_score&#39;, &#39;pairwise&#39;, &#39;pairwise_distances&#39;, &#39;pairwise_distances_argmin&#39;, &#39;pairwise_distances_argmin_min&#39;, &#39;pairwise_distances_chunked&#39;, &#39;pairwise_fast&#39;, &#39;pairwise_kernels&#39;, &#39;precision_recall_curve&#39;, &#39;precision_recall_fscore_support&#39;, &#39;precision_score&#39;, &#39;r2_score&#39;, &#39;ranking&#39;, &#39;recall_score&#39;, &#39;regression&#39;, &#39;roc_auc_score&#39;, &#39;roc_curve&#39;, &#39;scorer&#39;, &#39;silhouette_samples&#39;, &#39;silhouette_score&#39;, &#39;v_measure_score&#39;, &#39;zero_one_loss&#39;]]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Entropy]]></title>
    <url>%2F2018%2F11%2F21%2FEntropy%2F</url>
    <content type="text"><![CDATA[信息量概率$p$是对确定性的度量，那么信息量就是对不确定性的度量，公式定义为 I(x) = - \log p(x) \tag{1}信息量也被称为随机变量$x$的自信息(self-information) 底数为$2$时，单位为bit，底数为$e$时，单位为nat 信息熵信息熵(information entropy)定义为 H(X) = - \sum_{x} p(x) \log p(x) \tag{2}可看作信息量的期望,在$0-1$分布的信息熵为 H(p) = - p \log p - (1 - p) \log (1 - p)图像如下，可见在$p=0.5$时，熵最大。 函数$y=x \log x$的图像有 \lim_{x \rightarrow 0} y = \lim_{x \rightarrow 1} y = 0 联合熵根据信息熵的定义，推广到多维随机变量，就得到联合熵的定义式，以$2$维随机变量为例 H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y) \tag{3}可推广至多维。 交叉熵现在有关于样本集的两个概率分布$p(x)$和$q(x)$，其中$p(x)$为真实分布，$q(x)$非真实分布。 如果用真实分布$p(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为: H(p) = - \sum_x p(x) \log p(x)如果用非真实分布$q(x)$来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为: H(p, q) = - \sum_x p(x) \log q(x) \tag{4}注意 H(p, q) - H(p) = \sum_x p(x) \log \frac{p(x)}{q(x)} = D_{KL}(p||q)当用非真实分布$q(x)$得到的平均码长比真实分布$p(x)$得到的平均码长多出的比特数就是相对熵。我们希望通过最小化相对熵$D_{KL}(p||q)$使$q(x)$尽量趋近$p(x)$，即 q(x) = \arg \min_{q(x)} D_{KL} (p||q)而$H(p)$是样本集的熵，为固定的值，故 q(x) = \arg \min_{q(x)} H(p, q)即等价于最小化交叉熵。 条件熵条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。定义为在给定$X$下$Y$的条件概率分布的熵对$X$的期望，即 H(Y|X) = E_{p(x)} H(Y|X=x) = \sum_x p(x) H(Y|X=x) \tag{5}其中 H(Y|X=x) = - \sum_y p(y|x) \log p(y|x)故 H(Y|X) = \sum_x p(x) \left[- \sum_y p(y|x) \log p(y|x)\right] = - \sum_x \sum_y p(x, y) \log p(y|x)即 H(Y|X) = - \sum_{x, y} p(x, y) \log p(y|x) \tag{6}实际上，条件熵满足 H(Y|X) = H(X, Y) - H(X) \tag{7} 证明：已知 H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y) H(X) = - \sum_{x} p(x) \log p(x)则 H(X, Y) - H(X) = - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x} p(x) \log p(x) = - \sum_{x, y} p(x, y) \log p(x, y) + \sum_{x, y} p(x, y) \log p(x) = \sum_{x, y} p(x, y) \log \frac{p(x)}{p(x, y)} = \sum_{x, y} p(x, y) \log p(y|x) = H(Y|X) 相对熵相对熵(relative entropy)，又称KL散度(Kullback–Leibler divergence)。可以用来衡量两个概率分布之间的差异，就是求$p(x)$与$q(x)$之间的对数差在 pp 上的期望值。 D_{KL} (p||q) = E_{p(x)} \log \frac{p(x)}{q(x)} = \sum_x p(x) \log \frac{p(x)}{q(x)} \tag{8}注意 相对熵不具有对称性，即 D_{KL} (p||q) \neq D_{KL} (q||p) $D_{KL} (p||q) \geq 0$ 证明： D_{KL} (p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = - \sum_x p(x) \log \frac{q(x)}{p(x)}由Jensen inequality \sum_x p(x) \log \frac{q(x)}{p(x)} \leq \log \sum_x p(x) \frac{q(x)}{p(x)} = \log \sum_x q(x)所以 D_{KL} (p||q) \geq - \log \sum_x q(x)而$0 \leq q(x) \leq 1$，故 D_{KL} (p||q) \geq 0]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Non-parameter Estimation]]></title>
    <url>%2F2018%2F11%2F19%2FNon-parameter-Estimation%2F</url>
    <content type="text"><![CDATA[前言若参数估计时我们不知道样本的分布形式，那么就无法确定需要估计的概率密度函数，无法用最大似然估计、贝叶斯估计等参数估计方法，应该用非参数估计方法。 需要知道的是，作为非参数方法的共同问题是对样本数量需求较大，只要样本数目足够大众可以保证收敛于任何复杂的位置密度，但是计算量和存储量都比较大。当样本数很少时，如果能够对密度函数有先验认识，则参数估计能取得更好的估计效果。 基本原理若有$M$个样本$x^{(1)}, …, x^{(M)}$，依概率密度函数$p(x)$独立同分布抽样得到。 一个样本$x$落在区域$R$中的概率$P$可表示为 P = \int_R p(x) dx \tag{1}我们通过计算$P$来估计概率密度$p(x)$。 $K$个样本落入区域$R$的概率$P_K$为二项分布，即$K \sim B(M, P)$ P_K = \left(\begin{matrix} M\\K \end{matrix}\right) P^K (1-P)^{M-K} \tag{2}则$K$的期望与方差分别为 E(K) = MP; D(K) = MP(1-P)样本个数$M$越多，$D(K)$越大，即$K$在期望附近的波峰越明显，因此样本足够多时，用$K/M$作为$P$的一个估计非常准确，即 P \approx \frac{K}{M} \tag{3}若我们假设$p(x)$是连续的，且区域$R$足够小，记其体积为$V$，那么有 P = \int_R p(x)dx \approx p(x) V \tag{4}所以根据$(3)(4)$，得到 p(x) \approx \frac{K/M}{V} \tag{*}但是我们获得的其实为平滑后的概率密度函数 \frac{P}{V} = \frac{\int_R p(x)dx}{\int_R dx}我们希望其尽可能地趋近$p(x)$，那么必须要求$V \rightarrow 0$，但是这样就可能不包含任何样本，那么$p(x)\approx 0$，这样估计的结果毫无意义。 所以在实际中，一般构造多个包含样本$x$的区域$R_1, …, R_i, …, R_n$，第$i$个区域使用$i$个样本，记$V_i$为$R_i$的体积，$M_i$为落在$R_i$中的样本个数，则对$p(x)$第$i$次估计$p_i(x)$表示为 p_i(x) \approx \frac{M_i / M}{V_i} \tag{5}若要求$p_i(x)$收敛到$p(x)$，则必须满足 $\lim_{i\rightarrow \infty} V_i = 0$ $\lim_{i\rightarrow \infty} M_i = 0$ $\lim_{i\rightarrow \infty} \frac{M_i}{M} = 0$ 直方图法记不记得小学时的直方图统计，直方图方法的思想就是这样，以$1$维样本为例，我们将$x$的取值范围平均等分为$K$个区间，统计每个区间内样本的个数，由此计算区间的概率密度。 原理若共有$N$维样本$M$组，在每个维度上$K$等分，就有$K^N$个小空间，每个小空间的体积$V_i$可以定义为 V_i = \prod_{n=1}^N d_n, i=1,...,K^N其中 d_n = \frac{\max x_n - \min x_n}{K}假设样本落到各个小空间的概率相同，若第$i$个小空间包含$M_i$个样本，则该空间的概率密度$\hat{p_i}$为 \hat{p_i} = \frac{M_i / M}{V_i} \tag{6}估计的效果与小区间的大小密切相连，如果区域选择过大，会导致最终估计出来的概率密度函数非常粗糙；如果区域的选择过小，可能会导致有些区域内根本没有样本或者样本非常少，这样会导致估计出来的概率密度函数很不连续。 代码@Github: Non-parametric Estmation 我们可以用matplotlib.pyplot.hist()或numpy.histogram()实现 matplotlib 1n, bins, patches &#x3D; plt.hist(arr, bins&#x3D;10, normed&#x3D;0, facecolor&#x3D;&#39;black&#39;, edgecolor&#x3D;&#39;black&#39;,alpha&#x3D;1，histtype&#x3D;&#39;bar&#39;) Args 参数很多，选几个常用的讲解 arr: 需要计算直方图的一维数组 bins: 直方图的柱数，可选项，默认为10 normed: 是否将得到的直方图向量归一化。默认为0 facecolor: 直方图颜色 edgecolor: 直方图边框颜色 alpha: 透明度 histtype: 直方图类型，‘bar’, ‘barstacked’, ‘step’, ‘stepfilled’ Returns n: 直方图向量，是否归一化由参数normed设定 bins: 返回各个bin的区间范围 patches: 返回每个bin里面包含的数据，是一个list numpy 1hist, bin_edges &#x3D; histogram(a, bins&#x3D;10, range&#x3D;None, normed&#x3D;None, weights&#x3D;None, density&#x3D;None) 12345678910def histEstimate(X, n_bins, showfig&#x3D;False): &quot;&quot;&quot; 直方图密度估计 Args: n_bins: &#123;int&#125; 直方图的条数 Returns: hist: &#123;ndarray(n_bins,)&#125; &quot;&quot;&quot; n, bins, patches &#x3D; plt.hist(X, bins&#x3D;n_bins, normed&#x3D;1, facecolor&#x3D;&#39;lightblue&#39;, edgecolor&#x3D;&#39;white&#39;) if showfig: plt.show() return n, bins, patches matplotlib直方图显示如下 拟合各中心点显示如下 $K_n$近邻估计法随着样本数的增加，区域的体积应该尽可能小，同时又必须保证区域内有充分多的样本，但是每个区域的样本数有必须是总样本数的很小的一部分，而不是与直方图估计那样体积不变。 那么我们想，能否根据样本的分布调整分区大小呢，$K$近邻估计法就是一种采用可变大小区间的密度估计方法。 原理根据总样本确定参数$K_n$，在求样本$x$处的密度估计$\hat{p}(x)$时，调整区域体积$V(x)$，直到区域内恰好落入$K_n$个样本，估计公式为 \hat{p}(x) = \frac{K_n/M}{V(x)} \tag{7}一般指定超参数$a$，取 K_n = a × \sqrt{M} \tag{8} \hat{p}(x) = \frac{a × \sqrt{M} /M}{V(x)} = \frac{K_n'/M}{V'(x)}其中$K_n’ = a,V’(x) = V(x)×\frac{1}{\sqrt{M}}$ 在样本密度比较高的区域的体积就会比较小，而在密度低的区域的体积则会自动增大，这样就能够较好的兼顾在高密度区域估计的分辨率和在低密度区域估计的连续性。 Parzen窗法又称核密度估计。 原理我们暂时假设待估计点$x$的附近区间$R$为一个$N$维的超立方体，用$h$表示边的长度，那么 V_i = h^N即定义窗函数$\varphi(·)$，表示落入以$x$为中心的超立方体的区域的点 \varphi \left(\frac{x_i-x}{h}\right) = \begin{cases} 1 & \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2}, n=1,...,N \\ 0 & otherwise \end{cases} \tag{9} \frac{|x_{in}-x_n|}{h} \leq \frac{1}{2} 即 (x_i-x)_n \leq \frac{h}{2}这里的$h$起到单位化的作用，便于推广 那么落入以$x$为中心的超立方体的区域的点的个数为 M_i = \sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right) \tag{10}代入$p(x) \approx \frac{M_i/M}{V_i}$，我们得到 p(x) \approx \frac{\sum_{i=1}^M \varphi \left(\frac{x_i-x}{h}\right)/M}{V_i} = \frac{1}{M} \sum_{i=1}^M \frac{1}{V_i} \varphi \left(\frac{x_i-x}{h}\right) \tag{11}我们定义核函数(或称“窗函数”) \kappa(z) = \frac{1}{V_i} \varphi(z) \tag{12}核函数反应了一个观测样本$x_i$对在$x$处的概率密度估计的贡献，与样本$x_i$和$x$的距离有关。而概率密度估计就是在这一点上把所有观测样本的贡献进行平均 p(x) \approx \frac{1}{M} \sum_{i=1}^M \kappa\left(\frac{x_i-x}{h}\right) \tag{13}核函数核函数应满足概率密度的要求，即 \kappa(z) \geq 0 \And \int \kappa(z)dz = 1通常有以下几种核函数 均匀核 \kappa(z) = \begin{cases} 1 & |z_n| \leq \frac{1}{2}, n=1,...,N \\ 0 & otherwise \end{cases} 高斯核(正态核) 高斯核是将窗放大到整个空间，各个观测样本$x_i$对待观测点$x$的加权和(越远权值越小)。 \kappa(z) = \frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}} \exp \left(-\frac{1}{2} (z - \mu)^T \Sigma^{-1} (z - \mu)\right) 超球窗 \kappa(z) = \begin{cases} V^{-1} & ||z|| \leq 1 \\ 0 & otherwise \end{cases} $z=\frac{x_i-x}{h}$，故$||z||\leq 1$即$||x_i-x||^2\leq h^2$此时$h$表示超球体的半径 sklearnsklearn.neighbors.KernelDensity — scikit-learn 0.19.0 documentation - ApacheCN123456789101112131415161718&gt;&gt;&gt; from sklearn.neighbors import KernelDensity&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])&gt;&gt;&gt; kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)&gt;&gt;&gt; kde.score_samples(X)array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698, -0.41076071])&gt;&gt;&gt; kde.sample(10)array([[ 1.80042291, 1.1030739 ], [ 0.87299669, 1.0762352 ], [-2.40180586, -1.19554374], [-1.97985919, -1.19361193], [-2.95866231, -2.1972637 ], [-1.12739556, -0.80851063], [ 1.03756706, 1.24855099], [ 1.21729703, 1.02345815], [-2.11816867, -1.0486257 ], [-1.04875537, -0.89928711]]) 代码具体代码见@Github: Non-parametric Estmation 定义核函数如下1234# 高斯核gaussian = lambda z: np.exp(-0.5*(np.linalg.norm(z)**2)) / np.sqrt(2*np.pi)# 均匀核square = lambda z: 1 if (np.linalg.norm(z) &lt;= 0.5) else 0 密度估计函数如下，需要对连续范围内的各个点，即$x \in [min(X), max(X)]$进行估计获得p，作图显示$x-p$即可123456789101112131415161718192021def parzenEstimate(X, kernel, h, n_num=50): """ 核参数估计 Args: X: &#123;ndarray(n_samples,)&#125; kernel: &#123;function&#125; 可调用的核函数 h: &#123;float&#125; 核函数的参数 Returns: p: &#123;ndarray(n_num,)&#125; Notes: - 一维，故`V_i = h` - p(x) = \frac&#123;1&#125;&#123;M&#125; \sum_&#123;i=1&#125;^M \kappa \left( \frac&#123;x_i - x&#125;&#123;h&#125; \right) """ x = np.linspace(np.min(X), np.max(X), num=n_num) p = np.zeros(shape=(x.shape[0],)) z = lambda x, x_i, h: (x - x_i) / h V_i = h; n_samples = X.shape[0] for idx in range(x.shape[0]): for i in range(X.shape[0]): p[idx] += kernel(z(x[idx], X[i], h)) / V_i p[idx] /= n_samples return p 均匀核 $h=0.5$ $h=0.8$ $h=1.0$ $h=2.0$ 高斯核 $h=0.5$ $h=0.8$ $h=1.0$ $h=2.0$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Parameter Estimation]]></title>
    <url>%2F2018%2F11%2F19%2FParameter-Estimation%2F</url>
    <content type="text"><![CDATA[贝叶斯学派与频率学派有何不同？ - 任坤的回答 - 知乎 引言参数估计(parameter estimation)，统计推断的一种。根据从总体中抽取的随机样书．来估计总体分布中未知参数的过程。主要介绍最大似然估计(MLE: Maximum Likelihood Estimation)，最大后验概率估计(MAP: Maximum A Posteriori Estimation)，贝叶斯估计(Bayesian Estimation)。 解释一下“似然函数”和“后验概率”，在贝叶斯决策一节，给出定义如下 P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}上式中$ k=1,…,K $，各部分定义如下$P(c_k|x)$——后验概率(posteriori probability)$P(c_k)$——先验概率(priori probability)$p(x|c_k)$——$c_k$关于$x$的似然函数(likelihood)$p(x)$——证据因子(evidence) 引例以最经典的掷硬币实验为例，假设有一枚硬币，投掷一次出现正面记$”1”$，投掷$10$次的实验结果如下 \{ 0， 1， 1， 1， 1， 0， 1， 1， 1，0 \}记硬币投掷结果为随机变量$X$，且$ x \in {0, 1}$，硬币投掷一次服从二项分布，估计二项分布的参数$\theta$ 最大似然估计(MLE)似然函数 Likelihood function - Wikipedia 离散型 L(x | \theta) = p_{\theta}(x)=P_{\theta}(X = x) 连续型 L(x | \theta) = f_{\theta}(x) 很多人能讲出一大堆哲学理论来阐明这一对区别。但我觉得，从工程师角度来讲，这样理解就够了:频率 $vs$ 贝叶斯 = $P(X; w)$ $vs$ $P(X|w)$ 或 $P(X,w)$ 作者：许铁-巡洋舰科技链接：https://www.zhihu.com/question/20587681/answer/122348889来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 模型有数据集$D = {x_1, x_2, …, x_N}$，按$c$个类别分成${D_1, D_2, …, D_C}$，各个类别服从的概率分布密度函数模型已给出，估计参数$\hat{\Theta} = {\hat{\theta}_{c_1}, \hat{\theta}_{c_2}, …, \hat{\theta}_{c_C}} $ 假定 类别间独立，且各自服从概率分布密度函数为$p(x|c_j)$ 各类别的概率密度$p(x|c_j)$以参数$\theta_{c_j}$确定，即$p(x|c_j; \theta_{c_j})$ 故似然函数为 L(D | \Theta) = P(x_1, x_2, ..., x_N | \Theta) = \prod_{i=1}^N p(x_i | \theta_{x_i \in c_j}) 理解为，在参数$\Theta$为何值的条件下，实验结果出现数据集$D$的概率最大 求取其极大值对应的参数即可 一般取对数似然函数\log L(D | \Theta) = \sum_{i=1}^N \log p(x_i | \theta_{x_i \in c_j}) 极大值即对应梯度为$\vec{0}$的位置，即 ∇_\Theta \log L(D | \Theta) = \vec{0} \Rightarrow \hat{\Theta} Some comments about ML ML estimation is usually simpler than alternative methods. Has good convergence properties as the number of training samples increases. If the model chosen for p(x|θ) is correct, and independence assumptions among variables are true, ML will give very good results. If the model is wrong, ML will give poor results. —— Zhao Haitao. Maximum Likelihood and Bayes Estimation 例：正态分布的最大似然估计数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的的最大似然估计 P(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} L(D | \mu, \sigma^2) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} =\left( \frac{1}{\sqrt{2\pi} \sigma } \right)^N \prod_{i=1}^N e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}取对数似然 \log L(D | \mu, \sigma^2) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^21. 参数$\mu$的估计 \frac{∂}{∂\mu} L(D | \mu, \sigma^2) = \frac{1}{\sigma^2} (\sum_{i=1}^N x_i - N\mu) = 0 \Rightarrow \hat{\mu} = \frac{1}{N} \sum_{i=1}^N x_i2. 参数$\sigma^2$的估计 \frac{∂}{∂\sigma^2} \log L(D | \mu, \sigma^2) = - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 = 0 \Rightarrow \hat{\sigma^2} = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)^2 参数$\hat{\mu}, \hat{\sigma}^2$的值与样本均值和样本方差相等 最大后验概率估计(MAP) 模型最大似然估计是求参数$\theta$, 使似然函数$P(D | \theta)$最大，最大后验概率估计则是求$\theta$使$P(\theta | D)$最大 理解为，在已出现的实验样本$D$上，参数$\theta$取何值的概率最大 且注意到 P(\theta | D) = \frac{P(D | \theta)P(\theta)}{P(D)}故$MAP$不仅仅使似然函数$P(D | \theta)$最大，而且使$P(\theta)$最大，即 \theta = argmax L(\theta | D) L(\theta | D) = P(\theta) P(D | \theta) = P(\theta) \prod_{i=1}^N p(x_i | \theta) 比$ML$多了一项$P(\theta)$ 取对数后 \log L(\theta | D) = \sum_{i=1}^N \log p(x_i | \theta) + \log P(\theta) 求取极大值 ∇_\theta L(\theta | D) = 0 \Rightarrow \hat{\theta} $MAP$和$MLE$的区别：$MAP$允许我们把先验知识加入到估计模型中，这在样本很少的时候是很有用的，因为样本很少的时候我们的观测结果很可能出现偏差，此时先验知识会把估计的结果“拉”向先验，实际的预估结果将会在先验结果的两侧形成一个顶峰。通过调节先验分布的参数，比如beta分布的$\alpha, \beta$，我们还可以调节把估计的结果“拉”向先验的幅度，$\alpha, \beta$越大，这个顶峰越尖锐。这样的参数，我们叫做预估模型的“超参数”。极大似然估计，最大后验概率估计(MAP)，贝叶斯估计 - 李鑫o_O - CSDN博客 例：正态分布的最大后验概率估计数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的最大后验概率估计 p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} \log p(x_i | \mu, \sigma^2) = - \frac{1}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} (x_i - \mu)^2 1. 参数$\mu$的估计给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即 p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}} \log p(\mu) = - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2 则 \log L(\mu, \sigma^2 | D) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\mu_0}^2) - \frac{1}{2\sigma_{\mu_0}^2} (\mu - \mu_0)^2则 \frac{∂}{∂\mu} \log L(\mu, \sigma^2 | D) = \frac{1}{\sigma^2} \sum_{i=0}^N (x_i - \mu) - \frac{1}{\sigma_{\mu_0}^2} (\mu - \mu_0) = 0 \Rightarrow \hat{\mu} = \frac{\mu_0 \sigma^2 + \sigma_{\mu_0}^2 \sum_{i=0}^N x_i} {\sigma^2 + N \sigma_{\mu_0}^2} = \frac{\mu_0 + \frac{\sigma_{\mu_0}^2}{\sigma^2} \sum_{i=0}^N x_i} {1 + \frac{\sigma_{\mu_0}^2}{\sigma^2} N }2. 参数$\sigma^2$的估计给定先验条件：$\sigma^2$服从正态分布$N(\sigma_0^2, \sigma_{\sigma_0^2}^2)$，即 p(\sigma^2) = \frac{1}{\sqrt{2\pi} \sigma_{\sigma_0^2}} e^ {-\frac{(\sigma^2- \sigma_0^2)^2}{2 \sigma_{\sigma_0^2} ^2}} \log p(\sigma^2) = - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2 则 \log L(\mu, \sigma^2 | D) = - \frac{N}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2} \log(2\pi \sigma_{\sigma_0}^2) - \frac{1}{2\sigma_{\sigma_0}^2} (\sigma - \sigma_0)^2则 \frac{∂}{∂\sigma^2} \log L(\mu, \sigma^2 | D) = - \frac{N}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^N (x_i - \mu)^2 - \frac{1}{2\sigma_{\sigma_0}^2} \frac{\sigma - \sigma_0}{\sigma} \Rightarrow \hat{\sigma^2}(略) \frac{∂}{∂\sigma^2}(\sigma - \sigma_0)^2 = 2(\sigma - \sigma_0) \frac{∂}{∂\sigma^2} (\sigma - \sigma_0) = \frac{\sigma - \sigma_0}{\sigma} 贝叶斯估计模型 p(\theta | D) = \frac {P(D | \theta)p(\theta)} {P(D)} = a · p(\theta) \prod_{i=1}^N p(x_i | \theta)其中$a$是使 \int p(\theta | D) = 1利用“质心公式”求解贝叶斯的点估计 θ_{Bayes} = \int θ·p(θ|D) d θ例：正态分布的贝叶斯估计数据集(单类别)服从高斯分布$N(\mu, \sigma^2)$时的贝叶斯估计 p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}}参数$\mu$的估计给定先验条件：$\mu$服从正态分布$N(\mu_0, \sigma_{\mu_0}^2)$，即 p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}}则 P(\mu | D) = a · p(\mu) \prod_{i=1}^N p(x_i | \mu) = a · \frac{1}{\sqrt{2\pi}\sigma_{\mu_0}} e^ {-\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2}} \prod_{i=1}^N \frac{1}{\sqrt{2\pi} \sigma } e^ {-\frac{(x_i - \mu)^2}{2\sigma^2}} = a · \left( \frac{1}{\sqrt{2\pi}} \right)^{N + 1} \frac{1}{\sigma_{\mu_0} \sigma^N} e^ { -\frac{(\mu - \mu_0)^2}{2\sigma_{\mu_0}^2} -\sum_{i=1}^N \frac{(x_i - \mu)^2}{2\sigma^2} }易证 我已经想到了一个绝妙的证明,但是这台电脑的硬盘太小了,写不下。 p(\mu | D) = \frac{1}{\sqrt{2\pi}\sigma_N} e^ {-\frac{(\mu - \mu_N)^2}{2\sigma_N^2}}其中 \mu_N = \frac{N \sigma_0^2} {N \sigma_0^2 + \sigma^2} \frac{1}{N} \sum_{i=1}^N x_i +\frac{\sigma^2}{N \sigma_0^2 + \sigma^2} \mu_0 \sigma_N^2 = \frac{\sigma_0^2 \sigma^2} {N \sigma_0^2 + \sigma^2} 与$MLE$，$MAP$的区别 相比较$MLE$与$MAP$的点估计，贝叶斯估计得到的结果是参数$\theta$的密度函数$p(\theta | D)$ 最大后验概率估计为求取对应最大后验概率的点 \theta = argmax_\theta p(\theta | D) 贝叶斯估计为求取整个取值范围的概率密度$p(\theta | D)$，既然如此，必有 \int p(\theta | D) d\theta = 1 统计学习方法学习笔记（一）—极大似然估计与贝叶斯估计原理及区别 - YJ-20 - 博客园 p(\theta | D) = \frac {p(\theta) \prod_{i=1}^N p(x_i | \theta)} {\int_\theta p(\theta) \prod_{i=1}^N p(x_i | \theta) d\theta}由于$\theta$是满足一定概率分布的变量，所以在计算的时候需要将考虑所有$\theta$取值的情况，在计算过程中不可避免地高复杂度。所以计算时并不把所有地后验概率$p(\theta | D)$都找出来，而是采用类似于极大似然估计地思想，来极大化后验概率，得到这种有效的叫做$MAP$ 引例的求解已知硬币投掷结果服从$Bernoulli$分布 X 0 1 P 1-θ θ 或者 P(X_i) = \theta ^{X_i} (1 - \theta) ^{1 - X_i}最大似然估计实验结果中正面出现$7$次，反面出现$3$次，似然函数为 L(\theta) = \prod_{i=1}^{10} \theta ^{X_i} (1 - \theta) ^{1 - X_i} = \theta ^7 (1 - \theta) ^3取对数似然函数并求极大值 \log L(\theta) = 7 \log \theta + 3 \log (1 - \theta)令 \frac{∂}{∂ \theta} \log L(\theta) = \frac{7}{\theta} - \frac{3}{1-\theta} = 0解得 \theta = 0.7即硬币服从$B(1, 0.7)$的概率分布 做出$L(\theta)$图像验证，如下 最大后验概率估计给定先验条件 \theta \thicksim N(\theta_0, \sigma_{\theta_0}^2)则最大化 L(\theta | D) = \theta ^7 (1 - \theta) ^3 · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}}取对数 \log L(\theta | D) = 7 \log \theta + 3 \log (1 - \theta) - \frac{1}{2} \log(2\pi \sigma_{\theta_0}^2) - \frac{1}{2\sigma_{\theta_0}^2} (\theta - \theta_0)^2求取极大值点 \frac{∂}{∂\theta} \log L(\theta | D) = \frac {7}{\theta} - \frac{3}{1-\theta} - \frac{\theta - \theta_0}{\sigma_{\theta_0}^2} = 0得到 \theta^3 - (\theta_0 + 1) \theta^2 + (\theta_0 - 10\sigma_{\theta_0}^2) \theta + 7\sigma_{\theta_0}^2 = 0以下为选取不同先验条件时的$L(\theta | D)$图像，用于对比 第一张图为极大似然估计$L(D|\theta)$ 第二张图为先验概率密度函数$P(\theta)$ 第三张图为最大后验概率估计$L(\theta | D)$，$\hat{\theta}$由查表法求解代码见仓库 $\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.42$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.56$ $\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\Rightarrow$ $\hat{\theta} = 0.70$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\Rightarrow$ $\hat{\theta} = 0.50$ $\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$ $\Rightarrow$ $\hat{\theta} = 0.70$ 结论 由图$1, 2, 3$，可以看到当$\theta_0$偏移$0.7$时，$MAP$结果也相应偏移； 由图$2, 4, 5$，可以看到当$\sigma_{\theta_0}^2$越小，即越确定先验概率分布时，$MAP$结果也越趋向于先验概率分布。 贝叶斯估计先验条件为正态分布 \theta \thicksim N(\theta_0, \sigma_{\theta_0}^2) p(\theta | D) = a · p(\theta) \prod_{i=1}^N p(x_i | \theta) = a · \frac{1}{\sqrt{2\pi}\sigma_{\theta_0}} e^ {-\frac{(\theta - \theta_0)^2}{2\sigma_{\theta_0}^2}} · \theta ^7 (1 - \theta) ^3 参数$a$使用scipy.integrate.quad求解 选取不同先验条件时的$L(\theta | D)$图像，用于对比 $\theta_0 = 0.3, \sigma_{\theta_0} = 0.1$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.1$ $\theta_0 = 0.7, \sigma_{\theta_0} = 0.1$ $\theta_0 = 0.5, \sigma_{\theta_0} = 0.01$ $\theta_0 = 0.5, \sigma_{\theta_0} = 1.0$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Clustering]]></title>
    <url>%2F2018%2F11%2F16%2FClustering%2F</url>
    <content type="text"><![CDATA[目录这是第一篇关于无监督学习的博文，无监督的学习则不是尝试预测任何东西，而是寻找数据中的特征，在无监督学习中，有一个重要的方法称为聚类，是把具有相同特征的数据聚集在一组。 目录 基础知识 距离度量方法 hard vs. soft clustering 聚类方法的分类 常用聚类方法 K均值(K-means) 原理 计算步骤 缺点与部分解决方法 改进 类似的算法 代码 均值漂移(Meanshift) 原理 高斯权重 计算步骤 代码 谱聚类(Spectral Clustering) 原理 无向权重图 相似矩阵 拉普拉斯矩阵(Graph Laplacians) 无向图的切图 cut Ratio Cut N Cut 计算步骤 代码 DBSCAN 原理 计算思想 算法步骤 高斯混合模型(GMM) 层次聚类(Hierarchical Clustering) 图团体检测(Graph Community Detection) 基础知识距离度量方法机器学习中距离度量方法有很多，以下简单介绍几种。 机器学习常用的距离度量方法 - taotiezhengfeng的博客 - CSDN博客算法中的各种距离（欧式距离，马氏距离，闵可夫斯基距离……） - 啊哦123的博客 - CSDN博客 定义两个$n$维向量 x = [x_1, x_2, ..., x_n]^T y = [y_1, y_2, ..., y_n]^T 曼哈顿距离(Manhattan Distance) d = || x - y ||_1 = \sum_i |x_i - y_i| 欧氏距离(Euclidean Distance) d = || x - y ||_2 = \sqrt{\sum_i (x_i - y_i)^2} 闽可夫斯基距离(Minkowski Distance) d = || x - y ||_p = \left(\sum_i | x_i - y_i |^{p} \right)^{\frac{1}{p}} 当$p$取$1$时为曼哈顿距离，取$2$时为欧式距离。 余弦距离(Cosine) d = \frac{x^T y}{||x||_2 ||y||_2} = \frac{\sum_i x_i y_i}{\sqrt{\sum_i x_i^2} \sqrt{\sum_i y_i^2}} 突然想到为什么向量的夹角余弦是怎么来的，高中学习一直背的公式，现在给一下证明。证明：向量的夹角公式 从余弦定理(余弦定理用几何即可)出发，有 \cos \theta = \frac{a^2+b^2-c^2}{2ab}其中 \begin{cases} ||\vec{a}|| = \sqrt{x_1^2 + y_1^2} \\ ||\vec{b}|| = \sqrt{x_2^2 + y_2^2} \\ ||\vec{c}|| = \sqrt{(x_1 - x_2)^2 + (x_2 - y_2)^2} \end{cases}故 \cos \theta = \frac {(\sqrt{x_1^2 + y_1^2})^2 + (\sqrt{x_2^2 + y_2^2})^2 - (\sqrt{(x_1 - x_2)^2 + (x_2 - y_2)})^2} {2 \sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}} = \frac {x_1 x_2 + y_1 y_2} {\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}} = \frac{a^T b}{||a||·||b||} hard vs. soft clustering 硬聚类(hard clustering) 计算的是一个硬分配(hard ssignment)过程,即每个样本仅仅属于一个簇。 软聚类(soft clustering) 分配过程是软的，即一个样本的分配结果是在所有簇上的一个分布，在软分配结果中，一个样本可能对多个簇都具有隶属度。 聚类方法的分类 划分方法 K-means，K-medoids，GMM等。 层次方法 AGNES，DIANA，BIRCH，CURE和CURE-NS等。 基于密度的方法 DBSCAN，OPTICS，DENCLUE等。 其他 如STING等。 常用聚类方法K均值(K-means)是最为经典的基于划分的聚类方法，是十大经典数据挖掘算法之一，通常用于寻找次优解，再通过其他算法(如GMM)寻找更优的聚类结果。 原理给定$N$维数据集 X = [x^{(1)}, x^{(2)}, ..., x^{(M)}]指定类别数$K$与初始中心点$\mu^{(0)}$，将样本划分到中心点距离其最近的簇中，再根据本次划分更新各簇的中心$\mu^{(t)}$，如此迭代直至得到最好的聚类结果。预测测试样本时，将其划分到中心点距其最近的簇，也可通过KNN等方法。 一般使用欧式距离度量样本到各中心点的距离，也可选择余弦距离等，这也是K-means算法的关键 D(x^{(i)}, \mu_k) = || x^{(i)} - \mu_k ||_2^2定义损失函数为 J(\Omega) = \sum_i \sum_k r^{(i)}_k D(x^{(i)}, \mu_k)其中 r^{(i)}_k = \begin{cases} 1 & x^{(i)} \in C_k \\ 0 & otherwise \end{cases}或表示为 r^{(i)} = [0, ..., 1_k, ..., 0]^T在迭代过程中，损失函数的值不断下降，优化目标为 \min J(\Omega)计算步骤 随机选取$K$个中心点； 遍历所有数据，计算每个点到各中心点的距离； 将每个数据划分到最近的中心点中； 计算每个聚类的平均值，作为新的中心点； 重复步骤2-步骤4，直到这k个中线点不再变化(收敛)，或执行了足够多的迭代； K-means更新迭代过程如下图 缺点与部分解决方法 局部最优 初值敏感 初始点的选择会影响K-means聚类的结果，即可能会陷入局部最优解，如下图 可通过如下方法解决 多次选择初始点运行K-means算法，选择最优的作为输出结果； K-means++ 需要定义mean，对于标称型(categorical)数据不适用 需要给定聚类簇数目$K$ 这里给出一种选择簇数目的方法，选择多个$K$值进行聚类，计算代价函数，做成折线图后如下，可以看到在$K=3$处损失值的变化率出现较大变化，则可选择簇的数目为$3$。 噪声数据干扰大 对于非凸集(non-convex)数据无能为力 谱聚类可解决非凸集数据的聚类问题。 改进 K-means++ 改进初始点选择方法，第$1$个中心点随机选择；之后的初始中心点根据前面选择的中心点决定，若已选取$n$个初始聚类中心$(0&lt;n&lt;K)$，选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。 ISODATA 思想：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别. Kernel K-means 参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类。 类似的算法与K-means类似的算法有很多，例如 K-medoids K-means的取值范围可以是连续空间中的任意值，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。K-medoids的取值是数据样本范围中的样本，且可应用在非数值型数据样本上。 k-medians $K$中值，选择中位数更新各簇的中心点。 K-centers 混合类型数据的K-Centers聚类算法/The K-Centers Clustering Algorithm for Categorical and Mixe 代码@Github: K-Means12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697class KMeans(): def __init__(self, n_cluster, mode): self.n_cluster &#x3D; n_cluster # 簇的个数 self.mode &#x3D; mode # 距离度量方式 self.centroids &#x3D; None # 簇的中心 self.loss &#x3D; float(&#39;inf&#39;) # 优化目标值 plt.ion() def fit(self, X, max_iter&#x3D;5, min_move&#x3D;0.1, display&#x3D;False): def initializeCentroids(): &#39;&#39;&#39; 选择初始点 &#39;&#39;&#39; centroid &#x3D; np.zeros(shape&#x3D;(self.n_cluster, X.shape[1])) # 保存选出的点 pointIdx &#x3D; [] # 保存已选出的点的索引 for n in range(self.n_cluster): idx &#x3D; np.random.randint(0, X.shape[0]) # 随机选择一个点 while idx in pointIdx: # 若该点已选出，则丢弃重新选择 idx &#x3D; np.random.randint(0, X.shape[0]) pointIdx.append(idx) centroid[n] &#x3D; X[idx] return centroid def dist2Centroids(x, centroids, mode): &#39;&#39;&#39; 返回向量x到k个中心点的距离值 &#39;&#39;&#39; d &#x3D; np.zeros(shape&#x3D;(self.n_cluster,)) for n in range(self.n_cluster): d[n] &#x3D; mathFunc.distance(x, centroids[n], mode) return d def nearestInfo(centroids, mode): &#39;&#39;&#39; 每个点最近的簇中心索引、距离 &#39;&#39;&#39; ctIdx &#x3D; -np.ones(shape&#x3D;(X.shape[0],), dtype&#x3D;np.int8) # 每个点最近的簇中心索引，初始化为-1，可作为异常条件 ctDist &#x3D; np.ones(shape&#x3D;(X.shape[0],), dtype&#x3D;np.float) # 每个点到最近簇中心的距离 for i in range(X.shape[0]): dists &#x3D; dist2Centroids(X[i], centroids, mode) if mode &#x3D;&#x3D; &#39;Euclidean&#39;: ctIdx[i] &#x3D; np.argmin(dists) elif mode &#x3D;&#x3D; &#39;Cosine&#39;: ctIdx[i] &#x3D; np.argmax(dists) ctDist[i] &#x3D; dists[ctIdx[i]] # 保存最相似的距离度量，用于计算loss return ctIdx, ctDist def updateCentroids(ctIdx): &#39;&#39;&#39; 更新簇中心 &#39;&#39;&#39; centroids &#x3D; np.zeros(shape&#x3D;(self.n_cluster, X.shape[1])) for n in range(self.n_cluster): X_ &#x3D; X[ctIdx &#x3D;&#x3D; n] # 筛选出离簇中心Cn最近的样本点 centroids[n] &#x3D; np.mean(X_, axis&#x3D;0) # 根据筛选出的样本点更新中心值 return centroids def loss(dist): return np.mean(dist**2) # ----------------------------------------- loss_min &#x3D; float(&#39;inf&#39;) # 最优分类时的损失值，最小 n_iter &#x3D; 0 while n_iter &lt; max_iter: # 每次迭代选择不同的初始点 n_iter +&#x3D; 1; isDone &#x3D; False # 表示本次迭代是否已收敛 centroids_tmp &#x3D; initializeCentroids() # 选择本次迭代的初始点 loss_last &#x3D; float(&#39;inf&#39;) # 本次迭代中，中心点更新前的损失值 n_update &#x3D; 0 # 本次迭代的更新次数计数 while not isDone: n_update +&#x3D; 1 ctIdx, ctDist &#x3D; nearestInfo(centroids_tmp, mode&#x3D;self.mode) centroids_tmp &#x3D; updateCentroids(ctIdx) # 更新簇中心 # --- 可视化 --- if (display&#x3D;&#x3D;True) and (X.shape[1] &#x3D;&#x3D; 2): plt.ion() plt.figure(n_iter); plt.cla() plt.scatter(X[:, 0], X[:, 1], c&#x3D;ctIdx) plt.scatter(centroids_tmp[:, 0], centroids_tmp[:, 1], c&#x3D;&#39;r&#39;) plt.pause(0.5) # ------------- loss_now &#x3D; loss(ctDist); moved &#x3D; np.abs(loss_last - loss_now) if moved &lt; min_move: # 若移动过小，则本次迭代收敛 isDone &#x3D; True print(&#39;第%d次迭代结束，中心点更新%d次&#39; % (n_iter, n_update)) else: loss_last &#x3D; loss_now if loss_now &lt; loss_min: self.centroids &#x3D; centroids_tmp # 保存损失最小的模型(最优) loss_min &#x3D; loss_now # print(&#39;聚类结果已更新&#39;) self.loss &#x3D; loss_min print(&#39;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 迭代结束 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#39;) def predict(self, X): &#39;&#39;&#39; 各个样本的最近簇中心索引 &#39;&#39;&#39; labels &#x3D; -np.ones(shape&#x3D;(X.shape[0],), dtype&#x3D;np.int) # 初始化为-1，可用作异常条件 for i in range(X.shape[0]): dists_i &#x3D; np.zeros(shape&#x3D;(self.n_cluster,)) # 保存X[i]到中心点Cn的距离 for n in range(self.n_cluster): dists_i[n] &#x3D; mathFunc.distance(X[i], self.centroids[n], mode&#x3D;self.mode) if self.mode &#x3D;&#x3D; &#39;Euclidean&#39;: labels[i] &#x3D; np.argmin(dists_i) elif self.mode &#x3D;&#x3D; &#39;Cosine&#39;: labels[i] &#x3D; np.argmax(dists_i) return labels 簇数的选择代码如下123456789101112def chooseBestK(X, start, stop, step&#x3D;1, mode&#x3D;&#39;Euclidean&#39;): Ks &#x3D; np.arange(start, stop + 1, step, dtype&#x3D;np.int) # 待选择的K Losses &#x3D; np.zeros(shape&#x3D;Ks.shape) # 保存不同K值时的最小损失值 for k in range(1, Ks.shape[0] + 1): # 对于不同的K，训练模型，计算损失 print(&#39;K &#x3D; %d&#39;, k) estimator &#x3D; KMeans(n_cluster&#x3D;k, mode&#x3D;mode) estimator.fit(X, max_iter&#x3D;10, min_move&#x3D;0.01, display&#x3D;False) Losses[k - 1] &#x3D; estimator.loss plt.ioff() plt.figure(); plt.xlabel(&#39;n_clusters&#39;); plt.ylabel(&#39;loss&#39;) plt.plot(Ks, Losses) # 做出loss-K曲线 plt.show() 均值漂移(Meanshift)本质是一个迭代的过程，能够在一组数据的密度分布中寻找到局部极值，比较稳定，而且是无参密度估计(不需要事先知道样本数据的概率密度分布函数，完全依靠对样本点的计算)，而且在采样充分的情况下，一定会收敛，即可以对服从任意分布的数据进行密度估计。 原理有一个滑动窗口的思想，即利用当前中心点一定范围内(通常为球域)的点迭代更新中心点，重复移动窗口，直到满足收敛条件。简单的说，Meanshift就是沿着密度上升的方向寻找同属一个簇的数据点。 定义点$x_0$的$\epsilon$球域如下 S_h(x_0) = \{ x | (x - x_0)^T (x - x_0) \leq \epsilon \}若有$n$个点$(x_1, …, x_n)$落在中心点$ptCentroid$的邻域内，其分布如图 则偏移向量计算方式为 vecShift = \frac{1}{n} \sum_{i=1}^n (x_i - ptCentroid)中心点更新公式为 ptCentroid := ptCentroid + vecShift 展开后可发现，其更新公式即 vecShift = \frac{1}{n} \sum_{i=1}^n x_i - ptCentroid ptCentroid := \frac{1}{n} \sum_{i=1}^n x_i 一个滑动窗口的动态更新过程如下图初始化多个滑动窗口进行MeanShift算法，其更新过程如下，其中每个黑点代表滑动窗口的质心，每个灰点代表一个数据点 高斯权重基本思想是，距离当前中心点近的向量对更新结果权重大，而远的权重小，可减小远点的干扰，如下图，$vecShift_2$为高斯权重下的偏移向量 其偏移向量计算方式为 vecShift = \frac{1}{n} \sum_{i=1}^n w_i · (x_i - ptCentroid) w_i = \frac{\kappa(x_i - ptCentroid)}{\sum_j \kappa(x_j - ptCentroid)}其中 \kappa(z) = \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{||z||^2}{2\sigma^2} \right)中心点更新公式仍然为 ptCentroid := ptCentroid + vecShift 展开也可得到 ptCentroid := \frac{\sum_{i=1}^n w_i x_i}{\sum_j w_j} 计算步骤对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$\epsilon_0$，终止条件参数$\epsilon_1$，簇合并参数$\epsilon_2$，并指定样本距离度量方式，目标为将其划分为$K$个簇。 初始化： 在样本集中随机选择$K_0(K_0 \gg K)$个样本作为初始中心点，以邻域大小为$\epsilon_0$建立滑动窗口； 各个样本初始化一个标记向量，用于记录被各类别访问的次数； 以单个滑动窗口分析，记其中心点为$ptCentroid$，找到滑动窗口内的所有点，记作集合$M$，认为这些点属于该滑动窗口所属的簇类别，同时，这些点被该簇访问的次数$+1$； 以$ptCentroid$为中心，计算其到集合$M$中各个元素的向量，以这些向量计算得到偏移向量$vecShift$； 更新中心点：$ptCentroid = ptCentroid + vecShift$，即滑动窗口沿着$vecShift$方向移动，距离为$||vecShift||$； 重复步骤$2-4$，直到$||vecShift||&lt;\epsilon_1$，保存当前中心点； 如果收敛时当前簇$ptCentroid$与其它已经存在的簇的中心的距离小于阈值$\epsilon_2$，那么这两个簇合并。否则，把当前簇作为新的簇类，增加$1$类； 重复迭代直到所有的点都被标记访问； 根据每个样本被各簇的访问频率，取访问频率最大的那个簇类别作为当前点集的所属类。 即不同类型的滑窗沿着密度上升的方向进行移动，对各样本点进行标记，最后将样本划分为标记最多的类别；当两类非常接近时，合并为一类。 代码@Github: MeanShift 先定义了窗格对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class SlidingWindow(): """ Attributes: centroid: &#123;ndarray(n_features,)&#125; epsilon: &#123;float&#125; 滑动窗格大小，为半径的平方 sigma: &#123;float&#125; 高斯核函数的参数 label: &#123;int&#125; 该窗格的标记 X: &#123;ndarray(n_samples, n_features)&#125; containIdx: &#123;ndarray(n_contain,)&#125; 窗格内包含点的索引 """ def __init__(self, centroid, epsilon, sigma, label, X): self.centroid = centroid self.epsilon = epsilon self.sigma = sigma self.label = label self.containIdx = self.updateContain(X) def k(self, z): """ 高斯核函数 Args: z: &#123;ndarray(n_features,)&#125; Notes: - \kappa(z) = \frac&#123;1&#125;&#123;\sqrt&#123;2\pi&#125;&#125; \exp \left( - \frac&#123;||z||^2&#125;&#123;2\sigma^2&#125; \right) """ norm = np.linalg.norm(z) return np.exp(- 0.5 * (norm / self.sigma)**2) / np.sqrt(2*np.pi) def step(self, X): """ 更新滑动窗格的中心点和所包含点 Returns: &#123;float&#125; """ dshift = self.shift(X) self.containIdx = self.updateContain(X) return dshift def shift(self, X): """ 移动窗格 Args: vecShift: &#123;ndarray(n_features,)&#125; Returns: dshift: &#123;float&#125; 移动的距离 """ (n_samples, n_features) = X.shape n_contain = self.containIdx.shape[0] contain_weighted_sum = np.zeros(shape=(n_features, )) weight_sum = 0 # 按包含的点进行移动 for i_contain in range(n_contain): vector = X[self.containIdx[i_contain]] - self.centroid weight = self.k(vector) contain_weighted_sum += weight*X[self.containIdx[i_contain]] weight_sum += weight centroid = contain_weighted_sum / weight_sum # 计算移动的距离 dshift = np.linalg.norm(self.centroid - centroid) self.centroid = centroid return dshift def updateContain(self, X): """ 更新窗格内的点索引 Args: X: &#123;ndarray(n_samples, n_features)&#125; Notes: - 用欧式距离作为度量 """ d = lambda x_i, x_j: np.linalg.norm(x_i - x_j) n_samples = X.shape[0] containIdx = np.array([], dtype='int') for i_samples in range(n_samples): if d(X[i_samples], self.centroid) &lt; self.epsilon: containIdx = np.r_[containIdx, i_samples] return containIdx 聚类算法如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109class MeanShift(): """ Attributes: n_clusters: &#123;int&#125; 划分簇的个数 n_windows: &#123;int&#125; 滑动窗格的个数 epsilon: &#123;float&#125; 滑动窗格的大小 sigma: &#123;float&#125; &#123;float&#125; 高斯核参数 thresh: &#123;float&#125; 若两个窗格中心距离小于thresh，则合并两类簇 min_move: &#123;float&#125; 终止条件 windows: &#123;list[class SlidingWindow()]&#125; Note: - 假设所有点均被窗格划过 """ def __init__(self, n_clusters, n_windows=-1, epsilon=0.5, sigma=2, thresh=1e-2, min_move=1e-3): self.n_clusters = n_clusters self.n_windows = 5*n_clusters if (n_windows == -1) else n_windows self.epsilon = epsilon self.sigma = sigma self.thresh = thresh self.min_move = min_move self.windows = [] self.centroids = None def fit(self, X): (n_samples, n_features) = X.shape # 创建窗格 for i_windows in range(self.n_windows): idx = np.random.randint(n_samples) window = SlidingWindow(X[idx], self.epsilon, self.sigma, i_windows, X) # 将各窗格包含的点标记 n_contain = window.containIdx.shape[0] self.windows.append(window) dshift = float('inf') # 初始化为无穷大 plt.figure(); plt.ion() while dshift &gt; self.min_move: # ------ 做图显示 ------ plt.cla() plt.scatter(X[:, 0], X[:, 1], c='b') for i_windows in range(self.n_windows): centroid = self.windows[i_windows].centroid plt.scatter(centroid[0], centroid[1], c='r') plt.pause(0.5) # --------------------- dshift = self.step(X) plt.ioff() # 合并窗格 dists = np.zeros(shape=(self.n_windows, self.n_windows)) for i_windows in range(self.n_windows): for j_windows in range(i_windows): centroid_i = self.windows[i_windows].centroid centroid_j = self.windows[j_windows].centroid dists[i_windows, j_windows] = np.linalg.norm(centroid_i-centroid_j) dists[j_windows, i_windows] = dists[i_windows, j_windows] # 获得距离相近索引 index = np.where(dists&lt;self.thresh) # 用于标记类别 winlabel = np.zeros(shape=(self.n_windows,), dtype='int') label = 1; winlabel[0] = label for i_windows in range(self.n_windows): idx_row = index[0][i_windows] idx_col = index[1][i_windows] # 若其中一个点被标记，则将令一个点并入该类 if winlabel[idx_row]!=0: winlabel[idx_col] = winlabel[idx_row] elif winlabel[idx_col]!=0: winlabel[idx_row] = winlabel[idx_col] # 否则新创建类别 else: label += 1 winlabel[idx_row] = label winlabel[idx_col] = label # 将标签一样的窗格合并 labels = list(set(winlabel)) # 去重后的标签 n_labels = len(labels) # 标签种类数 self.centroids = np.zeros(shape=(n_labels, n_features)) # 记录最终聚类中心 for i_labels in range(n_labels): cnt = 0 for i_windows in range(self.n_windows): if winlabel[i_windows] == labels[i_labels]: self.centroids[i_labels] += self.windows[i_windows].centroid cnt += 1 self.centroids[i_labels] /= cnt # 取同类窗格中心点的均值 return self.centroids def step(self, X): """ update all sliding windows Returns: dshift: \sum_i^&#123;n_windows&#125; dshift_&#123;i&#125; """ dshift = 0 for i_windows in range(self.n_windows): dshift += self.windows[i_windows].step(X) # label the points n_contain = self.windows[i_windows].containIdx.shape[0] return dshift def predict(self, X): """ 简单的用近邻的方法求 """ (n_samples, n_features) = X.shape dists = np.zeros(shape=(n_samples, self.n_clusters)) for i_samples in range(n_samples): for i_clusters in range(self.n_clusters): dists[i_samples, i_clusters] = np.linalg.norm(X[i_samples]-self.centroids[i_clusters]) return np.argmin(dists, axis=1) 谱聚类(Spectral Clustering)谱聚类是从图论中演化出来的算法，后来在聚类中得到了广泛的应用，比起传统的K-Means算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多。 原理 谱聚类（spectral clustering）原理总结 - 刘建平Pinard - 博客园 无向权重图我们用点的集合$V$和边的集合$E$描述一个图，即$G(V, E)$，其中$V$即数据集中的点 V = [v_1, v_2, ..., v_n]而点$v_i, v_j$间连接权值$w_{ij}$组成邻接矩阵$W$，由于为无向图，故满足$w_{ij}=w_{ji}$ W = \left[ \begin{matrix} w_{11} & ... & w_{1n} \\ ... & ... & ... \\ w_{n1} & ... & w_{nn} \\ \end{matrix} \right]对于图中的任意一个点$v_i$，定义其度$d_i$为 d_i = \sum_{j=1}^n w_{ij}则我们可以得到一个度矩阵$D=diag(d_1, …, d_n)$ D = \left[ \begin{matrix} d_1 & & \\ & ... & \\ & & d_n\\ \end{matrix} \right]除此之外，对于$V$中子集$V_{sub} \subset V$，定义子集$V_{sub}$点的个数为 |V_{sub}| := n_{sub}另外，定义该子集中点的度之和为 vol(V_{sub}) = \sum_{i \in V_{sub}} d_i相似矩阵上面讲到的邻接矩阵$W$可以指定权值，但对于数据量庞大的数据集，这显然不是一个$wise$的选择。我们可以用相似矩阵$S$来获得邻接矩阵$W$，基本思想是，距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高。 构建邻接矩阵$W$的方法有三类：$\epsilon$-邻近法，$K$邻近法和全连接法，定义距离 d_{ij} = ||x^{(i)} - x^{(j)}||_2^2 $\epsilon$-邻近法 设置距离阈值$\epsilon$，用欧式距离度量两点的距离$d_{ij}$，然后通过下式确定邻接权值$w_{ij}$ w_{ij} = \begin{cases} 0 & d_{ij} > \epsilon \\ \epsilon & otherwise \end{cases} 两点间的权重要不就是$\epsilon$，要不就是0，距离远近度量很不精确，因此在实际应用中，我们很少使用$\epsilon$-邻近法。 $K$邻近法 第一种 只要一个点在另一个点的$K$近邻中，则保留$d_{ij}$ w_{ij} = \begin{cases} \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)}) or x^{(j)} \in KNN(x^{(i)}) \\ 0 & otherwise \end{cases} 第二种 互为$K$近邻时保留$d_{ij}$ w_{ij} = \begin{cases} \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) & x^{(i)} \in KNN(x^{(j)}) and x^{(j)} \in KNN(x^{(i)}) \\ 0 & otherwise \end{cases} 全连接法 可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和Sigmoid核函数。最常用的是高斯核函数RBF，此时相似矩阵和邻接矩阵相同 w_{ij} = \exp \left( -\frac{d_{ij}}{2\sigma^2} \right) 拉普拉斯矩阵(Graph Laplacians)定义 L = D - W正则化的拉普拉斯矩阵为 L = D^{-1} (D - W)具有的性质如下 $L^T = L$ 其特征值均为实数，即$\lambda_i \in \mathbb{R}$ 正定性：$\lambda_i \geq 0$ 对于任意向量$x$，都有 x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2 证明： x^T L x = x^T D x - x^T W x = \sum_i d_i x_i^2 - \sum_{ij} w_{ij} x_i x_j = \frac{1}{2} \left[ \sum_i d_i x_i^2 - 2\sum_{ij} w_{ij} x_i x_j + \sum_j d_j x_j^2 \right]其中$ d_i = \sum_j w_{ij} $，所以 x^T L x = \frac{1}{2} \sum_{i,j} w_{ij} (x_i - x_j)^2 无向图的切图cut我们希望把一张无向图$G(V, E)$按一定方法切成多个子图，各个子图间无连接，每个子图的点集为$V_1, …, V_K$，满足 $\bigcup_{k=1}^K V_k = V$ $V_i \cap V_j = \emptyset$ 定义两个子图点集合$A, B$之间的切图权重为 W(A, B) = \sum_{i \in A, j \in B} w_{ij} 共有$n_A × n_B$个权值作累加 那么对于$K$个子图点的集合$V_1, …, V_K$，定义切图为 cut(V_1, ..., V_K) = \frac{1}{2} \sum_{i=1}^K cut(V_i, \overline{V_i}) cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i})其中$\overline{V_i}$表示$V_i$的补集，或者 \overline{V_i} = \bigcup_{k \neq i} V_k通过最小化$cut(V_1, …, V_K)$使子图内权重和大，而子图间权重和小。但是这种方法存在问题，如下图 选择一个权重最小的边缘的点，比如$C$和$H$之间进行$cut$，这样可以最小化$cut(V_1, …, V_K)$，但是却不是最优的切图。 为解决上述问题，需要对每个子图的规模做出限定，以下介绍两种切图方式。 Ratio Cut不仅考虑最小化$cut(V_1, …, V_K)$，而且最大化每个子图的点个数，即 RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{|V_k|} cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) $W(V_k, \overline{V_k}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij}$ $|V_k| = n_k$ 如果按照遍历的方法求解，由前面分析，$W(V_k, \overline{V_k})$需计算$n_{V_k} × n_{\overline{V_k}}$次累加，计算量庞大，那么如何求解呢？ 定义指示向量$h_k$，其构成矩阵$H$ H = [ h_1, ..., h_k, ..., h_K]其中 h_k = \left[h_{k1}, h_{k2}, , ..., h_{kM} \right]^T h_{ki} = \begin{cases} \frac{1}{\sqrt{|V_k|}} & x^{(i)}\in V_k \\ 0 & otherwise \end{cases} $h_k$为单位向量，且两两正交 h_i^T h_j = \begin{cases} \sum_{|V_i|} \frac{1}{|V_i|} = |V_i| · \frac{1}{|V_i|} = 1 & i = j \\ 0 & i \neq j \end{cases} 那么由拉式矩阵性质$4$ h_k^T L h_k = \frac{1}{2} \sum_{i,j} w_{ij} (h_{ki} - h_{kj})^2 = \frac{1}{2} [ \sum_{i \in V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 + \sum_{i \notin V_k, j \in V_k} w_{ij} (h_{ki} - h_{kj})^2 + \sum_{i \in V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 + \sum_{i \notin V_k, j \notin V_k} w_{ij} (h_{ki} - h_{kj})^2 ] = \frac{1}{2} [ \sum_{i \in V_k, j \in V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - \frac{1}{\sqrt{|V_k|}})^2 + \sum_{i \notin V_k, j \in V_k} w_{ij} (0 - \frac{1}{\sqrt{|V_k|}})^2 + \sum_{i \in V_k, j \notin V_k} w_{ij} (\frac{1}{\sqrt{|V_k|}} - 0)^2 + \sum_{i \notin V_k, j \notin V_k} w_{ij} (0 - 0)^2 ] = \frac{1}{2} [ \sum_{i \notin V_k, j \in V_k} w_{ij} \frac{1}{|V_k|} + \sum_{i \in V_k, j \notin V_k} w_{ij} \frac{1}{|V_k|} ] cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) = \sum_{i \in V_k, j \in \overline{V_k}} w_{ij} h_k^T L h_k = \frac{1}{2} [\frac{1}{|V_k|} cut(V_k, \overline{V_k}) + \frac{1}{|V_k|} cut(V_k, \overline{V_k})] = \frac{1}{|V_k|} cut(V_k, \overline{V_k})推到这里就能理解为什么要定义$h_k$了 RatioCut(V_1, ..., V_K) = \frac{1}{2} \sum_k h_k^T L h_k并且 h_k^T L h_k = tr(H^T L H) H^T L H = \left[ \begin{matrix} — & h_1^T & — \\ & ... & \\ — & h_K^T & — \\ \end{matrix} \right] L \left[ \begin{matrix} | & & | \\ h_1 & ... & h_K \\ | & & | \end{matrix} \right] = \left[ \begin{matrix} h_1^T L h_1 & ... & h_1^T L h_K \\ ... & ... & ... \\ h_K^T L h_K & ... & h_K^T L h_K \\ \end{matrix} \right] 所以最终优化目标为 \min_H tr(H^T L H) s.t. H^T H = I H^T H = \left[ \begin{matrix} h_1^T h_1 & ... & h_1^T h_K \\ ... & ... & ... \\ h_K^T h_K & ... & h_K^T h_K \\ \end{matrix} \right] 而矩阵的正交相似变换$A = P \Lambda P^{-1}$满足 tr(A) = tr(\Lambda) = \sum_i \lambda_i故 tr(H^T L H) = tr(L) = \sum_{i=1}^M \lambda_i$\lambda_i$为矩阵$L$的特征值。 我们再进行维度规约，将维度从$M$降到$k_1$，即找到$k_1$个最小的特征值之和。 N Cut推导过程与RatioCut完全一致，只是将分母$|V_i|$换成$vol(V_i)$ NCut(V_1, ..., V_K) = \frac{1}{2} \sum_k \frac{cut(V_i, \overline{V_i})}{vol(V_i)} cut(V_i, \overline{V_i}) = W (V_i, \overline{V_i}) vol(V_{sub}) = \sum_{i \in V_{sub}} d_i 计算步骤对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，将其划分为$K$类$(C_1, …, C_K)$ 根据输入的相似矩阵的生成方式构建样本的相似矩阵$S_{M×M}$； 根据相似矩阵$S$构建邻接矩阵$W_{M×M}$； 构建度矩阵$D_{M×M}$； 计算拉普拉斯矩阵$L_{M×M}$，可进行规范化$ L := D^{-1}L $； 对$L$进行特征值分解(EVD)，得到特征对$ (\lambda_i, \alpha_i), i=1,…,M $； 指定超参数$K_1$，选取$K_1$个最小特征值对应的特征向量组成矩阵$F_{M×K_1}$，并将其按行标准化； 以$F$的行向量作为新的样本数($k_1$维，这里也有降维操作)进行聚类，划分为$K$类，可使用K-means； 聚类结果即为输出结果 注意是$K_1$个最小特征值对应的特征向量，别问我为什么知道。。。 代码@Github: Spectral Clustering1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class SpectralClustering(): """ Attributes: k: &#123;int&#125;, k &lt; n_samples sigma: &#123;float&#125; Notes: Steps: - similarity matrix [W_&#123;n×n&#125;] - diagonal matrix [D_&#123;n×n&#125;] is defined as D_&#123;ii&#125; = \begin&#123;cases&#125; \sum_j W_&#123;ij&#125; &amp; i \neq j \\ 0 &amp; i = j \end&#123;cases&#125; - Laplacian matrix [L_&#123;n×n&#125;], Laplacian matrix is defined as L = D - W or L = D^&#123;-1&#125; (D - W) - EVD: L \alpha_i = \lambda_i \alpha_i - takes the eigenvector corresponding to the largest eigenvalue as B_&#123;n×k&#125; = [\beta_1, \beta_2, ..., \beta_k] - apply K-Means to the row vectors of matrix B """ def __init__(self, k, n_clusters=2, sigma=1.0): self.kmeans = KMeans(n_clusters=n_clusters) self.k = k self.sigma = sigma def predict(self, X): n_samples = X.shape[0] # step 1 kernelGaussian = lambda z, sigma: np.exp(-0.5 * np.square(z/sigma)) W = np.zeros((n_samples, n_samples)) for i in range(n_samples): for j in range(i): W[i, j] = kernelGaussian(np.linalg.norm(X[i] - X[j]), self.sigma) W[j, i] = W[i, j] # step 2 D = np.diag(np.sum(W, axis=1)) # step 3 L = D - W L = np.linalg.inv(D).dot(L) # step 4 eigval, eigvec = np.linalg.eig(L) # step 5 order = np.argsort(eigval) eigvec = eigvec[:, order] beta = eigvec[:, :self.k] # step 6 self.kmeans.fit(beta) return self.kmeans.labels_ DBSCANDBSCAN(Density-Based Spatial Clustering of Applications with Noise)，具有噪声的基于密度的聚类方法。假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的。 DBSCAN密度聚类算法 - 刘建平Pinard - 博客园 原理先介绍几个关于密度的概念 $\epsilon$-邻域 对于样本$x^{(i)}$，其$\epsilon$-邻域包含样本集中与$x^{(i)}$距离不大于$\epsilon$的子样本集，其样本个数记作$|N_{\epsilon}(x^{(i)})|$。 N_{\epsilon}(x^{(i)}) = \{ x^{(j)} | d_{ij} \leq \epsilon \} 核心对象 对于任一样本$x^{(i)}$，若其$\epsilon$-邻域$N_{\epsilon}(x^{(i)})$至少包含$minPts$个样本，则该样本为核心对象。如图，选择若选取$\epsilon=5$，则红点均为核心对象 密度直达 若样本$x^{(j)} \in N_{\epsilon}(x^{(i)})$，且$x^{(i)}$为核心对象，则称$x^{(j)}$由$x^{(i)}$密度直达。不满足对称性，即反之不一定成立，除非$x^{(j)}$也为核心对象。如图，$x^{(8)}$可由$x^{(6)}$密度直达，而反之$x^{(6)}$不可由$x^{(8)}$密度直达，因为$x^{(8)}$不为核心对象。 密度可达 若存在样本序列$p_1, p_2, …, p_T$，满足$p_1 = x^{(i)}, p_T = x^{(j)}$，且$p_{t+1}$可由$p_t$密度直达，也就是说$p_1, p_2, …, p_{T-1}$均为核心对象，则称$x^{(j)}$由$x^{(i)}$密度可达。也不满足对称性。如图，$x^{(4)}$可由$x^{(1)}$密度可达，而$x^{(2)}$不可由$x^{(4)}$密度可达，因为$x^{(4)}$不为核心对象。 密度相连 存在核心对象$x^{(k)}$，使得$x^{(i)}$与$x^{(j)}$均由$x^{(k)}$密度可达，则称$x^{(i)}$与$x^{(j)}$密度相连。注意密度相连满足对称性。如图，$x^{(8)}$与$x^{(4)}$均可由$x^{(1)}$密度可达，则$x^{(8)}$与$x^{(4)}$密度相连。 计算思想DBSCAN的聚类思想是，由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个簇，这个簇里可能只有一个核心对象，也可能有多个核心对象，若有多个，则簇里的任意一个核心对象的$\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。 另外，考虑以下三个问题 噪音点 一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，这些样本点标记为噪音点，with Noise就是这个意思。 距离的度量 一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KDTree或者球树来快速的搜索最近邻。 类别重复时的判别 某些样本可能到两个核心对象的距离都小于$\epsilon$，但是这两个核心对象如下图所示，不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？ 一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说BDSCAN不是完全稳定的算法。 算法步骤对于给定的$N$维数据集$X = (x^{(1)}, x^{(2)}, …, x^{(M)})$，指定邻域参数$(\epsilon, minPts)$与样本距离度量方式，将其划分为$K$类。 检测数据库中尚未检查过的对象$p$，如果$p$未被处理(归为某个簇或者标记为噪声)，则检查其邻域： 若包含的对象数不小于$minPts$，建立新簇$C$，将其中的所有点加入候选集$N$； 对候选集$N$中所有尚未被处理的对象$q$，检查其邻域： 若至少包含$minPts$个对象，则将这些对象加入$N$； 如果$q$未归入任何一个簇，则将$q$加入$C$； 重复步骤$2$，继续检查$N$中未处理的对象，直到当前候选集$N$为空； 重复步骤$1$-$3$，直到所有对象都归入了某个簇或标记为噪声。 高斯混合模型(GMM)详情查看EM算法 &amp; GMM模型。 层次聚类(Hierarchical Clustering)层次聚类更多的是一种思想，而不是方法，通过从下往上不断合并簇，或者从上往下不断分离簇形成嵌套的簇。例如上面讲到的DBSCAN最后簇的合并就有这种思想。 层次的类通过“树状图”来表示，如下 主要的思想或方法有两种 自底向上的凝聚方法(agglomerative hierarchical clustering) 如AGNES。 自上向下的分裂方法(divisive hierarchical clustering) 如DIANA。 图团体检测(Graph Community Detection)略]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hidden Markov Model]]></title>
    <url>%2F2018%2F11%2F13%2FHidden-Markov-Model%2F</url>
    <content type="text"><![CDATA[前言隐马尔科夫模型(Hidden Markov Model, HMM)可用于标注问题，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。在语音识别、自然语言处理、生物信息等领域有广泛的应用。 基本概念定义定义：隐马尔科夫模型是关于时序的概率模型，描述有一个隐藏的马尔科夫链随机生成不可观测的状态随机序列(state sequence)，再由各个状态生成一个观测，多个时刻的观测组成观测随机序列(observation sequence)的过程。 隐马尔科夫模型由初始概率分布、状态转移概率分布、观测概率分布确定，形式定义如下 状态集合与观测集合：$Q$是所有可能状态(共$N$个)的集合，$V$是所有可能观测(共$M$个)的集合，即 Q = \{q_1, q_2, \cdots, q_N\}, \quad V = \{v_1, v_2, \cdots, v_M\} \tag{1.1} 状态序列与观测序列：共进行$T$次观测，状态序列为$S$，观测序列为$O$，即 S = (s^{(1)}, s^{(2)}, \cdots, s^{(T)}), \quad O = (o^{(1)}, o^{(2)}, \cdots, o^{(T)}) \tag{1.2} 初始状态概率：时刻$t = 1$处于状态$q_i$的概率记作$\pi_i$，即 \pi_i = P(s^{(1)} = q_i) \tag{1.3.1} 保存为初始状态概率向量$\pi$ \pi = \begin{bmatrix} \pi_1 & \pi_2 & \cdots & \pi_N \end{bmatrix}^T \tag{1.3.2} 注意，$\pi$是“状态概率向量”，$q_i$是“状态向量”。 状态转移概率：时刻$t$处于状态$q_i$的条件下，在时刻$t + 1$转移到状态$q_j$的概率为 a_{ij} = P(s^{(t+1)} = q_j | s^{(t)} = q_i) \tag{1.4.1} 保存为状态转移矩阵$A$ A_{N \times N} = \begin{bmatrix} a_{ij} \end{bmatrix} \tag{1.4.2} 观测概率：时刻$t$处于状态$q_i$的条件下，得到观测$v_j$的概率为 b_{ij} = P(o^{(t)} = v_j | s^{(t)} = q_i) \tag{1.5.1} 保存为观测概率矩阵$B$ B_{N \times M} = \begin{bmatrix} b_{ij} \end{bmatrix} \tag{1.5.2} 那么隐马尔科夫模型可由初始状态概率$\pi$、状态转移概率矩阵$A$、观测概率矩阵$B$描述。 基本假设 齐次马尔科夫性假设：在任意时刻$t$的状态只依赖于前一时刻$t-1$的状态，与其他时刻状态或观测无关； P(s^{(t)} | s^{(1)}, o^{(1)}, \cdots, s^{(T)}, o^{(T)}) = P(s^{(t)} | s^{(t-1)}), \quad t = 1, 2, \cdots, T \tag{2.1} 观测独立性假设：在任意时刻$t$的观测只依赖于时刻$t$的状态，与其他时刻状态或观测无关； P(o^{(t)} | s^{(1)}, o^{(1)}, \cdots, s^{(T)}, o^{(T)}) = P(o^{(t)} | s^{(t)}), \quad t = 1, 2, \cdots, T \tag{2.1} 基本问题在给定隐马尔科夫模型$\theta = (A, B, \pi)$下进行$T$次观测的过程如下： 根据初始状态分布$\pi$产生状态$s^{(1)}$； 根据状态$s^{(t - 1)}$与状态转移概率$a_{s^{(t)}}$，产生$s^{(t)}, t = 2, \cdots, T$； 按照状态$s^{(t)}$与观测概率分布$b_{s^{(t)}}$产生观测$o^{(t)}$； 重复2、3直至得到长度为$T$的观测序列。 相应地，马尔可夫模型就产生了三个基本问题： 概率计算问题：给定模型$\theta = (A, B, \pi)$和观测序列$O = (o^{(1)}, o^{(2)}, \cdots, o^{(T)})$，计算该模型下出现该观测序列的概率$P(O | \theta)$； 学习问题：给定观测序列$O = (o^{(1)}, o^{(2)}, \cdots, o^{(T)})$，估计模型参数$\theta = (A, B, \pi)$； 预测问题：给定模型$\theta = (A, B, \pi)$和观测序列$O = (o^{(1)}, o^{(2)}, \cdots, o^{(T)})$，求给定观测序列$O$下，使$P(S | O)$最大的状态序列$S = (s^{(1)}, s^{(2)}, \cdots, s^{(T)})$。 以下对上述三个问题分别进行求解。 概率计算问题直接计算法P(O | \theta) = \sum_S P(O | S, \theta) P(S | \theta)枚举$S = (s^{(1)}, \cdots, s^{(T)})$可能的组合，计算相应的观测，累加得到$P(O|\theta)$，这种方法计算时间复杂度为$O(TN^T)$阶，不可行。 前向-后向算法 前向算法 由概率乘法公式，可得各观测的联合分布满足 \begin{aligned} P(O | \theta) = P(o^{(1)}, \cdots, o^{(T)} | \theta) \\ = P(o^{(1)} | \theta) P(o^{(2)} | o^{(1)}, \theta) P(o^{(3)} | o^{(1)}, o^{(2)}, \theta) \cdots P(o^{(T)} | o^{(1)}, o^{(2)}, \cdots, o^{(T - 1)}, \theta) \\ \end{aligned} 注意到 \begin{cases} P(o^{(1)} | \theta) = \sum_{i = 1}^N P(s^{(1)} = q_i | \theta) P(o^{(1)} | s^{(1)} = q_i, \theta) = \sum_{i = 1}^N \pi_i b_{i, o^{(t)}} \\ P(o^{(2)} | o^{(1)}, \theta) = \sum_{i = 1}^N \underbrace{P(s^{(2)} = q_i | o^{(1)}, \theta)} P(o^{(2)} | s^{(2)} = q_i, o^{(1)}, \theta) \\ \begin{aligned} = \sum_{i = 1}^N \left( \sum_{j=1}^N P(s^{(2)} = q_i | s^{(1)} = q_j, o^{(1)}, \theta ) P(s^{(1)} = q_j | o^{(1)}, \theta ) \right) \times P(o^{(2)} | s^{(2)} \\ = q_i, o^{(1)}, \theta) = \sum_{i=1}^N \left( \sum_{j=1}^N \pi_j a_{ji} \right) b_{i, o^{(2)}} \end{aligned} \cdots \end{cases} 由上式，可发现递归规律。 定义(前向概率)：给定马尔科夫模型$\theta$，定义到时刻$t$时状态为$q_i$，且观测序列为$(o^{(1)}, \cdots, o^{(t)})$的概率为前向概率$\alpha^{(t)}_i$，即 \alpha^{(t)}_i = P(s^{(t)} = q_i, o^{(1)}, \cdots, o^{(t)} | \theta) \tag{3} 那么初始状态下有 \alpha^{(1)}_i = P(s^{(1)} = q_i, o^{(1)} | \theta) = \pi_i b_{i, o^{(1)}} \tag{4.1}, \quad i = 1, 2, \cdots, N 对于$t = 2, 3, T$时刻，由边缘概率公式和条件概率，得到递推公式 \begin{aligned} \alpha^{(t)}_i = P(s^{(t)} = q_i, o^{(1)}, \cdots, o^{(t)} | \theta) \\ = P(s^{(t)} = q_i, o^{(1)}, \cdots, o^{(t-1)} | o^{(t)}, \theta) P(o^{(t)} | s^{(t)} = q_i, \theta) \\ = \left( \sum_{j=1}^N \underbrace{P(s^{(t - 1)} = q_j, o^{(1)}, \cdots, o^{(t - 1)} | o^{(t)}, \theta)}_{\alpha^{(t-1)}_j} \underbrace{P(s^{(t)} = q_i | s^{(t - 1)} = q_j, o^{(t)}, \theta)}_{a_{ji}} \right) \underbrace{P(o^{(t)} | s^{(t)} = q_i, \theta)}_{b_{i, o^{(t)}}} \\ = \left( \sum_{j=1}^N \alpha^{(t-1)}_j a_{ji} \right) b_{i, o^{(t)}} \end{aligned} \tag{4.2} 那么到时刻$T$时，由边缘概率公式，可得到$P(O | \theta)$ \begin{aligned} P(O | \theta) = P(o^{(1)}, \cdots, o^{(T)} | \theta) = \sum_{i=1}^N P(s^{(t)} = q_i, o^{(1)}, \cdots, o^{(t)} | \theta) = \sum_{i=1}^N \alpha^{(T)}_i \tag{4.3} \end{aligned} 后向算法 定义(后向概率)：给定马尔科夫模型$\theta$，定义到时刻$t$时状态为$q_i$的条件下，观测序列为$(o^{(t + 1)}, \cdots, o^{(T)})$的概率为后向概率$\beta^{(t)}_i$，即 \beta^{(t)}_i = P( o^{(t + 1)}, \cdots, o^{(T)} | s^{(t)} = q_i, \theta) \tag{5} 在$T$时刻，有 \beta^{(T)}_i = 1, \quad i = 1, 2, \cdots, N \tag{6.1} 对$t = T - 1, T - 2, \cdots, 1$，有递推公式 \begin{aligned} \beta^{(t)}_i = P( o^{(t + 1)}, \cdots, o^{(T)} | s^{(t)} = q_i, \theta) \\ = \sum_{j=1}^N P(s^{(t + 1)} = q_j, o^{(t + 1)}, \cdots, o^{(T)} | s^{(t)} = q_i, \theta) \\ = \sum_{j=1}^N P(s^{(t + 1)} = q_j | s^{(t)} = q_i, \theta) P(o^{(t + 1)}, \cdots, o^{(T)} | s^{(t + 1)} = q_j, s^{(t)} = q_i, \theta) \\ = \sum_{j=1}^N \underbrace{P(s^{(t + 1)} = q_j | s^{(t)} = q_i, \theta)}_{a_{ij}} \underbrace{P(o^{(t + 1)} | s^{(t + 1)} = q_j, s^{(t)} = q_i, \theta)}_{b_{j, o^{(t + 1)}}} \\ \underbrace{P(o^{(t + 2)}, \cdots, o^{(T)} | s^{(t + 1)} = q_j, s^{(t)} = q_i, o^{(t + 1)}, \theta)}_{\beta^{(t + 1)}_j} \\ = \sum_{j=1}^N a_{ij} b_{j, o^{(t + 1)}} \beta^{(t + 1)}_j \end{aligned} \tag{6.2} 观测序列的概率用下式计算 P(O | \theta) = \sum_{i=1}^N \pi_i b_{i, o^{(1)}} \beta^{(1)}_i \tag{6.3} 前向-后向计算可统一写成 P(O | \theta) = \sum_{i=1}^N \sum_{j=1}^N \alpha^{(t)}_i a_{ij} b_{j, o^{(t + 1)}} \beta^{(t+1)}_j, \quad t = 1, 2, \cdots, T - 1 \tag{7} \begin{cases} \alpha^{(t)}_i = P(s^{(t)} = q_i, o^{(1)}, \cdots, o^{(t)} | \theta) \\ \beta^{(t)}_i = P( o^{(t + 1)}, \cdots, o^{(T)} | s^{(t)} = q_i, \theta) \end{cases} \alpha^{(t)}_i \beta^{(t)}_i = P(s^{(t)} = q_i, O | \theta) 学习问题监督学习方法若训练数据包含若干个长度$T$相同的观测序列和对应的状态序列${(O_1, S_1), (O_2, S_2), \cdots}$，那么可以用极大似然估计($MLE$)进行模型参数的学习。状态序列中时刻$t$为状态$q_i$转移到$t+1$时刻为$q_j$频数记作$A_{ij}$，时刻$t$为状态$q_i$并产生观测$v_j$的频数记作$B_{ij}$，初始状态$t=1$时状态为$q_i$的频数记作$\Pi_i$，那么各参数估计为 \begin{cases} \hat{a}_{ij} = \frac{A_{ij}}{\sum_{j=1}^N A_{ij}} \\ \hat{b}_{ij} = \frac{B_{ij}}{\sum_{j=1}^M B_{ij}} \\ \hat{\pi}_i = \frac{\Pi_i}{\sum_{i=1} \Pi_i} \end{cases} \tag{8} 多项分布(nultinomial distribution) 无监督学习方法若训练数据包含若干个长度$T$相同的观测序列${O_1, O_2, \cdots}$，但无对应状态序列。此时将状态序列数据视作隐变量，可以用$EM$算法求解，即$Baum-Welch$算法。 状态序列$S$与观测序列$O$在模型$\theta$下，联合概率分布为 P(O, S | \theta) = P(S | \theta) P(O | S, \theta) \tag{9.1}其中 \begin{cases} P(S | \theta) = P(s^{(1)} | \theta) P(s^{(2)} | s^{(1)}, \theta) \cdots P(s^{(T)} | s^{(1)}, s^{(2)}, \cdots, s^{(T - 1)}, \theta) = \pi_{s^{(1)}} a_{s^{(1)}s^{(2)}} \cdots a_{s^{(T - 1)}s^{(T)}} \\ P(O | S, \theta) = \sum _{t=1}^T P(o^{(t)} | s^{(t)}, \theta) = b_{s^{(1)}o^{(1)}} \cdots b_{s^{(T)}o^{(T)}}(观测独立性) \end{cases} \tag{9.2}那么 P(O, S | \theta) = \pi_{s^{(1)}} a_{s^{(1)}s^{(2)}} b_{s^{(1)}o^{(1)}} \cdots a_{s^{(T - 1)}s^{(T)}} b_{s^{(T)}o^{(T)}} \tag{9}确定$EM$算法的$E-step$，$Q$函数为 Q(\theta | \overline{\theta}) = \sum_S P(O, S | \overline{\theta}) \log P(O, S | \theta) \tag{10.1}那么$(9)$代入后得到 \begin{aligned} Q(\theta | \overline{\theta}) = \sum_S P(O, S | \overline{\theta}) \log (\pi_{s^{(1)}} a_{s^{(1)}s^{(2)}} b_{s^{(1)}o^{(1)}} \cdots a_{s^{(T - 1)}s^{(T)}} b_{s^{(T)}o^{(T)}}) \\ = \sum_S P(O, S | \overline{\theta}) \log \pi_{s^{(1)}} + \sum_S P(O, S | \overline{\theta}) \log \prod_{t=1}^{T-1} a_{s^{(t)}s^{(t+1)}} + \sum_S P(O, S | \overline{\theta}) \log \prod_{t=1}^{T} b_{s^{(t)}o^{(t)}} \\ = \sum_{i=1}^N P(O, s^{(1)} = q_i | \theta) \log \pi_i + \\ \sum_{i=1}^N \sum_{j=1}^N \sum_{t=1}^{T-1} P(O, s^{(t)} = q_i, s^{(t+1)} = q_j | \overline{\theta}) \log a_{ij} + \\ \sum_{i=1}^N \sum_{t=1}^{T} P(O, s^{(t)} = q_i | \theta) \log b_{i,o^{(t)}} \end{aligned} \tag{10.2}那么相应优化问题为 \begin{aligned} \theta = \arg_{\theta} \max Q(\theta | \overline{\theta}) \\ s.t. \begin{cases} \sum_{i=1}^N \pi_i = 1 \\ \sum_{j=1}^N a_{ij} = 1 \\ \sum_{j=1}^M b_{ij} = 1 \\ \end{cases} \end{aligned} \tag{10.3}并记 \begin{cases} \gamma^{(t)}_i = P(S^{(t)} = q_i | O, \theta) = \frac{P(S^{(t)} = q_i, O | \theta)}{P(O | \theta)} = \frac{\alpha^{(t)}_i \beta^{(t)}_i}{\sum_{j=1}^N \alpha^{(t)}_j \beta^{(t)}_j} \\ \xi^{(t)}_{ij} = P(S^{(t)} = q_i, S^{(t+1)} = q_j | O, \theta) = \frac{P(S^{(t)} = q_i, S^{(t+1)} = q_j, O | \theta)}{P(O | \theta)} = \frac{\alpha^{(t)}_i a_{ij} b_{j,o^{(t+1)}} \beta^{(t+1)}_j}{\sum_{i=1}^N \sum_{j=1}^N \alpha^{(t)}_i a_{ij} b_{j,o^{(t+1)}} \beta^{(t+1)}_j} \end{cases} \tag{10.4}用拉格朗日乘数法求解$(10.3)$，并将$(10.4)$代入，得 \Rightarrow \begin{cases} a_{ij} = \frac{\sum_{t=1}^{T-1} \xi^{(t)}_{ij}}{\sum_{t=1}^{T-1} \gamma^{(t)}_i} \\ b_{ij} = \frac{\sum_{t=1, o^{(t)}=v_j}^T \gamma^{(t)}_i}{\sum_{t=1}^T \gamma^{(t)}_i} \\ \pi_i = \gamma^{(1)}_i \end{cases} \tag{10}迭代求解参数$(A, B, \pi)$得到模型。 预测问题贪婪策略在每个时刻$t$选择在该时刻最有可能出现的状态$s^{(t) }$，从而得到状态序列$S^{ }$作为预测结果 S^{*} = (s^{(1)*}, \cdots, s^{(T)*}) \tag{11.1}其中 s^{(t)*} = \arg \max_{1 \leq i \leq N} \gamma^{(t)}_i \tag{11.2}动态规划用动态规划，在状态概率间求解概率最大路径，即维特比算法(Viterbi algorithm)，选择状态路径点$s^{(t)} \in Q$使得下式最大 \begin{aligned} P(s^{(1)}, \cdots, s^{(t)}, o^{(t)} | \theta) \end{aligned}例假设有$4$个盒子，每个盒子装有红白两色比例一定的球若干，在某盒子取球后，以一定几率停留在该盒子或转移到另一盒子，转移概率未知，进行$10$次取球，在哪个盒子取球未知，球的颜色序列如下 (红，红，白，白，红，白，红，红，白，红) 用已知观测数据学习马尔可夫模型； 用求得得马尔可夫模型计算出现该观测序列的概率$P(O | \theta)$； 求取最优的状态转移过程。 解析：该例中可能的状态集合为$Q = {q_1, q_2, q_3, q_4}$，$q_i$表示在第$i$个盒子进行取球；观测集合为$V = {v_1, v_2}$，表示红、白二色。那么相应的有状态转移概率矩阵$A_{4 \times 4}$、观测概率矩阵$B_{4 \times 2}$、初始状态概率向量$\pi_{4 \times 1}$。由于取球的盒子序列未知，即状态序列未知，用$EM$算法迭代求解。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[EM & GMM]]></title>
    <url>%2F2018%2F11%2F12%2FEM-GMM%2F</url>
    <content type="text"><![CDATA[EM算法Expectation Maximization Algorithm，是 Dempster, Laind, Rubin 于 1977 年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行 MLE 估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据，截尾数据，带有噪声等所谓的不完全数据。 引例：先挖个坑给出李航《统计学习方法》的三硬币模型例子，假设有$3$枚硬币$A, B, C$，各自出现正面的概率分别为$\pi, p, q$，先进行如下实验：先投掷硬币$A$，若结果为正面，则选择硬币$B$投掷一次，否则选择$C$，记录投掷结果如下 1, 1, 0, 1, 0, 0, 1, 0, 1, 1只能观测到实验结果，而投掷过程未知，即硬币$A$的投掷结果未知，现欲估计三枚硬币的参数$\pi, p, q$。 解：根据题意可以得到三个随机变量$X_1, X_2, X_3$的概率分布如下 P(X_1) = \pi ^ {X_1} (1 - \pi) ^ {1 - X_1} P(X_2) = p ^ {X_2} (1 - p) ^ {1 - X_2} P(X_3) = q ^ {X_3} (1 - q) ^ {1 - X_3}定义随机变量$X$表示观测结果为正面，由全概率公式可以得到 P(X) = P(X|X_1)P(X_1) + P(X|\overline{X_1})P(\overline{X_1}) = \pi p + (1 - \pi) q P(\overline{X}) = P(\overline{X}|X_1)P(X_1) + P(\overline{X}|\overline{X_1})P(\overline{X_1}) = \pi (1 - p) + (1 - \pi) (1 - q)即 P(X) = [\pi p + (1 - \pi) q] ^ {X} [\pi (1 - p) + (1 - \pi) (1 - q)] ^ {1 - X} \tag{0}利用最大似然估计，有 \log L(D | \theta) = 6 \log [\pi p + (1 - \pi) q] + 4 \log [\pi (1 - p) + (1 - \pi) (1 - q)]至此，我们一定能想到通过求似然函数极值来求解参数 \frac{∂ }{∂ \pi} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{∂ }{∂ p} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0 \frac{∂ }{∂ q} \log L = 0 \Rightarrow 5 \pi (p - q) + 5q - 3 = 0但是好像出了问题，并不能求解，所以我们引入EM算法迭代求解。 推导以$x^{(i)}$表示训练数据，$w_k$表示类别，设当前迭代参数为$\theta^{(t)}$，则下一次迭代应有 \theta^{(t+1)} = \arg \max \sum_i \log P(x^{(i)}|\theta) \tag{1}记隐变量为$w_k$，那么由边缘概率公式 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \tag{2} $P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)}|x^{(i)}, \theta)$至此已得出引例中的表达式，其中$P(w_k^{(i)}|x^{(i)}, \theta)$与$P(x^{(i)} | w_k^{(i)}, \theta)$均未知，而通过求极值不能解得参数。 我们引入迭代参数$\theta^{(t)}$，即第$t$次迭代时的参数$\theta$，该参数为已知变量 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)} | \theta^{(t)})} {P(w_k^{(i)} | \theta^{(t)})} \tag{3} $P(w_k^{(i)}|\theta^{(t)})$表示样本$x^{(i)}$类别为$w_k^{(i)}$的概率，注意上标。 引入Jensen不等式： For a real convex function $\varphi$, numbers $x_1, …, x_n$ in its domain, and positive weights $a_i$, Jensen’s inequality can be stated as: \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \leq \frac{\sum a_i \varphi(x_i)}{\sum a_i}and the inquality is reversed if $\varphi$ is concave, which is \varphi\left(\frac{\sum a_i x_i}{\sum a_i}\right) \geq \frac{\sum a_i \varphi(x_i)}{\sum a_i}Equality holds if and only if $x_1 = … = x_n$ or $\varphi$ is linear. $\log(·)$为凹函数(concave)，且满足 \sum_k P(w_k^{(i)} | \theta^{(t)}) = 1所以有 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{P(w_k^{(i)}|\theta^{(t)})} {P(w_k^{(i)}|\theta^{(t)})} \geq \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{4}由$Jensen$不等式，当且仅当$ P(x^{(i)}, w_k^{(i)}|\theta)=C $时取等号。 \sum_i \underbrace{\log}_{\varphi} \frac{\sum_k P(x^{(i)}, w_k^{(i)}|\theta) \frac{\underbrace{P(w_k^{(i)}|\theta^{(t)})}_{a}} {P(w_k^{(i)}|\theta^{(t)})}}{\sum_k \underbrace{P(w_k^{(i)} | \theta^{(t)})}_{a}} 此时我们得到似然函数$\sum_i \log P(x^{(i)}|\theta)$的一个下界，但必须保证这个下界是紧的，也就是至少有点能使等号成立。 定义 L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) - P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5}其中第一项即$\log P(X, w|\theta) | X, \theta^{(t)}$的期望 E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)}, w_k^{(i)}|\theta) \tag{5.1}第二项为$P(w | X, \theta^{(t)})$的信息熵 H[P(w | X, \theta^{(t)})] = - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(w_k^{(i)}|\theta^{(t)}) \tag{5.2}即 L(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right] + H[P(w | X, \theta^{(t)})] \tag{E-step}注意到$H[P(w | X, \theta^{(t)})]$项为常数，故也可设 Q(\theta|\theta^{(t)}) = E_w\left[ \log P(X, w|\theta) | X, \theta^{(t)} \right]代回$(1)$，得到最终的优化目标 \theta^{(t+1)} = \arg \max L(\theta|\theta^{(t)}) = \arg \max Q(\theta|\theta^{(t)}) \tag{M-step} 其中$x^{(i)}, w_k^{(i)}$的联合概率密度一般用条件概率计算 P(x^{(i)}, w_k^{(i)}|\theta) = P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta)那么 Q(\theta | \theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)} | \theta^{(t)}) \log \left[ P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta) \right] \tag{*}各部分解释如下 \begin{cases} P(w_k^{(i)} | \theta^{(t)}): \theta^{(t)}参数下，由x^{(i)}求得的隐变量的概率 \\ P(x^{(i)} | w_k^{(i)}, \theta): \theta参数下，隐变量取值为w_k^{(i)}下，x^{(i)}的条件分布概率 \\ P(w_k^{(i)} | \theta): \theta参数下，由x^{(i)}求得的隐变量的分布 \end{cases} 引入迭代变量，并经$Jensen$不等式处理后，各参数可通过迭代求解，需要不断最大化$L(\theta | \theta^{(t)})$来不断优化，这就是$EM$算法，$E-step$是指求出期望，$M-step$是指迭代更新参数 伪代码如下123456According to prior knowledge set $\theta$Repeat until convergence&#123; E-step: The expectation of hidden variables M-step: Finding the maximum of likelihood function&#125; 实际上，从边缘概率与条件概率入手，类似的有 \sum_i \log P(x^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)}, w_k^{(i)}|\theta) = \sum_i \log \sum_k P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta) \geq \sum_i \sum_k P(w_k^{(i)} | \theta) \log P(x^{(i)} | w_k^{(i)}, \theta) \tag{Jensen inequality} = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} \tag{6.1}而由$(4)$，类似的，引入迭代变量可以得到 \sum_i \log P(x^{(i)}|\theta) \geq L(\theta|\theta^{(t)}) \tag{6.2} L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} \tag{5} 则 \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)} | \theta) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta)} - \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(x^{(i)}, w_k^{(i)}|\theta)}{P(w_k^{(i)}|\theta^{(t)})} = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log \frac{P(w_k^{(i)}|\theta^{(t)})}{P(w_k^{(i)}|\theta)} \tag{6.3}而由KL散度( Kullback–Leibler divergence)(又称相对熵(relative entropy))定义 D(P||Q) = \sum P(x) \log \frac{P(x)}{Q(x)} 可知 \sum_i \log P(x^{(i)}|\theta) - L(\theta|\theta^{(t)}) = D\left[ P(w_k^{(i)}|\theta^{(t)}) || P(w_k^{(i)}|\theta) \right] \tag{6.4}即迭代的$P(w_k^{(i)}|\theta^{(t)})$与真实的$P(w_k^{(i)}|\theta)$之间的相对熵！ 引例的求解 Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta) 此题中 \begin{cases} P(w_k|\pi) = \pi^{w_k}(1-\pi)^{1-w_k} \\ P(x | w_1, p) = p^{x^{(i)}}(1-p)^{1-x^{(i)}} \\ P(x | w_2, q) = q^{x^{(i)}}(1-q)^{1-x^{(i)}} \end{cases} \tag{7} $E-step$ \begin{aligned} Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) \times \\ \log P(x^{(i)} | w_k^{(i)}, \pi, p, q) P(w_k^{(i)} | \pi, p, q) \end{aligned} \tag{8} 先求$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$，即第一次投掷结果为$w_k$的概率 P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_k}} {\sum_j \left[\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}\right]^{w_j} \left[(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}\right]^{1-w_j}} \tag{8.1} \frac{A正面， 投掷结果为x^{(i)}}{A正面， 投掷结果为x^{(i)} + A反面， 投掷结果为x^{(i)}} 即 \begin{cases} P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \\ P(w_2^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) = \frac {(1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} {\pi^{(t)}p^{(t)x^{(i)}}(1-p^{(t)})^{1-x^{(i)}} + (1-\pi^{(t)})q^{(t)x^{(i)}}(1-q^{(t)})^{1-x^{(i)}}} \end{cases} 记 \begin{cases} \mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) \\ \mu_2^{(i)} = 1 - \mu_1^{(i)} \end{cases} 注意$w^{(i)}_k$上标^{(i)} 再求$P(x^{(i)}, w_k^{(i)} | \pi, p, q)$，已知 P(x^{(i)}, w_k^{(i)} | \pi, p, q) = P(x^{(i)} | w_k^{(i)}, \pi, p, q) P(w_k^{(i)} | \pi, p, q) $P(x^{(i)} | w_k^{(i)}, \pi, p, q)$为$A$取正/反面下，得到结果$x^{(i)}$的概率；$P(w_k^{(i)} | \pi, p, q)$同$P(w_k^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)})$。 所以 P(x^{(i)}, w_k^{(i)} | \pi, p,q) = \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} \tag{8.2}综上 \begin{aligned} Q(\pi, p, q | \pi^{(t)}, p^{(t)}, q^{(t)}) \\ = \sum_i \sum_{k=1}^2 \mu^{(i)}_k \log \{ \left[\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}\right]^{w_k} \left[(1-\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}\right]^{1-w_k} \} \\ = \sum_i \mu_1^{(i)} \log \pi p^{x^{(i)}}(1-p)^{1-x^{(i)}} + (1 - \mu_1^{(i)}) \log (1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}} \end{aligned} \tag{9} $M-step$ $\frac{∂Q}{∂\pi} = 0$ \begin{aligned} \frac{∂Q}{∂\pi} = \sum_i \mu_1^{(i)} \frac {p^{x^{(i)}}(1-p)^{1-x^{(i)}}} {\pi p^{x^{(i)}}(1-p)^{1-x^{(i)}}} + (1 - \mu_1^{(i)}) \frac {- q^{x^{(i)}}(1-q)^{1-x^{(i)}}} {(1-\pi) q^{x^{(i)}}(1-q)^{1-x^{(i)}}} \\ = \sum_i \frac{\mu_1^{(i)}}{\pi} + \frac{\mu_1^{(i)} - 1}{1 - \pi} \\ = \sum_i \frac{\mu_1^{(i)} - \pi}{\pi(1 - \pi)} = \frac{\sum_i \mu_1^{(i)} - n\pi}{\pi(1 - \pi)} \\ = 0 \\ \Rightarrow \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)} \end{aligned} \tag{10.1} $\frac{∂Q}{∂p} = 0$ \begin{aligned} \frac{∂Q}{∂p} = \sum_i \mu_1^{(i)} \left[ \frac{x^{(i)}}{p} - \frac{1 - x^{(i)}}{1 - p} \right] \\ = \frac{1}{p(1 - p)} \sum_i \mu_1^{(i)} (x^{(i)} - p) \\ = \frac{1}{p(1 - p)} \left[ \sum_i \mu_1^{(i)} x^{(i)} - p \sum_i \mu_1^{(i)} \right] \\ = 0 \\ \Rightarrow p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}} \end{aligned}\tag{10.2} $\frac{∂Q}{∂q} = 0$ \begin{aligned} \frac{∂Q}{∂q} = \sum_i (1 - \mu_1^{(i)}) \left[ \frac{x^{(i)}}{q} - \frac{1 - x^{(i)}}{1 - q} \right] \\ = \frac{1}{q(1 - q)} \sum_i (1 - \mu_1^{(i)}) (x^{(i)} - q) \\ = \frac{1}{q(1 - q)} \left[ \sum_i (1 - \mu_1^{(i)}) x^{(i)} - q \sum_i (1 - \mu_1^{(i)}) \right] \\ = 0 \\ \Rightarrow q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})} \end{aligned} \tag{10.3} 综上 \begin{cases} \mu_1^{(i)} = P(w_1^{(i)}|\pi^{(t)}, p^{(t)}, q^{(t)}) \\ \pi^{(t+1)} = \frac{1}{n} \sum_i \mu_1^{(i)} \\ p^{(t+1)} = \frac{\sum_i \mu_1^{(i)} x^{(i)}}{\sum_i \mu_1^{(i)}} \\ q^{(t+1)} = \frac{\sum_i (1 - \mu_1^{(i)}) x^{(i)}}{\sum_i (1 - \mu_1^{(i)})} \\ \end{cases} \tag{10}多次迭代即可求解，终止条件可设置为 || \theta^{(t+1)} - \theta^{(t)} || < \epsilon \quad 或 \quad ||Q(\theta^{(t+1)} | \theta^{(t)}) - Q(\theta^{(t)} |\theta^{(t)})|| < \epsilonGMM模型Gaussian Mixture Model，是一种无监督学习算法，常用于聚类。当聚类问题中各个类别的尺寸不同、聚类间有相关关系的时候，往往使用GMM更合适。对一个样本来说，GMM得到的是其属于各个类的概率(通过计算后验概率得到)，而不是完全的属于某个类，这种聚类方法被成为软聚类。一般说来， 任意形状的概率分布都可以用多个高斯分布函数去近似，因而，GMM的应用也比较广泛。 高斯混合模型，指具有如下形式的概率分布模型： P(x|\mu, \Sigma) = \sum_{k=1}^K \pi_k N(x|\mu_k, \Sigma_k)其中 $\pi_k(0 \leq \pi_k \leq 1)$是系数，且$\sum_k \pi_k = 1$ $N(x|\mu_k, \Sigma_k)$为高斯密度函数 N(x|\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{n/2}|\Sigma_k|^{1/2}} \exp \left[ -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) \right] 即多个高斯分布叠加出来的玩意； 现在我们需要求取系数$\pi_k$及高斯模型的参数$(\mu_k, \Sigma_k)$； 与K-Means等聚类方法区别是，GMM求出的是连续的分布模型，可计算出“归属于”哪一类的概率。 推导 \log P(X|\pi, \mu, \Sigma) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \Sigma_k) s.t. \sum_k \pi_k = 1暴力求解以$1$维高斯分布为例 N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}}构造拉格朗日(Lagrange)函数 L(\pi, \mu, \sigma^2) = \sum_i \log \sum_k \pi_k N(x|\mu_k, \sigma_k^2) + \lambda \left(\sum_k \pi_k - 1 \right) \tag{5} \begin{cases} \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\mu_k}N(x^{(i)}|\mu_k, \sigma_k^2) \\ \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{∂}{∂\sigma_k^2}N(x^{(i)}|\mu_k, \sigma_k^2) \end{cases} \tag{6}其中 \frac{∂}{∂\mu_k} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{x-\mu_k}{\sigma_k^2} = N(x|\mu_k, \sigma_k^2) · \frac{x-\mu_k}{\sigma_k^2} \frac{∂}{∂\sigma_k^2} N(x|\mu_k, \sigma_k^2) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) $\frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k}\right) = - \frac{\sigma_k^{-3}}{2}; \frac{∂}{∂\sigma_k^2} \left(\frac{1}{\sigma_k^2}\right) = - \frac{1}{\sigma_k^4}$ = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(- \frac{\sigma_k^{-3}}{2}\right) + \frac{1}{\sqrt{2\pi}\sigma_k} e^{-\frac{(x - \mu_k)^2}{2\sigma_k^2}} \left(-\frac{(x - \mu_k)^2}{2}\right) \left(- \frac{1}{\sigma_k^4}\right) = N(x|\mu_k, \sigma_k^2) \left[ \frac{(x - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2}代回$(6)$可以得到 \begin{cases} \frac{∂}{∂\pi_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} + \lambda \\ \frac{∂}{∂\mu_k} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \frac{x^{(i)}-\mu_k}{\sigma_k^2} \\ \frac{∂}{∂\sigma_k^2} L(\pi, \mu, \sigma^2) = \sum_i \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] \frac{1}{2 \sigma_k^2} \end{cases} \tag{7}令 \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \sigma_k^2)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \sigma_j^2)} \tag{8} 通俗理解：$\gamma^{(i)}_k$表示样本$x^{(i)}$中来自类别$w_k$的“贡献百分比” 令$\frac{∂}{∂\mu_k} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到 \sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) = 0 \Rightarrow \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} 令$\frac{∂}{∂\sigma_k^2} \log P(X|\pi, \mu, \sigma^2) = 0$，整理得到 \sum_i \gamma^{(i)}_k \left[ \frac{(x^{(i)} - \mu_k)^2}{\sigma_k^2} - 1 \right] = 0 \Rightarrow \sigma_k^2 = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k)^2}{\sum_i \gamma^{(i)}_k} 对于$\frac{∂}{∂\pi_k} \log P(X|\pi, \mu, \sigma^2) = 0$，需要做一点处理 两边同乘$\pi_k$，得到 \sum_i \gamma^{(i)}_k = - \lambda \pi_k \tag{9} 然后两边对$k$作累加 \sum_k \sum_i \gamma^{(i)}_k = - \lambda \sum_k \pi_k $\sum_k \sum_i \gamma^{(i)}_k = \sum_i \sum_k \gamma^{(i)}_k = N, \sum_k \pi_k = 1$ N = - \lambda 或 \lambda = -N \tag{10} 代回$(9)$，得到 \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N} 综上，我们得到$4$个用于迭代的计算式，将其推广至多维即 \begin{cases} \gamma^{(i)}_k = \frac{\pi_k N(x^{(i)}|\mu_k, \Sigma_k)}{\sum_j \pi_j N(x^{(i)}|\mu_j, \Sigma_j)} \\ \mu_k = \frac{\sum_i \gamma^{(i)}_k x^{(i)}}{\sum_i \gamma^{(i)}_k} \\ \Sigma_k = \frac{\sum_i \gamma^{(i)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)}_k} \\ \pi_k = \frac{\sum_i \gamma^{(i)}_k}{N} \end{cases}EM算法求解 Q(\theta|\theta^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\theta^{(t)}) \log P(x^{(i)} | w_k^{(i)}, \theta) P(w_k^{(i)} | \theta) Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) \log P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k) P(w_k^{(i)} | \mu_k, \Sigma_k) $ M-step $ \begin{cases} P(w_k^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)}) = \frac{\pi_k^{(t)} N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})} {\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})} = \gamma^{(i)(t)}_k \\ P(x^{(i)} | w_k^{(i)}, \mu_k, \Sigma_k) P(w_k^{(i)}|\mu_k, \Sigma_k) = \pi_k N(x^{(i)}|\mu_k, \Sigma_k) \end{cases} 故 Q(\mu_k, \Sigma_k|\mu_k^{(t)}, \Sigma_k^{(t)}) = \sum_i \sum_k \gamma^{(i)(t)}_k \log \pi_k N(x^{(i)}|\mu_k, \Sigma_k) 通过求解极值可得 \begin{cases} \gamma^{(i)(t)}_k = \frac{\pi^{(t)}_k N(x^{(i)}|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_j \pi_j^{(t)} N(x^{(i)}|\mu_j^{(t)}, \Sigma_j^{(t)})} \\ \mu_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k x^{(i)}}{\sum_i \gamma^{(i)(t)}_k} \\ \Sigma_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k (x^{(i)} - \mu_k) (x^{(i)} - \mu_k)^T}{\sum_i \gamma^{(i)(t)}_k} \\ \pi_k^{(t+1)} = \frac{\sum_i \gamma^{(i)(t)}_k}{N} \end{cases} 伪代码为 123456789101112According to prior knowledge set \pi^&#123;(t)&#125;(n_clusters,) \mu^&#123;(t)&#125;(n_clusters, n_features) \Sigma^&#123;(t)&#125;(n_clusters, n_features, n_features)Repeat until convergence&#123; # E-step: calculate \gamma^&#123;(t)&#125; \gamma(n_samples, n_clusters) # M-step: update \pi, \mu, \Sigma \pi^&#123;(t+1)&#125;(n_clusters,) \mu^&#123;(t+1)&#125;(n_clusters, n_features) \Sigma^&#123;(t+1)&#125;(n_clusters, n_features, n_features)&#125; 初始点的选择可以随机选择，也可使用K-Means GMM算法收敛过程如下 代码@Github: GMM123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class GMM(): """ Gaussian Mixture Model Attributes: n_clusters &#123;int&#125; prior &#123;ndarray(n_clusters,)&#125; mu &#123;ndarray(n_clusters, n_features)&#125; sigma &#123;ndarray(n_clusters, n_features, n_features)&#125; """ def __init__(self, n_clusters): self.n_clusters = n_clusters self.prior = None self.mu = None self.sigma = None def fit(self, X, delta=0.01): """ Args: X &#123;ndarray(n_samples, n_features)&#125; delta &#123;float&#125; Notes: - Initialize with k-means """ (n_samples, n_features) = X.shape # initialize with k-means clf = KMeans(n_clusters=self.n_clusters) clf.fit(X) self.mu = clf.cluster_centers_ self.prior = np.zeros(self.n_clusters) self.sigma = np.zeros((self.n_clusters, n_features, n_features)) for k in range(self.n_clusters): X_ = X[clf.labels_==k] self.prior[k] = X_.shape[0] / X_.shape[0] self.sigma[k] = np.cov(X_.T) while True: mu_ = self.mu.copy() # E-step: updata gamma gamma = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): denominator = 0 for j in range(self.n_clusters): post = self.prior[k] *\ multiGaussian(X[i], self.mu[j], self.sigma[j]) denominator += post if j==k: numerator = post gamma[i, k] = numerator/denominator # M-step: updata prior, mu, sigma for k in range(self.n_clusters): sum1 = 0 sum2 = 0 sum3 = 0 for i in range(n_samples): sum1 += gamma[i, k] sum2 += gamma[i, k] * X[i] x_ = np.reshape(X[i] - self.mu[k], (n_features, 1)) sum3 += gamma[i, k] * x_.dot(x_.T) self.prior[k] = sum1 / n_samples self.mu[k] = sum2 / sum1 self.sigma[k] = sum3 / sum1 # to stop mu_delta = 0 for k in range(self.n_clusters): mu_delta += nl.norm(self.mu[k] - mu_[k]) print(mu_delta) if mu_delta &lt; delta: break return self.prior, self.mu, self.sigma def predict_proba(self, X): """ Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples, n_clusters)&#125; """ (n_samples, n_features) = X.shape y_pred_proba = np.zeros((n_samples, self.n_clusters)) for i in range(n_samples): for k in range(self.n_clusters): y_pred_proba[i, k] = self.prior[k] *\ multiGaussian(X[i], self.mu[k], self.sigma[k]) return y_pred_proba def predict(self, X): """ Args: X &#123;ndarray(n_samples, n_features)&#125; Returns: y_pred_proba &#123;ndarray(n_samples,)&#125; """ y_pred_proba = self.predict_proba(X) return np.argmax(y_pred_proba, axis=1)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data Augmentation]]></title>
    <url>%2F2018%2F11%2F02%2FData-Augmentation%2F</url>
    <content type="text"><![CDATA[“有时候不是由于算法好赢了。而是由于拥有很多其它的数据才赢了。” 数据集扩增在深度学习中,很多训练数据意味着能够用更深的网络，训练出更好的模型。既然这样，收集很多其它的数据不即可啦？假设能够收集很多其它能够用的数据当然好，比如ImageNet上图像数据量已达到$1400$万张，可是非常多时候，收集很多其它的数据意味着须要耗费很多其它的人力物力，这就需要使用一定的方法扩增数据集。 图像扩增大部分借助OpenCV库，这里推荐一位学长的博客，整理了大量的OpenCV使用方法. Ex2tron’s Blog TensorFlow也提供相应图像处理方法Module: tf.image | TensorFlow 需要注意的是，扩增过程中，需注意图像数据类型，可以将数据归一化到$(0, 1)$间再进行处理 翻转12345678def flip(image): """ Parameters: image &#123;ndarray(H, W, C)&#125; """ rand_var = np.random.random() image = image[:, ::-1, :] if rand_var &gt; 0.5 else image return image 旋转123456789101112def rotate(image, degree): """ Parameters: image &#123;ndarray(H, W, C)&#125; degree &#123;float&#125; """ (h, w) = image.shape[:2] center = (w // 2, h // 2) random_angel = np.random.randint(-degree, degree) M = cv2.getRotationMatrix2D(center, random_angel, 1.0) image = cv2.warpAffine(image, M, (w, h)) return image 噪声可手动实现，如椒盐噪声代码如下12345678910111213141516def saltnoise(image, salt=0.0): """ add salt &amp; pepper and gaussian noise Parameters: image &#123;ndarray(H, W, C)&#125; salt &#123;float(0, 1)&#125; number of salt pixel = salt*h*w Notes: TODO: gaussain noise """ (h, w) = image.shape[:2] n_salt = int(salt * h * w) for n in range(n_salt): hr = np.random.randint(0, h) wr = np.random.randint(0, w) issalt = (np.random.rand(1) &gt; 0.5) image[hr, wr] = 255 if issalt else 0 return image 也可调用scikit-image库，需要注意的是，skimage.util.random_noise()会将原图数据转换为$(0, 1)$间的浮点数1234567891011121314151617def noise(image, gaussian, salt, seed=None): """ add noise to image TODO Parameters: image &#123;ndarray(H, W, C)&#125; gaussian &#123;bool&#125;: salt &#123;bool&#125;: Notes: Function to add random noise of various types to a floating-point image. """ dtype = image.dtype if gaussian: image = skimage.util.random_noise(image, mode='gaussian', seed=seed) if salt: image = skimage.util.random_noise(image, mode='s&amp;p', seed=seed) image = (image * 255).astype(dtype) return image 亮度与对比度调整考虑到数据溢出，先转换为整形数据，再限制其值到$[0, 255]$ 注意数据类型 1234567891011def brightcontrast(image, brtadj=0, cstadj=1.0): """ adjust bright and contrast value Parameters: image &#123;ndarray(H, W, C)&#125; brtadj &#123;int&#125; if true, adjust bright cstadj &#123;float&#125; if true, adjust contrast """ dtype = image.dtype image = image.astype('int')*cstadj + brtadj image = np.clip(image, 0, 255).astype(dtype) return image 投射变换1234567891011121314151617181920212223242526def perspective(image, prop): """ 透射变换 Parameters: image &#123;ndarray(H, W, C)&#125; prop &#123;float&#125;: 在四个顶点多大的方格内选取新顶点，方格大小为(H*prop, W*prop) Notes: 在四个顶点周围随机选取新的点进行仿射变换，四个点对应左上、右上、左下、右下 """ (h, w) = image.shape[:2] ptsrc = np.zeros(shape=(4, 2)) ptdst = np.array([[0, 0], [0, w], [h, 0], [h, w]]) for i in range(4): hr = np.random.randint(0, int(h*prop)) wr = np.random.randint(0, int(w*prop)) if i == 0: ptsrc[i] = np.array([hr, wr]) elif i == 1: ptsrc[i] = np.array([hr, w - wr]) elif i == 2: ptsrc[i] = np.array([h - hr, wr]) elif i == 3: ptsrc[i] = np.array([h - hr, w - wr]) M = cv2.getPerspectiveTransform(ptsrc.astype('float32'), ptdst.astype('float32')) image = cv2.warpPerspective(image, M, (w, h)) return image]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[二次入坑raspberry-pi]]></title>
    <url>%2F2018%2F10%2F29%2F%E4%BA%8C%E6%AC%A1%E5%85%A5%E5%9D%91raspberry-pi%2F</url>
    <content type="text"><![CDATA[前言距上一次搭建树莓派平台已经两年了，保存的镜像出了问题，重新搭建一下。 系统下载从官网下载树莓派系统镜像，有以下几种可选 Raspberry Pi — Teach, Learn, and Make with Raspberry Pi Raspbian &amp; Raspbian Lite，基于Debian Noobs &amp; Noobs Lite Ubuntu MATE Snappy Ubuntu Core Windows 10 IOT 其余不太了解，之前安装的是Raspbian，对于Debian各种不适，换上界面优雅的Ubuntu Mate玩一下老老实实玩Raspbian，笑脸:-) 安装比较简单，准备micro-SD卡，用Win32 Disk Imager烧写镜像 Win32 Disk Imager download | SourceForge.net 安装完软件后可点击Read备份自己的镜像。 注意第二次开机前需要配置config.txt文件，否则hdmi无法显示 树莓派配置文档 config.txt 说明 | 树莓派实验室 123456disable_overscan&#x3D;1 hdmi_force_hotplug&#x3D;1hdmi_group&#x3D;2 # DMThdmi_mode&#x3D;32 # 1280x960hdmi_drive&#x3D;2config_hdmi_boost&#x3D;4 修改交换分区Ubuntu Mate查看交换分区1$ free -m 未设置时如下1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 0 0 0 创建和挂载12345678910111213141516# 获取权限$ sudo -i# 创建目录$ mkdir &#x2F;swap$ cd &#x2F;swap# 指定一个大小为1G的名为“swap”的交换文件$ dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;swap bs&#x3D;1M count&#x3D;1k# 创建交换文件$ mkswap swap# 挂载交换分区$ swapon swap# 卸载交换分区# $ swapoff swap 查看交换分区1$ free -m 未设置时如下1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 RaspbianWe will change the configuration in the file /etc/dphys-swapfile:1$ sudo nano &#x2F;etc&#x2F;dphys-swapfile The default value in Raspbian is:1CONF_SWAPSIZE&#x3D;100 We will need to change this to:1CONF_SWAPSIZE&#x3D;1024 Then you will need to stop and start the service that manages the swapfile own Rasbian:12$ sudo &#x2F;etc&#x2F;init.d&#x2F;dphys-swapfile stop$ sudo &#x2F;etc&#x2F;init.d&#x2F;dphys-swapfile start You can then verify the amount of memory + swap by issuing the following command:1$ free -m The output should look like:1234total used free shared buffers cachedMem: 435 56 379 0 3 16-/+ buffers/cache: 35 399Swap: 1023 0 1023 软件安装指令 apt-get 安装软件apt-get install softname1 softname2 softname3 ... 卸载软件apt-get remove softname1 softname2 softname3 ... 卸载并清除配置apt-get remove --purge softname1 更新软件信息数据库apt-get update 进行系统升级apt-get upgrade 搜索软件包apt-cache search softname1 softname2 softname3 ... 修正（依赖关系）安装：apt-get -f insta dpkg 安装.deb软件包dpkg -i xxx.deb 删除软件包dpkg -r xxx.deb 连同配置文件一起删除dpkg -r --purge xxx.deb 查看软件包信息dpkg -info xxx.deb 查看文件拷贝详情dpkg -L xxx.deb 查看系统中已安装软件包信息dpkg -l 重新配置软件包dpkg-reconfigure xx 卸载软件包及其配置文件，但无法解决依赖关系！sudo dpkg -p package_name 卸载软件包及其配置文件与依赖关系包sudo aptitude purge pkgname 清除所有已删除包的残馀配置文件dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P 软件源 备份原始文件 1$ sudo cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.backup 修改文件并添加国内源 1$ vi &#x2F;etc&#x2F;apt&#x2F;sources.list 注释元文件内的源并添加如下地址 123456789101112131415161718192021#Mirror.lupaworld.com 源更新服务器（浙江省杭州市双线服务器，网通同电信都可以用，亚洲地区官方更新服务器）：deb http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy main restricted universe multiversedeb http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy-security main restricted universe multiversedeb http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy-updates main restricted universe multiversedeb http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy-backports main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy-security main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy-updates main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirror.lupaworld.com&#x2F;ubuntu gutsy-backports main restricted universe multiverse#Ubuntu 官方源 deb http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy main restricted universe multiversedeb http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-security main restricted universe multiversedeb http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-updates main restricted universe multiversedeb http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-proposed main restricted universe multiversedeb http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-backports main restricted universe multiversedeb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy main restricted universe multiversedeb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-security main restricted universe multiversedeb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-updates main restricted universe multiversedeb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-proposed main restricted universe multiversedeb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu&#x2F; gutsy-backports main restricted universe multiverse 或者 1234567891011121314151617181920212223#阿里云deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiverse#网易163deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiverse 放置非官方源的包不完整，可在为不添加官方源 1deb http:&#x2F;&#x2F;archive.ubuntu.org.cn&#x2F;ubuntu-cn&#x2F; feisty main restricted universe multiverse 更新源 1$ sudo apt-get update 更新软件 1$ sudo apt-get dist-upgrade 常见的修复安装命令 1$ sudo apt-get -f install Python主要是Python和相关依赖包的安装，使用以下指令可导出已安装的依赖包1$ pip freeze &gt; requirements.txt 并使用指令安装到树莓派1$ pip install -r requirements.txt 注意pip更新1python -m pip install --upgrade pip 最新版本会报错1ImportError: cannot import name main 修改文件/usr/bin/pip123from pip import mainif __name__ == '__main__': sys.exit(main()) 改为123from pip import __main__if __name__ == '__main__': sys.exit(__main__._main()) 成功!!!失败了，笑脸:-)，手动安装吧。。。 部分包可使用pip3 123$ pip3 install numpy$ pip3 install pandas$ pip3 install sklearn 若需要权限，加入--user 部分包用apt-get，但是优先安装到Python2.7版本，笑脸:-) 123$ sudo apt-get install python-scipy$ sudo apt-get install python-matplotlib$ sudo apt-get install python-opencv 部分从PIPY下载.whl或.tar.gz文件 PyPI – the Python Package Index · PyPI tensorboardX-1.4-py2.py3-none-any.whl visdom-0.1.8.5.tar.gz 安装指令为 1$ pip3 install xxx.whl 12$ tar -zxvf xxx.tar.gz$ python setup.py install Pytorch源码安装 pytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration 安装方法Installation - From Source 需要用到miniconda，安装方法如下，注意中间回车按慢一点，有两次输入。。。。。(行我慢慢看条款不行么。。笑脸:-)) 第一次是是否同意条款，yes 第二次是添加到环境变量，yes，否则自己修改/home/pi/.bashrc添加到环境变量 1234567891011$ wget http:&#x2F;&#x2F;repo.continuum.io&#x2F;miniconda&#x2F;Miniconda3-latest-Linux-armv7l.sh$ sudo md5sum Miniconda3-latest-Linux-armv7l.sh # (optional) check md5$ sudo &#x2F;bin&#x2F;bash Miniconda3-latest-Linux-armv7l.sh # -&gt; change default directory to &#x2F;home&#x2F;pi&#x2F;miniconda3$ sudo nano &#x2F;home&#x2F;pi&#x2F;.bashrc # -&gt; add: export PATH&#x3D;&quot;&#x2F;home&#x2F;pi&#x2F;miniconda3&#x2F;bin:$PATH&quot;$ sudo reboot -h now$ conda $ python --version$ sudo chown -R pi miniconda3 然后就可以安装了没有对应版本的mkl，笑脸:-) 12345678910111213export CMAKE_PREFIX_PATH&#x3D;&quot;$(dirname $(which conda))&#x2F;..&#x2F;&quot; # [anaconda root directory]# Disable CUDAexport NO_CUDA&#x3D;1# Install basic dependenciesconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typingconda install -c mingfeima mkldnn# Install Pytorchgit clone --recursive https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorchcd pytorchpython setup.py install tensorflow 安装tensorflow需要的一些依赖和工具 1234567$ sudo apt-get update# For Python 2.7$ sudo apt-get install python-pip python-dev# For Python 3.3+$ sudo apt-get install python3-pip python3-dev 安装tensorflow 若下载失败，手动打开下面网页下载.whl包 1234567# For Python 2.7$ wget https:&#x2F;&#x2F;github.com&#x2F;samjabrahams&#x2F;tensorflow-on-raspberry-pi&#x2F;releases&#x2F;download&#x2F;v1.1.0&#x2F;tensorflow-1.1.0-cp27-none-linux_armv7l.whl$ sudo pip install tensorflow-1.1.0-cp27-none-linux_armv7l.whl# For Python 3.4$ wget https:&#x2F;&#x2F;github.com&#x2F;samjabrahams&#x2F;tensorflow-on-raspberry-pi&#x2F;releases&#x2F;download&#x2F;v1.1.0&#x2F;tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl$ sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl 卸载，重装mock 1234567# For Python 2.7$ sudo pip uninstall mock$ sudo pip install mock# For Python 3.3+$ sudo pip3 uninstall mock$ sudo pip3 install mock 安装的版本tensorflow v1.1.0没有models，因为1.0版本以后models就被Sam Abrahams独立出来了，例如classify_image.py就在models/tutorials/image/imagenet/里 tensorflow/models 其余 输入法 12$ sudo apt-get install fcitx fcitx-googlepinyin $ fcitx-module-cloudpinyin fcitx-sunpinyin git 1$ sudo apt-get install git 配置git和ssh 12345$ git config --global user.name &quot;Louis Hsu&quot;$ git config --global user.email is.louishsu@foxmail.com$ ssh-keygen -t rsa -C &quot;is.louishsu@foxmail.com&quot;$ cat ~&#x2F;.ssh&#x2F;id_rsa.pub # 添加到github]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Underfitting & Overfitting]]></title>
    <url>%2F2018%2F10%2F26%2FUnderfitting-Overfitting%2F</url>
    <content type="text"><![CDATA[原因分析放上一张非常经典的图，以下分别表示二分类模型中的欠拟合(underfit)、恰好(just right)、过拟合(overfit)，来自吴恩达课程笔记。 欠拟合的成因大多是模型不够复杂、拟合函数的能力不够； 过拟合成因是给定的数据集相对过于简单，使得模型在拟合函数时过分地考虑了噪声等不必要的数据间的关联，或者说相对于给定数据集，模型过于复杂、拟合能力过强。 判别方法学习曲线可通过学习曲线(Learning curve)进行欠拟合与过拟合的判别。 学习曲线就是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。 绘制横轴为训练样本的数量，纵轴为损失或其他评估准则。sklearn中学习曲线绘制例程如下1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltfrom sklearn.naive_bayes import GaussianNBfrom sklearn.datasets import load_digitsfrom sklearn.model_selection import learning_curvefrom sklearn.model_selection import ShuffleSplitdigits = load_digits(); X, y = digits.data, digits.targetcv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)estimator = GaussianNB()train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=cv, n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5))plt.figure()plt.title("Learning Curves (Naive Bayes)")plt.xlabel("Training examples")plt.ylabel("Score")train_scores_mean = np.mean(train_scores, axis=1)train_scores_std = np.std(train_scores, axis=1)test_scores_mean = np.mean(test_scores, axis=1)test_scores_std = np.std(test_scores, axis=1)plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")plt.grid(); plt.legend(loc="best")plt.show() 判别 欠拟合，即高偏差(high bias)，训练集和测试集的误差收敛但却很高； 过拟合，即高方差(high variance)，训练集和测试集的误差之间有大的差距。 欠拟合解决方法 增加迭代次数继续训练 增加模型复杂度 增加特征 减少正则化程度 采用Boosting等集成方法 此时增加数据集并不能改善欠拟合问题。 过拟合解决方法 提前停止训练 获取更多样本或数据扩增 重采样 上采样 增加随机噪声 GAN 图像数据的空间变换（平移旋转镜像） 尺度变换（缩放裁剪） 颜色变换 改变分辨率 对比度 亮度 降低模型复杂度 减少特征 增加正则化程度 神经网络可采用Dropout 多模型投票方法]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Cross Validation & Hyperparameter]]></title>
    <url>%2F2018%2F10%2F26%2FCross-Validation-Hyperparameter%2F</url>
    <content type="text"><![CDATA[交叉验证与超参数选择交叉验证以下简称交叉验证(Cross Validation)为CV.CV是用来验证分类器的性能一种统计分析方法,基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set),首先用训练集对分类器进行训练,在利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。 交叉验证的几种方法 k折交叉验证(K-fold) 将全部训练集$S$分成$k$个不相交的子集，假设$S$中的训练样例个数为$m$，则每个子集中有$(\frac{m}{k})$个训练样例，相应子集称作${s_1, s_2, …, s_k}$； 每次从分好的子集中，拿出$1$个作为测试集，其他$k-1$个作为训练集； 在$k-1$个训练集上训练出学习器模型，将模型放到测试集上，得到分类率； 计算k次求得的分类率平均值，作为该模型或者假设函数的真实分类率 留一法交叉验证(Leave One Out - LOO) 假设有$N$个样本，将每个样本作为测试样本，其他$(N-1)$个样本作为训练样本。这样得到$N$个分类器，$N$个测试结果。用这$N$个结果的平均值衡量模型的性能。 留P法交叉验证(Leave P Out - LPO) 将$P$个样本作为测试样本，其他$(N-P)$个样本作为训练样本。这样得到$\left(\begin{matrix} P \\ N \end{matrix}\right)$个训练测试对。当$P＞1$时，测试集会发生重叠。当$P=1$时，变成$LOO$。 scikit-learn中的交叉验证 K-fold 12345678&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; from sklearn.model_selection import KFold&gt;&gt;&gt; X = ["a", "b", "c", "d"]&gt;&gt;&gt; kf = KFold(n_splits=2)&gt;&gt;&gt; for train, test in kf.split(X):... print("%s %s" % (train, test))[2 3] [0 1][0 1] [2 3] Leave One Out (LOO) 123456789&gt;&gt;&gt; from sklearn.model_selection import LeaveOneOut&gt;&gt;&gt; X = [1, 2, 3, 4]&gt;&gt;&gt; loo = LeaveOneOut()&gt;&gt;&gt; for train, test in loo.split(X):... print("%s %s" % (train, test))[1 2 3] [0][0 2 3] [1][0 1 3] [2][0 1 2] [3] Leave P Out (LPO) 1234567891011&gt;&gt;&gt; from sklearn.model_selection import LeavePOut&gt;&gt;&gt; X = np.ones(4)&gt;&gt;&gt; lpo = LeavePOut(p=2)&gt;&gt;&gt; for train, test in lpo.split(X):... print("%s %s" % (train, test))[2 3] [0 1][1 3] [0 2][1 2] [0 3][0 3] [1 2][0 2] [1 3][0 1] [2 3] 使用交叉验证调整超参数超参数：在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。超参数例如 模型(SVM，Softmax，Multi-layer Neural Network,…)； 迭代算法(Adam, SGD, …)(不同的迭代算法还有各种不同的超参数，如beta1,beta2等等，但常见的做法是使用默认值，不进行调参)； 学习率(learning rate)； 正则化方程的选择(L0,L1,L2)，正则化系数； dropout的概率 … 确定调节范围超参数的种类多，调节范围大，需要先进行简单的测试确定调参范围。 模型选择 模型的选择很大程度上取决于具体的实际问题，但必须通过几项基本测试。 可以通过第一个epoch的loss，观察模型能否无BUG运行，注意此过程需要设置正则项系数为0，因为正则项引入的loss难以估算。 模型必须可以对于小数据集过拟合，否则应该尝试其他或者更复杂的模型。 若训练集与验证集loss均较大，则应该尝试其他或者更复杂的模型。 模型选择的方法为： 使用训练集训练出 10 个模型 用 10 个模型分别对交叉验证集计算得出交叉验证误差(代价函数的值) 选取代价函数值最小的模型 用步骤 3 中选出的模型对测试集计算得出推广误差(代价函数的值) —— Andrew Ng, Stanford University 学习率 loss基本不变：学习率过低 loss波动明显或者溢出：学习率过高 正则项系数 val_acc与acc相差较大：正则项系数过小 loss逐渐增大：正则项系数过大 超参数的确定 先粗调，再细调 先通过数量少，间距大的粗调确定细调的大致范围。然后在小范围内部进行间距小，数量大的细调。 尝试在对数空间内进行调节 即在对数空间内部随机生成测试参数，而不是在原空间生成，通常用于学习率以及正则项系数等的调节。出发点是该超参数的指数项对于模型的结果影响更显著；而同阶的数据之间即便原域相差较大，对于模型结果的影响反而不如不同阶的数据差距大。 超参数搜索 随机搜索参数值，而不是网格搜索。 超参数搜索scikit-learn提供超参数搜索方法，可参考官方文档 网格搜索 3.2.1. Exhaustive Grid Search 调用例程如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "min_samples_leaf": [1, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) 随机搜索 3.2.2. Randomized Parameter Optimization 调用例程如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "min_samples_leaf": sp_randint(1, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spam Classification]]></title>
    <url>%2F2018%2F10%2F26%2FSpam-Classification%2F</url>
    <content type="text"><![CDATA[踩坑？？？全部给我踩平！！！ 来自LintCode垃圾短信分类@Github: spam or ham 垒代码预处理及向量化观察各文本后，发现各文本中包含的单词多种多样，包含标点、数字等，例如123- Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...- XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here&gt;&gt; http:&#x2F;&#x2F;wap. - 07732584351 - Rodger Burns - MSG &#x3D; We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. 且按空格分词后，部分单词中仍包含whitespace，故选择的预处理方案是，去除分词后文本中的标点、数字、空格等，并将单词中字母全部转为小写。 中文分词可采用jieba(街霸？) 预处理后，按当前的文本内容建立字典，并统计各样本的词数向量，详细代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243class Words2Vector(): ''' 建立字典，将输入的词列表转换为向量，表示各词出现的次数 ''' def __init__(self): self.dict = None self.n_word = None def fit_transform(self, words): self.fit(words) return self.transform(words) def fit(self, words): """ @param &#123;list[list[str]]&#125; words """ words = _flatten(words) # 展开为1维列表 words = self.filt(words) # 滤除空格、数字、标点 self.word = list(set(words)) # 去重 self.n_word = len(set(words)) # 统计词的个数 self.dict = dict(zip(self.word, [_ for _ in range(self.n_word)])) # 各词在字典中的位置 def transform(self, words): """ @param &#123;list[list[str]]&#125; words @return &#123;ndarray&#125; retarray: vector """ retarray = np.zeros(shape=(len(words), self.n_word)) # 返回的词数向量 for i in range(len(words)): words[i] = self.filt(words[i]) # 滤除空格、数字、标点 for i in range(len(words)): for w in words[i]: if w in self.word: # 是否在训练集生成的字典中 retarray[i, self.dict[w]] += 1 # 查询字典，找到对应特征的下标 return retarray def filt(self, flattenWords): retWords = [] en_stops = set(stopwords.words('english')) # 停用词列表 for word in flattenWords: word = word.translate(str.maketrans('', '', string.whitespace)) # 去除空白 word = word.translate(str.maketrans('', '', string.punctuation)) # 去除标点 word = word.translate(str.maketrans('', '', string.digits)) # 去除数字 if word not in en_stops and (len(word) &gt; 1): # 删除停用词，并除去长度小于等于2的词 retWords.append(word.lower()) return retWords TF-IDF方法由词数向量可计算词频，但只用词频忽略了各文本在不同文档中的重要程度，关于TF-IDF，在另一篇博文中详细说明。 由于剔除了停用词等，部分向量不包含任何内容，即词数向量为$\vec{0}$，这时计算词频和单位化时，会出现nan的运算结果，故只对非空向量进行计算。 训练后需要保存的是IDF向量，TF向量在新样本输入后重新计算，故无需保存。 12345678910111213141516171819202122232425262728293031class TfidfVectorizer(): def __init__(self): self.idf = None def fit_transform(self, num_vec): self.fit(num_vec) return self.transform(num_vec) def fit(self, num_vec): """ @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature) """ num_vec[num_vec&gt;0] = 1 n_doc = num_vec.shape[0] n_term = np.sum(num_vec, axis=0) # 各词出现过的文档次数 self.idf = np.log((n_doc + 1) / (n_term + 1)) + 1 return self.idf def transform(self, num_vec): """ @param &#123;ndarray&#125;: num_vec, shape(N_sample, N_feature) """ # 求解词频向量，由于部分向量为空，故下句会出现问题 # tf = num_vec / np.sum(num_vec, axis=1).reshape(-1, 1) =&gt; nan # 解决方法：只对非空向量进行词频计算 tf = np.zeros(shape=num_vec.shape) n_terms = np.sum(num_vec, axis=1); idx = (n_terms!=0) tf[idx] = num_vec[idx] / n_terms[idx].reshape(-1, 1) # 计算词频，只对非空向量进行 tfidf = tf * self.idf tfidf[idx] /= np.linalg.norm(tfidf, axis=1)[idx].reshape(-1, 1) # 单位化，只对非空向量进行 return tfidf 贝叶斯决策各文本向量化后，就可通过机器学习算法进行模型的训练和预测，这里采用的是贝叶斯决策的方法，需要注意的有以下几点 似然函数$p(x|c_k)$与贝叶斯决策文中例不同，这里宜采用高斯分布作为分布模型； 按朴素贝叶斯计算$p(x|c_k)$，但注意此处不能将各维特征单独训练$1$维高斯分布模型，然后计算预测样本似然函数值时进行累乘，如下 p(x|c_k) = \prod_{j=1}^{N_feature} p(x_j|c_k)因为特征维度特别高，各个特征单独用$1$维高斯分布描述，累乘计算会下溢，故这里采用多元高斯分布 p(x|c_k) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}} · e^{-\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)} 且经主成分分析后，各维度间线性相关性降低，故假定 \Sigma_k = diag\{\sigma_{k1}, ..., \sigma_{kn}\} 但分母$(2\pi)^{\frac{n}{2}}|\Sigma_k|^{\frac{1}{2}}$在计算时不稳定，且各特征标准差大小相差无几，故这里假定 \Sigma_k = I 最终简化后的似然函数计算方法为 p(x|c_k) = e^{-\frac{1}{2} (x - \mu_k)^T (x - \mu_k)} 贝叶斯决策模型训练基于上述假设，只需训练多元高斯分布的各维均值$\mu_j$ 1234567891011121314151617181920212223def fit(self, labels, text): """ @param &#123;ndarray&#125; labels: shape(N_samples, ), labels[i] \in &#123;0, 1&#125; @param &#123;list[list[str]]&#125; words """ labels = self.encodeLabel(labels); words = self.text2words(self.clean(text)) vecwords = self.numvectorizer.fit_transform(words) # 向量化 vecwords = self.tfidfvectorizer.fit_transform(vecwords) # tfidf, shape(N_samples, N_features) isnotEmpty = (np.sum(vecwords, axis=1)!=0) # 去掉空的样本 vecwords = vecwords[isnotEmpty]; labels = labels[isnotEmpty] # vecwords = self.reduce_dim.fit_transform(vecwords) # 降维，计算量太大 self.n_features = vecwords.shape[1] labels = OneHotEncoder().fit_transform(labels.reshape((-1, 1))).toarray() self.priori = np.mean(labels, axis=0) # 先验概率 self.likelihood_mu = np.zeros(shape=(2, vecwords.shape[1])) # 设似然函数p(x|c)为高斯分布 for i in range(2): vec = vecwords[labels[:, i]==1] self.likelihood_mu[i] = np.mean(vec, axis=0) 贝叶斯决策模型预测决策函数为 if p(x|c_i)P(c_i) > p(x|c_j)P(c_j), then x \in c_i但实际效果显示，等先验概率$P(c_j)$结果更好$(???)$ 123456789101112131415161718192021222324252627def multigaussian(self, x, mu): """ 简化 """ x = x - mu a = np.exp(-0.5 * x.T.dot(x)) return adef predict(self, text): """ @param &#123;list[list[str]]&#125; words @note: p(x|c)P(c) P(c|x) = ------------ p(x) """ pred_porba = np.ones(shape=(len(self.clean(text)), 2)) words = self.text2words(text) vecwords = self.tfidfvectorizer.transform( self.numvectorizer.transform(words)) # 向量化 for i in range(vecwords.shape[0]): for c in range(2): # pred_porba[i, c] = self.priori[c] * self.multigaussian(vecwords[i], self.likelihood_mu[c]) pred_porba[i, c] = self.multigaussian(vecwords[i], self.likelihood_mu[c]) pred = np.argmax(pred_porba, axis=1) return self.decodeLabel(pred) 调包主要用到了scikit-learn机器学习包以下几个功能 sklearn.feature_extraction.text.TfidfVectorizer() sklearn.decomposition.PCA() sklearn.naive_bayes.BernoulliNB() 最终准确率在$97\%$左右，代码比较简单，不进行说明。 采用sklearn.linear_model import.LogisticRegressionCV()效果更佳 123456789101112131415161718192021222324252627282930def main(): trainfile = "./data/train.csv" testfile = "./data/test.csv" # 读取原始数据 data_train = pd.read_csv(trainfile, names=['Label', 'Text']) txt_train = list(data_train['Text'])[1: ]; label_train = list(data_train['Label'])[1: ] drop(txt_train) # 删除数字和标点 txt_test = list(pd.read_csv(testfile, names=['Text'])['Text'])[1: ] drop(txt_test) # 删除数字和标点 # 训练 vectorizer = TfidfVectorizer(stop_words='english') # 删除英文停用词 vec_train = vectorizer.fit_transform(txt_train).toarray() # 提取文本特征向量 # reduce_dim = PCA(n_components = 4096) # vec_train = reduce_dim.fit_transform(vec_train) estimator = BernoulliNB() estimator.fit(vec_train, label_train) # 训练朴素贝叶斯模型 # 测试 label_train_pred = estimator.predict(vec_train) acc = np.mean((label_train_pred==label_train).astype('float')) # 预测 vec_test = vectorizer.transform(txt_test).toarray() # vec_test = reduce_dim.transform(vec_test) label_test_pred = estimator.predict(vec_test) with open('./data/sampleSubmission.txt', 'w') as f: for i in range(label_test_pred.shape[0]): f.write(label_test_pred[i] + '\n')]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TF-IDF]]></title>
    <url>%2F2018%2F10%2F25%2FTF-IDF%2F</url>
    <content type="text"><![CDATA[引言正在做LintCode上的垃圾邮件分类，使用朴素贝叶斯方法解决，涉及到文本特征的提取。TF-IDF（词频-逆文档频率）算法是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 计算步骤词频(TF)Term Frequency，就是某个关键字出现的频率，具体来讲，就是词库中的某个词在当前文章中出现的频率。那么我们可以写出它的计算公式： TF_{ij} = \frac{n_{ij}}{\sum_k n_{i, k}}其中，$n_{ij}$表示关键词$j$在文档$i$中的出现次数。 单纯使用TF来评估关键词的重要性忽略了常用词的干扰。常用词就是指那些文章中大量用到的，但是不能反映文章性质的那种词，比如：因为、所以、因此等等的连词，在英文文章里就体现为and、the、of等等的词。这些词往往拥有较高的TF，所以仅仅使用TF来考察一个词的关键性，是不够的。 逆文档频率(IDF)Inverse Document Frequency，文档频率就是一个词在整个文库词典中出现的频率，逆文档频率用下式计算 IDF_j = \log \frac{|D|}{|D_j| + 1}其中，$|D|$表示总的文档数目，$|D_j|$表示关键词$j$出现过的文档数目 scikit-learn内为 IDF_j = \log \frac{|D| + 1}{|D_j| + 1} + 1 词频-逆文档频率(TF-IDF) TF-IDF_{i} = TF_i × IDF举例例如有如下$3$个文本123文本1：My dog ate my homework.文本2：My cat ate the sandwich.文本3：A dolphin ate the homework. 提取字典，一般需要处理大小写、去除停用词a，处理结果为1ate, cat, dog, dolphin, homework, my, sandwich, the 故各个文本的词数向量为123文本1：[1, 0, 1, 0, 1, 2, 0, 0]文本2：[1, 1, 0, 0, 0, 1, 1, 1]文本3：[1, 0, 0, 1, 1, 0, 0, 1] 各个文本的词频向量(TF)123文本1：[0.2 , 0. , 0.2 , 0. , 0.2 , 0.4 , 0. , 0. ]文本2：[0.2 , 0.2 , 0. , 0. , 0. , 0.2 , 0.2 , 0.2 ]文本3：[0.25, 0. , 0. , 0.25, 0.25, 0. , 0. , 0.25] 各词出现过的文档次数1[3, 1, 1, 1, 2, 2, 1, 2] 总文档数为$3$，各词的逆文档频率(IDF)向量 这里使用scikit-learn内的方法求解 1[1. , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207] 故各文档的TF-IDF向量为123456文本1：[0.2 , 0. , 0.33862944, 0. , 0.25753641, 0.51507283, 0. , 0. ]文本2：[0.2 , 0.33862944, 0. , 0. , 0. , 0.25753641, 0.33862944, 0.25753641]文本3：[0.25 , 0. , 0. , 0.4232868 , 0.32192052, 0. , 0. , 0.32192052] 经单位化后，有123456文本1：[0.28680065, 0. , 0.48559571, 0. , 0.36930805, 0.73861611, 0. , 0. ]文本2：[0.31544415, 0.53409337, 0. , 0. , 0. , 0.40619178, 0.53409337, 0.40619178]文本3：[0.37311881, 0. , 0. , 0.63174505, 0.4804584 , 0. , 0. , 0.4804584 ] 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; vec_num = np.array([ [1, 0, 1, 0, 1, 2, 0, 0], [1, 1, 0, 0, 0, 1, 1, 1], [1, 0, 0, 1, 1, 0, 0, 1] ])&gt;&gt;&gt; vec_tf = vec_num / np.sum(vec_num, axis=1).reshape(-1, 1)&gt;&gt;&gt; vec_tfarray([[0.2 , 0. , 0.2 , 0. , 0.2 , 0.4 , 0. , 0. ], [0.2 , 0.2 , 0. , 0. , 0. , 0.2 , 0.2 , 0.2 ], [0.25, 0. , 0. , 0.25, 0.25, 0. , 0. , 0.25]])&gt;&gt;&gt; vec_num[vec_num&gt;0] = 1&gt;&gt;&gt; n_showup = np.sum(vec_num, axis=0)&gt;&gt;&gt; n_showuparray([3, 1, 1, 1, 2, 2, 1, 2])&gt;&gt;&gt; d = 3&gt;&gt;&gt; vec_idf = np.log((d + 1) / (n_showup + 1)) + 1&gt;&gt;&gt; vec_idfarray([1. , 1.69314718, 1.69314718, 1.69314718, 1.28768207, 1.28768207, 1.69314718, 1.28768207])&gt;&gt;&gt; vec_tfidf = vec_tf * vec_idf&gt;&gt;&gt; vec_tfidfarray([[0.2 , 0. , 0.33862944, 0. , 0.25753641, 0.51507283, 0. , 0. ], [0.2 , 0.33862944, 0. , 0. , 0. , 0.25753641, 0.33862944, 0.25753641], [0.25 , 0. , 0. , 0.4232868 , 0.32192052, 0. , 0. , 0.32192052]])&gt;&gt;&gt; vec_tfidf = vec_tfidf / np.linalg.norm(vec_tfidf, axis=1).reshape((-1, 1))&gt;&gt;&gt; vec_tfidfarray([[0.28680065, 0. , 0.48559571, 0. , 0.36930805, 0.73861611, 0. , 0. ], [0.31544415, 0.53409337, 0. , 0. , 0. , 0.40619178, 0.53409337, 0.40619178], [0.37311881, 0. , 0. , 0.63174505, 0.4804584 , 0. , 0. , 0.4804584 ]]) 验证使用scikit-learn机器学习包计算结果123456789101112&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer&gt;&gt;&gt; vectorizer = TfidfVectorizer()&gt;&gt;&gt; text = [ "My dog ate my homework", "My cat ate the sandwich", "A dolphin ate the homework"]&gt;&gt;&gt; vectorizer.fit_transform(text).toarray()array([[0.28680065, 0. , 0.48559571, 0. , 0.36930805, 0.73861611, 0. , 0. ], [0.31544415, 0.53409337, 0. , 0. , 0. , 0.40619178, 0.53409337, 0.40619178], [0.37311881, 0. , 0. , 0.63174505, 0.4804584 , 0. , 0. , 0.4804584 ]])&gt;&gt;&gt; vectorizer.get_feature_names()['ate', 'cat', 'dog', 'dolphin', 'homework', 'my', 'sandwich', 'the']]]></content>
      <categories>
        <category>Practice</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2018%2F10%2F23%2FSVD%2F</url>
    <content type="text"><![CDATA[引言奇异值分解Singular Value Decomposition是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。 原理从特征值分解(EVD)讲起我们知道对于一个$n$阶方阵$A_{n×n}$，有 A\alpha_i = \lambda_i \alpha_i i = 1, ..., n取 P = \left[\alpha_1, \alpha_2, ..., \alpha_n\right]有下式成立 AP = P\Lambda其中 \Lambda = \left[ \begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_n \\ \end{matrix} \right] 特征值一般从大到小排列 利用该式可将方阵$A_{n×n}$化作对角阵$\Lambda_{n×n}$ \Lambda = P^{-1}AP或者 A = P \Lambda P^{-1} = \sum_{i=1}^n \lambda_i (P_{,i})(P_{,i})^{-1} “$_{i}$”表示第$i$行，“$_{,i}$”表示第$i$列 这样我们就可以理解为，矩阵$A$是由$n$个$n$阶矩阵$P_{,i}P^{-1}_{i}$加权组成，特征值$\lambda_i$即为权重。 以上为个人理解，不妥之处可以指出。 奇异值分解(SVD)定义对于长方阵$A_{m×n}$，不能进行特征值分解，可进行如下分解 A_{m×n} = U_{m×m} \Sigma_{m×n} V_{n×n}^T其中$U \in \mathbb{R}^{m×m}, V \in \mathbb{R}^{n×n}$，均为正交矩阵。矩阵$\Sigma_{m×n}$如下 对于$m&gt;n$ \Sigma_{m×n} = \left[ \begin{matrix} S_{n×n} \\ --- \\ O_{(m-n)×n} \end{matrix} \right] 对于$m&lt;n$ \Sigma_{m×n} = \left[ \begin{matrix} S_{m×m} & | & O_{m×(n-m)} \end{matrix} \right] 矩阵$S_{n×n}$为对角阵，对角元素从大到小排列 S_{n×n} = \left[ \begin{matrix} \sigma_1 & & \\ & ... & \\ & & \sigma_n\\ \end{matrix} \right]直观表示SVD分解如下 当取$r&lt;n$时，有部分奇异值分解，可用于降维 A_{m×n} = U_{m×r} \Sigma_{r×r} V_{r×n}^T计算 以下仅考虑$m&gt;n$的情况 令矩阵$A^T$与$A$相乘，有 A^TA = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T A^TA = V \Sigma^T \Sigma V^T 矩阵$U$为正交阵，即满足$U^TU=I$ 其中 \Sigma^T \Sigma = \left[ \begin{matrix} S^T_{n×n} & | & O^T_{n×(m-n)} \end{matrix} \right] \left[ \begin{matrix} S_{n×n} \\ --- \\ O_{(m-n)×n} \end{matrix} \right] = S_{n×n}^2 = \left[ \begin{matrix} \sigma_1^2 & & \\ & ... & \\ & & \sigma_n^2\\ \end{matrix} \right] 则 A^T A = V S^2 V^T 即矩阵$A^T A$相似对角化为$S^2$，对角元素$\sigma_i^2$与矩阵$V$的列向量$v_i(i=1, …, n)$为矩阵$A^T A$的特征对。 那么对矩阵$A^T A$进行特征值分解，有 (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i 则 v_i = \alpha^{(1)}_i \sigma_i = \sqrt{\lambda^{(1)}_i} 注：对于二次型$x^T (A^T A) x$ x^T (A^T A) x = (Ax)^T(Ax) \geq 0故矩阵$A^T A$半正定，$\sigma_i = \sqrt{\lambda_i}$有解 同理，令矩阵$A$与$A^T$相乘，可证得 A A^T = U \Sigma \Sigma^T U^T 其中 \Sigma \Sigma^T = \left[ \begin{matrix} S_{n×n} \\ --- \\ O_{(m-n)×n} \end{matrix} \right] \left[ \begin{matrix} S^T_{n×n} & | & O^T_{n×(m-n)} \end{matrix} \right] = \left[ \begin{matrix} S^2_{n×n} & O_{n×(m-n)} \\ O_{(m-n)×n} & O_{(m-n)×(m-n)} \end{matrix} \right] 即矩阵$A A^T$相似对角化，对角元素$\sigma_i^2$与矩阵$U$的列向量$u_i(i=1, …, m)$为矩阵$A A^T$的特征对。 对矩阵$A A^T$进行特征值分解，有 (A^T A) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i 则 u_i = \alpha^{(2)}_i \sigma_i = \sqrt{\lambda^{(2)}_i} 同理可证得$A A^T$半正定，略。 一般来说，为减少计算量，计算奇异值分解只进行一次特征值分解，如对于矩阵$X_{m×n}(m&gt;n)$，选取$n$阶矩阵$X^T X$进行特征值分解计算$v_i$，计算$u_i$方法下面介绍。 根据前面推导，我们有特征值分解 (A^T A) \alpha^{(1)}_i = \lambda^{(1)}_i \alpha^{(1)}_i (A A^T) \alpha^{(2)}_i = \lambda^{(2)}_i \alpha^{(2)}_i其中$\lambda^{(1)}_i = \lambda^{(2)}_i = \sigma_i^2$，$v_i = \alpha^{(1)}_i$，$u_i = \alpha^{(2)}_i$，即 A^T A v_i = \sigma_i^2 v_i \tag{1} A A^T u_i = \sigma_i^2 u_i \tag{2}$(1)$式左右乘$A$，有 A A^T A v_i = \sigma_i^2 A v_i发现什么？这是另一个特征值分解的表达式！ (A A^T) (A v_i) = \sigma_i^2 (A v_i)故 u_i \propto A v_i 或 u_i = k · A v_i \tag{3}现在求解系数$k$，根据定义 A = U \Sigma V^T \Rightarrow AV = U \Sigma则 A v_i = \sigma_i u_i \Rightarrow u_i = \frac{1}{\sigma_i} A v_i或者 U = A V \Sigma^{-1} 注：只能求前$n$个$u_i$，之后的需要列写方程求解 举栗将矩阵$A$进行分解 A = \left[ \begin{matrix} 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{matrix} \right]为减少计算量，取$A^T A$计算 A^T A = \left[ \begin{matrix} 2 & 1 \\ 1 & 2 \end{matrix} \right]特征值分解，有 A\left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] \left[ \begin{matrix} 3 & \\ & 1 \end{matrix} \right]故 \Sigma = \left[ \begin{matrix} \sqrt{3} & \\ & 1 \end{matrix} \right] V = \left[ \begin{matrix} \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{matrix} \right] U = A V \Sigma^{-1} = \left[ \begin{matrix} \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{2}} \\ \frac{2}{\sqrt{6}} & 0 \\ \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{2}} \end{matrix} \right]123456789101112131415161718192021222324&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; A = np.array([ [0, 1], [1, 1], [1, 0] ])&gt;&gt;&gt; ATA = A.T.dot(A)&gt;&gt;&gt; eigval, eigvec= np.linalg.eig(ATA)&gt;&gt;&gt; V = eigvec.copy()&gt;&gt;&gt; S = np.diag(np.sqrt(eigval))&gt;&gt;&gt; U = A.dot(V).dot(np.linalg.inv(S))&gt;&gt;&gt; Uarray([[ 0.40824829, 0.70710678], [ 0.81649658, 0. ], [ 0.40824829, -0.70710678]])&gt;&gt;&gt; Sarray([[1.73205081, 0. ], [0. , 1. ]])&gt;&gt;&gt; Varray([[ 0.70710678, -0.70710678], [ 0.70710678, 0.70710678]])&gt;&gt;&gt; # 验证&gt;&gt;&gt; U.dot(S).dot(V.T)array([[-2.23711432e-17, 1.00000000e+00], [ 1.00000000e+00, 1.00000000e+00], [ 1.00000000e+00, -2.23711432e-17]]) 理解展开表达式，取$r \leq n$时， A = U_{m×r} \Sigma_{r×r} V_{r×n}^T = \sum_{i=1}^r \sigma_i (U_{,i}) (V_{,i})^T就得到与PCA相同的结论，矩阵$A$可由$r$个$m×n$的矩阵$(U_{,i}) (V_{,i})^T$加权组成。一般来说，前$10\%$甚至$1\%$的奇异值就占了全部奇异值之和的$99\%$，极大地保留了信息，而大大减少了存储空间。 以图片为例，若原有24bit图片，其大小为(1024, 768)，则不计图片信息，仅仅数据共占1024×768×3 B，或2.25 MB。用奇异值分解进行压缩，保留$60\%$的奇异值，可达到几乎无损的程度，此时需要保存向量矩阵$U_{1024×60}$，$V_{60×768}$以及$60$个奇异值，以浮点数float32存储，一共占420 KB即可。 (1024 × 60 + 60 × 768 + 60) × 4 / 2^{10} = 420.23说句题外话，存储量的压缩必然以计算量的增大为代价，相反亦然，所以需要协调好RAM与ROM容量，考虑计算机的计算速度。换句话说，空间和时间上必然是互补的，哲学的味道hhhh。 分解结果的信息保留分解后各样本间的欧式距离与角度信息应不变，给出证明如下设有$m$组$n$维样本样本 X_{n×m} = [X^{(1)}, X^{(2)}, ..., X^{(m)}]经奇异值分解，有 X_{n×m} = U_{n×r} \Sigma_{r×r} V_{r×m}^T记 Z_{r×m} = \Sigma V^T = [Z^{(1)}, Z^{(2)}, ..., Z^{(N)}]有 X = U Z 欧式距离 || X^{(i)} - X^{(j)} ||_2^2 = || U (Z^{(i)} - Z^{(j)}) ||_2^2 = \left[ U (Z^{(i)} - Z^{(j)}) \right]^T \left[ U (Z^{(i)} - Z^{(j)}) \right] = (Z^{(i)} - Z^{(j)})^T U^T U (Z^{(i)} - Z^{(j)}) = || Z^{(i)} - Z^{(j)} ||_2^2 即 || X^{(i)} - X^{(j)} ||_2^2 = || Z^{(i)} - Z^{(j)} ||_2^2 角度信息 \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{||UZ^{(i)}||_2||UZ^{(j)}||_2} = \frac{(UZ^{(i)})^T(UZ^{(j)})}{\sqrt{(UZ^{(i)})^T(UZ^{(i)})} \sqrt{(UZ^{(j)})^T(UZ^{(j)})}} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} 即 \frac{X^{(i)T}X^{(j)}}{||X^{(i)}||_2||X^{(j)}||_2} = \frac{Z^{(i)T}Z^{(j)}}{||Z^{(i)}||_2||Z^{(j)}||_2} 代码@Github: Code of SVD对图片进行了分解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798class SVD(): """ Singular Value Decomposition Attributes: m &#123;int&#125; n &#123;int&#125; r &#123;int&#125;: if r == -1, then r = n isTrains &#123;bool&#125;: isTrains = True if input.shape[0] &lt; input.shape[1] U &#123;ndarray(m, r)&#125; S &#123;ndarray(r, )&#125; V &#123;ndarray(n, r)&#125; Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - Reassign r if eigvals contains zero - Singular values are stored in a 1-dim array `S` - X' = U S V^T """ def __init__(self, r=-1): self.m = None self.n = None self.r = r self.isTrans = False self.U = None self.S = None self.V = None def fit(self, X): """ calculate components Notes: - Transpose input matrix if m &lt; n, and m, n := n, m - reassign self.r if eigvals contains zero """ (self.m, self.n) = X.shape if self.m &lt; self.n: X = X.T self.m, self.n = self.n, self.m self.isTrans = True self.r = self.n if (self.r == -1) else self.r XTX = X.T.dot(X) eigval, eigvec = np.linalg.eig(X.T.dot(X)) eigval, eigvec = np.real(eigval), np.real(eigvec) self.S = np.sqrt(np.clip(eigval, 0, float('inf'))) self.S = self.S[self.S &gt; 0] self.r = min(self.r, self.S.shape[0]) # reassign self.r order = np.argsort(eigval)[::-1][: self.r] # sort eigval from large to small eigval = eigval[order]; eigvec = eigvec[:, order] self.V = eigvec.copy() self.U = X.dot(self.V).dot( np.linalg.inv(np.diag(self.S))) return self.U, self.S, self.V def compose(self, r=-1): """ merge first r components Parameters: r &#123;int&#125;: if r==-1, merge all components Returns: X &#123;ndarray(m, n)&#125; """ if r == -1: X = self.U.dot(np.diag(self.S)).dot(self.V.T) X = X.T if self.isTrans else X else: (m, n) = (self.n, self.m) if self.isTrans else (self.m, self.n) X = np.zeros(shape=(m, n)) for i in range(r): X += self.__getitem__(i) return X def __getitem__(self, idx): """ get a component Parameters: index &#123;int&#125;: range from (0, self.r) """ u = self.U[:, idx] v = self.V[:, idx] s = self.S[idx] x = s * u.reshape(self.m, 1).\ dot(v.reshape(1, self.n)) x = x.T if self.isTrans else x return x def showComponets(self, r=-1): """ display components Notes: - Resize components' shape into (40, 30) """ m, n = self.m, self.n r = self.r if r==-1 else r n_images = 10; m_images = r // n_images + 1 m_size, n_size = 40, 30 showfig = np.zeros(shape=(m_images*m_size, n_images*n_size)) for i in range(r): m_pos = i // n_images n_pos = i % n_images component = self.__getitem__(i) component = component.T if self.isTrans else component component = cv2.resize(component, (30, 40)) showfig[m_pos*m_size: (m_pos+1)*m_size, n_pos*n_size: (n_pos+1)*n_size] = component plt.figure('components') plt.imshow(showfig) plt.show() 用上面的代码进行实验1234567891011121314# 读取一张图片X = load_images()[0].reshape((32, 32))showmat2d(X)# 对图片进行奇异值分解decomposer = SVD(r=-1)decomposer.fit(X)# 显示一下分量decomposer.showComponets(r=-1)# 将全部分量组合，并显示X_ = decomposer.compose(r=-1)showmat2d(X_)# 将前5个分量组合，并显示X_ = decomposer.compose(r=5)showmat2d(X_) 载入原图如下 分量显示如下 组合分量显示如下 组合全部 组合前5个分量 应用：推荐系统 详情查看Basic-Machine-Learning-Algorithm/ZhaoHaitao, ECUST/recommend.py 数据可从MoiveLens下载，实验使用ml-100k.zip。假设有用户$N_{user}$人，电影$N_{item}$部，先每个人对其看过的部分电影已进行评分，及其稀疏，现希望从这些数据预测出未知数据。 存储数据为矩阵 M_{N_{user} \times N_{item}} = \left[\begin{matrix} r_{ij} \end{matrix}\right]现希望从用户相似度的角度，找出臭味相投的一些用户，用他们的评分均值作为该用户的评分，先对上述矩阵进行SVD分解 M_{N_{user} \times N_{item}} = U_{N_{user} \times N_{user}} \cdot \Sigma_{N_{user} \times N_{item}} \cdot (V_{N_{item} \times N_{item}})^T \tag{4}其中 U_{N_{user} \times N_{user}} = \left[\begin{matrix} \vec{u_{1}} & \vec{u_{2}} & \cdots & \vec{u_{N_{user}}} \end{matrix}\right]V_{N_{item} \times N_{item}} = \left[\begin{matrix} \vec{v_{1}} & \vec{v_{2}} & \cdots & \vec{v_{N_{item}}} \end{matrix}\right]利用矩阵$V$提取用户的特征，设特征维度为$D_u$ M^u = M_{N_{user} \times N_{item}} \cdot V_{N_{item} \times D_u} \tag{5}而 V_{N_{item} \times N_{item}} = \left[\begin{matrix} V_{N_{item} \times D_u} & | & V_{N_{item} \times (N_{item} - D_u)} \end{matrix}\right] (V_{N_{item} \times N_{item}})^T \cdot V_{N_{item} \times D_u} = \left[\begin{matrix} I_{D_u \times D_u} \\ --- \\ O_{(N_{item} - D_u) \times D_u} \end{matrix}\right] \tag{6}所以$(4)(6)$代入$(5)$，亦可化简为 M^u = U_{N_{user} \times N_{user}} \cdot \Sigma_{N_{user} \times N_{item}} \cdot (V_{N_{item} \times N_{item}})^T \cdot V_{N_{item} \times D_u}= U_{N_{user} \times D_u} \cdot \Sigma_{D_u \times D_u} \tag{7}现计算用户间的相似度矩阵，可用余弦度量，即 s_{ij} = \frac{M^{u_iT} M^u_j}{||M^u_i|| ||M^u_j||}或者 M^u_i := \frac{M^u_i}{||M^u_i||}S_{N_{user} \times N_{user}} = M^u \cdot M^{uT} \tag{8}对于某部电影$\text{item}^{(j)}$，先找到该用户$\text{user}^{(i)}$的最近似的几个用户$\text{user}^{(k)}, k \in {1, \cdots, N_{user} }, k \neq i$，取其均值作为该用户的评分。 若利用电影的相似度，只需 M^i = (U_{N_{item} \times D_i})^T \cdot M_{N_{user} \times N_{item}} \tag{9}以下同。 12345678910111213141516171819202122232425262728def according_to_user(train_matrix, test_matrix, cols=80, n_keep=50): # 将矩阵SVD分解 _, _, vh = np.linalg.svd(train_matrix) # 压缩原矩阵，A' = A V[:, :k] train_compressed_col = train_matrix.dot(vh[: cols].T) # N_USERS x cols # 计算相似度矩阵 similarity_user = get_cosine_similarity_matrix(train_compressed_col) # 预测 pred_matrix = np.zeros_like(test_matrix) # 保存预测结果 to_pred = np.array(np.where(test_matrix != 0)) # 需要预测的数据位置, (2, n) for i in range(to_pred.shape[1]): r, c = to_pred[:, i] # r为用户索引，c为电影索引 id = np.argsort(similarity_user[r])[::-1] # 将用户以相似度从大到小排序 id = id[1: n_keep + 1] # 获取相似度最大的几个用户，除自身 rates = train_matrix[id, c] # 获取这几个用户对该电影的评分 rates = rates[rates!=0] # 已评价的数据 rate = np.mean(rates) if rates.shape[0] != 0 else 0 pred_matrix[r, c] = rate return pred_matrix]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[删除停用词]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%88%A0%E9%99%A4%E5%81%9C%E7%94%A8%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[删除停用词 - Python文本处理教程™ 停用词是对句子没有多大意义的英语单词。 在不牺牲句子含义的情况下，可以安全地忽略它们。 例如，the, he, have等等的单词已经在名为语料库的语料库中捕获了这些单词。 下载语料库 安装nltk模块 1pip install nltk 下载语料库 12import nltknltk.download(&#39;stopwords&#39;) 使用库料库 验证停用词 123456789101112131415161718192021222324252627&gt;&gt;&gt; from nltk.corpus import stopwords&gt;&gt;&gt; stopwords.words('english')['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',"she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when','where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn',"mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"] 除了英语之外，具有这些停用词的各种语言如下。 12345&gt;&gt;&gt; stopwords.fileids()['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian','spanish', 'swedish', 'turkish'] 示例 从单词列表中删除停用词。 12345678910111213&gt;&gt;&gt; from nltk.corpus import stopwords&gt;&gt;&gt; en_stops = set(stopwords.words('english'))&gt;&gt;&gt; &gt;&gt;&gt; all_words = ['There', 'is', 'a', 'tree','near','the','river']&gt;&gt;&gt; for word in all_words: if word not in en_stops: print(word) Theretreenearriver]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PCA]]></title>
    <url>%2F2018%2F10%2F22%2FPCA%2F</url>
    <content type="text"><![CDATA[引言PCA全称Principal Component Analysis，即主成分分析，是一种常用的数据降维方法。它可以通过线性变换将原始数据变换为一组各维度线性无关的表示，以此来提取数据的主要线性分量。 向量的投影现有两个任意不共线向量$\vec{u}, \vec{v}$，将$\vec{u}$投射到$\vec{v}$上 投影后，可以得到两个正交向量 \vec{u}' · (\vec{u} - \vec{u}') = 0我们设 \vec{u}' = \mu \vec{v} \tag{1}代入后有 \mu \vec{v} · (\vec{u} - \mu \vec{v}) = 0引入矩阵运算，即 (\mu v)^T (u - \mu v) = 0有 v^T u = \mu v^T v则得到$u’$以$v$为基向量的坐标 \mu = (v^T v)^{-1} v^T u \tag{2}所以得到 u' = v (v^T v)^{-1} v^T u \tag{*} 坐标变换求解投影向量：$u’$可视作$u$经坐标变换$u’ = P u$得到，所以 P = v (v^T v)^{-1} v^T 推广至多个向量的投影，即得到 P = X (X^T X)^{-1} X^T这与线性回归中得到的结论一致。 实际上 u' = v (v^T v)^{-1} v^T u = \frac{v}{||v||} (\frac{v}{||v||})^T u记单位向量$\frac{v}{||v||}$为$v_0$，得到 u' = v_0 v_0^T u由几何关系，可以计算得投影后的长度为 d = ||u|| \cos \theta = ||u|| \frac{v^T u}{||u||||v||} = v_0^T u所以在向量投影中，$u^T v_0$表示以$v_0$为基向量的坐标。 PCA现在有$N$维数据集$D={x^{(1)}, x^{(2)}, …, x^{(M)}}$，其中$x^{(i)} = \left[x^{(i)}_1, x^{(i)}_2, …, x^{(i)}_N\right]^T$，各维特征$D_{j}$间存在线性相关性，利用主成分分析可使 数据维度降低； 提取主成分，且各成分间不相关。 说明 由于选取的特征轴是正交的，所以计算结果线性无关； 提取了方差较大的几个特征，为主要线性分量。 以二维空间中的数据$x^{(i)} = \left[\begin{matrix} x^{(i)}_1 \ x^{(i)}_2\end{matrix}\right]$为例，每个样本点可降至一维空间，如下图所示。 主轴可有无穷多种选择，那么问题就是如何选取最优的主轴。先给出PCA的计算步骤。 计算步骤输入的$M$个$N$维样本，有样本矩阵 X_{N×M} = \left[x^{(1)}, x^{(2)}, ..., x^{(M)} \right] = \left[ \begin{matrix} x^{(1)}_1 & x^{(2)}_1 & ... & x^{(M)}_1 \\ x^{(1)}_2 & x^{(2)}_2 & ... & x^{(M)}_2 \\ ... \\ x^{(1)}_N & x^{(2)}_N & ... & x^{(M)}_N \\ \end{matrix} \right]投影 对每个维度(行)进行去均值化 X_j := X_j - \mu_j 其中$\mu_j = \overline{X_j}$，$j = 1, 2, …, N$ 求各维度间的协方差矩阵$\Sigma_{N×N}$ \Sigma_{ij} = Cov(x_i, x_j) 或 \Sigma = \frac{1}{M} X X^T 注： X X^T = \left[ \begin{matrix} \sum_{i=1}^M x^{(i)}_1 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_1 x^{(i)}_N \\ \sum_{i=1}^M x^{(i)}_2 x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ \sum_{i=1}^M x^{(i)}_N x^{(i)}_1 & \sum_{i=1}^M x^{(i)}_N x^{(i)}_2 & ... & \sum_{i=1}^M x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M \left[ \begin{matrix} x^{(i)}_1 x^{(i)}_1 & x^{(i)}_1 x^{(i)}_2 & ... & x^{(i)}_1 x^{(i)}_N \\ x^{(i)}_2 x^{(i)}_1 & x^{(i)}_2 x^{(i)}_2 & ... & x^{(i)}_2 x^{(i)}_N \\ ... & ... & ... & ... \\ x^{(i)}_N x^{(i)}_1 & x^{(i)}_N x^{(i)}_2 & ... & x^{(i)}_N x^{(i)}_N \end{matrix} \right] = \sum_{i=1}^M x^{(i)} x^{(i)T} 协方差定义式如下，这也是PCA去均值化的原因 Cov(x,y)≝\frac{1}{n-1} ∑_{i=1}^n (x_i−\overline{x})^T(y_i−\overline{y})其中$x=[x_1, x_2, …, x_n]^T, y=[y_1, y_2, …, y_n]^T$ 求协方差矩阵$\Sigma$的特征值$λ_i$及其对应特征向量$α_i$，$i=1, …, N$； 按照特征值从大到小排列特征对$(λ_i,α_i)$，选取$K$个最大特征值对应的特征向量作为降维后的主轴$ \beta_1, \beta_2, …, \beta_K $，其中$\beta_k$为单位向量 \beta_k = \left[ \beta_{k1}, \beta_{k2}, ..., \beta_{kN} \right]^T记 B_{N×K} = \left[ \beta_1, \beta_2, ..., \beta_K \right]$K$的选取方法有如下两种： 指定选取$K$个主轴 保留$99\%$的方差\frac{\sum_{i=1}^K \lambda_i}{\sum_{j=1}^N \lambda_j} > 0.99 将样本点投射到$K$维坐标系上 样本$X^{(i)}$投射到主成分轴$\beta_k$上，其坐标表示为向量，为 S^{(i)}_k = X^{(i)T}\beta_k 注意此时的基座标为$\beta_k$，或者说$X’^{(i)} = S^{(i)} \frac{\beta_k}{||\beta_k||}$ 所有样本在主轴$\beta_k$上的投影坐标即 S = B^T X 其中$S_{K×M}$，$B_{N×K}$，$X_{N×M}$ 若取$K=N$，可进行数据重建，去除维度间的相关性，如下 复原第$5$步中，样本点向量$X^{(i)}$的主要分量投射到$K$个$N$维向量上，投影坐标为$S^{(i)}_k$，即 X^{(i)} \approx \sum_{k=1}^K S^{(i)}_k \beta_k以上就是样本点的复原公式，矩阵形式即 \hat{X} = BS其中$\hat{X}_{N×M}$，$B_{N×K}$，$S_{K×M}$ 考虑到已去均值化，故 \hat{X}_j \approx \hat{X}_j + \mu_j证明 投影向量的$2$范数最大，或者说，投影后的坐标平方和(方差)最大 当所有样本$X$投射到第一主轴$\beta_1$上，其坐标为 S_1 = X^T \beta_1所有元素的平方和，或向量$S_1$的$2$范数为 ||S_1||_2^2 = S_1^T S_1 = \beta_1^T X X^T \beta_1 \tag{1}即优化目标为 \max ||S_1||_2^2 s.t. ||\beta_1||_2^2 = 1矩阵$C_{N \times N}=XX^T$为对称矩阵，故可单位正交化(半正定) C = W \Lambda W^T W = \left[\begin{matrix} | & & |\\ w_1 & ... & w_N\\ | & & |\\ \end{matrix}\right] \Lambda = \left[\begin{matrix} \lambda_1 & & \\ & ... & \\ & & \lambda_N\\ \end{matrix}\right]其中$\lambda_1 &gt; …&gt; \lambda_N$，$w_i(i=1,…,N)$为矩阵$C$的特征向量(单位向量，互相正交) 实际上$R(C) \leq (n-1)$，即最多有$(n-1)$个特征值大于$0$。 ||S_1||_2^2 = \beta_1^T W \Lambda W^T \beta_1 \tag{2}令$\alpha_1 = W^T \beta_1, \beta_1 = W \alpha_1$，可得 ||S_1||_2^2 = \alpha_1^T \Lambda \alpha_1 \tag{3}即 ||S_1||_2^2 = \sum_{i=1}^N \lambda_i \alpha_{1i}^2 \tag{4}假设特征值已降序排序，那么进一步 \sum_{i=1}^N \lambda_i \alpha_{1i}^2 \leq \lambda_1 \sum_{i=1}^N \alpha_{1i}^2 \tag{5}且由于$\beta_1^T\beta_1 = 1$，故 1 = \beta_1^T\beta_1 = \alpha_1^T W^T W \alpha = \alpha^T \alpha = \sum_{i=1}^N \alpha_{1i}^2可得 ||S_1||_2^2 = \sum_{i=1}^N \lambda_i \alpha_{1i}^2 \leq \lambda_1 \tag{6}为使$(6)$取等号，即达最大值，可使 \begin{cases} \alpha_{11} = 1 \\ \alpha_{12} = ... = \alpha_{1N} = 0 \end{cases}即令 \beta_1 = W \alpha_1 = w_1 $\alpha_1 = [1, 0, …, 0]^T$ 所以$\beta_1$对应矩阵$C=XX^T$的特征向量$w_1$，且有 ||S_1||_2^2 = \lambda_1 或者第一主成分的证明也可以这样，建立优化目标 \beta_1 = \arg \max ||S_1||_2^2s.t. ||\beta_1||_2^2 = 1构造拉格朗日函数 L(\beta_1, \lambda_1) = ||S_1||_2^2 + \lambda_1 (1 - ||\beta_1||_2^2)也即 L(\beta_1, \lambda_1) = \beta_1^T X X^T \beta_1 + \lambda_1 (1 - \beta_1^T \beta_1)求其极值点 ▽_{\beta_1}L(\beta_1) = 2 X X^T \beta_1 - 2 \lambda_1 \beta_1 = 0有 X X^T \beta_1 = \lambda_1 \beta_1可见$\beta_1$即方阵$X X^T$的特征向量 当我们希望用更多的主成分刻画数据，如已经求得主成分$\beta_1, …, \beta_{r-1}$，现需求解$\beta_r$，引入正交约束$\beta_r^T \beta_i = 0$，即目标函数为 ||S_r||_2^2 = \beta_r^T C \beta_r s.t. \beta_r^T \beta_i = 0, i = 1, ..., r-1 ||\beta_r||_2^2 = 1令$\beta_r = W \alpha_r$，则 ||S_r||_2^2 = \alpha_r^T \Lambda \alpha_r = \sum_i \lambda_i \alpha_{ri}^2 \tag{7}而根据正交约束，$\beta_r = W \alpha_r, \beta_i = w_i$代入后有 0 = \beta_r^T \beta_i = (\alpha_r^T W^T) w_i = \alpha_{ri}, i = 1, ..., r-1 \tag{8} $ W^T w_i = \left[0, …, 1_i, …, 0\right]^T$ $(8)$代入$(7)$后得到 ||S_r||_2^2 = \sum_i \lambda_i \alpha_{ri}^2 = \lambda_r \alpha_{rr}^2 \tag{9}又因为$\beta_r^T \beta_r = 1$(单位向量)，故 \beta_r^T \beta_r = \alpha_r^T W^T W \alpha_r = \alpha_r^T \alpha_r = \sum_i \alpha_{ri}^2 = 1 \tag{10}那么类似的，为在满足正交和单位约束下，使$(9)$取最大，取 \begin{cases} \alpha_{rr} = 1\\ \alpha_{ri} = 0, i = 1, ..., N, i \neq r \end{cases} $\alpha_r = [0, …, 1_r, …, 0]$ 则此时 \beta_r = W \alpha_r = w_r且有 ||S_r||_2^2 = \lambda_r证毕。 证明过程总结如下 \begin{aligned} \left. \begin{aligned} \left. \begin{aligned} ||S_1||_2^2 = S_1^T S_1 \\ S_1 = X^T \beta_1 \end{aligned} \right\} \Rightarrow ||S_1||_2^2 = \beta_1^T \underbrace{X X^T}_C \beta_1 \\ C = X X^T = W \Lambda W^T \end{aligned} \right\} \Rightarrow \\ \left. \begin{aligned} ||S_1||_2^2 = \beta_1^T W \Lambda \underbrace{W^T \beta_1}_{\alpha_1} = \sum_{i=1}^N \lambda_i \alpha_{1i} \leq \lambda_1 \sum_{i=1}^N \alpha_{1i} \\ \beta_1^T \beta_1 = \alpha_1^T W^T W \alpha = \alpha_1^T \alpha = \sum_{i=1}^N \alpha_{1i} = 1(单位约束) \end{aligned} \right\} \Rightarrow \\ ||S_1||_2^2 \leq \lambda_1 \quad 为使||S_1||_2^2极大化，取 \\ \begin{cases} \alpha_{11} = 1\\ \alpha_{1i} = 0, i = 2, 3, \cdots, N \end{cases} \Rightarrow \beta_1 = W \alpha_1 = w_1 \end{aligned} \tag{*1} \begin{aligned} \left. \begin{aligned} \left. \begin{aligned} ||S_r||_2^2 = S_r^T S_r \\ S_r = X^T \beta_r \end{aligned} \right\} \Rightarrow ||S_r||_2^2 = \beta_r^T \underbrace{X X^T}_C \beta_r \\ C = X X^T = W \Lambda W^T \end{aligned} \right\} \Rightarrow \\ \left. \begin{aligned} ||S_r||_2^2 = \beta_r^T W \Lambda \underbrace{W^T \beta_r}_{\alpha_r} = \sum_{i=1}^N \lambda_i \alpha_{ri} \\ \beta_r^T \beta_i =(W \alpha_r)^T (w_i) = \alpha_{ri} = 0, i \neq r (正交约束) \\ \beta_r^T \beta_r = \alpha_r^T W^T W \alpha = \alpha_r^T \alpha = \sum_{i=1}^N \alpha_{1i} = 1(单位约束) \end{aligned} \right\} \Rightarrow \\ ||S_r||_2^2 = \lambda_r \alpha_{rr} \quad 为使||S_r||_2^2极大化，取 \\ \begin{cases} \alpha_{rr} = 1 \\ \alpha_{ri} = 0, i = \neq r \end{cases} \Rightarrow \beta_r = W \alpha_r = w_r \end{aligned} \tag{*2}白化(whitening)whitening的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。 数据的whitening必须满足两个条件： 不同特征间相关性最小，接近$0$； 所有特征的方差相等（不一定为$1$）。 常见的白化操作有PCA whitening和ZCA whitening。 Whitening - Ufldl PCA whitening PCA whitening指将数据$X$经过PCA降维为$S$后，可以看出$S$中每一维是独立的，满足whitening的第一个条件，这是只需要将$S$中的每一维都除以标准差就得到了每一维的方差为$1$，也就是说方差相等。 X_{PCAwhite, j} = \frac{X_{rot, j}}{\sqrt{\lambda_j}} ZCA whitening ZCA whitening是指数据$X$先经过PCA变换为$S$，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为$1$的操作，最后将得到的矩阵左乘一个特征向量矩阵$U$即可。 X_{ZCAwhite} = U · X_{PCAwhite} Kernel PCAKernel PCA的思想是在高维的特征空间中求解协方差矩阵 \Sigma = \frac{1}{M} \sum_{i=1}^M \Phi(X^{(i)}) \Phi(X^{(i)})^T其中$\Phi(X^{(i)})$表示将样本$i$映射到高维空间后中的向量，即 \Phi(X^{(i)}) = \left[ \phi^{(i)}_1, \phi^{(i)}_2, ..., \phi^{(i)}_{N'} \right]^T其中$N’ &gt; N$，由于$\Phi(X^{(i)})$为隐式的，故设置核函数求解，记 \kappa(i, j) = \Phi(X^{(i)}) \Phi(X^{(i)})^T 关于核技巧，移步非线性支持向量机 应用可利用PCA与线性回归求解$3$维空间中平面的法向量 利用PCA重建数据(不降维，此时为$3$维)，此时第$1, 2$主成分轴可张成所求平面，即该平面可表示为 \Pi = span \{ \beta_1, \beta_2 \} 就是说，第一、二主成分是这些点“拉伸”最大的方向 :-)，好懂不？ 由正交投影可知，平面外一点$y$可通过最小二乘(线性回归)的方法投射到平面上，向量运算，不考虑偏置项，即 \hat{y} = \theta_1 x_1 + \theta_2 x_2 \tag{*} 其中$x_1, x_2$表示第一、第二主成分$\beta_1, \beta_2$，为$3$维向量 \hat{y} = \left[ \begin{matrix} \hat{y_1} \\ \hat{y_2} \\ \hat{y_3} \\ \end{matrix} \right] x_i = \left[ \begin{matrix} x_{i1} \\ x_{i2} \\ x_{i3} \\ \end{matrix} \right] 可利用公式求解回归参数$\theta$ \theta = (X^TX+\lambda I)^{-1} X^T y 注意：$X(n_samples, n_features)$，这里把$(x_{1j}, x_{2j}, y_{j})作为一组样本$ 此时该参数表示在主轴上的坐标$(\theta_1, \theta_2)$，带回$(*))$即可解得$\hat{y}$ \hat{y} = \theta_1 \beta_1 + \theta_2 \beta_2 \tag{*} 通俗理解，一掌把$y$拍平在了平面$\Pi$上，变成了$\hat{y}$，但是哪有这么好拍。。。这个时候刺在掌心里一定有一个垂直的向量分量，即为该平面的法向量 \vec{n} = y - \hat{y} 也可使用粗暴一点的方法，直接将第三主成分作为法向量。 或者直接上投影公式： \hat{y} = Py P = X (X^TX+\lambda I)^{-1} X^T ![projection](/PCA/projection.jpg) 总体的运算流程如下 - 利用所有样本点(近似平面)计算主成分，第一、二主成分张成平面$\Pi$； - 选出其中一个样本点，将平行于平面$\Pi$的成分投射到$\Pi$上； - 该样本点剩余分量即法向量； - 一般来说，取所有点法向量的均值。 程序@Github: PCA 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class PrincipalComponentAnalysis(): def __init__(self, n_component=-1): self.n_component = n_component self.meanVal = None self.axis = None def fit(self, X, prop=0.99): ''' the parameter 'prop' is only for 'n_component = -1' ''' # 第一步: 归一化 self.meanVal = np.mean(X, axis=0) # 训练样本每个特征上的的均值 X_normalized = (X - self.meanVal) # 归一化训练样本 # 第二步：计算协方差矩阵 # cov = X_normalized.T.dot(X_normalized) cov = np.cov(X_normalized.T) # 协方差矩阵 eigVal, eigVec = np.linalg.eig(cov) # EVD order = np.argsort(eigVal)[::-1] # 从大到小排序 eigVal = eigVal[order] eigVec = eigVec.T[order].T # 选择主成分的数量 if self.n_component == -1: sumOfEigVal = np.sum(eigVal) sum_tmp = 0 for k in range(eigVal.shape[0]): sum_tmp += eigVal[k] if sum_tmp &gt; prop * sumOfEigVal: # 平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 K 值 self.n_component = k + 1 break # 选择投影坐标轴 self.axis = eigVec[:, :self.n_component] # 选择前n_component个特征向量作为投影坐标轴 def transform(self, X): # 第一步：归一化 X_normalized = (X - self.meanVal) # 归一化测试样本 # 第二步：投影 X_nxk · V_kxk' = X'_nxk' X_transformed = X_normalized.dot(self.axis) return X_transformed def fit_transform(self, X, prop=0.99): self.fit(X, prop=prop) return self.transform(X) def transform_inv(self, X_transformed): # 视投影向量长度为一个单位长度，投影结果为投影向量上的坐标 # X'_nxk' · V_kxk'.T = X''_nxk X_restructed = X_transformed.dot(self.axis.T) # 还原数据 X_restructed = X_restructed + self.meanVal return X_restructed 实验结果 Demo1: PCA applied on 2-d datasets Demo2: PCA applied on wild face origin reduced restructured]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Activate Functions]]></title>
    <url>%2F2018%2F10%2F20%2FActivate-Functions%2F</url>
    <content type="text"><![CDATA[SigAI 理解神经网络的激活函数机器学习笔记：形象的解释神经网络激活函数的作用是什么？ - 不说话的汤姆猫 - 博客园 激活函数的作用复合函数神经网络可以看作一个多层复合函数，以下图隐含层的激活函数为例，讲解其非线性作用。 记激活函数为$\sigma(·)$，上图神经网络各层间具有如下关系 a = \sigma(w^{(1)}_{11}x + w^{(1)}_{12}y + b^{(1)}_1)b = \sigma(w^{(1)}_{21}x + w^{(1)}_{22}y + b^{(1)}_2)c = \sigma(w^{(1)}_{31}x + w^{(1)}_{32}y + b^{(1)}_3)输出层采用线性单元 A = w^{(2)}_{1}a + w^{(2)}_{2}b + w^{(2)}_{3}c + b^{(2)} 为便于作图，固定参数 W^{(1)} = \left[ \begin{matrix} 1 & 1 \\ 0.1 & -1 \\ 1 & -1 \end{matrix} \right], b^{(1)} = \left[ \begin{matrix} -2 \\ 1.5 \\ -1 \end{matrix} \right] W^{(2)} = \left[ \begin{matrix} 1 & 2 & 3 \end{matrix} \right], b^{(2)} = \left[ \begin{matrix} -1 \end{matrix} \right] 线性单元作为激活函数 此时神经网络的输出为 A = (x + y - 2) + 2 (0.1x - y + 1.5) + 3 (x - y - 1)- 1 可见仍为线性函数，做出图像如下所示 非线性单元作为激活函数 此时神经网络的输出为 A = \sigma(x + y - 2) + 2 \sigma(0.1x - y + 1.5) + 3 \sigma(x - y - 1)- 1 激活函数选择Sigmoid，做出图像如下所示 分割平面神经网络可实现逻辑运算，各个神经元视作分割超平面时，可分割出不同形状的平面，在线性和非线性激活函数时分割效果如图。当神经元组合的情况更复杂时，表达能力就会更强。 激活函数的性质已经证明，只要激活函数选择得当，神经元个数足够多，使用3层即包含一个隐含层的神经网络就可以实现对任何一个从输入向量到输出向量的连续映射函数的逼近，这个结论称为万能逼近（universal approximation）定理。 如果$\varphi(x)$是一个非常数、有界、单调递增的连续函数，$I_{m}$是$m$维的单位立方体，$I_{m}$中的连续函数空间为$C(I_{m})$。对于任意$\varepsilon&gt;0$以及函数$f\in C(I_{m})$，存在整数$N$，实数$v_{i},b_{i}$，实向量$w_{i}\in R^{m}$，通过它们构造函数$F(x)$作为函数$f$的逼近： F(x) = \sum_{i=1}^N v_i \varphi(w_i^T x + b_i)对任意的$X\in I_{m}$满足： | F(x) - f(x) | < \varepsilonCybenko, G. Approximation by superpositions of a sigmoid function. Mathematics of Control, Signals, and Systems, 2, 303-314, 1989. 这个定理对激活函数的要求是必须非常数、有界、单调递增，并且连续。 神经网络的训练使用梯度下降法进行求解，需要计算损失函数对参数的梯度值，涉及到计算激活函数的导数，因此激活函数必须是可导的。实际应用时并不要求它在定义域内处处可导，只要是几乎处处可导即可。 定义$R$为一维欧氏空间，$E\subset R$是它的一个子集，$mE$为点集$E$的Lebesgue测度。如果$E$为$R$中的可测集，$f(x)$为定义在上$E$的实函数，如果存在$N\subset E$，满足：$mN=0$，对于任意的$x_{0}\in E/N$函数$f(x)$在$x_{0}$处都可导，则称$f(x)$在$E$上几乎处处可导。 如果将激活函数输入值$x$看做是随机变量，则它落在这些不可导点处的概率是$0$。在计算机实现时，因此有一定的概率会落在不可导点处，但概率非常小。 例如ReLU函数在$x=0$处不可导 f(x) = \begin{cases} x & x \geq 0 \\ 0 & x < 0 \end{cases} 常用的激活函数]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Feedforward Neural Network]]></title>
    <url>%2F2018%2F10%2F20%2FFeedforward-Neural-Network%2F</url>
    <content type="text"><![CDATA[前言前馈神经网络是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一，既可以用于解决分类问题，也可以用于解决回归问题。 简介前馈神经网络也叫作多层感知机，包含输入层，隐含层和输出层三个部分。它的目的是为了实现输入到输出的映射。 y = f(x;W)由于各层采用了非线性激活函数，神经网络具有良好的非线性特性，如下图所示。 激活函数为线性单元 激活函数为非线性单元 前馈神经网络可用于解决非线性的分类或回归问题，参数通过反向传播算法(Back Propagation)学习。 结构神经元与网络结构图单个神经元的示意图如下，输入为前一层的输出参数$X^{(l-1)}$ h_{w, b}(x) = \sigma (WX + b)$\sigma(·)$表示激活函数。 以下为典型的神经网络结构图 第一层为输入层input layer，一般不设置权值，预处理在输入网络前完成； 最后一层为输出层output layer； 其余层称为隐藏层hidden layer，隐藏层用于提取数据特征，隐藏层层数与各层神经元个数为超参数。 神经元权值取值不同，可实现不同的逻辑运算，单个超平面只能进行二元划分，利用逻辑运算可将多个超平面划分的区域拼接起来，如图 以下说明逻辑运算的实现方法其中 f(z) = \begin{cases} 1 & z \geq 0 \\ 0 & otherwise \end{cases} 与运算 $a ∧ b$ w_1 = 20, w_2 = 20, b = -30 或运算 $a ∧ b$ w_1 = 20, w_2 = 20, b = -10 非运算 $a = \overline{b}$ w_1 = -20, w_2 = 0, b = 0 异或运算 $a \bigoplus b$，可通过组合运算实现 a \bigoplus b = (\overline{a} ∧ b) ∨ (a ∧ \overline{b}) 激活函数 隐藏层的激活函数，详情可查看另一篇博文：神经网络的激活函数； 输出层的激活函数 回归问题时，采用线性单元即可 f(x) = x 分类问题时，一般有以下几种选择 单类别概率输出 即每个神经元的输出对应该类别的$0-1$分布输出，这就需要将输出值限制在$[0, 1]$内，例如 P(y=1|x )= max\{0, min\{1, z\}\} 但是可以看到，当$(w^Tx+b)$处于单位区间外时，模型的输出对它的参数的梯度都将为$0$ ，不利于网络的训练，故采用$S$形函数Sigmoid(详情) P(y=1|x ) = \frac{1}{1+e^{-(w^Tx+b)}} $(1)$ Sigmoid函数定义域为$(-\infty, \infty)$，值域为$(0, 1)$，且在整个定义域上单调递增，即为单值函数，故可将线性输出单元的结果映射到$(0, 1)$范围内；$(2)$ 在定义域上处处可导。 多类别的概率输出 即每个神经元的输出对应判别为该类别的概率，且有 \sum_{i=1}^C y_i = 1 例如 y_i = \frac{z_i}{\sum_j z_j} 但是分式求导异常麻烦，故采用Softmax函数(详情)作为输出结点的激活函数，该函数求导结果比较简洁，且可利用输出计算导数，计算量减少。 Softmax(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right] 损失函数 回归问题 常见的用于回归问题的损失函数为MSE，即 L(y, \hat{y}) = \frac{1}{2M} \sum_{i=1}^M (\hat{y}^{(i)} - y^{(i)})^2 分类问题 一般采用交叉熵作为损失函数，如下 L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M 1\{y^{(i)}_j=k\} \log (\hat{y}^{(i)}_j) 1\{y^{(i)}_j=k\} = \begin{cases} 1 & y^{(i)}_j = k \\ 0 & y^{(i)}_j \neq k \end{cases} j = 1, ..., N 或者 L(\hat{y}, y) = - \frac{1}{M} \sum_{i=1}^M y^{(i)T} \log (\hat{y}^{(i)}) 其中$y^{(i)}, \hat{y}^{(i)}$均表示向量，采用one-hot编码。 梯度推导以上内容网上资料一大堆，进入重点，反向传播时的梯度推导，给出网络结构如下。 回归与分类在输出层有所区别； 各层激活函数的输入变量以$z^{(l)}$表示，输出变量均以$x^{(l)}$表示； $W^{(l)}$表示从第$l$层到第$(l+1)$层的权值矩阵，则$w^{(l)}_{ij}$表示第$l$层第$j$个神经元到$(l+1)$层第$i$个神经元的连接权值； $b^{(l)}$表示第$l$层到第$(l+1)$层的偏置，则$b^{(l)}_i$表示到第$(l+1)$层第$i$个神经元的偏置值； 各层变量维度推广为输入$d_{i}$，中间层$d_{h}$，输出层$d_{o}$； 全连接，部分线条已省略，激活函数已省略； 则各层参数矩阵为 W^{(1)} = \left[ \begin{matrix} w^{(1)}_{11} & ... & w^{(1)}_{1d_i} \\ ... & ... & ... \\ w^{(1)}_{d_h1} & ... & w^{(1)}_{d_hd_i} \end{matrix} \right] b^{(1)} = \left[ \begin{matrix} b^{(1)}_{1} \\ ... \\ b^{(1)}_{d_h} \end{matrix} \right] W^{(2)} = \left[ \begin{matrix} w^{(2)}_{11} & ... & w^{(2)}_{1d_h} \\ ... & ... & ... \\ w^{(2)}_{d_o1} & ... & w^{(2)}_{d_od_h} \end{matrix} \right] b^{(2)} = \left[ \begin{matrix} b^{(2)}_{1} \\ ... \\ b^{(2)}_{d_o} \end{matrix} \right]有 Z^{(2)} = W^{(1)} X^{(1)} + b^{(1)} X^{(2)} = \sigma_1 (Z^{(2)}) Z^{(3)} = W^{(2)} X^{(2)} + b^{(2)} X^{(3)} = \sigma_2 (Z^{(3)}) X^{(1)} = X \hat{Y} = X^{(3)}回归问题损失函数采用MSE，即 L(Y, \hat{Y}) = \frac{1}{M} \sum_{i=1}^M L(Y^{(i)}, \hat{Y}^{(i)}) L(Y^{(i)}, \hat{Y}^{(i)}) = \frac{1}{2} || \hat{Y}^{(i)} - Y^{(i)} ||_2^2 = \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}^{(i)}_{d_2} - y^{(i)}_{d_2})^2下面推导单个样本的损失函数的梯度，该批数据的梯度为均值。 省略样本标记$^{(i)}$ 隐含层到输出层 对权值矩阵的梯度 \frac{∂L}{∂w^{(2)}_{ij}} = \frac{∂}{∂w^{(2)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2} \tag{1} 其中 \begin{cases} \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\ z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \end{cases} 且 \frac{∂}{∂w^{(2)}_{ij}} \hat{y}_{d_2} = \sigma_2' (z_{d_2}^{(3)}) \frac{∂z_{d_2}^{(3)}}{∂w^{(2)}_{ij}} \tag{2} \frac{∂}{∂w^{(2)}_{ij}} z_{d_2}^{(3)} = \begin{cases} x^{(2)}_{d_1} & d_1 = j, d_2 = i \\ 0 & otherwise \end{cases} \tag{3} $(3)$代入$(2)$，再代入$(1)$可得到 \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) x^{(2)}_{d_1} | _{d_1=j, d_2=i} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \tag{*1} 对偏置矩阵的梯度 \frac{∂L}{∂b^{(2)}_i} = \frac{∂}{∂b^{(2)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(2)}_i} \hat{y}_{d_2} \tag{4} 其中 \begin{cases} \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\ z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \end{cases} 有 \frac{∂}{∂b^{(2)}_i} z_{d_2}^{(3)} = \begin{cases} 1 & d_2 = i \\ 0 & otherwise \end{cases} \tag{5} 所以 \frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) | _{d_2=i} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \tag{*2} 输入层到隐含层 对权值矩阵的梯度 \frac{∂L}{∂w^{(1)}_{ij}} = \frac{∂}{∂w^{(1)}_{ij}} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2} \tag{6} 其中 \begin{cases} \hat{y}_{d_2} = \sigma_2 (z_{d_2}^{(3)}) \\ z_{d_2}^{(3)} = \sum_{d_1=1}^{d_h} w^{(2)}_{d_2 d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2} \\ x^{(2)}_{d_1} = \sigma_1 (z_{d_1}^{(2)}) \\ z_{d_1}^{(2)} = \sum_{d_0=1}^{d_i} w^{(1)}_{d_1 d_0} x^{(1)}_{d_0} + b^{(1)}_{d_1} \end{cases} 故 \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2} = \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}} \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} \tag{7} 其中 \frac{∂\hat{y}_{d_2}}{∂z_{d_2}^{(3)}} = \sigma_2' (z_{d_2}^{(3)}) \tag{8} \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} = \sum_{d1=1}^{d_h} w^{(2)}_{d_2 d_1} \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} \tag{9} \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} = \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}} \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}} \tag{10} 而其中 \frac{∂x^{(2)}_{d_1}}{∂z_{d_1}^{(2)}} = x^{(2)}_{d_1} = \sigma_1' (z_{d_1}^{(2)}) \tag{11} \frac{∂z_{d_1}^{(2)}}{∂w^{(1)}_{ij}} = \begin{cases} x^{(1)}_{d_0} & d_1 = i, d_0 = j\\ 0 & otherwise \end{cases} \tag{12} $(11),(12)$代入$(10)$得到 \frac{∂x^{(2)}_{d_1}}{∂w^{(1)}_{ij}} = \sigma_1' (z_{d_1}^{(2)}) x^{(1)}_{d_0} | _{d_1 = i, d_0 = j} \tag{13} $(13)$代回$(9)$，有 \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} = \sum_{d1=1}^{d_h} \left[ w^{(2)}_{d_2 d_1} \sigma_1' (z_{d_1}^{(2)}) x^{(1)}_{d_0} \right] | _{d_1 = i, d_0 = j} = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{14} 将$(8),(14)$代入$(7)$得到 \frac{∂}{∂w^{(1)}_{ij}} \hat{y}_{d_2} = \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{15} $(15)$代入$(6)$有 \frac{∂L}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*3} 对偏置矩阵的梯度 \frac{∂L}{∂b^{(1)}_i} = \frac{∂}{∂b^{(1)}_i} \frac{1}{2} \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2})^2 = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2} \tag{16} 同理可得 \frac{∂}{∂b^{(1)}_i} \hat{y}_{d_2} = \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) \tag{17} 所以 \frac{∂L}{∂b^{(1)}_i} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) \tag{*4} 综上所述 \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_{i}^{(3)}) x^{(2)}_{j} \frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \frac{∂L}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j \frac{∂L}{∂b^{(1)}_i} = \sum_{d_2=1}^{d_o} (\hat{y}_{d_2} - y_{d_2}) \sigma_2' (z_{d_2}^{(3)}) w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)})令 \begin{cases} \delta^{(2)}_i = (\hat{y}_{i} - y_{i}) \sigma_2' (z_i^{(3)}) \\ \delta^{(1)}_i = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) \end{cases}有 \begin{cases} \frac{∂L}{∂w^{(2)}_{ij}} = \delta^{(2)}_i x^{(2)}_{j}\\ \frac{∂L}{∂b^{(2)}_i} = \delta^{(2)}_i\\ \frac{∂L}{∂w^{(1)}_{ij}} = \delta^{(1)}_i x^{(1)}_j\\ \frac{∂L}{∂b^{(1)}_i} = \delta^{(1)}_i \end{cases}至此推导完毕。 当隐藏层采用Sigmoid函数，输出层采用线性单元，可得到 \sigma_1' (z_i^{(2)}) = \sigma_1 (z_i^{(2)}) \left[1 - \sigma_1 (z_i^{(2)}) \right] = x_i^{(2)} (1 - x_i^{(2)}) \sigma_2' (z_i^{(3)}) = z_i^{(3)}此时 \begin{cases} \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_{i} - y_{i}) z_i^{(3)} x^{(2)}_{j} \\ \frac{∂L}{∂b^{(2)}_i} = (\hat{y}_{i} - y_{i}) z_i^{(3)} \\ \frac{∂L}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} x^{(1)}_j \\ \frac{∂L}{∂b^{(1)}_i} = \sum_{d_2=1}^{d_o} \delta^{(2)}_{d_2} w^{(2)}_{d_2 i} z_i^{(2)} \end{cases}可以看到，计算梯度时使用的数据在上一次前向传播时已计算得，故可减少计算量。 分类问题损失函数采用Cross Entropy，即 L(\hat{y}, y) = \frac{1}{M} \sum_{i=1}^M L(\hat{y}^{(i)}, y^{(i)}) L(\hat{y}^{(i)}, y^{(i)}) = - y^{(i)T} \log (\hat{y}^{(i)})上式中，$y^{(i)}, \hat{y}^{(i)}$均为列向量，且$y^{(i)}$表示one-hot编码后的标签向量，也可写作 L(\hat{y}^{(i)}, y^{(i)}) = - \log \hat{y}^{(i)}_{y^{(i)}} 由该式可以看出，若输出层激活函数采用Sigmoid作为激活函数，则隐藏层——输出层之间权值矩阵$W^{(2)}$只会更新$w^{(i)}_{y^{(i)} d_1}, d_1 = 1, …, d_h$； 一般采用SoftMax作为输出层激活函数，Sigmoid下面不作推导。 关于SoftMax的梯度，移步SoftMax Regression中查看详细推导过程，这里直接给出结论。对于 S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]其梯度为 \frac{∂S(x)}{∂x_i}_{K×1} = \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] - \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right] = \left( \left[ \begin{matrix} 0 \\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i省略样本标记$^{(i)}$ 隐含层到输出层 对权值矩阵的梯度 \frac{∂L}{∂w^{(2)}_{ij}} = - \frac{∂}{∂w^{(2)}_{ij}} \log \hat{y}_{y} = - \frac{1}{\hat{y}_y} \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}} \tag{18} 其中$\hat{y}_{y}$与$z^{(3)}_{d_2}(d_2 = 1, …, d_o) $均有联系，故 \frac{∂\hat{y}_{y}}{∂w^{(2)}_{ij}} = \sum_{d2=1}^{d_o} \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}} \tag{19} 而 \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} = \begin{cases} \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\ - \hat{y}_{y} \hat{y}_{d_2} & otherwise \end{cases} \frac{∂z^{(3)}_{d_2}}{∂w^{(2)}_{ij}} = \begin{cases} x^{(2)}_{d_1} & i = d_2, j = d_1 \\ 0 & otherwise \end{cases} $z^{(3)}_{d_2} = \sum_{d_1=0}^{d_h} w^{(2)}_{d_2d_1} x^{(2)}_{d_1} + b^{(2)}_{d_2}$ 代回$(19)$，再带回$(18)$，有 \frac{∂L}{∂w^{(2)}_{ij}} = - \frac{1}{\hat{y}_{y}} \sum_{d_2=1}^{d_o} \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} x^{(2)}_{d_1} | _{d_2=i, d_1=j} = \begin{cases} - \frac{1}{\hat{y}_{y}} \hat{y}_{y} (1 - \hat{y}_i) x^{(2)}_j & i = y \\ - \frac{1}{\hat{y}_{y}} (- \hat{y}_{y} \hat{y}_i) x^{(2)}_j & otherwise \end{cases} = \begin{cases} (\hat{y}_i - 1) x^{(2)}_j & i = y \\ \hat{y}_i x^{(2)}_j & otherwise \end{cases} 即 \frac{∂L}{∂w^{(2)}_{ij}} = (\hat{y}_i - y_i) x^{(2)}_j \tag{*5} 对偏置矩阵的梯度 \frac{∂L}{∂b^{(2)}_i} = \hat{y}_i - y_i \tag{*6} 输入层到隐含层 对权值矩阵的梯度 \frac{∂L}{∂w^{(1)}_{ij}} = - \frac{∂}{∂w^{(1)}_{ij}} \log \hat{y}_{y} = - \frac{1}{\hat{y}_{y}} \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}} \tag{20} 其中 \frac{∂\hat{y}_{y}}{∂w^{(1)}_{ij}} = \sum_{d_2=1}^{d_o} \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}} \tag{21} $\frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}}$部分与回归相同，有 \frac{∂z_{d_2}^{(3)}}{∂w^{(1)}_{ij}} = w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j 由上面分析可得 \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} = \begin{cases} \hat{y}_{y} (1 - \hat{y}_{d_2}) & d_2 = y \\ - \hat{y}_{y} \hat{y}_{d_2} & otherwise \end{cases} 故代回$(20)$可得到 \frac{∂L}{∂w^{(1)}_{ij}} = - \frac{1}{\hat{y}_{y}} \sum_{d_2=1}^{d_o} \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} \frac{∂z^{(3)}_{d_2}}{∂w^{(1)}_{ij}} = - \frac{1}{\hat{y}_{y}} \sum_{d_2=1}^{d_o} \frac{∂\hat{y}_{y}}{∂z^{(3)}_{d_2}} w^{(2)}_{d_2 i} \sigma_1' (z_i^{(2)}) x^{(1)}_j = \left[ \sum_{d_2=1, d_2 \neq y}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} + (\hat{y}_y - 1) w^{(2)}_{y i} \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j = \left[ \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} - w^{(2)}_{y i} \right] \sigma_1' (z_i^{(2)}) x^{(1)}_j \tag{*7} 对偏置矩阵的梯度 \frac{∂L}{∂b^{(1)}_i} = \left[ \sum_{d_2=1}^{d_o} \hat{y}_{d_2} w^{(2)}_{d_2 i} - w^{(2)}_{y i} \right] \sigma_1' (z_i^{(2)}) \tag{*8} 至此推导完毕。 这个推导，仅供参考 过拟合问题和其他算法一样，前馈神经网络也存在过拟合的问题，解决方法有以下几种 正则化 与线性回归类似，神经网络也可以加入范数惩罚项，以下$C$表示普通的损失函数，$\lambda$为惩罚系数，$n$为样本数目，$w$表示权值参数。 L1正则化 惩罚项为网络所有权值的绝对值之和。 C = C_0 + \frac{\lambda}{n} \sum_w |w| L2正则化 又称权值衰减weights decay，惩罚项为网络所有权值的平方和。 C = C_0 + \frac{\lambda}{2n} \sum_w w^2 Dropout 以概率大小为p使部分神经元输出值直接为0，如此可以使反向传播时相关权值系数不做更新，只有被保留下来的权值和偏置值会被更新。 增加训练数据大小 可在原数据上加以变换或噪声，图像的扩增方法可查看图像数据集扩增。 程序@Github: Code of Neural Network 使用PyTorch实现神经网络，以下为模型定义123456789101112131415class AnnNet(nn.Module): def __init__(self): super(AnnNet, self).__init__() self.input_size = 28 * 28 self.hidden_size = 100 self.output_size = 10 self.fc1 = nn.Linear(self.input_size, self.hidden_size) # input - hidden self.fc2 = nn.Linear(self.hidden_size, self.output_size ) # hidden - output # self.activate = nn.Sigmoid() # 参数更新非常慢，特别是层数多时 self.activate = nn.ReLU() # 事实证明ReLU作为激活函数更加合适 self.softmax = nn.Softmax() def forward(self, X): h = self.activate(self.fc1(X)) y_pred = self.softmax(self.fc2(h)) return y_pred]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分类问题的决策平面]]></title>
    <url>%2F2018%2F10%2F19%2F%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%86%B3%E7%AD%96%E5%B9%B3%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[引言对于分类问题，计算结果一般为概率值，那么如何根据计算得的概率进行判别分类呢？ 这部分理解后，Logistic回归与Softmax回归的模型就很容易推得。 判别函数对于一个类别为$K$的分类问题，如果对于所有的$ i,j=1,…,K, j\neq i$，有 g_i(x) > g_j(x)则此分类器将这个样本对应的特征向量$x$判别为$w_i$，则此分类器的作用是，计算$K$个判别函数并选取与最大判别值最大对应的类别。 判别函数的形式并不唯一，可以将所有的判别函数乘上相同的正常数或者加上一个相同的常量而不影响其判决结果。更一般的情况下，我们使用单调递增函数$f(·)$进行映射，将每一个$g_i(x)$替换成$f(g_i(x))$，分类结果不变。 ——《模式识别原理与应用课程笔记》 例如最小风险贝叶斯决策 正态分布下的判别函数 多元高斯分布（The Multivariate normal distribution） - bingjianing - 博客园 由大数定理可知，在样本足够的情况下，数据服从正态分布。多元正态分布形式如下 f(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))其中 x = [x_1, ..., x_n]^T\mu = [\mu_1, ..., \mu_n]^T\Sigma_{ij} = cov(x_i, x_j) 在最小错误率判别时 g_i(x) = P(x|c_i)P(c_i)即 g_i(x) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i)) · P(c_i)取对数运算，并舍去常数项，展开整理得 g_i(x) = -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i) \tag{0} 注： 协方差矩阵 $ \Sigma^T = \Sigma $ 独立同分布: $\Sigma_i = \sigma^2 I$$\Sigma_i^{-1} = \frac{1}{\sigma^2} I$代入$(0)$，有 g_i(x) = \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)\tag{1}定义 w_i = \frac{1}{\sigma^2}\mu_i^Tw_0 = - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i)有一般形式如下，表示取$c_i$的概率 g_i(x) = w_i x + w_0\tag{2}设决策平面为 w^T (x−x_0)=0\tag{3}决策平面上，取$c_i$和$c_j$的概率相等，即 g_i(x) = g_j(x)可得 (\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} \tag{4} 推导过程如下，将$(1)$代入上式$ \frac{1}{\sigma^2}\mu_i^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_i ^T \mu_i) + ln P(c_i) = \frac{1}{\sigma^2}\mu_j^T x - \frac{1}{2\sigma^2} (x^Tx + \mu_j ^T \mu_j) + ln P(c_j) $$ \mu_i^T x - \frac{1}{2} \mu_i ^T \mu_i + ln P(c_i) = \mu_j^T x - \frac{1}{2} \mu_j ^T \mu_j + ln P(c_j) $$ (\mu_i - \mu_j)^Tx = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)} $ 由$(3)$$(4)$，利用待定系数法，可得 w = \mu_i - \mu_j w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j) -ln \frac{P(c_i)}{P(c_j)}特别地，当等先验概率时，即$P(c_i) = P(c_j)$时 w^T x_0 = \frac{1}{2} (\mu_i ^T \mu_i - \mu_j ^T \mu_j)故 x_0 = \frac{1}{2}(\mu_i + \mu_j)结论：等先验概率时超平面$ w^T (x−x_0)=0 $平分判别空间 $\mu_i$与$\mu_j$分别表示两个类别的中心，由向量运算，$x_0$为两类中心的连线的中点。 不独立但同分布: $\Sigma_i = \Sigma$代入$(0)$后可得 g_i(x) = \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) \tag{5}定义 w_i = \mu_i^T \Sigma ^{-1}w_0 = - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i)有一般形式如下，表示取$c_i$的概率 g_i(x) = w_i x + w_0\tag{6}同样的，设决策平面为 w^T (x−x_0)=0\tag{7}决策平面上，取$c_i$和$c_j$的概率相等，即 g_i(x) = g_j(x)有 (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i - \mu_j)^T \Sigma ^{-1} (\mu_i - \mu_j) - ln \frac{P(c_i)}{P(c_j)} $ \mu_i^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x - \frac{1}{2}x^T \Sigma ^{-1} x -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $$ \mu_i^T \Sigma ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma ^{-1} \mu_i + ln P(c_i) = \mu_j^T \Sigma ^{-1} x -\frac{1}{2} \mu_j ^T \Sigma ^{-1} \mu_j + ln P(c_j) $$ (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j) - ln \frac{P(c_i)}{P(c_j)} $ 特别的，当取等先验概率时 (\mu_i - \mu_j)^T \Sigma ^{-1} x = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j)由$(7)$$(8)$，利用待定系数法 w^T = (\mu_i - \mu_j)^T \Sigma^{-1}w^T x_0 = \frac{1}{2} (\mu_i ^T \Sigma ^{-1} \mu_i + \mu_j ^T \Sigma ^{-1} \mu_j) 注： 协方差矩阵 $ \Sigma^T = \Sigma $ w = \Sigma^{-1}(\mu_i - \mu_j)x_0 = \frac{1}{2} (\mu_i + \mu_j)由于通常$w=Σ^{−1}(μ_i−μ_j)$并非朝着$(μ_i−μ_j)$的方向，因而通常分离两类的超平面也并非与均值的连线垂直正交。但是， 如果先验概率相等，其判定面确实是与均值连线交于中点$x_0$处的。如果先验概率不等，最优边界超平面将远离可能性较大的均值。同前，如果偏移量足够大，判定面可以不落在两个均值向量之间。 不独立且不同分布: $\Sigma_i = \Sigma_i(∀) $g_i(x) = -\frac{1}{2}x^T \Sigma_i ^{-1} x + \mu_i^T \Sigma_i ^{-1} x -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)定义 W_i = -\frac{1}{2} \Sigma_i ^{-1}w_i = \mu_i^T \Sigma_i ^{-1}w_0 = -\frac{1}{2} \mu_i ^T \Sigma_i ^{-1} \mu_i + ln P(c_i)有 g_i(x) = x^TW_ix + w_ix + w_0]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Bayes Decision]]></title>
    <url>%2F2018%2F10%2F18%2FBayes-Decision%2F</url>
    <content type="text"><![CDATA[原理基于贝叶斯公式 P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}P(x)=\sum_j p(x|c_j)P(c_j)几种常用的贝叶斯决策最小错误率贝叶斯决策在分类问题中，我们往往希望尽可能减少分类错误，即目标是追求最小错误率。假设有$K$分类问题，由贝叶斯公式 P(c_k|x)=\frac{p(x|c_k)P(c_k)}{p(x)}上式中$ k=1,…,K $，各部分定义如下 $P(c_k|x)$——后验概率(posteriori probability)$P(c_k)$——先验概率(priori probability)，$p(x|c_k)$——$c_k$关于$x$的似然函数(likelihood)，$p(x)$——证据因子(evidence)， 证据因子由下式计算 p(x)=\sum_{j=0}^K p(x|c_j)P(c_j)以上就是从样本中训练的参数，在预测阶段，定义决策规则为 $if$ $P(c_i|x)&gt;P(c_j|x)$, $then$ $ x \in c_i $ 由于分母为标量，对于任意输入的样本特征$x$，$P(x)$一定，故决策规则可简化为 $if$ $P(x|c_i)P(c_i)&gt;P(x|c_j)P(c_j)$, $then$ $ x \in c_i $ 而对于分类错误的样本，如样本$x$属于分类$c_i$，但错误分类为$c_{err}, err \neq i$，样本的错误分类概率为 P(error|x) = P(c_{err}|x)上式被称作误差概率，某类后验概率越大，则相应的误差概率就越小，定义平均误差概率 P_{mean} = \int P(error|x)P(x)dx带有拒绝域的最小错误率贝叶斯决策一些情况下，某样本对应特征$x$计算结果中，属于各类别的概率没有显著比较大的数值，换句话说都比较小，那么对这次的判别就不太信任，选择拒绝决策结果。将决策平面划分为两个区域 Acquired = \{x|max_j P(c_j|x)\geq 1-t\}Rejected = \{x|max_j P(c_j|x) < 1-t\}其中$t$为阈值，$t$越小时，拒绝域$Rejected$越大，当满足 1-t \leq \frac{1}{K}或者 t \geq \frac{K-1}{K}此时拒绝域为 Rejected = \{x|max_j P(c_j|x) < \frac{1}{K}\}而当且仅当各分类概率相等时才有 $ max_j P(c_j|x) = \frac{1}{K} $，因此此时拒绝域为空，接受所有决策结果 最小风险贝叶斯决策在决策过程中，不同类型的决策错误所产生的代价是不同的。引入风险函数 \lambda_{i, j} = \lambda (\alpha_i|c_j)表示实际类别为$c_j$时，采取错误判断为$c_i$的行为$\alpha_i$所产生的损失。该函数称为损失函数，通常它可以用表格的形式给出，叫做决策表，形如定义条件风险 R(\alpha_i|c_j) = \sum_j \lambda (\alpha_i|c_j) P(c_j|x)特别地，取$0-1$损失时，即最小错误率贝叶斯决策 \lambda (\alpha_i|c_j) = \begin{cases} 0 & i = j \\ 1 & i \neq j \end{cases} 可能比较抽象，这里举了一个例子 关于判别函数可查看分类问题的决策平面 程序 为帮助理解，先手动计算一遍结果 先验概率(priori probability):$ P(Y = -1) = \frac{6}{15} $$ P(Y = 1) = \frac{9}{15} $似然函数(likelihood)$ P(X^{(1)} = 1|Y=-1) = \frac{3}{6}$$ P(X^{(1)} = 2|Y=-1) = \frac{2}{6}$$ P(X^{(1)} = 3|Y=-1) = \frac{1}{6}$$ P(X^{(2)} = S|Y=-1) = \frac{3}{6}$$ P(X^{(2)} = M|Y=-1) = \frac{2}{6}$$ P(X^{(2)} = L|Y=-1) = \frac{1}{6}$$ P(X^{(1)} = 1|Y=1) = \frac{2}{9}$$ P(X^{(1)} = 2|Y=1) = \frac{3}{9}$$ P(X^{(1)} = 3|Y=1) = \frac{4}{9}$$ P(X^{(2)} = S|Y=1) = \frac{1}{9}$$ P(X^{(2)} = M|Y=1) = \frac{4}{9}$$ P(X^{(2)} = L|Y=1) = \frac{4}{9}$ 注意：证据因子(evidence)不能用如下朴素贝叶斯求解 P(X) = P(X^{(1)}) P(X^{(2)})而是 P(X) = P(X^{(1)}|Y=-1)P(Y = -1) + P(X^{(2)}|Y=-1)P(Y = -1)一般分子用朴素贝叶斯求解 P(X|Y) = P(X^{(1)}|Y) P(X^{(2)}|Y)将其加和作为分母 c_k: P(X)_k = \sum_{k=0}^2 P(X^{(1)}|Y=k) P(X^{(2)}|Y=k)P(Y_k|X) = \frac{P(X|Y_k)P(Y_k)}{P(X)_k}选取最大概率的$ k $类别作为判别类别 k = argmax_k P(Y_k|X)代码@Github: Code for Naive Bayes Decision training step12345678910def fit(self, X, y): X_encoded = self.featureEncoder.fit_transform(X).toarray() y_encoded = OneHotEncoder().fit_transform(y.reshape((-1, 1))).toarray() self.P_X = np.mean(X_encoded, axis=0) # one-hot编码下，各列的均值即各特征的概率 self.P_Y = np.mean(y_encoded, axis=0) # one-hot编码下，各列的均值即各了别的概率 self.n_labels, self.n_features = y_encoded.shape[1], X_encoded.shape[1] self.P_X_Y = np.zeros(shape=(self.n_labels, self.n_features)) # 各个类别下，分别统计各特征的概率 for i in range(self.n_labels): X_encoded_of_yi = X_encoded[y_encoded[:, i]==1] # 取出属于i类别的样本 self.P_X_Y[i] = np.mean(X_encoded_of_yi, axis=0) # one-hot编码下，各列的均值即各特征的概率 predict step1234567891011def predict(self, X): X_encoded = self.featureEncoder.transform(X).toarray() n_samples = X_encoded.shape[0] y_pred_prob = np.zeros(shape=(n_samples, self.n_labels)) for i in range(n_samples): for j in range(self.n_labels): P_Xi_encoded_Yj = X_encoded[i] * self.P_X_Y[j] # 在Yj类别下，选出输入样本Xi对应的条件概率 P_Xi_encoded_Yj[P_Xi_encoded_Yj==0.0] = 1.0 # 将为0值替换为1，便于求解ΠP(Xi|yc)，只要将各元素累乘即可 y_pred_prob[i, j] = self.P_Y[j] * P_Xi_encoded_Yj.prod() y_pred_prob[i] /= np.sum(y_pred_prob[i]) # 分母一般是将分子加和，不能假定各特征独立并用朴素贝叶斯计算分母 return np.argmax(y_pred_prob, axis=1) main123456789101112X = [ [1, 0], [1, 1], [1, 1], [1, 0], [1, 0], [2, 0], [2, 1], [2, 1], [2, 2], [2, 2], [3, 2], [3, 1], [3, 2], [3, 2], [3, 2]]y = [0 ,0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]estimator = NaiveBayes()estimator.fit(X, y)X_test = np.array([[2, 0], [1, 1]])y_pred = estimator.predict(X_test)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Softmax Regression]]></title>
    <url>%2F2018%2F10%2F18%2FSoftmax-Regression%2F</url>
    <content type="text"><![CDATA[Unsupervised Feature Learning and Deep Learning Tutorial 引言Logistic Regression中采用的非线性函数为Sigmoid，将输出值映射到$(0, 1)$之间作为概率输出，处理的是二分类问题，那么对于多分类的问题怎么处理呢？ 模型 由Logistic回归推广而来 SoftmaxSoftmax在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类$(K&gt;2)$问题，分类器最后的输出单元需要Softmax函数进行数值处理。 S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]其中$x$为矩阵形式的向量，其维度为$(K×1)$，$K$为类别数目。Softmax的输出向量维度与$x$相同，各元素$x_i$加和为$1$，可用于表示取各个类别的概率。 注意到，对于函数$e^x$ \lim_{x \rightarrow - \infty} e^x = 0\lim_{x \rightarrow + \infty} e^x = +\infty 假设所有的$x_i$等于某常数$c$，理论上对所有$x_i$上式结果为$\frac{1}{n}$ 若$c$为很小的负数，$e^c$下溢，结果为$NaN$； 若$c$量级很大，$e^c$上溢，结果为$NaN$。 在数值计算时并不稳定，但是Softmax所有输入增加同一常数时，输出不变，得稳定版本： S(x) := S(x - max(x_i)) e^{x_{max} - max(x_i)} = 1 减去最大值导致$e^x$最大为$1$，排除上溢； 分母中至少有一项为$1$，排除分母下溢导致处以$0$的情况。 其对数 log S(x)_i = x_i - log ({\sum_{k=1}^K exp(x_k)}) 注意到，第一项表示输入$x_i$总是对代价函数有直接的贡献。这一项不会饱和，所以即使$x_i$对上式的第二项的贡献很小，学习依然可以进行； 当最大化对数似然时，第一项鼓励$x_i$被推高，而第二项则鼓励所有的$x$被压低； 第二项$log ({\sum_{k=1}^K exp(x_k)})$可以大致近似为$max(x_k)$，这种近似是基于对任何明显小于$max(x_k)$的$x_k$都是不重要的，负对数似然代价函数总是强烈地惩罚最活跃的不正确预测 除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习 作者：NirHeavenX来源：CSDN原文：https://blog.csdn.net/qsczse943062710/article/details/61912464版权声明：本文为博主原创文章，转载请附上博文链接！ Softmax解决多分类问题对于具有$K$个分类的问题，每个类别训练一组参数$ w_k $ z_k^{(i)} = w_k^Tx^{(i)}或写作矩阵形式 z^{(i)} = W^Tx^{(i)}其中 x^{(i)} = \left[ \begin{matrix} x_0^{(i)}\\ x_1^{(i)}\\ ...\\ x_n^{(i)} \end{matrix} \right]_{n×1}, x_0^{(i)}=1 W = [w_1, w_2, ..., w_K]_{(n+1)×K} w_i = \left[ \begin{matrix} w_{i0}\\ w_{i1}\\ ...\\ w_{in} \end{matrix} \right]_{n×1}最终各类别输出概率为 \hat{y}^{(i)} = Softmax(z^{(i)}) 产生了一个奇怪的脑洞。。。二分类问题 p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} } = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } }定义二分类线性单元输出的差值为 z = x_1 - x_2得到 p(x_1) = \frac{1}{1 + e^{-z}}以$x_1 = [x_{11}, x_{12}]^T$为例(二维特征)，取$w_1=1, w_2=2, b=3$ p(x_1) = \frac{1}{1 + e^{-(w_1 x_{11} + w_2 x_{12} + b)}} 而多分类问题，以$3$分类为例 p(x_1) = \frac{ e^{x_1} }{ e^{x_1} + e^{x_2} + e^{x_3}} = \frac{ 1 }{ 1 + e^{ - (x_1 - x_2) } + e^{ - (x_1 - x_3)} }定义线性单元输出的差值为 z_{12} = x_1 - x_2 z_{13} = x_1 - x_3 p(x_1) = \frac{ 1 }{ 1 + e^{ - z_{12} } + e^{ - z_{13}} }做出图像为 损失函数由交叉熵理解CrossEnt = \sum_j p_j log \frac{1}{q_j}而对于样本$ (X^{(i)}, y^{(i)}) $，为确定事件，故标签概率各元素的取值$p_j$为$ y^{(i)}_j ∈ {0,1}$，$ q_j即预测输出的概率值\hat{y}^{(i)}_j$ 一般取各个样本损失的均值$(\frac{1}{N})$ L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N 1\{y^{(i)}_j=k\} log (\hat{y}^{(i)}_j) 1\{y^{(i)}_j=k\} = \begin{cases} 1 & y^{(i)}_j = k \\ 0 & y^{(i)}_j \neq k \end{cases}可对实际标签$y^{(i)}$采取One-Hot编码，便于计算 y^{(i)} = \left[ \begin{matrix} 0, ..., 1_{y^{(i)}}, ..., 0 \end{matrix} \right]^T则 L(\hat{y}, y) = - \frac{1}{N} \sum_{i=1}^N y^{(i)T} log (\hat{y}^{(i)}) 实际上，由熵定义 H(p) = \sum_x p(x) \log \frac{1}{p(x)}交叉熵为 H(p, q) = \sum_x p(x) \log \frac{1}{q(x)}K-L散度为 D_{KL}(p || q) = \sum_x p(x) \log \frac{p(x)}{q(x)}也即 D_{KL}(p || q) = H(p, q) - H(p)常常用于衡量两个概率分布$p(x), q(x)$之间的差异。而对于固定的数据集，$H(p)$为常熟，故最小化交叉熵$H(p, q)$实际上为最小化K-L散度$D_{KL}(p || q)$。 由决策平面理解从贝叶斯决策和分类问题的决策平面可知，对于类别$c_i$，有 P(c_i|x) = \frac{P(x|c_i)}{\sum_{j=0}^KP(x|c_j)} 假设每个类别的样本服从正态分布，先验概率相等，各类别样本特征间协方差相等。证明略. 梯度推导Softmax函数的导数对于 S(x) = \frac {1} {\sum_{k=1}^K exp(x_k)} \left[ \begin{matrix} exp(x_1)\\ exp(x_2)\\ ...\\ exp(x_K) \end{matrix} \right]一般输出作为概率值，记 P = S(x)p_i = S(x)_i对向量$x$中某元素求导 \frac{∂S(x)}{∂x_i} = \frac{∂}{∂x_i} \left[ \begin{matrix} ...\\ \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}\\ ...\\ \end{matrix} \right] $(1)$ $i=k$$\frac{∂}{∂x_i} \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{exp’(x_i)·\sum_{j=1}^K exp(x_j) - exp(x_i)·(\sum_{j=1}^K exp(x_j))’}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)·\sum_{j=1}^K exp(x_j) - exp^2(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)} -(\frac{exp(x_i)}{\sum_{j=1}^K exp(x_j)})^2$$ = p_i (1 - p_i)$ $(2)$ $i\neq k$$\frac{∂}{∂x_i} \frac{exp(x_k)}{\sum_{j=1}^K exp(x_j)}$$ = \frac{exp’(x_k)·\sum_{j=1}^K exp(x_j) - exp(x_k)·(\sum_{j=1}^K exp(x_j))’}{(\sum_{j=1}^K exp(x_j))^2}$$ = \frac{- exp(x_k)exp(x_i)}{(\sum_{j=1}^K exp(x_j))^2}$$= - p_i p_k$ 综上 \frac{∂S(x)}{∂x_i}_{K×1} = \left[ \begin{matrix} 0\\ ...\\ p_i\\ ...\\ 0 \end{matrix} \right] - \left[ \begin{matrix} p_i p_1\\ ...\\ p_i^2\\ ...\\ p_i p_K \end{matrix} \right] = \left( \left[ \begin{matrix} 0\\ ...\\ 1\\ ...\\ 0 \end{matrix} \right] - p \right)p_i 损失函数梯度在OneHot编码下，损失函数形式为 L(\hat{y},y) = \frac{1}{N} \sum_{i=1}^N L (y^{(i)}, \hat{y}^{(i)}) L (y^{(i)}, \hat{y}^{(i)}) = - y^{(i)T} log \hat{y}^{(i)} \hat{y}^{(i)} = S(z^{(i)}) z^{(i)} = W^T x^{(i)}即只考虑实际分类对应的概率值 L (y^{(i)}, \hat{y}^{(i)}) = - log \hat{y}^{(i)}_{y^{(i)}} 由于 $S(z^{(i)})_{t^{(i)}}$与$z^{(i)}$向量各个元素都有关，由链式求导法则 \frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } ( \sum_{k=1}^K \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} \frac{∂z^{(i)}_k}{∂w_{pq}} )$1.$ 考察 $\frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k}$ \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_k} = ​ \begin{cases} ​ \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_k) & k=y^{(i)} \\ ​ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_k & k \neq y^{(i)} ​ \end{cases}$2.$ 考察 $\frac{∂z^{(i)}_k}{∂w_{pq}}$ \frac{∂z^{(i)}_k}{∂w_{pq}} = \begin{cases} \frac{∂z^{(i)}_k}{∂w_{pq}} = x^{(i)}_p & k=q\\ \frac{∂z^{(i)}_k}{∂w_{pq}} = 0 & k \neq q \end{cases} 综上所述 \frac{∂ L^{(i)} }{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q} \frac{∂z^{(i)}_q}{∂w_{pq}}其中 \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q} = \begin{cases} \hat{y}^{(i)}_{y^{(i)}} (1 - \hat{y}^{(i)}_q) & q = y^{(i)}\\ - \hat{y}^{(i)}_{y^{(i)}} \hat{y}^{(i)}_q & q \neq y^{(i)} \end{cases} \frac{∂z^{(i)}_q}{∂w_{pq}} = x^{(i)}_p故对于单个样本$(X^{(i)}, y^{(i)})$，当样本标签采用$OneHot$编码时 \frac{∂L^{(i)}}{∂w_{pq}} = - \frac{1}{ \hat{y}^{(i)}_{y^{(i)}} } \frac{∂ \hat{y}^{(i)}_{y^{(i)}} }{∂z^{(i)}_q} x^{(i)}_p = \begin{cases} (\hat{y}^{(i)}_q - 1)x^{(i)}_p & q = y^{(i)}\\ \hat{y}^{(i)}_qx^{(i)}_p & q \neq y^{(i)} \end{cases} 注： 这里可以约分去掉$\hat{y}^{(i)}_{y^{(i)}}$ \frac{∂L^{(i)}}{∂w_{pq}} = ( \hat{y}^{(i)}_q - y^{(i)}_q) x^{(i)}_p更一般的，写成矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量) ∇_W L = X^T(\hat{Y} - Y) 用线性模型解决分类和回归问题时，形式竟如此统一! 至此为止，梯度推导结束，利用梯度下降法迭代求解参数矩阵$W$即可。 W := W - \alpha ∇_W L代码@GitHub: Code of Softmax Regression Softmax12345678def softmax(X): """ 数值计算稳定版本的softmax函数 @param &#123;ndarray&#125; X: shape(batch_size, n_labels) """ X_max = np.max(X, axis=1).reshape((-1, 1)) # 每行的最大值 X = X - X_max # 每行减去最大值 X = np.exp(X) return X / np.sum(X, axis=1).reshape((-1, 1)) cost function1234567891011def crossEnt(self, y_label_true, y_prob_pred): """ 计算交叉熵损失函数 @param &#123;ndarray&#125; y_label_true: 真实标签 shape(batch_size,) @param &#123;ndarray&#125; y_prob_pred: 预测输出 shape(batch_size, n_labels) """ mask = self.encoder.transform(y_label_true.reshape(-1, 1)).toarray() # shape(batch_size, n_labels) y_prob_masked = np.sum(mask * y_prob_pred, axis=1) # 每行真实标签对应的预测输出值 y_prob_masked[y_prob_masked==0.] = 1. y_loss = np.log(y_prob_masked) loss = - np.mean(y_loss) # 求各样本损失的均值 return loss gradient12345678910def grad(self, X_train, y_train, y_prob_pred): """ 计算梯度 \frac &#123;∂L&#125; &#123;∂W_&#123;pq&#125;&#125; @param X_train: 训练集特征 @param y_train: 训练集标签 @param y_prob_pred: 训练集预测概率输出 @param y_label_pred: 训练集预测标签输出 """ y_train = self.encoder.transform(y_train) dW = X_train.T.dot(y_prob_pred - y_train) return dW training step省略可视化和验证部分的代码123456789101112131415161718192021222324252627282930313233343536def fit(self, X_train, X_valid, y_train, y_valid, min_acc=0.95, max_epoch=20, batch_size=20): """ 训练 """ # 添加首1列，输入到偏置w0 X_train = np.c_[np.ones(shape=(X_train.shape[0],)), X_train] X_valid = np.c_[np.ones(shape=(X_valid.shape[0],)), X_valid] X_train = self.scaler.fit_transform(X_train) # 尺度归一化 X_valid = self.scaler.transform(X_valid) # 尺度归一化 self.encoder.fit(y_train.reshape(-1, 1)) self.n_features = X_train.shape[1] self.n_labels = self.encoder.transform(y_train).shape[1] # 初始化参数 self.W = np.random.normal(loc=0, scale=1.0, size=(self.n_features, self.n_labels)) n_batch = X_train.shape[0] // batch_size # 可视化相关 plt.ion() plt.figure('loss'); plt.figure('accuracy') loss_train_epoch = []; loss_valid_epoch = [] acc_train_epoch = []; acc_valid_epoch = [] for i_epoch in range(max_epoch): for i_batch in range(n_batch): # 批处理梯度下降 n1, n2 = i_batch * batch_size, (i_batch + 1) * batch_size X_train_batch, y_train_batch = X_train[n1: n2], y_train[n1: n2] # 预测 y_prob_train = self.predict(X_train_batch, preprocessed=True) # 计算损失 loss_train_batch = self.crossEnt(y_train_batch, y_prob_train) # 计算准确率 y_label_train = np.argmax(y_prob_train, axis=1) a = y_train_batch.reshape((-1,)) acc_train_batch = np.mean((y_label_train == y_train_batch.reshape((-1,))).astype('float')) # 计算梯度 dW dW = self.grad(X_train_batch, y_train_batch, y_prob_train) # 更新参数 self.W -= self.lr * dW predict step123456789101112def predict(self, X, preprocessed=False): """ 对输入的样本进行预测，输出标签 @param &#123;ndarray&#125; X: shape(batch_size, n_features) @return &#123;ndarray&#125; y_prob: probability, shape(batch_size, n_labels) &#123;ndarray&#125; y_label: labels, shape(batch_size,) """ if not preprocessed: # 训练过程中调用此函数时，不用加首1列 X = np.c_[np.ones(shape=(X.shape[0],)), X] # 添加首1项，输入到偏置w0 X = self.scaler.transform(X) y_prob = softmax(X.dot(self.W)) # 预测概率值 shape(batch_size, n_labels) return y_prob 实验结果以下蓝线为训练集参数，红线为验证集参数，若稳定训练(如batch_size = 20的结果)，最终准确率在$80\%$左右。 由于随机梯度下降(SGD)遍历次数太多，运行较慢，没有用SGD方法训练，就前几个epoch来看，效果没有batch_size = 20的好； 添加隐含层形成三层结构的前馈神经网络，可提高准确率； 还有一点，使用批处理梯度下降(n_batch = 1)训练时，可以看到损失值已经趋于$0$，但准确率却很低，说明已经陷入局部最优解。 batch size = 20 损失 准确率 batch_size = 200 损失 准确率 n_batch = 1 损失 准确率 感悟推公式要我老命。。。。 Softmax回归可以视作不含隐含层的前馈神经网络。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[引言逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 模型先给出模型，推导过程稍后给出，逻辑回归包含Sigmoid函数 f(z) = \frac{1}{1+e^{-z}}其图像如下 定义 z = w^Tx其中$x=[x_0, x_1, …, x_n]^T, x_0=1$ h_w(x) = g(z) = \frac{1}{1+e^{-z}}损失函数由最大似然估计推导对于二元分类问题，其取值作为随机变量，服从二项分布 $B(1, p)$，其中$p$即为预测输出概率$\hat{y}$ P(y^{(i)}) = (\hat{y}^{(i)})^{y^{(i)}}(1-\hat{y}^{(i)})^{1-y^{(i)}}由极大似然估计 L = \prod_{i=1}^N P(y^{(i)}) = \prod_{i=1}^N (\hat{y}^{(i)})^{y^{(i)}}(1-\hat{y}^{(i)})^{1-y^{(i)}}取对数似然函数 logL = \sum_{i=1}^N [y^{(i)} log \hat{y}^{(i)} + (1-y^{(i)}) log (1-\hat{y}^{(i)})]优化目标是 w = argmax_w logL优化问题一般表述成minimize问题，添加负号，构成Neg Log Likelihood损失 w = argmin_w (-logL)一般取均值 L(\hat{y}, y)=- \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1 - y^{(i)})log(1-\hat{y}^{(i)})]其中$y$表示真实值，$\hat{y}$表示预测值 从交叉熵理解已知交叉熵cross entropy定义如下 CrossEnt = \sum_i p_i log \frac{1}{q_i}而对于样本$ (X_i, y_i) $，为确定事件，故标签概率的取值为$ p_i = y_i ∈ {0,1}$，$ q_i即预测输出的概率值\hat{y}_i $，可得到与上面相同的推导结论 从决策平面和贝叶斯决策理解 相关内容查看分类问题的决策平面和贝叶斯决策，逻辑回归考虑的一般是等先验概率问题，故决策函数定义为 if \quad P(c_i|x)>P(c_j|x) \quad then \quad x \in c_i, \quad i, j = 1, 2 从贝叶斯决策可知，对于类别$c_1$，有 P(c_1|x) = \frac{P(x|c_1)}{P(x|c_1) + P(x|c_2)}设在各个类别下，特征$x$服从正态分布 P(x|c_i) = \frac{1}{ (2\pi)^{\frac{n}{2}} |\Sigma_i|^{\frac{1}{2}}} exp(-\frac{1}{2} (x-\mu_i)^T \Sigma^{-1} (x-\mu_i))则 P(c_1|x) = \frac {1} { 1 + exp(-z) } P(c_2|x) = 1 - P(c_1|x) = \frac{exp(-z)}{1+exp(-z)} $P(c_1|x) = \frac{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}$ $P(c_1|x) = \frac{1}{1 + \frac{exp(-\frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)}{exp(-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}}$ 假定各分类的样本方差相等，$ \Sigma_1 = \Sigma_2 = \sigma^2 I $ $ P(c_1|x) = \frac {1}{1 + exp(- [ \frac{1}{\sigma^2} (\mu_1-\mu_2)^T x - \frac{1}{2 \sigma^2} (\mu_1^T\mu_1 - \mu_2^T\mu_2) ])}$ 令 w = \frac{1}{\sigma^2} (\mu_1 -\mu_2)b = - \frac{1}{2\sigma^2}(\mu_1^T \mu_1 - \mu_2^T \mu_2)即可得到 P(c_1|x) = \frac {1} { 1 + exp(-z) }其中 z = w^T x + b 梯度推导先推导Sigmoid函数的导数 f'(z) = (1 - f(z))f(z)值得注意的是，从$f’(z)$的图像可以看到，在$ x=0 $处$f’(z)$取极大值，且 f'(z)_{max} = f'(z)|_{z=0} = 0.25 \lim_{z \rightarrow \infty} f'(z) = 0在多层神经网络反向传播更新参数时，由于梯度多次累乘，Sigmoid作为激活函数会存在“梯度消失”的问题，使得参数更新非常缓慢。 $ f’(z) $$ = (\frac{1}{1+e^{-z}})’ $$ = \frac​ {-(1+e^{-z})’}​ {(1+e^{-z})^2} $$ = \frac​ {e^{-z}}​ {(1+e^{-z})^2} $$ = \frac​ {e^{-z}}​ {1+e^{-z}}​ \frac​ {1}​ {1+e^{-z}}$$ = (1 - f(z))f(z)$ 利用链式求导法则可得 \begin{aligned} \frac{∂L}{∂w_j} = -\frac{∂}{∂w_j} \frac{1}{N} \sum_i [y^{(i)} log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})] \\ = - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\frac{∂\hat{y}^{(i)}}{∂w_j}-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\frac{∂\hat{y}^{(i)}}{∂w_j}] \\ = - \frac{1}{N} \sum_i [y^{(i)} \frac{1}{\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})x_j-(1-y^{(i)})\frac{1}{1-\hat{y}^{(i)}}\hat{y}^{(i)}(1-\hat{y}^{(i)})x_j] \\ = - \frac{1}{N} \sum_i [y^{(i)} (1-\hat{y}^{(i)})x_j-(1-y^{(i)}) y^{(i)} x_j] \\ = \frac{1}{N} \sum_i (\hat{y}^{(i)} - y^{(i)})x_j \end{aligned}写作矩阵形式，记$X = [x_1, x_2, …, x_m]^T$，$x_i$为样本特征(列向量) ∇_w L = X^T (\hat{Y} - Y)训练和线性回归一样，采用梯度下降法求解 w := w - \alpha ∇_w L处理多分类问题假设有$K$个类别，则依次以类别$c_i$为正样本训练模型，一共训练$K$个。测试样本在每个模型上计算，最终将概率最大的作为分类结果。 这样划分数据集，会使训练集正负样本数目严重不对称，特别是类别很多的情况，对结果会产生影响。可推广至softmax回归解决这个问题。 程序代码@Github: Code for Logistic Regression cost function123456789101112131415def lossFunctionDerivative(self, X, theta, y_true): ''' 计算损失函数对参数theta的梯度 对theta[j]的梯度为：(y_pred - y_true)*x[j] ''' err = self.predict_prob(X, theta) - y_true return X.T.dot(err)/y_true.shape[0]def lossFunction(self, y_pred_prob, y_true): ''' 未使用 计算损失值: Cross-Entropy y_pred_prob, y_true: NumPy array, shape=(n,) ''' tmp = y_true*np.log(y_pred_prob) + (1 - y_true)*np.log(1 - y_pred_prob) return np.mean(-tmp) training step123456789101112131415161718192021def gradDescent(self, min_acc, learning_rate=0.01, max_iter=10000): acc = 0; n_iter = 0 for n_iter in range(max_iter): for n in range(self.n_batch): X_batch = self.X[n*self.batch_size:(n+1)*self.batch_size] t_batch = self.t[n*self.batch_size:(n+1)*self.batch_size] grad = self.lossFunctionDerivative(X_batch, self.theta, t_batch) self.theta -= learning_rate * grad # 梯度下降 acc = self.accuracyRate(self.predict_prob(self.X, self.theta), self.t) if acc &gt; min_acc: print('第%d次迭代, 第%d批数据' % (n_iter, n)) print("当前总体样本准确率为: ", acc) print("当前参数值为: ", self.theta) return self.theta if n_iter%100 == 0: print('第%d次迭代' % n_iter) print('准确率： ', acc) print("超过迭代次数") print("当前总体样本准确率为: ", acc) print("当前参数值为: ", self.theta) return self.theta 实验结果]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression]]></title>
    <url>%2F2018%2F10%2F18%2FLinear-Regression%2F</url>
    <content type="text"><![CDATA[引言线性回归可以说是机器学习最基础的算法 模型\hat{y}^{(i)} = w^Tx^{(i)}其中$x^{(i)}_{(n + 1) \times 1}$为第$i$个$n$维向量，$i = 1, \cdots, m$，$w_{(n + 1) \times 1}$为权值向量 \begin{cases} x^{(i)}=\begin{bmatrix} x_0^{(i)} & x_1^{(i)} & ... & x_n^{(i)} \end{bmatrix}^T, x_0^{(i)}=1 \\ w = \begin{bmatrix} w_0 & w_1 & \cdots & w_n \end{bmatrix}^T \end{cases}$x_0^{(i)}=1$项对应偏置项$b$，即$b=w_0$。 注：若需要获得非线性特征，可构造高次特征如$x^{(i)}_{n+1} = (x^{(i)}_{j})^a, a \neq 0, 1$。 损失函数定义误差e^{(i)} = \hat{y}^{(i)} - y^{(i)}其中$y^{(i)}$表示真实值 定义损失函数单个样本的误差定义为 L_{single}(\hat{y}^{(i)}, y^{(i)})=\frac{1}{2}||e^{(i)}||_2^2=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2所有样本的误差(经验误差/风险)定义为 L_{emp}(y, t)=\frac{1}{2N}\sum_i (\hat{y}^{(i)}-y^{(i)})^2也可以定义为误差的和而不是均值，对结果无影响，可视作学习率$α$除去一个常数 梯度推导 \begin{aligned} \frac{\partial L}{\partial w_j} = \frac{\partial }{\partial w_j}\frac{1}{2N}\sum_i(\hat{y}^{(i)}-y^{(i)})^2 \\ = \frac{1}{2N} \sum_i \frac{\partial }{\partial w_j} (\hat{y}^{(i)}-y^{(i)})^2 \\ = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) \frac{\partial t^{(i)}}{\partial w_j} \\ = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} \end{aligned}或者使用矩阵推导，记$m$个$n$维样本组成的样本矩阵为$X_{m \times (n+1)} = \begin{bmatrix} x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \end{bmatrix}^T$，$x^{(i)}$为第$i$个样本的特征(行向量)，$Y_{m \times 1}$为$m$个样本groundtruth组成的向量，那么有 L = \frac{1}{2}(Xw-Y)^T(Xw-Y)∇_w L = X^T(\hat{Y}-Y) \begin{aligned} ∇_w L = \frac{1}{2} ∇_w (w^TX^TXw - Y^TXw - w^TX^TY + Y^TY) \\ = \frac{1}{2} (2X^TXw - X^TY - X^TY) = X^T(Xw-Y) \end{aligned} 在梯度为$\vec{0}$的点，即$∇_w L = \vec{0}$时求解得$w$的无偏估计$\hat{w}$ $$ 令$X^T(Xw-Y) = 0$，有\begin{aligned} X^T(Xw-Y) = 0 \Rightarrow \ X^TXw = X^TY \Rightarrow \ \hat{w} =(X^TX)^{-1} X^TY\end{aligned} 其中$X^+=(X^TX)^{-1} X^T$，表示矩阵$X_{m×n}$的伪逆。但是当 $$ rank(X_{m \times (n + 1)}) < n + 1 样本数$m$小于特征维度数$n+1$； 存在多重共线性问题。 $X^TX$奇异无法求逆，用岭回归可以解决这个问题，在后面介绍，求得岭回归估计如下 \hat{w}(\lambda) =(X^TX+\lambda I)^{-1}X^TY训练采用梯度下降法求解 w := w - \alpha ∇_w L其中$w$表示参数向量 进一步思考：为什么使用梯度下降可以求取最优解呢？ ∇_w^2 L = ∇_w X^T(Xw-Y) = X^TX而对于矩阵 $ X^TX $ u^T(X^TX)u = (Xu)^T(Xu) \geq 0即损失函数的Hessian矩阵$∇_w^2 L$为正定矩阵，$L$为凸函数，存在全局最优解 从投影的角度理解线性回归 线性回归的正则化添加$L_2$正则也称为岭回归(Ridge Regression)，添加$L_1$正则称为Lasso回归(一般用作特征选择)。 岭回归为克服过拟合问题，可加入正则化项$||w||_2^2$，此时损失函数(结构误差/风险)定义为 L(\hat{y}, y) = \frac{1}{2} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_2^2那么优化问题如下 \hat{w}^{ridge} = \arg \min L(\hat{y}, y)或者 \begin{aligned} \hat{w}^{ridge} = \arg \min \left\{ \frac{1}{2} ||\hat{y}^{(i)}-y^{(i)}||_2^2 \right\} \\ s.t. \quad ||w||_2^2 \leq t(与\lambda有关) \end{aligned}几何意义 梯度为 \frac{\partial L}{\partial w_j} = \frac{1}{N} \sum_i (\hat{y}^{(i)}-y^{(i)}) x_j^{(i)} + \frac{\lambda}{N}w_j, j = 1, ..., n + 1此时对应的岭回归参数估计如下 \hat{w}(\lambda) =(X^TX+\lambda I)^{-1}X^TY 当$X^TX$出现奇异时，添加矩阵$\lambda I$可以使其奇异程度降低； 岭回归估计比最小二乘估计稳定，后者是其特殊情况($\lambda=0$)。 性质 $\hat{w}(\lambda)$是$w$的有偏估计，残差平方和增大但是系数检验效果更好； 当认为$\lambda$是与$y$无关的常数时，$\hat{w}(\lambda) =(X^TX+\lambda I)^{-1}X^TY = (X^TX+\lambda I)^{-1}X^T X \hat{w}$，即$\hat{w}(\lambda)$是$\hat{w}$的线性变换； 对于任意$\lambda &gt; 0$，$||\hat{w}|| \neq 0$，总有$|| \hat{w}(\lambda) || &lt; ||\hat{w}||$； 存在$\lambda &gt; 0$，使得$MSE(\hat{w}(\lambda)) &lt; MSE(\hat{w})$。 由于参数$\lambda$并不是唯一确定的，所以得到的$\hat{w}(\lambda)$是回归参数$w$的一个估计族，$\hat{w}_{j}(\lambda) - \lambda$图称为岭迹图，根据以下规则可以筛选有效特征 当$\lambda = 0$时为最小二乘估计，参数$\hat{w}_{j}$不应趋向无穷； 当不存在奇异时，岭迹应稳定渐进趋向$0$； 通过岭迹图可以剔除变量解决多重共线性问题(个别情形下适用) 可以剔除掉标准化岭回归系数比较稳定且绝对值很小的自变量; 随着$\lambda$的增加，回归系数不稳定，震荡趋于零的自变量也可以剔除; 如果依照上述去掉变量的原则，有若干个回归系数不稳定，究竟去掉几个，去掉哪几个，这幵无一般原则可循，这需根据去掉某个变量后重新进行岭回归分析的效果来确定。 一些岭迹图 通过岭迹图选择参数$\lambda$的原则 各回归系数的岭估计基本稳定(如正负)； 用最小二乘估计时符号不合理的回归系数，其岭估计的符号变得合理； 回归系数没有不合乎实际意义的绝对值； 残差平方和增大不太多。 还有方差扩大因子法，注意不同方法建议的选择可能不一致。 存在的问题 岭参数$\lambda$计算方法太多且差异很大； 用岭迹图进行变量筛选，随意性太大，且只能一定程度消除多重共线性，而不能解决其他问题； 岭回归返回的模型若没有经过特征筛选，包含全部变量。 Lasso回归Lasso(The Least Absolute Shrinkage and Selectionator operator)通过构造一阶惩罚函数获得一个精炼的模型，最终能确定一些指标的系数为$0$，擅长处理具有多重共线性的数据，与岭回归一样时有偏估计。 Lasso回归的优化问题是 \hat{w}^{lasso} = \arg \min \left\{ \frac{1}{2} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_1 \right\}或者 \begin{aligned} \hat{w}^{lasso} = \arg \min \left\{ \frac{1}{2} ||\hat{y}^{(i)}-y^{(i)}||_2^2 \right\} \\ s.t. \quad ||w||_1 \leq t(与\lambda有关) \end{aligned}几何意义 但是$L_1$范数不可导，优化目标问题很复杂，可以用最小角回归(Least Angle Regression, LAR)解决，通过收缩的(shrinkage)方法依次选择变量加入。 首先介绍相关系数的几何意义，将$A, B$两个向量去中心化、单位化，即 \begin{cases} \hat{a}_i = (a_i - \overline{A}) / \sigma_A \\ \hat{b}_i = (b_i - \overline{B}) / \sigma_B \end{cases}那么相关系数也即这两个向量的余弦距离 r_{AB} = \frac{\sum_{i=1}^n (a_i - \overline{A})(b_i - \overline{B})}{\sigma_A \sigma_B} = \frac{1}{n} \sum_{i=1}^n \hat{a}_i \hat{b}_i = \hat{A} \cdot \hat{B}= \cosLAR具体算法如下，是一种线性的方法，求解结果与Lasso结果几乎一致(并不完全一致，但近似相同) 初始化：计算输出向量$Y$和属性向量$X_j, j = 1, \cdots. (n + 1)$的相关系数$r_j$，并按相关系数从大到小将属性排序； 第一个特征选择：从原点开始沿着相关系数最大的属性$X_1$游走，得到的向量作为预测输出$\hat{Y}$，残差对应为$Y - \hat{Y}$，那么在这个过程中残差与$X_1$的相关性降低r_1' = (Y - \hat{Y}) \cdot X_1 剩余特征加入：当步骤2中相关系数降低至存在特征$X_2$与残差的相关系数与$r_j’$相等时，即游走到$w_1 X_1$时，将该特征加入，开始沿着$w_1 X_1, X_2 - w_1 X_1$两向量的角平分线$(w_1 X_1 + X_2’)$游走，其中$X_2’ = X_2 - w1 X_1$； 重复步骤2，直至所有加入的特征与残差$Y - \hat{Y}$相关系数小于指定的较小常数$\epsilon$； 此时剩余的特征被丢弃，达到特征筛选的作用。 如下图，当残差为$R_2$时，$R_2$几乎与$X_1, X_2$垂直，相关系数为$0$，停止游走，特征$X_3$被丢弃。 弹性网(elasticnet) \hat{w} = \arg \min \left\{ \frac{1}{2} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda \left[ \alpha ||w||_2^2 + (1 - \alpha) ||w||_1 \right] \right\}q阶惩罚 \hat{w} = \arg \min \left\{ \frac{1}{2} ||\hat{y}^{(i)}-y^{(i)}||_2^2 + \lambda ||w||_q^q \right\} 局部加权线性回归目标函数定义为 L(y, t)=\frac{1}{2N}\sum_i w^{(i)} (\hat{y}^{(i)}-y^{(i)})^2其中 w^{(i)} = e^{-\frac{(x^{(i)}-x)^2}{2\tau^2}}$x$表示输入的预测样本，$x^{(i)}$表示训练样本 离很近的样本，权值接近于1，而对于离很远的样本，此时权值接近于0，这样就是在局部构成线性回归，它依赖的也只是周边的点。 对于线性回归算法，一旦拟合出适合训练数据的参数$w$，保存这些参数$w$，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。而对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数$w$），没有固定的参数$w$，所以是非参数算法。 代码@Github: Code for Linear Regression training step12345678910111213141516171819202122232425262728293031323334353637383940414243def fit(self, X, y, learning_rate=0.01, max_iter=5000, min_loss=10): # --------------- 数据预处理部分 --------------- # 加入全1列 X = np.c_[np.ones(shape=(X.shape[0])), X] # 构造高次特征 if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] # ---------------- 参数迭代部分 ---------------- # 初始化参数 self.theta = np.random.uniform(-1, 1, size=(X.shape[1],)) # 数据批次 n_batch = X.shape[0] if self.n_batch==-1 else self.n_batch batch_size = X.shape[0] // n_batch # 停止条件 n_iter = 0; loss = float('inf') # 开始迭代 for n_iter in range(max_iter): for n in range(n_batch): n1, n2 = n*batch_size, (n+1)*batch_size X_batch = X[n1: n2]; y_batch = y[n1: n2] grad = self.lossFunctionDerivative(X_batch, y_batch) self.theta -= learning_rate * grad loss = self.score(y_batch, self.predict(X_batch)) if loss &lt; min_loss: print('第%d次迭代, 第%d批数据' % (n_iter, n)) print("当前总体样本损失为: ", loss) return self.theta if n_iter%100 == 0: print('第%d次迭代' % n_iter) print("当前总体样本损失为: ", loss) print("超过迭代次数") print("当前总体样本损失为: ", loss) return self.thetadef lossFunctionDerivative(self, X, y): y_pred = self.predict(X) # theta = self.theta; # ！注意：theta = self.theta 不仅仅是赋值，类似引用，修改theta会影响self.theta theta = self.theta.copy() theta[0] = 0 # θ0不需要正则化 return (X.T.dot(y_pred - y) + self.regularize * theta) / X.shape[0] predict step123456789def predict(self, X, preprocessed=False): if preprocessed: # 加入全1列 X = np.c_[np.ones(shape=(X.shape[0])), X] # 构造高次特征 if self.n_ploy &gt; 1: for i in range(2, self.n_ploy + 1): X = np.c_[X, X[:, 1]**i] return X.dot(self.theta) 运行结果 无正则化 正则化]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
</search>
